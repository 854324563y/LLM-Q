[ft_instance_hyperparameter]
data_type=fp16
enable_custom_all_reduce=0
enable_flash_attn=false ;or export BYTENN_FLASH_ATTENTION=ON

tensor_para_size=1
pipeline_para_size=1

model_name=llama2_70b
model_dir=/data00/liusongwei.zju/local/llm_models/llama2/7b_wz/7B_FT/1-gpu
; model_dir=/opt/tiger/llm_models/llama2/70b_zh/70B_FT/4-gpu

[request]
beam_width=1 # beam width for beam search
top_k=1 ; k value for top k sampling
top_p=0.9 ; p value for top p sampling
; temperature=0.2 ; Use for sampling
temperature=1 ; Use for sampling
repetition_penalty=1.0 ; Use for sampling
presence_penalty=0.0  ; Only one of repetition_penalty and presence_penalty are allowed.
len_penalty=0.0
beam_search_diversity_rate=0.0
request_batch_size=1 # determine by the request
request_output_len=2048 # determine by the request. 16384-max_input_len

[llama2_7b]
head_num = 32
kv_head_num = 32
size_per_head = 128
inter_size = 11008
num_layer = 32
rotary_embedding = 128
layernorm_eps = 1e-05
vocab_size = 59452 
start_id = 1
end_id = 54064
weight_data_type = fp16

[openllama_7b]
head_num = 32
kv_head_num = 32
size_per_head = 128
inter_size = 11008
num_layer = 32
rotary_embedding = 128
layernorm_eps = 1e-06
vocab_size = 36156 #59452
start_id = 1
end_id = 54064
weight_data_type = fp16

[llama2_70b]
head_num = 64
kv_head_num = 8
size_per_head = 128
inter_size = 28672
num_layer = 80
rotary_embedding = 128
layernorm_eps = 1e-05
vocab_size = 32000
start_id = 1
end_id = 2
weight_data_type = fp16
rope_scaling=4

