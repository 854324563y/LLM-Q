[2025-02-18 15:02:56 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation/llama-13b-hf-w4a4', save_dir='./log-calibration-compensation/quant/llama-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-18 15:09:53 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 15:09:53 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-18 15:09:53 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 15:09:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 15:10:01 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:10:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.1065305694937706 norm:0.06827468425035477 max memory_allocated 29266.39501953125 
[2025-02-18 15:11:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.06239621341228485 norm:0.03665239363908768 max memory_allocated 29266.39501953125 
[2025-02-18 15:12:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.05644625425338745 norm:0.03208355978131294 max memory_allocated 29266.39501953125 
[2025-02-18 15:13:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.05436592176556587 norm:0.029313990846276283 max memory_allocated 29266.39501953125 
[2025-02-18 15:14:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.05204138159751892 norm:0.024974355474114418 max memory_allocated 29266.39501953125 
[2025-02-18 15:14:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.05074365437030792 norm:0.02219722419977188 max memory_allocated 29266.39501953125 
[2025-02-18 15:15:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.049208953976631165 norm:0.01991237886250019 max memory_allocated 29266.39501953125 
[2025-02-18 15:16:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.047861456871032715 norm:0.01741291955113411 max memory_allocated 29266.39501953125 
[2025-02-18 15:17:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.046830594539642334 norm:0.015443263575434685 max memory_allocated 29266.39501953125 
[2025-02-18 15:18:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.04518119990825653 norm:0.0128709077835083 max memory_allocated 29266.39501953125 
[2025-02-18 15:18:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.04474201053380966 norm:0.012104785069823265 max memory_allocated 29266.39501953125 
[2025-02-18 15:19:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.04454544559121132 norm:0.011316273361444473 max memory_allocated 29266.39501953125 
[2025-02-18 15:20:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.04396729916334152 norm:0.010304948315024376 max memory_allocated 29266.39501953125 
[2025-02-18 15:21:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.04386628791689873 norm:0.010030919685959816 max memory_allocated 29266.39501953125 
[2025-02-18 15:22:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.04348281770944595 norm:0.009241726249456406 max memory_allocated 29266.39501953125 
[2025-02-18 15:23:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.04315454512834549 norm:0.008853982202708721 max memory_allocated 29266.39501953125 
[2025-02-18 15:23:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.04316133260726929 norm:0.00855853222310543 max memory_allocated 29266.39501953125 
[2025-02-18 15:24:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.04322418197989464 norm:0.008416332304477692 max memory_allocated 29266.39501953125 
[2025-02-18 15:25:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.04319204017519951 norm:0.008065593428909779 max memory_allocated 29266.39501953125 
[2025-02-18 15:26:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.04298420250415802 norm:0.007775492500513792 max memory_allocated 29266.39501953125 
[2025-02-18 15:26:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 15:26:39 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:27:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.3347373306751251 norm:0.15188820660114288 max memory_allocated 29266.39501953125 
[2025-02-18 15:28:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.26573944091796875 norm:0.0966235101222992 max memory_allocated 29266.39501953125 
[2025-02-18 15:29:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.23817619681358337 norm:0.06866304576396942 max memory_allocated 29266.39501953125 
[2025-02-18 15:29:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.21690353751182556 norm:0.05282939597964287 max memory_allocated 29266.39501953125 
[2025-02-18 15:30:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.20957788825035095 norm:0.050158172845840454 max memory_allocated 29266.39501953125 
[2025-02-18 15:31:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.19932584464550018 norm:0.04187774285674095 max memory_allocated 29266.39501953125 
[2025-02-18 15:32:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.1883428990840912 norm:0.03215247020125389 max memory_allocated 29266.39501953125 
[2025-02-18 15:33:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.18437714874744415 norm:0.029453560709953308 max memory_allocated 29266.39501953125 
[2025-02-18 15:34:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.18094439804553986 norm:0.02729830890893936 max memory_allocated 29266.39501953125 
[2025-02-18 15:34:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.17794832587242126 norm:0.02513604238629341 max memory_allocated 29266.39501953125 
[2025-02-18 15:35:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.17513799667358398 norm:0.023695241659879684 max memory_allocated 29266.39501953125 
[2025-02-18 15:36:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.1729019284248352 norm:0.02263871766626835 max memory_allocated 29266.39501953125 
[2025-02-18 15:37:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.17256493866443634 norm:0.02274680882692337 max memory_allocated 29266.39501953125 
[2025-02-18 15:38:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.170561745762825 norm:0.0215693898499012 max memory_allocated 29266.39501953125 
[2025-02-18 15:38:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.16908949613571167 norm:0.02109362557530403 max memory_allocated 29266.39501953125 
[2025-02-18 15:39:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.16603884100914001 norm:0.020151153206825256 max memory_allocated 29266.39501953125 
[2025-02-18 15:40:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.15985825657844543 norm:0.019726501777768135 max memory_allocated 29266.39501953125 
[2025-02-18 15:41:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.14991715550422668 norm:0.016984853893518448 max memory_allocated 29266.39501953125 
[2025-02-18 15:42:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.14666777849197388 norm:0.017268182709813118 max memory_allocated 29266.39501953125 
[2025-02-18 15:43:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.14494584500789642 norm:0.01653822883963585 max memory_allocated 29266.39501953125 
[2025-02-18 15:43:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 15:43:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:44:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.329524964094162 norm:0.05675450712442398 max memory_allocated 29266.39501953125 
[2025-02-18 15:45:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.2804344892501831 norm:0.038468122482299805 max memory_allocated 29266.39501953125 
[2025-02-18 15:45:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.25983983278274536 norm:0.03088577277958393 max memory_allocated 29266.39501953125 
[2025-02-18 15:46:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.2453077733516693 norm:0.02623838558793068 max memory_allocated 29266.39501953125 
[2025-02-18 15:47:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.23551498353481293 norm:0.023013530299067497 max memory_allocated 29266.39501953125 
[2025-02-18 15:48:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.22848287224769592 norm:0.020778726786375046 max memory_allocated 29266.39501953125 
[2025-02-18 15:49:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.2242407649755478 norm:0.019727645441889763 max memory_allocated 29266.39501953125 
[2025-02-18 15:49:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.2206299751996994 norm:0.01886596903204918 max memory_allocated 29266.39501953125 
[2025-02-18 15:50:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.21768061816692352 norm:0.0181434229016304 max memory_allocated 29266.39501953125 
[2025-02-18 15:51:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.21655383706092834 norm:0.018078526481986046 max memory_allocated 29266.39501953125 
[2025-02-18 15:52:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.21456485986709595 norm:0.01786988414824009 max memory_allocated 29266.39501953125 
[2025-02-18 15:53:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.21470224857330322 norm:0.017953388392925262 max memory_allocated 29266.39501953125 
[2025-02-18 15:54:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.21477629244327545 norm:0.01788363792002201 max memory_allocated 29266.39501953125 
[2025-02-18 15:54:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.21433497965335846 norm:0.017782827839255333 max memory_allocated 29266.39501953125 
[2025-02-18 15:55:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.21295438706874847 norm:0.018578262999653816 max memory_allocated 29266.39501953125 
[2025-02-18 15:56:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.2118416279554367 norm:0.017441337928175926 max memory_allocated 29266.39501953125 
[2025-02-18 15:57:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.21178466081619263 norm:0.017829127609729767 max memory_allocated 29266.39501953125 
[2025-02-18 15:58:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.210576131939888 norm:0.016467127948999405 max memory_allocated 29266.39501953125 
[2025-02-18 15:59:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.21145007014274597 norm:0.016896652057766914 max memory_allocated 29266.39501953125 
[2025-02-18 15:59:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.21085557341575623 norm:0.01717965304851532 max memory_allocated 29266.39501953125 
[2025-02-18 16:00:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 16:00:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.3342137038707733 norm:0.03338564559817314 max memory_allocated 29266.81298828125 
[2025-02-18 16:01:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.3157752752304077 norm:0.018067846074700356 max memory_allocated 29266.81298828125 
[2025-02-18 16:02:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.3060886859893799 norm:0.010684294626116753 max memory_allocated 29266.81298828125 
[2025-02-18 16:03:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.29706069827079773 norm:0.007176135201007128 max memory_allocated 29266.81298828125 
[2025-02-18 16:04:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.2898194193840027 norm:0.005596647970378399 max memory_allocated 29266.81298828125 
[2025-02-18 16:05:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.28828880190849304 norm:0.005022391211241484 max memory_allocated 29266.81298828125 
[2025-02-18 16:05:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.28637218475341797 norm:0.004457315430045128 max memory_allocated 29266.81298828125 
[2025-02-18 16:06:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.28434890508651733 norm:0.003963431343436241 max memory_allocated 29266.81298828125 
[2025-02-18 16:07:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.2831658720970154 norm:0.003730198834091425 max memory_allocated 29266.81298828125 
[2025-02-18 16:08:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.2811151146888733 norm:0.0034374219831079245 max memory_allocated 29266.81298828125 
[2025-02-18 16:09:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.28052547574043274 norm:0.003340028692036867 max memory_allocated 29266.81298828125 
[2025-02-18 16:09:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.28169530630111694 norm:0.003335268935188651 max memory_allocated 29266.81298828125 
[2025-02-18 16:10:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.280059278011322 norm:0.003167669987305999 max memory_allocated 29266.81298828125 
[2025-02-18 16:11:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.27913618087768555 norm:0.0030788001604378223 max memory_allocated 29266.81298828125 
[2025-02-18 16:12:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.27839481830596924 norm:0.003015464637428522 max memory_allocated 29266.81298828125 
[2025-02-18 16:13:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.2785486876964569 norm:0.0030026682652533054 max memory_allocated 29266.81298828125 
[2025-02-18 16:14:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.2781408727169037 norm:0.0030063344165682793 max memory_allocated 29266.81298828125 
[2025-02-18 16:14:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.27832382917404175 norm:0.0029139977414160967 max memory_allocated 29266.81298828125 
[2025-02-18 16:15:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.2779500484466553 norm:0.002858578460291028 max memory_allocated 29266.81298828125 
[2025-02-18 16:16:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.2772625684738159 norm:0.00283402344211936 max memory_allocated 29266.81298828125 
[2025-02-18 16:16:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 16:17:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.4312317669391632 norm:0.04666768014431 max memory_allocated 29266.81298828125 
[2025-02-18 16:18:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.4038940668106079 norm:0.01681746169924736 max memory_allocated 29266.81298828125 
[2025-02-18 16:19:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.3973589241504669 norm:0.01044445764273405 max memory_allocated 29266.81298828125 
[2025-02-18 16:20:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.39631447196006775 norm:0.006874675862491131 max memory_allocated 29266.81298828125 
[2025-02-18 16:20:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.3920077681541443 norm:0.005310717970132828 max memory_allocated 29266.81298828125 
[2025-02-18 16:21:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.38772475719451904 norm:0.004460331052541733 max memory_allocated 29266.81298828125 
[2025-02-18 16:22:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.38607555627822876 norm:0.003870697459205985 max memory_allocated 29266.81298828125 
[2025-02-18 16:23:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.3842676281929016 norm:0.003559625707566738 max memory_allocated 29266.81298828125 
[2025-02-18 16:24:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.38225671648979187 norm:0.0033230127301067114 max memory_allocated 29266.81298828125 
[2025-02-18 16:25:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.38002800941467285 norm:0.0030846591107547283 max memory_allocated 29266.81298828125 
[2025-02-18 16:25:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.37969040870666504 norm:0.0029849298298358917 max memory_allocated 29266.81298828125 
[2025-02-18 16:26:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.378294974565506 norm:0.0029003338422626257 max memory_allocated 29266.81298828125 
[2025-02-18 16:27:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.37766045331954956 norm:0.0028513013385236263 max memory_allocated 29266.81298828125 
[2025-02-18 16:28:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.377197802066803 norm:0.002822131384164095 max memory_allocated 29266.81298828125 
[2025-02-18 16:29:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.37675076723098755 norm:0.0027732013259083033 max memory_allocated 29266.81298828125 
[2025-02-18 16:29:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.37789496779441833 norm:0.0027818684466183186 max memory_allocated 29266.81298828125 
[2025-02-18 16:30:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.3781256079673767 norm:0.0028672898188233376 max memory_allocated 29266.81298828125 
[2025-02-18 16:31:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.378303200006485 norm:0.002977411262691021 max memory_allocated 29266.81298828125 
[2025-02-18 16:32:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.37874293327331543 norm:0.002966725267469883 max memory_allocated 29266.81298828125 
[2025-02-18 16:33:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.37882763147354126 norm:0.0030591830145567656 max memory_allocated 29266.81298828125 
[2025-02-18 16:33:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 16:34:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.539254903793335 norm:0.06439504772424698 max memory_allocated 29267.18798828125 
[2025-02-18 16:35:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.5143070816993713 norm:0.03263498842716217 max memory_allocated 29267.18798828125 
[2025-02-18 16:36:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.5008777379989624 norm:0.017609432339668274 max memory_allocated 29267.18798828125 
[2025-02-18 16:36:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.49560749530792236 norm:0.012613455764949322 max memory_allocated 29267.18798828125 
[2025-02-18 16:37:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.48840031027793884 norm:0.009570998139679432 max memory_allocated 29267.18798828125 
[2025-02-18 16:38:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.48087427020072937 norm:0.007578914985060692 max memory_allocated 29267.18798828125 
[2025-02-18 16:39:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.47589489817619324 norm:0.006468848325312138 max memory_allocated 29267.18798828125 
[2025-02-18 16:40:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.47196635603904724 norm:0.0057748653925955296 max memory_allocated 29267.18798828125 
[2025-02-18 16:40:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.46843281388282776 norm:0.005257201846688986 max memory_allocated 29267.18798828125 
[2025-02-18 16:41:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.4656379222869873 norm:0.004831921309232712 max memory_allocated 29267.18798828125 
[2025-02-18 16:42:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.4634753167629242 norm:0.004638667218387127 max memory_allocated 29267.18798828125 
[2025-02-18 16:43:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.4591006934642792 norm:0.0041048601269721985 max memory_allocated 29267.18798828125 
[2025-02-18 16:44:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.4576669931411743 norm:0.0039316690526902676 max memory_allocated 29267.18798828125 
[2025-02-18 16:45:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.4573611617088318 norm:0.0038074611220508814 max memory_allocated 29267.18798828125 
[2025-02-18 16:45:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.4571455717086792 norm:0.0037819291464984417 max memory_allocated 29267.18798828125 
[2025-02-18 16:46:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.45690062642097473 norm:0.0038444853853434324 max memory_allocated 29267.18798828125 
[2025-02-18 16:47:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.4563048481941223 norm:0.0038141803815960884 max memory_allocated 29267.18798828125 
[2025-02-18 16:48:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.4560392498970032 norm:0.0037369229830801487 max memory_allocated 29267.18798828125 
[2025-02-18 16:49:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.45531362295150757 norm:0.003684298601001501 max memory_allocated 29267.18798828125 
[2025-02-18 16:50:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.45414483547210693 norm:0.003533124690875411 max memory_allocated 29267.18798828125 
[2025-02-18 16:50:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 16:51:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.6157791018486023 norm:0.013599571771919727 max memory_allocated 29267.18798828125 
[2025-02-18 16:51:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.6025585532188416 norm:0.008920828811824322 max memory_allocated 29267.18798828125 
[2025-02-18 16:52:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.5934720635414124 norm:0.00600500637665391 max memory_allocated 29267.18798828125 
[2025-02-18 16:53:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.590774416923523 norm:0.004904992878437042 max memory_allocated 29267.18798828125 
[2025-02-18 16:54:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.5864129662513733 norm:0.0045981742441654205 max memory_allocated 29267.18798828125 
[2025-02-18 16:55:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.583962619304657 norm:0.004537454806268215 max memory_allocated 29267.18798828125 
[2025-02-18 16:56:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.5818318724632263 norm:0.004590821918100119 max memory_allocated 29267.18798828125 
[2025-02-18 16:56:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.5819951891899109 norm:0.004941049963235855 max memory_allocated 29267.18798828125 
[2025-02-18 16:57:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.5809829235076904 norm:0.005009910091757774 max memory_allocated 29267.18798828125 
[2025-02-18 16:58:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.5802538394927979 norm:0.004672365728765726 max memory_allocated 29267.18798828125 
[2025-02-18 16:59:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.5801613926887512 norm:0.004593183286488056 max memory_allocated 29267.18798828125 
[2025-02-18 17:00:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.5794348120689392 norm:0.0047305491752922535 max memory_allocated 29267.18798828125 
[2025-02-18 17:00:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.5784794092178345 norm:0.004466562997549772 max memory_allocated 29267.18798828125 
[2025-02-18 17:01:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.577689528465271 norm:0.004474456422030926 max memory_allocated 29267.18798828125 
[2025-02-18 17:02:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.5764958262443542 norm:0.004386886954307556 max memory_allocated 29267.18798828125 
[2025-02-18 17:03:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.5760627388954163 norm:0.004398627206683159 max memory_allocated 29267.18798828125 
[2025-02-18 17:04:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.5762977600097656 norm:0.004365060478448868 max memory_allocated 29267.18798828125 
[2025-02-18 17:05:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.5746767520904541 norm:0.0042053163051605225 max memory_allocated 29267.18798828125 
[2025-02-18 17:05:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.5738179683685303 norm:0.003960287664085627 max memory_allocated 29267.18798828125 
[2025-02-18 17:06:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.5740324854850769 norm:0.004115331918001175 max memory_allocated 29267.18798828125 
[2025-02-18 17:06:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 17:07:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.6751795411109924 norm:0.017496440559625626 max memory_allocated 29267.56298828125 
[2025-02-18 17:08:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.6576111316680908 norm:0.009211025200784206 max memory_allocated 29267.56298828125 
[2025-02-18 17:09:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.6503086090087891 norm:0.005509288981556892 max memory_allocated 29267.56298828125 
[2025-02-18 17:10:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.6456353068351746 norm:0.0037936107255518436 max memory_allocated 29267.56298828125 
[2025-02-18 17:11:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.6422905325889587 norm:0.0031655924394726753 max memory_allocated 29267.56298828125 
[2025-02-18 17:11:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.6388426423072815 norm:0.002815342042595148 max memory_allocated 29267.56298828125 
[2025-02-18 17:12:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.6339845657348633 norm:0.002437232993543148 max memory_allocated 29267.56298828125 
[2025-02-18 17:13:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.6311538815498352 norm:0.002242680639028549 max memory_allocated 29267.56298828125 
[2025-02-18 17:14:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.6304507255554199 norm:0.002173854736611247 max memory_allocated 29267.56298828125 
[2025-02-18 17:15:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.6306824088096619 norm:0.002168029546737671 max memory_allocated 29267.56298828125 
[2025-02-18 17:16:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.6313713192939758 norm:0.0021616423036903143 max memory_allocated 29267.56298828125 
[2025-02-18 17:16:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.6318891048431396 norm:0.002185935154557228 max memory_allocated 29267.56298828125 
[2025-02-18 17:17:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.6304354071617126 norm:0.0021246655378490686 max memory_allocated 29267.56298828125 
[2025-02-18 17:18:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.629259467124939 norm:0.002059217542409897 max memory_allocated 29267.56298828125 
[2025-02-18 17:19:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.6307680606842041 norm:0.0021423720754683018 max memory_allocated 29267.56298828125 
[2025-02-18 17:20:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.6318663358688354 norm:0.002193180378526449 max memory_allocated 29267.56298828125 
[2025-02-18 17:21:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.6309683322906494 norm:0.002196483314037323 max memory_allocated 29267.56298828125 
[2025-02-18 17:21:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.6302455067634583 norm:0.002160680713132024 max memory_allocated 29267.56298828125 
[2025-02-18 17:22:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.6307211518287659 norm:0.0021913591772317886 max memory_allocated 29267.56298828125 
[2025-02-18 17:23:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.6303854584693909 norm:0.00218231906183064 max memory_allocated 29267.56298828125 
[2025-02-18 17:23:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 17:24:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.753616213798523 norm:0.029460230842232704 max memory_allocated 29267.56298828125 
[2025-02-18 17:25:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.7332403659820557 norm:0.016829857602715492 max memory_allocated 29267.56298828125 
[2025-02-18 17:26:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.7261790037155151 norm:0.010910158976912498 max memory_allocated 29267.56298828125 
[2025-02-18 17:27:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.7209433913230896 norm:0.007750733755528927 max memory_allocated 29267.56298828125 
[2025-02-18 17:27:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.7144173383712769 norm:0.006061031017452478 max memory_allocated 29267.56298828125 
[2025-02-18 17:28:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.7107652425765991 norm:0.0050757997669279575 max memory_allocated 29267.56298828125 
[2025-02-18 17:29:32 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.707874059677124 norm:0.004528936930000782 max memory_allocated 29267.56298828125 
[2025-02-18 17:30:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.7046883702278137 norm:0.004104475490748882 max memory_allocated 29267.56298828125 
[2025-02-18 17:31:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.7037672996520996 norm:0.0039644911885261536 max memory_allocated 29267.56298828125 
[2025-02-18 17:32:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.7033452987670898 norm:0.003857579780742526 max memory_allocated 29267.56298828125 
[2025-02-18 17:32:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.700832724571228 norm:0.0036672279238700867 max memory_allocated 29267.56298828125 
[2025-02-18 17:33:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.700355589389801 norm:0.003686615265905857 max memory_allocated 29267.56298828125 
[2025-02-18 17:34:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.6989428997039795 norm:0.003616483649238944 max memory_allocated 29267.56298828125 
[2025-02-18 17:35:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.6969959735870361 norm:0.00353112630546093 max memory_allocated 29267.56298828125 
[2025-02-18 17:36:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.6959398984909058 norm:0.0035015845205634832 max memory_allocated 29267.56298828125 
[2025-02-18 17:36:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.6944162845611572 norm:0.0033046535681933165 max memory_allocated 29267.56298828125 
[2025-02-18 17:37:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.693347692489624 norm:0.0033095180988311768 max memory_allocated 29267.56298828125 
[2025-02-18 17:38:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.6931005120277405 norm:0.0032948837615549564 max memory_allocated 29267.56298828125 
[2025-02-18 17:39:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.6934788227081299 norm:0.0032909391447901726 max memory_allocated 29267.56298828125 
[2025-02-18 17:40:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.6932743191719055 norm:0.0033242569770663977 max memory_allocated 29267.56298828125 
[2025-02-18 17:40:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 17:41:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.8144599199295044 norm:0.024280015379190445 max memory_allocated 29267.56298828125 
[2025-02-18 17:42:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.7959016561508179 norm:0.01312636025249958 max memory_allocated 29267.56298828125 
[2025-02-18 17:42:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.7879055738449097 norm:0.007709966041147709 max memory_allocated 29267.56298828125 
[2025-02-18 17:43:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.7846159934997559 norm:0.005094378720968962 max memory_allocated 29267.56298828125 
[2025-02-18 17:44:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.7833748459815979 norm:0.00412024324759841 max memory_allocated 29267.56298828125 
[2025-02-18 17:45:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.7800716161727905 norm:0.0036842674016952515 max memory_allocated 29267.56298828125 
[2025-02-18 17:46:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.7802592515945435 norm:0.0036761118099093437 max memory_allocated 29267.56298828125 
[2025-02-18 17:47:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.7775287628173828 norm:0.0034560870844870806 max memory_allocated 29267.56298828125 
[2025-02-18 17:47:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.7748354077339172 norm:0.003322259057313204 max memory_allocated 29267.56298828125 
[2025-02-18 17:48:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.7748607993125916 norm:0.0032595989760011435 max memory_allocated 29267.56298828125 
[2025-02-18 17:49:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.774629533290863 norm:0.0032920422963798046 max memory_allocated 29267.56298828125 
[2025-02-18 17:50:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.7741121053695679 norm:0.0032814021687954664 max memory_allocated 29267.56298828125 
[2025-02-18 17:51:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.7719753384590149 norm:0.0031857523135840893 max memory_allocated 29267.56298828125 
[2025-02-18 17:52:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.7723281383514404 norm:0.003160684835165739 max memory_allocated 29267.56298828125 
[2025-02-18 17:52:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.7713558077812195 norm:0.003125874325633049 max memory_allocated 29267.56298828125 
[2025-02-18 17:53:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.7714600563049316 norm:0.003118104301393032 max memory_allocated 29267.56298828125 
[2025-02-18 17:54:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.768540620803833 norm:0.002977306954562664 max memory_allocated 29267.56298828125 
[2025-02-18 17:55:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.7678133845329285 norm:0.0029408703558146954 max memory_allocated 29267.56298828125 
[2025-02-18 17:56:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.7682811617851257 norm:0.002941276878118515 max memory_allocated 29267.56298828125 
[2025-02-18 17:56:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.7691946625709534 norm:0.0029826629906892776 max memory_allocated 29267.56298828125 
[2025-02-18 17:57:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 17:58:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.8747789263725281 norm:0.0461435541510582 max memory_allocated 29267.56298828125 
[2025-02-18 17:58:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.8544897437095642 norm:0.025358714163303375 max memory_allocated 29267.56298828125 
[2025-02-18 17:59:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.8519564270973206 norm:0.015794415026903152 max memory_allocated 29267.56298828125 
[2025-02-18 18:00:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.8453094363212585 norm:0.010147473774850368 max memory_allocated 29267.56298828125 
[2025-02-18 18:01:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.8416568636894226 norm:0.007237272337079048 max memory_allocated 29267.56298828125 
[2025-02-18 18:02:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.8433670997619629 norm:0.006053084973245859 max memory_allocated 29267.56298828125 
[2025-02-18 18:03:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.8420843482017517 norm:0.00519966334104538 max memory_allocated 29267.56298828125 
[2025-02-18 18:03:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.841157853603363 norm:0.004624553024768829 max memory_allocated 29267.56298828125 
[2025-02-18 18:04:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.8384524583816528 norm:0.004142591264098883 max memory_allocated 29267.56298828125 
[2025-02-18 18:05:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.8361196517944336 norm:0.003828616812825203 max memory_allocated 29267.56298828125 
[2025-02-18 18:06:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.8333022594451904 norm:0.0035264489706605673 max memory_allocated 29267.56298828125 
[2025-02-18 18:07:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.8339141607284546 norm:0.00342714786529541 max memory_allocated 29267.56298828125 
[2025-02-18 18:07:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.8355876803398132 norm:0.003327888436615467 max memory_allocated 29267.56298828125 
[2025-02-18 18:08:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.8347999453544617 norm:0.003207193687558174 max memory_allocated 29267.56298828125 
[2025-02-18 18:09:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.8339031338691711 norm:0.0030393681954592466 max memory_allocated 29267.56298828125 
[2025-02-18 18:10:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.83428955078125 norm:0.0029433618765324354 max memory_allocated 29267.56298828125 
[2025-02-18 18:11:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.8337934017181396 norm:0.002869074698537588 max memory_allocated 29267.56298828125 
[2025-02-18 18:12:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.8330099582672119 norm:0.0027906601317226887 max memory_allocated 29267.56298828125 
[2025-02-18 18:12:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.8322882652282715 norm:0.0027241669595241547 max memory_allocated 29267.56298828125 
[2025-02-18 18:13:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.831434965133667 norm:0.002679771976545453 max memory_allocated 29267.56298828125 
[2025-02-18 18:13:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 18:14:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.939323365688324 norm:0.02420378103852272 max memory_allocated 29268.31298828125 
[2025-02-18 18:15:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.9250056743621826 norm:0.014205080457031727 max memory_allocated 29268.31298828125 
[2025-02-18 18:16:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.9163615107536316 norm:0.008919954299926758 max memory_allocated 29268.31298828125 
[2025-02-18 18:17:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.9134780168533325 norm:0.0062469495460391045 max memory_allocated 29268.31298828125 
[2025-02-18 18:18:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.9106270670890808 norm:0.004881999921053648 max memory_allocated 29268.31298828125 
[2025-02-18 18:18:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.9102068543434143 norm:0.00404556468129158 max memory_allocated 29268.31298828125 
[2025-02-18 18:19:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.9088990688323975 norm:0.003495751414448023 max memory_allocated 29268.31298828125 
[2025-02-18 18:20:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.9080848693847656 norm:0.003185183508321643 max memory_allocated 29268.31298828125 
[2025-02-18 18:21:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.9070240259170532 norm:0.0029053487814962864 max memory_allocated 29268.31298828125 
[2025-02-18 18:22:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.9071976542472839 norm:0.0028040132019668818 max memory_allocated 29268.31298828125 
[2025-02-18 18:23:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.9069193005561829 norm:0.0026556102093309164 max memory_allocated 29268.31298828125 
[2025-02-18 18:23:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.9068878889083862 norm:0.002587660914286971 max memory_allocated 29268.31298828125 
[2025-02-18 18:24:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.9058878421783447 norm:0.0025841426104307175 max memory_allocated 29268.31298828125 
[2025-02-18 18:25:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.9052515625953674 norm:0.002515533473342657 max memory_allocated 29268.31298828125 
[2025-02-18 18:26:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.9051014184951782 norm:0.0024858294054865837 max memory_allocated 29268.31298828125 
[2025-02-18 18:27:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.9047934412956238 norm:0.0024864848237484694 max memory_allocated 29268.31298828125 
[2025-02-18 18:27:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.9050173759460449 norm:0.002454460831359029 max memory_allocated 29268.31298828125 
[2025-02-18 18:28:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.9052286744117737 norm:0.002449403051286936 max memory_allocated 29268.31298828125 
[2025-02-18 18:29:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.9044050574302673 norm:0.002427185419946909 max memory_allocated 29268.31298828125 
[2025-02-18 18:30:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.9038423299789429 norm:0.0024119766894727945 max memory_allocated 29268.31298828125 
[2025-02-18 18:30:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 18:31:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.9695783853530884 norm:0.020882252603769302 max memory_allocated 29268.50048828125 
[2025-02-18 18:32:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.9609874486923218 norm:0.01168745569884777 max memory_allocated 29268.50048828125 
[2025-02-18 18:33:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.9531803727149963 norm:0.007050223648548126 max memory_allocated 29268.50048828125 
[2025-02-18 18:34:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.950915515422821 norm:0.005227845627814531 max memory_allocated 29268.50048828125 
[2025-02-18 18:34:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.9497464895248413 norm:0.004483839962631464 max memory_allocated 29268.50048828125 
[2025-02-18 18:35:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.9483708739280701 norm:0.0038471391890197992 max memory_allocated 29268.50048828125 
[2025-02-18 18:36:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.9481464624404907 norm:0.003461236832663417 max memory_allocated 29268.50048828125 
[2025-02-18 18:37:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.9511158466339111 norm:0.0033301350194960833 max memory_allocated 29268.50048828125 
[2025-02-18 18:38:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.9526678323745728 norm:0.0032608078327029943 max memory_allocated 29268.50048828125 
[2025-02-18 18:38:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.9525984525680542 norm:0.0032048963475972414 max memory_allocated 29268.50048828125 
[2025-02-18 18:39:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.9517751932144165 norm:0.003055625595152378 max memory_allocated 29268.50048828125 
[2025-02-18 18:40:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.9521734714508057 norm:0.00309972302056849 max memory_allocated 29268.50048828125 
[2025-02-18 18:41:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.9518903493881226 norm:0.0031071826815605164 max memory_allocated 29268.50048828125 
[2025-02-18 18:42:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.9513914585113525 norm:0.0030597804579883814 max memory_allocated 29268.50048828125 
[2025-02-18 18:43:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.9509182572364807 norm:0.0030262907966971397 max memory_allocated 29268.50048828125 
[2025-02-18 18:43:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.9506293535232544 norm:0.0029672186356037855 max memory_allocated 29268.50048828125 
[2025-02-18 18:44:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.9504570364952087 norm:0.0029861920047551394 max memory_allocated 29268.50048828125 
[2025-02-18 18:45:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.9491357803344727 norm:0.0028831101953983307 max memory_allocated 29268.50048828125 
[2025-02-18 18:46:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.9491415619850159 norm:0.0029151551425457 max memory_allocated 29268.50048828125 
[2025-02-18 18:47:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.9486077427864075 norm:0.002874360652640462 max memory_allocated 29268.50048828125 
[2025-02-18 18:47:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 18:48:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.9995689392089844 norm:0.008255275897681713 max memory_allocated 29268.68798828125 
[2025-02-18 18:49:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.9920927286148071 norm:0.00602165749296546 max memory_allocated 29268.68798828125 
[2025-02-18 18:49:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.9917877912521362 norm:0.004787299316376448 max memory_allocated 29268.68798828125 
[2025-02-18 18:50:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.9876332879066467 norm:0.0032235546968877316 max memory_allocated 29268.68798828125 
[2025-02-18 18:51:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.9846194982528687 norm:0.0025171886663883924 max memory_allocated 29268.68798828125 
[2025-02-18 18:52:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.9839317202568054 norm:0.002285654190927744 max memory_allocated 29268.68798828125 
[2025-02-18 18:53:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.9812256097793579 norm:0.002203078707680106 max memory_allocated 29268.68798828125 
[2025-02-18 18:54:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.9807586669921875 norm:0.0021284010726958513 max memory_allocated 29268.68798828125 
[2025-02-18 18:54:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.9791933298110962 norm:0.002043176209554076 max memory_allocated 29268.68798828125 
[2025-02-18 18:55:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.9787346124649048 norm:0.002048115013167262 max memory_allocated 29268.68798828125 
[2025-02-18 18:56:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.9781492352485657 norm:0.001991699682548642 max memory_allocated 29268.68798828125 
[2025-02-18 18:57:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.978770911693573 norm:0.0020432244054973125 max memory_allocated 29268.68798828125 
[2025-02-18 18:58:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.9787597060203552 norm:0.0020596119575202465 max memory_allocated 29268.68798828125 
[2025-02-18 18:58:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.9762930274009705 norm:0.0021815099753439426 max memory_allocated 29268.68798828125 
[2025-02-18 18:59:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.9786376357078552 norm:0.0022349213249981403 max memory_allocated 29268.68798828125 
[2025-02-18 19:00:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.9810283184051514 norm:0.0024172309786081314 max memory_allocated 29268.68798828125 
[2025-02-18 19:01:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.9820261597633362 norm:0.0027448965702205896 max memory_allocated 29268.68798828125 
[2025-02-18 19:02:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.9842090606689453 norm:0.002983693964779377 max memory_allocated 29268.68798828125 
[2025-02-18 19:03:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.9829393625259399 norm:0.0031271318439394236 max memory_allocated 29268.68798828125 
[2025-02-18 19:03:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.9818168878555298 norm:0.003125022631138563 max memory_allocated 29268.68798828125 
[2025-02-18 19:04:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 19:05:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:1.0810058116912842 norm:0.033754147589206696 max memory_allocated 29268.87548828125 
[2025-02-18 19:05:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:1.0619077682495117 norm:0.019864028319716454 max memory_allocated 29268.87548828125 
[2025-02-18 19:06:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:1.0585660934448242 norm:0.013847747817635536 max memory_allocated 29268.87548828125 
[2025-02-18 19:07:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:1.0553524494171143 norm:0.010316664353013039 max memory_allocated 29268.87548828125 
[2025-02-18 19:08:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:1.0541943311691284 norm:0.0084783174097538 max memory_allocated 29268.87548828125 
[2025-02-18 19:09:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:1.0487804412841797 norm:0.006425593513995409 max memory_allocated 29268.87548828125 
[2025-02-18 19:09:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:1.0459027290344238 norm:0.005691990721970797 max memory_allocated 29268.87548828125 
[2025-02-18 19:10:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:1.051522970199585 norm:0.006864474155008793 max memory_allocated 29268.87548828125 
[2025-02-18 19:11:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:1.0527825355529785 norm:0.009016856551170349 max memory_allocated 29268.87548828125 
[2025-02-18 19:12:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:1.0502715110778809 norm:0.010080810636281967 max memory_allocated 29268.87548828125 
[2025-02-18 19:13:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:1.0509259700775146 norm:0.010007350705564022 max memory_allocated 29268.87548828125 
[2025-02-18 19:14:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:1.0504456758499146 norm:0.010864547453820705 max memory_allocated 29268.87548828125 
[2025-02-18 19:14:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:1.0445581674575806 norm:0.009270841255784035 max memory_allocated 29268.87548828125 
[2025-02-18 19:15:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:1.0410116910934448 norm:0.008426981046795845 max memory_allocated 29268.87548828125 
[2025-02-18 19:16:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:1.040075659751892 norm:0.008358387276530266 max memory_allocated 29268.87548828125 
[2025-02-18 19:17:22 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:1.0386078357696533 norm:0.008276930078864098 max memory_allocated 29268.87548828125 
[2025-02-18 19:18:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:1.0384342670440674 norm:0.007910863496363163 max memory_allocated 29268.87548828125 
[2025-02-18 19:19:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:1.0366666316986084 norm:0.0069790612906217575 max memory_allocated 29268.87548828125 
[2025-02-18 19:19:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:1.03509521484375 norm:0.007097206078469753 max memory_allocated 29268.87548828125 
[2025-02-18 19:20:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:1.0348620414733887 norm:0.006878254935145378 max memory_allocated 29268.87548828125 
[2025-02-18 19:20:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 19:21:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:1.1733523607254028 norm:0.07016787678003311 max memory_allocated 29269.06298828125 
[2025-02-18 19:22:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:1.1513983011245728 norm:0.03951241075992584 max memory_allocated 29269.06298828125 
[2025-02-18 19:23:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:1.1425262689590454 norm:0.026252584531903267 max memory_allocated 29269.06298828125 
[2025-02-18 19:24:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:1.1421163082122803 norm:0.02003394067287445 max memory_allocated 29269.06298828125 
[2025-02-18 19:25:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:1.1389744281768799 norm:0.015009880997240543 max memory_allocated 29269.06298828125 
[2025-02-18 19:25:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:1.1359686851501465 norm:0.011716451495885849 max memory_allocated 29269.06298828125 
[2025-02-18 19:26:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:1.126001000404358 norm:0.00855478085577488 max memory_allocated 29269.06298828125 
[2025-02-18 19:27:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:1.1215944290161133 norm:0.008232544176280499 max memory_allocated 29269.06298828125 
[2025-02-18 19:28:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:1.1165578365325928 norm:0.010280558839440346 max memory_allocated 29269.06298828125 
[2025-02-18 19:29:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:1.105819821357727 norm:0.014006362296640873 max memory_allocated 29269.06298828125 
[2025-02-18 19:29:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:1.0993859767913818 norm:0.01593136414885521 max memory_allocated 29269.06298828125 
[2025-02-18 19:30:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:1.0965150594711304 norm:0.020609883591532707 max memory_allocated 29269.06298828125 
[2025-02-18 19:31:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:1.0940335988998413 norm:0.0209646075963974 max memory_allocated 29269.06298828125 
[2025-02-18 19:32:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:1.0935018062591553 norm:0.02367868460714817 max memory_allocated 29269.06298828125 
[2025-02-18 19:33:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:1.0933932065963745 norm:0.024478597566485405 max memory_allocated 29269.06298828125 
[2025-02-18 19:34:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:1.0932785272598267 norm:0.02639925479888916 max memory_allocated 29269.06298828125 
[2025-02-18 19:34:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:1.092241883277893 norm:0.023308731615543365 max memory_allocated 29269.06298828125 
[2025-02-18 19:35:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:1.0926555395126343 norm:0.022158853709697723 max memory_allocated 29269.06298828125 
[2025-02-18 19:36:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:1.093822956085205 norm:0.023952092975378036 max memory_allocated 29269.06298828125 
[2025-02-18 19:37:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:1.0931687355041504 norm:0.02203955315053463 max memory_allocated 29269.06298828125 
[2025-02-18 19:37:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 19:38:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:1.1912178993225098 norm:0.022203562781214714 max memory_allocated 29269.25048828125 
[2025-02-18 19:39:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:1.1749341487884521 norm:0.012917472049593925 max memory_allocated 29269.25048828125 
[2025-02-18 19:40:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:1.168616533279419 norm:0.009093687869608402 max memory_allocated 29269.25048828125 
[2025-02-18 19:40:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:1.1678798198699951 norm:0.007199681363999844 max memory_allocated 29269.25048828125 
[2025-02-18 19:41:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:1.1664879322052002 norm:0.005236248020082712 max memory_allocated 29269.25048828125 
[2025-02-18 19:42:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:1.1672388315200806 norm:0.004742549732327461 max memory_allocated 29269.25048828125 
[2025-02-18 19:43:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:1.1670496463775635 norm:0.004413734655827284 max memory_allocated 29269.25048828125 
[2025-02-18 19:44:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:1.1685091257095337 norm:0.004447123501449823 max memory_allocated 29269.25048828125 
[2025-02-18 19:45:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:1.170480728149414 norm:0.0044957962818443775 max memory_allocated 29269.25048828125 
[2025-02-18 19:45:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:1.1720658540725708 norm:0.004536885302513838 max memory_allocated 29269.25048828125 
[2025-02-18 19:46:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:1.1715468168258667 norm:0.0044288793578743935 max memory_allocated 29269.25048828125 
[2025-02-18 19:47:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:1.171334147453308 norm:0.004361152648925781 max memory_allocated 29269.25048828125 
[2025-02-18 19:48:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:1.1704561710357666 norm:0.004242729861289263 max memory_allocated 29269.25048828125 
[2025-02-18 19:49:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:1.1692674160003662 norm:0.004115098621696234 max memory_allocated 29269.25048828125 
[2025-02-18 19:50:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:1.168436884880066 norm:0.00404133228585124 max memory_allocated 29269.25048828125 
[2025-02-18 19:50:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:1.1671453714370728 norm:0.003939715214073658 max memory_allocated 29269.25048828125 
[2025-02-18 19:51:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:1.166922926902771 norm:0.003920332528650761 max memory_allocated 29269.25048828125 
[2025-02-18 19:52:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:1.1666170358657837 norm:0.0039038308896124363 max memory_allocated 29269.25048828125 
[2025-02-18 19:53:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:1.1666313409805298 norm:0.003907099366188049 max memory_allocated 29269.25048828125 
[2025-02-18 19:54:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:1.1662880182266235 norm:0.003880004398524761 max memory_allocated 29269.25048828125 
[2025-02-18 19:54:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 19:55:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:1.2696110010147095 norm:0.024225391447544098 max memory_allocated 29269.43798828125 
[2025-02-18 19:56:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:1.252047061920166 norm:0.013121549971401691 max memory_allocated 29269.43798828125 
[2025-02-18 19:56:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:1.248732328414917 norm:0.00987821351736784 max memory_allocated 29269.43798828125 
[2025-02-18 19:57:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:1.2478760480880737 norm:0.007713313214480877 max memory_allocated 29269.43798828125 
[2025-02-18 19:58:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:1.2460904121398926 norm:0.0059468988329172134 max memory_allocated 29269.43798828125 
[2025-02-18 19:59:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:1.2433922290802002 norm:0.005342213436961174 max memory_allocated 29269.43798828125 
[2025-02-18 20:00:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:1.241263508796692 norm:0.004954087547957897 max memory_allocated 29269.43798828125 
[2025-02-18 20:01:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:1.238378882408142 norm:0.00597743596881628 max memory_allocated 29269.43798828125 
[2025-02-18 20:01:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:1.2355306148529053 norm:0.007436548359692097 max memory_allocated 29269.43798828125 
[2025-02-18 20:02:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:1.2346062660217285 norm:0.00782465748488903 max memory_allocated 29269.43798828125 
[2025-02-18 20:03:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:1.2327218055725098 norm:0.007561888080090284 max memory_allocated 29269.43798828125 
[2025-02-18 20:04:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:1.2315659523010254 norm:0.00760321319103241 max memory_allocated 29269.43798828125 
[2025-02-18 20:05:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:1.2305169105529785 norm:0.007174175698310137 max memory_allocated 29269.43798828125 
[2025-02-18 20:05:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:1.229705810546875 norm:0.006881989538669586 max memory_allocated 29269.43798828125 
[2025-02-18 20:06:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:1.2293452024459839 norm:0.0064999619498848915 max memory_allocated 29269.43798828125 
[2025-02-18 20:07:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:1.228630781173706 norm:0.0065435804426670074 max memory_allocated 29269.43798828125 
[2025-02-18 20:08:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:1.2283862829208374 norm:0.006540791131556034 max memory_allocated 29269.43798828125 
[2025-02-18 20:09:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:1.2281399965286255 norm:0.006427193060517311 max memory_allocated 29269.43798828125 
[2025-02-18 20:10:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:1.2276824712753296 norm:0.006287636701017618 max memory_allocated 29269.43798828125 
[2025-02-18 20:10:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:1.2275065183639526 norm:0.006267358548939228 max memory_allocated 29269.43798828125 
[2025-02-18 20:11:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 20:12:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:1.411488652229309 norm:0.05670667067170143 max memory_allocated 29269.62548828125 
[2025-02-18 20:12:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:1.3880351781845093 norm:0.033983875066041946 max memory_allocated 29269.62548828125 
[2025-02-18 20:13:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:1.383373737335205 norm:0.024783456698060036 max memory_allocated 29269.62548828125 
[2025-02-18 20:14:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:1.3859468698501587 norm:0.018989406526088715 max memory_allocated 29269.62548828125 
[2025-02-18 20:15:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:1.3838956356048584 norm:0.014426062814891338 max memory_allocated 29269.62548828125 
[2025-02-18 20:16:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:1.3791005611419678 norm:0.011221975088119507 max memory_allocated 29269.62548828125 
[2025-02-18 20:16:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:1.3757191896438599 norm:0.009557081386446953 max memory_allocated 29269.62548828125 
[2025-02-18 20:17:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:1.3691887855529785 norm:0.00879424437880516 max memory_allocated 29269.62548828125 
[2025-02-18 20:18:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:1.363179326057434 norm:0.011763349175453186 max memory_allocated 29269.62548828125 
[2025-02-18 20:19:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:1.3590989112854004 norm:0.014200241304934025 max memory_allocated 29269.62548828125 
[2025-02-18 20:20:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:1.3551304340362549 norm:0.014877239242196083 max memory_allocated 29269.62548828125 
[2025-02-18 20:21:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:1.353549838066101 norm:0.015977926552295685 max memory_allocated 29269.62548828125 
[2025-02-18 20:21:51 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:1.3529722690582275 norm:0.01742403395473957 max memory_allocated 29269.62548828125 
[2025-02-18 20:22:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:1.3528426885604858 norm:0.018261877819895744 max memory_allocated 29269.62548828125 
[2025-02-18 20:23:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:1.353594422340393 norm:0.018125230446457863 max memory_allocated 29269.62548828125 
[2025-02-18 20:24:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:1.352370262145996 norm:0.017609847709536552 max memory_allocated 29269.62548828125 
[2025-02-18 20:25:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:1.3516995906829834 norm:0.01719030924141407 max memory_allocated 29269.62548828125 
[2025-02-18 20:25:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:1.3516440391540527 norm:0.01737738586962223 max memory_allocated 29269.62548828125 
[2025-02-18 20:26:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:1.3504599332809448 norm:0.01662055402994156 max memory_allocated 29269.62548828125 
[2025-02-18 20:27:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:1.3497835397720337 norm:0.016553370282053947 max memory_allocated 29269.62548828125 
[2025-02-18 20:27:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 20:28:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:1.5104315280914307 norm:0.018719295039772987 max memory_allocated 29269.81298828125 
[2025-02-18 20:29:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:1.4926499128341675 norm:0.009894398041069508 max memory_allocated 29269.81298828125 
[2025-02-18 20:30:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:1.4881669282913208 norm:0.006990303285419941 max memory_allocated 29269.81298828125 
[2025-02-18 20:31:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:1.486774206161499 norm:0.005311976186931133 max memory_allocated 29269.81298828125 
[2025-02-18 20:32:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:1.4855996370315552 norm:0.004343322012573481 max memory_allocated 29269.81298828125 
[2025-02-18 20:32:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:1.4841622114181519 norm:0.003634154098108411 max memory_allocated 29269.81298828125 
[2025-02-18 20:33:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:1.4833177328109741 norm:0.00323176896199584 max memory_allocated 29269.81298828125 
[2025-02-18 20:34:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:1.483209490776062 norm:0.0030062259174883366 max memory_allocated 29269.81298828125 
[2025-02-18 20:35:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:1.4831993579864502 norm:0.0028414453845471144 max memory_allocated 29269.81298828125 
[2025-02-18 20:36:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:1.4817392826080322 norm:0.0026934256311506033 max memory_allocated 29269.81298828125 
[2025-02-18 20:36:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:1.4803463220596313 norm:0.0025537407491356134 max memory_allocated 29269.81298828125 
[2025-02-18 20:37:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:1.4790593385696411 norm:0.002446505008265376 max memory_allocated 29269.81298828125 
[2025-02-18 20:38:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:1.478104591369629 norm:0.00237213633954525 max memory_allocated 29269.81298828125 
[2025-02-18 20:39:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:1.4773719310760498 norm:0.0023251366801559925 max memory_allocated 29269.81298828125 
[2025-02-18 20:40:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:1.4770989418029785 norm:0.002287583891302347 max memory_allocated 29269.81298828125 
[2025-02-18 20:41:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:1.4765794277191162 norm:0.0022324600722640753 max memory_allocated 29269.81298828125 
[2025-02-18 20:41:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:1.4760161638259888 norm:0.0022051308769732714 max memory_allocated 29269.81298828125 
[2025-02-18 20:42:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:1.4755336046218872 norm:0.002171293832361698 max memory_allocated 29269.81298828125 
[2025-02-18 20:43:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:1.4754737615585327 norm:0.0021616006270051003 max memory_allocated 29269.81298828125 
[2025-02-18 20:44:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:1.4753395318984985 norm:0.0021326574496924877 max memory_allocated 29269.81298828125 
[2025-02-18 20:44:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 20:45:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:1.688167929649353 norm:0.043381430208683014 max memory_allocated 29270.00048828125 
[2025-02-18 20:46:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:1.6590145826339722 norm:0.022561656311154366 max memory_allocated 29270.00048828125 
[2025-02-18 20:47:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:1.652922511100769 norm:0.01638178899884224 max memory_allocated 29270.00048828125 
[2025-02-18 20:47:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:1.6521860361099243 norm:0.012862461619079113 max memory_allocated 29270.00048828125 
[2025-02-18 20:48:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:1.653670310974121 norm:0.010547075420618057 max memory_allocated 29270.00048828125 
[2025-02-18 20:49:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:1.6526308059692383 norm:0.00871629361063242 max memory_allocated 29270.00048828125 
[2025-02-18 20:50:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:1.6502381563186646 norm:0.007538208272308111 max memory_allocated 29270.00048828125 
[2025-02-18 20:51:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:1.6472954750061035 norm:0.006443101912736893 max memory_allocated 29270.00048828125 
[2025-02-18 20:52:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:1.6448414325714111 norm:0.0058027226477861404 max memory_allocated 29270.00048828125 
[2025-02-18 20:52:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:1.6416921615600586 norm:0.005274950992316008 max memory_allocated 29270.00048828125 
[2025-02-18 20:53:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:1.6402556896209717 norm:0.004971198737621307 max memory_allocated 29270.00048828125 
[2025-02-18 20:54:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:1.6367547512054443 norm:0.004784370306879282 max memory_allocated 29270.00048828125 
[2025-02-18 20:55:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:1.6339478492736816 norm:0.005243080202490091 max memory_allocated 29270.00048828125 
[2025-02-18 20:56:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:1.6329536437988281 norm:0.005863327533006668 max memory_allocated 29270.00048828125 
[2025-02-18 20:56:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:1.6327370405197144 norm:0.005795077420771122 max memory_allocated 29270.00048828125 
[2025-02-18 20:57:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:1.6313761472702026 norm:0.00605414342135191 max memory_allocated 29270.00048828125 
[2025-02-18 20:58:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:1.6307423114776611 norm:0.006170592270791531 max memory_allocated 29270.00048828125 
[2025-02-18 20:59:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:1.6302026510238647 norm:0.0065730479545891285 max memory_allocated 29270.00048828125 
[2025-02-18 21:00:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:1.6301904916763306 norm:0.0065277935937047005 max memory_allocated 29270.00048828125 
[2025-02-18 21:01:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:1.630576491355896 norm:0.0068090432323515415 max memory_allocated 29270.00048828125 
[2025-02-18 21:01:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 21:02:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:1.8472868204116821 norm:0.012779942713677883 max memory_allocated 29270.18798828125 
[2025-02-18 21:03:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:1.8342022895812988 norm:0.007836979813873768 max memory_allocated 29270.18798828125 
[2025-02-18 21:03:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:1.83135986328125 norm:0.00584027636796236 max memory_allocated 29270.18798828125 
[2025-02-18 21:04:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:1.830501914024353 norm:0.004400923382490873 max memory_allocated 29270.18798828125 
[2025-02-18 21:05:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:1.8295493125915527 norm:0.003492443822324276 max memory_allocated 29270.18798828125 
[2025-02-18 21:06:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:1.8277907371520996 norm:0.00297466479241848 max memory_allocated 29270.18798828125 
[2025-02-18 21:07:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:1.8259615898132324 norm:0.002738173818215728 max memory_allocated 29270.18798828125 
[2025-02-18 21:07:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:1.8261611461639404 norm:0.0026767197996377945 max memory_allocated 29270.18798828125 
[2025-02-18 21:08:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:1.8244847059249878 norm:0.0024923388846218586 max memory_allocated 29270.18798828125 
[2025-02-18 21:09:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:1.8222136497497559 norm:0.0023964475840330124 max memory_allocated 29270.18798828125 
[2025-02-18 21:10:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:1.819368839263916 norm:0.0023577248211950064 max memory_allocated 29270.18798828125 
[2025-02-18 21:11:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:1.818274974822998 norm:0.0025066877715289593 max memory_allocated 29270.18798828125 
[2025-02-18 21:12:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:1.8178819417953491 norm:0.0026925913989543915 max memory_allocated 29270.18798828125 
[2025-02-18 21:12:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:1.8172550201416016 norm:0.002944305771961808 max memory_allocated 29270.18798828125 
[2025-02-18 21:13:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:1.816663384437561 norm:0.0030041164718568325 max memory_allocated 29270.18798828125 
[2025-02-18 21:14:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:1.815929651260376 norm:0.002998954616487026 max memory_allocated 29270.18798828125 
[2025-02-18 21:15:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:1.815516471862793 norm:0.003116743639111519 max memory_allocated 29270.18798828125 
[2025-02-18 21:16:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:1.8154555559158325 norm:0.0031830205116420984 max memory_allocated 29270.18798828125 
[2025-02-18 21:16:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:1.8150224685668945 norm:0.003201597835868597 max memory_allocated 29270.18798828125 
[2025-02-18 21:17:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:1.8149285316467285 norm:0.003270841436460614 max memory_allocated 29270.18798828125 
[2025-02-18 21:18:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 21:18:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:2.1037259101867676 norm:0.020778119564056396 max memory_allocated 29270.37548828125 
[2025-02-18 21:19:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:2.0935332775115967 norm:0.01330557931214571 max memory_allocated 29270.37548828125 
[2025-02-18 21:20:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:2.091817617416382 norm:0.01018348429352045 max memory_allocated 29270.37548828125 
[2025-02-18 21:21:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:2.0918784141540527 norm:0.008072108030319214 max memory_allocated 29270.37548828125 
[2025-02-18 21:22:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:2.0913331508636475 norm:0.006664037238806486 max memory_allocated 29270.37548828125 
[2025-02-18 21:23:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:2.085874557495117 norm:0.005587718915194273 max memory_allocated 29270.37548828125 
[2025-02-18 21:23:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:2.08138370513916 norm:0.004819280002266169 max memory_allocated 29270.37548828125 
[2025-02-18 21:24:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:2.0797319412231445 norm:0.0044783297926187515 max memory_allocated 29270.37548828125 
[2025-02-18 21:25:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:2.0775339603424072 norm:0.0040823919698596 max memory_allocated 29270.37548828125 
[2025-02-18 21:26:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:2.0759925842285156 norm:0.003843100508674979 max memory_allocated 29270.37548828125 
[2025-02-18 21:27:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:2.074410915374756 norm:0.0035971642937511206 max memory_allocated 29270.37548828125 
[2025-02-18 21:27:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:2.0728228092193604 norm:0.003379414090886712 max memory_allocated 29270.37548828125 
[2025-02-18 21:28:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:2.071619987487793 norm:0.0033020891714841127 max memory_allocated 29270.37548828125 
[2025-02-18 21:29:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:2.0711872577667236 norm:0.0032503807451575994 max memory_allocated 29270.37548828125 
[2025-02-18 21:30:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:2.070364475250244 norm:0.00384760950691998 max memory_allocated 29270.37548828125 
[2025-02-18 21:31:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:2.0704174041748047 norm:0.005170420743525028 max memory_allocated 29270.37548828125 
[2025-02-18 21:32:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:2.0695855617523193 norm:0.005949465557932854 max memory_allocated 29270.37548828125 
[2025-02-18 21:32:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:2.07057523727417 norm:0.00703416857868433 max memory_allocated 29270.37548828125 
[2025-02-18 21:33:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:2.070080041885376 norm:0.007528846617788076 max memory_allocated 29270.37548828125 
[2025-02-18 21:34:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:2.069899082183838 norm:0.00761059345677495 max memory_allocated 29270.37548828125 
[2025-02-18 21:34:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 21:35:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:2.360266923904419 norm:0.01946183107793331 max memory_allocated 29270.56298828125 
[2025-02-18 21:36:30 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:2.3547685146331787 norm:0.013152016326785088 max memory_allocated 29270.56298828125 
[2025-02-18 21:37:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:2.3549704551696777 norm:0.009579483419656754 max memory_allocated 29270.56298828125 
[2025-02-18 21:38:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:2.3528127670288086 norm:0.007299815770238638 max memory_allocated 29270.56298828125 
[2025-02-18 21:38:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:2.351179599761963 norm:0.006062306929379702 max memory_allocated 29270.56298828125 
[2025-02-18 21:39:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:2.348740816116333 norm:0.005171020980924368 max memory_allocated 29270.56298828125 
[2025-02-18 21:40:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:2.346534252166748 norm:0.0044655390083789825 max memory_allocated 29270.56298828125 
[2025-02-18 21:41:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:2.343851089477539 norm:0.0039543649181723595 max memory_allocated 29270.56298828125 
[2025-02-18 21:42:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:2.3427624702453613 norm:0.00367177277803421 max memory_allocated 29270.56298828125 
[2025-02-18 21:43:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:2.3418922424316406 norm:0.0033388689626008272 max memory_allocated 29270.56298828125 
[2025-02-18 21:43:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:2.340566635131836 norm:0.003182580228894949 max memory_allocated 29270.56298828125 
[2025-02-18 21:44:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:2.339024782180786 norm:0.002994572278112173 max memory_allocated 29270.56298828125 
[2025-02-18 21:45:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:2.3378472328186035 norm:0.0028708423487842083 max memory_allocated 29270.56298828125 
[2025-02-18 21:46:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:2.336972951889038 norm:0.002770031802356243 max memory_allocated 29270.56298828125 
[2025-02-18 21:47:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:2.3363170623779297 norm:0.0026883604004979134 max memory_allocated 29270.56298828125 
[2025-02-18 21:48:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:2.3355982303619385 norm:0.0026131398044526577 max memory_allocated 29270.56298828125 
[2025-02-18 21:48:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:2.3348164558410645 norm:0.0025351454969495535 max memory_allocated 29270.56298828125 
[2025-02-18 21:49:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:2.3338894844055176 norm:0.0024817262310534716 max memory_allocated 29270.56298828125 
[2025-02-18 21:50:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:2.332719326019287 norm:0.0024146970827132463 max memory_allocated 29270.56298828125 
[2025-02-18 21:51:18 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:2.332383871078491 norm:0.0023719181772321463 max memory_allocated 29270.56298828125 
[2025-02-18 21:51:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 21:52:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:2.632844924926758 norm:0.01329658180475235 max memory_allocated 29270.75048828125 
[2025-02-18 21:53:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:2.6240508556365967 norm:0.00798757839947939 max memory_allocated 29270.75048828125 
[2025-02-18 21:54:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:2.6215243339538574 norm:0.005622264929115772 max memory_allocated 29270.75048828125 
[2025-02-18 21:54:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:2.6209545135498047 norm:0.0043697962537407875 max memory_allocated 29270.75048828125 
[2025-02-18 21:55:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:2.6190342903137207 norm:0.003532783128321171 max memory_allocated 29270.75048828125 
[2025-02-18 21:56:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:2.6159913539886475 norm:0.003057886380702257 max memory_allocated 29270.75048828125 
[2025-02-18 21:57:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:2.6134331226348877 norm:0.0027761892415583134 max memory_allocated 29270.75048828125 
[2025-02-18 21:58:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:2.6118099689483643 norm:0.0025407769717276096 max memory_allocated 29270.75048828125 
[2025-02-18 21:59:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:2.61022686958313 norm:0.0023586228489875793 max memory_allocated 29270.75048828125 
[2025-02-18 21:59:49 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:2.607436180114746 norm:0.00248152669519186 max memory_allocated 29270.75048828125 
[2025-02-18 22:00:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:2.605027198791504 norm:0.0033075010869652033 max memory_allocated 29270.75048828125 
[2025-02-18 22:01:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:2.6036746501922607 norm:0.003299047239124775 max memory_allocated 29270.75048828125 
[2025-02-18 22:02:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:2.6023924350738525 norm:0.003236548975110054 max memory_allocated 29270.75048828125 
[2025-02-18 22:03:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:2.6016831398010254 norm:0.0032269300427287817 max memory_allocated 29270.75048828125 
[2025-02-18 22:03:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:2.6011133193969727 norm:0.003218597499653697 max memory_allocated 29270.75048828125 
[2025-02-18 22:04:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:2.6003870964050293 norm:0.0032384321093559265 max memory_allocated 29270.75048828125 
[2025-02-18 22:05:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:2.5994670391082764 norm:0.0032310558017343283 max memory_allocated 29270.75048828125 
[2025-02-18 22:06:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:2.5988247394561768 norm:0.003197719110175967 max memory_allocated 29270.75048828125 
[2025-02-18 22:07:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:2.598367214202881 norm:0.0032254294492304325 max memory_allocated 29270.75048828125 
[2025-02-18 22:08:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:2.598109722137451 norm:0.003204433247447014 max memory_allocated 29270.75048828125 
[2025-02-18 22:08:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 22:09:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:2.9579858779907227 norm:0.018562298268079758 max memory_allocated 29270.93798828125 
[2025-02-18 22:10:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:2.951234817504883 norm:0.012249061837792397 max memory_allocated 29270.93798828125 
[2025-02-18 22:10:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:2.948903799057007 norm:0.00913579948246479 max memory_allocated 29270.93798828125 
[2025-02-18 22:11:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:2.9459235668182373 norm:0.007153898011893034 max memory_allocated 29270.93798828125 
[2025-02-18 22:12:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:2.9442343711853027 norm:0.005827862769365311 max memory_allocated 29270.93798828125 
[2025-02-18 22:13:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:2.944005012512207 norm:0.0049414001405239105 max memory_allocated 29270.93798828125 
[2025-02-18 22:14:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:2.9417028427124023 norm:0.00431859539821744 max memory_allocated 29270.93798828125 
[2025-02-18 22:14:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:2.940431594848633 norm:0.0038772926200181246 max memory_allocated 29270.93798828125 
[2025-02-18 22:15:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:2.9398152828216553 norm:0.003577612340450287 max memory_allocated 29270.93798828125 
[2025-02-18 22:16:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:2.939584255218506 norm:0.0033946773037314415 max memory_allocated 29270.93798828125 
[2025-02-18 22:17:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:2.9379937648773193 norm:0.003243205137550831 max memory_allocated 29270.93798828125 
[2025-02-18 22:18:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:2.9366440773010254 norm:0.0030655553564429283 max memory_allocated 29270.93798828125 
[2025-02-18 22:19:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:2.9356141090393066 norm:0.002950067864730954 max memory_allocated 29270.93798828125 
[2025-02-18 22:19:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:2.9346795082092285 norm:0.002860476030036807 max memory_allocated 29270.93798828125 
[2025-02-18 22:20:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:2.9342262744903564 norm:0.0027914184611290693 max memory_allocated 29270.93798828125 
[2025-02-18 22:21:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:2.9337494373321533 norm:0.0027550430968403816 max memory_allocated 29270.93798828125 
[2025-02-18 22:22:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:2.9332263469696045 norm:0.002708981279283762 max memory_allocated 29270.93798828125 
[2025-02-18 22:23:09 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:2.9328460693359375 norm:0.0026717069558799267 max memory_allocated 29270.93798828125 
[2025-02-18 22:23:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:2.9324769973754883 norm:0.0026327953673899174 max memory_allocated 29270.93798828125 
[2025-02-18 22:24:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:2.931959867477417 norm:0.0025995748583227396 max memory_allocated 29270.93798828125 
[2025-02-18 22:25:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 22:25:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:3.2596535682678223 norm:0.014821916818618774 max memory_allocated 29271.12548828125 
[2025-02-18 22:26:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:3.251938581466675 norm:0.009278381243348122 max memory_allocated 29271.12548828125 
[2025-02-18 22:27:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:3.2486932277679443 norm:0.006688237190246582 max memory_allocated 29271.12548828125 
[2025-02-18 22:28:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:3.2473580837249756 norm:0.0052530947141349316 max memory_allocated 29271.12548828125 
[2025-02-18 22:29:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:3.243621826171875 norm:0.004565572831779718 max memory_allocated 29271.12548828125 
[2025-02-18 22:30:01 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:3.2403011322021484 norm:0.003944299183785915 max memory_allocated 29271.12548828125 
[2025-02-18 22:30:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:3.2377450466156006 norm:0.0035891334991902113 max memory_allocated 29271.12548828125 
[2025-02-18 22:31:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:3.2357571125030518 norm:0.003433413803577423 max memory_allocated 29271.12548828125 
[2025-02-18 22:32:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:3.233839511871338 norm:0.0032560660038143396 max memory_allocated 29271.12548828125 
[2025-02-18 22:33:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:3.232034683227539 norm:0.0030943136662244797 max memory_allocated 29271.12548828125 
[2025-02-18 22:34:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:3.2308037281036377 norm:0.0031134458258748055 max memory_allocated 29271.12548828125 
[2025-02-18 22:34:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:3.2299299240112305 norm:0.003000326920300722 max memory_allocated 29271.12548828125 
[2025-02-18 22:35:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:3.228914260864258 norm:0.002938568592071533 max memory_allocated 29271.12548828125 
[2025-02-18 22:36:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:3.2280080318450928 norm:0.00295760715380311 max memory_allocated 29271.12548828125 
[2025-02-18 22:37:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:3.2272400856018066 norm:0.0029590807389467955 max memory_allocated 29271.12548828125 
[2025-02-18 22:38:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:3.226539373397827 norm:0.0029649848584085703 max memory_allocated 29271.12548828125 
[2025-02-18 22:39:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:3.2259349822998047 norm:0.0029952540062367916 max memory_allocated 29271.12548828125 
[2025-02-18 22:39:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:3.224161148071289 norm:0.004000416956841946 max memory_allocated 29271.12548828125 
[2025-02-18 22:40:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:3.223875045776367 norm:0.005997806321829557 max memory_allocated 29271.12548828125 
[2025-02-18 22:41:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:3.2235379219055176 norm:0.006796684581786394 max memory_allocated 29271.12548828125 
[2025-02-18 22:41:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 22:42:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:3.5927209854125977 norm:0.00993658322840929 max memory_allocated 29271.31298828125 
[2025-02-18 22:43:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:3.5873351097106934 norm:0.006690008100122213 max memory_allocated 29271.31298828125 
[2025-02-18 22:44:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:3.5844407081604004 norm:0.005094687920063734 max memory_allocated 29271.31298828125 
[2025-02-18 22:45:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:3.5819220542907715 norm:0.004005449824035168 max memory_allocated 29271.31298828125 
[2025-02-18 22:45:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:3.5797083377838135 norm:0.003264504252001643 max memory_allocated 29271.31298828125 
[2025-02-18 22:46:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:3.5788207054138184 norm:0.002813861472532153 max memory_allocated 29271.31298828125 
[2025-02-18 22:47:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:3.5764169692993164 norm:0.002471155021339655 max memory_allocated 29271.31298828125 
[2025-02-18 22:48:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:3.57466197013855 norm:0.002263932954519987 max memory_allocated 29271.31298828125 
[2025-02-18 22:49:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:3.5725579261779785 norm:0.002092777518555522 max memory_allocated 29271.31298828125 
[2025-02-18 22:50:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:3.570748805999756 norm:0.0019754935055971146 max memory_allocated 29271.31298828125 
[2025-02-18 22:50:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:3.569089651107788 norm:0.001879687188193202 max memory_allocated 29271.31298828125 
[2025-02-18 22:51:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:3.567553997039795 norm:0.0017902132822200656 max memory_allocated 29271.31298828125 
[2025-02-18 22:52:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:3.5664589405059814 norm:0.001738961786031723 max memory_allocated 29271.31298828125 
[2025-02-18 22:53:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:3.56520414352417 norm:0.0017008959548547864 max memory_allocated 29271.31298828125 
[2025-02-18 22:54:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:3.5643861293792725 norm:0.0016638361848890781 max memory_allocated 29271.31298828125 
[2025-02-18 22:55:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:3.563621759414673 norm:0.001641185488551855 max memory_allocated 29271.31298828125 
[2025-02-18 22:55:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:3.563103437423706 norm:0.0016297209076583385 max memory_allocated 29271.31298828125 
[2025-02-18 22:56:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:3.562579393386841 norm:0.0016154998447746038 max memory_allocated 29271.31298828125 
[2025-02-18 22:57:28 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:3.562023401260376 norm:0.0016118658240884542 max memory_allocated 29271.31298828125 
[2025-02-18 22:58:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:3.5613927841186523 norm:0.0016120601212605834 max memory_allocated 29271.31298828125 
[2025-02-18 22:58:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 22:59:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:3.976907253265381 norm:0.010866057127714157 max memory_allocated 29271.50048828125 
[2025-02-18 23:00:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:3.968862533569336 norm:0.007707300130277872 max memory_allocated 29271.50048828125 
[2025-02-18 23:01:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:3.9676926136016846 norm:0.007272098213434219 max memory_allocated 29271.50048828125 
[2025-02-18 23:01:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:3.9658172130584717 norm:0.006380744278430939 max memory_allocated 29271.50048828125 
[2025-02-18 23:02:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:3.9647116661071777 norm:0.005554452072829008 max memory_allocated 29271.50048828125 
[2025-02-18 23:03:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:3.9629716873168945 norm:0.004785989411175251 max memory_allocated 29271.50048828125 
[2025-02-18 23:04:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:3.9618639945983887 norm:0.004136896692216396 max memory_allocated 29271.50048828125 
[2025-02-18 23:05:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:3.960282802581787 norm:0.0037860472220927477 max memory_allocated 29271.50048828125 
[2025-02-18 23:05:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:3.9580419063568115 norm:0.003682010108605027 max memory_allocated 29271.50048828125 
[2025-02-18 23:06:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:3.956273317337036 norm:0.003559464355930686 max memory_allocated 29271.50048828125 
[2025-02-18 23:07:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:3.9556283950805664 norm:0.003680040594190359 max memory_allocated 29271.50048828125 
[2025-02-18 23:08:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:3.954585075378418 norm:0.0037317604292184114 max memory_allocated 29271.50048828125 
[2025-02-18 23:09:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:3.9536330699920654 norm:0.003659767098724842 max memory_allocated 29271.50048828125 
[2025-02-18 23:10:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:3.9533822536468506 norm:0.0036047971807420254 max memory_allocated 29271.50048828125 
[2025-02-18 23:10:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:3.9525420665740967 norm:0.0035347670782357454 max memory_allocated 29271.50048828125 
[2025-02-18 23:11:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:3.9515271186828613 norm:0.0035455790348351 max memory_allocated 29271.50048828125 
[2025-02-18 23:12:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:3.951105833053589 norm:0.0034482060000300407 max memory_allocated 29271.50048828125 
[2025-02-18 23:13:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:3.951075553894043 norm:0.0034557071048766375 max memory_allocated 29271.50048828125 
[2025-02-18 23:14:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:3.9497203826904297 norm:0.008527480065822601 max memory_allocated 29271.50048828125 
[2025-02-18 23:15:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:3.948680877685547 norm:0.021989768370985985 max memory_allocated 29271.50048828125 
[2025-02-18 23:15:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 23:16:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:4.305568695068359 norm:0.011401251889765263 max memory_allocated 29271.68798828125 
[2025-02-18 23:16:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:4.299149513244629 norm:0.007055160589516163 max memory_allocated 29271.68798828125 
[2025-02-18 23:17:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:4.297077178955078 norm:0.005174155812710524 max memory_allocated 29271.68798828125 
[2025-02-18 23:18:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:4.294652938842773 norm:0.00393744045868516 max memory_allocated 29271.68798828125 
[2025-02-18 23:19:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:4.29443359375 norm:0.003187791910022497 max memory_allocated 29271.68798828125 
[2025-02-18 23:20:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:4.292736053466797 norm:0.0027161366306245327 max memory_allocated 29271.68798828125 
[2025-02-18 23:21:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:4.291082382202148 norm:0.002443032106384635 max memory_allocated 29271.68798828125 
[2025-02-18 23:21:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:4.288272380828857 norm:0.002268457319587469 max memory_allocated 29271.68798828125 
[2025-02-18 23:22:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:4.285421371459961 norm:0.002149196108803153 max memory_allocated 29271.68798828125 
[2025-02-18 23:23:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:4.284026145935059 norm:0.0021035741083323956 max memory_allocated 29271.68798828125 
[2025-02-18 23:24:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:4.28287410736084 norm:0.0020442954264581203 max memory_allocated 29271.68798828125 
[2025-02-18 23:25:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:4.2816162109375 norm:0.002007101895287633 max memory_allocated 29271.68798828125 
[2025-02-18 23:26:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:4.280549049377441 norm:0.001981846522539854 max memory_allocated 29271.68798828125 
[2025-02-18 23:26:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:4.279758453369141 norm:0.001977778971195221 max memory_allocated 29271.68798828125 
[2025-02-18 23:27:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:4.27862024307251 norm:0.001934356172569096 max memory_allocated 29271.68798828125 
[2025-02-18 23:28:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:4.277553081512451 norm:0.0019308974733576179 max memory_allocated 29271.68798828125 
[2025-02-18 23:29:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:4.276920795440674 norm:0.0019391227979213 max memory_allocated 29271.68798828125 
[2025-02-18 23:30:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:4.2737860679626465 norm:0.006407313980162144 max memory_allocated 29271.68798828125 
[2025-02-18 23:30:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:4.271472454071045 norm:0.020239103585481644 max memory_allocated 29271.68798828125 
[2025-02-18 23:31:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:4.270500659942627 norm:0.021917659789323807 max memory_allocated 29271.68798828125 
[2025-02-18 23:31:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 23:32:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:4.663101673126221 norm:0.011793547309935093 max memory_allocated 29271.87548828125 
[2025-02-18 23:33:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:4.655407905578613 norm:0.008440135046839714 max memory_allocated 29271.87548828125 
[2025-02-18 23:34:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:4.652438163757324 norm:0.006383820902556181 max memory_allocated 29271.87548828125 
[2025-02-18 23:35:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:4.6489152908325195 norm:0.005010964814573526 max memory_allocated 29271.87548828125 
[2025-02-18 23:36:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:4.646130084991455 norm:0.004330333787947893 max memory_allocated 29271.87548828125 
[2025-02-18 23:36:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:4.643641471862793 norm:0.003721741260960698 max memory_allocated 29271.87548828125 
[2025-02-18 23:37:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:4.641596794128418 norm:0.0032118232920765877 max memory_allocated 29271.87548828125 
[2025-02-18 23:38:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:4.639799118041992 norm:0.002798778237774968 max memory_allocated 29271.87548828125 
[2025-02-18 23:39:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:4.638009548187256 norm:0.002476269146427512 max memory_allocated 29271.87548828125 
[2025-02-18 23:40:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:4.636208534240723 norm:0.002201856579631567 max memory_allocated 29271.87548828125 
[2025-02-18 23:41:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:4.6345953941345215 norm:0.002052788622677326 max memory_allocated 29271.87548828125 
[2025-02-18 23:41:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:4.633841514587402 norm:0.002003701403737068 max memory_allocated 29271.87548828125 
[2025-02-18 23:42:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:4.633183479309082 norm:0.001890608691610396 max memory_allocated 29271.87548828125 
[2025-02-18 23:43:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:4.63256311416626 norm:0.001830603345297277 max memory_allocated 29271.87548828125 
[2025-02-18 23:44:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:4.631707668304443 norm:0.001781863160431385 max memory_allocated 29271.87548828125 
[2025-02-18 23:45:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:4.631107807159424 norm:0.001741891959682107 max memory_allocated 29271.87548828125 
[2025-02-18 23:46:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:4.630463600158691 norm:0.0016947442200034857 max memory_allocated 29271.87548828125 
[2025-02-18 23:46:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:4.6299848556518555 norm:0.001672017970122397 max memory_allocated 29271.87548828125 
[2025-02-18 23:47:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:4.62970495223999 norm:0.0016519245691597462 max memory_allocated 29271.87548828125 
[2025-02-18 23:48:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:4.629335403442383 norm:0.00164083915296942 max memory_allocated 29271.87548828125 
[2025-02-18 23:48:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 23:49:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:5.0338311195373535 norm:0.007235981523990631 max memory_allocated 29272.06298828125 
[2025-02-18 23:50:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:5.026008129119873 norm:0.0045866575092077255 max memory_allocated 29272.06298828125 
[2025-02-18 23:51:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:5.023227214813232 norm:0.003367953235283494 max memory_allocated 29272.06298828125 
[2025-02-18 23:52:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:5.020894527435303 norm:0.002541670575737953 max memory_allocated 29272.06298828125 
[2025-02-18 23:52:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:5.01861572265625 norm:0.0022141695953905582 max memory_allocated 29272.06298828125 
[2025-02-18 23:53:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:5.016690254211426 norm:0.002065416658297181 max memory_allocated 29272.06298828125 
[2025-02-18 23:54:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:5.01505184173584 norm:0.0020084395073354244 max memory_allocated 29272.06298828125 
[2025-02-18 23:55:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:5.01357364654541 norm:0.001956904772669077 max memory_allocated 29272.06298828125 
[2025-02-18 23:56:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:5.01276159286499 norm:0.0019450224936008453 max memory_allocated 29272.06298828125 
[2025-02-18 23:57:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:5.011825084686279 norm:0.00191831449046731 max memory_allocated 29272.06298828125 
[2025-02-18 23:57:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:5.010793209075928 norm:0.0018963784677907825 max memory_allocated 29272.06298828125 
[2025-02-18 23:58:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:5.009707927703857 norm:0.0018678302876651287 max memory_allocated 29272.06298828125 
[2025-02-18 23:59:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:5.008821487426758 norm:0.0018396859522908926 max memory_allocated 29272.06298828125 
[2025-02-19 00:00:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:5.008119106292725 norm:0.0018183673964813352 max memory_allocated 29272.06298828125 
[2025-02-19 00:01:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:5.0075273513793945 norm:0.0018167693633586168 max memory_allocated 29272.06298828125 
[2025-02-19 00:01:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:5.00729513168335 norm:0.0017990260384976864 max memory_allocated 29272.06298828125 
[2025-02-19 00:02:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:5.00694465637207 norm:0.001805043313652277 max memory_allocated 29272.06298828125 
[2025-02-19 00:03:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:5.006304740905762 norm:0.0017821412766352296 max memory_allocated 29272.06298828125 
[2025-02-19 00:04:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:5.0062255859375 norm:0.0017867663409560919 max memory_allocated 29272.06298828125 
[2025-02-19 00:05:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:5.006065368652344 norm:0.001782990526407957 max memory_allocated 29272.06298828125 
[2025-02-19 00:05:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-19 00:06:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:5.398564338684082 norm:0.01112874411046505 max memory_allocated 29272.25048828125 
[2025-02-19 00:07:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:5.391131401062012 norm:0.007468551862984896 max memory_allocated 29272.25048828125 
[2025-02-19 00:07:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:5.386765480041504 norm:0.005736036691814661 max memory_allocated 29272.25048828125 
[2025-02-19 00:08:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:5.382993221282959 norm:0.004473439417779446 max memory_allocated 29272.25048828125 
[2025-02-19 00:09:38 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:5.379398345947266 norm:0.003480480285361409 max memory_allocated 29272.25048828125 
[2025-02-19 00:10:27 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:5.376500129699707 norm:0.0030027003958821297 max memory_allocated 29272.25048828125 
[2025-02-19 00:11:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:5.37384033203125 norm:0.0025676246732473373 max memory_allocated 29272.25048828125 
[2025-02-19 00:12:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:5.371792793273926 norm:0.002335028722882271 max memory_allocated 29272.25048828125 
[2025-02-19 00:12:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:5.369337558746338 norm:0.002193430671468377 max memory_allocated 29272.25048828125 
[2025-02-19 00:13:44 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:5.367692947387695 norm:0.002072336617857218 max memory_allocated 29272.25048828125 
[2025-02-19 00:14:34 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:5.3667802810668945 norm:0.0020253078546375036 max memory_allocated 29272.25048828125 
[2025-02-19 00:15:23 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:5.365342617034912 norm:0.001963202841579914 max memory_allocated 29272.25048828125 
[2025-02-19 00:16:12 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:5.364265441894531 norm:0.0019101232755929232 max memory_allocated 29272.25048828125 
[2025-02-19 00:17:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:5.36346435546875 norm:0.0019020100589841604 max memory_allocated 29272.25048828125 
[2025-02-19 00:17:51 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:5.3627519607543945 norm:0.0018726828275248408 max memory_allocated 29272.25048828125 
[2025-02-19 00:18:40 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:5.362336158752441 norm:0.0018545500934123993 max memory_allocated 29272.25048828125 
[2025-02-19 00:19:29 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:5.361814975738525 norm:0.0018394081853330135 max memory_allocated 29272.25048828125 
[2025-02-19 00:20:19 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:5.361209869384766 norm:0.0018237000331282616 max memory_allocated 29272.25048828125 
[2025-02-19 00:21:08 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:5.360738754272461 norm:0.0018072619568556547 max memory_allocated 29272.25048828125 
[2025-02-19 00:21:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:5.36020040512085 norm:0.0017907475121319294 max memory_allocated 29272.25048828125 
[2025-02-19 00:22:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-19 00:23:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:5.831836223602295 norm:0.009185206145048141 max memory_allocated 29272.43798828125 
[2025-02-19 00:23:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:5.8226470947265625 norm:0.0057748956605792046 max memory_allocated 29272.43798828125 
[2025-02-19 00:24:43 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:5.817837715148926 norm:0.004384501837193966 max memory_allocated 29272.43798828125 
[2025-02-19 00:25:33 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:5.813508033752441 norm:0.003396232845261693 max memory_allocated 29272.43798828125 
[2025-02-19 00:26:22 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:5.810542583465576 norm:0.002811322221532464 max memory_allocated 29272.43798828125 
[2025-02-19 00:27:11 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:5.8083624839782715 norm:0.0024256380274891853 max memory_allocated 29272.43798828125 
[2025-02-19 00:28:00 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:5.807251453399658 norm:0.002159884199500084 max memory_allocated 29272.43798828125 
[2025-02-19 00:28:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:5.805355548858643 norm:0.001981158275157213 max memory_allocated 29272.43798828125 
[2025-02-19 00:29:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:5.80371618270874 norm:0.0018388947937637568 max memory_allocated 29272.43798828125 
[2025-02-19 00:30:28 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:5.802044868469238 norm:0.001750475144945085 max memory_allocated 29272.43798828125 
[2025-02-19 00:31:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:5.80079460144043 norm:0.0016908242832869291 max memory_allocated 29272.43798828125 
[2025-02-19 00:32:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:5.799633026123047 norm:0.001630654325708747 max memory_allocated 29272.43798828125 
[2025-02-19 00:32:56 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:5.798646926879883 norm:0.0015914454124867916 max memory_allocated 29272.43798828125 
[2025-02-19 00:33:46 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:5.7976460456848145 norm:0.001555727911181748 max memory_allocated 29272.43798828125 
[2025-02-19 00:34:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:5.7971320152282715 norm:0.0015486918855458498 max memory_allocated 29272.43798828125 
[2025-02-19 00:35:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:5.796750545501709 norm:0.0015288644935935736 max memory_allocated 29272.43798828125 
[2025-02-19 00:36:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:5.796136856079102 norm:0.0015176841989159584 max memory_allocated 29272.43798828125 
[2025-02-19 00:37:03 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:5.795732498168945 norm:0.0014991112984716892 max memory_allocated 29272.43798828125 
[2025-02-19 00:37:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:5.795270919799805 norm:0.001487236237153411 max memory_allocated 29272.43798828125 
[2025-02-19 00:38:41 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:5.794812202453613 norm:0.0014814590103924274 max memory_allocated 29272.43798828125 
[2025-02-19 00:38:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-19 00:39:49 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:6.393675804138184 norm:0.031482845544815063 max memory_allocated 29272.62548828125 
[2025-02-19 00:40:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:6.38295316696167 norm:0.022635115310549736 max memory_allocated 29272.62548828125 
[2025-02-19 00:41:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:6.377289295196533 norm:0.01773538812994957 max memory_allocated 29272.62548828125 
[2025-02-19 00:42:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:6.373334884643555 norm:0.013963006436824799 max memory_allocated 29272.62548828125 
[2025-02-19 00:43:06 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:6.36888313293457 norm:0.011542703956365585 max memory_allocated 29272.62548828125 
[2025-02-19 00:43:55 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:6.36527156829834 norm:0.009795982390642166 max memory_allocated 29272.62548828125 
[2025-02-19 00:44:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:6.361248016357422 norm:0.008529500104486942 max memory_allocated 29272.62548828125 
[2025-02-19 00:45:34 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:6.357728481292725 norm:0.007550294045358896 max memory_allocated 29272.62548828125 
[2025-02-19 00:46:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:6.353670120239258 norm:0.0067272912710905075 max memory_allocated 29272.62548828125 
[2025-02-19 00:47:12 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:6.3505635261535645 norm:0.006087778601795435 max memory_allocated 29272.62548828125 
[2025-02-19 00:48:02 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:6.348505020141602 norm:0.005510978400707245 max memory_allocated 29272.62548828125 
[2025-02-19 00:48:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:6.34619665145874 norm:0.004996336065232754 max memory_allocated 29272.62548828125 
[2025-02-19 00:49:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:6.343867301940918 norm:0.004565115086734295 max memory_allocated 29272.62548828125 
[2025-02-19 00:50:29 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:6.341557025909424 norm:0.0042334673926234245 max memory_allocated 29272.62548828125 
[2025-02-19 00:51:18 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:6.340005874633789 norm:0.0039442721754312515 max memory_allocated 29272.62548828125 
[2025-02-19 00:52:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:6.337948322296143 norm:0.0036948835477232933 max memory_allocated 29272.62548828125 
[2025-02-19 00:52:57 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:6.335883140563965 norm:0.0035004247911274433 max memory_allocated 29272.62548828125 
[2025-02-19 00:53:46 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:6.334465026855469 norm:0.003332966472953558 max memory_allocated 29272.62548828125 
[2025-02-19 00:54:36 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:6.332780838012695 norm:0.0031811860390007496 max memory_allocated 29272.62548828125 
[2025-02-19 00:55:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:6.3316874504089355 norm:0.003052152693271637 max memory_allocated 29272.62548828125 
[2025-02-19 00:55:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-19 00:56:32 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:6.956866264343262 norm:0.020740212872624397 max memory_allocated 29272.81298828125 
[2025-02-19 00:57:22 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:6.940405368804932 norm:0.015704644843935966 max memory_allocated 29272.81298828125 
[2025-02-19 00:58:11 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:6.931807041168213 norm:0.012403015978634357 max memory_allocated 29272.81298828125 
[2025-02-19 00:59:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:6.924751281738281 norm:0.01005240436643362 max memory_allocated 29272.81298828125 
[2025-02-19 00:59:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:6.919429779052734 norm:0.008450631983578205 max memory_allocated 29272.81298828125 
[2025-02-19 01:00:39 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:6.913212299346924 norm:0.007166220340877771 max memory_allocated 29272.81298828125 
[2025-02-19 01:01:28 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:6.909176826477051 norm:0.006037158891558647 max memory_allocated 29272.81298828125 
[2025-02-19 01:02:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:6.904447555541992 norm:0.00519643584266305 max memory_allocated 29272.81298828125 
[2025-02-19 01:03:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:6.900938510894775 norm:0.004612095654010773 max memory_allocated 29272.81298828125 
[2025-02-19 01:03:56 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:6.898178577423096 norm:0.004154018126428127 max memory_allocated 29272.81298828125 
[2025-02-19 01:04:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:6.895409107208252 norm:0.0038079479709267616 max memory_allocated 29272.81298828125 
[2025-02-19 01:05:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:6.893447399139404 norm:0.0036393229383975267 max memory_allocated 29272.81298828125 
[2025-02-19 01:06:24 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:6.89251184463501 norm:0.0036429285537451506 max memory_allocated 29272.81298828125 
[2025-02-19 01:07:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:6.890488624572754 norm:0.0035973163321614265 max memory_allocated 29272.81298828125 
[2025-02-19 01:08:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:6.889946460723877 norm:0.003610954387113452 max memory_allocated 29272.81298828125 
[2025-02-19 01:08:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:6.889226913452148 norm:0.0036218995228409767 max memory_allocated 29272.81298828125 
[2025-02-19 01:09:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:6.8878655433654785 norm:0.0036009589675813913 max memory_allocated 29272.81298828125 
[2025-02-19 01:10:30 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:6.887415409088135 norm:0.0036148158833384514 max memory_allocated 29272.81298828125 
[2025-02-19 01:11:20 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:6.8878173828125 norm:0.0035921738017350435 max memory_allocated 29272.81298828125 
[2025-02-19 01:12:09 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:6.885126113891602 norm:0.0035238482523709536 max memory_allocated 29272.81298828125 
[2025-02-19 01:12:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-19 01:12:28 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:13:17 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:7.487776756286621 norm:0.1088905781507492 max memory_allocated 29273.14501953125 
[2025-02-19 01:14:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:7.446709632873535 norm:0.11522703617811203 max memory_allocated 29273.14501953125 
[2025-02-19 01:14:56 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:7.429269790649414 norm:0.1026855930685997 max memory_allocated 29273.14501953125 
[2025-02-19 01:15:45 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:7.414848327636719 norm:0.0894002616405487 max memory_allocated 29273.14501953125 
[2025-02-19 01:16:35 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:7.4025726318359375 norm:0.07701732963323593 max memory_allocated 29273.14501953125 
[2025-02-19 01:17:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:7.393276214599609 norm:0.06657390296459198 max memory_allocated 29273.14501953125 
[2025-02-19 01:18:14 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:7.385849475860596 norm:0.05827586352825165 max memory_allocated 29273.14501953125 
[2025-02-19 01:19:03 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:7.3798675537109375 norm:0.05237652361392975 max memory_allocated 29273.14501953125 
[2025-02-19 01:19:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:7.374974250793457 norm:0.048558250069618225 max memory_allocated 29273.14501953125 
[2025-02-19 01:20:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:7.37087869644165 norm:0.04540127515792847 max memory_allocated 29273.14501953125 
[2025-02-19 01:21:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:7.367224216461182 norm:0.0434957779943943 max memory_allocated 29273.14501953125 
[2025-02-19 01:22:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:7.363866806030273 norm:0.04234835505485535 max memory_allocated 29273.14501953125 
[2025-02-19 01:23:10 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:7.3608198165893555 norm:0.0411166176199913 max memory_allocated 29273.14501953125 
[2025-02-19 01:24:00 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:7.358041763305664 norm:0.0401102714240551 max memory_allocated 29273.14501953125 
[2025-02-19 01:24:49 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:7.355776309967041 norm:0.03935170918703079 max memory_allocated 29273.14501953125 
[2025-02-19 01:25:39 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:7.353572845458984 norm:0.03865132853388786 max memory_allocated 29273.14501953125 
[2025-02-19 01:26:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:7.351988315582275 norm:0.03797108307480812 max memory_allocated 29273.14501953125 
[2025-02-19 01:27:18 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:7.35064697265625 norm:0.03729992359876633 max memory_allocated 29273.14501953125 
[2025-02-19 01:28:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:7.349774360656738 norm:0.03698313608765602 max memory_allocated 29273.14501953125 
[2025-02-19 01:28:56 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:7.348494529724121 norm:0.036470022052526474 max memory_allocated 29273.14501953125 
[2025-02-19 01:29:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-19 01:29:15 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:30:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:8.178566932678223 norm:0.10575126111507416 max memory_allocated 29273.33251953125 
[2025-02-19 01:30:54 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:8.14169692993164 norm:0.09917917102575302 max memory_allocated 29273.33251953125 
[2025-02-19 01:31:43 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:8.121758460998535 norm:0.08403593301773071 max memory_allocated 29273.33251953125 
[2025-02-19 01:32:33 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:8.107650756835938 norm:0.06993850320577621 max memory_allocated 29273.33251953125 
[2025-02-19 01:33:22 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:8.096419334411621 norm:0.05817367509007454 max memory_allocated 29273.33251953125 
[2025-02-19 01:34:12 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:8.085561752319336 norm:0.049399375915527344 max memory_allocated 29273.33251953125 
[2025-02-19 01:35:01 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:8.076371192932129 norm:0.043989092111587524 max memory_allocated 29273.33251953125 
[2025-02-19 01:35:50 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:8.069355964660645 norm:0.04044024273753166 max memory_allocated 29273.33251953125 
[2025-02-19 01:36:40 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:8.06515884399414 norm:0.03831099718809128 max memory_allocated 29273.33251953125 
[2025-02-19 01:37:29 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:8.059677124023438 norm:0.03622287139296532 max memory_allocated 29273.33251953125 
[2025-02-19 01:38:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:8.055502891540527 norm:0.0349547453224659 max memory_allocated 29273.33251953125 
[2025-02-19 01:39:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:8.049934387207031 norm:0.033691614866256714 max memory_allocated 29273.33251953125 
[2025-02-19 01:39:58 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:8.046628952026367 norm:0.032810233533382416 max memory_allocated 29273.33251953125 
[2025-02-19 01:40:47 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:8.043495178222656 norm:0.03217751905322075 max memory_allocated 29273.33251953125 
[2025-02-19 01:41:37 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:8.040970802307129 norm:0.03157567232847214 max memory_allocated 29273.33251953125 
[2025-02-19 01:42:26 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:8.03808879852295 norm:0.030554598197340965 max memory_allocated 29273.33251953125 
[2025-02-19 01:43:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:8.035998344421387 norm:0.03119414672255516 max memory_allocated 29273.33251953125 
[2025-02-19 01:44:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:8.033540725708008 norm:0.030784662812948227 max memory_allocated 29273.33251953125 
[2025-02-19 01:44:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:8.031946182250977 norm:0.030064839869737625 max memory_allocated 29273.33251953125 
[2025-02-19 01:45:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:8.030248641967773 norm:0.029804781079292297 max memory_allocated 29273.33251953125 
[2025-02-19 01:45:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-19 01:46:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:46:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:10.602952003479004 norm:0.524015486240387 max memory_allocated 29273.52001953125 
[2025-02-19 01:47:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:10.183460235595703 norm:0.4938008785247803 max memory_allocated 29273.52001953125 
[2025-02-19 01:48:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:10.072101593017578 norm:0.41318607330322266 max memory_allocated 29273.52001953125 
[2025-02-19 01:49:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:10.009012222290039 norm:0.3577703535556793 max memory_allocated 29273.52001953125 
[2025-02-19 01:50:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:9.979324340820312 norm:0.3284057378768921 max memory_allocated 29273.52001953125 
[2025-02-19 01:50:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:9.952199935913086 norm:0.3002097010612488 max memory_allocated 29273.52001953125 
[2025-02-19 01:51:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:9.926032066345215 norm:0.27103644609451294 max memory_allocated 29273.52001953125 
[2025-02-19 01:52:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:9.905780792236328 norm:0.2550097107887268 max memory_allocated 29273.52001953125 
[2025-02-19 01:53:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:9.890003204345703 norm:0.24027079343795776 max memory_allocated 29273.52001953125 
[2025-02-19 01:54:16 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:9.865516662597656 norm:0.2202340066432953 max memory_allocated 29273.52001953125 
[2025-02-19 01:55:06 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:9.850855827331543 norm:0.20479825139045715 max memory_allocated 29273.52001953125 
[2025-02-19 01:55:55 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:9.832468032836914 norm:0.1857054978609085 max memory_allocated 29273.52001953125 
[2025-02-19 01:56:44 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:9.813636779785156 norm:0.1751130223274231 max memory_allocated 29273.52001953125 
[2025-02-19 01:57:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:9.807815551757812 norm:0.16449099779129028 max memory_allocated 29273.52001953125 
[2025-02-19 01:58:23 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:9.792377471923828 norm:0.1527082473039627 max memory_allocated 29273.52001953125 
[2025-02-19 01:59:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:9.788924217224121 norm:0.14952915906906128 max memory_allocated 29273.52001953125 
[2025-02-19 02:00:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:9.774591445922852 norm:0.14004075527191162 max memory_allocated 29273.52001953125 
[2025-02-19 02:00:51 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:9.766528129577637 norm:0.13658712804317474 max memory_allocated 29273.52001953125 
[2025-02-19 02:01:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:9.761137008666992 norm:0.13705268502235413 max memory_allocated 29273.52001953125 
[2025-02-19 02:02:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:9.744046211242676 norm:0.13094504177570343 max memory_allocated 29273.52001953125 
[2025-02-19 02:02:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-19 02:02:49 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 02:03:38 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:17.245441436767578 norm:0.4766002893447876 max memory_allocated 29273.70751953125 
[2025-02-19 02:04:27 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:16.88703155517578 norm:0.40284937620162964 max memory_allocated 29273.70751953125 
[2025-02-19 02:05:17 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:16.70468521118164 norm:0.37426671385765076 max memory_allocated 29273.70751953125 
[2025-02-19 02:06:06 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:16.451200485229492 norm:0.375510573387146 max memory_allocated 29273.70751953125 
[2025-02-19 02:06:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:16.2633113861084 norm:0.3650605380535126 max memory_allocated 29273.70751953125 
[2025-02-19 02:07:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:16.200170516967773 norm:0.34086787700653076 max memory_allocated 29273.70751953125 
[2025-02-19 02:08:34 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:16.154096603393555 norm:0.3260941505432129 max memory_allocated 29273.70751953125 
[2025-02-19 02:09:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:16.118078231811523 norm:0.3139805793762207 max memory_allocated 29273.70751953125 
[2025-02-19 02:10:13 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:16.081050872802734 norm:0.30110234022140503 max memory_allocated 29273.70751953125 
[2025-02-19 02:11:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:16.047176361083984 norm:0.28809279203414917 max memory_allocated 29273.70751953125 
[2025-02-19 02:11:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:16.04595375061035 norm:0.27231165766716003 max memory_allocated 29273.70751953125 
[2025-02-19 02:12:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:16.012386322021484 norm:0.26567551493644714 max memory_allocated 29273.70751953125 
[2025-02-19 02:13:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:15.96465015411377 norm:0.25211837887763977 max memory_allocated 29273.70751953125 
[2025-02-19 02:14:20 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:15.933679580688477 norm:0.24852126836776733 max memory_allocated 29273.70751953125 
[2025-02-19 02:15:09 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:15.907110214233398 norm:0.24207745492458344 max memory_allocated 29273.70751953125 
[2025-02-19 02:15:59 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:15.87264633178711 norm:0.23679447174072266 max memory_allocated 29273.70751953125 
[2025-02-19 02:16:48 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:15.847482681274414 norm:0.23520764708518982 max memory_allocated 29273.70751953125 
[2025-02-19 02:17:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:15.816015243530273 norm:0.2306329309940338 max memory_allocated 29273.70751953125 
[2025-02-19 02:18:27 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:15.790064811706543 norm:0.22563603520393372 max memory_allocated 29273.70751953125 
[2025-02-19 02:19:16 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:15.769316673278809 norm:0.22502127289772034 max memory_allocated 29273.70751953125 
[2025-02-19 02:19:31 root] (main_calibration.py 365): INFO 40178.068843364716
[2025-02-19 02:20:34 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-19 02:22:28 root] (main_calibration.py 158): INFO wikitext2 : 10.76968765258789
[2025-02-19 02:22:28 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-19 02:25:26 root] (main_calibration.py 158): INFO c4 : 16.115005493164062
