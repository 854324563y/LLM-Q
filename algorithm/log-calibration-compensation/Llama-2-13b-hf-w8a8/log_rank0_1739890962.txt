[2025-02-18 15:02:42 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation/Llama-2-13b-hf-w8a8', save_dir='./log-calibration-compensation/quant/Llama-2-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-18 15:02:44 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 15:02:45 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-18 15:02:45 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 15:02:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 15:02:52 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:03:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0018458918202668428 norm:0.0042881169356405735 max memory_allocated 29269.39501953125 
[2025-02-18 15:04:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0008972297655418515 norm:0.0024310925509780645 max memory_allocated 29269.39501953125 
[2025-02-18 15:05:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0007646938320249319 norm:0.0023994657676666975 max memory_allocated 29269.39501953125 
[2025-02-18 15:06:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0006694126059301198 norm:0.0021055981051176786 max memory_allocated 29269.39501953125 
[2025-02-18 15:07:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0006061255116946995 norm:0.0019159902585670352 max memory_allocated 29269.39501953125 
[2025-02-18 15:07:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0005608522333204746 norm:0.0017814608290791512 max memory_allocated 29269.39501953125 
[2025-02-18 15:08:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0005260963225737214 norm:0.0016573516186326742 max memory_allocated 29269.39501953125 
[2025-02-18 15:09:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.000496138702146709 norm:0.0015171101549640298 max memory_allocated 29269.39501953125 
[2025-02-18 15:10:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.00047585763968527317 norm:0.0014423583634197712 max memory_allocated 29269.39501953125 
[2025-02-18 15:11:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0004592238983605057 norm:0.001359511399641633 max memory_allocated 29269.39501953125 
[2025-02-18 15:12:00 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.000445093319285661 norm:0.0012763686245307326 max memory_allocated 29269.39501953125 
[2025-02-18 15:12:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0004341445164754987 norm:0.0012191307032480836 max memory_allocated 29269.39501953125 
[2025-02-18 15:13:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0004237877728883177 norm:0.00112716865260154 max memory_allocated 29269.39501953125 
[2025-02-18 15:14:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0004155825008638203 norm:0.0010721172438934445 max memory_allocated 29269.39501953125 
[2025-02-18 15:15:19 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0004106272244825959 norm:0.0010182386031374335 max memory_allocated 29269.39501953125 
[2025-02-18 15:16:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.00040377239929512143 norm:0.0009576587472110987 max memory_allocated 29269.39501953125 
[2025-02-18 15:16:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00039891802589409053 norm:0.0009054038091562688 max memory_allocated 29269.39501953125 
[2025-02-18 15:17:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00039339455543085933 norm:0.0008555554086342454 max memory_allocated 29269.39501953125 
[2025-02-18 15:18:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00038909082650206983 norm:0.0008146423497237265 max memory_allocated 29269.39501953125 
[2025-02-18 15:19:27 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0003880697477143258 norm:0.0007758697029203176 max memory_allocated 29269.39501953125 
[2025-02-18 15:19:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 15:19:55 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:20:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0024761890526860952 norm:0.002763900440186262 max memory_allocated 29269.39501953125 
[2025-02-18 15:21:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0017645377665758133 norm:0.0016278295079246163 max memory_allocated 29269.39501953125 
[2025-02-18 15:22:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0015998361632227898 norm:0.0015687502454966307 max memory_allocated 29269.39501953125 
[2025-02-18 15:23:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.00149641209281981 norm:0.0014030688907951117 max memory_allocated 29269.39501953125 
[2025-02-18 15:24:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0014417747734114528 norm:0.0013437605230137706 max memory_allocated 29269.39501953125 
[2025-02-18 15:24:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.001401754911057651 norm:0.001280263182707131 max memory_allocated 29269.39501953125 
[2025-02-18 15:25:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0013757789274677634 norm:0.001229289686307311 max memory_allocated 29269.39501953125 
[2025-02-18 15:26:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0013573302421718836 norm:0.0011789352865889668 max memory_allocated 29269.39501953125 
[2025-02-18 15:27:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0013348651118576527 norm:0.0010856129229068756 max memory_allocated 29269.39501953125 
[2025-02-18 15:28:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0013242467539384961 norm:0.0010228907922282815 max memory_allocated 29269.39501953125 
[2025-02-18 15:29:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.00131230428814888 norm:0.000929660745896399 max memory_allocated 29269.39501953125 
[2025-02-18 15:29:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0013076573377475142 norm:0.0008920124382711947 max memory_allocated 29269.39501953125 
[2025-02-18 15:30:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.001301009557209909 norm:0.0008433209150098264 max memory_allocated 29269.39501953125 
[2025-02-18 15:31:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.001298587885685265 norm:0.0008110618800856173 max memory_allocated 29269.39501953125 
[2025-02-18 15:32:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.001293211942538619 norm:0.0007257519755512476 max memory_allocated 29269.39501953125 
[2025-02-18 15:33:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0012857301626354456 norm:0.0006446807528845966 max memory_allocated 29269.39501953125 
[2025-02-18 15:34:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0012833288637921214 norm:0.0005814091418869793 max memory_allocated 29269.39501953125 
[2025-02-18 15:34:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0012733852490782738 norm:0.0005154742393642664 max memory_allocated 29269.39501953125 
[2025-02-18 15:35:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0012668808922171593 norm:0.0004812315746676177 max memory_allocated 29269.39501953125 
[2025-02-18 15:36:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0012610340490937233 norm:0.0004448917170520872 max memory_allocated 29269.39501953125 
[2025-02-18 15:36:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 15:37:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:37:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.002641659928485751 norm:0.002369016408920288 max memory_allocated 29269.77001953125 
[2025-02-18 15:38:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.001979607157409191 norm:0.0015933703398332 max memory_allocated 29269.77001953125 
[2025-02-18 15:39:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.001790322596207261 norm:0.0016008673701435328 max memory_allocated 29269.77001953125 
[2025-02-18 15:40:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0016745955217629671 norm:0.0014379008207470179 max memory_allocated 29269.77001953125 
[2025-02-18 15:41:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0016091472934931517 norm:0.0013538694474846125 max memory_allocated 29269.77001953125 
[2025-02-18 15:41:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.001578918076120317 norm:0.0013157844077795744 max memory_allocated 29269.77001953125 
[2025-02-18 15:42:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0015544783091172576 norm:0.0012648396659642458 max memory_allocated 29269.77001953125 
[2025-02-18 15:43:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0015420503914356232 norm:0.0012099635787308216 max memory_allocated 29269.77001953125 
[2025-02-18 15:44:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0015274881152436137 norm:0.0011190332006663084 max memory_allocated 29269.77001953125 
[2025-02-18 15:45:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0015144338831305504 norm:0.0010260266717523336 max memory_allocated 29269.77001953125 
[2025-02-18 15:46:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0015019082929939032 norm:0.0009359382092952728 max memory_allocated 29269.77001953125 
[2025-02-18 15:46:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.001501360209658742 norm:0.0008598115528002381 max memory_allocated 29269.77001953125 
[2025-02-18 15:47:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0014874341432005167 norm:0.0007586352294310927 max memory_allocated 29269.77001953125 
[2025-02-18 15:48:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0014775512972846627 norm:0.0006629132549278438 max memory_allocated 29269.77001953125 
[2025-02-18 15:49:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0014708770904690027 norm:0.0006060022278688848 max memory_allocated 29269.77001953125 
[2025-02-18 15:50:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.001497855642810464 norm:0.0005904504214413464 max memory_allocated 29269.77001953125 
[2025-02-18 15:51:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0015786542790010571 norm:0.0007639718241989613 max memory_allocated 29269.77001953125 
[2025-02-18 15:51:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0015484674368053675 norm:0.0007117917994037271 max memory_allocated 29269.77001953125 
[2025-02-18 15:52:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0016217685770243406 norm:0.0007067238911986351 max memory_allocated 29269.77001953125 
[2025-02-18 15:53:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0018716902704909444 norm:0.0013234944781288505 max memory_allocated 29269.77001953125 
[2025-02-18 15:53:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 15:54:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.002446436556056142 norm:0.0005631115636788309 max memory_allocated 29269.77001953125 
[2025-02-18 15:55:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.002014035824686289 norm:0.0002721231139730662 max memory_allocated 29269.77001953125 
[2025-02-18 15:56:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0018297623610123992 norm:0.00016853057604748756 max memory_allocated 29269.77001953125 
[2025-02-18 15:57:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0017277156002819538 norm:0.00011535771773196757 max memory_allocated 29269.77001953125 
[2025-02-18 15:58:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.00166320672724396 norm:8.305180381285027e-05 max memory_allocated 29269.77001953125 
[2025-02-18 15:59:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0016179977683350444 norm:6.143864447949454e-05 max memory_allocated 29269.77001953125 
[2025-02-18 15:59:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0015947462525218725 norm:4.711331348516978e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:00:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.001586815109476447 norm:3.585560989449732e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:01:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0015788652235642076 norm:2.8416885470505804e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:02:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0015732968458905816 norm:2.3902486645965837e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:03:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0015653659356757998 norm:1.9432458429946564e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:03:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0015599433099851012 norm:1.742071981425397e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:04:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.001555000664666295 norm:1.5833231373107992e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:05:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0015578074380755424 norm:1.5066823834786192e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:06:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0015532454708591104 norm:1.3998334907228127e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:07:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0015532438410446048 norm:1.3625296560348943e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:08:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0015538705047219992 norm:1.3296724318934139e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:08:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0015495942207053304 norm:1.3194243365433067e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:09:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0015475146938115358 norm:1.2915684237668756e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:10:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0015491640660911798 norm:1.3140368537278846e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:10:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 16:11:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0019365347689017653 norm:9.757151565281674e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:12:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0017884706612676382 norm:4.734572576126084e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:13:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0017358737532049417 norm:3.0247851100284606e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:14:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0017111755441874266 norm:2.1854679289390333e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:15:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.001699338317848742 norm:1.734519719320815e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:15:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0016908015822991729 norm:1.4516577721224166e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:16:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0016886679222807288 norm:1.2443636478565168e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:17:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0016840649768710136 norm:1.177957710751798e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:18:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0016846794169396162 norm:1.0998310244758613e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:19:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0016813455149531364 norm:1.0036224921350367e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:20:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0016797158168628812 norm:1.0080078936880454e-05 max memory_allocated 29269.77001953125 
[2025-02-18 16:20:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.001680005225352943 norm:9.693842912383843e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:21:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0016781388549134135 norm:9.798462997423485e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:22:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0016768964705988765 norm:9.73160786088556e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:23:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0016756540862843394 norm:9.409664016857278e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:24:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.001675193547271192 norm:9.538397534925025e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:25:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0016742947045713663 norm:9.588633474777453e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:25:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0016736420802772045 norm:9.4969354904606e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:26:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0016733729280531406 norm:9.552272786095273e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:27:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0016745402244850993 norm:9.45219198911218e-06 max memory_allocated 29269.77001953125 
[2025-02-18 16:27:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 16:28:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.002047722926363349 norm:9.724519622977823e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:29:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.001913933316245675 norm:4.5132459490559995e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:30:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0018599617760628462 norm:2.9128143069101498e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:31:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0018331990577280521 norm:2.1717347408412024e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:32:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.001817596610635519 norm:1.725789115880616e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:32:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0018065584590658545 norm:1.4636980267823674e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:33:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0017969552427530289 norm:1.2909477845823858e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:34:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.001789479749277234 norm:1.1739532055798918e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:35:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.001784368883818388 norm:1.084074392565526e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:36:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0017825287068262696 norm:1.0199787539022509e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:37:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0017779985209926963 norm:9.802142812986858e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:37:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0017746014054864645 norm:9.394956578034908e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:38:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0017759830225259066 norm:9.08655329112662e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:39:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.001774841919541359 norm:8.893103768059518e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:40:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.001775324926711619 norm:8.744248589209747e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:41:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0017749977996572852 norm:8.614963917352725e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:42:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0017736435402184725 norm:8.571631951781455e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:42:50 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0017718920717015862 norm:8.467571205983404e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:43:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0017725232755765319 norm:8.39123094920069e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:44:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0017724887002259493 norm:8.33723879622994e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:44:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 16:45:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.002157623413950205 norm:0.00013993446191307157 max memory_allocated 29270.18798828125 
[2025-02-18 16:46:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.001995723694562912 norm:6.462587771238759e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:47:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.001930724480189383 norm:3.9394177292706445e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:48:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.001897176611237228 norm:2.7800411771750078e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:49:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0018787404987961054 norm:2.1412221030914225e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:49:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.001867407001554966 norm:1.7202917661052197e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:50:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0018603566568344831 norm:1.4679908417747356e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:51:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0018540496239438653 norm:1.2696335033979267e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:52:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0018498132703825831 norm:1.1308437933621462e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:53:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.001846307422965765 norm:1.0395420758868568e-05 max memory_allocated 29270.18798828125 
[2025-02-18 16:54:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0018438482657074928 norm:9.786570444703102e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:54:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0018415404483675957 norm:9.337257324659731e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:55:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.0018401374109089375 norm:8.971126590040512e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:56:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.001838918193243444 norm:8.801784133538604e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:57:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.001837381743825972 norm:8.616532795713283e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:58:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0018364163115620613 norm:8.470700777252205e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:58:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0018356491345912218 norm:8.395320037379861e-06 max memory_allocated 29270.18798828125 
[2025-02-18 16:59:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0018359062960371375 norm:8.303988579427823e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:00:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0018346343422308564 norm:8.269644240499474e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:01:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.001835510483942926 norm:8.24032485979842e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:01:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 17:02:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0021812506020069122 norm:8.151525253197178e-05 max memory_allocated 29270.18798828125 
[2025-02-18 17:03:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0020489755552262068 norm:3.936290158890188e-05 max memory_allocated 29270.18798828125 
[2025-02-18 17:04:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.0019934396259486675 norm:2.5800887669902295e-05 max memory_allocated 29270.18798828125 
[2025-02-18 17:05:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0019636284559965134 norm:1.930241887748707e-05 max memory_allocated 29270.18798828125 
[2025-02-18 17:06:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0019445385551080108 norm:1.562370016472414e-05 max memory_allocated 29270.18798828125 
[2025-02-18 17:06:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.0019322052830830216 norm:1.3127508282195777e-05 max memory_allocated 29270.18798828125 
[2025-02-18 17:07:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0019226802978664637 norm:1.1394366083550267e-05 max memory_allocated 29270.18798828125 
[2025-02-18 17:08:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0019162562675774097 norm:1.032522959576454e-05 max memory_allocated 29270.18798828125 
[2025-02-18 17:09:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.0019110142020508647 norm:9.645883437769953e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:10:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0019074155716225505 norm:9.162848982668947e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:10:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0019050907576456666 norm:8.624600013718009e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:11:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0019027128582820296 norm:8.377582162211183e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:12:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0018994860583916306 norm:8.113561307254713e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:13:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0018993407720699906 norm:7.964486940181814e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:14:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.001898548100143671 norm:7.775608537485823e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:15:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0018968833610415459 norm:7.707960321567953e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:15:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0018970929086208344 norm:7.6280748544377275e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:16:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0018972251564264297 norm:7.608912255818723e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:17:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0018969483207911253 norm:7.536867542512482e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:18:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.001897222362458706 norm:7.52826372263371e-06 max memory_allocated 29270.18798828125 
[2025-02-18 17:18:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 17:19:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.00234365020878613 norm:0.00020950540783815086 max memory_allocated 29270.75048828125 
[2025-02-18 17:20:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.002151009626686573 norm:0.00010139807272935286 max memory_allocated 29270.75048828125 
[2025-02-18 17:21:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.002070969669148326 norm:6.234952161321416e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:22:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.002028506947681308 norm:4.321797678130679e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:22:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.002003306522965431 norm:3.2472729799337685e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:23:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0019877345766872168 norm:2.5552610168233514e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:24:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.001976573606953025 norm:2.0803377992706373e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:25:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0019694999791681767 norm:1.7355192539980635e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:26:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.001964669208973646 norm:1.4882062714605127e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:27:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0019602475222200155 norm:1.3254005352791864e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:27:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0019564733374863863 norm:1.1957194146816619e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:28:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.001953383907675743 norm:1.0909185220953077e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:29:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0019510335987433791 norm:1.0124298569280654e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:30:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0019501445349305868 norm:9.452415724808816e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:31:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0019478462636470795 norm:8.951189556682948e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:32:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0019455774454399943 norm:8.603040441812482e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:32:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0019443694036453962 norm:8.278392670035828e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:33:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0019438964081928134 norm:8.043579327932093e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:34:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0019433291163295507 norm:7.844052561267745e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:35:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0019426289945840836 norm:7.657726200704928e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:35:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 17:36:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.0023043151013553143 norm:6.421637954190373e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:37:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.002190722618252039 norm:3.1917654268909246e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:38:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0021372244227677584 norm:2.089118424919434e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:39:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0021067315246909857 norm:1.5916317352093756e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:39:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0020891211461275816 norm:1.2346037692623213e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:40:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0020775310695171356 norm:1.0071507858810946e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:41:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0020690597593784332 norm:8.703764251549728e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:42:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.002061468316242099 norm:7.847930646676105e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:43:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0020560561679303646 norm:7.20218667993322e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:44:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0020512621849775314 norm:6.774022494937526e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:44:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0020485983695834875 norm:6.4281375671271235e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:45:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0020468374714255333 norm:6.14135888099554e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:46:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0020451946184039116 norm:5.958352630841546e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:47:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0020443592220544815 norm:5.800462986371713e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:48:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.002043625572696328 norm:5.68056566407904e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:49:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.002041922416538 norm:5.667899131367449e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:49:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.002043355954810977 norm:5.482010692503536e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:50:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0020420157816261053 norm:5.574040642386535e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:51:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.002042716834694147 norm:5.446112481877208e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:52:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0020411221776157618 norm:5.485144356498495e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:52:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 17:53:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0023540123365819454 norm:5.874261478311382e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:54:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.002254239749163389 norm:2.945714004454203e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:55:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.002209968864917755 norm:1.9033128410228528e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:56:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0021893843077123165 norm:1.4248879779188428e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:56:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0021761367097496986 norm:1.1482204172352795e-05 max memory_allocated 29270.75048828125 
[2025-02-18 17:57:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0021687003318220377 norm:9.789227988221683e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:58:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0021616590674966574 norm:8.448495464108419e-06 max memory_allocated 29270.75048828125 
[2025-02-18 17:59:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.0021581286564469337 norm:7.5976504376740195e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:00:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.002154802903532982 norm:7.114426807675045e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:01:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0021517996210604906 norm:6.701503934891662e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:01:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0021503593306988478 norm:6.3471261455561034e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:02:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0021495805121958256 norm:6.080421826482052e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:03:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.002149618463590741 norm:5.8936839195666835e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:04:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.002147461986169219 norm:5.862167654413497e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:05:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0021474279928952456 norm:5.654449523717631e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:06:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.002148089464753866 norm:5.575172508542892e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:06:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.002146974205970764 norm:5.4542283578484785e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:07:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.002145446138456464 norm:5.417864485934842e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:08:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.002145372563973069 norm:5.410339326772373e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:09:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0021453541703522205 norm:5.375012733566109e-06 max memory_allocated 29270.75048828125 
[2025-02-18 18:09:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 18:10:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0024880110286176205 norm:6.990527617745101e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:11:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.002398809650912881 norm:3.1374340323964134e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:12:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.002361165825277567 norm:1.994357444345951e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:13:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0023380154743790627 norm:1.4397544873645529e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:13:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0023235122207552195 norm:1.135836100729648e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:14:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.0023145980667322874 norm:9.2434156613308e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:15:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.00230944505892694 norm:7.900644050096162e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:16:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0023040487430989742 norm:7.0787532422400545e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:17:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0023012824822217226 norm:6.381077582773287e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:18:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.002299298532307148 norm:5.892632088944083e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:18:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.002297684084624052 norm:5.704049726773519e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:19:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.002296705963090062 norm:5.422214144346071e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:20:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0022964216768741608 norm:5.292055448080646e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:21:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.00229530967772007 norm:5.223385414865334e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:22:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0022946037352085114 norm:5.126991709403228e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:23:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.002293352037668228 norm:5.097097528050654e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:23:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0022920407354831696 norm:4.999815246264916e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:24:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0022929427213966846 norm:5.036635684518842e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:25:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0022934291046112776 norm:4.983818143955432e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:26:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.0022922735661268234 norm:4.958114004693925e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:26:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 18:27:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0025847924407571554 norm:4.393310518935323e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:28:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0024926995392888784 norm:2.1842908608959988e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:29:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002454588655382395 norm:1.492712544859387e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:30:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0024315661285072565 norm:1.1247003385506105e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:30:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0024202794302254915 norm:8.990744390757754e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:31:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.002410625573247671 norm:7.584354989376152e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:32:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.002404438564553857 norm:6.803965334256645e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:33:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0024002944119274616 norm:6.106251021265052e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:34:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0023968638852238655 norm:5.656856501445873e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:35:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.002393923234194517 norm:5.4534712035092525e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:35:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0023914955090731382 norm:5.160446562513243e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:36:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0023893974721431732 norm:5.0201756494061556e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:37:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.002387041924521327 norm:4.887468094239011e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:38:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0023863385431468487 norm:4.840360816160683e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:39:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.0023844658862799406 norm:4.685979547502939e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:39:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0023843953385949135 norm:4.620901108864928e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:40:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.002383228624239564 norm:4.584410362440394e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:41:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.002383099403232336 norm:4.539368546829792e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:42:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.002383391372859478 norm:4.509226073423633e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:43:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.002384201157838106 norm:4.513654403126566e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:43:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 18:44:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.002732757478952408 norm:8.187665662262589e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:45:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0026352149434387684 norm:3.807667235378176e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:46:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.002595932688564062 norm:2.4015558665269054e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:47:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0025715618394315243 norm:1.7216603737324476e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:47:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.002555929124355316 norm:1.3298104022396728e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:48:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0025444685015827417 norm:1.0812497748702299e-05 max memory_allocated 29271.31298828125 
[2025-02-18 18:49:32 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.002537447027862072 norm:9.133331332122907e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:50:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.002531704492866993 norm:7.960174116306007e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:51:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.002527388045564294 norm:7.209400337160332e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:52:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.0025254322681576014 norm:6.5111435105791315e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:52:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0025227947626262903 norm:6.164786555018509e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:53:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0025206380523741245 norm:5.794974640593864e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:54:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0025192610919475555 norm:5.564433649851708e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:55:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.0025199302472174168 norm:5.386194516177056e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:56:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.002519123489037156 norm:5.217546913627302e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:56:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.0025196049828082323 norm:5.10784775542561e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:57:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.002518916968256235 norm:5.051203061157139e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:58:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0025185593403875828 norm:4.987310148862889e-06 max memory_allocated 29271.31298828125 
[2025-02-18 18:59:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.00251829344779253 norm:4.941397037327988e-06 max memory_allocated 29271.31298828125 
[2025-02-18 19:00:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0025182818062603474 norm:4.929937858833e-06 max memory_allocated 29271.31298828125 
[2025-02-18 19:00:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 19:01:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.0029156459495425224 norm:0.00011383633682271466 max memory_allocated 29271.87548828125 
[2025-02-18 19:02:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.00279979407787323 norm:5.269203393254429e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:03:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.002750505693256855 norm:3.227793058613315e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:04:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.002723416779190302 norm:2.246035546704661e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:04:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.002705903025344014 norm:1.694749153102748e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:05:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.002694718074053526 norm:1.3582523024524562e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:06:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0026874460745602846 norm:1.120809065469075e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:07:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0026803985238075256 norm:9.70449582382571e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:08:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0026746203657239676 norm:8.526159945176914e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:09:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0026714003179222345 norm:7.659305083507206e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:09:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0026676959823817015 norm:7.0743953983765095e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:10:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.002664522733539343 norm:6.5590993472142145e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:11:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.0026634573005139828 norm:6.257887889660196e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:12:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.002661515725776553 norm:5.920928742852993e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:13:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0026620766147971153 norm:5.7118945733236615e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:14:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.002663056366145611 norm:5.537614470085828e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:14:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0026614717207849026 norm:5.391497779783094e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:15:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.002659610705450177 norm:5.258899363980163e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:16:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.00265859835781157 norm:5.173262707103277e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:17:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.002657477278262377 norm:5.134736511536175e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:17:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 19:18:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.002983400598168373 norm:6.492055399576202e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:19:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.002888391027227044 norm:3.642771844170056e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:20:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.002842410933226347 norm:2.4487788323312998e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:21:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0028166389092803 norm:1.8655160602065735e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:21:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.002800919581204653 norm:1.4815843314863741e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:22:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.002789245219901204 norm:1.2292165592953097e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:23:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0027807268779724836 norm:1.0230529369437136e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:24:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0027749836444854736 norm:8.923864697862882e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:25:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0027708210982382298 norm:8.032261575863231e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:26:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0027682690415531397 norm:7.306521638383856e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:26:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0027659765910357237 norm:6.738367574143922e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:27:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.002763373777270317 norm:6.310367098194547e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:28:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.002762393793091178 norm:5.939810762356501e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:29:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0027620899491012096 norm:5.738059826398967e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:30:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.0027617132291197777 norm:5.562911610468291e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:30:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0027618457097560167 norm:5.4966967582004145e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:31:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.002761270385235548 norm:5.431192221294623e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:32:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.0027605295181274414 norm:5.388202680478571e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:33:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0027604743372648954 norm:5.33239926880924e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:34:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.0027602622285485268 norm:5.285278803057736e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:34:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 19:35:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.00322366738691926 norm:0.00010367145296186209 max memory_allocated 29271.87548828125 
[2025-02-18 19:36:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.003113467711955309 norm:5.3569347073789686e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:37:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.003061783965677023 norm:3.474634650046937e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:38:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.0030309685971587896 norm:2.518171640986111e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:38:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0030098857823759317 norm:1.9727525796042755e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:39:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.002994928974658251 norm:1.62339838425396e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:40:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.002985419938340783 norm:1.3769184988632333e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:41:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0029779630713164806 norm:1.1901871403097175e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:42:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.00297141307964921 norm:1.0517218470340595e-05 max memory_allocated 29271.87548828125 
[2025-02-18 19:43:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.0029664544854313135 norm:9.526190297037829e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:43:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.002962386468425393 norm:8.802876436675433e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:44:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.002959650941193104 norm:8.284022442239802e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:45:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0029581100679934025 norm:7.701294634898659e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:46:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0029577319510281086 norm:7.312744401133386e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:47:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.002956276759505272 norm:7.09338155502337e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:47:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.002954576862975955 norm:6.848838438600069e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:48:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.0029528955928981304 norm:6.679523721686564e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:49:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.002951212227344513 norm:6.613769073737785e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:50:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0029511279426515102 norm:6.534185104101198e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:51:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.0029511793982237577 norm:6.444173322961433e-06 max memory_allocated 29271.87548828125 
[2025-02-18 19:51:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 19:52:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.0032991264015436172 norm:6.774673238396645e-05 max memory_allocated 29272.43798828125 
[2025-02-18 19:53:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0032191872596740723 norm:3.4437343856552616e-05 max memory_allocated 29272.43798828125 
[2025-02-18 19:54:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.0031819443684071302 norm:2.303352448507212e-05 max memory_allocated 29272.43798828125 
[2025-02-18 19:55:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.0031612254679203033 norm:1.7846617993200198e-05 max memory_allocated 29272.43798828125 
[2025-02-18 19:55:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0031461447943001986 norm:1.4604815987695474e-05 max memory_allocated 29272.43798828125 
[2025-02-18 19:56:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.0031344937160611153 norm:1.2782383237208705e-05 max memory_allocated 29272.43798828125 
[2025-02-18 19:57:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.0031257825903594494 norm:1.1765196177293546e-05 max memory_allocated 29272.43798828125 
[2025-02-18 19:58:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.003119018394500017 norm:1.0499765267013572e-05 max memory_allocated 29272.43798828125 
[2025-02-18 19:59:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0031157960183918476 norm:9.617643627279904e-06 max memory_allocated 29272.43798828125 
[2025-02-18 19:59:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.003113845130428672 norm:9.002577826322522e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:00:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.0031101261265575886 norm:9.274553121940698e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:01:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.0031071235425770283 norm:9.07286630535964e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:02:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.003106432966887951 norm:8.261955372290686e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:03:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.003105730749666691 norm:8.225908459280618e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:04:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.003104252042248845 norm:8.19980868982384e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:04:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.003102582646533847 norm:8.379077371500898e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:05:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.003101764479652047 norm:8.225947567552794e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:06:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0031012087129056454 norm:8.454526323475875e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:07:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.0031004585325717926 norm:7.896608622104395e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:08:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0031001484021544456 norm:8.145314495777711e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:08:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 20:09:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.003516657743602991 norm:7.317125709960237e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:10:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0034227720461785793 norm:3.724926500581205e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:11:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.003377422923222184 norm:2.4628421670058742e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:11:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.0033497123513370752 norm:1.832879934227094e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:12:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.003331330604851246 norm:1.4662007743027061e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:13:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.0033181668259203434 norm:1.2388370123517234e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:14:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.0033082463778555393 norm:1.0728483175626025e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:15:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.003300219774246216 norm:9.659435818321072e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:16:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.0032938304357230663 norm:8.789794264885131e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:16:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.003288231324404478 norm:8.229092600231525e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:17:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.003283679485321045 norm:7.802958862157539e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:18:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.0032814028672873974 norm:7.417521828756435e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:19:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.0032787281088531017 norm:7.1744962042430416e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:20:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.003277297131717205 norm:6.956944162084255e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:21:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.003275217255577445 norm:6.790985935367644e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:21:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.0032734975684434175 norm:6.644291715929285e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:22:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.0032725194469094276 norm:6.5505560087331105e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:23:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.0032711320091038942 norm:6.4580572143313475e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:24:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0032704633194953203 norm:6.350273451971589e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:25:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.003269769949838519 norm:6.282289632508764e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:25:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 20:26:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.0037515093572437763 norm:6.331762415356934e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:27:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.0036680111661553383 norm:3.575188384274952e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:28:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.003624924924224615 norm:2.523560578993056e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:28:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.003597584320232272 norm:1.9266757590230554e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:29:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.003577813506126404 norm:1.578143383085262e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:30:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.003563352394849062 norm:1.3440841939882375e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:31:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0035524049308151007 norm:1.1878482837346382e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:32:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0035434928722679615 norm:1.0689875125535764e-05 max memory_allocated 29272.43798828125 
[2025-02-18 20:33:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0035372537095099688 norm:9.784239409782458e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:33:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.0035313512198626995 norm:9.103115189645905e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:34:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.0035263323225080967 norm:8.533971595170442e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:35:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.0035216761752963066 norm:8.075489859038498e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:36:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.0035187257453799248 norm:7.670261766179465e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:37:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.003516905941069126 norm:7.378385816991795e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:38:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.0035148714669048786 norm:7.161755092965905e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:38:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.0035122702829539776 norm:6.955439403100172e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:39:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.0035100793465971947 norm:6.779866453143768e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:40:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.0035083391703665257 norm:6.658623988187173e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:41:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.003507093293592334 norm:6.532226962008281e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:42:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0035065747797489166 norm:6.4540718085481785e-06 max memory_allocated 29272.43798828125 
[2025-02-18 20:42:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 20:43:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.004005615599453449 norm:7.173866470111534e-05 max memory_allocated 29273.00048828125 
[2025-02-18 20:44:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.0039121005684137344 norm:3.8069159927545115e-05 max memory_allocated 29273.00048828125 
[2025-02-18 20:45:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.0038672236260026693 norm:2.621548446768429e-05 max memory_allocated 29273.00048828125 
[2025-02-18 20:45:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.0038379384204745293 norm:1.995992352021858e-05 max memory_allocated 29273.00048828125 
[2025-02-18 20:46:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.0038190821651369333 norm:1.6210507965297438e-05 max memory_allocated 29273.00048828125 
[2025-02-18 20:47:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0038045465480536222 norm:1.3766158190264832e-05 max memory_allocated 29273.00048828125 
[2025-02-18 20:48:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.0037929704412817955 norm:1.1897979675268289e-05 max memory_allocated 29273.00048828125 
[2025-02-18 20:49:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.0037829161155968904 norm:1.0634460522851441e-05 max memory_allocated 29273.00048828125 
[2025-02-18 20:50:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.0037757453974336386 norm:9.625888196751475e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:50:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.0037695132195949554 norm:8.910043106880039e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:51:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.0037640882655978203 norm:8.471071851090528e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:52:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.0037595657631754875 norm:8.011999852897134e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:53:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.0037565561942756176 norm:7.654907676624134e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:54:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.003753419267013669 norm:7.314559752558125e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:55:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.0037508762907236814 norm:7.088557140377816e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:55:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.003749572904780507 norm:6.8720428316737525e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:56:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.0037479973398149014 norm:6.7012156250712e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:57:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0037460888270288706 norm:6.608074272662634e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:58:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.0037446822971105576 norm:6.444942300731782e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:59:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.003744821297004819 norm:6.37275843473617e-06 max memory_allocated 29273.00048828125 
[2025-02-18 20:59:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 21:00:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.004373548552393913 norm:6.705665146000683e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:01:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.004286978393793106 norm:3.8070462323958054e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:02:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.00424371100962162 norm:2.712339482968673e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:02:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.004214223008602858 norm:2.122570549545344e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:03:43 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.004194627981632948 norm:1.7771446437109262e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:04:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.004178167320787907 norm:1.5570954928989522e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:05:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.004167539533227682 norm:1.377118951495504e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:06:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.0041573261842131615 norm:1.2499657714215573e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:07:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.004149754997342825 norm:1.1659009942377452e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:07:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.0041435169987380505 norm:1.0933194062090479e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:08:40 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.004137424752116203 norm:1.0294013009115588e-05 max memory_allocated 29273.18798828125 
[2025-02-18 21:09:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.0041325148195028305 norm:9.821220373851247e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:10:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.0041292328387498856 norm:9.393354957865085e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:11:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.004128082189708948 norm:9.046385457622819e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:11:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.00412508612498641 norm:8.754770533414558e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:12:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.0041230604983866215 norm:8.477943083562423e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:13:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.0041215443052351475 norm:8.27692110760836e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:14:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.004120717756450176 norm:8.101756975520402e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:15:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.004119696095585823 norm:7.971796549099963e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:16:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.004119785502552986 norm:7.850601832615212e-06 max memory_allocated 29273.18798828125 
[2025-02-18 21:16:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 21:17:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.00483672833070159 norm:7.232004281831905e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:18:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.004733302630484104 norm:4.413973874761723e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:19:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.004676681477576494 norm:3.151139026158489e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:19:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.004639823455363512 norm:2.481375304341782e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:20:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.004615209065377712 norm:2.0408650016179308e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:21:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.004595814738422632 norm:1.7509300960227847e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:22:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.0045799934305250645 norm:1.534103648737073e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:23:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.004567879252135754 norm:1.3742605005973019e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:24:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.004556729923933744 norm:1.2462961421988439e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:24:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.004549652338027954 norm:1.1442930372140836e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:25:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.004542756825685501 norm:1.0692696378100663e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:26:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.004537258297204971 norm:1.0081807886308525e-05 max memory_allocated 29273.37548828125 
[2025-02-18 21:27:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.0045332838781178 norm:9.544317435938865e-06 max memory_allocated 29273.37548828125 
[2025-02-18 21:28:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.004527912940829992 norm:9.07153844309505e-06 max memory_allocated 29273.37548828125 
[2025-02-18 21:29:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.0045261080376803875 norm:8.668432201375253e-06 max memory_allocated 29273.37548828125 
[2025-02-18 21:29:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.004522970877587795 norm:8.40520533529343e-06 max memory_allocated 29273.37548828125 
[2025-02-18 21:30:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.004520026035606861 norm:8.104271728370804e-06 max memory_allocated 29273.37548828125 
[2025-02-18 21:31:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.004517934285104275 norm:7.87651697464753e-06 max memory_allocated 29273.37548828125 
[2025-02-18 21:32:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.004516980145126581 norm:7.659290531591978e-06 max memory_allocated 29273.37548828125 
[2025-02-18 21:33:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.00451637664809823 norm:7.46889509173343e-06 max memory_allocated 29273.37548828125 
[2025-02-18 21:33:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 21:34:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.005316704977303743 norm:0.0001737104175845161 max memory_allocated 29273.56298828125 
[2025-02-18 21:35:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.005192855838686228 norm:9.54285278567113e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:36:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.005131340119987726 norm:6.227401172509417e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:36:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.005094124469906092 norm:4.499367787502706e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:37:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.005068872589617968 norm:3.510728129185736e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:38:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.005049342289566994 norm:2.8424212359823287e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:39:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.005035636015236378 norm:2.3613283701706678e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:40:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.005023809149861336 norm:2.01915991056012e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:41:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.005014035850763321 norm:1.757847348926589e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:41:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.005005964543670416 norm:1.5412864740937948e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:42:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.004999049473553896 norm:1.388122564094374e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:43:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.004993888549506664 norm:1.2486731975513976e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:44:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.004989835433661938 norm:1.1374551831977442e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:45:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.004986815620213747 norm:1.0453482900629751e-05 max memory_allocated 29273.56298828125 
[2025-02-18 21:46:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.004983810242265463 norm:9.721110473037697e-06 max memory_allocated 29273.56298828125 
[2025-02-18 21:46:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.004980993922799826 norm:9.121244147536345e-06 max memory_allocated 29273.56298828125 
[2025-02-18 21:47:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.004978478886187077 norm:8.61516673467122e-06 max memory_allocated 29273.56298828125 
[2025-02-18 21:48:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.00497668469324708 norm:8.169160537363496e-06 max memory_allocated 29273.56298828125 
[2025-02-18 21:49:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.004975032992660999 norm:7.831113180145621e-06 max memory_allocated 29273.56298828125 
[2025-02-18 21:50:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.004973802249878645 norm:7.516749064961914e-06 max memory_allocated 29273.56298828125 
[2025-02-18 21:50:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 21:51:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.005748894065618515 norm:0.00015154998982325196 max memory_allocated 29273.75048828125 
[2025-02-18 21:52:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.005650423001497984 norm:0.001153389923274517 max memory_allocated 29273.75048828125 
[2025-02-18 21:53:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.005589268170297146 norm:4.4566702854353935e-05 max memory_allocated 29273.75048828125 
[2025-02-18 21:53:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.005564355757087469 norm:3.358265894348733e-05 max memory_allocated 29273.75048828125 
[2025-02-18 21:54:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.005544788204133511 norm:2.7332218451192603e-05 max memory_allocated 29273.75048828125 
[2025-02-18 21:55:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.005530334077775478 norm:2.3103015337255783e-05 max memory_allocated 29273.75048828125 
[2025-02-18 21:56:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.005518661346286535 norm:1.9982897356385365e-05 max memory_allocated 29273.75048828125 
[2025-02-18 21:57:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.005508893169462681 norm:1.7812724763643928e-05 max memory_allocated 29273.75048828125 
[2025-02-18 21:58:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.005500429309904575 norm:1.5833735233172774e-05 max memory_allocated 29273.75048828125 
[2025-02-18 21:58:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.005493186414241791 norm:1.4226547136786394e-05 max memory_allocated 29273.75048828125 
[2025-02-18 21:59:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.005486768204718828 norm:1.300175063079223e-05 max memory_allocated 29273.75048828125 
[2025-02-18 22:00:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.005481582600623369 norm:1.1983978765783831e-05 max memory_allocated 29273.75048828125 
[2025-02-18 22:01:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.005477402359247208 norm:1.1077841918449849e-05 max memory_allocated 29273.75048828125 
[2025-02-18 22:02:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.005473686382174492 norm:1.0395157914899755e-05 max memory_allocated 29273.75048828125 
[2025-02-18 22:03:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.0054703326895833015 norm:9.853320079855621e-06 max memory_allocated 29273.75048828125 
[2025-02-18 22:03:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.00546762952581048 norm:9.330439752375241e-06 max memory_allocated 29273.75048828125 
[2025-02-18 22:04:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.005465659778565168 norm:8.85797999217175e-06 max memory_allocated 29273.75048828125 
[2025-02-18 22:05:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.0054634627886116505 norm:8.432538379565813e-06 max memory_allocated 29273.75048828125 
[2025-02-18 22:06:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.005461736582219601 norm:8.117654033412691e-06 max memory_allocated 29273.75048828125 
[2025-02-18 22:07:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.005460137501358986 norm:7.770145202812273e-06 max memory_allocated 29273.75048828125 
[2025-02-18 22:07:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 22:08:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.00619781669229269 norm:9.041721932590008e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:09:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.00612772349268198 norm:4.657750832848251e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:10:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.006091914605349302 norm:3.082799230469391e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:10:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.006072632502764463 norm:2.328393020434305e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:11:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.006058910395950079 norm:1.867484752438031e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:12:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.00604771776124835 norm:1.556448478368111e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:13:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.006038342602550983 norm:1.337824960501166e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:14:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.006032082252204418 norm:1.1894865565409418e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:15:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.0060269273817539215 norm:1.0755358744063415e-05 max memory_allocated 29273.93798828125 
[2025-02-18 22:15:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.006021673791110516 norm:9.799919098441023e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:16:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.006017256528139114 norm:9.054194379132241e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:17:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.006013656500726938 norm:8.435446943622082e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:18:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.006010276731103659 norm:7.901273420429789e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:19:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.006007692310959101 norm:7.500946594518609e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:20:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.00600493885576725 norm:7.154157174227294e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:20:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.006002870853990316 norm:6.879449756524991e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:21:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.00600044010207057 norm:6.6180614339828026e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:22:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.005998881068080664 norm:6.4079467847477645e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:23:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.005996886640787125 norm:6.203514203662053e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:24:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.005995400715619326 norm:6.018663953000214e-06 max memory_allocated 29273.93798828125 
[2025-02-18 22:24:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 22:25:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.006897422950714827 norm:8.233674452640116e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:26:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.0068101100623607635 norm:4.793660627910867e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:27:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.006765283178538084 norm:3.320128234918229e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:27:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.00673522986471653 norm:2.506237433408387e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:28:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.006716646254062653 norm:1.987396171898581e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:29:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.00670212134718895 norm:1.6384687114623375e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:30:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.006689883302897215 norm:1.3861297702533193e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:31:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.006678973790258169 norm:1.2015063475701027e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:32:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.006671844981610775 norm:1.0664220098988153e-05 max memory_allocated 29274.12548828125 
[2025-02-18 22:32:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.006665342021733522 norm:9.60326815402368e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:33:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.0066595072858035564 norm:8.776938557275571e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:34:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.006655301433056593 norm:8.08017466624733e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:35:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.006652196403592825 norm:7.5669136094802525e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:36:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.006647862959653139 norm:7.076501788105816e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:37:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.006645407062023878 norm:6.698614015476778e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:37:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.0066443514078855515 norm:6.41520318822586e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:38:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.006641310174018145 norm:6.196429239935242e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:39:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.006639302708208561 norm:5.940562914474867e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:40:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.006638377904891968 norm:5.816614702780498e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:41:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.006636728998273611 norm:5.651878382195719e-06 max memory_allocated 29274.12548828125 
[2025-02-18 22:41:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 22:42:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.007371885236352682 norm:2.361306360398885e-05 max memory_allocated 29274.31298828125 
[2025-02-18 22:43:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.0073368907906115055 norm:1.4712722986587323e-05 max memory_allocated 29274.31298828125 
[2025-02-18 22:44:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.007318466901779175 norm:1.0972677955578547e-05 max memory_allocated 29274.31298828125 
[2025-02-18 22:44:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.007304469123482704 norm:8.933421668189112e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:45:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.007294822484254837 norm:7.732453013886698e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:46:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.00728612719103694 norm:6.922979537193896e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:47:26 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.0072790393605828285 norm:6.375120392476674e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:48:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.007274214178323746 norm:5.942410098214168e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:49:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.007269730791449547 norm:5.632638476527063e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:49:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.007265499792993069 norm:5.400289410317782e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:50:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.007261497899889946 norm:5.2103641792200506e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:51:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.0072581153362989426 norm:5.048896582593443e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:52:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.007255913689732552 norm:4.9282421059615444e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:53:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.007254420313984156 norm:4.821369202545611e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:54:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.007252341136336327 norm:4.730993168777786e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:54:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.007249785121530294 norm:4.663250365410931e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:55:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.007248816546052694 norm:4.6066306822467595e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:56:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.007248119451105595 norm:4.549566256173421e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:57:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.007246732711791992 norm:4.499865553952986e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:58:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.007245480082929134 norm:4.462052856979426e-06 max memory_allocated 29274.31298828125 
[2025-02-18 22:58:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 22:59:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.00826337467879057 norm:0.00011931266635656357 max memory_allocated 29274.50048828125 
[2025-02-18 23:00:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.008170981891453266 norm:6.889191718073562e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:01:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.008125570602715015 norm:4.7158217057585716e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:01:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.008095312863588333 norm:3.5070814192295074e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:02:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.008075200952589512 norm:2.7410082111600786e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:03:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.008063260465860367 norm:2.221130307589192e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:04:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.008050433360040188 norm:1.874902591225691e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:05:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.008042414672672749 norm:1.6136164049385116e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:06:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.008035401813685894 norm:1.4092109267949127e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:06:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.008029173128306866 norm:1.2524391422630288e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:07:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.008022939786314964 norm:1.1171096957696136e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:08:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.008018188178539276 norm:1.019935734802857e-05 max memory_allocated 29274.50048828125 
[2025-02-18 23:09:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.008014004677534103 norm:9.296726602769922e-06 max memory_allocated 29274.50048828125 
[2025-02-18 23:10:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.008011250756680965 norm:8.602807611168828e-06 max memory_allocated 29274.50048828125 
[2025-02-18 23:11:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.00800964143127203 norm:8.01313672127435e-06 max memory_allocated 29274.50048828125 
[2025-02-18 23:11:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.008006899617612362 norm:7.496766556869261e-06 max memory_allocated 29274.50048828125 
[2025-02-18 23:12:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.008005769923329353 norm:7.076243946357863e-06 max memory_allocated 29274.50048828125 
[2025-02-18 23:13:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.008004928939044476 norm:6.634798410232179e-06 max memory_allocated 29274.50048828125 
[2025-02-18 23:14:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.008004417642951012 norm:6.327812570816604e-06 max memory_allocated 29274.50048828125 
[2025-02-18 23:15:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.008002662099897861 norm:6.130748715804657e-06 max memory_allocated 29274.50048828125 
[2025-02-18 23:15:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 23:16:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.008923658169806004 norm:4.4313557737041265e-05 max memory_allocated 29274.68798828125 
[2025-02-18 23:17:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.008873002603650093 norm:2.5456552975811064e-05 max memory_allocated 29274.68798828125 
[2025-02-18 23:18:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.008846689946949482 norm:1.7756514353095554e-05 max memory_allocated 29274.68798828125 
[2025-02-18 23:18:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.008832800202071667 norm:1.3744558600592427e-05 max memory_allocated 29274.68798828125 
[2025-02-18 23:19:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.008819648995995522 norm:1.1126338904432487e-05 max memory_allocated 29274.68798828125 
[2025-02-18 23:20:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.008809680119156837 norm:9.44543626246741e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:21:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.008802009746432304 norm:8.245067874668166e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:22:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.00879440363496542 norm:7.359943538176594e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:23:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.008789142593741417 norm:6.6870093178295065e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:23:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.008783973753452301 norm:6.224579919944517e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:24:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.008780969306826591 norm:5.761905867984751e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:25:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.008777935989201069 norm:5.379177764552878e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:26:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.00877396296709776 norm:5.117286036693258e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:27:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.008771909400820732 norm:4.8710699047660455e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:27:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.008769381791353226 norm:4.676213393395301e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:28:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.008766538463532925 norm:4.5613469410454854e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:29:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.008763996884226799 norm:4.4444632294471376e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:30:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.0087630869820714 norm:4.3279205783619545e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:31:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.00876179151237011 norm:4.2403808038216084e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:32:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.008761031553149223 norm:4.164584424870554e-06 max memory_allocated 29274.68798828125 
[2025-02-18 23:32:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 23:33:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.009824655950069427 norm:6.859047425678e-05 max memory_allocated 29274.87548828125 
[2025-02-18 23:34:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.009748462587594986 norm:3.836924588540569e-05 max memory_allocated 29274.87548828125 
[2025-02-18 23:34:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.009708649478852749 norm:2.606233647384215e-05 max memory_allocated 29274.87548828125 
[2025-02-18 23:35:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.009686517529189587 norm:1.9409897504374385e-05 max memory_allocated 29274.87548828125 
[2025-02-18 23:36:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.009666502475738525 norm:1.544106999062933e-05 max memory_allocated 29274.87548828125 
[2025-02-18 23:37:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.009652731008827686 norm:1.2667278497247025e-05 max memory_allocated 29274.87548828125 
[2025-02-18 23:38:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.009643406607210636 norm:1.0897655556618702e-05 max memory_allocated 29274.87548828125 
[2025-02-18 23:39:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.009635606780648232 norm:9.60071793087991e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:39:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.00962925422936678 norm:8.540920134691987e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:40:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.009624054655432701 norm:7.639718205609825e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:41:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.009618757292628288 norm:6.881669833092019e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:42:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.009613871574401855 norm:6.382230367307784e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:43:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.009610203094780445 norm:5.948822035861667e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:44:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.009607246145606041 norm:5.631447947962442e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:44:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.00960532296448946 norm:5.310988399287453e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:45:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.009602105244994164 norm:5.026423423259985e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:46:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.009600676596164703 norm:4.821601123694563e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:47:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.00959996972233057 norm:4.632390300685074e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:48:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.009598850272595882 norm:4.481466021388769e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:49:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.009598147124052048 norm:4.3470458876981866e-06 max memory_allocated 29274.87548828125 
[2025-02-18 23:49:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 23:50:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.010852999985218048 norm:0.00013542403758037835 max memory_allocated 29275.06298828125 
[2025-02-18 23:51:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.010753094218671322 norm:8.670178795000538e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:51:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.01069549098610878 norm:6.227123230928555e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:52:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.010659391060471535 norm:4.7877063479973e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:53:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.01063181646168232 norm:3.847133120871149e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:54:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.010611283592879772 norm:3.175964957335964e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:55:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.010595091618597507 norm:2.6844596504815854e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:56:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.010580581612884998 norm:2.313645018148236e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:56:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.01056960504502058 norm:2.0308303646743298e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:57:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.0105611402541399 norm:1.7999522242462263e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:58:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.010555238462984562 norm:1.5924313629511744e-05 max memory_allocated 29275.06298828125 
[2025-02-18 23:59:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.010549027472734451 norm:1.4354417544382159e-05 max memory_allocated 29275.06298828125 
[2025-02-19 00:00:08 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.010544372722506523 norm:1.3047977517999243e-05 max memory_allocated 29275.06298828125 
[2025-02-19 00:00:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.01053865160793066 norm:1.2047104064549785e-05 max memory_allocated 29275.06298828125 
[2025-02-19 00:01:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.010534207336604595 norm:1.1024146260751877e-05 max memory_allocated 29275.06298828125 
[2025-02-19 00:02:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.010531565174460411 norm:1.0120749720954336e-05 max memory_allocated 29275.06298828125 
[2025-02-19 00:03:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.010529449209570885 norm:9.338450581708457e-06 max memory_allocated 29275.06298828125 
[2025-02-19 00:04:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.010526762343943119 norm:8.837523637339473e-06 max memory_allocated 29275.06298828125 
[2025-02-19 00:05:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.01052398607134819 norm:8.381456609640736e-06 max memory_allocated 29275.06298828125 
[2025-02-19 00:05:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.010522345080971718 norm:7.884009392000735e-06 max memory_allocated 29275.06298828125 
[2025-02-19 00:06:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-19 00:07:08 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.011784820817410946 norm:9.734281775308773e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:07:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.011704088188707829 norm:5.734368824050762e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:08:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.011663849465548992 norm:3.931830724468455e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:09:37 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.011638155207037926 norm:2.9091908800182864e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:10:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.011619155295193195 norm:2.2634918423136696e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:11:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.011603410355746746 norm:1.836917181208264e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:12:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.01159166730940342 norm:1.5444375094375573e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:12:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.011582151986658573 norm:1.3142467651050538e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:13:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.011575963348150253 norm:1.1355010428815149e-05 max memory_allocated 29275.25048828125 
[2025-02-19 00:14:34 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.011570858769118786 norm:9.96383278106805e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:15:24 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.011566316708922386 norm:8.874725608620793e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:16:13 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.01155979372560978 norm:8.003607035789173e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:17:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.011556534096598625 norm:7.571897640445968e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:17:52 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.01154831238090992 norm:6.787600341340294e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:18:42 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.011545157060027122 norm:6.392329396476271e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:19:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.011542352847754955 norm:6.015920462232316e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:20:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.01154076587408781 norm:5.637892627419205e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:21:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.011538466438651085 norm:5.40766495760181e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:22:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.011536389589309692 norm:5.210880317463307e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:22:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.011535071767866611 norm:4.993123184249271e-06 max memory_allocated 29275.25048828125 
[2025-02-19 00:23:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-19 00:24:01 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.012894632294774055 norm:9.420907736057416e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:24:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.012791234068572521 norm:5.909485480515286e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:25:40 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.012735897675156593 norm:4.2204752389807254e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:26:30 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.012698497623205185 norm:3.209441274520941e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:27:19 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.012676406651735306 norm:2.528484219510574e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:28:09 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.012658628635108471 norm:2.5433992050238885e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:28:58 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.012645314447581768 norm:1.754720688040834e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:29:48 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.012633414939045906 norm:1.5194211300695315e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:30:37 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.012621925212442875 norm:1.3478566870617215e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:31:27 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.012614184059202671 norm:1.1880624697369058e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:32:17 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.012610907666385174 norm:1.0409285096102394e-05 max memory_allocated 29275.43798828125 
[2025-02-19 00:33:06 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.012602834962308407 norm:9.592932656232733e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:33:56 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.012600695714354515 norm:8.472177796647884e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:34:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.012600390240550041 norm:7.657423338969238e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:35:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.012596769258379936 norm:7.097953130141832e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:36:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.012593663297593594 norm:6.670045877399389e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:37:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.012591095641255379 norm:6.2924009398557246e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:38:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.012588444165885448 norm:6.013753591105342e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:38:53 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.012588052079081535 norm:5.679353762388928e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:39:43 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.012587619945406914 norm:5.399201654654462e-06 max memory_allocated 29275.43798828125 
[2025-02-19 00:39:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-19 00:40:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.014116456732153893 norm:6.400166603270918e-05 max memory_allocated 29275.62548828125 
[2025-02-19 00:41:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.014045080170035362 norm:3.70664092770312e-05 max memory_allocated 29275.62548828125 
[2025-02-19 00:42:34 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.01400809921324253 norm:2.5394036129000597e-05 max memory_allocated 29275.62548828125 
[2025-02-19 00:43:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.013982818461954594 norm:1.8945764168165624e-05 max memory_allocated 29275.62548828125 
[2025-02-19 00:44:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.013966369442641735 norm:1.4951511730032507e-05 max memory_allocated 29275.62548828125 
[2025-02-19 00:45:02 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.013952594250440598 norm:1.2309601515880786e-05 max memory_allocated 29275.62548828125 
[2025-02-19 00:45:52 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.0139414481818676 norm:1.032980526360916e-05 max memory_allocated 29275.62548828125 
[2025-02-19 00:46:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.013930493965744972 norm:8.925472684495617e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:47:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.013926886022090912 norm:7.728454875177704e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:48:21 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.013922634534537792 norm:6.987147116888082e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:49:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.013918576762080193 norm:6.306313935056096e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:50:00 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.013913268223404884 norm:5.914336725254543e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:50:49 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.0139091107994318 norm:5.531868737307377e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:51:39 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.013906633481383324 norm:5.217075340624433e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:52:28 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.013904409483075142 norm:4.994048595108325e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:53:18 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.013901922851800919 norm:4.7585244828951545e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:54:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.013900039717555046 norm:4.651772087527206e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:54:57 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.01389884203672409 norm:4.506461209530244e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:55:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.013896692544221878 norm:4.394142251840094e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:56:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.013895940035581589 norm:4.347282811067998e-06 max memory_allocated 29275.62548828125 
[2025-02-19 00:56:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-19 00:57:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.015446830540895462 norm:2.4351071260753088e-05 max memory_allocated 29275.81298828125 
[2025-02-19 00:58:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.01539003849029541 norm:1.5538867955910973e-05 max memory_allocated 29275.81298828125 
[2025-02-19 00:59:31 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.015361111611127853 norm:1.177893045678502e-05 max memory_allocated 29275.81298828125 
[2025-02-19 01:00:20 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.015343266539275646 norm:9.505244634055998e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:01:10 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.015332814306020737 norm:8.207774044421967e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:02:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.015323792584240437 norm:7.213891421997687e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:02:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.015314576216042042 norm:6.527259301947197e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:03:39 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.015306507237255573 norm:6.0286151892796624e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:04:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.015300724655389786 norm:5.6732151278993115e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:05:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.015295663848519325 norm:5.386171778809512e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:06:08 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.015291626565158367 norm:5.08864650328178e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:06:58 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.015289343893527985 norm:4.859610271523707e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:07:47 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.015284351073205471 norm:4.741192242363468e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:08:37 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.015282338485121727 norm:4.607032678904943e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:09:26 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.015279863961040974 norm:4.533451829047408e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:10:16 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.01527734100818634 norm:4.45118985226145e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:11:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.015275604091584682 norm:4.44001170762931e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:11:55 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.015274764969944954 norm:4.364254436950432e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:12:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.015272564254701138 norm:4.318776063882979e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:13:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.015271145850419998 norm:4.320385414757766e-06 max memory_allocated 29275.81298828125 
[2025-02-19 01:13:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-19 01:13:58 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:14:47 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.017573874443769455 norm:0.0016412138938903809 max memory_allocated 29276.14501953125 
[2025-02-19 01:15:37 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.01733400486409664 norm:0.001418479485437274 max memory_allocated 29276.14501953125 
[2025-02-19 01:16:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.017215687781572342 norm:0.0012055560946464539 max memory_allocated 29276.14501953125 
[2025-02-19 01:17:17 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.017133887857198715 norm:0.0010185933206230402 max memory_allocated 29276.14501953125 
[2025-02-19 01:18:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.017066601663827896 norm:0.0008401170489378273 max memory_allocated 29276.14501953125 
[2025-02-19 01:18:56 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.01700960472226143 norm:0.0006957704899832606 max memory_allocated 29276.14501953125 
[2025-02-19 01:19:46 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.01697082445025444 norm:0.0005823139217682183 max memory_allocated 29276.14501953125 
[2025-02-19 01:20:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.016940170899033546 norm:0.000523981754668057 max memory_allocated 29276.14501953125 
[2025-02-19 01:21:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.01693297177553177 norm:0.000522517366334796 max memory_allocated 29276.14501953125 
[2025-02-19 01:22:15 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.016922764480113983 norm:0.0005389410071074963 max memory_allocated 29276.14501953125 
[2025-02-19 01:23:05 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.01690652407705784 norm:0.0005041579133830965 max memory_allocated 29276.14501953125 
[2025-02-19 01:23:55 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.01688559539616108 norm:0.00048316307947970927 max memory_allocated 29276.14501953125 
[2025-02-19 01:24:44 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.016879644244909286 norm:0.00043668795842677355 max memory_allocated 29276.14501953125 
[2025-02-19 01:25:34 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.016872936859726906 norm:0.0004874965234193951 max memory_allocated 29276.14501953125 
[2025-02-19 01:26:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.016895800828933716 norm:0.00039080591523088515 max memory_allocated 29276.14501953125 
[2025-02-19 01:27:14 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.01685779169201851 norm:0.00047562667168676853 max memory_allocated 29276.14501953125 
[2025-02-19 01:28:03 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.016867976635694504 norm:0.0003833480877801776 max memory_allocated 29276.14501953125 
[2025-02-19 01:28:53 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.01684030517935753 norm:0.00042989879148080945 max memory_allocated 29276.14501953125 
[2025-02-19 01:29:43 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.01684943586587906 norm:0.000395947543438524 max memory_allocated 29276.14501953125 
[2025-02-19 01:30:32 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.01683712564408779 norm:0.0004158808442298323 max memory_allocated 29276.14501953125 
[2025-02-19 01:30:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-19 01:30:59 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:31:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.019228283315896988 norm:0.0013252443168312311 max memory_allocated 29276.33251953125 
[2025-02-19 01:32:38 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.0190342478454113 norm:0.0010402991902083158 max memory_allocated 29276.33251953125 
[2025-02-19 01:33:28 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.018938390538096428 norm:0.0007224989240057766 max memory_allocated 29276.33251953125 
[2025-02-19 01:34:18 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.018891187384724617 norm:0.0007172951591201127 max memory_allocated 29276.33251953125 
[2025-02-19 01:35:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.018857162445783615 norm:0.0007576963980682194 max memory_allocated 29276.33251953125 
[2025-02-19 01:35:57 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.01881447248160839 norm:0.0006739047821611166 max memory_allocated 29276.33251953125 
[2025-02-19 01:36:47 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.018784163519740105 norm:0.0006011258228681982 max memory_allocated 29276.33251953125 
[2025-02-19 01:37:37 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.018763510510325432 norm:0.0004949752474203706 max memory_allocated 29276.33251953125 
[2025-02-19 01:38:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.01875888928771019 norm:0.0005060639814473689 max memory_allocated 29276.33251953125 
[2025-02-19 01:39:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.018778076395392418 norm:0.0005399251822382212 max memory_allocated 29276.33251953125 
[2025-02-19 01:40:07 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.018747080117464066 norm:0.0005376170156523585 max memory_allocated 29276.33251953125 
[2025-02-19 01:40:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.01869150623679161 norm:0.00042182145989499986 max memory_allocated 29276.33251953125 
[2025-02-19 01:41:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.018679790198802948 norm:0.0004050012503284961 max memory_allocated 29276.33251953125 
[2025-02-19 01:42:36 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.018671134486794472 norm:0.0003735331411007792 max memory_allocated 29276.33251953125 
[2025-02-19 01:43:26 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.018667351454496384 norm:0.00038561099790968 max memory_allocated 29276.33251953125 
[2025-02-19 01:44:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.018666591495275497 norm:0.0003786435991059989 max memory_allocated 29276.33251953125 
[2025-02-19 01:45:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.018670132383704185 norm:0.0004162118420936167 max memory_allocated 29276.33251953125 
[2025-02-19 01:45:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.018677813932299614 norm:0.0004009646363556385 max memory_allocated 29276.33251953125 
[2025-02-19 01:46:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.018679684028029442 norm:0.00043944918434135616 max memory_allocated 29276.33251953125 
[2025-02-19 01:47:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.018674487248063087 norm:0.00037914482527412474 max memory_allocated 29276.33251953125 
[2025-02-19 01:47:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-19 01:47:56 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:48:46 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.02299663797020912 norm:0.0015615805750712752 max memory_allocated 29276.52001953125 
[2025-02-19 01:49:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.022701963782310486 norm:0.0014266357757151127 max memory_allocated 29276.52001953125 
[2025-02-19 01:50:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.02256498485803604 norm:0.0010656578233465552 max memory_allocated 29276.52001953125 
[2025-02-19 01:51:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.02249275892972946 norm:0.001118193264119327 max memory_allocated 29276.52001953125 
[2025-02-19 01:52:05 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.022429732605814934 norm:0.0010636765509843826 max memory_allocated 29276.52001953125 
[2025-02-19 01:52:55 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.02239222265779972 norm:0.0010141240200027823 max memory_allocated 29276.52001953125 
[2025-02-19 01:53:45 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.02234600856900215 norm:0.000922459876164794 max memory_allocated 29276.52001953125 
[2025-02-19 01:54:35 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.022304730489850044 norm:0.0008232287946157157 max memory_allocated 29276.52001953125 
[2025-02-19 01:55:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.02226628176867962 norm:0.0007905432721599936 max memory_allocated 29276.52001953125 
[2025-02-19 01:56:14 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.022258475422859192 norm:0.0007775263511575758 max memory_allocated 29276.52001953125 
[2025-02-19 01:57:04 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.022218478843569756 norm:0.0007595937931910157 max memory_allocated 29276.52001953125 
[2025-02-19 01:57:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.02219393663108349 norm:0.0007169597665779293 max memory_allocated 29276.52001953125 
[2025-02-19 01:58:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.02217360958456993 norm:0.0006976154982112348 max memory_allocated 29276.52001953125 
[2025-02-19 01:59:33 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.022142764180898666 norm:0.0006495715351775289 max memory_allocated 29276.52001953125 
[2025-02-19 02:00:23 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.02214902825653553 norm:0.0006796828820370138 max memory_allocated 29276.52001953125 
[2025-02-19 02:01:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.022127095609903336 norm:0.0006228219135664403 max memory_allocated 29276.52001953125 
[2025-02-19 02:02:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.022135309875011444 norm:0.0006356349913403392 max memory_allocated 29276.52001953125 
[2025-02-19 02:02:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.022139664739370346 norm:0.0006918731378391385 max memory_allocated 29276.52001953125 
[2025-02-19 02:03:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.022359373047947884 norm:0.001091386191546917 max memory_allocated 29276.52001953125 
[2025-02-19 02:04:32 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.02300458401441574 norm:0.0022340903524309397 max memory_allocated 29276.52001953125 
[2025-02-19 02:04:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-19 02:05:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 02:05:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.06443952023983002 norm:0.020514117553830147 max memory_allocated 29276.70751953125 
[2025-02-19 02:06:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.059382643550634384 norm:0.019306592643260956 max memory_allocated 29276.70751953125 
[2025-02-19 02:07:31 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.05826018005609512 norm:0.017239069566130638 max memory_allocated 29276.70751953125 
[2025-02-19 02:08:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.057516343891620636 norm:0.015550775453448296 max memory_allocated 29276.70751953125 
[2025-02-19 02:09:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.05705944448709488 norm:0.014259458519518375 max memory_allocated 29276.70751953125 
[2025-02-19 02:10:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.05682535469532013 norm:0.013897308148443699 max memory_allocated 29276.70751953125 
[2025-02-19 02:10:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.05645090714097023 norm:0.01331911887973547 max memory_allocated 29276.70751953125 
[2025-02-19 02:11:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.0560467466711998 norm:0.012024945579469204 max memory_allocated 29276.70751953125 
[2025-02-19 02:12:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.05601963400840759 norm:0.011822803877294064 max memory_allocated 29276.70751953125 
[2025-02-19 02:13:20 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.05622399225831032 norm:0.012477023527026176 max memory_allocated 29276.70751953125 
[2025-02-19 02:14:09 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.05604797229170799 norm:0.012461239472031593 max memory_allocated 29276.70751953125 
[2025-02-19 02:14:59 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.055891525000333786 norm:0.01218963973224163 max memory_allocated 29276.70751953125 
[2025-02-19 02:15:49 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.055324677377939224 norm:0.011086542159318924 max memory_allocated 29276.70751953125 
[2025-02-19 02:16:39 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.05510473996400833 norm:0.009826886467635632 max memory_allocated 29276.70751953125 
[2025-02-19 02:17:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.05518651008605957 norm:0.00989474169909954 max memory_allocated 29276.70751953125 
[2025-02-19 02:18:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.05523364618420601 norm:0.010178742930293083 max memory_allocated 29276.70751953125 
[2025-02-19 02:19:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.055160507559776306 norm:0.009720887057483196 max memory_allocated 29276.70751953125 
[2025-02-19 02:19:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.055275801569223404 norm:0.010099350474774837 max memory_allocated 29276.70751953125 
[2025-02-19 02:20:48 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.054960571229457855 norm:0.00949445553123951 max memory_allocated 29276.70751953125 
[2025-02-19 02:21:38 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.05501345172524452 norm:0.009475696831941605 max memory_allocated 29276.70751953125 
[2025-02-19 02:21:53 root] (main_calibration.py 365): INFO 40748.02487230301
[2025-02-19 02:23:12 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-19 02:25:09 root] (main_calibration.py 158): INFO wikitext2 : 4.898600101470947
[2025-02-19 02:25:09 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-19 02:28:11 root] (main_calibration.py 158): INFO c4 : 6.487994194030762
[2025-02-19 04:31:31 root] (main_calibration.py 169): INFO {'wikitext2': 4.898600101470947, 'c4': 6.487994194030762, 'results': {'boolq': {'acc': 0.6880733944954128, 'acc_stderr': 0.008102818891778097}, 'arc_easy': {'acc': 0.7314814814814815, 'acc_stderr': 0.009094042554994844, 'acc_norm': 0.5765993265993266, 'acc_norm_stderr': 0.010138671005289058}, 'piqa': {'acc': 0.7905331882480957, 'acc_stderr': 0.009494302979819803, 'acc_norm': 0.7916213275299239, 'acc_norm_stderr': 0.009476125383049462}, 'hellaswag': {'acc': 0.595399322844055, 'acc_stderr': 0.004898115110975033, 'acc_norm': 0.7650866361282613, 'acc_norm_stderr': 0.004230782375004438}, 'arc_challenge': {'acc': 0.4539249146757679, 'acc_stderr': 0.014549221105171864, 'acc_norm': 0.44112627986348124, 'acc_norm_stderr': 0.014509747749064666}, 'winogrande': {'acc': 0.6921862667719021, 'acc_stderr': 0.012972946661205019}}, 'versions': {'boolq': 1, 'arc_easy': 0, 'piqa': 0, 'hellaswag': 0, 'arc_challenge': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
