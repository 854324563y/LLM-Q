[2025-02-18 15:02:32 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation/Llama-2-7b-hf-w8a8', save_dir='./log-calibration-compensation/quant/Llama-2-7b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-18 15:05:08 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 15:05:08 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-18 15:05:09 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 15:05:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 15:05:15 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:05:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0011073396308347583 norm:0.003419907297939062 max memory_allocated 22560.81005859375 
[2025-02-18 15:06:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0004917070618830621 norm:0.000598559039644897 max memory_allocated 22560.81005859375 
[2025-02-18 15:06:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.00040930628892965615 norm:0.000758194481022656 max memory_allocated 22560.81005859375 
[2025-02-18 15:07:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0003584922233130783 norm:0.0008961856365203857 max memory_allocated 22560.81005859375 
[2025-02-18 15:08:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0003227224515285343 norm:0.0008596227853558958 max memory_allocated 22560.81005859375 
[2025-02-18 15:08:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0002982151636388153 norm:0.0008376071345992386 max memory_allocated 22560.81005859375 
[2025-02-18 15:09:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.00027928524650633335 norm:0.0007915623718872666 max memory_allocated 22560.81005859375 
[2025-02-18 15:09:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0002654039126355201 norm:0.0007794926641508937 max memory_allocated 22560.81005859375 
[2025-02-18 15:10:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.00025642954278737307 norm:0.0007707288605161011 max memory_allocated 22560.81005859375 
[2025-02-18 15:10:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00024999797460623085 norm:0.0007921505020931363 max memory_allocated 22560.81005859375 
[2025-02-18 15:11:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.00024283847596962005 norm:0.0007474458543583751 max memory_allocated 22560.81005859375 
[2025-02-18 15:11:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.00023681463790126145 norm:0.0007141000241972506 max memory_allocated 22560.81005859375 
[2025-02-18 15:12:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.00023338021128438413 norm:0.0007151574827730656 max memory_allocated 22560.81005859375 
[2025-02-18 15:13:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.00023058411898091435 norm:0.0007059812778607011 max memory_allocated 22560.81005859375 
[2025-02-18 15:13:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0002264670911245048 norm:0.0006703308317810297 max memory_allocated 22560.81005859375 
[2025-02-18 15:14:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.00022657059889752418 norm:0.0006919170846231282 max memory_allocated 22560.81005859375 
[2025-02-18 15:14:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00022245246509555727 norm:0.0006337128579616547 max memory_allocated 22560.81005859375 
[2025-02-18 15:15:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00022177396749611944 norm:0.0006323760026134551 max memory_allocated 22560.81005859375 
[2025-02-18 15:15:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0002207130310125649 norm:0.0006173864821903408 max memory_allocated 22560.81005859375 
[2025-02-18 15:16:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.00021964374172966927 norm:0.0005974709638394415 max memory_allocated 22560.81005859375 
[2025-02-18 15:16:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 15:16:41 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:17:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.00232752226293087 norm:0.003313066903501749 max memory_allocated 22560.98193359375 
[2025-02-18 15:17:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.00149850407615304 norm:0.0008580113062635064 max memory_allocated 22560.98193359375 
[2025-02-18 15:18:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0013045801315456629 norm:0.000948459783103317 max memory_allocated 22560.98193359375 
[2025-02-18 15:18:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0012291495222598314 norm:0.00105013488791883 max memory_allocated 22560.98193359375 
[2025-02-18 15:19:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0011409244034439325 norm:0.001051583793014288 max memory_allocated 22560.98193359375 
[2025-02-18 15:20:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0010548753198236227 norm:0.0010744446190074086 max memory_allocated 22560.98193359375 
[2025-02-18 15:20:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.001015429850667715 norm:0.0011427933350205421 max memory_allocated 22560.98193359375 
[2025-02-18 15:21:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0009902114979922771 norm:0.0011577820405364037 max memory_allocated 22560.98193359375 
[2025-02-18 15:21:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0009745312854647636 norm:0.001160118728876114 max memory_allocated 22560.98193359375 
[2025-02-18 15:22:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0009587819804437459 norm:0.0011619140859693289 max memory_allocated 22560.98193359375 
[2025-02-18 15:22:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0009509666124358773 norm:0.0013377422001212835 max memory_allocated 22560.98193359375 
[2025-02-18 15:23:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0009451730875298381 norm:0.0013150513404980302 max memory_allocated 22560.98193359375 
[2025-02-18 15:24:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0009309626184403896 norm:0.0012307276483625174 max memory_allocated 22560.98193359375 
[2025-02-18 15:24:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0009304492850787938 norm:0.0012271319283172488 max memory_allocated 22560.98193359375 
[2025-02-18 15:25:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0009240234503522515 norm:0.0012307666474953294 max memory_allocated 22560.98193359375 
[2025-02-18 15:25:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0009109015227295458 norm:0.0012089626397937536 max memory_allocated 22560.98193359375 
[2025-02-18 15:26:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0009041220764629543 norm:0.0010742184240370989 max memory_allocated 22560.98193359375 
[2025-02-18 15:26:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0008965581655502319 norm:0.0009771824115887284 max memory_allocated 22560.98193359375 
[2025-02-18 15:27:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0008830137667246163 norm:0.0009059211006388068 max memory_allocated 22560.98193359375 
[2025-02-18 15:27:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0008836677297949791 norm:0.0008476388175040483 max memory_allocated 22560.98193359375 
[2025-02-18 15:28:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 15:28:10 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:28:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0021939603611826897 norm:0.0040673743933439255 max memory_allocated 22561.15380859375 
[2025-02-18 15:29:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0014607318444177508 norm:0.0027431228663772345 max memory_allocated 22561.15380859375 
[2025-02-18 15:29:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0012550886021927 norm:0.0018797209486365318 max memory_allocated 22561.15380859375 
[2025-02-18 15:30:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.001159164821729064 norm:0.0014931079931557178 max memory_allocated 22561.15380859375 
[2025-02-18 15:30:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0010972314048558474 norm:0.0012122243642807007 max memory_allocated 22561.15380859375 
[2025-02-18 15:31:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0010618993546813726 norm:0.0010501033393666148 max memory_allocated 22561.15380859375 
[2025-02-18 15:32:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0010411117691546679 norm:0.0009265769040212035 max memory_allocated 22561.15380859375 
[2025-02-18 15:32:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.001029931241646409 norm:0.0008776418981142342 max memory_allocated 22561.15380859375 
[2025-02-18 15:33:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.001020818599499762 norm:0.0008184702601283789 max memory_allocated 22561.15380859375 
[2025-02-18 15:33:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0010148740839213133 norm:0.0007728474447503686 max memory_allocated 22561.15380859375 
[2025-02-18 15:34:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0010076008038595319 norm:0.0007135391351766884 max memory_allocated 22561.15380859375 
[2025-02-18 15:34:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.001004295190796256 norm:0.0006741859833709896 max memory_allocated 22561.15380859375 
[2025-02-18 15:35:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0010002409107983112 norm:0.0006500240415334702 max memory_allocated 22561.15380859375 
[2025-02-18 15:36:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0009979812894016504 norm:0.0006279642111621797 max memory_allocated 22561.15380859375 
[2025-02-18 15:36:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0009990825783461332 norm:0.0005996344843879342 max memory_allocated 22561.15380859375 
[2025-02-18 15:37:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0009983533527702093 norm:0.0005652663530781865 max memory_allocated 22561.15380859375 
[2025-02-18 15:37:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0009942606557160616 norm:0.0005252405535429716 max memory_allocated 22561.15380859375 
[2025-02-18 15:38:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0009932746179401875 norm:0.0004884309018962085 max memory_allocated 22561.15380859375 
[2025-02-18 15:38:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.000993484864011407 norm:0.00045603481703437865 max memory_allocated 22561.15380859375 
[2025-02-18 15:39:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0009919911390170455 norm:0.00040614939644001424 max memory_allocated 22561.15380859375 
[2025-02-18 15:39:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 15:40:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0021244329400360584 norm:0.0007152569596655667 max memory_allocated 22561.21044921875 
[2025-02-18 15:40:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0016311535146087408 norm:0.0002994088572449982 max memory_allocated 22561.21044921875 
[2025-02-18 15:41:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0014585049357265234 norm:0.00018475700926501304 max memory_allocated 22561.21044921875 
[2025-02-18 15:41:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0013669482432305813 norm:0.00012623198563233018 max memory_allocated 22561.21044921875 
[2025-02-18 15:42:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.0013174063060432673 norm:8.963728760136291e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:43:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0012883299496024847 norm:6.483319157268852e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:43:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.00127219851128757 norm:4.794667984242551e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:44:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0012652913574129343 norm:3.549050961737521e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:44:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0012621681671589613 norm:2.643912739586085e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:45:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.001259713084436953 norm:1.98017278307816e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:45:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0012591523118317127 norm:1.6330057405866683e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:46:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0012587076053023338 norm:1.3471436432155315e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:46:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0012573441490530968 norm:1.2117983715143055e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:47:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0012551203835755587 norm:1.1667851140373386e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:48:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0012527292128652334 norm:1.1305048246867955e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:48:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.001252589514479041 norm:1.1056934454245493e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:49:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0012524264166131616 norm:1.098336997529259e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:49:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0012512576067820191 norm:1.1072144843637943e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:50:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0012504219776019454 norm:1.1030298082914669e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:50:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0012483568862080574 norm:1.0977984857163392e-05 max memory_allocated 22561.21044921875 
[2025-02-18 15:51:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 15:51:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.002115468494594097 norm:0.00041925342520698905 max memory_allocated 22561.38232421875 
[2025-02-18 15:52:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.001714055659249425 norm:0.00018201350758317858 max memory_allocated 22561.38232421875 
[2025-02-18 15:52:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0015862854197621346 norm:0.0001121452369261533 max memory_allocated 22561.38232421875 
[2025-02-18 15:53:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0015283703105524182 norm:7.174319034675136e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:53:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0014975739177316427 norm:5.150819561094977e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:54:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0014767375541850924 norm:3.592070788727142e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:55:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.001467130146920681 norm:2.6747784431790933e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:55:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0014605920296162367 norm:2.137385672540404e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:56:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0014574653469026089 norm:1.7582067812327296e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:56:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.001453476375900209 norm:1.5269150026142597e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:57:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0014502262929454446 norm:1.3701722309633624e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:57:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0014499183744192123 norm:1.2576759218063671e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:58:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0014493117341771722 norm:1.17019717436051e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:58:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.001445900765247643 norm:1.1604018254729453e-05 max memory_allocated 22561.38232421875 
[2025-02-18 15:59:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0014474231284111738 norm:1.1490550605230965e-05 max memory_allocated 22561.38232421875 
[2025-02-18 16:00:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0014446009881794453 norm:1.1336021998431534e-05 max memory_allocated 22561.38232421875 
[2025-02-18 16:00:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0014430128503590822 norm:1.1192989404662512e-05 max memory_allocated 22561.38232421875 
[2025-02-18 16:01:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0014419027138501406 norm:1.1146360520797316e-05 max memory_allocated 22561.38232421875 
[2025-02-18 16:01:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0014410072471946478 norm:1.1074516805820167e-05 max memory_allocated 22561.38232421875 
[2025-02-18 16:02:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0014408041024580598 norm:1.094470098905731e-05 max memory_allocated 22561.38232421875 
[2025-02-18 16:02:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 16:03:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.001963219838216901 norm:0.0002557290717959404 max memory_allocated 22561.55419921875 
[2025-02-18 16:03:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.00169748580083251 norm:0.00011230868403799832 max memory_allocated 22561.55419921875 
[2025-02-18 16:04:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0016044838121160865 norm:7.026684761513025e-05 max memory_allocated 22561.55419921875 
[2025-02-18 16:04:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0015615645097568631 norm:4.5810334995621815e-05 max memory_allocated 22561.55419921875 
[2025-02-18 16:05:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0015433714725077152 norm:3.160156120429747e-05 max memory_allocated 22561.55419921875 
[2025-02-18 16:05:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0015312969917431474 norm:2.1668363842763938e-05 max memory_allocated 22561.55419921875 
[2025-02-18 16:06:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0015221734065562487 norm:1.5787398297106847e-05 max memory_allocated 22561.55419921875 
[2025-02-18 16:07:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0015160992043092847 norm:1.252115544048138e-05 max memory_allocated 22561.55419921875 
[2025-02-18 16:07:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0015142704360187054 norm:1.0836176443262957e-05 max memory_allocated 22561.55419921875 
[2025-02-18 16:08:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0015126937068998814 norm:9.7793872555485e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:08:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.001510498346760869 norm:9.2810178102809e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:09:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.001507888431660831 norm:8.991653885459527e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:09:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0015070268418639898 norm:8.723332030058373e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:10:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0015068516368046403 norm:8.67123162606731e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:10:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0015035623218864202 norm:8.621765118732583e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:11:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0015038666315376759 norm:8.535452252544928e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:12:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0015041731530800462 norm:8.48755007609725e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:12:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.001502555562183261 norm:8.487574632454198e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:13:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0015029269270598888 norm:8.478169547743164e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:13:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0015021703438833356 norm:8.446234460279811e-06 max memory_allocated 22561.55419921875 
[2025-02-18 16:13:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 16:14:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0020650236401706934 norm:0.00020281804609112442 max memory_allocated 22561.72607421875 
[2025-02-18 16:15:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0018232272705063224 norm:8.84981345734559e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:15:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.001735061639919877 norm:5.587824125541374e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:16:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0016947091789916158 norm:3.989580363850109e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:16:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0016721922438591719 norm:2.907626549131237e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:17:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0016552910674363375 norm:2.1561727407970466e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:17:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.001645949319936335 norm:1.7128231775132008e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:18:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0016425587236881256 norm:1.3697052963834722e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:19:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0016399134183302522 norm:1.1354264643159695e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:19:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0016353507526218891 norm:1.0460857993166428e-05 max memory_allocated 22561.72607421875 
[2025-02-18 16:20:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0016335849650204182 norm:9.618596777727362e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:20:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0016331461956724524 norm:8.94086952030193e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:21:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.001629891456104815 norm:8.796398105914705e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:21:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0016276785172522068 norm:8.650497875350993e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:22:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0016267015598714352 norm:8.648780749354046e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:22:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.001625405391678214 norm:8.393702955800109e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:23:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0016259360127151012 norm:8.420465746894479e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:24:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.001625441131182015 norm:9.064631740329787e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:24:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0016251315828412771 norm:8.564426025259309e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:25:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.001624956727027893 norm:8.483692909067031e-06 max memory_allocated 22561.72607421875 
[2025-02-18 16:25:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 16:25:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0022113281302154064 norm:0.00024304960970766842 max memory_allocated 22561.89794921875 
[2025-02-18 16:26:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0019413736881688237 norm:0.00010130800365004689 max memory_allocated 22561.89794921875 
[2025-02-18 16:27:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.001852288143709302 norm:6.656740151811391e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:27:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0018065060721710324 norm:4.690153946285136e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:28:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0017778389155864716 norm:3.439314241404645e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:28:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.001763214822858572 norm:2.7251664505456574e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:29:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0017517085652798414 norm:2.204976226494182e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:29:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.001744976849295199 norm:1.8110355085809715e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:30:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.001737214159220457 norm:1.4779915545659605e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:31:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0017323889769613743 norm:1.2304460142331664e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:31:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0017295496072620153 norm:1.0046048373624217e-05 max memory_allocated 22561.89794921875 
[2025-02-18 16:32:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0017280357424169779 norm:9.451754522160627e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:32:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0017255558632314205 norm:8.580918802181259e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:33:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0017259794985875487 norm:7.954285138112027e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:33:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.001726042479276657 norm:7.626497335877502e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:34:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0017250906676054 norm:7.6479136623675e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:34:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0017238399013876915 norm:7.534456926805433e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:35:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0017217968124896288 norm:7.381079285551095e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:36:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0017214595573022962 norm:7.2915390774142e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:36:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.001722340122796595 norm:7.298753189388663e-06 max memory_allocated 22561.89794921875 
[2025-02-18 16:36:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 16:37:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.002060484839603305 norm:7.681697024963796e-05 max memory_allocated 22562.06982421875 
[2025-02-18 16:37:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0019232816994190216 norm:3.6515277315629646e-05 max memory_allocated 22562.06982421875 
[2025-02-18 16:38:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.001873847097158432 norm:2.2272870410233736e-05 max memory_allocated 22562.06982421875 
[2025-02-18 16:39:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0018477047560736537 norm:1.5674171663704328e-05 max memory_allocated 22562.06982421875 
[2025-02-18 16:39:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0018322488758713007 norm:1.1829160939669237e-05 max memory_allocated 22562.06982421875 
[2025-02-18 16:40:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.001823311671614647 norm:9.581087397236843e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:40:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.001816153060644865 norm:8.314209480886348e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:41:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0018137850565835834 norm:7.56028430259903e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:41:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.001811600523069501 norm:7.094495231285691e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:42:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.001809971872717142 norm:6.842661150585627e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:43:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0018075838452205062 norm:6.849209512438392e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:43:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0018058930290862918 norm:6.6114325818489306e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:44:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0018048608908429742 norm:6.537806712003658e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:44:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0018043569289147854 norm:6.549332738359226e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:45:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0018050470389425755 norm:6.564440809597727e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:45:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0018048789352178574 norm:6.58971248412854e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:46:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0018043741583824158 norm:6.616220161959063e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:46:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0018029424827545881 norm:6.649730494245887e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:47:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0018020720453932881 norm:6.680577826045919e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:48:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0018013687804341316 norm:6.575473889824934e-06 max memory_allocated 22562.06982421875 
[2025-02-18 16:48:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 16:48:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.0023510544560849667 norm:0.00023766998492646962 max memory_allocated 22562.24169921875 
[2025-02-18 16:49:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.002068071160465479 norm:8.425983833149076e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:49:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0020008229184895754 norm:5.722113564843312e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:50:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0019640768878161907 norm:4.4481803342932835e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:51:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.001941229566000402 norm:3.2580570405116305e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:51:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.001922959927469492 norm:2.550190401962027e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:52:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0019151100423187017 norm:2.1478648704942316e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:52:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0019095172174274921 norm:1.8568685845821165e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:53:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0019045835360884666 norm:1.6344565665349364e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:53:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0018998405430465937 norm:1.4714888493472245e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:54:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0018967724172398448 norm:1.3102646335028112e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:55:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0018940068548545241 norm:1.1220910892006941e-05 max memory_allocated 22562.24169921875 
[2025-02-18 16:55:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0018927180208265781 norm:9.941352800524328e-06 max memory_allocated 22562.24169921875 
[2025-02-18 16:56:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0018897497793659568 norm:9.216872058459558e-06 max memory_allocated 22562.24169921875 
[2025-02-18 16:56:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0018881182186305523 norm:8.36054914543638e-06 max memory_allocated 22562.24169921875 
[2025-02-18 16:57:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0018890723586082458 norm:7.585167168144835e-06 max memory_allocated 22562.24169921875 
[2025-02-18 16:57:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0018859975971281528 norm:6.648092949035345e-06 max memory_allocated 22562.24169921875 
[2025-02-18 16:58:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.001885248115286231 norm:6.412451057258295e-06 max memory_allocated 22562.24169921875 
[2025-02-18 16:58:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0018858093535527587 norm:5.7852275858749636e-06 max memory_allocated 22562.24169921875 
[2025-02-18 16:59:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.001886833691969514 norm:5.68833229408483e-06 max memory_allocated 22562.24169921875 
[2025-02-18 16:59:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 17:00:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.002232495229691267 norm:0.00013834162382408977 max memory_allocated 22562.41357421875 
[2025-02-18 17:00:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0020800286438316107 norm:6.341266998788342e-05 max memory_allocated 22562.41357421875 
[2025-02-18 17:01:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0020216333214193583 norm:3.9789050788385794e-05 max memory_allocated 22562.41357421875 
[2025-02-18 17:01:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.001996608218178153 norm:2.810219484672416e-05 max memory_allocated 22562.41357421875 
[2025-02-18 17:02:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0019803200848400593 norm:2.133392081304919e-05 max memory_allocated 22562.41357421875 
[2025-02-18 17:03:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0019703989382833242 norm:1.6614110791124403e-05 max memory_allocated 22562.41357421875 
[2025-02-18 17:03:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0019652005285024643 norm:1.3169035810278729e-05 max memory_allocated 22562.41357421875 
[2025-02-18 17:04:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.00195873249322176 norm:1.0830426617758349e-05 max memory_allocated 22562.41357421875 
[2025-02-18 17:04:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0019541610963642597 norm:9.138225323113147e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:05:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.001953225117176771 norm:7.836649274395313e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:05:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0019517024047672749 norm:7.155009370762855e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:06:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0019502026261761785 norm:6.43264502286911e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:07:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.001949420548044145 norm:6.02791124038049e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:07:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0019504778319969773 norm:5.802914074592991e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:08:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0019506231183186173 norm:5.68623408980784e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:08:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0019502961076796055 norm:5.602121746051125e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:09:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0019474215805530548 norm:5.552404218178708e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:09:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0019468874670565128 norm:5.484175289893756e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:10:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0019479690818116069 norm:5.4482043196912855e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:10:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0019468663958832622 norm:5.480557319970103e-06 max memory_allocated 22562.41357421875 
[2025-02-18 17:11:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 17:11:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0022416217252612114 norm:7.540482329204679e-05 max memory_allocated 22562.58544921875 
[2025-02-18 17:12:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.002124042483046651 norm:3.9303624362219125e-05 max memory_allocated 22562.58544921875 
[2025-02-18 17:12:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0020748849492520094 norm:2.5952949727070518e-05 max memory_allocated 22562.58544921875 
[2025-02-18 17:13:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.002048714552074671 norm:1.906035868159961e-05 max memory_allocated 22562.58544921875 
[2025-02-18 17:13:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.002036779886111617 norm:1.5245967006194405e-05 max memory_allocated 22562.58544921875 
[2025-02-18 17:14:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002025596797466278 norm:1.2240943760843948e-05 max memory_allocated 22562.58544921875 
[2025-02-18 17:15:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0020221006125211716 norm:1.0500680218683556e-05 max memory_allocated 22562.58544921875 
[2025-02-18 17:15:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.002016752026975155 norm:8.911460099625401e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:16:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.002012748969718814 norm:7.729681783530395e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:16:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.002011349657550454 norm:7.050489784887759e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:17:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0020087070297449827 norm:6.347140697471332e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:17:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0020059901289641857 norm:5.96781455897144e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:18:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0020060280803591013 norm:5.7679840210767e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:19:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.002006370574235916 norm:5.627725840895437e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:19:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.002005512360483408 norm:5.475226316775661e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:20:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0020047009456902742 norm:5.439445885713212e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:20:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0020041747484356165 norm:5.32970580024994e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:21:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.002004465088248253 norm:5.30237912244047e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:21:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0020032087340950966 norm:5.2213931667211e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:22:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.002003145171329379 norm:5.268888799037086e-06 max memory_allocated 22562.58544921875 
[2025-02-18 17:22:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 17:23:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0022676456719636917 norm:5.27736046933569e-05 max memory_allocated 22562.75732421875 
[2025-02-18 17:23:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.002177250338718295 norm:2.9247134079923853e-05 max memory_allocated 22562.75732421875 
[2025-02-18 17:24:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0021335210185498 norm:1.9263094145571813e-05 max memory_allocated 22562.75732421875 
[2025-02-18 17:24:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0021142344921827316 norm:1.4873394320602529e-05 max memory_allocated 22562.75732421875 
[2025-02-18 17:25:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0021027703769505024 norm:1.1664364137686789e-05 max memory_allocated 22562.75732421875 
[2025-02-18 17:25:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.0020957726519554853 norm:9.683380085334647e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:26:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0020911500323563814 norm:7.888402251410298e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:27:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.002086940221488476 norm:6.924297849764116e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:27:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0020846158731728792 norm:6.2477220126311295e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:28:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.002082596067339182 norm:5.716843588743359e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:28:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.002082876395434141 norm:5.472410521178972e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:29:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0020832254085689783 norm:5.247454737400403e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:29:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0020816063042730093 norm:5.099694135424215e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:30:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0020810705609619617 norm:5.002281795896124e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:31:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.0020813066512346268 norm:4.960540081810905e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:31:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0020818186458200216 norm:4.901078682451043e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:32:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.002081099897623062 norm:4.854202416026965e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:32:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0020804041996598244 norm:4.809669462702004e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:33:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0020811643917113543 norm:4.809353868040489e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:33:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0020799466874450445 norm:4.745534170069732e-06 max memory_allocated 22562.75732421875 
[2025-02-18 17:34:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 17:34:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.002420495031401515 norm:0.00012912585225421935 max memory_allocated 22562.92919921875 
[2025-02-18 17:35:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0022668091114610434 norm:6.283562106546015e-05 max memory_allocated 22562.92919921875 
[2025-02-18 17:35:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.002201838418841362 norm:3.99508498958312e-05 max memory_allocated 22562.92919921875 
[2025-02-18 17:36:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0021676518954336643 norm:2.8114667657064274e-05 max memory_allocated 22562.92919921875 
[2025-02-18 17:36:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.0021486948244273663 norm:2.163763747375924e-05 max memory_allocated 22562.92919921875 
[2025-02-18 17:37:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.002134796464815736 norm:1.7328013200312853e-05 max memory_allocated 22562.92919921875 
[2025-02-18 17:38:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.0021257339976727962 norm:1.438077015336603e-05 max memory_allocated 22562.92919921875 
[2025-02-18 17:38:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.002120013814419508 norm:1.2413387594278902e-05 max memory_allocated 22562.92919921875 
[2025-02-18 17:39:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.002113365801051259 norm:1.0240491064905655e-05 max memory_allocated 22562.92919921875 
[2025-02-18 17:39:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.002109572058543563 norm:8.798065209703054e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:40:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.002108289860188961 norm:7.4892827797157224e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:40:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.002106599509716034 norm:6.915389803907601e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:41:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.002105433726683259 norm:6.2788071772956755e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:41:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.002103896113112569 norm:5.699931534763891e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:42:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0021037356927990913 norm:5.402954229793977e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:43:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.0021042050793766975 norm:5.058966053184122e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:43:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.002106007421389222 norm:4.907806214760058e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:44:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.002104787388816476 norm:4.832759714190615e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:44:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0021045696921646595 norm:4.767474820255302e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:45:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0021044635213911533 norm:4.677606739278417e-06 max memory_allocated 22562.92919921875 
[2025-02-18 17:45:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 17:46:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.0023955218493938446 norm:5.578431591857225e-05 max memory_allocated 22563.10107421875 
[2025-02-18 17:46:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0023039490915834904 norm:2.914059950853698e-05 max memory_allocated 22563.10107421875 
[2025-02-18 17:47:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0022639241069555283 norm:2.0355764718260616e-05 max memory_allocated 22563.10107421875 
[2025-02-18 17:47:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.002238274086266756 norm:1.5143690689001232e-05 max memory_allocated 22563.10107421875 
[2025-02-18 17:48:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.002222974319010973 norm:1.2028405762976035e-05 max memory_allocated 22563.10107421875 
[2025-02-18 17:48:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.002214128850027919 norm:1.0370057680120226e-05 max memory_allocated 22563.10107421875 
[2025-02-18 17:49:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0022069388069212437 norm:8.602693924331106e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:50:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.002201777184382081 norm:7.540137630712707e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:50:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0021980307064950466 norm:6.860522262286395e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:51:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.002194362925365567 norm:6.15754379396094e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:51:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0021919067949056625 norm:5.770853931608144e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:52:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.002190634375438094 norm:5.484188932314282e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:52:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.0021904930472373962 norm:5.3023386499262415e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:53:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.002190184546634555 norm:5.15206238560495e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:53:56 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0021891025826334953 norm:5.01337581226835e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:54:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.0021885118912905455 norm:4.9489735829411075e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:55:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0021875968668609858 norm:4.930109298584284e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:55:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.0021872431971132755 norm:4.867955794907175e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:56:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0021867549512535334 norm:4.809963684238028e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:56:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.0021859006956219673 norm:4.778035872732289e-06 max memory_allocated 22563.10107421875 
[2025-02-18 17:56:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 17:57:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.0028425143100321293 norm:0.0002468633756507188 max memory_allocated 22563.27294921875 
[2025-02-18 17:58:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.0024629675317555666 norm:6.892918463563547e-05 max memory_allocated 22563.27294921875 
[2025-02-18 17:58:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0023946589790284634 norm:4.956678822054528e-05 max memory_allocated 22563.27294921875 
[2025-02-18 17:59:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.002345141489058733 norm:3.707505311467685e-05 max memory_allocated 22563.27294921875 
[2025-02-18 17:59:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0023150905035436153 norm:2.7276910259388387e-05 max memory_allocated 22563.27294921875 
[2025-02-18 18:00:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.002296335296705365 norm:2.2371281374944374e-05 max memory_allocated 22563.27294921875 
[2025-02-18 18:00:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0022811212111264467 norm:1.8489419744582847e-05 max memory_allocated 22563.27294921875 
[2025-02-18 18:01:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.002270027296617627 norm:1.554628943267744e-05 max memory_allocated 22563.27294921875 
[2025-02-18 18:02:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.002262030728161335 norm:1.352670824417146e-05 max memory_allocated 22563.27294921875 
[2025-02-18 18:02:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0022557873744517565 norm:1.1921310033358168e-05 max memory_allocated 22563.27294921875 
[2025-02-18 18:03:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0022531638387590647 norm:1.0803163604578003e-05 max memory_allocated 22563.27294921875 
[2025-02-18 18:03:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0022499356418848038 norm:9.60162105911877e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:04:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0022474140860140324 norm:8.631589480501134e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:04:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0022458427120000124 norm:7.799972991051618e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:05:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.002243214286863804 norm:7.108556019375101e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:05:57 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.002241841284558177 norm:6.462246346927714e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:06:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.002242365153506398 norm:6.088044756324962e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:07:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.00224180705845356 norm:5.750351192546077e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:07:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.002240423345938325 norm:5.350299943529535e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:08:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.0022403779439628124 norm:5.070151473773876e-06 max memory_allocated 22563.27294921875 
[2025-02-18 18:08:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 18:08:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.003136186394840479 norm:0.0003976609732490033 max memory_allocated 22563.44482421875 
[2025-02-18 18:09:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.002657088451087475 norm:0.00010413828567834571 max memory_allocated 22563.44482421875 
[2025-02-18 18:10:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.002558185253292322 norm:6.769164610886946e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:10:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.0025070130359381437 norm:5.164093818166293e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:11:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0024661815259605646 norm:3.897686838172376e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:11:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0024406646843999624 norm:3.192143049091101e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:12:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.0024205173831433058 norm:2.6649082428775728e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:12:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0024062131997197866 norm:2.3024513211566955e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:13:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.002396556083112955 norm:2.0340754417702556e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:14:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.002387317130342126 norm:1.820205943658948e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:14:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.002380537334829569 norm:1.671950849413406e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:15:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.002376096323132515 norm:1.516267548140604e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:15:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.002371103735640645 norm:1.3945630598755088e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:16:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0023656990379095078 norm:1.2267999409232289e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:16:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.0023632748052477837 norm:1.1139176422148012e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:17:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.002361926017329097 norm:1.017343583953334e-05 max memory_allocated 22563.44482421875 
[2025-02-18 18:17:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.0023602719884365797 norm:9.570719157636631e-06 max memory_allocated 22563.44482421875 
[2025-02-18 18:18:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.002358534839004278 norm:9.040817531058565e-06 max memory_allocated 22563.44482421875 
[2025-02-18 18:19:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0023571806959807873 norm:8.511340638506226e-06 max memory_allocated 22563.44482421875 
[2025-02-18 18:19:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.0023568037431687117 norm:8.020086170290597e-06 max memory_allocated 22563.44482421875 
[2025-02-18 18:19:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 18:20:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.003197833662852645 norm:0.0004031290882267058 max memory_allocated 22563.61669921875 
[2025-02-18 18:20:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0027250556740909815 norm:0.00010073680459754542 max memory_allocated 22563.61669921875 
[2025-02-18 18:21:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.002640918828547001 norm:6.479777221102268e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:22:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.0026037965435534716 norm:5.095067899674177e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:22:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0025776627007871866 norm:4.1237391997128725e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:23:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.002553105354309082 norm:3.2198844564845785e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:23:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.002538608154281974 norm:2.6825109671335667e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:24:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.002528830897063017 norm:2.3579343178425916e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:24:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0025214191991835833 norm:2.096389653161168e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:25:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.0025132198352366686 norm:1.826269544835668e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:26:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.0025091739371418953 norm:1.6638934539514594e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:26:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.002503720810636878 norm:1.504108604422072e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:27:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.0025001238100230694 norm:1.4019751688465476e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:27:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.0024965452030301094 norm:1.268288633582415e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:28:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.002495061606168747 norm:1.1531819836818613e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:28:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.002493277657777071 norm:1.0621783076203428e-05 max memory_allocated 22563.61669921875 
[2025-02-18 18:29:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.002491499064490199 norm:9.850548849499319e-06 max memory_allocated 22563.61669921875 
[2025-02-18 18:29:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0024900962598621845 norm:9.068719009519555e-06 max memory_allocated 22563.61669921875 
[2025-02-18 18:30:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.0024892378132790327 norm:8.71193697093986e-06 max memory_allocated 22563.61669921875 
[2025-02-18 18:31:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.00248758215457201 norm:8.171853551175445e-06 max memory_allocated 22563.61669921875 
[2025-02-18 18:31:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 18:31:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.003575712675228715 norm:0.0003996361047029495 max memory_allocated 22563.78857421875 
[2025-02-18 18:32:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.003060439135879278 norm:0.00011663244367809966 max memory_allocated 22563.78857421875 
[2025-02-18 18:32:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.0029681066516786814 norm:8.29985729069449e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:33:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.0029161041602492332 norm:6.635777390329167e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:34:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.002865399466827512 norm:4.9213522288482636e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:34:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.0028348080813884735 norm:3.9735736208967865e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:35:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.0028120509814471006 norm:3.4306351153645664e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:35:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.002797963796183467 norm:2.9108203307259828e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:36:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.0027869446203112602 norm:2.4900122298276983e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:36:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.0027772090397775173 norm:2.1148714949958958e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:37:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.0027704210951924324 norm:1.7937471056939103e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:38:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.002766628051176667 norm:1.5447139958268963e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:38:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.0027620356995612383 norm:1.3539874089474324e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:39:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.0027595695573836565 norm:1.2049233191646636e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:39:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.0027580056339502335 norm:1.1452337275841273e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:40:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.002755803056061268 norm:1.024207358568674e-05 max memory_allocated 22563.78857421875 
[2025-02-18 18:40:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.002753255423158407 norm:9.40961399464868e-06 max memory_allocated 22563.78857421875 
[2025-02-18 18:41:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.0027529429644346237 norm:8.631316632090602e-06 max memory_allocated 22563.78857421875 
[2025-02-18 18:41:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.002752474509179592 norm:8.333528057846706e-06 max memory_allocated 22563.78857421875 
[2025-02-18 18:42:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.002749913139268756 norm:8.02204704086762e-06 max memory_allocated 22563.78857421875 
[2025-02-18 18:42:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 18:43:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.0035011691506952047 norm:0.00022915539739187807 max memory_allocated 22563.96044921875 
[2025-02-18 18:43:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.0032229458447545767 norm:8.964035077951849e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:44:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.0031597930938005447 norm:6.619538180530071e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:44:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.003112198319286108 norm:4.876271486864425e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:45:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.0030779759399592876 norm:3.640613067545928e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:46:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.0030583261977881193 norm:2.9030810765107162e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:46:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.003041164018213749 norm:2.398782453383319e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:47:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0030296931508928537 norm:1.9933469957322814e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:47:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.003024045843631029 norm:1.6791242160252295e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:48:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.00301884557120502 norm:1.4238240510167088e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:48:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.003013228066265583 norm:1.2504470760177355e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:49:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.0030100159347057343 norm:1.0918809493887238e-05 max memory_allocated 22563.96044921875 
[2025-02-18 18:50:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.003007748629897833 norm:9.75432249106234e-06 max memory_allocated 22563.96044921875 
[2025-02-18 18:50:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.003005667356774211 norm:8.695727956364863e-06 max memory_allocated 22563.96044921875 
[2025-02-18 18:51:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.003003173740580678 norm:7.984239346114919e-06 max memory_allocated 22563.96044921875 
[2025-02-18 18:51:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.003002013312652707 norm:7.3557330324547365e-06 max memory_allocated 22563.96044921875 
[2025-02-18 18:52:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.0030015259981155396 norm:6.851448233646806e-06 max memory_allocated 22563.96044921875 
[2025-02-18 18:52:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.0030007557943463326 norm:6.547906650666846e-06 max memory_allocated 22563.96044921875 
[2025-02-18 18:53:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.003000017488375306 norm:6.257728728087386e-06 max memory_allocated 22563.96044921875 
[2025-02-18 18:53:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0029989092145115137 norm:6.086636403779266e-06 max memory_allocated 22563.96044921875 
[2025-02-18 18:54:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 18:54:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.004484639037400484 norm:0.0006635095924139023 max memory_allocated 22564.13232421875 
[2025-02-18 18:55:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.003773047123104334 norm:0.00018614839063957334 max memory_allocated 22564.13232421875 
[2025-02-18 18:55:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.0035951838362962008 norm:0.00010103816021000966 max memory_allocated 22564.13232421875 
[2025-02-18 18:56:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.0035369584802538157 norm:7.903757796157151e-05 max memory_allocated 22564.13232421875 
[2025-02-18 18:56:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.0035006790421903133 norm:6.716879579471424e-05 max memory_allocated 22564.13232421875 
[2025-02-18 18:57:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0034642480313777924 norm:5.4323441872838885e-05 max memory_allocated 22564.13232421875 
[2025-02-18 18:58:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.0034350419882684946 norm:4.297214400139637e-05 max memory_allocated 22564.13232421875 
[2025-02-18 18:58:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.0034145123790949583 norm:3.590355481719598e-05 max memory_allocated 22564.13232421875 
[2025-02-18 18:59:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.003401267109438777 norm:3.0245249945437536e-05 max memory_allocated 22564.13232421875 
[2025-02-18 18:59:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.0033913219813257456 norm:2.673398921615444e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:00:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.0033816765062510967 norm:2.3785174562362954e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:00:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.0033744038082659245 norm:2.101876089000143e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:01:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.0033688130788505077 norm:1.910276296257507e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:02:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.0033623315393924713 norm:1.75755558302626e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:02:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.003358580870553851 norm:1.597454320290126e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:03:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.003355824388563633 norm:1.4191279660735745e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:03:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.0033524164464324713 norm:1.3089328604110051e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:04:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0033497789409011602 norm:1.201954910357017e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:04:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.0033464492298662663 norm:1.097007316275267e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:05:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.0033461013808846474 norm:1.0314560313418042e-05 max memory_allocated 22564.13232421875 
[2025-02-18 19:05:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 19:06:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.004443746991455555 norm:0.0004454201553016901 max memory_allocated 22564.30419921875 
[2025-02-18 19:06:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.003993115853518248 norm:0.0001364047930110246 max memory_allocated 22564.30419921875 
[2025-02-18 19:07:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.0038972627371549606 norm:8.670345414429903e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:07:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.003858141601085663 norm:6.88898580847308e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:08:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.003825933439657092 norm:5.4864995036041364e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:09:00 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.003799628233537078 norm:4.371170507511124e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:09:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.003779686987400055 norm:3.976487278123386e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:10:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.003764983732253313 norm:3.1068295356817544e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:10:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.0037531277630478144 norm:2.7297061024000868e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:11:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.0037444273475557566 norm:2.4294931790791452e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:11:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.0037360130809247494 norm:2.161290103686042e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:12:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.003729557851329446 norm:1.9424893253017217e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:12:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.0037242949474602938 norm:1.7409558495273814e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:13:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.003720503766089678 norm:1.5580344552290626e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:14:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.0037175393663346767 norm:1.4195520634530112e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:14:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.003716531442478299 norm:1.3326595762919169e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:15:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.003713741432875395 norm:1.2099579180357978e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:15:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.003710002638399601 norm:1.1049580280086957e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:16:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.0037086207885295153 norm:1.0146530257770792e-05 max memory_allocated 22564.30419921875 
[2025-02-18 19:16:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.003707114839926362 norm:9.330022294307128e-06 max memory_allocated 22564.30419921875 
[2025-02-18 19:17:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 19:17:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.0052131530828773975 norm:0.0006540664471685886 max memory_allocated 22564.47607421875 
[2025-02-18 19:18:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.004481706768274307 norm:0.0001605733559699729 max memory_allocated 22564.47607421875 
[2025-02-18 19:18:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.004352865740656853 norm:9.181098721455783e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:19:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.004319687839597464 norm:7.926192483864725e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:19:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.0042972806841135025 norm:7.008228567428887e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:20:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.004269691184163094 norm:5.5343389249173924e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:21:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.004243990406394005 norm:4.340065424912609e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:21:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.0042312899604439735 norm:3.6501503927866e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:22:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.004223307128995657 norm:3.067169018322602e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:22:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.004218525253236294 norm:6.275728810578585e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:23:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.004212945234030485 norm:2.3176409740699455e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:23:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.0042074741795659065 norm:2.0530766050796956e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:24:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.004203257616609335 norm:1.8104208720615134e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:24:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.00420160498470068 norm:1.578899536980316e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:25:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.004199841991066933 norm:1.4211630514182616e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:26:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.004198671784251928 norm:1.2821693417208735e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:26:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.0041975947096943855 norm:1.138615971285617e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:27:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.004196306690573692 norm:1.0186367035203148e-05 max memory_allocated 22564.47607421875 
[2025-02-18 19:27:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.004195613320916891 norm:9.342328667116817e-06 max memory_allocated 22564.47607421875 
[2025-02-18 19:28:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.004194422159343958 norm:8.543718649889342e-06 max memory_allocated 22564.47607421875 
[2025-02-18 19:28:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 19:29:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.005233094096183777 norm:0.0002822137321345508 max memory_allocated 22564.64794921875 
[2025-02-18 19:29:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.004980916157364845 norm:0.00011866675777127966 max memory_allocated 22564.64794921875 
[2025-02-18 19:30:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.004905208013951778 norm:8.022809925023466e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:30:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.004862611182034016 norm:5.984692688798532e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:31:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.004833250306546688 norm:4.6042729081818834e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:31:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.004811487626284361 norm:3.688708966365084e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:32:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.004796631634235382 norm:2.9485336199286394e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:33:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.004788251593708992 norm:2.3974407667992637e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:33:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.004782510455697775 norm:2.013386256294325e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:34:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.004776695277541876 norm:1.708753188722767e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:34:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.004773055203258991 norm:1.4626369193138089e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:35:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.004769634455442429 norm:1.2371886441542301e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:35:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.004766474477946758 norm:1.0746469342848286e-05 max memory_allocated 22564.64794921875 
[2025-02-18 19:36:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.0047646923922002316 norm:9.204586604028009e-06 max memory_allocated 22564.64794921875 
[2025-02-18 19:36:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.0047625587321817875 norm:8.116741810226813e-06 max memory_allocated 22564.64794921875 
[2025-02-18 19:37:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.004761047661304474 norm:7.374553206318524e-06 max memory_allocated 22564.64794921875 
[2025-02-18 19:38:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.004760038107633591 norm:6.633832526858896e-06 max memory_allocated 22564.64794921875 
[2025-02-18 19:38:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.004759199917316437 norm:6.2903463913244195e-06 max memory_allocated 22564.64794921875 
[2025-02-18 19:39:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.004757863003760576 norm:5.990330464555882e-06 max memory_allocated 22564.64794921875 
[2025-02-18 19:39:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.004757397808134556 norm:5.713401151297148e-06 max memory_allocated 22564.64794921875 
[2025-02-18 19:39:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 19:40:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.006041863467544317 norm:0.0004531043814495206 max memory_allocated 22564.81982421875 
[2025-02-18 19:41:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.005631548352539539 norm:0.00015544438792858273 max memory_allocated 22564.81982421875 
[2025-02-18 19:41:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.005530364345759153 norm:9.571667033014819e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:42:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.0054856655187904835 norm:7.269207708304748e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:42:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.005452833604067564 norm:5.782593507319689e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:43:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.0054295845329761505 norm:4.666927270591259e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:43:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.005412457976490259 norm:3.735892096301541e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:44:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.005400617606937885 norm:2.9994269425515085e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:45:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.005390596110373735 norm:2.482365380274132e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:45:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.005382366478443146 norm:2.1394214854808524e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:46:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.005376309156417847 norm:1.831433837651275e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:46:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.005373332183808088 norm:1.5899709978839383e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:47:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.005368323065340519 norm:1.4032594663149212e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:47:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.005364663433283567 norm:1.2132158190070186e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:48:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.005361140239983797 norm:1.0709612070058938e-05 max memory_allocated 22564.81982421875 
[2025-02-18 19:48:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.005362289026379585 norm:9.359257092000917e-06 max memory_allocated 22564.81982421875 
[2025-02-18 19:49:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.0053602647967636585 norm:8.380713552469388e-06 max memory_allocated 22564.81982421875 
[2025-02-18 19:50:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.005358103662729263 norm:7.634142093593255e-06 max memory_allocated 22564.81982421875 
[2025-02-18 19:50:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.005358235444873571 norm:7.119147994671948e-06 max memory_allocated 22564.81982421875 
[2025-02-18 19:51:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.00535682775080204 norm:6.7280316216056235e-06 max memory_allocated 22564.81982421875 
[2025-02-18 19:51:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 19:51:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.0071993679739534855 norm:0.0004212848434690386 max memory_allocated 22564.99169921875 
[2025-02-18 19:52:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.006720107980072498 norm:0.00021120831661392003 max memory_allocated 22564.99169921875 
[2025-02-18 19:53:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.006562897004187107 norm:0.00015095113485585898 max memory_allocated 22564.99169921875 
[2025-02-18 19:53:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.006476274225860834 norm:0.00011856370838358998 max memory_allocated 22564.99169921875 
[2025-02-18 19:54:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.006406601518392563 norm:9.492409299127758e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:54:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.006353264208883047 norm:7.851880218368024e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:55:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.00632146280258894 norm:6.668549758614972e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:55:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.006272938102483749 norm:5.7138793636113405e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:56:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.006252769380807877 norm:4.939372593071312e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:57:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.0062341210432350636 norm:4.346795685705729e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:57:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.006218262482434511 norm:3.852277222904377e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:58:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.006203177385032177 norm:3.444475805736147e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:58:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.006193595007061958 norm:3.095329157076776e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:59:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.006185248028486967 norm:2.7811758627649397e-05 max memory_allocated 22564.99169921875 
[2025-02-18 19:59:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.006178920157253742 norm:2.5102061044890434e-05 max memory_allocated 22564.99169921875 
[2025-02-18 20:00:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.00617167167365551 norm:2.282301284139976e-05 max memory_allocated 22564.99169921875 
[2025-02-18 20:00:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.006165557540953159 norm:2.0946396034560166e-05 max memory_allocated 22564.99169921875 
[2025-02-18 20:01:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.006151258479803801 norm:1.9604522094596177e-05 max memory_allocated 22564.99169921875 
[2025-02-18 20:02:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.0061493306420743465 norm:1.7936072254087776e-05 max memory_allocated 22564.99169921875 
[2025-02-18 20:02:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.00614688266068697 norm:1.6282818251056597e-05 max memory_allocated 22564.99169921875 
[2025-02-18 20:02:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 20:03:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.007757019717246294 norm:0.00047068489948287606 max memory_allocated 22565.16357421875 
[2025-02-18 20:03:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.007294895127415657 norm:0.00018116735736839473 max memory_allocated 22565.16357421875 
[2025-02-18 20:04:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.0071632652543485165 norm:0.00011240516323596239 max memory_allocated 22565.16357421875 
[2025-02-18 20:05:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.007105305325239897 norm:8.506744052283466e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:05:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.007063981145620346 norm:6.771321932319552e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:06:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.00703508360311389 norm:5.557256008614786e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:06:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.007011418230831623 norm:4.647436799132265e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:07:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.006991000846028328 norm:3.8630103517789394e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:07:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.006977218668907881 norm:3.26314038829878e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:08:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.006970105692744255 norm:2.7704769308911636e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:09:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.006962669547647238 norm:2.38935936067719e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:09:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.00695590628311038 norm:2.0643245079554617e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:10:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.006950106006115675 norm:1.8090173398377374e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:10:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.006948388181626797 norm:1.5548173905699514e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:11:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.0069474042393267155 norm:1.3169854355510324e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:11:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.006943345069885254 norm:1.1519357030920219e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:12:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.00694187730550766 norm:1.0239664334221743e-05 max memory_allocated 22565.16357421875 
[2025-02-18 20:12:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.0069412291049957275 norm:9.22772778721992e-06 max memory_allocated 22565.16357421875 
[2025-02-18 20:13:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.006941615603864193 norm:8.332383913511876e-06 max memory_allocated 22565.16357421875 
[2025-02-18 20:14:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.006942585576325655 norm:7.513213404308772e-06 max memory_allocated 22565.16357421875 
[2025-02-18 20:14:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 20:14:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.008632482960820198 norm:0.0004097003838978708 max memory_allocated 22565.33544921875 
[2025-02-18 20:15:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.008230478502810001 norm:0.00015583109052386135 max memory_allocated 22565.33544921875 
[2025-02-18 20:15:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.008137533441185951 norm:0.00011127591278636828 max memory_allocated 22565.33544921875 
[2025-02-18 20:16:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.008079955354332924 norm:8.487761078868061e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:17:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.008037896826863289 norm:6.571463018190116e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:17:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.008008155040442944 norm:8.011427416931838e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:18:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.007984098978340626 norm:4.2222014599246904e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:18:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.00796709954738617 norm:3.5902223316952586e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:19:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.007953488267958164 norm:3.1089257390704006e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:19:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.007942529395222664 norm:2.740380477916915e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:20:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.0079337852075696 norm:2.3636048354092054e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:21:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.007927587255835533 norm:2.096419848385267e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:21:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.007921088486909866 norm:1.844906000769697e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:22:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.00791595783084631 norm:1.5839177649468184e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:22:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.007912132889032364 norm:1.4280673894973006e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:23:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.00791064091026783 norm:1.2385783520585392e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:23:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.007907703518867493 norm:1.1201491361134686e-05 max memory_allocated 22565.33544921875 
[2025-02-18 20:24:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.007905368693172932 norm:9.96183280221885e-06 max memory_allocated 22565.33544921875 
[2025-02-18 20:24:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.00790520291775465 norm:8.810428880678955e-06 max memory_allocated 22565.33544921875 
[2025-02-18 20:25:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.007905183359980583 norm:7.854785508243367e-06 max memory_allocated 22565.33544921875 
[2025-02-18 20:25:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 20:25:43 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:26:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.010101763531565666 norm:0.002410748042166233 max memory_allocated 22565.62255859375 
[2025-02-18 20:26:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.00961250625550747 norm:0.0015983503544703126 max memory_allocated 22565.62255859375 
[2025-02-18 20:27:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.009468047879636288 norm:0.0012562116608023643 max memory_allocated 22565.62255859375 
[2025-02-18 20:27:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.009378256276249886 norm:0.001041986863128841 max memory_allocated 22565.62255859375 
[2025-02-18 20:28:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.00930353719741106 norm:0.0008548091282136738 max memory_allocated 22565.62255859375 
[2025-02-18 20:29:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.00924030039459467 norm:0.0006875547114759684 max memory_allocated 22565.62255859375 
[2025-02-18 20:29:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.009190800599753857 norm:0.0005559775163419545 max memory_allocated 22565.62255859375 
[2025-02-18 20:30:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.00915459729731083 norm:0.0004707667394541204 max memory_allocated 22565.62255859375 
[2025-02-18 20:30:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.009128551930189133 norm:0.00042251264676451683 max memory_allocated 22565.62255859375 
[2025-02-18 20:31:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.00911443680524826 norm:0.00042485236190259457 max memory_allocated 22565.62255859375 
[2025-02-18 20:31:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.009094995446503162 norm:0.00038435772876255214 max memory_allocated 22565.62255859375 
[2025-02-18 20:32:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.009091547690331936 norm:0.0004062218067701906 max memory_allocated 22565.62255859375 
[2025-02-18 20:33:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.00907285325229168 norm:0.00033434166107326746 max memory_allocated 22565.62255859375 
[2025-02-18 20:33:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.009058363735675812 norm:0.00033619216992519796 max memory_allocated 22565.62255859375 
[2025-02-18 20:34:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.009049709886312485 norm:0.00030476588290184736 max memory_allocated 22565.62255859375 
[2025-02-18 20:34:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.009043456986546516 norm:0.00030621723271906376 max memory_allocated 22565.62255859375 
[2025-02-18 20:35:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.009038202464580536 norm:0.00029003527015447617 max memory_allocated 22565.62255859375 
[2025-02-18 20:35:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.009036322124302387 norm:0.00028974987799301744 max memory_allocated 22565.62255859375 
[2025-02-18 20:36:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.009050210006535053 norm:0.00031583034433424473 max memory_allocated 22565.62255859375 
[2025-02-18 20:36:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.009059998206794262 norm:0.0003399867855478078 max memory_allocated 22565.62255859375 
[2025-02-18 20:37:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 20:37:11 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:37:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.01133367232978344 norm:0.0018800240941345692 max memory_allocated 22565.79443359375 
[2025-02-18 20:38:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.010892286896705627 norm:0.0012663428205996752 max memory_allocated 22565.79443359375 
[2025-02-18 20:38:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.010733800008893013 norm:0.0010543940588831902 max memory_allocated 22565.79443359375 
[2025-02-18 20:39:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.010625675320625305 norm:0.0007733848760835826 max memory_allocated 22565.79443359375 
[2025-02-18 20:40:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.010552764870226383 norm:0.0007293823873624206 max memory_allocated 22565.79443359375 
[2025-02-18 20:40:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.010492263361811638 norm:0.0006563190836459398 max memory_allocated 22565.79443359375 
[2025-02-18 20:41:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.010439506731927395 norm:0.0005234907730482519 max memory_allocated 22565.79443359375 
[2025-02-18 20:41:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.010404464788734913 norm:0.0004642116546165198 max memory_allocated 22565.79443359375 
[2025-02-18 20:42:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.010377321392297745 norm:0.0004205498262308538 max memory_allocated 22565.79443359375 
[2025-02-18 20:42:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.01035582646727562 norm:0.00035859382478520274 max memory_allocated 22565.79443359375 
[2025-02-18 20:43:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.01034817285835743 norm:0.00039198293234221637 max memory_allocated 22565.79443359375 
[2025-02-18 20:43:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.010406277142465115 norm:0.0005082049174234271 max memory_allocated 22565.79443359375 
[2025-02-18 20:44:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.010590781457722187 norm:0.0010168957524001598 max memory_allocated 22565.79443359375 
[2025-02-18 20:45:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.01044773031026125 norm:0.0006369328475557268 max memory_allocated 22565.79443359375 
[2025-02-18 20:45:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.010344275273382664 norm:0.0004184501012787223 max memory_allocated 22565.79443359375 
[2025-02-18 20:46:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.01030327845364809 norm:0.0003134037251584232 max memory_allocated 22565.79443359375 
[2025-02-18 20:46:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.010276013985276222 norm:0.00025505712255835533 max memory_allocated 22565.79443359375 
[2025-02-18 20:47:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.010261429473757744 norm:0.00021791378094349056 max memory_allocated 22565.79443359375 
[2025-02-18 20:47:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.01025572419166565 norm:0.00020665999909397215 max memory_allocated 22565.79443359375 
[2025-02-18 20:48:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.010249337181448936 norm:0.0001967484859051183 max memory_allocated 22565.79443359375 
[2025-02-18 20:48:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 20:48:39 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:49:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.019266072660684586 norm:0.00338520435616374 max memory_allocated 22565.96630859375 
[2025-02-18 20:49:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.01625477336347103 norm:0.0037505708169192076 max memory_allocated 22565.96630859375 
[2025-02-18 20:50:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.015586870722472668 norm:0.0035663857124745846 max memory_allocated 22565.96630859375 
[2025-02-18 20:50:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.01512401644140482 norm:0.0024072665255516768 max memory_allocated 22565.96630859375 
[2025-02-18 20:51:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.014871441759169102 norm:0.0018879782874137163 max memory_allocated 22565.96630859375 
[2025-02-18 20:52:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.014744558371603489 norm:0.001669748337008059 max memory_allocated 22565.96630859375 
[2025-02-18 20:52:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.014624337665736675 norm:0.0012958265142515302 max memory_allocated 22565.96630859375 
[2025-02-18 20:53:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.01451842486858368 norm:0.0009505729540251195 max memory_allocated 22565.96630859375 
[2025-02-18 20:53:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.014459163881838322 norm:0.0009705979027785361 max memory_allocated 22565.96630859375 
[2025-02-18 20:54:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.014406685717403889 norm:0.0008548600599169731 max memory_allocated 22565.96630859375 
[2025-02-18 20:54:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.014345858246088028 norm:0.0007707642507739365 max memory_allocated 22565.96630859375 
[2025-02-18 20:55:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.01434321515262127 norm:0.0007184647838585079 max memory_allocated 22565.96630859375 
[2025-02-18 20:55:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.014287897385656834 norm:0.0006672479212284088 max memory_allocated 22565.96630859375 
[2025-02-18 20:56:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.0142366923391819 norm:0.0005934089422225952 max memory_allocated 22565.96630859375 
[2025-02-18 20:57:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.014222885482013226 norm:0.000598482380155474 max memory_allocated 22565.96630859375 
[2025-02-18 20:57:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.01421267818659544 norm:0.000571703480090946 max memory_allocated 22565.96630859375 
[2025-02-18 20:58:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.01418302208185196 norm:0.0005719943437725306 max memory_allocated 22565.96630859375 
[2025-02-18 20:58:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.014192729257047176 norm:0.000525516748894006 max memory_allocated 22565.96630859375 
[2025-02-18 20:59:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.014415634796023369 norm:0.0011136431712657213 max memory_allocated 22565.96630859375 
[2025-02-18 20:59:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.014417757280170918 norm:0.0011124893790110946 max memory_allocated 22565.96630859375 
[2025-02-18 21:00:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 21:00:07 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 21:00:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.033649615943431854 norm:0.005784454755485058 max memory_allocated 22566.13818359375 
[2025-02-18 21:01:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.03121798112988472 norm:0.00429527135565877 max memory_allocated 22566.13818359375 
[2025-02-18 21:01:48 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.029691625386476517 norm:0.0037039245944470167 max memory_allocated 22566.13818359375 
[2025-02-18 21:02:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.028586506843566895 norm:0.0032147811725735664 max memory_allocated 22566.13818359375 
[2025-02-18 21:02:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.02777816727757454 norm:0.0028760607820004225 max memory_allocated 22566.13818359375 
[2025-02-18 21:03:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.027146581560373306 norm:0.002610363531857729 max memory_allocated 22566.13818359375 
[2025-02-18 21:04:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.02666761912405491 norm:0.0023763244971632957 max memory_allocated 22566.13818359375 
[2025-02-18 21:04:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.026292061433196068 norm:0.0022554700262844563 max memory_allocated 22566.13818359375 
[2025-02-18 21:05:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.0259761493653059 norm:0.0021568764932453632 max memory_allocated 22566.13818359375 
[2025-02-18 21:05:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.02572423219680786 norm:0.002219573361799121 max memory_allocated 22566.13818359375 
[2025-02-18 21:06:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.025823304429650307 norm:0.0030033148359507322 max memory_allocated 22566.13818359375 
[2025-02-18 21:06:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.025709323585033417 norm:0.002799125388264656 max memory_allocated 22566.13818359375 
[2025-02-18 21:07:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.02545333094894886 norm:0.0023075530771166086 max memory_allocated 22566.13818359375 
[2025-02-18 21:08:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.02523871138691902 norm:0.002066399436444044 max memory_allocated 22566.13818359375 
[2025-02-18 21:08:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.025145402178168297 norm:0.0020391931757330894 max memory_allocated 22566.13818359375 
[2025-02-18 21:09:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.025068504735827446 norm:0.00204286421649158 max memory_allocated 22566.13818359375 
[2025-02-18 21:09:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.025047671049833298 norm:0.0021544669289141893 max memory_allocated 22566.13818359375 
[2025-02-18 21:10:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.02506846748292446 norm:0.0023057342041283846 max memory_allocated 22566.13818359375 
[2025-02-18 21:10:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.025044836103916168 norm:0.0023879166692495346 max memory_allocated 22566.13818359375 
[2025-02-18 21:11:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.024996154010295868 norm:0.002424572128802538 max memory_allocated 22566.13818359375 
[2025-02-18 21:11:32 root] (main_calibration.py 365): INFO 21983.607414722443
[2025-02-18 21:12:04 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-18 21:13:15 root] (main_calibration.py 158): INFO wikitext2 : 5.484590530395508
[2025-02-18 21:13:15 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-18 21:15:06 root] (main_calibration.py 158): INFO c4 : 6.997063159942627
[2025-02-18 22:54:02 root] (main_calibration.py 169): INFO {'wikitext2': 5.484590530395508, 'c4': 6.997063159942627, 'results': {'winogrande': {'acc': 0.6708760852407262, 'acc_stderr': 0.013206387089091467}, 'hellaswag': {'acc': 0.5659231228838877, 'acc_stderr': 0.004946221512145285, 'acc_norm': 0.7296355307707628, 'acc_norm_stderr': 0.004432403734882272}, 'arc_challenge': {'acc': 0.3967576791808874, 'acc_stderr': 0.014296513020180628, 'acc_norm': 0.4121160409556314, 'acc_norm_stderr': 0.014383915302225398}, 'boolq': {'acc': 0.709480122324159, 'acc_stderr': 0.007940549952156418}, 'arc_easy': {'acc': 0.6931818181818182, 'acc_stderr': 0.00946307583519895, 'acc_norm': 0.5395622895622896, 'acc_norm_stderr': 0.01022761638628902}, 'piqa': {'acc': 0.7823721436343852, 'acc_stderr': 0.009627407474840883, 'acc_norm': 0.7682263329706203, 'acc_norm_stderr': 0.009845143772794048}}, 'versions': {'winogrande': 0, 'hellaswag': 0, 'arc_challenge': 0, 'boolq': 1, 'arc_easy': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
