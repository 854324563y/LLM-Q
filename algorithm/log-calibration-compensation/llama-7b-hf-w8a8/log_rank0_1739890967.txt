[2025-02-18 15:02:47 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation/llama-7b-hf-w8a8', save_dir='./log-calibration-compensation/quant/llama-7b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-18 15:06:24 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 15:06:24 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-18 15:06:24 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 15:06:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 15:06:31 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:07:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0011717770248651505 norm:0.0019736222457140684 max memory_allocated 22557.81005859375 
[2025-02-18 15:07:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.00066559505648911 norm:0.0003882455639541149 max memory_allocated 22557.81005859375 
[2025-02-18 15:08:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0005533931544050574 norm:0.0005446335999295115 max memory_allocated 22557.81005859375 
[2025-02-18 15:08:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0004912353469990194 norm:0.0005974031519144773 max memory_allocated 22557.81005859375 
[2025-02-18 15:09:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.00045356908231042325 norm:0.0006421697326004505 max memory_allocated 22557.81005859375 
[2025-02-18 15:09:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0004281516885384917 norm:0.0006289361044764519 max memory_allocated 22557.81005859375 
[2025-02-18 15:10:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.00041223489097319543 norm:0.0006251789163798094 max memory_allocated 22557.81005859375 
[2025-02-18 15:10:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.00040054923738352954 norm:0.0006105936481617391 max memory_allocated 22557.81005859375 
[2025-02-18 15:11:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.00039229035610333085 norm:0.0005864676204510033 max memory_allocated 22557.81005859375 
[2025-02-18 15:12:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0003851937362924218 norm:0.0005620219162665308 max memory_allocated 22557.81005859375 
[2025-02-18 15:12:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0003800271369982511 norm:0.0005381138762459159 max memory_allocated 22557.81005859375 
[2025-02-18 15:13:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0003754534409381449 norm:0.0005091221537441015 max memory_allocated 22557.81005859375 
[2025-02-18 15:13:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0003729533636942506 norm:0.00047900262870825827 max memory_allocated 22557.81005859375 
[2025-02-18 15:14:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0003735609061550349 norm:0.0005084360018372536 max memory_allocated 22557.81005859375 
[2025-02-18 15:14:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0003687045827973634 norm:0.0004445973318070173 max memory_allocated 22557.81005859375 
[2025-02-18 15:15:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0003679207875393331 norm:0.00043546443339437246 max memory_allocated 22557.81005859375 
[2025-02-18 15:15:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00036560927401296794 norm:0.0003998352913185954 max memory_allocated 22557.81005859375 
[2025-02-18 15:16:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00036430582986213267 norm:0.0003743409179151058 max memory_allocated 22557.81005859375 
[2025-02-18 15:17:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00036340561928227544 norm:0.0003620992065407336 max memory_allocated 22557.81005859375 
[2025-02-18 15:17:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0003618690534494817 norm:0.00034296410740353167 max memory_allocated 22557.81005859375 
[2025-02-18 15:17:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 15:17:46 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:18:20 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.001429448602721095 norm:0.001665160758420825 max memory_allocated 22557.98193359375 
[2025-02-18 15:18:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.000998341478407383 norm:0.00065149727743119 max memory_allocated 22557.98193359375 
[2025-02-18 15:19:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0009090285748243332 norm:0.00179576245136559 max memory_allocated 22557.98193359375 
[2025-02-18 15:19:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0008419082150794566 norm:0.0017922907136380672 max memory_allocated 22557.98193359375 
[2025-02-18 15:20:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0007997340289875865 norm:0.0017608379712328315 max memory_allocated 22557.98193359375 
[2025-02-18 15:21:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0007719222921878099 norm:0.0017070920439437032 max memory_allocated 22557.98193359375 
[2025-02-18 15:21:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0007489505806006491 norm:0.0016107979463413358 max memory_allocated 22557.98193359375 
[2025-02-18 15:22:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0007303677266463637 norm:0.001492078648880124 max memory_allocated 22557.98193359375 
[2025-02-18 15:22:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0007161065004765987 norm:0.00140222383197397 max memory_allocated 22557.98193359375 
[2025-02-18 15:23:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0007042586803436279 norm:0.0013182376278564334 max memory_allocated 22557.98193359375 
[2025-02-18 15:23:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0006966389482840896 norm:0.0012361666886135936 max memory_allocated 22557.98193359375 
[2025-02-18 15:24:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0006850500358268619 norm:0.0010818473529070616 max memory_allocated 22557.98193359375 
[2025-02-18 15:24:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0006767553277313709 norm:0.0009633207810111344 max memory_allocated 22557.98193359375 
[2025-02-18 15:25:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0006703162798658013 norm:0.0008587834308855236 max memory_allocated 22557.98193359375 
[2025-02-18 15:26:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0006642441730946302 norm:0.0007491836440749466 max memory_allocated 22557.98193359375 
[2025-02-18 15:26:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.000653763534501195 norm:0.0006163485813885927 max memory_allocated 22557.98193359375 
[2025-02-18 15:27:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0006471571396104991 norm:0.0005374219617806375 max memory_allocated 22557.98193359375 
[2025-02-18 15:27:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0006426179315894842 norm:0.0004980333033017814 max memory_allocated 22557.98193359375 
[2025-02-18 15:28:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0006415101233869791 norm:0.0005035724607296288 max memory_allocated 22557.98193359375 
[2025-02-18 15:28:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0006455841939896345 norm:0.000541169021744281 max memory_allocated 22557.98193359375 
[2025-02-18 15:29:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 15:29:04 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:29:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0012926679337397218 norm:0.0012994371354579926 max memory_allocated 22558.15380859375 
[2025-02-18 15:30:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.001024575554765761 norm:0.0007190060568973422 max memory_allocated 22558.15380859375 
[2025-02-18 15:30:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0009613764705136418 norm:0.0006440937868319452 max memory_allocated 22558.15380859375 
[2025-02-18 15:31:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0009183012880384922 norm:0.0005351011641323566 max memory_allocated 22558.15380859375 
[2025-02-18 15:31:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0008864256669767201 norm:0.0005160553264431655 max memory_allocated 22558.15380859375 
[2025-02-18 15:32:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0008765009697526693 norm:0.0005811564042232931 max memory_allocated 22558.15380859375 
[2025-02-18 15:32:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0008678818703629076 norm:0.0007344910409301519 max memory_allocated 22558.15380859375 
[2025-02-18 15:33:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0008602619636803865 norm:0.0007578487857244909 max memory_allocated 22558.15380859375 
[2025-02-18 15:34:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0008519011316820979 norm:0.0006999081233516335 max memory_allocated 22558.15380859375 
[2025-02-18 15:34:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0008493544883094728 norm:0.0006975423311814666 max memory_allocated 22558.15380859375 
[2025-02-18 15:35:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0008487811428494751 norm:0.0007172796758823097 max memory_allocated 22558.15380859375 
[2025-02-18 15:35:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0008411634480580688 norm:0.000600965868216008 max memory_allocated 22558.15380859375 
[2025-02-18 15:36:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0008408550056628883 norm:0.0005475908401422203 max memory_allocated 22558.15380859375 
[2025-02-18 15:36:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0008360911160707474 norm:0.0005069769686087966 max memory_allocated 22558.15380859375 
[2025-02-18 15:37:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0008328481926582754 norm:0.000498364504892379 max memory_allocated 22558.15380859375 
[2025-02-18 15:37:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0008681804174557328 norm:0.0005807324196211994 max memory_allocated 22558.15380859375 
[2025-02-18 15:38:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.002202867064625025 norm:0.001946992240846157 max memory_allocated 22558.15380859375 
[2025-02-18 15:39:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0008262993069365621 norm:0.00026974992942996323 max memory_allocated 22558.15380859375 
[2025-02-18 15:39:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0008256472647190094 norm:0.0002615822886582464 max memory_allocated 22558.15380859375 
[2025-02-18 15:40:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0008249164675362408 norm:0.00025938067119568586 max memory_allocated 22558.15380859375 
[2025-02-18 15:40:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 15:40:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0016058738110587 norm:0.00039923578151501715 max memory_allocated 22558.21044921875 
[2025-02-18 15:41:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.001245663850568235 norm:0.00017652582027949393 max memory_allocated 22558.21044921875 
[2025-02-18 15:42:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0011199774453416467 norm:9.765361028257757e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:42:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0010544074466452003 norm:6.730829773005098e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:43:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.0010161548852920532 norm:4.586864815792069e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:43:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0010086663533002138 norm:3.0394006898859516e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:44:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0010078642517328262 norm:2.0594474335666746e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:44:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.001002810662612319 norm:1.5906360204098746e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:45:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0009992753621190786 norm:1.3375536582316272e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:45:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0009966932702809572 norm:1.1366154467395972e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:46:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0009930802043527365 norm:1.0232102795271203e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:46:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0009935603011399508 norm:9.941082680597901e-06 max memory_allocated 22558.21044921875 
[2025-02-18 15:47:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0009912005625665188 norm:9.359159776067827e-06 max memory_allocated 22558.21044921875 
[2025-02-18 15:48:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0009902635356411338 norm:9.121860784944147e-06 max memory_allocated 22558.21044921875 
[2025-02-18 15:48:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0009889299981296062 norm:9.032291927724145e-06 max memory_allocated 22558.21044921875 
[2025-02-18 15:49:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.000988105544820428 norm:9.023951861308888e-06 max memory_allocated 22558.21044921875 
[2025-02-18 15:49:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0009880817960947752 norm:8.95001085154945e-06 max memory_allocated 22558.21044921875 
[2025-02-18 15:50:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.000987138249911368 norm:8.867345968610607e-06 max memory_allocated 22558.21044921875 
[2025-02-18 15:50:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0009858502307906747 norm:1.1961189557041507e-05 max memory_allocated 22558.21044921875 
[2025-02-18 15:51:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0009847687324509025 norm:8.819036338536534e-06 max memory_allocated 22558.21044921875 
[2025-02-18 15:51:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 15:52:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0018163868226110935 norm:0.0003872753295581788 max memory_allocated 22558.38232421875 
[2025-02-18 15:52:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0014534364454448223 norm:0.00015966327919159085 max memory_allocated 22558.38232421875 
[2025-02-18 15:53:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0013279947452247143 norm:9.379349648952484e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:53:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.001284428988583386 norm:5.723512003896758e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:54:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0012753534829244018 norm:3.503514017211273e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:54:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0012656489852815866 norm:2.499102811270859e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:55:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0012586418306455016 norm:1.990841337828897e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:56:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0012522372417151928 norm:1.6893947758944705e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:56:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0012478404678404331 norm:1.5193416402325965e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:57:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.001245420891791582 norm:1.399351458530873e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:57:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0012431701179593801 norm:1.3091802429698873e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:58:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0012410478666424751 norm:1.2866383258369751e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:58:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.001238952623680234 norm:1.2618940672837198e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:59:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0012375774094834924 norm:1.2226831131556537e-05 max memory_allocated 22558.38232421875 
[2025-02-18 15:59:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0012368413154035807 norm:1.205322132591391e-05 max memory_allocated 22558.38232421875 
[2025-02-18 16:00:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.001234811032190919 norm:1.1849368092953227e-05 max memory_allocated 22558.38232421875 
[2025-02-18 16:00:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0012346296571195126 norm:1.1853347132273484e-05 max memory_allocated 22558.38232421875 
[2025-02-18 16:01:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0012335520004853606 norm:1.1682213880703785e-05 max memory_allocated 22558.38232421875 
[2025-02-18 16:02:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0012338559608906507 norm:1.1561629435163923e-05 max memory_allocated 22558.38232421875 
[2025-02-18 16:02:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.001233584014698863 norm:1.1541704225237481e-05 max memory_allocated 22558.38232421875 
[2025-02-18 16:02:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 16:03:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0019283525180071592 norm:0.0002879992825910449 max memory_allocated 22558.55419921875 
[2025-02-18 16:03:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.001620020717382431 norm:0.00012967253860551864 max memory_allocated 22558.55419921875 
[2025-02-18 16:04:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0015159696340560913 norm:7.99228364485316e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:05:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0014673307305201888 norm:5.497811798704788e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:05:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0014446205459535122 norm:3.868009662255645e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:06:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.001430365489795804 norm:2.8292881324887276e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:06:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0014248703373596072 norm:2.0721812688861974e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:07:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0014197869459167123 norm:1.5606383385602385e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:07:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.001417119288817048 norm:1.2858621630584821e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:08:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0014151395298540592 norm:1.1388076927687507e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:08:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0014141699066385627 norm:1.0661598935257643e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:09:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0014124116860330105 norm:1.014079407468671e-05 max memory_allocated 22558.55419921875 
[2025-02-18 16:10:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0014117234386503696 norm:9.926191523845773e-06 max memory_allocated 22558.55419921875 
[2025-02-18 16:10:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0014121548738330603 norm:9.762337867869064e-06 max memory_allocated 22558.55419921875 
[2025-02-18 16:11:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0014119278639554977 norm:9.719770787341986e-06 max memory_allocated 22558.55419921875 
[2025-02-18 16:11:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0014117633691057563 norm:9.605438208382111e-06 max memory_allocated 22558.55419921875 
[2025-02-18 16:12:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0014114680234342813 norm:9.736225365486462e-06 max memory_allocated 22558.55419921875 
[2025-02-18 16:12:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.001407682429999113 norm:9.628198313293979e-06 max memory_allocated 22558.55419921875 
[2025-02-18 16:13:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0014082134002819657 norm:9.564335414324887e-06 max memory_allocated 22558.55419921875 
[2025-02-18 16:13:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0014081067638471723 norm:9.539516213408206e-06 max memory_allocated 22558.55419921875 
[2025-02-18 16:14:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 16:14:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.002343048807233572 norm:0.0005566153558902442 max memory_allocated 22558.72607421875 
[2025-02-18 16:15:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0018057073466479778 norm:0.00018072300008498132 max memory_allocated 22558.72607421875 
[2025-02-18 16:15:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.0016980175860226154 norm:0.00011592688679229468 max memory_allocated 22558.72607421875 
[2025-02-18 16:16:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.001639117137528956 norm:8.114780212054029e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:16:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.001615813816897571 norm:6.342699634842575e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:17:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.001600633724592626 norm:5.1106562750646845e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:17:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0015852937940508127 norm:4.186741716694087e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:18:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0015746449353173375 norm:3.402338188607246e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:19:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0015713369939476252 norm:2.9314758648979478e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:19:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0015649478882551193 norm:2.3603301087860018e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:20:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0015626951353624463 norm:1.8834563888958655e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:20:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0015594136202707887 norm:1.7390526409144513e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:21:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.001556732109747827 norm:1.4715639736095909e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:21:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0015564730856567621 norm:1.2370646800263785e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:22:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0015554351266473532 norm:1.0642126653692685e-05 max memory_allocated 22558.72607421875 
[2025-02-18 16:22:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0015554509591311216 norm:9.528116606816184e-06 max memory_allocated 22558.72607421875 
[2025-02-18 16:23:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0015564183704555035 norm:8.99102906259941e-06 max memory_allocated 22558.72607421875 
[2025-02-18 16:24:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.001554681221023202 norm:8.572727892897092e-06 max memory_allocated 22558.72607421875 
[2025-02-18 16:24:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0015577246667817235 norm:8.048566996876616e-06 max memory_allocated 22558.72607421875 
[2025-02-18 16:25:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0015560127794742584 norm:7.811354407749604e-06 max memory_allocated 22558.72607421875 
[2025-02-18 16:25:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 16:25:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0023396709002554417 norm:0.00036647357046604156 max memory_allocated 22558.89794921875 
[2025-02-18 16:26:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.001896388828754425 norm:0.00012258770584594458 max memory_allocated 22558.89794921875 
[2025-02-18 16:26:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.0018089337972924113 norm:8.083248394541442e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:27:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0017595693934708834 norm:5.695530489902012e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:28:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0017354770097881556 norm:4.463393270270899e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:28:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.001720973290503025 norm:3.543491766322404e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:29:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.001709997421130538 norm:2.8059810574632138e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:29:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0016951144207268953 norm:2.3497828806284815e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:30:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.0016960044158622622 norm:2.0025661797262728e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:30:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0016929241828620434 norm:1.5535077181993984e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:31:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.001691219164058566 norm:1.3521319488063455e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:31:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0016888295067474246 norm:1.1363365047145635e-05 max memory_allocated 22558.89794921875 
[2025-02-18 16:32:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.001687613665126264 norm:9.75000875769183e-06 max memory_allocated 22558.89794921875 
[2025-02-18 16:33:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0016879176255315542 norm:8.723831342649646e-06 max memory_allocated 22558.89794921875 
[2025-02-18 16:33:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0016860561445355415 norm:7.987609933479689e-06 max memory_allocated 22558.89794921875 
[2025-02-18 16:34:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0016834255075082183 norm:7.797959369781893e-06 max memory_allocated 22558.89794921875 
[2025-02-18 16:34:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0016848503146320581 norm:7.1932731771084946e-06 max memory_allocated 22558.89794921875 
[2025-02-18 16:35:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0016856520669534802 norm:6.939192644495051e-06 max memory_allocated 22558.89794921875 
[2025-02-18 16:35:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.001687029842287302 norm:6.903963367221877e-06 max memory_allocated 22558.89794921875 
[2025-02-18 16:36:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0016863776836544275 norm:6.699901405227138e-06 max memory_allocated 22558.89794921875 
[2025-02-18 16:36:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 16:37:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0022380847949534655 norm:0.00020556226081680506 max memory_allocated 22559.06982421875 
[2025-02-18 16:37:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0019456379814073443 norm:7.630906475242227e-05 max memory_allocated 22559.06982421875 
[2025-02-18 16:38:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.0018762842519208789 norm:5.0342663598712534e-05 max memory_allocated 22559.06982421875 
[2025-02-18 16:38:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0018384382128715515 norm:3.337282032589428e-05 max memory_allocated 22559.06982421875 
[2025-02-18 16:39:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0018231325084343553 norm:2.6524214263190515e-05 max memory_allocated 22559.06982421875 
[2025-02-18 16:39:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0018114435952156782 norm:2.1746927814092487e-05 max memory_allocated 22559.06982421875 
[2025-02-18 16:40:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0018049310892820358 norm:1.7224158000317402e-05 max memory_allocated 22559.06982421875 
[2025-02-18 16:40:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.001799601479433477 norm:1.4506210391118657e-05 max memory_allocated 22559.06982421875 
[2025-02-18 16:41:32 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0017958672251552343 norm:1.178297861770261e-05 max memory_allocated 22559.06982421875 
[2025-02-18 16:42:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.001794068026356399 norm:9.721812602947466e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:42:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0017923139967024326 norm:8.81737196323229e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:43:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0017927669687196612 norm:7.5272323556419e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:43:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0017927596345543861 norm:6.788037353544496e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:44:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0017907045548781753 norm:6.275825398915913e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:44:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.001790141686797142 norm:6.054746336303651e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:45:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0017900411039590836 norm:5.8858254305960145e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:45:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0017902019899338484 norm:5.646607405651594e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:46:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.001789770438335836 norm:5.571307156060357e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:47:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0017910362221300602 norm:5.579933258559322e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:47:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.001789333182387054 norm:5.518871603271691e-06 max memory_allocated 22559.06982421875 
[2025-02-18 16:47:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 16:48:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002536405110731721 norm:0.0002689023967832327 max memory_allocated 22559.24169921875 
[2025-02-18 16:48:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.002114395145326853 norm:7.014036236796528e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:49:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0020711657125502825 norm:5.7656230637803674e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:50:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.002024893881753087 norm:3.925967757822946e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:50:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0020046387799084187 norm:2.9790042390231974e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:51:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0019949981942772865 norm:2.5238805392291397e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:51:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.001986319664865732 norm:2.1392475900938734e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:52:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0019781473092734814 norm:1.6759204299887642e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:52:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.001974763348698616 norm:1.447450904379366e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:53:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0019717507530003786 norm:1.2749942470691167e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:53:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0019708815962076187 norm:1.1154192179674283e-05 max memory_allocated 22559.24169921875 
[2025-02-18 16:54:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.001967858290299773 norm:9.432119441044051e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:54:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0019665039144456387 norm:8.678504855197389e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:55:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0019680364057421684 norm:7.903287041699514e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:56:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.00196461146697402 norm:7.322591500269482e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:56:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0019644766580313444 norm:6.445575309044216e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:57:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.001964341150596738 norm:5.838523520651506e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:57:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0019635299686342478 norm:5.699466328223934e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:58:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0019648612942546606 norm:5.310724191076588e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:58:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0019652340561151505 norm:5.105225682200398e-06 max memory_allocated 22559.24169921875 
[2025-02-18 16:59:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 16:59:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0025031203404068947 norm:0.00017483689589425921 max memory_allocated 22559.41357421875 
[2025-02-18 17:00:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.002222469076514244 norm:5.952951323706657e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:00:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.002172089647501707 norm:4.4331532990327105e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:01:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0021354537457227707 norm:3.0481774956570007e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:01:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.002116857096552849 norm:2.3977007003850304e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:02:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.002104927320033312 norm:1.966815943887923e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:02:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0020974865183234215 norm:1.6917909306357615e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:03:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.00209080264903605 norm:1.4512032066704705e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:04:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.002086409367620945 norm:1.2566819350467995e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:04:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0020837734919041395 norm:1.077617343980819e-05 max memory_allocated 22559.41357421875 
[2025-02-18 17:05:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0020810095593333244 norm:9.386721103510354e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:05:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0020804035011678934 norm:8.24767630547285e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:06:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.00208075693808496 norm:7.483686204068363e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:06:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0020794272422790527 norm:6.695538104395382e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:07:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0020775103475898504 norm:6.3305033108918e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:07:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0020783385261893272 norm:5.825022071803687e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:08:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0020775310695171356 norm:5.432571924757212e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:08:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0020772572606801987 norm:5.066392077424098e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:09:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0020762248896062374 norm:4.938388428854523e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:10:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.002076173899695277 norm:4.8807787607074715e-06 max memory_allocated 22559.41357421875 
[2025-02-18 17:10:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 17:10:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.003092797240242362 norm:0.00044125434942543507 max memory_allocated 22559.58544921875 
[2025-02-18 17:11:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.00238414597697556 norm:8.99200385902077e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:11:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0022844320628792048 norm:5.785963367088698e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:12:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0022560141514986753 norm:5.023913399782032e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:13:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0022243671119213104 norm:3.7150435673538595e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:13:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.0022011769469827414 norm:2.700717595871538e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:14:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0021920250728726387 norm:2.2621345124207437e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:14:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.00218679360114038 norm:1.9910574337700382e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:15:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0021817325614392757 norm:1.7922817278304137e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:15:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.002180310431867838 norm:1.631665145396255e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:16:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0021771802566945553 norm:1.4914651728759054e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:16:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0021733036264777184 norm:1.3260927516967058e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:17:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0021744086407124996 norm:1.2131064977438655e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:18:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0021718305069953203 norm:1.1168805031047668e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:18:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0021696295589208603 norm:1.0343877875129692e-05 max memory_allocated 22559.58544921875 
[2025-02-18 17:19:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0021685962565243244 norm:9.198228326567914e-06 max memory_allocated 22559.58544921875 
[2025-02-18 17:19:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0021673201117664576 norm:8.699175850779284e-06 max memory_allocated 22559.58544921875 
[2025-02-18 17:20:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0021658518817275763 norm:8.103624168143142e-06 max memory_allocated 22559.58544921875 
[2025-02-18 17:20:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0021660036873072386 norm:7.302332051040139e-06 max memory_allocated 22559.58544921875 
[2025-02-18 17:21:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.002165587153285742 norm:6.87895135342842e-06 max memory_allocated 22559.58544921875 
[2025-02-18 17:21:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 17:22:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0028375887777656317 norm:0.00021340462262742221 max memory_allocated 22559.75732421875 
[2025-02-18 17:22:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0024874694645404816 norm:7.707897020736709e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:23:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002423546276986599 norm:5.418296859716065e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:23:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0023838733322918415 norm:4.0405455365544185e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:24:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.002354276366531849 norm:2.9756924050161615e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:24:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.0023402858059853315 norm:2.3964435968082398e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:25:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.002331533469259739 norm:2.0182793377898633e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:25:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0023247201461344957 norm:1.693335798336193e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:26:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.002320280997082591 norm:1.4785023267904762e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:27:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0023149861954152584 norm:1.3083335943520069e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:27:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0023126143496483564 norm:1.1763454494939651e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:28:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.002310113748535514 norm:1.059164605976548e-05 max memory_allocated 22559.75732421875 
[2025-02-18 17:28:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0023082136176526546 norm:9.56030453380663e-06 max memory_allocated 22559.75732421875 
[2025-02-18 17:29:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0023066303692758083 norm:8.751948371354956e-06 max memory_allocated 22559.75732421875 
[2025-02-18 17:29:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002308379393070936 norm:7.930173524073325e-06 max memory_allocated 22559.75732421875 
[2025-02-18 17:30:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.002307719783857465 norm:7.286204436240951e-06 max memory_allocated 22559.75732421875 
[2025-02-18 17:30:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0023071537725627422 norm:6.864230726932874e-06 max memory_allocated 22559.75732421875 
[2025-02-18 17:31:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0023060115054249763 norm:6.593144917133031e-06 max memory_allocated 22559.75732421875 
[2025-02-18 17:32:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0023060592357069254 norm:6.34846310276771e-06 max memory_allocated 22559.75732421875 
[2025-02-18 17:32:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.002306113252416253 norm:6.206412308529252e-06 max memory_allocated 22559.75732421875 
[2025-02-18 17:32:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 17:33:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.002904164604842663 norm:0.00017544454021845013 max memory_allocated 22559.92919921875 
[2025-02-18 17:33:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.002621418796479702 norm:6.378604302881286e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:34:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.0025692246854305267 norm:4.825837822863832e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:34:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0025306770112365484 norm:3.4080301702488214e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:35:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.002507575089111924 norm:2.6260691811330616e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:36:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.002495088381692767 norm:2.163111639674753e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:36:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.0024853183422237635 norm:1.7824120732257143e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:37:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.002479635179042816 norm:1.551765853946563e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:37:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.0024739340879023075 norm:1.3393850167631172e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:38:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.002469660947099328 norm:1.1574227755772881e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:38:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0024674853775650263 norm:1.0279567504767329e-05 max memory_allocated 22559.92919921875 
[2025-02-18 17:39:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0024649379774928093 norm:9.134397259913385e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:39:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.002462992211803794 norm:8.085035005933605e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:40:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.0024619358591735363 norm:7.3832734415191226e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:41:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.002461153082549572 norm:6.91937020746991e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:41:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.002462935633957386 norm:6.416322776203742e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:42:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0024628671817481518 norm:6.047370789019624e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:42:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.002462598029524088 norm:5.825835614814423e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:43:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.002462422475218773 norm:5.6861399571062066e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:43:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.002462133765220642 norm:5.5164027799037285e-06 max memory_allocated 22559.92919921875 
[2025-02-18 17:43:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 17:44:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.003284637350589037 norm:0.00026434456231072545 max memory_allocated 22560.10107421875 
[2025-02-18 17:45:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0028979219496250153 norm:0.00010198359814239666 max memory_allocated 22560.10107421875 
[2025-02-18 17:45:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.002827874617651105 norm:7.250214548548684e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:46:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0027731105219572783 norm:5.1316812459845096e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:46:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.0027435091324150562 norm:3.905031917383894e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:47:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.00272398442029953 norm:3.1273826607503e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:47:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0027113936375826597 norm:2.5963232474168763e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:48:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0027015432715415955 norm:2.215367931057699e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:49:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.002694337395951152 norm:1.8759385056910105e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:49:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.002688271226361394 norm:1.6273999790428206e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:50:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.002684874925762415 norm:1.41359050758183e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:50:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0026821126230061054 norm:1.2416436220519245e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:51:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.0026817787438631058 norm:1.1102594726253301e-05 max memory_allocated 22560.10107421875 
[2025-02-18 17:51:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0026800476480275393 norm:9.912201676343102e-06 max memory_allocated 22560.10107421875 
[2025-02-18 17:52:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0026783915236592293 norm:8.962167157733347e-06 max memory_allocated 22560.10107421875 
[2025-02-18 17:52:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.002677238779142499 norm:8.323472684423905e-06 max memory_allocated 22560.10107421875 
[2025-02-18 17:53:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0026777861639857292 norm:7.623960755154258e-06 max memory_allocated 22560.10107421875 
[2025-02-18 17:53:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.002677653217688203 norm:7.09557207301259e-06 max memory_allocated 22560.10107421875 
[2025-02-18 17:54:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.002677538665011525 norm:6.649879196629627e-06 max memory_allocated 22560.10107421875 
[2025-02-18 17:55:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.002678432036191225 norm:6.390732323779957e-06 max memory_allocated 22560.10107421875 
[2025-02-18 17:55:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 17:55:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.00331124453805387 norm:0.00015420276031363755 max memory_allocated 22560.27294921875 
[2025-02-18 17:56:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.0030665607191622257 norm:6.065057823434472e-05 max memory_allocated 22560.27294921875 
[2025-02-18 17:56:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0030100946314632893 norm:4.4179221731610596e-05 max memory_allocated 22560.27294921875 
[2025-02-18 17:57:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0029722978360950947 norm:3.134881262667477e-05 max memory_allocated 22560.27294921875 
[2025-02-18 17:58:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0029496215283870697 norm:2.3723212507320568e-05 max memory_allocated 22560.27294921875 
[2025-02-18 17:58:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.002937308046966791 norm:1.9151046217302792e-05 max memory_allocated 22560.27294921875 
[2025-02-18 17:59:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0029282914474606514 norm:1.6279596820822917e-05 max memory_allocated 22560.27294921875 
[2025-02-18 17:59:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0029222043231129646 norm:1.3969130122859497e-05 max memory_allocated 22560.27294921875 
[2025-02-18 18:00:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0029163677245378494 norm:1.1916419680346735e-05 max memory_allocated 22560.27294921875 
[2025-02-18 18:00:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0029124757274985313 norm:1.0428502719150856e-05 max memory_allocated 22560.27294921875 
[2025-02-18 18:01:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.002910661045461893 norm:9.125092219619546e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:01:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0029094438068568707 norm:8.29794862511335e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:02:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.002908341819420457 norm:7.724330316705164e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:02:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.002908869180828333 norm:7.242152605613228e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:03:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.0029067452996969223 norm:6.8944364102208056e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:04:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0029052654281258583 norm:6.66099731461145e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:04:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.0029051858000457287 norm:6.472497716458747e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:05:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.002906338544562459 norm:6.3859856709314045e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:05:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0029061269015073776 norm:6.273083272390068e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:06:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.0029056454077363014 norm:6.216431302163983e-06 max memory_allocated 22560.27294921875 
[2025-02-18 18:06:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 18:07:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.00459348876029253 norm:0.0005804704269394279 max memory_allocated 22560.44482421875 
[2025-02-18 18:07:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.0036451146006584167 norm:0.000145182668347843 max memory_allocated 22560.44482421875 
[2025-02-18 18:08:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.00348842004314065 norm:9.441576548852026e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:08:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.003439798718318343 norm:8.07634205557406e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:09:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0033817938528954983 norm:6.009783464833163e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:09:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0033397299703210592 norm:4.59626862721052e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:10:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.0033166646026074886 norm:3.8556499930564314e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:10:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0032987017184495926 norm:3.325651050545275e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:11:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.003284041304141283 norm:2.939979822258465e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:12:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.003274246584624052 norm:2.5778837880352512e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:12:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.0032666344195604324 norm:2.279198270116467e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:13:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.0032597656827419996 norm:2.0606472389772534e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:13:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0032546091824769974 norm:1.857836286944803e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:14:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.003251534653827548 norm:1.6753792806412093e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:14:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.003247617743909359 norm:1.5287836504285224e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:15:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.003246033564209938 norm:1.380260528094368e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:15:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.003243671264499426 norm:1.2422365216480102e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:16:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.003243179526180029 norm:1.1518175597302616e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:16:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.003243569750338793 norm:1.0744939572759904e-05 max memory_allocated 22560.44482421875 
[2025-02-18 18:17:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.0032412062864750624 norm:9.935330126609188e-06 max memory_allocated 22560.44482421875 
[2025-02-18 18:17:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 18:18:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.005108056124299765 norm:0.0006243851967155933 max memory_allocated 22560.61669921875 
[2025-02-18 18:18:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.004162263125181198 norm:0.00015262355736922473 max memory_allocated 22560.61669921875 
[2025-02-18 18:19:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.0040015000849962234 norm:9.673619933892041e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:19:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.00394729757681489 norm:8.518723188899457e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:20:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.003887041937559843 norm:6.494549597846344e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:21:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.0038389842957258224 norm:4.878423715126701e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:21:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.003811659524217248 norm:4.0532071579946205e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:22:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0037928151432424784 norm:3.523574923747219e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:22:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.003779410617426038 norm:3.106700023636222e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:23:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.00376852136105299 norm:2.7725123800337315e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:23:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.003762347623705864 norm:2.4893797672120854e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:24:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.003757796250283718 norm:2.2733802325092256e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:24:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.003751191310584545 norm:2.037125887000002e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:25:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.0037458196748048067 norm:1.8575899957795627e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:26:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.003742617554962635 norm:1.706252442090772e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:26:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.0037397509440779686 norm:1.5712406820966862e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:27:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.0037359651178121567 norm:1.4634740182373207e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:27:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.003732493380084634 norm:1.4046368960407563e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:28:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.003731482196599245 norm:1.2933928701386321e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:28:47 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0037320225965231657 norm:1.2305298696446698e-05 max memory_allocated 22560.61669921875 
[2025-02-18 18:28:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 18:29:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.005322604440152645 norm:0.0003994532744400203 max memory_allocated 22560.78857421875 
[2025-02-18 18:30:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.004689774941653013 norm:0.00013395925634540617 max memory_allocated 22560.78857421875 
[2025-02-18 18:30:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.004562713205814362 norm:9.717440116219223e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:31:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.004493198357522488 norm:7.763948815409094e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:31:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.004426981788128614 norm:5.745989619754255e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:32:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.004386149812489748 norm:4.54797045676969e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:32:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.004361283034086227 norm:3.793886935454793e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:33:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.0043410127982497215 norm:3.252883834647946e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:33:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.004326543305069208 norm:2.803333700285293e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:34:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.00431603379547596 norm:2.4434815713902935e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:35:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.004305685870349407 norm:2.1235615349723957e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:35:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.004298944491893053 norm:1.9006456568604335e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:36:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.004293230827897787 norm:1.6885946024558507e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:36:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.004288475029170513 norm:1.5462454030057415e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:37:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.004285962320864201 norm:1.400734436174389e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:37:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.004283920396119356 norm:1.2895753570774104e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:38:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.00428262073546648 norm:1.2069030162820127e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:38:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.004282526206225157 norm:1.1425866432546172e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:39:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.004282437264919281 norm:1.090311252482934e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:40:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.004282970447093248 norm:1.0506079888727982e-05 max memory_allocated 22560.78857421875 
[2025-02-18 18:40:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 18:40:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.006794925779104233 norm:0.0008661904721520841 max memory_allocated 22560.96044921875 
[2025-02-18 18:41:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.005572363268584013 norm:0.00022931919374968857 max memory_allocated 22560.96044921875 
[2025-02-18 18:41:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.005307670217007399 norm:0.00012398784747347236 max memory_allocated 22560.96044921875 
[2025-02-18 18:42:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.005231764167547226 norm:0.00010327836207579821 max memory_allocated 22560.96044921875 
[2025-02-18 18:43:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.005189958959817886 norm:9.326711005996913e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:43:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.005136491730809212 norm:7.346653728745878e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:44:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.005091351922601461 norm:5.706147931050509e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:44:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.005058835260570049 norm:4.652870484278537e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:45:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.005042029079049826 norm:4.1017807234311476e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:45:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.005027480889111757 norm:3.693345206556842e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:46:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.005016014911234379 norm:3.311676118755713e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:46:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.005009060725569725 norm:2.994707210746128e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:47:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.005003166850656271 norm:2.6723542760009877e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:47:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.004997038282454014 norm:2.410368324490264e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:48:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.004990542307496071 norm:2.19552603084594e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:49:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.004987945314496756 norm:1.976762905542273e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:49:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.004984046332538128 norm:1.8266982806380838e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:50:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.004980678670108318 norm:1.6582931493758224e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:50:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.004976238124072552 norm:1.5574376448057592e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:51:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.004975045099854469 norm:1.4156801626086235e-05 max memory_allocated 22560.96044921875 
[2025-02-18 18:51:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 18:52:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.009898346848785877 norm:0.0023636433761566877 max memory_allocated 22561.13232421875 
[2025-02-18 18:52:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.007105145137757063 norm:0.0005785576067864895 max memory_allocated 22561.13232421875 
[2025-02-18 18:53:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.0064413174986839294 norm:0.00024195740115828812 max memory_allocated 22561.13232421875 
[2025-02-18 18:53:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.006248194258660078 norm:0.00016266833699773997 max memory_allocated 22561.13232421875 
[2025-02-18 18:54:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.006190999876707792 norm:0.00014917756197974086 max memory_allocated 22561.13232421875 
[2025-02-18 18:54:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0061624436639249325 norm:0.0001425925293006003 max memory_allocated 22561.13232421875 
[2025-02-18 18:55:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.006108618341386318 norm:0.00012036658881697804 max memory_allocated 22561.13232421875 
[2025-02-18 18:55:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.006053626071661711 norm:9.508936636848375e-05 max memory_allocated 22561.13232421875 
[2025-02-18 18:56:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.006011532619595528 norm:7.867727254051715e-05 max memory_allocated 22561.13232421875 
[2025-02-18 18:57:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.005979552865028381 norm:6.680046499241143e-05 max memory_allocated 22561.13232421875 
[2025-02-18 18:57:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.005960645154118538 norm:5.910796971875243e-05 max memory_allocated 22561.13232421875 
[2025-02-18 18:58:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.0059463209472596645 norm:5.29423778061755e-05 max memory_allocated 22561.13232421875 
[2025-02-18 18:58:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.005934312008321285 norm:4.8046982556115836e-05 max memory_allocated 22561.13232421875 
[2025-02-18 18:59:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.00592300109565258 norm:4.3505326175363734e-05 max memory_allocated 22561.13232421875 
[2025-02-18 18:59:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.00591554120182991 norm:4.024448207928799e-05 max memory_allocated 22561.13232421875 
[2025-02-18 19:00:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.005908287595957518 norm:3.676921551232226e-05 max memory_allocated 22561.13232421875 
[2025-02-18 19:00:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.005901912227272987 norm:3.356240995344706e-05 max memory_allocated 22561.13232421875 
[2025-02-18 19:01:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.005900858901441097 norm:3.082594776060432e-05 max memory_allocated 22561.13232421875 
[2025-02-18 19:01:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.005895746871829033 norm:2.8839185688411817e-05 max memory_allocated 22561.13232421875 
[2025-02-18 19:02:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.0058921403251588345 norm:2.6357609385740943e-05 max memory_allocated 22561.13232421875 
[2025-02-18 19:02:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 19:03:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.00931035727262497 norm:0.0013030802365392447 max memory_allocated 22561.30419921875 
[2025-02-18 19:03:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.007664156146347523 norm:0.0003189913113601506 max memory_allocated 22561.30419921875 
[2025-02-18 19:04:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.007325462996959686 norm:0.0001604807621333748 max memory_allocated 22561.30419921875 
[2025-02-18 19:04:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.007257961202412844 norm:0.00013741781003773212 max memory_allocated 22561.30419921875 
[2025-02-18 19:05:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.007232680916786194 norm:0.0001257875410374254 max memory_allocated 22561.30419921875 
[2025-02-18 19:06:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.0071639409288764 norm:9.577639139024541e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:06:36 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.007112238556146622 norm:7.484611705876887e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:07:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.0070806886069476604 norm:6.116658914834261e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:07:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.007066455669701099 norm:5.3741623560199514e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:08:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.007058189250528812 norm:4.8393034376204014e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:08:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.007052001543343067 norm:4.44266916019842e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:09:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.007042272016406059 norm:3.838906195596792e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:09:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.007034815847873688 norm:3.315785579616204e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:10:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.007032693363726139 norm:2.888629023800604e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:11:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.007030898239463568 norm:2.586377195257228e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:11:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.007029297761619091 norm:2.2831412934465334e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:12:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.007029552012681961 norm:2.0587760445778258e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:12:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.007025233469903469 norm:1.921477451105602e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:13:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.007027429528534412 norm:1.7659865989116952e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:13:47 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.007027162238955498 norm:1.6603640688117594e-05 max memory_allocated 22561.30419921875 
[2025-02-18 19:13:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 19:14:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.01033724844455719 norm:0.0010732392547652125 max memory_allocated 22561.47607421875 
[2025-02-18 19:15:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.008875020779669285 norm:0.00028469902463257313 max memory_allocated 22561.47607421875 
[2025-02-18 19:15:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.008544208481907845 norm:0.00014549201296176761 max memory_allocated 22561.47607421875 
[2025-02-18 19:16:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.008448679931461811 norm:0.00011971017374889925 max memory_allocated 22561.47607421875 
[2025-02-18 19:16:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.008401350118219852 norm:0.00011089130566688254 max memory_allocated 22561.47607421875 
[2025-02-18 19:17:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.008347840048372746 norm:9.417210094397888e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:17:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.008297485299408436 norm:7.58653914090246e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:18:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.008257007226347923 norm:6.190053682075813e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:18:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.008236290886998177 norm:5.390368096414022e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:19:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.008216492831707 norm:4.7053967136889696e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:20:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.008205173537135124 norm:4.07873849326279e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:20:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.008193599991500378 norm:3.645601464086212e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:21:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.008180801756680012 norm:3.300071693956852e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:21:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.008173909969627857 norm:3.0172934202710167e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:22:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.008167671971023083 norm:2.7876287276740186e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:22:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.008158890530467033 norm:2.6409565180074424e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:23:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.008153365924954414 norm:2.4001983547350392e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:23:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.008150395937263966 norm:2.151950502593536e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:24:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.00814562477171421 norm:1.9517814507707953e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:25:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.008145482279360294 norm:1.7759191905497573e-05 max memory_allocated 22561.47607421875 
[2025-02-18 19:25:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 19:25:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.011040769517421722 norm:0.0006684871623292565 max memory_allocated 22561.64794921875 
[2025-02-18 19:26:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.010106523521244526 norm:0.0002316817844985053 max memory_allocated 22561.64794921875 
[2025-02-18 19:26:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.009867774322628975 norm:0.0001470024581067264 max memory_allocated 22561.64794921875 
[2025-02-18 19:27:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.009777745231986046 norm:0.00012053373211529106 max memory_allocated 22561.64794921875 
[2025-02-18 19:28:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.009712478145956993 norm:9.86026570899412e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:28:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.009649936109781265 norm:7.749455835437402e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:29:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.009605715982615948 norm:6.134136492619291e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:29:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.00957666989415884 norm:5.1993705710629e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:30:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.00955850351601839 norm:4.5725773816229776e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:30:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.009542123414576054 norm:4.062059088028036e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:31:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.00952823180705309 norm:3.600952186388895e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:31:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.009514419361948967 norm:3.269217268098146e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:32:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.009508535265922546 norm:2.900551407947205e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:32:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.009500746615231037 norm:2.58744021266466e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:33:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.009495088830590248 norm:2.304717418155633e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:34:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.009494181722402573 norm:2.096375828841701e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:34:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.009489950723946095 norm:1.948475619428791e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:35:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.00948785338550806 norm:1.7738519090926275e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:35:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.009481089189648628 norm:1.7688691514194943e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:36:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.009477538987994194 norm:1.5513513062614948e-05 max memory_allocated 22561.64794921875 
[2025-02-18 19:36:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 19:37:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.012696649879217148 norm:0.0007573814946226776 max memory_allocated 22561.81982421875 
[2025-02-18 19:37:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.011751973070204258 norm:0.0003243227256461978 max memory_allocated 22561.81982421875 
[2025-02-18 19:38:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.011473382823169231 norm:0.0002276904124300927 max memory_allocated 22561.81982421875 
[2025-02-18 19:38:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.011331198737025261 norm:0.00018045185424853116 max memory_allocated 22561.81982421875 
[2025-02-18 19:39:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.01122377160936594 norm:0.00014376788749359548 max memory_allocated 22561.81982421875 
[2025-02-18 19:39:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.011142179369926453 norm:0.00011569306661840528 max memory_allocated 22561.81982421875 
[2025-02-18 19:40:21 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.011078635230660439 norm:9.534977289149538e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:40:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.011033585295081139 norm:8.07803007774055e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:41:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.01099929865449667 norm:6.948759983060881e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:42:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.010973460972309113 norm:6.077380385249853e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:42:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.010952092707157135 norm:5.33231686858926e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:43:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.010937044396996498 norm:4.7366993385367095e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:43:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.010921729728579521 norm:4.208245809422806e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:44:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.010912314057350159 norm:3.7444409827003255e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:44:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.010904697701334953 norm:3.338287569931708e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:45:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.010897699743509293 norm:3.0034056180738844e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:45:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.010890806093811989 norm:2.738353214226663e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:46:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.010887960903346539 norm:2.4884837330318987e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:46:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.010884747840464115 norm:2.2751448341296054e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:47:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.010879495181143284 norm:2.102772668877151e-05 max memory_allocated 22561.81982421875 
[2025-02-18 19:47:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 19:48:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.01391532551497221 norm:0.0007296852418221533 max memory_allocated 22561.99169921875 
[2025-02-18 19:48:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.012990851886570454 norm:0.0002445158315822482 max memory_allocated 22561.99169921875 
[2025-02-18 19:49:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.012790807522833347 norm:0.0001613548374734819 max memory_allocated 22561.99169921875 
[2025-02-18 19:49:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.012715399265289307 norm:0.00013429642422124743 max memory_allocated 22561.99169921875 
[2025-02-18 19:50:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.012659811414778233 norm:0.00010932589793810621 max memory_allocated 22561.99169921875 
[2025-02-18 19:51:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.012600832618772984 norm:8.512213389622048e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:51:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.012556902132928371 norm:6.845230382168666e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:52:09 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.012529131025075912 norm:5.671366670867428e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:52:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.012510189786553383 norm:4.8727699322625995e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:53:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.012498043477535248 norm:4.18496856582351e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:53:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.012489614076912403 norm:3.608311453717761e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:54:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.012481206096708775 norm:3.163357905577868e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:54:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.012477690353989601 norm:2.7737565687857568e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:55:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.012471720576286316 norm:2.4289551220135763e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:56:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.012470751069486141 norm:2.1183079297770746e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:56:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.01246761903166771 norm:1.8744907720247284e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:57:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.012464320287108421 norm:1.6668385796947405e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:57:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.012462398037314415 norm:1.4807293155172374e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:58:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.012462187558412552 norm:1.3665776350535452e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:58:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.012460621073842049 norm:1.2590659935085569e-05 max memory_allocated 22561.99169921875 
[2025-02-18 19:58:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 19:59:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.015785938128829002 norm:0.0007415179861709476 max memory_allocated 22562.16357421875 
[2025-02-18 20:00:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.014836777001619339 norm:0.0002808665158227086 max memory_allocated 22562.16357421875 
[2025-02-18 20:00:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.0145941823720932 norm:0.00018603417265694588 max memory_allocated 22562.16357421875 
[2025-02-18 20:01:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.014503763057291508 norm:0.0001524544641142711 max memory_allocated 22562.16357421875 
[2025-02-18 20:01:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.01442541554570198 norm:0.0001201330596813932 max memory_allocated 22562.16357421875 
[2025-02-18 20:02:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.014356465078890324 norm:9.374764340464026e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:02:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.014313990250229836 norm:7.755518890917301e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:03:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.014283471740782261 norm:6.496394780697301e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:03:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.0142589770257473 norm:5.514717850019224e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:04:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.014241178520023823 norm:4.785400233231485e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:05:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.014229722321033478 norm:4.172871558694169e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:05:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.014221412129700184 norm:3.655850377981551e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:06:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.014213303104043007 norm:3.238645876990631e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:06:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.01420721597969532 norm:2.7701365979737602e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:07:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.014200009405612946 norm:2.4343042241525836e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:07:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.014195711351931095 norm:2.1237659893813543e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:08:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.014191372320055962 norm:1.8906346667790785e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:08:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.014186893589794636 norm:1.7008176655508578e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:09:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.014185541309416294 norm:1.5097004506969824e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:10:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.014186657965183258 norm:1.3162955838197377e-05 max memory_allocated 22562.16357421875 
[2025-02-18 20:10:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 20:10:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.01814698614180088 norm:0.0009537202422507107 max memory_allocated 22562.33544921875 
[2025-02-18 20:11:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.016872702166438103 norm:0.0003325972065795213 max memory_allocated 22562.33544921875 
[2025-02-18 20:11:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.01660129241645336 norm:0.00023379019694402814 max memory_allocated 22562.33544921875 
[2025-02-18 20:12:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.016480719670653343 norm:0.0001916807086672634 max memory_allocated 22562.33544921875 
[2025-02-18 20:13:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.016365105286240578 norm:0.00014939841639716178 max memory_allocated 22562.33544921875 
[2025-02-18 20:13:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.01627485267817974 norm:0.00011713825369952247 max memory_allocated 22562.33544921875 
[2025-02-18 20:14:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.01621386781334877 norm:9.767714072950184e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:14:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.016176648437976837 norm:8.363704546354711e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:15:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.01615147478878498 norm:7.151403406169266e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:15:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.01613605208694935 norm:6.0299425967969e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:16:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.0161245446652174 norm:5.0952501624124125e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:16:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.016116516664624214 norm:4.274200546205975e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:17:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.016108371317386627 norm:3.670582009362988e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:17:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.016097215935587883 norm:3.18240090564359e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:18:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.016091715544462204 norm:2.7980870072497055e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:19:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.016090473160147667 norm:2.384614708716981e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:19:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.01609015092253685 norm:2.109121487592347e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:20:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.016088224947452545 norm:1.9712430002982728e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:20:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.016086578369140625 norm:1.8779621314024553e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:21:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.016084780916571617 norm:1.704047645034734e-05 max memory_allocated 22562.33544921875 
[2025-02-18 20:21:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 20:21:29 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:22:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.020784202963113785 norm:0.0034679160453379154 max memory_allocated 22562.62255859375 
[2025-02-18 20:22:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.01979147456586361 norm:0.0023431209847331047 max memory_allocated 22562.62255859375 
[2025-02-18 20:23:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.019427290186285973 norm:0.0017798508051782846 max memory_allocated 22562.62255859375 
[2025-02-18 20:23:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.019217614084482193 norm:0.0015054832911118865 max memory_allocated 22562.62255859375 
[2025-02-18 20:24:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.0189973134547472 norm:0.0011672733817249537 max memory_allocated 22562.62255859375 
[2025-02-18 20:24:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.01880859024822712 norm:0.0010480359196662903 max memory_allocated 22562.62255859375 
[2025-02-18 20:25:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.018705591559410095 norm:0.0009132998529821634 max memory_allocated 22562.62255859375 
[2025-02-18 20:25:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.018635105341672897 norm:0.0008036830695345998 max memory_allocated 22562.62255859375 
[2025-02-18 20:26:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.018581369891762733 norm:0.0007264595478773117 max memory_allocated 22562.62255859375 
[2025-02-18 20:27:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.018549107015132904 norm:0.0006624196539632976 max memory_allocated 22562.62255859375 
[2025-02-18 20:27:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.018536444753408432 norm:0.0006878778804093599 max memory_allocated 22562.62255859375 
[2025-02-18 20:28:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.01851765066385269 norm:0.0007038117037154734 max memory_allocated 22562.62255859375 
[2025-02-18 20:28:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.018569577485322952 norm:0.0007983247633092105 max memory_allocated 22562.62255859375 
[2025-02-18 20:29:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.019038518890738487 norm:0.0014807842671871185 max memory_allocated 22562.62255859375 
[2025-02-18 20:29:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.022236105054616928 norm:0.004812791012227535 max memory_allocated 22562.62255859375 
[2025-02-18 20:30:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.01880166493356228 norm:0.0012172982096672058 max memory_allocated 22562.62255859375 
[2025-02-18 20:30:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.01860288344323635 norm:0.0007489346317015588 max memory_allocated 22562.62255859375 
[2025-02-18 20:31:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.018588820472359657 norm:0.0009326705476269126 max memory_allocated 22562.62255859375 
[2025-02-18 20:32:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.01857662945985794 norm:0.0009750368189997971 max memory_allocated 22562.62255859375 
[2025-02-18 20:32:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.018556801602244377 norm:0.0009327487205155194 max memory_allocated 22562.62255859375 
[2025-02-18 20:32:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 20:32:47 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:33:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.02340562641620636 norm:0.004175291862338781 max memory_allocated 22562.79443359375 
[2025-02-18 20:33:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.022464504465460777 norm:0.0026021001394838095 max memory_allocated 22562.79443359375 
[2025-02-18 20:34:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.022166354581713676 norm:0.0017679447773844004 max memory_allocated 22562.79443359375 
[2025-02-18 20:35:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.02189532481133938 norm:0.001360730966553092 max memory_allocated 22562.79443359375 
[2025-02-18 20:35:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.021701138466596603 norm:0.0013922294601798058 max memory_allocated 22562.79443359375 
[2025-02-18 20:36:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.021577434614300728 norm:0.0012136682635173202 max memory_allocated 22562.79443359375 
[2025-02-18 20:36:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.02149771898984909 norm:0.0010986337438225746 max memory_allocated 22562.79443359375 
[2025-02-18 20:37:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.02144528739154339 norm:0.0009766460862010717 max memory_allocated 22562.79443359375 
[2025-02-18 20:37:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.021395079791545868 norm:0.0008457032963633537 max memory_allocated 22562.79443359375 
[2025-02-18 20:38:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.021360063925385475 norm:0.0007496895268559456 max memory_allocated 22562.79443359375 
[2025-02-18 20:38:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.02133297547698021 norm:0.0006839412963017821 max memory_allocated 22562.79443359375 
[2025-02-18 20:39:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.021318668499588966 norm:0.0006705090636387467 max memory_allocated 22562.79443359375 
[2025-02-18 20:39:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.021317124366760254 norm:0.0007075835019350052 max memory_allocated 22562.79443359375 
[2025-02-18 20:40:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.021303270012140274 norm:0.0007054040324874222 max memory_allocated 22562.79443359375 
[2025-02-18 20:41:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.021306153386831284 norm:0.0007151549216359854 max memory_allocated 22562.79443359375 
[2025-02-18 20:41:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.02131752483546734 norm:0.0007170909666456282 max memory_allocated 22562.79443359375 
[2025-02-18 20:42:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.02159765362739563 norm:0.0011098127579316497 max memory_allocated 22562.79443359375 
[2025-02-18 20:42:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.02164367213845253 norm:0.0012584852520376444 max memory_allocated 22562.79443359375 
[2025-02-18 20:43:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.02162153273820877 norm:0.0011127021862193942 max memory_allocated 22562.79443359375 
[2025-02-18 20:43:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.021508987993001938 norm:0.000931236834730953 max memory_allocated 22562.79443359375 
[2025-02-18 20:44:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 20:44:05 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:44:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.03236318379640579 norm:0.004519497510045767 max memory_allocated 22562.96630859375 
[2025-02-18 20:45:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.03082265704870224 norm:0.0028259512037038803 max memory_allocated 22562.96630859375 
[2025-02-18 20:45:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.03005821816623211 norm:0.002350964117795229 max memory_allocated 22562.96630859375 
[2025-02-18 20:46:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.029479747638106346 norm:0.0019665227737277746 max memory_allocated 22562.96630859375 
[2025-02-18 20:46:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.029104748740792274 norm:0.0020070280879735947 max memory_allocated 22562.96630859375 
[2025-02-18 20:47:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.028826169669628143 norm:0.0017519479151815176 max memory_allocated 22562.96630859375 
[2025-02-18 20:47:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.028601787984371185 norm:0.0015061332378536463 max memory_allocated 22562.96630859375 
[2025-02-18 20:48:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.028423137962818146 norm:0.001329080667346716 max memory_allocated 22562.96630859375 
[2025-02-18 20:49:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.02827947586774826 norm:0.0011493335478007793 max memory_allocated 22562.96630859375 
[2025-02-18 20:49:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.028205059468746185 norm:0.0010818014852702618 max memory_allocated 22562.96630859375 
[2025-02-18 20:50:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.028213778510689735 norm:0.0011930306209251285 max memory_allocated 22562.96630859375 
[2025-02-18 20:50:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.028165249153971672 norm:0.0011398687493056059 max memory_allocated 22562.96630859375 
[2025-02-18 20:51:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.028222298249602318 norm:0.0012838849797844887 max memory_allocated 22562.96630859375 
[2025-02-18 20:51:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.028374921530485153 norm:0.001591600477695465 max memory_allocated 22562.96630859375 
[2025-02-18 20:52:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.028399718925356865 norm:0.0014996238751336932 max memory_allocated 22562.96630859375 
[2025-02-18 20:52:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.028251076117157936 norm:0.0012887862976640463 max memory_allocated 22562.96630859375 
[2025-02-18 20:53:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.028080929070711136 norm:0.0010037451284006238 max memory_allocated 22562.96630859375 
[2025-02-18 20:54:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.027996931225061417 norm:0.0008796643232926726 max memory_allocated 22562.96630859375 
[2025-02-18 20:54:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.027951903641223907 norm:0.000782000832259655 max memory_allocated 22562.96630859375 
[2025-02-18 20:55:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.027920762076973915 norm:0.0007134897750802338 max memory_allocated 22562.96630859375 
[2025-02-18 20:55:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 20:55:23 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:55:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.06470297276973724 norm:0.008329044096171856 max memory_allocated 22563.13818359375 
[2025-02-18 20:56:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.06045679748058319 norm:0.007468851748853922 max memory_allocated 22563.13818359375 
[2025-02-18 20:57:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.05767522007226944 norm:0.006461733486503363 max memory_allocated 22563.13818359375 
[2025-02-18 20:57:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.055837348103523254 norm:0.00604925025254488 max memory_allocated 22563.13818359375 
[2025-02-18 20:58:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.05437081307172775 norm:0.005489854142069817 max memory_allocated 22563.13818359375 
[2025-02-18 20:58:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.05338164418935776 norm:0.0051701124757528305 max memory_allocated 22563.13818359375 
[2025-02-18 20:59:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.052740391343832016 norm:0.004822064191102982 max memory_allocated 22563.13818359375 
[2025-02-18 20:59:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.05215611308813095 norm:0.005047678016126156 max memory_allocated 22563.13818359375 
[2025-02-18 21:00:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.051889777183532715 norm:0.004879079759120941 max memory_allocated 22563.13818359375 
[2025-02-18 21:00:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.05154498293995857 norm:0.005043498240411282 max memory_allocated 22563.13818359375 
[2025-02-18 21:01:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.05127629637718201 norm:0.004444333724677563 max memory_allocated 22563.13818359375 
[2025-02-18 21:02:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.0510546900331974 norm:0.004895901307463646 max memory_allocated 22563.13818359375 
[2025-02-18 21:02:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.050852157175540924 norm:0.004333282820880413 max memory_allocated 22563.13818359375 
[2025-02-18 21:03:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.050473958253860474 norm:0.004303464200347662 max memory_allocated 22563.13818359375 
[2025-02-18 21:03:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.050346191972494125 norm:0.004143242724239826 max memory_allocated 22563.13818359375 
[2025-02-18 21:04:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.05018910765647888 norm:0.004624003078788519 max memory_allocated 22563.13818359375 
[2025-02-18 21:04:48 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.05036076903343201 norm:0.0042298054322600365 max memory_allocated 22563.13818359375 
[2025-02-18 21:05:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.050348326563835144 norm:0.004714035429060459 max memory_allocated 22563.13818359375 
[2025-02-18 21:05:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.05039256066083908 norm:0.004208092577755451 max memory_allocated 22563.13818359375 
[2025-02-18 21:06:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.04993825405836105 norm:0.004147733096033335 max memory_allocated 22563.13818359375 
[2025-02-18 21:06:38 root] (main_calibration.py 365): INFO 21614.121939897537
[2025-02-18 21:07:16 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-18 21:08:26 root] (main_calibration.py 158): INFO wikitext2 : 5.692480087280273
[2025-02-18 21:08:26 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-18 21:10:15 root] (main_calibration.py 158): INFO c4 : 7.095885753631592
[2025-02-18 22:50:09 root] (main_calibration.py 169): INFO {'wikitext2': 5.692480087280273, 'c4': 7.095885753631592, 'results': {'hellaswag': {'acc': 0.5641306512646883, 'acc_stderr': 0.004948567856373858, 'acc_norm': 0.7290380402310297, 'acc_norm_stderr': 0.004435481515909397}, 'arc_easy': {'acc': 0.672979797979798, 'acc_stderr': 0.009626235849372203, 'acc_norm': 0.5210437710437711, 'acc_norm_stderr': 0.01025069260202258}, 'piqa': {'acc': 0.7834602829162133, 'acc_stderr': 0.009609984714384607, 'acc_norm': 0.7731229597388466, 'acc_norm_stderr': 0.00977158425921518}, 'boolq': {'acc': 0.7311926605504587, 'acc_stderr': 0.007754057418983877}, 'arc_challenge': {'acc': 0.3856655290102389, 'acc_stderr': 0.01422425097325717, 'acc_norm': 0.41552901023890787, 'acc_norm_stderr': 0.01440136664121639}, 'winogrande': {'acc': 0.6771902131018153, 'acc_stderr': 0.013140498173357957}}, 'versions': {'hellaswag': 0, 'arc_easy': 0, 'piqa': 0, 'boolq': 1, 'arc_challenge': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
