[2025-02-18 15:02:45 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation/Llama-2-13b-hf-w4a4', save_dir='./log-calibration-compensation/quant/Llama-2-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-18 15:02:47 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 15:02:47 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-18 15:02:47 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 15:02:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 15:02:54 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:03:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.06203644350171089 norm:0.04868236556649208 max memory_allocated 29269.39501953125 
[2025-02-18 15:04:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.03808058798313141 norm:0.0262463316321373 max memory_allocated 29269.39501953125 
[2025-02-18 15:05:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.03390514478087425 norm:0.02359703741967678 max memory_allocated 29269.39501953125 
[2025-02-18 15:06:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.03143851459026337 norm:0.01928805187344551 max memory_allocated 29269.39501953125 
[2025-02-18 15:07:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0305266622453928 norm:0.023273417726159096 max memory_allocated 29269.39501953125 
[2025-02-18 15:07:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.030119681730866432 norm:0.019758740440011024 max memory_allocated 29269.39501953125 
[2025-02-18 15:08:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.028507761657238007 norm:0.017106886953115463 max memory_allocated 29269.39501953125 
[2025-02-18 15:09:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.027346136048436165 norm:0.009281295351684093 max memory_allocated 29269.39501953125 
[2025-02-18 15:10:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.027737166732549667 norm:0.009523442015051842 max memory_allocated 29269.39501953125 
[2025-02-18 15:11:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.027041450142860413 norm:0.009861255064606667 max memory_allocated 29269.39501953125 
[2025-02-18 15:12:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.02689703367650509 norm:0.007938540540635586 max memory_allocated 29269.39501953125 
[2025-02-18 15:12:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.026578519493341446 norm:0.006724325940012932 max memory_allocated 29269.39501953125 
[2025-02-18 15:13:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.027690071612596512 norm:0.011010980233550072 max memory_allocated 29269.39501953125 
[2025-02-18 15:14:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.02736436389386654 norm:0.007095206528902054 max memory_allocated 29269.39501953125 
[2025-02-18 15:15:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.02805369906127453 norm:0.007460945285856724 max memory_allocated 29269.39501953125 
[2025-02-18 15:16:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.027918359264731407 norm:0.008064593188464642 max memory_allocated 29269.39501953125 
[2025-02-18 15:16:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.027580782771110535 norm:0.007825899869203568 max memory_allocated 29269.39501953125 
[2025-02-18 15:17:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.02796698734164238 norm:0.008122642524540424 max memory_allocated 29269.39501953125 
[2025-02-18 15:18:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0276393573731184 norm:0.007299160584807396 max memory_allocated 29269.39501953125 
[2025-02-18 15:19:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.02858693152666092 norm:0.006434048991650343 max memory_allocated 29269.39501953125 
[2025-02-18 15:19:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 15:19:55 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:20:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.15292920172214508 norm:0.045112717896699905 max memory_allocated 29269.39501953125 
[2025-02-18 15:21:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.12278636544942856 norm:0.03221597895026207 max memory_allocated 29269.39501953125 
[2025-02-18 15:22:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.1147695779800415 norm:0.026436137035489082 max memory_allocated 29269.39501953125 
[2025-02-18 15:23:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.10983256995677948 norm:0.020867610350251198 max memory_allocated 29269.39501953125 
[2025-02-18 15:24:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.10644174367189407 norm:0.016140080988407135 max memory_allocated 29269.39501953125 
[2025-02-18 15:24:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.10403628647327423 norm:0.013119426555931568 max memory_allocated 29269.39501953125 
[2025-02-18 15:25:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.10275782644748688 norm:0.01101120375096798 max memory_allocated 29269.39501953125 
[2025-02-18 15:26:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.10209546983242035 norm:0.009332606568932533 max memory_allocated 29269.39501953125 
[2025-02-18 15:27:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.10173027217388153 norm:0.008587758988142014 max memory_allocated 29269.39501953125 
[2025-02-18 15:28:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.10156455636024475 norm:0.008262015879154205 max memory_allocated 29269.39501953125 
[2025-02-18 15:29:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.10102341324090958 norm:0.007626901380717754 max memory_allocated 29269.39501953125 
[2025-02-18 15:29:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.10104130208492279 norm:0.007548145949840546 max memory_allocated 29269.39501953125 
[2025-02-18 15:30:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.10117081552743912 norm:0.007163357455283403 max memory_allocated 29269.39501953125 
[2025-02-18 15:31:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.1011447086930275 norm:0.006945190951228142 max memory_allocated 29269.39501953125 
[2025-02-18 15:32:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.10123423486948013 norm:0.006870171055197716 max memory_allocated 29269.39501953125 
[2025-02-18 15:33:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.10130733251571655 norm:0.006728214677423239 max memory_allocated 29269.39501953125 
[2025-02-18 15:34:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.10131222754716873 norm:0.006576605141162872 max memory_allocated 29269.39501953125 
[2025-02-18 15:34:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.10138778388500214 norm:0.006609393283724785 max memory_allocated 29269.39501953125 
[2025-02-18 15:35:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.10132794827222824 norm:0.0064743198454380035 max memory_allocated 29269.39501953125 
[2025-02-18 15:36:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.10135053843259811 norm:0.0063982997089624405 max memory_allocated 29269.39501953125 
[2025-02-18 15:36:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 15:37:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:37:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.17297163605690002 norm:0.026091163977980614 max memory_allocated 29269.77001953125 
[2025-02-18 15:38:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.15352341532707214 norm:0.01976146176457405 max memory_allocated 29269.77001953125 
[2025-02-18 15:39:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.14680273830890656 norm:0.015017423778772354 max memory_allocated 29269.77001953125 
[2025-02-18 15:40:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.1434112787246704 norm:0.011561188846826553 max memory_allocated 29269.77001953125 
[2025-02-18 15:41:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.14055752754211426 norm:0.008577066473662853 max memory_allocated 29269.77001953125 
[2025-02-18 15:41:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.13904815912246704 norm:0.006499672774225473 max memory_allocated 29269.77001953125 
[2025-02-18 15:42:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.13802878558635712 norm:0.005498580634593964 max memory_allocated 29269.77001953125 
[2025-02-18 15:43:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.13764914870262146 norm:0.005216382909566164 max memory_allocated 29269.77001953125 
[2025-02-18 15:44:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.13717590272426605 norm:0.004873041529208422 max memory_allocated 29269.77001953125 
[2025-02-18 15:45:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.13693895936012268 norm:0.004618224687874317 max memory_allocated 29269.77001953125 
[2025-02-18 15:46:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.13665100932121277 norm:0.004557199310511351 max memory_allocated 29269.77001953125 
[2025-02-18 15:46:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.13671983778476715 norm:0.0045335604809224606 max memory_allocated 29269.77001953125 
[2025-02-18 15:47:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.13677987456321716 norm:0.004553056787699461 max memory_allocated 29269.77001953125 
[2025-02-18 15:48:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.13691219687461853 norm:0.004544507246464491 max memory_allocated 29269.77001953125 
[2025-02-18 15:49:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.13691699504852295 norm:0.004536865279078484 max memory_allocated 29269.77001953125 
[2025-02-18 15:50:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.1369975060224533 norm:0.0045823268592357635 max memory_allocated 29269.77001953125 
[2025-02-18 15:51:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.13696648180484772 norm:0.004547547083348036 max memory_allocated 29269.77001953125 
[2025-02-18 15:51:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.13708001375198364 norm:0.004514856263995171 max memory_allocated 29269.77001953125 
[2025-02-18 15:52:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.1370110809803009 norm:0.004418843891471624 max memory_allocated 29269.77001953125 
[2025-02-18 15:53:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.13712440431118011 norm:0.004378886893391609 max memory_allocated 29269.77001953125 
[2025-02-18 15:53:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 15:54:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.37351861596107483 norm:0.11129714548587799 max memory_allocated 29269.77001953125 
[2025-02-18 15:55:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.28874024748802185 norm:0.03441226854920387 max memory_allocated 29269.77001953125 
[2025-02-18 15:56:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.2638682723045349 norm:0.026528630405664444 max memory_allocated 29269.77001953125 
[2025-02-18 15:57:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.2456807792186737 norm:0.01801244542002678 max memory_allocated 29269.77001953125 
[2025-02-18 15:58:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.23883238434791565 norm:0.0143284285441041 max memory_allocated 29269.77001953125 
[2025-02-18 15:59:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.2391912043094635 norm:0.013947812840342522 max memory_allocated 29269.77001953125 
[2025-02-18 15:59:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.2387945055961609 norm:0.013511093333363533 max memory_allocated 29269.77001953125 
[2025-02-18 16:00:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.23864108324050903 norm:0.013667446561157703 max memory_allocated 29269.77001953125 
[2025-02-18 16:01:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.24008709192276 norm:0.013613232411444187 max memory_allocated 29269.77001953125 
[2025-02-18 16:02:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.23634907603263855 norm:0.011919270269572735 max memory_allocated 29269.77001953125 
[2025-02-18 16:03:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.23564507067203522 norm:0.012927461415529251 max memory_allocated 29269.77001953125 
[2025-02-18 16:03:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.24146705865859985 norm:0.013937942683696747 max memory_allocated 29269.77001953125 
[2025-02-18 16:04:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.249958798289299 norm:0.014142150059342384 max memory_allocated 29269.77001953125 
[2025-02-18 16:05:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.24753649532794952 norm:0.013315718621015549 max memory_allocated 29269.77001953125 
[2025-02-18 16:06:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.2484029233455658 norm:0.015048036351799965 max memory_allocated 29269.77001953125 
[2025-02-18 16:07:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.24889662861824036 norm:0.015773817896842957 max memory_allocated 29269.77001953125 
[2025-02-18 16:08:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.24883849918842316 norm:0.013985782861709595 max memory_allocated 29269.77001953125 
[2025-02-18 16:08:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.25351378321647644 norm:0.016013028100132942 max memory_allocated 29269.77001953125 
[2025-02-18 16:09:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.2484908252954483 norm:0.014035215601325035 max memory_allocated 29269.77001953125 
[2025-02-18 16:10:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.2509174048900604 norm:0.014851660467684269 max memory_allocated 29269.77001953125 
[2025-02-18 16:10:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 16:11:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.2942069470882416 norm:0.013833697885274887 max memory_allocated 29269.77001953125 
[2025-02-18 16:12:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.27870213985443115 norm:0.006647278554737568 max memory_allocated 29269.77001953125 
[2025-02-18 16:13:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.27343299984931946 norm:0.004556810948997736 max memory_allocated 29269.77001953125 
[2025-02-18 16:14:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.269099622964859 norm:0.003062531119212508 max memory_allocated 29269.77001953125 
[2025-02-18 16:15:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.26756343245506287 norm:0.0024353035259991884 max memory_allocated 29269.77001953125 
[2025-02-18 16:15:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.26659706234931946 norm:0.002077216748148203 max memory_allocated 29269.77001953125 
[2025-02-18 16:16:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.26582983136177063 norm:0.001860223594121635 max memory_allocated 29269.77001953125 
[2025-02-18 16:17:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.26529166102409363 norm:0.0016882678028196096 max memory_allocated 29269.77001953125 
[2025-02-18 16:18:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.2653087079524994 norm:0.0016272321809083223 max memory_allocated 29269.77001953125 
[2025-02-18 16:19:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.2653162479400635 norm:0.0015564103377982974 max memory_allocated 29269.77001953125 
[2025-02-18 16:20:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.26518794894218445 norm:0.0014842896489426494 max memory_allocated 29269.77001953125 
[2025-02-18 16:20:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.26520052552223206 norm:0.001464588101953268 max memory_allocated 29269.77001953125 
[2025-02-18 16:21:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.26481854915618896 norm:0.001463759341277182 max memory_allocated 29269.77001953125 
[2025-02-18 16:22:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.2647872567176819 norm:0.0014044762356206775 max memory_allocated 29269.77001953125 
[2025-02-18 16:23:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.26462119817733765 norm:0.0013735839165747166 max memory_allocated 29269.77001953125 
[2025-02-18 16:24:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.26479053497314453 norm:0.0013696362730115652 max memory_allocated 29269.77001953125 
[2025-02-18 16:25:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.26471203565597534 norm:0.0013522750232368708 max memory_allocated 29269.77001953125 
[2025-02-18 16:25:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.2646186053752899 norm:0.0013396337162703276 max memory_allocated 29269.77001953125 
[2025-02-18 16:26:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.26453569531440735 norm:0.0013349624350667 max memory_allocated 29269.77001953125 
[2025-02-18 16:27:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.2644987106323242 norm:0.0013353395042940974 max memory_allocated 29269.77001953125 
[2025-02-18 16:27:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 16:28:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.3363448679447174 norm:0.012416568584740162 max memory_allocated 29270.18798828125 
[2025-02-18 16:29:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.3214339315891266 norm:0.005691428203135729 max memory_allocated 29270.18798828125 
[2025-02-18 16:30:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.3144094944000244 norm:0.003459782339632511 max memory_allocated 29270.18798828125 
[2025-02-18 16:31:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.3107783794403076 norm:0.0027340396773070097 max memory_allocated 29270.18798828125 
[2025-02-18 16:32:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.3082635998725891 norm:0.002121893921867013 max memory_allocated 29270.18798828125 
[2025-02-18 16:32:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.30693313479423523 norm:0.001847073552198708 max memory_allocated 29270.18798828125 
[2025-02-18 16:33:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.3061395287513733 norm:0.0016803548205643892 max memory_allocated 29270.18798828125 
[2025-02-18 16:34:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.3057524859905243 norm:0.0016013856511563063 max memory_allocated 29270.18798828125 
[2025-02-18 16:35:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.305175244808197 norm:0.0014903377741575241 max memory_allocated 29270.18798828125 
[2025-02-18 16:36:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.3047280013561249 norm:0.0014371367869898677 max memory_allocated 29270.18798828125 
[2025-02-18 16:37:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.3044276237487793 norm:0.0013865515356883407 max memory_allocated 29270.18798828125 
[2025-02-18 16:37:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.3042408227920532 norm:0.0013451259583234787 max memory_allocated 29270.18798828125 
[2025-02-18 16:38:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.3042025566101074 norm:0.0013575906632468104 max memory_allocated 29270.18798828125 
[2025-02-18 16:39:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.304132878780365 norm:0.0012951933313161135 max memory_allocated 29270.18798828125 
[2025-02-18 16:40:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.3040166199207306 norm:0.001288257073611021 max memory_allocated 29270.18798828125 
[2025-02-18 16:41:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.3040495812892914 norm:0.001306589343585074 max memory_allocated 29270.18798828125 
[2025-02-18 16:42:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.3036839962005615 norm:0.0012804842554032803 max memory_allocated 29270.18798828125 
[2025-02-18 16:42:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.3037525713443756 norm:0.0012522628530859947 max memory_allocated 29270.18798828125 
[2025-02-18 16:43:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.30372828245162964 norm:0.0012681895168498158 max memory_allocated 29270.18798828125 
[2025-02-18 16:44:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.30354663729667664 norm:0.001249008346349001 max memory_allocated 29270.18798828125 
[2025-02-18 16:44:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 16:45:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.3867894411087036 norm:0.02479734644293785 max memory_allocated 29270.18798828125 
[2025-02-18 16:46:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.3670137822628021 norm:0.014599675312638283 max memory_allocated 29270.18798828125 
[2025-02-18 16:47:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.3571575880050659 norm:0.009245675057172775 max memory_allocated 29270.18798828125 
[2025-02-18 16:48:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.3521176874637604 norm:0.006626472808420658 max memory_allocated 29270.18798828125 
[2025-02-18 16:49:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.34983447194099426 norm:0.00503302738070488 max memory_allocated 29270.18798828125 
[2025-02-18 16:49:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.34702444076538086 norm:0.003927825950086117 max memory_allocated 29270.18798828125 
[2025-02-18 16:50:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.3446994721889496 norm:0.0028861728496849537 max memory_allocated 29270.18798828125 
[2025-02-18 16:51:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.34351563453674316 norm:0.00249443668872118 max memory_allocated 29270.18798828125 
[2025-02-18 16:52:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.34283941984176636 norm:0.0021682095248252153 max memory_allocated 29270.18798828125 
[2025-02-18 16:53:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.342110812664032 norm:0.0019534442108124495 max memory_allocated 29270.18798828125 
[2025-02-18 16:54:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.34137266874313354 norm:0.001778843579813838 max memory_allocated 29270.18798828125 
[2025-02-18 16:54:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.3408297300338745 norm:0.0016308145131915808 max memory_allocated 29270.18798828125 
[2025-02-18 16:55:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.34008726477622986 norm:0.0015830043703317642 max memory_allocated 29270.18798828125 
[2025-02-18 16:56:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.3399466574192047 norm:0.0015469230711460114 max memory_allocated 29270.18798828125 
[2025-02-18 16:57:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.34021520614624023 norm:0.0015008138725534081 max memory_allocated 29270.18798828125 
[2025-02-18 16:58:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.33975714445114136 norm:0.0014869305305182934 max memory_allocated 29270.18798828125 
[2025-02-18 16:59:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.3395022451877594 norm:0.0014677848666906357 max memory_allocated 29270.18798828125 
[2025-02-18 16:59:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.33899492025375366 norm:0.0014386280672624707 max memory_allocated 29270.18798828125 
[2025-02-18 17:00:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.33923858404159546 norm:0.0014305149670690298 max memory_allocated 29270.18798828125 
[2025-02-18 17:01:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.3390688896179199 norm:0.00140799256041646 max memory_allocated 29270.18798828125 
[2025-02-18 17:01:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 17:02:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.44091957807540894 norm:0.010320199653506279 max memory_allocated 29270.18798828125 
[2025-02-18 17:03:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.4167013168334961 norm:0.005459546111524105 max memory_allocated 29270.18798828125 
[2025-02-18 17:04:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.4058375358581543 norm:0.003888905979692936 max memory_allocated 29270.18798828125 
[2025-02-18 17:05:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.40242207050323486 norm:0.0030875380616635084 max memory_allocated 29270.18798828125 
[2025-02-18 17:06:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.3995499610900879 norm:0.0026108906604349613 max memory_allocated 29270.18798828125 
[2025-02-18 17:06:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.39809566736221313 norm:0.002369069494307041 max memory_allocated 29270.18798828125 
[2025-02-18 17:07:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.3966183364391327 norm:0.0022287475876510143 max memory_allocated 29270.18798828125 
[2025-02-18 17:08:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.39477643370628357 norm:0.0021445578895509243 max memory_allocated 29270.18798828125 
[2025-02-18 17:09:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.3940288722515106 norm:0.0019993360619992018 max memory_allocated 29270.18798828125 
[2025-02-18 17:10:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.3935113251209259 norm:0.001916217734105885 max memory_allocated 29270.18798828125 
[2025-02-18 17:11:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.3923685550689697 norm:0.0018523478647693992 max memory_allocated 29270.18798828125 
[2025-02-18 17:11:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.39219287037849426 norm:0.0017989840125665069 max memory_allocated 29270.18798828125 
[2025-02-18 17:12:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.3919052481651306 norm:0.0017316638259217143 max memory_allocated 29270.18798828125 
[2025-02-18 17:13:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.3911888599395752 norm:0.0016732901567593217 max memory_allocated 29270.18798828125 
[2025-02-18 17:14:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.39129477739334106 norm:0.0016584028489887714 max memory_allocated 29270.18798828125 
[2025-02-18 17:15:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.39131540060043335 norm:0.0016496486496180296 max memory_allocated 29270.18798828125 
[2025-02-18 17:15:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.3915289640426636 norm:0.0016363081522285938 max memory_allocated 29270.18798828125 
[2025-02-18 17:16:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.3919205069541931 norm:0.0016653743805363774 max memory_allocated 29270.18798828125 
[2025-02-18 17:17:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.3918362259864807 norm:0.001647631055675447 max memory_allocated 29270.18798828125 
[2025-02-18 17:18:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.392128586769104 norm:0.0016886721132323146 max memory_allocated 29270.18798828125 
[2025-02-18 17:18:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 17:19:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.48220381140708923 norm:0.033384207636117935 max memory_allocated 29270.75048828125 
[2025-02-18 17:20:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.4599483907222748 norm:0.020340217277407646 max memory_allocated 29270.75048828125 
[2025-02-18 17:21:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.4509604275226593 norm:0.014311669394373894 max memory_allocated 29270.75048828125 
[2025-02-18 17:22:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.4432966113090515 norm:0.010902725160121918 max memory_allocated 29270.75048828125 
[2025-02-18 17:22:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.44069865345954895 norm:0.00860688928514719 max memory_allocated 29270.75048828125 
[2025-02-18 17:23:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.43794235587120056 norm:0.007025891914963722 max memory_allocated 29270.75048828125 
[2025-02-18 17:24:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.4355810284614563 norm:0.005540854763239622 max memory_allocated 29270.75048828125 
[2025-02-18 17:25:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.4323163628578186 norm:0.004347932059317827 max memory_allocated 29270.75048828125 
[2025-02-18 17:26:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.4304887056350708 norm:0.003648214740678668 max memory_allocated 29270.75048828125 
[2025-02-18 17:27:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.43062204122543335 norm:0.0031897537410259247 max memory_allocated 29270.75048828125 
[2025-02-18 17:27:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.42924660444259644 norm:0.002850155346095562 max memory_allocated 29270.75048828125 
[2025-02-18 17:28:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.4275008738040924 norm:0.0026268898509442806 max memory_allocated 29270.75048828125 
[2025-02-18 17:29:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.42763006687164307 norm:0.0024215737357735634 max memory_allocated 29270.75048828125 
[2025-02-18 17:30:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.4280852675437927 norm:0.002362092025578022 max memory_allocated 29270.75048828125 
[2025-02-18 17:31:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.42771971225738525 norm:0.0022764215245842934 max memory_allocated 29270.75048828125 
[2025-02-18 17:32:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.4265967607498169 norm:0.0021460119169205427 max memory_allocated 29270.75048828125 
[2025-02-18 17:32:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.42606911063194275 norm:0.0021206955425441265 max memory_allocated 29270.75048828125 
[2025-02-18 17:33:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.42580485343933105 norm:0.0020379316993057728 max memory_allocated 29270.75048828125 
[2025-02-18 17:34:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.4257151782512665 norm:0.0019866004586219788 max memory_allocated 29270.75048828125 
[2025-02-18 17:35:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.4253804087638855 norm:0.0019433170091360807 max memory_allocated 29270.75048828125 
[2025-02-18 17:35:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 17:36:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.5609755516052246 norm:0.02476460300385952 max memory_allocated 29270.93798828125 
[2025-02-18 17:37:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.5286110043525696 norm:0.013012975454330444 max memory_allocated 29270.93798828125 
[2025-02-18 17:38:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.5121654868125916 norm:0.008972255513072014 max memory_allocated 29270.93798828125 
[2025-02-18 17:39:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.5020046830177307 norm:0.006539757363498211 max memory_allocated 29270.93798828125 
[2025-02-18 17:39:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.4962238669395447 norm:0.00511364359408617 max memory_allocated 29270.93798828125 
[2025-02-18 17:40:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.4905712306499481 norm:0.003989384975284338 max memory_allocated 29270.93798828125 
[2025-02-18 17:41:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.486614853143692 norm:0.0032737674191594124 max memory_allocated 29270.93798828125 
[2025-02-18 17:42:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.4848296642303467 norm:0.003092328319326043 max memory_allocated 29270.93798828125 
[2025-02-18 17:43:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.48327288031578064 norm:0.002853148616850376 max memory_allocated 29270.93798828125 
[2025-02-18 17:44:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.48296216130256653 norm:0.0027477461844682693 max memory_allocated 29270.93798828125 
[2025-02-18 17:44:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.48157504200935364 norm:0.002618900965899229 max memory_allocated 29270.93798828125 
[2025-02-18 17:45:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.4813748598098755 norm:0.002555719343945384 max memory_allocated 29270.93798828125 
[2025-02-18 17:46:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.48106300830841064 norm:0.0025156508199870586 max memory_allocated 29270.93798828125 
[2025-02-18 17:47:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.48138904571533203 norm:0.00257129967212677 max memory_allocated 29270.93798828125 
[2025-02-18 17:48:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.4807160794734955 norm:0.0024200784973800182 max memory_allocated 29270.93798828125 
[2025-02-18 17:49:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.4806586802005768 norm:0.002382962265983224 max memory_allocated 29270.93798828125 
[2025-02-18 17:49:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.47977587580680847 norm:0.0023304414935410023 max memory_allocated 29270.93798828125 
[2025-02-18 17:50:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.47944003343582153 norm:0.002354822587221861 max memory_allocated 29270.93798828125 
[2025-02-18 17:51:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.4792580008506775 norm:0.0023293846752494574 max memory_allocated 29270.93798828125 
[2025-02-18 17:52:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.4794079661369324 norm:0.002309344010427594 max memory_allocated 29270.93798828125 
[2025-02-18 17:52:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 17:53:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.5767309665679932 norm:0.02113850973546505 max memory_allocated 29271.12548828125 
[2025-02-18 17:54:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.5526366233825684 norm:0.011171195656061172 max memory_allocated 29271.12548828125 
[2025-02-18 17:55:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.5398076772689819 norm:0.007170538883656263 max memory_allocated 29271.12548828125 
[2025-02-18 17:56:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.5327978134155273 norm:0.005185380578041077 max memory_allocated 29271.12548828125 
[2025-02-18 17:56:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.5294756889343262 norm:0.0043960437178611755 max memory_allocated 29271.12548828125 
[2025-02-18 17:57:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.5271745920181274 norm:0.003987213131040335 max memory_allocated 29271.12548828125 
[2025-02-18 17:58:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.5262784361839294 norm:0.0037346533499658108 max memory_allocated 29271.12548828125 
[2025-02-18 17:59:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.5240001678466797 norm:0.003309505293145776 max memory_allocated 29271.12548828125 
[2025-02-18 18:00:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.5228844285011292 norm:0.003077466506510973 max memory_allocated 29271.12548828125 
[2025-02-18 18:01:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.522344172000885 norm:0.0029382770881056786 max memory_allocated 29271.12548828125 
[2025-02-18 18:01:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.5210868120193481 norm:0.0026637318078428507 max memory_allocated 29271.12548828125 
[2025-02-18 18:02:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.5198062062263489 norm:0.0024493448436260223 max memory_allocated 29271.12548828125 
[2025-02-18 18:03:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.5184804201126099 norm:0.002350706374272704 max memory_allocated 29271.12548828125 
[2025-02-18 18:04:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.5182853937149048 norm:0.0022783661261200905 max memory_allocated 29271.12548828125 
[2025-02-18 18:05:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.5167875289916992 norm:0.00217882776632905 max memory_allocated 29271.12548828125 
[2025-02-18 18:06:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.5164452791213989 norm:0.002125006401911378 max memory_allocated 29271.12548828125 
[2025-02-18 18:06:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.5155626535415649 norm:0.0020684481132775545 max memory_allocated 29271.12548828125 
[2025-02-18 18:07:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.515177845954895 norm:0.0020586736500263214 max memory_allocated 29271.12548828125 
[2025-02-18 18:08:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.5154905915260315 norm:0.0020479809027165174 max memory_allocated 29271.12548828125 
[2025-02-18 18:09:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.5164609551429749 norm:0.0020658131688833237 max memory_allocated 29271.12548828125 
[2025-02-18 18:09:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 18:10:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.5989261865615845 norm:0.021148230880498886 max memory_allocated 29271.31298828125 
[2025-02-18 18:11:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.5793943405151367 norm:0.0102402213960886 max memory_allocated 29271.31298828125 
[2025-02-18 18:12:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.5695904493331909 norm:0.007172059267759323 max memory_allocated 29271.31298828125 
[2025-02-18 18:13:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.5651991963386536 norm:0.00553114665672183 max memory_allocated 29271.31298828125 
[2025-02-18 18:13:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.5615162253379822 norm:0.004589067306369543 max memory_allocated 29271.31298828125 
[2025-02-18 18:14:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.5595937371253967 norm:0.004007888026535511 max memory_allocated 29271.31298828125 
[2025-02-18 18:15:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.5581623315811157 norm:0.0034321709536015987 max memory_allocated 29271.31298828125 
[2025-02-18 18:16:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.5567991733551025 norm:0.0030175878200680017 max memory_allocated 29271.31298828125 
[2025-02-18 18:17:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.5564627051353455 norm:0.0027673770673573017 max memory_allocated 29271.31298828125 
[2025-02-18 18:18:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.5563936233520508 norm:0.0025714500807225704 max memory_allocated 29271.31298828125 
[2025-02-18 18:18:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.5552400350570679 norm:0.0023910216987133026 max memory_allocated 29271.31298828125 
[2025-02-18 18:19:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.5546729564666748 norm:0.0022622020915150642 max memory_allocated 29271.31298828125 
[2025-02-18 18:20:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.5548762679100037 norm:0.0022223007399588823 max memory_allocated 29271.31298828125 
[2025-02-18 18:21:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.5543320178985596 norm:0.002172084990888834 max memory_allocated 29271.31298828125 
[2025-02-18 18:22:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.5531917214393616 norm:0.002065105829387903 max memory_allocated 29271.31298828125 
[2025-02-18 18:23:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.5521862506866455 norm:0.001972637604922056 max memory_allocated 29271.31298828125 
[2025-02-18 18:23:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.5519000291824341 norm:0.0019231997430324554 max memory_allocated 29271.31298828125 
[2025-02-18 18:24:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.5521602630615234 norm:0.0018905655015259981 max memory_allocated 29271.31298828125 
[2025-02-18 18:25:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.5519348382949829 norm:0.0018520044395700097 max memory_allocated 29271.31298828125 
[2025-02-18 18:26:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.5517735481262207 norm:0.0018042665906250477 max memory_allocated 29271.31298828125 
[2025-02-18 18:26:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 18:27:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.6408871412277222 norm:0.018160875886678696 max memory_allocated 29271.31298828125 
[2025-02-18 18:28:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.6129245162010193 norm:0.00899418257176876 max memory_allocated 29271.31298828125 
[2025-02-18 18:29:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.6020440459251404 norm:0.006097392179071903 max memory_allocated 29271.31298828125 
[2025-02-18 18:30:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.5971064567565918 norm:0.004850482102483511 max memory_allocated 29271.31298828125 
[2025-02-18 18:30:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.5907778739929199 norm:0.00377809745259583 max memory_allocated 29271.31298828125 
[2025-02-18 18:31:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.587275505065918 norm:0.0031087149400264025 max memory_allocated 29271.31298828125 
[2025-02-18 18:32:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.5862873792648315 norm:0.00271305488422513 max memory_allocated 29271.31298828125 
[2025-02-18 18:33:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.5835846066474915 norm:0.0022965767420828342 max memory_allocated 29271.31298828125 
[2025-02-18 18:34:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.5826990604400635 norm:0.002180111361667514 max memory_allocated 29271.31298828125 
[2025-02-18 18:35:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.5808451771736145 norm:0.002018571365624666 max memory_allocated 29271.31298828125 
[2025-02-18 18:35:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.5812584161758423 norm:0.0019938445184379816 max memory_allocated 29271.31298828125 
[2025-02-18 18:36:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.5811254978179932 norm:0.0019599278457462788 max memory_allocated 29271.31298828125 
[2025-02-18 18:37:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.5797786712646484 norm:0.0019182207761332393 max memory_allocated 29271.31298828125 
[2025-02-18 18:38:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.5789353251457214 norm:0.0018251739675179124 max memory_allocated 29271.31298828125 
[2025-02-18 18:39:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.5796794891357422 norm:0.0018042243318632245 max memory_allocated 29271.31298828125 
[2025-02-18 18:40:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.5797532796859741 norm:0.0017513445345684886 max memory_allocated 29271.31298828125 
[2025-02-18 18:40:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.5772038698196411 norm:0.0016342890448868275 max memory_allocated 29271.31298828125 
[2025-02-18 18:41:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.5756627917289734 norm:0.0015932363457977772 max memory_allocated 29271.31298828125 
[2025-02-18 18:42:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.5750545859336853 norm:0.0015451707877218723 max memory_allocated 29271.31298828125 
[2025-02-18 18:43:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.5754451751708984 norm:0.0015439328271895647 max memory_allocated 29271.31298828125 
[2025-02-18 18:43:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 18:44:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.6617293953895569 norm:0.019764529541134834 max memory_allocated 29271.68798828125 
[2025-02-18 18:45:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.6360312700271606 norm:0.010241112671792507 max memory_allocated 29271.68798828125 
[2025-02-18 18:46:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.6260528564453125 norm:0.006997123826295137 max memory_allocated 29271.68798828125 
[2025-02-18 18:47:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.6215200424194336 norm:0.005418622866272926 max memory_allocated 29271.68798828125 
[2025-02-18 18:47:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.6184480786323547 norm:0.004443765617907047 max memory_allocated 29271.68798828125 
[2025-02-18 18:48:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.617973804473877 norm:0.003924825228750706 max memory_allocated 29271.68798828125 
[2025-02-18 18:49:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.6161946058273315 norm:0.0036248755641281605 max memory_allocated 29271.68798828125 
[2025-02-18 18:50:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.6151531934738159 norm:0.0032253358513116837 max memory_allocated 29271.68798828125 
[2025-02-18 18:51:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.6130015254020691 norm:0.0027781727258116007 max memory_allocated 29271.68798828125 
[2025-02-18 18:52:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.6095151901245117 norm:0.0025029806420207024 max memory_allocated 29271.68798828125 
[2025-02-18 18:52:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.6077592372894287 norm:0.00242606527172029 max memory_allocated 29271.68798828125 
[2025-02-18 18:53:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.6065055131912231 norm:0.0021819171961396933 max memory_allocated 29271.68798828125 
[2025-02-18 18:54:32 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.6066039800643921 norm:0.002223104238510132 max memory_allocated 29271.68798828125 
[2025-02-18 18:55:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.6051926016807556 norm:0.0021451713982969522 max memory_allocated 29271.68798828125 
[2025-02-18 18:56:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.6045553088188171 norm:0.0021394717041403055 max memory_allocated 29271.68798828125 
[2025-02-18 18:57:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.6042811274528503 norm:0.002070391783490777 max memory_allocated 29271.68798828125 
[2025-02-18 18:57:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.6033040285110474 norm:0.002035299316048622 max memory_allocated 29271.68798828125 
[2025-02-18 18:58:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.6026161313056946 norm:0.001955984393134713 max memory_allocated 29271.68798828125 
[2025-02-18 18:59:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.6020089983940125 norm:0.0019467078382149339 max memory_allocated 29271.68798828125 
[2025-02-18 19:00:21 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.6018825769424438 norm:0.0019708708859980106 max memory_allocated 29271.68798828125 
[2025-02-18 19:00:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 19:01:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.675025999546051 norm:0.01952420547604561 max memory_allocated 29271.87548828125 
[2025-02-18 19:02:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.650113046169281 norm:0.010820403695106506 max memory_allocated 29271.87548828125 
[2025-02-18 19:03:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.6388411521911621 norm:0.007255286909639835 max memory_allocated 29271.87548828125 
[2025-02-18 19:04:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.6346853971481323 norm:0.005837501026690006 max memory_allocated 29271.87548828125 
[2025-02-18 19:04:55 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.6322532296180725 norm:0.00501085352152586 max memory_allocated 29271.87548828125 
[2025-02-18 19:05:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.6292705535888672 norm:0.004267532844096422 max memory_allocated 29271.87548828125 
[2025-02-18 19:06:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.627402663230896 norm:0.003766148816794157 max memory_allocated 29271.87548828125 
[2025-02-18 19:07:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.6251950860023499 norm:0.0033042393624782562 max memory_allocated 29271.87548828125 
[2025-02-18 19:08:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.6240816712379456 norm:0.002983715385198593 max memory_allocated 29271.87548828125 
[2025-02-18 19:09:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.6242854595184326 norm:0.0027775168418884277 max memory_allocated 29271.87548828125 
[2025-02-18 19:09:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.623674213886261 norm:0.002604137174785137 max memory_allocated 29271.87548828125 
[2025-02-18 19:10:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.6229425072669983 norm:0.002458546543493867 max memory_allocated 29271.87548828125 
[2025-02-18 19:11:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.6215939521789551 norm:0.002270608441904187 max memory_allocated 29271.87548828125 
[2025-02-18 19:12:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.6209179162979126 norm:0.0021483798045665026 max memory_allocated 29271.87548828125 
[2025-02-18 19:13:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.620753288269043 norm:0.0020842733792960644 max memory_allocated 29271.87548828125 
[2025-02-18 19:14:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.6202681660652161 norm:0.0020430716685950756 max memory_allocated 29271.87548828125 
[2025-02-18 19:14:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.619475781917572 norm:0.0019098544726148248 max memory_allocated 29271.87548828125 
[2025-02-18 19:15:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.6195037364959717 norm:0.0018473342061042786 max memory_allocated 29271.87548828125 
[2025-02-18 19:16:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.619867205619812 norm:0.0017917363438755274 max memory_allocated 29271.87548828125 
[2025-02-18 19:17:22 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.6200357675552368 norm:0.0018152415286749601 max memory_allocated 29271.87548828125 
[2025-02-18 19:17:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 19:18:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.6736922860145569 norm:0.012445569038391113 max memory_allocated 29272.06298828125 
[2025-02-18 19:19:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.6567507982254028 norm:0.006999127566814423 max memory_allocated 29272.06298828125 
[2025-02-18 19:20:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.6484901905059814 norm:0.005108719691634178 max memory_allocated 29272.06298828125 
[2025-02-18 19:21:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.6443872451782227 norm:0.0042076618410646915 max memory_allocated 29272.06298828125 
[2025-02-18 19:21:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.6415884494781494 norm:0.003469101618975401 max memory_allocated 29272.06298828125 
[2025-02-18 19:22:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.6382919549942017 norm:0.002924790373072028 max memory_allocated 29272.06298828125 
[2025-02-18 19:23:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.6355104446411133 norm:0.002579569350928068 max memory_allocated 29272.06298828125 
[2025-02-18 19:24:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.6336343288421631 norm:0.002329300856217742 max memory_allocated 29272.06298828125 
[2025-02-18 19:25:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.6329082250595093 norm:0.0021966455969959497 max memory_allocated 29272.06298828125 
[2025-02-18 19:26:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.632643461227417 norm:0.0020902983378618956 max memory_allocated 29272.06298828125 
[2025-02-18 19:26:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.6332639455795288 norm:0.0020610687788575888 max memory_allocated 29272.06298828125 
[2025-02-18 19:27:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.6332477331161499 norm:0.0019736704416573048 max memory_allocated 29272.06298828125 
[2025-02-18 19:28:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.6341546773910522 norm:0.0019743661396205425 max memory_allocated 29272.06298828125 
[2025-02-18 19:29:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.6341970562934875 norm:0.0019391990499570966 max memory_allocated 29272.06298828125 
[2025-02-18 19:30:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.6335275769233704 norm:0.001863702549599111 max memory_allocated 29272.06298828125 
[2025-02-18 19:31:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.6341051459312439 norm:0.0018474161624908447 max memory_allocated 29272.06298828125 
[2025-02-18 19:31:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.6328703761100769 norm:0.001729644718579948 max memory_allocated 29272.06298828125 
[2025-02-18 19:32:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.6319462060928345 norm:0.001686124480329454 max memory_allocated 29272.06298828125 
[2025-02-18 19:33:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.6314586997032166 norm:0.0016360528534278274 max memory_allocated 29272.06298828125 
[2025-02-18 19:34:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.6311880350112915 norm:0.001606946112588048 max memory_allocated 29272.06298828125 
[2025-02-18 19:34:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 19:35:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.7098326683044434 norm:0.024618394672870636 max memory_allocated 29272.25048828125 
[2025-02-18 19:36:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.6893441677093506 norm:0.013873145915567875 max memory_allocated 29272.25048828125 
[2025-02-18 19:37:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.6785305142402649 norm:0.009199408814311028 max memory_allocated 29272.25048828125 
[2025-02-18 19:38:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.6714204549789429 norm:0.006835584994405508 max memory_allocated 29272.25048828125 
[2025-02-18 19:38:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.6679493188858032 norm:0.005463249981403351 max memory_allocated 29272.25048828125 
[2025-02-18 19:39:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.6629371643066406 norm:0.004577594343572855 max memory_allocated 29272.25048828125 
[2025-02-18 19:40:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.6587985754013062 norm:0.003910424653440714 max memory_allocated 29272.25048828125 
[2025-02-18 19:41:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.6563019752502441 norm:0.003537393407896161 max memory_allocated 29272.25048828125 
[2025-02-18 19:42:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.6553328037261963 norm:0.0033303217496722937 max memory_allocated 29272.25048828125 
[2025-02-18 19:43:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.6535555720329285 norm:0.003096414264291525 max memory_allocated 29272.25048828125 
[2025-02-18 19:43:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.6519115567207336 norm:0.002828684402629733 max memory_allocated 29272.25048828125 
[2025-02-18 19:44:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.6499960422515869 norm:0.002541992114856839 max memory_allocated 29272.25048828125 
[2025-02-18 19:45:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.6492676138877869 norm:0.0023536840453743935 max memory_allocated 29272.25048828125 
[2025-02-18 19:46:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.6490835547447205 norm:0.002248887438327074 max memory_allocated 29272.25048828125 
[2025-02-18 19:47:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.6483322381973267 norm:0.0021514391992241144 max memory_allocated 29272.25048828125 
[2025-02-18 19:48:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.6483301520347595 norm:0.0021106721833348274 max memory_allocated 29272.25048828125 
[2025-02-18 19:48:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.6478581428527832 norm:0.002071217866614461 max memory_allocated 29272.25048828125 
[2025-02-18 19:49:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.6482270359992981 norm:0.0020815087482333183 max memory_allocated 29272.25048828125 
[2025-02-18 19:50:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.648314893245697 norm:0.0020666304044425488 max memory_allocated 29272.25048828125 
[2025-02-18 19:51:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.6485533118247986 norm:0.002087984699755907 max memory_allocated 29272.25048828125 
[2025-02-18 19:51:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 19:52:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.7141104936599731 norm:0.032161250710487366 max memory_allocated 29272.43798828125 
[2025-02-18 19:53:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.6946356296539307 norm:0.018613070249557495 max memory_allocated 29272.43798828125 
[2025-02-18 19:54:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.6850677728652954 norm:0.012521489523351192 max memory_allocated 29272.43798828125 
[2025-02-18 19:55:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.6796385645866394 norm:0.009703120216727257 max memory_allocated 29272.43798828125 
[2025-02-18 19:55:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.6760731339454651 norm:0.007870380766689777 max memory_allocated 29272.43798828125 
[2025-02-18 19:56:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.6727538704872131 norm:0.00636825617402792 max memory_allocated 29272.43798828125 
[2025-02-18 19:57:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.668864905834198 norm:0.005287020467221737 max memory_allocated 29272.43798828125 
[2025-02-18 19:58:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.6670883893966675 norm:0.004718330223113298 max memory_allocated 29272.43798828125 
[2025-02-18 19:59:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.6671246290206909 norm:0.004337583668529987 max memory_allocated 29272.43798828125 
[2025-02-18 19:59:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.6665273904800415 norm:0.004196017514914274 max memory_allocated 29272.43798828125 
[2025-02-18 20:00:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.6660635471343994 norm:0.004043838940560818 max memory_allocated 29272.43798828125 
[2025-02-18 20:01:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.6659441590309143 norm:0.0038913022726774216 max memory_allocated 29272.43798828125 
[2025-02-18 20:02:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.6661927700042725 norm:0.0036327671259641647 max memory_allocated 29272.43798828125 
[2025-02-18 20:03:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.664176344871521 norm:0.003391642589122057 max memory_allocated 29272.43798828125 
[2025-02-18 20:04:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.6618611216545105 norm:0.0031118448823690414 max memory_allocated 29272.43798828125 
[2025-02-18 20:04:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.661597728729248 norm:0.002938850549980998 max memory_allocated 29272.43798828125 
[2025-02-18 20:05:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.6614120602607727 norm:0.0029133108910173178 max memory_allocated 29272.43798828125 
[2025-02-18 20:06:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.6607199907302856 norm:0.002827331190928817 max memory_allocated 29272.43798828125 
[2025-02-18 20:07:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.6603005528450012 norm:0.002745564328506589 max memory_allocated 29272.43798828125 
[2025-02-18 20:08:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.6596450805664062 norm:0.0026838923804461956 max memory_allocated 29272.43798828125 
[2025-02-18 20:08:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 20:09:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.7374798655509949 norm:0.0398017019033432 max memory_allocated 29272.43798828125 
[2025-02-18 20:10:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.7121905088424683 norm:0.02138581871986389 max memory_allocated 29272.43798828125 
[2025-02-18 20:11:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.7025318741798401 norm:0.014792267233133316 max memory_allocated 29272.43798828125 
[2025-02-18 20:11:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.6972764134407043 norm:0.011539921164512634 max memory_allocated 29272.43798828125 
[2025-02-18 20:12:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.693408727645874 norm:0.008986142463982105 max memory_allocated 29272.43798828125 
[2025-02-18 20:13:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.6892505884170532 norm:0.007233419455587864 max memory_allocated 29272.43798828125 
[2025-02-18 20:14:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.6864448189735413 norm:0.006200240459293127 max memory_allocated 29272.43798828125 
[2025-02-18 20:15:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.6845138072967529 norm:0.005387155804783106 max memory_allocated 29272.43798828125 
[2025-02-18 20:16:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.6846835017204285 norm:0.005214031785726547 max memory_allocated 29272.43798828125 
[2025-02-18 20:16:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.6841069459915161 norm:0.004824729636311531 max memory_allocated 29272.43798828125 
[2025-02-18 20:17:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.6834608912467957 norm:0.0044439854100346565 max memory_allocated 29272.43798828125 
[2025-02-18 20:18:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.6836525201797485 norm:0.004198547452688217 max memory_allocated 29272.43798828125 
[2025-02-18 20:19:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.6823101043701172 norm:0.0038318680599331856 max memory_allocated 29272.43798828125 
[2025-02-18 20:20:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.6805142164230347 norm:0.0034739826805889606 max memory_allocated 29272.43798828125 
[2025-02-18 20:21:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.6793765425682068 norm:0.0032400276977568865 max memory_allocated 29272.43798828125 
[2025-02-18 20:21:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.6784752607345581 norm:0.0030408003367483616 max memory_allocated 29272.43798828125 
[2025-02-18 20:22:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.678108811378479 norm:0.002885814756155014 max memory_allocated 29272.43798828125 
[2025-02-18 20:23:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.6769676804542542 norm:0.0026991991326212883 max memory_allocated 29272.43798828125 
[2025-02-18 20:24:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.6763508915901184 norm:0.0026105327997356653 max memory_allocated 29272.43798828125 
[2025-02-18 20:25:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.6752464771270752 norm:0.002378877019509673 max memory_allocated 29272.43798828125 
[2025-02-18 20:25:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 20:26:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.748393177986145 norm:0.01843775250017643 max memory_allocated 29272.81298828125 
[2025-02-18 20:27:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.7323505282402039 norm:0.010031420737504959 max memory_allocated 29272.81298828125 
[2025-02-18 20:28:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.7259400486946106 norm:0.007538340520113707 max memory_allocated 29272.81298828125 
[2025-02-18 20:28:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.7223113179206848 norm:0.006113273091614246 max memory_allocated 29272.81298828125 
[2025-02-18 20:29:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.7193182110786438 norm:0.005128622520714998 max memory_allocated 29272.81298828125 
[2025-02-18 20:30:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.7161294221878052 norm:0.004194086417555809 max memory_allocated 29272.81298828125 
[2025-02-18 20:31:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.7136778831481934 norm:0.0035345610231161118 max memory_allocated 29272.81298828125 
[2025-02-18 20:32:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.7122637033462524 norm:0.0031891786493360996 max memory_allocated 29272.81298828125 
[2025-02-18 20:33:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.7105779051780701 norm:0.0029789521358907223 max memory_allocated 29272.81298828125 
[2025-02-18 20:33:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.710355281829834 norm:0.0028200079686939716 max memory_allocated 29272.81298828125 
[2025-02-18 20:34:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.7102050185203552 norm:0.0028257977683097124 max memory_allocated 29272.81298828125 
[2025-02-18 20:35:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.7097295522689819 norm:0.0025790103245526552 max memory_allocated 29272.81298828125 
[2025-02-18 20:36:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.7094252109527588 norm:0.002464042045176029 max memory_allocated 29272.81298828125 
[2025-02-18 20:37:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.7091919779777527 norm:0.002443371806293726 max memory_allocated 29272.81298828125 
[2025-02-18 20:38:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.7098175287246704 norm:0.002344151260331273 max memory_allocated 29272.81298828125 
[2025-02-18 20:38:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.7099659442901611 norm:0.0022973648738116026 max memory_allocated 29272.81298828125 
[2025-02-18 20:39:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.7092878818511963 norm:0.0022211961913853884 max memory_allocated 29272.81298828125 
[2025-02-18 20:40:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.7094959020614624 norm:0.002214651321992278 max memory_allocated 29272.81298828125 
[2025-02-18 20:41:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.7093974351882935 norm:0.0021241677459329367 max memory_allocated 29272.81298828125 
[2025-02-18 20:42:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.7088043689727783 norm:0.0021104984916746616 max memory_allocated 29272.81298828125 
[2025-02-18 20:42:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 20:43:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.7899964451789856 norm:0.018778249621391296 max memory_allocated 29273.00048828125 
[2025-02-18 20:44:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.7754525542259216 norm:0.010364580899477005 max memory_allocated 29273.00048828125 
[2025-02-18 20:45:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.7690975666046143 norm:0.007613783702254295 max memory_allocated 29273.00048828125 
[2025-02-18 20:45:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.7653896808624268 norm:0.0061184074729681015 max memory_allocated 29273.00048828125 
[2025-02-18 20:46:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.7626315951347351 norm:0.005109802354127169 max memory_allocated 29273.00048828125 
[2025-02-18 20:47:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.7609312534332275 norm:0.0043490915559232235 max memory_allocated 29273.00048828125 
[2025-02-18 20:48:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.7588939666748047 norm:0.0037810925859957933 max memory_allocated 29273.00048828125 
[2025-02-18 20:49:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.7564717531204224 norm:0.003298770869150758 max memory_allocated 29273.00048828125 
[2025-02-18 20:50:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.7554060816764832 norm:0.0028827101923525333 max memory_allocated 29273.00048828125 
[2025-02-18 20:50:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.7540876269340515 norm:0.0026679513975977898 max memory_allocated 29273.00048828125 
[2025-02-18 20:51:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.7530792355537415 norm:0.0024916459806263447 max memory_allocated 29273.00048828125 
[2025-02-18 20:52:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.7526344060897827 norm:0.0023875990882515907 max memory_allocated 29273.00048828125 
[2025-02-18 20:53:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.7522263526916504 norm:0.002299371175467968 max memory_allocated 29273.00048828125 
[2025-02-18 20:54:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.7522958517074585 norm:0.002371327020227909 max memory_allocated 29273.00048828125 
[2025-02-18 20:55:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.7520185708999634 norm:0.0022902742493897676 max memory_allocated 29273.00048828125 
[2025-02-18 20:55:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.7522631883621216 norm:0.0022498820908367634 max memory_allocated 29273.00048828125 
[2025-02-18 20:56:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.7528244853019714 norm:0.002177222864702344 max memory_allocated 29273.00048828125 
[2025-02-18 20:57:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.7528657913208008 norm:0.00209795031696558 max memory_allocated 29273.00048828125 
[2025-02-18 20:58:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.7531528472900391 norm:0.0022998570930212736 max memory_allocated 29273.00048828125 
[2025-02-18 20:59:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.752443790435791 norm:0.002012239070609212 max memory_allocated 29273.00048828125 
[2025-02-18 20:59:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 21:00:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.8476746082305908 norm:0.016446441411972046 max memory_allocated 29273.18798828125 
[2025-02-18 21:01:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.8350945115089417 norm:0.010347946546971798 max memory_allocated 29273.18798828125 
[2025-02-18 21:02:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.8282380104064941 norm:0.007767493836581707 max memory_allocated 29273.18798828125 
[2025-02-18 21:02:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.8240942358970642 norm:0.006274484563618898 max memory_allocated 29273.18798828125 
[2025-02-18 21:03:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.8215329051017761 norm:0.005173265002667904 max memory_allocated 29273.18798828125 
[2025-02-18 21:04:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.8197731375694275 norm:0.004535582847893238 max memory_allocated 29273.18798828125 
[2025-02-18 21:05:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.8177057504653931 norm:0.003978588152676821 max memory_allocated 29273.18798828125 
[2025-02-18 21:06:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.8157321810722351 norm:0.0034492667764425278 max memory_allocated 29273.18798828125 
[2025-02-18 21:07:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.8148306608200073 norm:0.0031936606392264366 max memory_allocated 29273.18798828125 
[2025-02-18 21:07:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.8131387233734131 norm:0.00283468933776021 max memory_allocated 29273.18798828125 
[2025-02-18 21:08:43 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.8127631545066833 norm:0.0027181904297322035 max memory_allocated 29273.18798828125 
[2025-02-18 21:09:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.8125460147857666 norm:0.00259331613779068 max memory_allocated 29273.18798828125 
[2025-02-18 21:10:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.8118374943733215 norm:0.0024709636345505714 max memory_allocated 29273.18798828125 
[2025-02-18 21:11:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.8117899298667908 norm:0.0023681893944740295 max memory_allocated 29273.18798828125 
[2025-02-18 21:12:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.8118217587471008 norm:0.0023092494811862707 max memory_allocated 29273.18798828125 
[2025-02-18 21:12:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.8108841180801392 norm:0.0022165910340845585 max memory_allocated 29273.18798828125 
[2025-02-18 21:13:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.8099362254142761 norm:0.0021019813138991594 max memory_allocated 29273.18798828125 
[2025-02-18 21:14:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.8098084330558777 norm:0.0020562780555337667 max memory_allocated 29273.18798828125 
[2025-02-18 21:15:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.8092542886734009 norm:0.002037565689533949 max memory_allocated 29273.18798828125 
[2025-02-18 21:16:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.8082928657531738 norm:0.002037437865510583 max memory_allocated 29273.18798828125 
[2025-02-18 21:16:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 21:17:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.9047753810882568 norm:0.014985772781074047 max memory_allocated 29273.18798828125 
[2025-02-18 21:18:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.8945447206497192 norm:0.010131041519343853 max memory_allocated 29273.18798828125 
[2025-02-18 21:19:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.888809859752655 norm:0.00767895532771945 max memory_allocated 29273.18798828125 
[2025-02-18 21:19:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.8868247270584106 norm:0.006330262403935194 max memory_allocated 29273.18798828125 
[2025-02-18 21:20:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.8841838836669922 norm:0.005071064457297325 max memory_allocated 29273.18798828125 
[2025-02-18 21:21:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.8830480575561523 norm:0.004402452614158392 max memory_allocated 29273.18798828125 
[2025-02-18 21:22:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.8823778033256531 norm:0.0038342608604580164 max memory_allocated 29273.18798828125 
[2025-02-18 21:23:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.8797966241836548 norm:0.0032805302180349827 max memory_allocated 29273.18798828125 
[2025-02-18 21:24:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.8784343004226685 norm:0.0028810263611376286 max memory_allocated 29273.18798828125 
[2025-02-18 21:24:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.8777397274971008 norm:0.0025870308745652437 max memory_allocated 29273.18798828125 
[2025-02-18 21:25:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.8774355053901672 norm:0.002404160099104047 max memory_allocated 29273.18798828125 
[2025-02-18 21:26:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.8770161271095276 norm:0.002239816589280963 max memory_allocated 29273.18798828125 
[2025-02-18 21:27:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.8765496611595154 norm:0.002106793923303485 max memory_allocated 29273.18798828125 
[2025-02-18 21:28:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.8753337860107422 norm:0.001995327416807413 max memory_allocated 29273.18798828125 
[2025-02-18 21:29:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.8754785656929016 norm:0.0019135461188852787 max memory_allocated 29273.18798828125 
[2025-02-18 21:29:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.8755255937576294 norm:0.0018810470355674624 max memory_allocated 29273.18798828125 
[2025-02-18 21:30:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.8754702210426331 norm:0.0019030588446184993 max memory_allocated 29273.18798828125 
[2025-02-18 21:31:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.8744927644729614 norm:0.00178242358379066 max memory_allocated 29273.18798828125 
[2025-02-18 21:32:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.8747936487197876 norm:0.0018102985341101885 max memory_allocated 29273.18798828125 
[2025-02-18 21:33:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.8738783597946167 norm:0.0017543572466820478 max memory_allocated 29273.18798828125 
[2025-02-18 21:33:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 21:34:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.9814907908439636 norm:0.02574067935347557 max memory_allocated 29273.56298828125 
[2025-02-18 21:35:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.9711613655090332 norm:0.016620926558971405 max memory_allocated 29273.56298828125 
[2025-02-18 21:36:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.9660167098045349 norm:0.012058684602379799 max memory_allocated 29273.56298828125 
[2025-02-18 21:36:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.9626573324203491 norm:0.009291496127843857 max memory_allocated 29273.56298828125 
[2025-02-18 21:37:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.9597975611686707 norm:0.007496306672692299 max memory_allocated 29273.56298828125 
[2025-02-18 21:38:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.9576660990715027 norm:0.006234961096197367 max memory_allocated 29273.56298828125 
[2025-02-18 21:39:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.9567492604255676 norm:0.00533443596214056 max memory_allocated 29273.56298828125 
[2025-02-18 21:40:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.9559888243675232 norm:0.004731814842671156 max memory_allocated 29273.56298828125 
[2025-02-18 21:41:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.9556170105934143 norm:0.004279954358935356 max memory_allocated 29273.56298828125 
[2025-02-18 21:41:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.955672562122345 norm:0.003816023236140609 max memory_allocated 29273.56298828125 
[2025-02-18 21:42:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.9548484683036804 norm:0.0035439725033938885 max memory_allocated 29273.56298828125 
[2025-02-18 21:43:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.9534395933151245 norm:0.0032023168168962 max memory_allocated 29273.56298828125 
[2025-02-18 21:44:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.9535016417503357 norm:0.0029068232979625463 max memory_allocated 29273.56298828125 
[2025-02-18 21:45:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.9531991481781006 norm:0.0027764057740569115 max memory_allocated 29273.56298828125 
[2025-02-18 21:46:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.951653003692627 norm:0.0026384673546999693 max memory_allocated 29273.56298828125 
[2025-02-18 21:46:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.9522472023963928 norm:0.002554106991738081 max memory_allocated 29273.56298828125 
[2025-02-18 21:47:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.9519914388656616 norm:0.0024699990171939135 max memory_allocated 29273.56298828125 
[2025-02-18 21:48:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.9523072838783264 norm:0.0023207792546600103 max memory_allocated 29273.56298828125 
[2025-02-18 21:49:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.9512417316436768 norm:0.002289444673806429 max memory_allocated 29273.56298828125 
[2025-02-18 21:50:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.9512695670127869 norm:0.002228127559646964 max memory_allocated 29273.56298828125 
[2025-02-18 21:50:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 21:51:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:1.053855299949646 norm:0.015586573630571365 max memory_allocated 29273.75048828125 
[2025-02-18 21:52:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:1.0499423742294312 norm:0.012027043849229813 max memory_allocated 29273.75048828125 
[2025-02-18 21:53:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:1.0480570793151855 norm:0.010080231353640556 max memory_allocated 29273.75048828125 
[2025-02-18 21:53:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:1.0448764562606812 norm:0.00806403998285532 max memory_allocated 29273.75048828125 
[2025-02-18 21:54:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:1.0430680513381958 norm:0.006818879395723343 max memory_allocated 29273.75048828125 
[2025-02-18 21:55:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:1.04082190990448 norm:0.005576008465141058 max memory_allocated 29273.75048828125 
[2025-02-18 21:56:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:1.0397394895553589 norm:0.004809195641428232 max memory_allocated 29273.75048828125 
[2025-02-18 21:57:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:1.038377285003662 norm:0.004456795286387205 max memory_allocated 29273.75048828125 
[2025-02-18 21:58:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:1.0377081632614136 norm:0.00405252818018198 max memory_allocated 29273.75048828125 
[2025-02-18 21:58:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:1.0376527309417725 norm:0.0037599129136651754 max memory_allocated 29273.75048828125 
[2025-02-18 21:59:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:1.0364910364151 norm:0.0034311427734792233 max memory_allocated 29273.75048828125 
[2025-02-18 22:00:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:1.0365803241729736 norm:0.00339891598559916 max memory_allocated 29273.75048828125 
[2025-02-18 22:01:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:1.0367655754089355 norm:0.003430164884775877 max memory_allocated 29273.75048828125 
[2025-02-18 22:02:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:1.0370546579360962 norm:0.0033893371000885963 max memory_allocated 29273.75048828125 
[2025-02-18 22:03:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:1.036191463470459 norm:0.0031858100555837154 max memory_allocated 29273.75048828125 
[2025-02-18 22:03:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:1.0368843078613281 norm:0.0032119951210916042 max memory_allocated 29273.75048828125 
[2025-02-18 22:04:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:1.0353013277053833 norm:0.002983536571264267 max memory_allocated 29273.75048828125 
[2025-02-18 22:05:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:1.0371285676956177 norm:0.0031109661795198917 max memory_allocated 29273.75048828125 
[2025-02-18 22:06:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:1.0364329814910889 norm:0.0029951005708426237 max memory_allocated 29273.75048828125 
[2025-02-18 22:07:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:1.034265160560608 norm:0.002985768485814333 max memory_allocated 29273.75048828125 
[2025-02-18 22:07:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 22:08:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:1.1545026302337646 norm:0.010642732493579388 max memory_allocated 29273.93798828125 
[2025-02-18 22:09:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:1.1492125988006592 norm:0.007737430278211832 max memory_allocated 29273.93798828125 
[2025-02-18 22:10:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:1.1456890106201172 norm:0.0057388367131352425 max memory_allocated 29273.93798828125 
[2025-02-18 22:10:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:1.14271879196167 norm:0.00449572317302227 max memory_allocated 29273.93798828125 
[2025-02-18 22:11:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:1.1409162282943726 norm:0.003780416212975979 max memory_allocated 29273.93798828125 
[2025-02-18 22:12:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:1.1408662796020508 norm:0.0033105688635259867 max memory_allocated 29273.93798828125 
[2025-02-18 22:13:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:1.138964056968689 norm:0.002968069165945053 max memory_allocated 29273.93798828125 
[2025-02-18 22:14:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:1.1382472515106201 norm:0.0026783437933772802 max memory_allocated 29273.93798828125 
[2025-02-18 22:15:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:1.1370090246200562 norm:0.0024914683308452368 max memory_allocated 29273.93798828125 
[2025-02-18 22:15:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:1.1376923322677612 norm:0.0023833797313272953 max memory_allocated 29273.93798828125 
[2025-02-18 22:16:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:1.1368259191513062 norm:0.0021376602817326784 max memory_allocated 29273.93798828125 
[2025-02-18 22:17:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:1.1375311613082886 norm:0.0020116723608225584 max memory_allocated 29273.93798828125 
[2025-02-18 22:18:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:1.13624906539917 norm:0.0018498378340154886 max memory_allocated 29273.93798828125 
[2025-02-18 22:19:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:1.1361783742904663 norm:0.0018296332564204931 max memory_allocated 29273.93798828125 
[2025-02-18 22:20:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:1.1353226900100708 norm:0.0016425265930593014 max memory_allocated 29273.93798828125 
[2025-02-18 22:20:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:1.1359093189239502 norm:0.0016085734823718667 max memory_allocated 29273.93798828125 
[2025-02-18 22:21:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:1.1342777013778687 norm:0.0015971451066434383 max memory_allocated 29273.93798828125 
[2025-02-18 22:22:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:1.134914517402649 norm:0.001482242252677679 max memory_allocated 29273.93798828125 
[2025-02-18 22:23:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:1.1336896419525146 norm:0.0015422679716721177 max memory_allocated 29273.93798828125 
[2025-02-18 22:24:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:1.1346006393432617 norm:0.0014863242395222187 max memory_allocated 29273.93798828125 
[2025-02-18 22:24:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 22:25:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:1.2786332368850708 norm:0.013050817884504795 max memory_allocated 29274.12548828125 
[2025-02-18 22:26:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:1.2722541093826294 norm:0.00913940742611885 max memory_allocated 29274.12548828125 
[2025-02-18 22:27:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:1.268492579460144 norm:0.0071833571419119835 max memory_allocated 29274.12548828125 
[2025-02-18 22:27:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:1.266080379486084 norm:0.005991022568196058 max memory_allocated 29274.12548828125 
[2025-02-18 22:28:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:1.2650387287139893 norm:0.005068782716989517 max memory_allocated 29274.12548828125 
[2025-02-18 22:29:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:1.2650083303451538 norm:0.004520494490861893 max memory_allocated 29274.12548828125 
[2025-02-18 22:30:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:1.263280987739563 norm:0.004055536352097988 max memory_allocated 29274.12548828125 
[2025-02-18 22:31:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:1.2628886699676514 norm:0.003599800867959857 max memory_allocated 29274.12548828125 
[2025-02-18 22:32:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:1.262963056564331 norm:0.003448018804192543 max memory_allocated 29274.12548828125 
[2025-02-18 22:32:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:1.2625813484191895 norm:0.003070671111345291 max memory_allocated 29274.12548828125 
[2025-02-18 22:33:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:1.2635034322738647 norm:0.0031520482152700424 max memory_allocated 29274.12548828125 
[2025-02-18 22:34:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:1.263952612876892 norm:0.0029831198044121265 max memory_allocated 29274.12548828125 
[2025-02-18 22:35:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:1.2611383199691772 norm:0.002622063970193267 max memory_allocated 29274.12548828125 
[2025-02-18 22:36:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:1.2644439935684204 norm:0.002626667032018304 max memory_allocated 29274.12548828125 
[2025-02-18 22:37:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:1.2632321119308472 norm:0.002569775329902768 max memory_allocated 29274.12548828125 
[2025-02-18 22:37:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:1.2645807266235352 norm:0.002551117679104209 max memory_allocated 29274.12548828125 
[2025-02-18 22:38:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:1.2645035982131958 norm:0.0026609653141349554 max memory_allocated 29274.12548828125 
[2025-02-18 22:39:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:1.2651275396347046 norm:0.002798894653096795 max memory_allocated 29274.12548828125 
[2025-02-18 22:40:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:1.2636988162994385 norm:0.0027493757661432028 max memory_allocated 29274.12548828125 
[2025-02-18 22:41:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:1.2656043767929077 norm:0.0022306947503238916 max memory_allocated 29274.12548828125 
[2025-02-18 22:41:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 22:42:28 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:1.3871268033981323 norm:0.0054963394068181515 max memory_allocated 29274.31298828125 
[2025-02-18 22:43:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:1.3829853534698486 norm:0.003930885344743729 max memory_allocated 29274.31298828125 
[2025-02-18 22:44:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:1.3810210227966309 norm:0.0031342492438852787 max memory_allocated 29274.31298828125 
[2025-02-18 22:44:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:1.3798539638519287 norm:0.002607544418424368 max memory_allocated 29274.31298828125 
[2025-02-18 22:45:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:1.378668189048767 norm:0.002308965427801013 max memory_allocated 29274.31298828125 
[2025-02-18 22:46:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:1.3776944875717163 norm:0.002076280303299427 max memory_allocated 29274.31298828125 
[2025-02-18 22:47:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:1.377697229385376 norm:0.0018872225191444159 max memory_allocated 29274.31298828125 
[2025-02-18 22:48:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:1.3770982027053833 norm:0.001789027824997902 max memory_allocated 29274.31298828125 
[2025-02-18 22:49:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:1.377662181854248 norm:0.001578501658514142 max memory_allocated 29274.31298828125 
[2025-02-18 22:49:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:1.3769434690475464 norm:0.0015768795274198055 max memory_allocated 29274.31298828125 
[2025-02-18 22:50:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:1.3768607378005981 norm:0.0015694843605160713 max memory_allocated 29274.31298828125 
[2025-02-18 22:51:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:1.3768295049667358 norm:0.0015155407600104809 max memory_allocated 29274.31298828125 
[2025-02-18 22:52:26 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:1.377116322517395 norm:0.0014624565374106169 max memory_allocated 29274.31298828125 
[2025-02-18 22:53:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:1.3765462636947632 norm:0.0014499555109068751 max memory_allocated 29274.31298828125 
[2025-02-18 22:54:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:1.3752052783966064 norm:0.0015108821680769324 max memory_allocated 29274.31298828125 
[2025-02-18 22:54:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:1.376285195350647 norm:0.0014298439491540194 max memory_allocated 29274.31298828125 
[2025-02-18 22:55:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:1.3760474920272827 norm:0.0015380512923002243 max memory_allocated 29274.31298828125 
[2025-02-18 22:56:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:1.3752896785736084 norm:0.0014188175555318594 max memory_allocated 29274.31298828125 
[2025-02-18 22:57:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:1.3753209114074707 norm:0.001353299943730235 max memory_allocated 29274.31298828125 
[2025-02-18 22:58:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:1.3748317956924438 norm:0.0013636612566187978 max memory_allocated 29274.31298828125 
[2025-02-18 22:58:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 22:59:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:1.503861427307129 norm:0.017546795308589935 max memory_allocated 29274.50048828125 
[2025-02-18 23:00:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:1.4969216585159302 norm:0.012000106275081635 max memory_allocated 29274.50048828125 
[2025-02-18 23:01:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:1.4934755563735962 norm:0.009056298062205315 max memory_allocated 29274.50048828125 
[2025-02-18 23:01:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:1.4919168949127197 norm:0.007553106639534235 max memory_allocated 29274.50048828125 
[2025-02-18 23:02:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:1.4903337955474854 norm:0.006482047960162163 max memory_allocated 29274.50048828125 
[2025-02-18 23:03:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:1.4891769886016846 norm:0.005539418663829565 max memory_allocated 29274.50048828125 
[2025-02-18 23:04:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:1.488553762435913 norm:0.004930717870593071 max memory_allocated 29274.50048828125 
[2025-02-18 23:05:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:1.488781452178955 norm:0.004399296827614307 max memory_allocated 29274.50048828125 
[2025-02-18 23:06:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:1.4870014190673828 norm:0.003828618908300996 max memory_allocated 29274.50048828125 
[2025-02-18 23:06:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:1.4875526428222656 norm:0.0037472129333764315 max memory_allocated 29274.50048828125 
[2025-02-18 23:07:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:1.4870702028274536 norm:0.0035488384310156107 max memory_allocated 29274.50048828125 
[2025-02-18 23:08:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:1.4871736764907837 norm:0.003545055165886879 max memory_allocated 29274.50048828125 
[2025-02-18 23:09:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:1.487264633178711 norm:0.003507942194119096 max memory_allocated 29274.50048828125 
[2025-02-18 23:10:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:1.4872355461120605 norm:0.003392745042219758 max memory_allocated 29274.50048828125 
[2025-02-18 23:11:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:1.4868814945220947 norm:0.0037579413037747145 max memory_allocated 29274.50048828125 
[2025-02-18 23:11:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:1.4846540689468384 norm:0.0027427971363067627 max memory_allocated 29274.50048828125 
[2025-02-18 23:12:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:1.4852021932601929 norm:0.0027492616791278124 max memory_allocated 29274.50048828125 
[2025-02-18 23:13:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:1.4873511791229248 norm:0.003725410671904683 max memory_allocated 29274.50048828125 
[2025-02-18 23:14:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:1.4853489398956299 norm:0.0029907929711043835 max memory_allocated 29274.50048828125 
[2025-02-18 23:15:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:1.4864704608917236 norm:0.003319176146760583 max memory_allocated 29274.50048828125 
[2025-02-18 23:15:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 23:16:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:1.650492548942566 norm:0.008104044012725353 max memory_allocated 29274.68798828125 
[2025-02-18 23:17:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:1.6466987133026123 norm:0.005632351152598858 max memory_allocated 29274.68798828125 
[2025-02-18 23:18:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:1.643265962600708 norm:0.004382945131510496 max memory_allocated 29274.68798828125 
[2025-02-18 23:18:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:1.6421101093292236 norm:0.0037800725549459457 max memory_allocated 29274.68798828125 
[2025-02-18 23:19:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:1.6410706043243408 norm:0.003518717596307397 max memory_allocated 29274.68798828125 
[2025-02-18 23:20:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:1.6395044326782227 norm:0.0029644493479281664 max memory_allocated 29274.68798828125 
[2025-02-18 23:21:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:1.6392831802368164 norm:0.002545800991356373 max memory_allocated 29274.68798828125 
[2025-02-18 23:22:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:1.6396290063858032 norm:0.0026572830975055695 max memory_allocated 29274.68798828125 
[2025-02-18 23:23:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:1.6399600505828857 norm:0.002270345576107502 max memory_allocated 29274.68798828125 
[2025-02-18 23:23:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:1.6383724212646484 norm:0.0023159461561590433 max memory_allocated 29274.68798828125 
[2025-02-18 23:24:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:1.6402956247329712 norm:0.0022739737760275602 max memory_allocated 29274.68798828125 
[2025-02-18 23:25:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:1.6387927532196045 norm:0.0018658601911738515 max memory_allocated 29274.68798828125 
[2025-02-18 23:26:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:1.6425645351409912 norm:0.003100514877587557 max memory_allocated 29274.68798828125 
[2025-02-18 23:27:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:1.6372263431549072 norm:0.002166324993595481 max memory_allocated 29274.68798828125 
[2025-02-18 23:28:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:1.6417738199234009 norm:0.0023300503380596638 max memory_allocated 29274.68798828125 
[2025-02-18 23:28:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:1.6390522718429565 norm:0.002457332331687212 max memory_allocated 29274.68798828125 
[2025-02-18 23:29:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:1.639173984527588 norm:0.0021402479615062475 max memory_allocated 29274.68798828125 
[2025-02-18 23:30:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:1.6423475742340088 norm:0.003151073819026351 max memory_allocated 29274.68798828125 
[2025-02-18 23:31:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:1.6375930309295654 norm:0.0022088857367634773 max memory_allocated 29274.68798828125 
[2025-02-18 23:32:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:1.6399174928665161 norm:0.0022207333240658045 max memory_allocated 29274.68798828125 
[2025-02-18 23:32:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 23:33:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:1.8270047903060913 norm:0.015914257615804672 max memory_allocated 29274.87548828125 
[2025-02-18 23:34:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:1.8195652961730957 norm:0.009892082773149014 max memory_allocated 29274.87548828125 
[2025-02-18 23:35:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:1.815248966217041 norm:0.007387435995042324 max memory_allocated 29274.87548828125 
[2025-02-18 23:35:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:1.812289834022522 norm:0.005900243297219276 max memory_allocated 29274.87548828125 
[2025-02-18 23:36:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:1.8087615966796875 norm:0.0048123132437467575 max memory_allocated 29274.87548828125 
[2025-02-18 23:37:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:1.8088082075119019 norm:0.004171077162027359 max memory_allocated 29274.87548828125 
[2025-02-18 23:38:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:1.8082762956619263 norm:0.0038234919775277376 max memory_allocated 29274.87548828125 
[2025-02-18 23:39:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:1.8088849782943726 norm:0.0033933958038687706 max memory_allocated 29274.87548828125 
[2025-02-18 23:40:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:1.8096063137054443 norm:0.003254111623391509 max memory_allocated 29274.87548828125 
[2025-02-18 23:40:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:1.8090718984603882 norm:0.003689035540446639 max memory_allocated 29274.87548828125 
[2025-02-18 23:41:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:1.8080337047576904 norm:0.003220024285838008 max memory_allocated 29274.87548828125 
[2025-02-18 23:42:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:1.8094278573989868 norm:0.003272689413279295 max memory_allocated 29274.87548828125 
[2025-02-18 23:43:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:1.8098280429840088 norm:0.002800676040351391 max memory_allocated 29274.87548828125 
[2025-02-18 23:44:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:1.8309251070022583 norm:0.022088339552283287 max memory_allocated 29274.87548828125 
[2025-02-18 23:44:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:1.8026344776153564 norm:0.0027828749734908342 max memory_allocated 29274.87548828125 
[2025-02-18 23:45:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:1.8064422607421875 norm:0.002390172565355897 max memory_allocated 29274.87548828125 
[2025-02-18 23:46:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:1.806084394454956 norm:0.003541637444868684 max memory_allocated 29274.87548828125 
[2025-02-18 23:47:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:1.8073495626449585 norm:0.0026507957372814417 max memory_allocated 29274.87548828125 
[2025-02-18 23:48:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:1.811496376991272 norm:0.005590504966676235 max memory_allocated 29274.87548828125 
[2025-02-18 23:49:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:1.809137225151062 norm:0.004824301693588495 max memory_allocated 29274.87548828125 
[2025-02-18 23:49:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 23:50:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:1.9934388399124146 norm:0.010576029308140278 max memory_allocated 29275.06298828125 
[2025-02-18 23:51:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:1.9940409660339355 norm:0.008496868424117565 max memory_allocated 29275.06298828125 
[2025-02-18 23:51:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:1.9872924089431763 norm:0.006950306240469217 max memory_allocated 29275.06298828125 
[2025-02-18 23:52:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:1.9849846363067627 norm:0.005952843930572271 max memory_allocated 29275.06298828125 
[2025-02-18 23:53:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:1.9872428178787231 norm:0.006324576213955879 max memory_allocated 29275.06298828125 
[2025-02-18 23:54:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:1.9900853633880615 norm:0.006906711496412754 max memory_allocated 29275.06298828125 
[2025-02-18 23:55:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:1.9889369010925293 norm:0.007230373099446297 max memory_allocated 29275.06298828125 
[2025-02-18 23:56:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:1.9874699115753174 norm:0.007203077897429466 max memory_allocated 29275.06298828125 
[2025-02-18 23:56:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:1.9948471784591675 norm:0.006906280759721994 max memory_allocated 29275.06298828125 
[2025-02-18 23:57:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:2.006779909133911 norm:0.0690721943974495 max memory_allocated 29275.06298828125 
[2025-02-18 23:58:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:1.9671785831451416 norm:0.005595196504145861 max memory_allocated 29275.06298828125 
[2025-02-18 23:59:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:1.969090223312378 norm:0.005610574968159199 max memory_allocated 29275.06298828125 
[2025-02-19 00:00:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:1.9713112115859985 norm:0.0056542763486504555 max memory_allocated 29275.06298828125 
[2025-02-19 00:01:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:1.9729030132293701 norm:0.005616443697363138 max memory_allocated 29275.06298828125 
[2025-02-19 00:01:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:1.975359320640564 norm:0.00580368610098958 max memory_allocated 29275.06298828125 
[2025-02-19 00:02:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:1.977702021598816 norm:0.005832095164805651 max memory_allocated 29275.06298828125 
[2025-02-19 00:03:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:1.9839165210723877 norm:0.006091889459639788 max memory_allocated 29275.06298828125 
[2025-02-19 00:04:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:1.995572805404663 norm:0.010270455852150917 max memory_allocated 29275.06298828125 
[2025-02-19 00:05:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:1.9935318231582642 norm:0.005038488656282425 max memory_allocated 29275.06298828125 
[2025-02-19 00:06:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:1.9899933338165283 norm:0.004041104111820459 max memory_allocated 29275.06298828125 
[2025-02-19 00:06:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-19 00:07:11 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:2.179400682449341 norm:0.019563669338822365 max memory_allocated 29275.25048828125 
[2025-02-19 00:08:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:2.1714072227478027 norm:0.013331100344657898 max memory_allocated 29275.25048828125 
[2025-02-19 00:08:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:2.1660823822021484 norm:0.010115676559507847 max memory_allocated 29275.25048828125 
[2025-02-19 00:09:40 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:2.161733627319336 norm:0.008033516816794872 max memory_allocated 29275.25048828125 
[2025-02-19 00:10:30 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:2.1595678329467773 norm:0.006665805354714394 max memory_allocated 29275.25048828125 
[2025-02-19 00:11:20 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:2.1578848361968994 norm:0.005744209513068199 max memory_allocated 29275.25048828125 
[2025-02-19 00:12:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:2.1559064388275146 norm:0.00493808975443244 max memory_allocated 29275.25048828125 
[2025-02-19 00:12:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:2.154446601867676 norm:0.004335872828960419 max memory_allocated 29275.25048828125 
[2025-02-19 00:13:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:2.1518077850341797 norm:0.0037897243164479733 max memory_allocated 29275.25048828125 
[2025-02-19 00:14:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:2.151439666748047 norm:0.0034477240405976772 max memory_allocated 29275.25048828125 
[2025-02-19 00:15:29 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:2.149301052093506 norm:0.0031150956638157368 max memory_allocated 29275.25048828125 
[2025-02-19 00:16:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:2.148919105529785 norm:0.0028919160831719637 max memory_allocated 29275.25048828125 
[2025-02-19 00:17:08 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:2.149840831756592 norm:0.0027903220616281033 max memory_allocated 29275.25048828125 
[2025-02-19 00:17:58 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:2.148907423019409 norm:0.0026325189974159002 max memory_allocated 29275.25048828125 
[2025-02-19 00:18:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:2.1482303142547607 norm:0.0025585193652659655 max memory_allocated 29275.25048828125 
[2025-02-19 00:19:38 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:2.1474804878234863 norm:0.0025537428446114063 max memory_allocated 29275.25048828125 
[2025-02-19 00:20:27 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:2.1479642391204834 norm:0.0021810438483953476 max memory_allocated 29275.25048828125 
[2025-02-19 00:21:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:2.147984027862549 norm:0.002253188518807292 max memory_allocated 29275.25048828125 
[2025-02-19 00:22:07 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:2.1471526622772217 norm:0.002241540467366576 max memory_allocated 29275.25048828125 
[2025-02-19 00:22:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:2.146116018295288 norm:0.002121678786352277 max memory_allocated 29275.25048828125 
[2025-02-19 00:23:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-19 00:24:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:2.3546462059020996 norm:0.016391471028327942 max memory_allocated 29275.43798828125 
[2025-02-19 00:24:55 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:2.347201347351074 norm:0.01178041473031044 max memory_allocated 29275.43798828125 
[2025-02-19 00:25:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:2.3446407318115234 norm:0.009701740927994251 max memory_allocated 29275.43798828125 
[2025-02-19 00:26:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:2.343327283859253 norm:0.008169510401785374 max memory_allocated 29275.43798828125 
[2025-02-19 00:27:25 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:2.34363055229187 norm:0.007016878575086594 max memory_allocated 29275.43798828125 
[2025-02-19 00:28:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:2.3435609340667725 norm:0.005888720974326134 max memory_allocated 29275.43798828125 
[2025-02-19 00:29:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:2.3438851833343506 norm:0.005166059825569391 max memory_allocated 29275.43798828125 
[2025-02-19 00:29:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:2.3434653282165527 norm:0.004542517475783825 max memory_allocated 29275.43798828125 
[2025-02-19 00:30:44 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:2.342869758605957 norm:0.004332766402512789 max memory_allocated 29275.43798828125 
[2025-02-19 00:31:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:2.3428220748901367 norm:0.004150762222707272 max memory_allocated 29275.43798828125 
[2025-02-19 00:32:23 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:2.3443355560302734 norm:0.004536107648164034 max memory_allocated 29275.43798828125 
[2025-02-19 00:33:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:2.343709945678711 norm:0.00460780318826437 max memory_allocated 29275.43798828125 
[2025-02-19 00:34:03 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:2.3427250385284424 norm:0.004262780770659447 max memory_allocated 29275.43798828125 
[2025-02-19 00:34:53 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:2.342252492904663 norm:0.004397390875965357 max memory_allocated 29275.43798828125 
[2025-02-19 00:35:43 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:2.341254711151123 norm:0.0036847812589257956 max memory_allocated 29275.43798828125 
[2025-02-19 00:36:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:2.34165620803833 norm:0.0032098630908876657 max memory_allocated 29275.43798828125 
[2025-02-19 00:37:22 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:2.3399107456207275 norm:0.0028097890317440033 max memory_allocated 29275.43798828125 
[2025-02-19 00:38:12 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:2.3389222621917725 norm:0.003985132556408644 max memory_allocated 29275.43798828125 
[2025-02-19 00:39:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:2.3432655334472656 norm:0.00742840301245451 max memory_allocated 29275.43798828125 
[2025-02-19 00:39:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:2.3354110717773438 norm:0.003544978331774473 max memory_allocated 29275.43798828125 
[2025-02-19 00:40:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-19 00:41:01 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:2.5806424617767334 norm:0.016262853518128395 max memory_allocated 29275.62548828125 
[2025-02-19 00:41:50 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:2.5728907585144043 norm:0.01029420830309391 max memory_allocated 29275.62548828125 
[2025-02-19 00:42:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:2.5673227310180664 norm:0.007175840903073549 max memory_allocated 29275.62548828125 
[2025-02-19 00:43:30 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:2.5640058517456055 norm:0.00557329785078764 max memory_allocated 29275.62548828125 
[2025-02-19 00:44:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:2.561589002609253 norm:0.004603132605552673 max memory_allocated 29275.62548828125 
[2025-02-19 00:45:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:2.5586729049682617 norm:0.0037669059820473194 max memory_allocated 29275.62548828125 
[2025-02-19 00:46:00 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:2.5568008422851562 norm:0.003248825203627348 max memory_allocated 29275.62548828125 
[2025-02-19 00:46:50 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:2.5557656288146973 norm:0.002912479219958186 max memory_allocated 29275.62548828125 
[2025-02-19 00:47:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:2.555011749267578 norm:0.0025620819069445133 max memory_allocated 29275.62548828125 
[2025-02-19 00:48:29 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:2.553906202316284 norm:0.002313072793185711 max memory_allocated 29275.62548828125 
[2025-02-19 00:49:19 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:2.551905393600464 norm:0.002013731747865677 max memory_allocated 29275.62548828125 
[2025-02-19 00:50:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:2.5514094829559326 norm:0.0019050926202908158 max memory_allocated 29275.62548828125 
[2025-02-19 00:50:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:2.551072597503662 norm:0.0018101200694218278 max memory_allocated 29275.62548828125 
[2025-02-19 00:51:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:2.550846815109253 norm:0.0017667048377916217 max memory_allocated 29275.62548828125 
[2025-02-19 00:52:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:2.5508408546447754 norm:0.0017429252620786428 max memory_allocated 29275.62548828125 
[2025-02-19 00:53:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:2.5500435829162598 norm:0.0017163266893476248 max memory_allocated 29275.62548828125 
[2025-02-19 00:54:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:2.5491156578063965 norm:0.001623830758035183 max memory_allocated 29275.62548828125 
[2025-02-19 00:55:07 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:2.548684597015381 norm:0.001505151391029358 max memory_allocated 29275.62548828125 
[2025-02-19 00:55:57 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:2.548828363418579 norm:0.0015601292252540588 max memory_allocated 29275.62548828125 
[2025-02-19 00:56:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:2.54837965965271 norm:0.0015498384600505233 max memory_allocated 29275.62548828125 
[2025-02-19 00:57:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-19 00:57:55 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:2.82051420211792 norm:0.010819606482982635 max memory_allocated 29275.81298828125 
[2025-02-19 00:58:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:2.813931703567505 norm:0.009033875539898872 max memory_allocated 29275.81298828125 
[2025-02-19 00:59:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:2.8112778663635254 norm:0.007689869962632656 max memory_allocated 29275.81298828125 
[2025-02-19 01:00:25 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:2.808067798614502 norm:0.0066066766157746315 max memory_allocated 29275.81298828125 
[2025-02-19 01:01:14 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:2.8054919242858887 norm:0.005674782209098339 max memory_allocated 29275.81298828125 
[2025-02-19 01:02:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:2.804086685180664 norm:0.00488923117518425 max memory_allocated 29275.81298828125 
[2025-02-19 01:02:54 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:2.8022141456604004 norm:0.004211065359413624 max memory_allocated 29275.81298828125 
[2025-02-19 01:03:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:2.8017630577087402 norm:0.00389518472366035 max memory_allocated 29275.81298828125 
[2025-02-19 01:04:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:2.8000845909118652 norm:0.003465106477960944 max memory_allocated 29275.81298828125 
[2025-02-19 01:05:24 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:2.7993826866149902 norm:0.003213431453332305 max memory_allocated 29275.81298828125 
[2025-02-19 01:06:14 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:2.7983338832855225 norm:0.0030155405402183533 max memory_allocated 29275.81298828125 
[2025-02-19 01:07:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:2.7974488735198975 norm:0.0027901562862098217 max memory_allocated 29275.81298828125 
[2025-02-19 01:07:53 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:2.7975339889526367 norm:0.0027163487393409014 max memory_allocated 29275.81298828125 
[2025-02-19 01:08:43 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:2.7984628677368164 norm:0.0027399829123169184 max memory_allocated 29275.81298828125 
[2025-02-19 01:09:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:2.7955737113952637 norm:0.0024064439348876476 max memory_allocated 29275.81298828125 
[2025-02-19 01:10:23 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:2.7947332859039307 norm:0.0022804324980825186 max memory_allocated 29275.81298828125 
[2025-02-19 01:11:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:2.7978172302246094 norm:0.0026332647539675236 max memory_allocated 29275.81298828125 
[2025-02-19 01:12:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:2.794083595275879 norm:0.0021235798485577106 max memory_allocated 29275.81298828125 
[2025-02-19 01:12:53 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:2.797081232070923 norm:0.0024337915237993 max memory_allocated 29275.81298828125 
[2025-02-19 01:13:43 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:2.7933590412139893 norm:0.0020311756525188684 max memory_allocated 29275.81298828125 
[2025-02-19 01:13:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-19 01:14:01 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:14:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:3.116483211517334 norm:0.08299751579761505 max memory_allocated 29276.14501953125 
[2025-02-19 01:15:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:3.106541872024536 norm:0.08433044701814651 max memory_allocated 29276.14501953125 
[2025-02-19 01:16:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:3.092444896697998 norm:0.07552552968263626 max memory_allocated 29276.14501953125 
[2025-02-19 01:17:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:3.084949493408203 norm:0.06965304911136627 max memory_allocated 29276.14501953125 
[2025-02-19 01:18:11 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:3.0773777961730957 norm:0.06467852741479874 max memory_allocated 29276.14501953125 
[2025-02-19 01:19:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:3.2296504974365234 norm:0.12555792927742004 max memory_allocated 29276.14501953125 
[2025-02-19 01:19:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:3.0872631072998047 norm:0.045217301696538925 max memory_allocated 29276.14501953125 
[2025-02-19 01:20:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:3.0454094409942627 norm:0.059768784791231155 max memory_allocated 29276.14501953125 
[2025-02-19 01:21:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:3.044621229171753 norm:0.06361520290374756 max memory_allocated 29276.14501953125 
[2025-02-19 01:22:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:3.0455331802368164 norm:0.06071613356471062 max memory_allocated 29276.14501953125 
[2025-02-19 01:23:11 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:3.046487808227539 norm:0.05804577097296715 max memory_allocated 29276.14501953125 
[2025-02-19 01:24:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:3.048433303833008 norm:0.05711226910352707 max memory_allocated 29276.14501953125 
[2025-02-19 01:24:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:3.0597691535949707 norm:0.06210513785481453 max memory_allocated 29276.14501953125 
[2025-02-19 01:25:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:3.0582098960876465 norm:0.06163923442363739 max memory_allocated 29276.14501953125 
[2025-02-19 01:26:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:3.045757532119751 norm:0.05646069347858429 max memory_allocated 29276.14501953125 
[2025-02-19 01:27:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:3.0397064685821533 norm:0.05329390615224838 max memory_allocated 29276.14501953125 
[2025-02-19 01:28:11 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:3.03969669342041 norm:0.053101371973752975 max memory_allocated 29276.14501953125 
[2025-02-19 01:29:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:3.0546116828918457 norm:0.059768423438072205 max memory_allocated 29276.14501953125 
[2025-02-19 01:29:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:3.0344676971435547 norm:0.049115266650915146 max memory_allocated 29276.14501953125 
[2025-02-19 01:30:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:3.031311511993408 norm:0.04551605135202408 max memory_allocated 29276.14501953125 
[2025-02-19 01:30:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-19 01:30:59 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:31:49 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:3.3688461780548096 norm:0.10811422765254974 max memory_allocated 29276.33251953125 
[2025-02-19 01:32:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:3.338665246963501 norm:0.10455663502216339 max memory_allocated 29276.33251953125 
[2025-02-19 01:33:29 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:3.333688735961914 norm:0.09436159580945969 max memory_allocated 29276.33251953125 
[2025-02-19 01:34:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:3.321253776550293 norm:0.08607420325279236 max memory_allocated 29276.33251953125 
[2025-02-19 01:35:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:3.303739547729492 norm:0.07765006273984909 max memory_allocated 29276.33251953125 
[2025-02-19 01:35:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:3.288860321044922 norm:0.07070428133010864 max memory_allocated 29276.33251953125 
[2025-02-19 01:36:49 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:3.279973268508911 norm:0.06589014083147049 max memory_allocated 29276.33251953125 
[2025-02-19 01:37:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:3.274306535720825 norm:0.06274112313985825 max memory_allocated 29276.33251953125 
[2025-02-19 01:38:29 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:3.2639453411102295 norm:0.058557283133268356 max memory_allocated 29276.33251953125 
[2025-02-19 01:39:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:3.25642728805542 norm:0.05490526556968689 max memory_allocated 29276.33251953125 
[2025-02-19 01:40:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:3.25262451171875 norm:0.05308675020933151 max memory_allocated 29276.33251953125 
[2025-02-19 01:40:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:3.248631715774536 norm:0.05216042324900627 max memory_allocated 29276.33251953125 
[2025-02-19 01:41:50 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:3.2443456649780273 norm:0.05013725906610489 max memory_allocated 29276.33251953125 
[2025-02-19 01:42:40 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:3.240845203399658 norm:0.048809729516506195 max memory_allocated 29276.33251953125 
[2025-02-19 01:43:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:3.2379205226898193 norm:0.04729447886347771 max memory_allocated 29276.33251953125 
[2025-02-19 01:44:20 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:3.2386817932128906 norm:0.04935302957892418 max memory_allocated 29276.33251953125 
[2025-02-19 01:45:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:3.231849431991577 norm:0.045426927506923676 max memory_allocated 29276.33251953125 
[2025-02-19 01:46:00 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:3.2339775562286377 norm:0.04764118418097496 max memory_allocated 29276.33251953125 
[2025-02-19 01:46:50 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:3.2288575172424316 norm:0.04390272498130798 max memory_allocated 29276.33251953125 
[2025-02-19 01:47:40 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:3.2300217151641846 norm:0.04625876247882843 max memory_allocated 29276.33251953125 
[2025-02-19 01:47:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-19 01:47:58 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:48:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:3.818338394165039 norm:0.12135032564401627 max memory_allocated 29276.52001953125 
[2025-02-19 01:49:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:3.784736394882202 norm:0.10843724012374878 max memory_allocated 29276.52001953125 
[2025-02-19 01:50:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:3.771204948425293 norm:0.09008940309286118 max memory_allocated 29276.52001953125 
[2025-02-19 01:51:18 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:3.7459027767181396 norm:0.07809191197156906 max memory_allocated 29276.52001953125 
[2025-02-19 01:52:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:3.7485575675964355 norm:0.07224743068218231 max memory_allocated 29276.52001953125 
[2025-02-19 01:52:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:3.726017713546753 norm:0.060384541749954224 max memory_allocated 29276.52001953125 
[2025-02-19 01:53:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:3.748952865600586 norm:0.06527554243803024 max memory_allocated 29276.52001953125 
[2025-02-19 01:54:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:3.7147774696350098 norm:0.052462972700595856 max memory_allocated 29276.52001953125 
[2025-02-19 01:55:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:3.7262961864471436 norm:0.05602462589740753 max memory_allocated 29276.52001953125 
[2025-02-19 01:56:18 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:3.7056188583374023 norm:0.04915550351142883 max memory_allocated 29276.52001953125 
[2025-02-19 01:57:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:3.729055404663086 norm:0.05267420411109924 max memory_allocated 29276.52001953125 
[2025-02-19 01:57:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:3.7055864334106445 norm:0.04862477630376816 max memory_allocated 29276.52001953125 
[2025-02-19 01:58:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:3.7314400672912598 norm:0.04960842803120613 max memory_allocated 29276.52001953125 
[2025-02-19 01:59:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:3.706235647201538 norm:0.04734722524881363 max memory_allocated 29276.52001953125 
[2025-02-19 02:00:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:3.7340664863586426 norm:0.056131649762392044 max memory_allocated 29276.52001953125 
[2025-02-19 02:01:17 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:3.725221633911133 norm:0.05602828040719032 max memory_allocated 29276.52001953125 
[2025-02-19 02:02:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:3.705000400543213 norm:0.044992271810770035 max memory_allocated 29276.52001953125 
[2025-02-19 02:02:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:3.6894478797912598 norm:0.04253781959414482 max memory_allocated 29276.52001953125 
[2025-02-19 02:03:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:3.6886537075042725 norm:0.04260730743408203 max memory_allocated 29276.52001953125 
[2025-02-19 02:04:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:3.6840686798095703 norm:0.04243546351790428 max memory_allocated 29276.52001953125 
[2025-02-19 02:04:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-19 02:05:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 02:05:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:5.406193733215332 norm:0.24336622655391693 max memory_allocated 29276.70751953125 
[2025-02-19 02:06:42 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:5.277602672576904 norm:0.22502341866493225 max memory_allocated 29276.70751953125 
[2025-02-19 02:07:32 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:5.263761043548584 norm:0.20254138112068176 max memory_allocated 29276.70751953125 
[2025-02-19 02:08:22 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:5.257170677185059 norm:0.18631742894649506 max memory_allocated 29276.70751953125 
[2025-02-19 02:09:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:5.250176906585693 norm:0.17335301637649536 max memory_allocated 29276.70751953125 
[2025-02-19 02:10:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:5.26000452041626 norm:0.16592098772525787 max memory_allocated 29276.70751953125 
[2025-02-19 02:10:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:5.2493062019348145 norm:0.15359675884246826 max memory_allocated 29276.70751953125 
[2025-02-19 02:11:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:5.257097244262695 norm:0.14764413237571716 max memory_allocated 29276.70751953125 
[2025-02-19 02:12:31 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:5.236001014709473 norm:0.13304612040519714 max memory_allocated 29276.70751953125 
[2025-02-19 02:13:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:5.2254180908203125 norm:0.12488975375890732 max memory_allocated 29276.70751953125 
[2025-02-19 02:14:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:5.206347465515137 norm:0.10493501275777817 max memory_allocated 29276.70751953125 
[2025-02-19 02:15:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:5.1868896484375 norm:0.10491453856229782 max memory_allocated 29276.70751953125 
[2025-02-19 02:15:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:5.198022842407227 norm:0.09706920385360718 max memory_allocated 29276.70751953125 
[2025-02-19 02:16:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:5.205096244812012 norm:0.10766445845365524 max memory_allocated 29276.70751953125 
[2025-02-19 02:17:31 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:5.281611442565918 norm:0.12403668463230133 max memory_allocated 29276.70751953125 
[2025-02-19 02:18:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:5.430050373077393 norm:0.2005949318408966 max memory_allocated 29276.70751953125 
[2025-02-19 02:19:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:5.406647682189941 norm:0.2084737867116928 max memory_allocated 29276.70751953125 
[2025-02-19 02:20:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:5.341717720031738 norm:0.2309754490852356 max memory_allocated 29276.70751953125 
[2025-02-19 02:20:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:5.284685134887695 norm:0.2458600252866745 max memory_allocated 29276.70751953125 
[2025-02-19 02:21:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:5.256916522979736 norm:0.3246176540851593 max memory_allocated 29276.70751953125 
[2025-02-19 02:21:56 root] (main_calibration.py 365): INFO 40748.91221904755
[2025-02-19 02:23:14 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-19 02:25:09 root] (main_calibration.py 158): INFO wikitext2 : 9.898995399475098
[2025-02-19 02:25:09 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-19 02:28:09 root] (main_calibration.py 158): INFO c4 : 13.576382637023926
[2025-02-19 02:28:49 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Tue Feb 18 03:11:31 2025) since it couldn't be found locally at winogrande., or remotely on the Hugging Face Hub.
[2025-02-19 04:26:22 root] (main_calibration.py 169): INFO {'wikitext2': 9.898995399475098, 'c4': 13.576382637023926, 'results': {'piqa': {'acc': 0.676278563656148, 'acc_stderr': 0.01091676501070877, 'acc_norm': 0.6686615886833515, 'acc_norm_stderr': 0.01098207745895735}, 'arc_easy': {'acc': 0.5155723905723906, 'acc_stderr': 0.010254806331961899, 'acc_norm': 0.41919191919191917, 'acc_norm_stderr': 0.010124905282491185}, 'arc_challenge': {'acc': 0.2986348122866894, 'acc_stderr': 0.013374078615068749, 'acc_norm': 0.3293515358361775, 'acc_norm_stderr': 0.013734057652635474}, 'winogrande': {'acc': 0.5374901341752171, 'acc_stderr': 0.01401292818333658}, 'boolq': {'acc': 0.6125382262996942, 'acc_stderr': 0.00852066653613694}, 'hellaswag': {'acc': 0.4347739494124676, 'acc_stderr': 0.004947141797384122, 'acc_norm': 0.5622385978888668, 'acc_norm_stderr': 0.004950973231188737}}, 'versions': {'piqa': 0, 'arc_easy': 0, 'arc_challenge': 0, 'winogrande': 0, 'boolq': 1, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
