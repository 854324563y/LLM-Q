[2025-02-18 15:02:51 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation/llama-13b-hf-w8a8', save_dir='./log-calibration-compensation/quant/llama-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-18 15:09:53 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 15:09:53 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-18 15:09:53 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 15:09:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 15:10:01 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:10:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0017439129296690226 norm:0.0020392967853695154 max memory_allocated 29266.39501953125 
[2025-02-18 15:11:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.001081806723959744 norm:0.0014307068195194006 max memory_allocated 29266.39501953125 
[2025-02-18 15:12:27 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0009037462878040969 norm:0.0012171361595392227 max memory_allocated 29266.39501953125 
[2025-02-18 15:13:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0007958704954944551 norm:0.0010664158035069704 max memory_allocated 29266.39501953125 
[2025-02-18 15:14:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0007285350584425032 norm:0.0009663749951869249 max memory_allocated 29266.39501953125 
[2025-02-18 15:14:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0006838207482360303 norm:0.000889345770701766 max memory_allocated 29266.39501953125 
[2025-02-18 15:15:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0006543524796143174 norm:0.000838166568428278 max memory_allocated 29266.39501953125 
[2025-02-18 15:16:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0006325372378341854 norm:0.0007810753304511309 max memory_allocated 29266.39501953125 
[2025-02-18 15:17:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0006177426548674703 norm:0.0007342585013248026 max memory_allocated 29266.39501953125 
[2025-02-18 15:18:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0006043667672201991 norm:0.0006659005302935839 max memory_allocated 29266.39501953125 
[2025-02-18 15:19:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0005950005142949522 norm:0.0006236572517082095 max memory_allocated 29266.39501953125 
[2025-02-18 15:19:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0005870300810784101 norm:0.000573266705032438 max memory_allocated 29266.39501953125 
[2025-02-18 15:20:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.000580853782594204 norm:0.0005379632930271327 max memory_allocated 29266.39501953125 
[2025-02-18 15:21:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0005755159072577953 norm:0.0004974908661097288 max memory_allocated 29266.39501953125 
[2025-02-18 15:22:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.000570094445720315 norm:0.0004573478945530951 max memory_allocated 29266.39501953125 
[2025-02-18 15:23:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0005666975630447268 norm:0.0004251327191013843 max memory_allocated 29266.39501953125 
[2025-02-18 15:23:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0005632865359075367 norm:0.0003964483621530235 max memory_allocated 29266.39501953125 
[2025-02-18 15:24:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0005605742917396128 norm:0.0003724933776538819 max memory_allocated 29266.39501953125 
[2025-02-18 15:25:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0005579923745244741 norm:0.00035090273013338447 max memory_allocated 29266.39501953125 
[2025-02-18 15:26:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0005558201810345054 norm:0.000333072297507897 max memory_allocated 29266.39501953125 
[2025-02-18 15:26:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 15:26:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:27:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0015313074691221118 norm:0.0016735069220885634 max memory_allocated 29266.39501953125 
[2025-02-18 15:28:20 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0010421674232929945 norm:0.0003353186766617 max memory_allocated 29266.39501953125 
[2025-02-18 15:29:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0009334406931884587 norm:0.0006433766684494913 max memory_allocated 29266.39501953125 
[2025-02-18 15:29:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0008717359160073102 norm:0.001035758526995778 max memory_allocated 29266.39501953125 
[2025-02-18 15:30:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0008317912579514086 norm:0.0010966634145006537 max memory_allocated 29266.39501953125 
[2025-02-18 15:31:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0008032078039832413 norm:0.0011187297059223056 max memory_allocated 29266.39501953125 
[2025-02-18 15:32:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0007812610128894448 norm:0.0010884874500334263 max memory_allocated 29266.39501953125 
[2025-02-18 15:33:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0007643075077794492 norm:0.0010554571636021137 max memory_allocated 29266.39501953125 
[2025-02-18 15:34:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0007512018783017993 norm:0.0010041070636361837 max memory_allocated 29266.39501953125 
[2025-02-18 15:34:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0007395504508167505 norm:0.0009454915998503566 max memory_allocated 29266.39501953125 
[2025-02-18 15:35:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0007316890405490994 norm:0.0009043518221005797 max memory_allocated 29266.39501953125 
[2025-02-18 15:36:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0007238903781399131 norm:0.0008456768118776381 max memory_allocated 29266.39501953125 
[2025-02-18 15:37:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0007196227088570595 norm:0.0008144230232574046 max memory_allocated 29266.39501953125 
[2025-02-18 15:38:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0007166277500800788 norm:0.0007911825086921453 max memory_allocated 29266.39501953125 
[2025-02-18 15:39:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0007118541980162263 norm:0.0007460040505975485 max memory_allocated 29266.39501953125 
[2025-02-18 15:39:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0007063032826408744 norm:0.0006776235532015562 max memory_allocated 29266.39501953125 
[2025-02-18 15:40:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0007009428227320313 norm:0.0006099946331232786 max memory_allocated 29266.39501953125 
[2025-02-18 15:41:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0006984443170949817 norm:0.0005675572901964188 max memory_allocated 29266.39501953125 
[2025-02-18 15:42:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.000694887014105916 norm:0.0005172837409190834 max memory_allocated 29266.39501953125 
[2025-02-18 15:43:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0006913746474310756 norm:0.0004743424942716956 max memory_allocated 29266.39501953125 
[2025-02-18 15:43:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 15:43:25 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:44:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0015912556555122137 norm:0.0009944797493517399 max memory_allocated 29266.39501953125 
[2025-02-18 15:45:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0012482346501201391 norm:0.0004892438882961869 max memory_allocated 29266.39501953125 
[2025-02-18 15:45:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0011387564009055495 norm:0.0007664377335458994 max memory_allocated 29266.39501953125 
[2025-02-18 15:46:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0010821925243362784 norm:0.0009777711238712072 max memory_allocated 29266.39501953125 
[2025-02-18 15:47:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0010508684208616614 norm:0.0010694842785596848 max memory_allocated 29266.39501953125 
[2025-02-18 15:48:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.001018845709040761 norm:0.001039960770867765 max memory_allocated 29266.39501953125 
[2025-02-18 15:49:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0009951316751539707 norm:0.000999170122668147 max memory_allocated 29266.39501953125 
[2025-02-18 15:49:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.000980398035608232 norm:0.0009269472793675959 max memory_allocated 29266.39501953125 
[2025-02-18 15:50:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0009702935931272805 norm:0.0009142229682765901 max memory_allocated 29266.39501953125 
[2025-02-18 15:51:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0009586979285813868 norm:0.0008044549613259733 max memory_allocated 29266.39501953125 
[2025-02-18 15:52:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0009529031813144684 norm:0.0007843513740226626 max memory_allocated 29266.39501953125 
[2025-02-18 15:53:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0009476264240220189 norm:0.0007724386523477733 max memory_allocated 29266.39501953125 
[2025-02-18 15:54:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0009406678727827966 norm:0.0007111516315490007 max memory_allocated 29266.39501953125 
[2025-02-18 15:54:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0009353663772344589 norm:0.0006558887544088066 max memory_allocated 29266.39501953125 
[2025-02-18 15:55:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.000939075427595526 norm:0.0006558963796123862 max memory_allocated 29266.39501953125 
[2025-02-18 15:56:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0009297601063735783 norm:0.0005616183625534177 max memory_allocated 29266.39501953125 
[2025-02-18 15:57:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0009316272917203605 norm:0.0006099874153733253 max memory_allocated 29266.39501953125 
[2025-02-18 15:58:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0009226392721757293 norm:0.000562330475077033 max memory_allocated 29266.39501953125 
[2025-02-18 15:59:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0009185055969282985 norm:0.0005252333940006793 max memory_allocated 29266.39501953125 
[2025-02-18 15:59:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0009214761666953564 norm:0.0004741813463624567 max memory_allocated 29266.39501953125 
[2025-02-18 16:00:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 16:00:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.001938419882208109 norm:0.0005898659001104534 max memory_allocated 29266.81298828125 
[2025-02-18 16:01:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0015014203963801265 norm:0.00023445695114787668 max memory_allocated 29266.81298828125 
[2025-02-18 16:02:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0013474392471835017 norm:0.00014138442929834127 max memory_allocated 29266.81298828125 
[2025-02-18 16:03:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0012726642889901996 norm:9.868488268693909e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:04:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.001234419527463615 norm:7.2694499976933e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:05:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.001214384101331234 norm:5.5892422096803784e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:05:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0011961569543927908 norm:4.4124644773546606e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:06:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0011849632719531655 norm:3.5800214391201735e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:07:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.00117696903180331 norm:3.0054690796532668e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:08:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0011723345378413796 norm:2.4656354071339592e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:09:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0011672775726765394 norm:2.0308352759457193e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:09:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0011643124744296074 norm:1.6863585187820718e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:10:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0011624045437201858 norm:1.402972611685982e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:11:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.001158273546025157 norm:1.2161541235400364e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:12:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.001157457591034472 norm:1.0714004929468501e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:13:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.00115568982437253 norm:9.969436177925672e-06 max memory_allocated 29266.81298828125 
[2025-02-18 16:14:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0011543817818164825 norm:9.53634298639372e-06 max memory_allocated 29266.81298828125 
[2025-02-18 16:14:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0011541657149791718 norm:9.09252412384376e-06 max memory_allocated 29266.81298828125 
[2025-02-18 16:15:43 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0011519809486344457 norm:8.988101399154402e-06 max memory_allocated 29266.81298828125 
[2025-02-18 16:16:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.001151412259787321 norm:8.80933839653153e-06 max memory_allocated 29266.81298828125 
[2025-02-18 16:16:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 16:17:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0036118829157203436 norm:0.0026891715824604034 max memory_allocated 29266.81298828125 
[2025-02-18 16:18:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0019807806238532066 norm:0.0006148883840069175 max memory_allocated 29266.81298828125 
[2025-02-18 16:19:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.001720702974125743 norm:0.00036249280674383044 max memory_allocated 29266.81298828125 
[2025-02-18 16:20:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0016175293130800128 norm:0.00026758015155792236 max memory_allocated 29266.81298828125 
[2025-02-18 16:20:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0015293752076104283 norm:0.0001889111881610006 max memory_allocated 29266.81298828125 
[2025-02-18 16:21:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0014713804703205824 norm:0.00013942008081357926 max memory_allocated 29266.81298828125 
[2025-02-18 16:22:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0014437888748943806 norm:0.00011360578355379403 max memory_allocated 29266.81298828125 
[2025-02-18 16:23:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0014278785092756152 norm:0.00010017109889304265 max memory_allocated 29266.81298828125 
[2025-02-18 16:24:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0014140844577923417 norm:8.78408900462091e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:25:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0014012388419359922 norm:7.65463337302208e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:25:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0013927940744906664 norm:6.858885171823204e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:26:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.00138387200422585 norm:6.097519872128032e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:27:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0013768546050414443 norm:5.568470442085527e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:28:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0013738257111981511 norm:5.13108134327922e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:29:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.001367769786156714 norm:4.559037188300863e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:29:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0013641018886119127 norm:3.998813917860389e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:30:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0013603752013295889 norm:3.533930066623725e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:31:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0013547533890232444 norm:3.10540635837242e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:32:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.00134929153136909 norm:2.725342710618861e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:33:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0013456676388159394 norm:2.3654445612919517e-05 max memory_allocated 29266.81298828125 
[2025-02-18 16:33:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 16:34:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.002895094221457839 norm:0.0015102927573025227 max memory_allocated 29267.18798828125 
[2025-02-18 16:35:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0019223256967961788 norm:0.00036066441680304706 max memory_allocated 29267.18798828125 
[2025-02-18 16:35:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0017940286779776216 norm:0.000255977880442515 max memory_allocated 29267.18798828125 
[2025-02-18 16:36:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.001692590187303722 norm:0.00017433150787837803 max memory_allocated 29267.18798828125 
[2025-02-18 16:37:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0016307616606354713 norm:0.00012640250497497618 max memory_allocated 29267.18798828125 
[2025-02-18 16:38:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0015926874475553632 norm:9.943896293407306e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:39:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0015699202194809914 norm:8.356546459253877e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:40:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0015538545558229089 norm:7.119784277165309e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:40:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0015413800720125437 norm:6.252353341551498e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:41:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.001530373003333807 norm:5.5143995268736035e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:42:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0015237698098644614 norm:4.971543239662424e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:43:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0015173956053331494 norm:4.4459586206357926e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:44:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0015117547009140253 norm:4.020866617793217e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:44:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0015070720110088587 norm:3.528494562488049e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:45:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.001503315637819469 norm:3.1301398848881945e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:46:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0014997510006651282 norm:2.75526999757858e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:47:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.001497372635640204 norm:2.441797732899431e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:48:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.001496025244705379 norm:2.1199244656600058e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:49:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0014951903140172362 norm:1.8120501408702694e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:49:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0014933175407350063 norm:1.5565292414976284e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:50:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 16:51:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.002493362408131361 norm:0.0003272923640906811 max memory_allocated 29267.18798828125 
[2025-02-18 16:51:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.002129987347871065 norm:0.00011863680992973968 max memory_allocated 29267.18798828125 
[2025-02-18 16:52:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.0020458826329559088 norm:7.786358764860779e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:53:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0019827920477837324 norm:5.348703052732162e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:54:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.001955881016328931 norm:4.190994513919577e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:55:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0019346163608133793 norm:3.499299782561138e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:55:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0019183621043339372 norm:3.098688830505125e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:56:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0019047785317525268 norm:2.7948992283199914e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:57:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0018932722741737962 norm:2.501330709492322e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:58:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.00188839144539088 norm:2.269660762976855e-05 max memory_allocated 29267.18798828125 
[2025-02-18 16:59:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.001882866956293583 norm:2.108179614879191e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:00:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0018761191749945283 norm:1.9602714019129053e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:00:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.001873956061899662 norm:1.8442629880155437e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:01:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.001875799847766757 norm:1.7592214135220274e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:02:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0018668101401999593 norm:1.676062856859062e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:03:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0018706724513322115 norm:1.6432375559816137e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:04:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0018682549707591534 norm:1.5913181414362043e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:04:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.00186985544860363 norm:1.5668927517253906e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:05:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0018659881316125393 norm:1.5042996892589144e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:06:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0018687252886593342 norm:1.4924592505849432e-05 max memory_allocated 29267.18798828125 
[2025-02-18 17:06:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 17:07:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.003006701823323965 norm:0.00041939917718991637 max memory_allocated 29267.56298828125 
[2025-02-18 17:08:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0022139211650937796 norm:9.824327571550384e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:09:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.002078142948448658 norm:6.484340701717883e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:10:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.00203118659555912 norm:5.2113173296675086e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:10:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.001986498013138771 norm:3.650684448075481e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:11:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.00196335231885314 norm:2.8176356863696128e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:12:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.001952719409018755 norm:2.311567368451506e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:13:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0019439724273979664 norm:1.9699116819538176e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:14:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.0019356331322342157 norm:1.690867793513462e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:15:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0019300354178994894 norm:1.5845867892494425e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:15:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0019271986093372107 norm:1.4262796867114957e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:16:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0019233312923461199 norm:1.2829222214350011e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:17:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0019209056627005339 norm:1.1766728675866034e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:18:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.001918299589306116 norm:1.1102238204330206e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:19:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0019161622039973736 norm:1.035803506965749e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:19:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.001914938329719007 norm:9.811788913793862e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:20:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.001913952175527811 norm:9.107626283366699e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:21:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.001914341002702713 norm:8.76301055541262e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:22:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0019128154963254929 norm:8.175321454473305e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:23:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0019118390046060085 norm:8.073021490417887e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:23:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 17:24:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.002636224962770939 norm:0.00018166127847507596 max memory_allocated 29267.56298828125 
[2025-02-18 17:25:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0022496539168059826 norm:5.8157100284006447e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:26:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.002171622822061181 norm:4.0529408579459414e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:26:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.002126498380675912 norm:2.9246490157674998e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:27:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0021011484786868095 norm:2.231438338640146e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:28:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.002085801213979721 norm:1.8171935153077357e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:29:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0020743694622069597 norm:1.574607267684769e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:30:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.002065348206087947 norm:1.3840512110618874e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:30:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0020609269849956036 norm:1.199173402710585e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:31:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0020587174221873283 norm:1.0298330380464904e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:32:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0020558799151331186 norm:9.459298780711833e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:33:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.002053818665444851 norm:8.622031600680202e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:34:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.002051715971902013 norm:7.969970283738803e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:35:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.002050420269370079 norm:7.413150797219714e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:35:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0020494144409894943 norm:7.108676072675735e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:36:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.002048737369477749 norm:7.060210919007659e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:37:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.002050727605819702 norm:6.638367722189287e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:38:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.002050482202321291 norm:6.51446680421941e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:39:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0020495732314884663 norm:6.462657438532915e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:39:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0020482433028519154 norm:6.322304216155317e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:40:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 17:41:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.0026884109247475863 norm:0.0001485138782300055 max memory_allocated 29267.56298828125 
[2025-02-18 17:41:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.002343504223972559 norm:4.441821511136368e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:42:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0022776215337216854 norm:3.066532372031361e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:43:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.002246786840260029 norm:2.2492840798804536e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:44:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0022252206690609455 norm:1.6582311218371615e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:45:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.002215369138866663 norm:1.3400757779891137e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:45:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0022084699012339115 norm:1.117521333071636e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:46:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.00220437441021204 norm:9.902320016408339e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:47:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.002199621871113777 norm:8.922038432501722e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:48:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0021982756443321705 norm:8.14412396721309e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:49:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.002196424873545766 norm:7.494197689084103e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:50:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0021950870286673307 norm:6.8934991759306286e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:50:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.002194616012275219 norm:6.472857876360649e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:51:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0021932278759777546 norm:6.108564775786363e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:52:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0021936027333140373 norm:5.8675409491115715e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:53:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0021923459134995937 norm:5.79229981667595e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:54:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0021917696576565504 norm:5.706901902158279e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:55:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.002191814361140132 norm:5.594920367002487e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:55:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0021901808213442564 norm:5.569546374317724e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:56:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0021910450886934996 norm:5.515891189133981e-06 max memory_allocated 29267.56298828125 
[2025-02-18 17:56:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 17:57:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0034154357854276896 norm:0.0003718637162819505 max memory_allocated 29267.56298828125 
[2025-02-18 17:58:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.002640525344759226 norm:9.366810991195962e-05 max memory_allocated 29267.56298828125 
[2025-02-18 17:59:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0024913307279348373 norm:5.144681199453771e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:00:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0024510230869054794 norm:4.164225174463354e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:01:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.002418151590973139 norm:3.1740401027491316e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:01:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0023941979743540287 norm:2.4511395167792216e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:02:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.002381568308919668 norm:2.0279458112781867e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:03:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.002374299569055438 norm:1.7116863091359846e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:04:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.002366593573242426 norm:1.5132538464968093e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:05:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.002363218693062663 norm:1.3823952940583695e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:05:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0023603844456374645 norm:1.21264201879967e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:06:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0023585523013025522 norm:1.1022941180272028e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:07:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0023552225902676582 norm:1.015195175568806e-05 max memory_allocated 29267.56298828125 
[2025-02-18 18:08:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0023546465672552586 norm:9.245768524124287e-06 max memory_allocated 29267.56298828125 
[2025-02-18 18:09:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.002353362739086151 norm:8.597239684604574e-06 max memory_allocated 29267.56298828125 
[2025-02-18 18:10:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.002352582523599267 norm:7.947723133838736e-06 max memory_allocated 29267.56298828125 
[2025-02-18 18:10:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.002353777876123786 norm:7.292319423868321e-06 max memory_allocated 29267.56298828125 
[2025-02-18 18:11:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.002352287992835045 norm:6.94852315064054e-06 max memory_allocated 29267.56298828125 
[2025-02-18 18:12:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.002350592752918601 norm:6.745048267475795e-06 max memory_allocated 29267.56298828125 
[2025-02-18 18:13:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0023496903013437986 norm:6.589838903892087e-06 max memory_allocated 29267.56298828125 
[2025-02-18 18:13:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 18:14:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0030864437576383352 norm:0.00013796656276099384 max memory_allocated 29268.31298828125 
[2025-02-18 18:15:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0027373535558581352 norm:4.7538353101117536e-05 max memory_allocated 29268.31298828125 
[2025-02-18 18:16:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.002650201553478837 norm:2.889820098062046e-05 max memory_allocated 29268.31298828125 
[2025-02-18 18:16:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0026166809257119894 norm:2.2052812710171565e-05 max memory_allocated 29268.31298828125 
[2025-02-18 18:17:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0025903533678501844 norm:1.6114254322019406e-05 max memory_allocated 29268.31298828125 
[2025-02-18 18:18:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002576136961579323 norm:1.2763888662448153e-05 max memory_allocated 29268.31298828125 
[2025-02-18 18:19:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0025678144302219152 norm:1.0953458513540681e-05 max memory_allocated 29268.31298828125 
[2025-02-18 18:20:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0025614083278924227 norm:9.561272236169316e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:21:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.002557223429903388 norm:8.266571967396885e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:21:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0025533835869282484 norm:7.606498911627568e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:22:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.002550932113081217 norm:6.940240837138845e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:23:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0025498755276203156 norm:6.430807843571529e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:24:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.00254789344035089 norm:5.994174898660276e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:25:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0025467670056968927 norm:5.693020284525119e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:25:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0025458107702434063 norm:5.430570581665961e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:26:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.002545273629948497 norm:5.16692398377927e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:27:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.002545855473726988 norm:5.061930551164551e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:28:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.002545028692111373 norm:5.002822945243679e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:29:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0025434205308556557 norm:4.937397079629591e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:30:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.002543647773563862 norm:4.915533736493671e-06 max memory_allocated 29268.31298828125 
[2025-02-18 18:30:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 18:31:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0032211211510002613 norm:0.00014471786562353373 max memory_allocated 29268.50048828125 
[2025-02-18 18:31:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0028854478150606155 norm:4.898284169030376e-05 max memory_allocated 29268.50048828125 
[2025-02-18 18:32:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0028164940886199474 norm:3.08333765133284e-05 max memory_allocated 29268.50048828125 
[2025-02-18 18:33:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0027851315680891275 norm:2.2890710170031525e-05 max memory_allocated 29268.50048828125 
[2025-02-18 18:34:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0027642245404422283 norm:1.7154792658402584e-05 max memory_allocated 29268.50048828125 
[2025-02-18 18:35:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.002751340623944998 norm:1.3924651284469292e-05 max memory_allocated 29268.50048828125 
[2025-02-18 18:36:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0027436455711722374 norm:1.1961497875745408e-05 max memory_allocated 29268.50048828125 
[2025-02-18 18:36:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.002738194540143013 norm:1.0481472600076813e-05 max memory_allocated 29268.50048828125 
[2025-02-18 18:37:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0027335714548826218 norm:9.510587005934212e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:38:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.002729262225329876 norm:8.765708116698079e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:39:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.002727108309045434 norm:8.132236871460918e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:40:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0027255406603217125 norm:7.75302032707259e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:40:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0027241522911936045 norm:7.411433216475416e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:41:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0027228135149925947 norm:7.204002031357959e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:42:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002722665434703231 norm:6.9034608713991474e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:43:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0027213930152356625 norm:6.677446435787715e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:44:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0027206838130950928 norm:6.515518180094659e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:45:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.002720475662499666 norm:6.376195869961521e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:45:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.002720906399190426 norm:6.3976326600823086e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:46:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.002720621647313237 norm:6.377687441272428e-06 max memory_allocated 29268.50048828125 
[2025-02-18 18:46:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 18:47:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.003365354146808386 norm:9.434225648874417e-05 max memory_allocated 29268.68798828125 
[2025-02-18 18:48:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.003130336757749319 norm:3.4486729418858886e-05 max memory_allocated 29268.68798828125 
[2025-02-18 18:49:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.003075096756219864 norm:2.4587743610027246e-05 max memory_allocated 29268.68798828125 
[2025-02-18 18:50:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0030438019894063473 norm:1.8976763385580853e-05 max memory_allocated 29268.68798828125 
[2025-02-18 18:51:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.0030219717882573605 norm:1.4957108760427218e-05 max memory_allocated 29268.68798828125 
[2025-02-18 18:51:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.003011698368936777 norm:1.281241566175595e-05 max memory_allocated 29268.68798828125 
[2025-02-18 18:52:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.003002567682415247 norm:1.1486419680295512e-05 max memory_allocated 29268.68798828125 
[2025-02-18 18:53:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0029961823020130396 norm:1.055190932675032e-05 max memory_allocated 29268.68798828125 
[2025-02-18 18:54:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.002991939429193735 norm:9.604396836948581e-06 max memory_allocated 29268.68798828125 
[2025-02-18 18:55:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.0029884404502809048 norm:8.926564078137744e-06 max memory_allocated 29268.68798828125 
[2025-02-18 18:56:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.002985545666888356 norm:8.392856216232758e-06 max memory_allocated 29268.68798828125 
[2025-02-18 18:56:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0029837859328836203 norm:8.027060175663792e-06 max memory_allocated 29268.68798828125 
[2025-02-18 18:57:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0029834257438778877 norm:7.810775059624575e-06 max memory_allocated 29268.68798828125 
[2025-02-18 18:58:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.002982882782816887 norm:7.667786121601239e-06 max memory_allocated 29268.68798828125 
[2025-02-18 18:59:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.002981281839311123 norm:7.520950020989403e-06 max memory_allocated 29268.68798828125 
[2025-02-18 19:00:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.0029813048895448446 norm:7.4402896643732674e-06 max memory_allocated 29268.68798828125 
[2025-02-18 19:00:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.002980761928483844 norm:7.386579000012716e-06 max memory_allocated 29268.68798828125 
[2025-02-18 19:01:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.002980331424623728 norm:7.341551736317342e-06 max memory_allocated 29268.68798828125 
[2025-02-18 19:02:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0029807984828948975 norm:7.344051027757814e-06 max memory_allocated 29268.68798828125 
[2025-02-18 19:03:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0029803693760186434 norm:7.278661996679148e-06 max memory_allocated 29268.68798828125 
[2025-02-18 19:03:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 19:04:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.00403069332242012 norm:0.00021479721181094646 max memory_allocated 29268.87548828125 
[2025-02-18 19:05:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0035088045988231897 norm:7.56929803173989e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:06:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0033761044032871723 norm:4.5239074097480625e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:06:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0033319841604679823 norm:3.519649908412248e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:07:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.003297640010714531 norm:2.7418458557804115e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:08:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.0032704195473343134 norm:2.185485209338367e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:09:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0032538645900785923 norm:1.8346390788792633e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:10:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0032421385403722525 norm:1.5985913705662824e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:11:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0032347156666219234 norm:1.4188952263793908e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:11:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.003227583598345518 norm:1.2859353773819748e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:12:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0032229567877948284 norm:1.154114397650119e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:13:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.003220462240278721 norm:1.0580159141682088e-05 max memory_allocated 29268.87548828125 
[2025-02-18 19:14:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.003216708777472377 norm:9.863545528787654e-06 max memory_allocated 29268.87548828125 
[2025-02-18 19:15:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.003214893164113164 norm:9.316151590610389e-06 max memory_allocated 29268.87548828125 
[2025-02-18 19:15:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0032135662622749805 norm:8.891924153431319e-06 max memory_allocated 29268.87548828125 
[2025-02-18 19:16:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.0032117138616740704 norm:8.489357242069673e-06 max memory_allocated 29268.87548828125 
[2025-02-18 19:17:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0032098768278956413 norm:8.155978321155999e-06 max memory_allocated 29268.87548828125 
[2025-02-18 19:18:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.0032097192015498877 norm:7.881766578066163e-06 max memory_allocated 29268.87548828125 
[2025-02-18 19:19:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0032082307152450085 norm:7.713443665124942e-06 max memory_allocated 29268.87548828125 
[2025-02-18 19:20:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.003208005102351308 norm:7.559410278190626e-06 max memory_allocated 29268.87548828125 
[2025-02-18 19:20:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 19:21:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.004912111442536116 norm:0.0005192446988075972 max memory_allocated 29269.06298828125 
[2025-02-18 19:22:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.003975342959165573 norm:0.00013281872088555247 max memory_allocated 29269.06298828125 
[2025-02-18 19:22:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.003797238226979971 norm:7.809433009242639e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:23:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0037423758767545223 norm:6.398277764674276e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:24:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0037040747702121735 norm:5.2660892833955586e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:25:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.003668178804218769 norm:4.174778223386966e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:26:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0036463746801018715 norm:3.489669688860886e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:26:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0036289808340370655 norm:3.0073855668888427e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:27:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0036158179864287376 norm:2.6360370611655526e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:28:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0036071944050490856 norm:2.3690883608651347e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:29:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0035986590664833784 norm:2.1234347514109686e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:30:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.003592661814764142 norm:1.9458902897895314e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:31:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.003588834311813116 norm:1.7956564988708124e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:31:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.00358444731682539 norm:1.6548383428016677e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:32:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.00358206732198596 norm:1.5647534382878803e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:33:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0035793182905763388 norm:1.4729490430909209e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:34:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.0035750376991927624 norm:1.3562137610279024e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:35:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.0035732057876884937 norm:1.2667102055274881e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:35:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.003572199260815978 norm:1.197335950564593e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:36:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.003570135682821274 norm:1.1664437806757633e-05 max memory_allocated 29269.06298828125 
[2025-02-18 19:36:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 19:37:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.004736986011266708 norm:0.0002521888818591833 max memory_allocated 29269.25048828125 
[2025-02-18 19:38:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.0041849310509860516 norm:7.430649566231295e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:39:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.004065674263983965 norm:4.671654096455313e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:40:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.004027604125440121 norm:3.92784240830224e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:41:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.003991713281720877 norm:3.080764872720465e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:41:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0039641582407057285 norm:2.4278841010527685e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:42:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.003947320394217968 norm:2.0231087546562776e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:43:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.003934829030185938 norm:1.752796561049763e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:44:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.003926330711692572 norm:1.5667848856537603e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:45:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.003920185379683971 norm:1.4329241821542382e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:46:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.003914618864655495 norm:1.3086313629173674e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:46:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.003909219987690449 norm:1.2184857951069716e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:47:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0039055091328918934 norm:1.1285265827609692e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:48:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.003903701202943921 norm:1.0335366823710501e-05 max memory_allocated 29269.25048828125 
[2025-02-18 19:49:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.0039025014266371727 norm:9.539843631500844e-06 max memory_allocated 29269.25048828125 
[2025-02-18 19:50:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.003901189658790827 norm:8.979623089544475e-06 max memory_allocated 29269.25048828125 
[2025-02-18 19:50:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.003899424336850643 norm:8.637464816274587e-06 max memory_allocated 29269.25048828125 
[2025-02-18 19:51:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.003899582428857684 norm:8.302320566144772e-06 max memory_allocated 29269.25048828125 
[2025-02-18 19:52:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.003898803610354662 norm:8.100578270386904e-06 max memory_allocated 29269.25048828125 
[2025-02-18 19:53:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.00389812677167356 norm:7.938946509966627e-06 max memory_allocated 29269.25048828125 
[2025-02-18 19:53:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 19:54:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.005112833343446255 norm:0.00023879815125837922 max memory_allocated 29269.43798828125 
[2025-02-18 19:55:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.004576459061354399 norm:7.577862561447546e-05 max memory_allocated 29269.43798828125 
[2025-02-18 19:56:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.0044408743269741535 norm:4.47261736553628e-05 max memory_allocated 29269.43798828125 
[2025-02-18 19:57:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.004392756149172783 norm:3.583146462915465e-05 max memory_allocated 29269.43798828125 
[2025-02-18 19:57:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0043621063232421875 norm:2.9973774871905334e-05 max memory_allocated 29269.43798828125 
[2025-02-18 19:58:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.004334092140197754 norm:2.39448927459307e-05 max memory_allocated 29269.43798828125 
[2025-02-18 19:59:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.004316353239119053 norm:1.9602339307311922e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:00:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.004304821603000164 norm:1.680228342593182e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:01:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.004297945648431778 norm:1.5291458112187684e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:01:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.004289718344807625 norm:1.4002423995407298e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:02:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.0042835078202188015 norm:1.3036853488301858e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:03:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.004279014654457569 norm:1.2245734978932887e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:04:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.004275500774383545 norm:1.156505004473729e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:05:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.004272897727787495 norm:1.0955350262520369e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:06:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.0042697591707110405 norm:1.0395450772193726e-05 max memory_allocated 29269.43798828125 
[2025-02-18 20:06:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.00426799152046442 norm:9.953872904588934e-06 max memory_allocated 29269.43798828125 
[2025-02-18 20:07:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.004267423879355192 norm:9.695211701910011e-06 max memory_allocated 29269.43798828125 
[2025-02-18 20:08:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.004267050884664059 norm:9.405051059729885e-06 max memory_allocated 29269.43798828125 
[2025-02-18 20:09:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.004265205934643745 norm:8.965683264250401e-06 max memory_allocated 29269.43798828125 
[2025-02-18 20:10:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0042652832344174385 norm:8.796852853265591e-06 max memory_allocated 29269.43798828125 
[2025-02-18 20:10:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 20:11:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.0060043008998036385 norm:0.0003849466738756746 max memory_allocated 29269.62548828125 
[2025-02-18 20:12:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.005229901988059282 norm:0.00013974911416880786 max memory_allocated 29269.62548828125 
[2025-02-18 20:12:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.005035457666963339 norm:8.545788296032697e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:13:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.004969988018274307 norm:6.567405944224447e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:14:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.004926010966300964 norm:5.321086064213887e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:15:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.004888483788818121 norm:4.228758916724473e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:16:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.0048610251396894455 norm:3.473044125712477e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:16:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.004842501133680344 norm:2.908534588641487e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:17:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.004830518737435341 norm:2.487717938493006e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:18:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.004818658344447613 norm:2.2212952899280936e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:19:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.00481028575450182 norm:1.9837490981444716e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:20:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.0048042964190244675 norm:1.8178792743128724e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:21:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.004801944829523563 norm:1.6084048183984123e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:21:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.004798810929059982 norm:1.4571706742572132e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:22:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.0047960905358195305 norm:1.3446778211800847e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:23:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.004794260952621698 norm:1.256898212886881e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:24:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.004791786894202232 norm:1.1807277587649878e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:25:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.004790326114743948 norm:1.121095829148544e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:26:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.004788517486304045 norm:1.0655724508978892e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:26:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.004787295591086149 norm:1.0383673725300469e-05 max memory_allocated 29269.62548828125 
[2025-02-18 20:27:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 20:27:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.006453562527894974 norm:0.00033385728602297604 max memory_allocated 29269.81298828125 
[2025-02-18 20:28:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.0057862489484250546 norm:0.00010661302076186985 max memory_allocated 29269.81298828125 
[2025-02-18 20:29:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.005612812004983425 norm:6.264785042731091e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:30:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.005553608760237694 norm:5.1112590881530195e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:31:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.005513822194188833 norm:4.15916510974057e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:32:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.005476773716509342 norm:3.2756721338955685e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:32:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.005451753735542297 norm:2.722083081607707e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:33:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.005436368752270937 norm:2.3655982658965513e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:34:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.005424100439995527 norm:2.1175143047003075e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:35:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.0054158978164196014 norm:1.921341026900336e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:36:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.005407148506492376 norm:1.7672224203124642e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:36:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.0054003531113266945 norm:1.65221808856586e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:37:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.005394631996750832 norm:1.5403911675093696e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:38:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.005389628931879997 norm:1.4312673556560185e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:39:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.005385738331824541 norm:1.3406935067905579e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:40:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.0053824009373784065 norm:1.276901730307145e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:41:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.005380010697990656 norm:1.2193395377835259e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:41:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.005378614645451307 norm:1.1629477739916183e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:42:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.0053765797056257725 norm:1.1199506843695417e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:43:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.005374562460929155 norm:1.0860027941816952e-05 max memory_allocated 29269.81298828125 
[2025-02-18 20:43:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 20:44:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.007531933486461639 norm:0.0005157343111932278 max memory_allocated 29270.00048828125 
[2025-02-18 20:45:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.006582709960639477 norm:0.00016875489382073283 max memory_allocated 29270.00048828125 
[2025-02-18 20:46:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.006320595275610685 norm:9.531965770293027e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:47:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.006228300742805004 norm:7.290999928954989e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:47:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.006177959498018026 norm:5.993101513013244e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:48:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.006131192669272423 norm:4.8194604460150003e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:49:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.006094862706959248 norm:3.878733332385309e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:50:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.006071191281080246 norm:3.219806603738107e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:51:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.0060544125735759735 norm:2.7757243515225127e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:52:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.006040127482265234 norm:2.46388644882245e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:52:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.006031881086528301 norm:2.1955620468361303e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:53:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.006022722460329533 norm:2.002519613597542e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:54:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.006016097497195005 norm:1.8342765542911366e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:55:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.006010975688695908 norm:1.6874912034836598e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:56:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.006005503237247467 norm:1.5686646293033846e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:56:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.006002472713589668 norm:1.451132175134262e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:57:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.005998878739774227 norm:1.3542798114940524e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:58:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0059983753599226475 norm:1.2767891348630656e-05 max memory_allocated 29270.00048828125 
[2025-02-18 20:59:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.005996469408273697 norm:1.218015040649334e-05 max memory_allocated 29270.00048828125 
[2025-02-18 21:00:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.0059947483241558075 norm:1.1432516657805536e-05 max memory_allocated 29270.00048828125 
[2025-02-18 21:00:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 21:01:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.008551309816539288 norm:0.0007556919590570033 max memory_allocated 29270.18798828125 
[2025-02-18 21:02:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.007442261092364788 norm:0.00020794763986486942 max memory_allocated 29270.18798828125 
[2025-02-18 21:02:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.007170397322624922 norm:0.00011032729526050389 max memory_allocated 29270.18798828125 
[2025-02-18 21:03:47 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.007079841569066048 norm:8.718524622963741e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:04:36 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.007027496118098497 norm:7.441471825586632e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:05:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.006980759557336569 norm:5.905160287511535e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:06:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.006944106426090002 norm:4.78545407531783e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:07:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.006917633581906557 norm:3.996781015302986e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:07:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.006898058578372002 norm:3.485182605800219e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:08:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.006881986279040575 norm:3.083188130403869e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:09:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.006869987118989229 norm:2.779046553769149e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:10:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.00686025433242321 norm:2.552035584812984e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:11:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.006852299440652132 norm:2.342141669942066e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:11:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.006845626048743725 norm:2.1591633412754163e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:12:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.006839876063168049 norm:1.979750959435478e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:13:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.006834567990154028 norm:1.8404489310341887e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:14:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.0068306392058730125 norm:1.719919418974314e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:15:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.006827362813055515 norm:1.6124642570503056e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:16:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.0068244775757193565 norm:1.5166813682299107e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:16:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.0068232412450015545 norm:1.4240675227483734e-05 max memory_allocated 29270.18798828125 
[2025-02-18 21:17:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 21:18:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.008939944207668304 norm:0.00036202347837388515 max memory_allocated 29270.37548828125 
[2025-02-18 21:18:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.008259079419076443 norm:0.00012867472833022475 max memory_allocated 29270.37548828125 
[2025-02-18 21:19:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.008062913082540035 norm:7.821214967407286e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:20:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.007984314113855362 norm:6.11672003287822e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:21:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.007933278568089008 norm:4.9874339310918e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:22:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.007892336696386337 norm:4.012702265754342e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:22:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.007859951816499233 norm:3.256033596699126e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:23:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.007837806828320026 norm:2.7821180992759764e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:24:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.00782288983464241 norm:2.4420840418315493e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:25:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.007811491843312979 norm:2.190523809986189e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:26:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.007803408429026604 norm:2.019830935751088e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:27:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.007793956436216831 norm:1.8783655832521617e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:27:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.007788076996803284 norm:1.731241718516685e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:28:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.007783594541251659 norm:1.63502263603732e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:29:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.007778111379593611 norm:1.551908462715801e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:30:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.007774342317134142 norm:1.5027952940727118e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:31:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.007773295044898987 norm:1.4372734767675865e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:31:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.007772359065711498 norm:1.3873725038138218e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:32:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.007770161144435406 norm:1.3546177797252312e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:33:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.007767257746309042 norm:1.3293675692693796e-05 max memory_allocated 29270.37548828125 
[2025-02-18 21:33:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 21:34:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.0103768240660429 norm:0.0004949702997691929 max memory_allocated 29270.56298828125 
[2025-02-18 21:35:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.009593451395630836 norm:0.00018522828759159893 max memory_allocated 29270.56298828125 
[2025-02-18 21:36:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.009363369084894657 norm:0.00011973890650551766 max memory_allocated 29270.56298828125 
[2025-02-18 21:37:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.009257231839001179 norm:9.415126987732947e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:38:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.009182171896100044 norm:7.56772788008675e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:38:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.009123087860643864 norm:6.04666602157522e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:39:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.009079274721443653 norm:4.9793026846600696e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:40:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.00904116127640009 norm:4.1743507608771324e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:41:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.00901343859732151 norm:3.6498240660876036e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:42:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.008995676413178444 norm:3.245816333219409e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:42:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.008980758488178253 norm:2.918255268014036e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:43:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.008970420807600021 norm:2.6064353733090684e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:44:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.008961536921560764 norm:2.324713750567753e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:45:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.008955643512308598 norm:2.100011261063628e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:46:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.008949271403253078 norm:1.9748445993172936e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:47:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.008945201523602009 norm:1.8468792404746637e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:47:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.008944212459027767 norm:1.7621125152800232e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:48:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.008939355611801147 norm:1.694580350886099e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:49:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.00893784686923027 norm:1.6340789443347603e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:50:18 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.008935809135437012 norm:1.587791120982729e-05 max memory_allocated 29270.56298828125 
[2025-02-18 21:50:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 21:51:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.011198299005627632 norm:0.000471475679660216 max memory_allocated 29270.75048828125 
[2025-02-18 21:52:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.010525448247790337 norm:0.000158123264554888 max memory_allocated 29270.75048828125 
[2025-02-18 21:53:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.010358765721321106 norm:9.851773211266845e-05 max memory_allocated 29270.75048828125 
[2025-02-18 21:53:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.01028926856815815 norm:7.453341095242649e-05 max memory_allocated 29270.75048828125 
[2025-02-18 21:54:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.010246125981211662 norm:5.7235200074501336e-05 max memory_allocated 29270.75048828125 
[2025-02-18 21:55:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.01021104771643877 norm:4.4517655624076724e-05 max memory_allocated 29270.75048828125 
[2025-02-18 21:56:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.010185807012021542 norm:3.477076461422257e-05 max memory_allocated 29270.75048828125 
[2025-02-18 21:57:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.010169455781579018 norm:2.8848577130702324e-05 max memory_allocated 29270.75048828125 
[2025-02-18 21:57:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.010154961608350277 norm:2.502759343769867e-05 max memory_allocated 29270.75048828125 
[2025-02-18 21:58:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.010148235596716404 norm:2.2107342374511063e-05 max memory_allocated 29270.75048828125 
[2025-02-18 21:59:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.010144075378775597 norm:1.9824987248284742e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:00:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.010140152648091316 norm:1.8646485841600224e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:01:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.01013670302927494 norm:1.7638869394431822e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:02:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.010135764256119728 norm:1.686609539319761e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:02:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.010133148171007633 norm:1.6332902305293828e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:03:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.010131878778338432 norm:1.5866524336161092e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:04:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.01012941263616085 norm:1.5651901776436716e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:05:21 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.010125340893864632 norm:1.5397155948448926e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:06:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.010124854743480682 norm:1.5286954294424504e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:07:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.010124532505869865 norm:1.5172444363997784e-05 max memory_allocated 29270.75048828125 
[2025-02-18 22:07:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 22:08:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.012511871755123138 norm:0.0004702737496700138 max memory_allocated 29270.93798828125 
[2025-02-18 22:08:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.011822303757071495 norm:0.0001662513823248446 max memory_allocated 29270.93798828125 
[2025-02-18 22:09:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.011660957708954811 norm:0.00011318123870296404 max memory_allocated 29270.93798828125 
[2025-02-18 22:10:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.011575767770409584 norm:8.842251554597169e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:11:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.011513066478073597 norm:6.87667416059412e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:12:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.011465254239737988 norm:5.4030129831517115e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:13:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.011434687301516533 norm:4.438548057805747e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:13:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.01140931248664856 norm:3.8395985029637814e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:14:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.011393079534173012 norm:3.315935828140937e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:15:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.011381343938410282 norm:2.9660506697837263e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:16:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.011371854692697525 norm:2.6008710847236216e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:17:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.011365782469511032 norm:2.3035645426716655e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:17:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.0113624706864357 norm:2.0379393390612677e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:18:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.011355150490999222 norm:1.8928631106973626e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:19:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.011353598907589912 norm:1.7273145203944296e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:20:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.011351557448506355 norm:1.6203883205889724e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:21:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.011347425170242786 norm:1.5365985746029764e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:22:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.011345972307026386 norm:1.4363140508066863e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:22:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.011345438659191132 norm:1.3779712389805354e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:23:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.011343682184815407 norm:1.3452590792439878e-05 max memory_allocated 29270.93798828125 
[2025-02-18 22:23:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 22:24:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.013236409053206444 norm:0.0002115237439284101 max memory_allocated 29271.12548828125 
[2025-02-18 22:25:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.01293954998254776 norm:0.00010373016993980855 max memory_allocated 29271.12548828125 
[2025-02-18 22:26:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.012831333093345165 norm:7.096494664438069e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:27:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.012763144448399544 norm:5.217680154601112e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:28:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.012716798111796379 norm:4.034094308735803e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:28:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.012684066779911518 norm:3.310538886580616e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:29:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.012659203261137009 norm:2.7815172870759852e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:30:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.01264195330440998 norm:2.346246583329048e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:31:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.01262998953461647 norm:2.0007919374620542e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:32:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.012621600180864334 norm:1.7548341929796152e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:33:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.01261382270604372 norm:1.5689769497839734e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:33:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.012610236182808876 norm:1.4140136045170948e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:34:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.012608738616108894 norm:1.3115346519043669e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:35:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.012605135329067707 norm:1.2544493984023575e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:36:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.012601964175701141 norm:1.2057123967679217e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:37:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.01259914506226778 norm:1.1725042895704973e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:37:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.012598798610270023 norm:1.1466045179986395e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:38:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.012597495689988136 norm:1.1354555681464262e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:39:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.012595484033226967 norm:1.1110840205219574e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:40:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.012595291249454021 norm:1.0980949809891172e-05 max memory_allocated 29271.12548828125 
[2025-02-18 22:40:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 22:41:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.01472929771989584 norm:0.00023099126701708883 max memory_allocated 29271.31298828125 
[2025-02-18 22:42:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.014433934353291988 norm:0.00011026085849152878 max memory_allocated 29271.31298828125 
[2025-02-18 22:43:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.014324216172099113 norm:7.772439857944846e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:44:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.014253811910748482 norm:5.791554940515198e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:44:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.014205418527126312 norm:4.5592001697514206e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:45:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.014168759807944298 norm:3.7257927033351734e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:46:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.01414492353796959 norm:3.208858470316045e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:47:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.014130562543869019 norm:2.785832475638017e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:48:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.014118283987045288 norm:2.493141801096499e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:48:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.014106055721640587 norm:2.2192562028067186e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:49:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.014096499420702457 norm:2.007321745622903e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:50:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.01408814825117588 norm:1.847408384492155e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:51:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.014079494401812553 norm:1.7473763364250772e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:52:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.014073695056140423 norm:1.614281973161269e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:53:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.014068045653402805 norm:1.554517621116247e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:53:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.014063906855881214 norm:1.4873255167913157e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:54:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.014060014858841896 norm:1.4431439922191203e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:55:28 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.014053520746529102 norm:1.398951826558914e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:56:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.014049803838133812 norm:1.3655808288604021e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:57:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.014046579599380493 norm:1.3417996342468541e-05 max memory_allocated 29271.31298828125 
[2025-02-18 22:57:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 22:58:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.017283661291003227 norm:0.0009769306052476168 max memory_allocated 29271.50048828125 
[2025-02-18 22:59:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.016108570620417595 norm:0.0002702201309148222 max memory_allocated 29271.50048828125 
[2025-02-18 22:59:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.015865083783864975 norm:0.00015509818331338465 max memory_allocated 29271.50048828125 
[2025-02-18 23:00:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.015780115500092506 norm:0.00012430660717654973 max memory_allocated 29271.50048828125 
[2025-02-18 23:01:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.015729611739516258 norm:0.00010365148773416877 max memory_allocated 29271.50048828125 
[2025-02-18 23:02:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.015679117292165756 norm:8.283786155516282e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:03:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.015629298985004425 norm:6.556906009791419e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:03:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.015594758093357086 norm:5.498539030668326e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:04:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.015575249679386616 norm:4.724284735857509e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:05:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.015561653301119804 norm:4.271055513527244e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:06:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.015549900941550732 norm:3.9175258280010894e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:07:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.015538113191723824 norm:3.573993672034703e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:08:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.015529418364167213 norm:3.352815838297829e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:08:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.015521696768701077 norm:3.0632465495727956e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:09:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.015515054576098919 norm:2.8196742277941667e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:10:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.015511933714151382 norm:2.5806566554820165e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:11:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.015506785362958908 norm:2.3937845980981365e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:12:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.015504530631005764 norm:2.2365084078046493e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:12:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.015500432811677456 norm:2.0682124159066007e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:13:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.01549897063523531 norm:1.9411583707551472e-05 max memory_allocated 29271.50048828125 
[2025-02-18 23:14:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 23:14:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.017622552812099457 norm:0.00024696087348274887 max memory_allocated 29271.68798828125 
[2025-02-18 23:15:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.017326802015304565 norm:0.00011440595699241385 max memory_allocated 29271.68798828125 
[2025-02-18 23:16:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.0172364953905344 norm:8.214749686885625e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:17:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.017170703038573265 norm:5.974395025987178e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:18:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.017125410959124565 norm:4.595809514285065e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:19:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.01709255762398243 norm:3.6617351724999025e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:19:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.01707141287624836 norm:3.079273665207438e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:20:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.017055761069059372 norm:2.599539584480226e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:21:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.017046388238668442 norm:2.1374622519942932e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:22:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.017039921134710312 norm:1.7919250240083784e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:23:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.017035942524671555 norm:1.542265272291843e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:23:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.01703362911939621 norm:1.3777681488136295e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:24:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.017032256349921227 norm:1.279685693589272e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:25:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.017029104754328728 norm:1.2176553354947828e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:26:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.017025528475642204 norm:1.18851603474468e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:27:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.01702510379254818 norm:1.153438279288821e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:28:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.017024550586938858 norm:1.1354322850820608e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:28:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.017021529376506805 norm:1.1247499060118571e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:29:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.017019903287291527 norm:1.1161237125634216e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:30:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.017019592225551605 norm:1.1013933544745669e-05 max memory_allocated 29271.68798828125 
[2025-02-18 23:30:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 23:31:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.01937931776046753 norm:0.0002255431900266558 max memory_allocated 29271.87548828125 
[2025-02-18 23:32:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.01911565475165844 norm:0.00012491414963733405 max memory_allocated 29271.87548828125 
[2025-02-18 23:33:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.01900114119052887 norm:8.630229422124103e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:34:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.018928389996290207 norm:6.480133743025362e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:34:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.01888570934534073 norm:5.115104795549996e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:35:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.01885116472840309 norm:4.2238574678776786e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:36:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.018826577812433243 norm:3.543001366779208e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:37:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.018806686624884605 norm:3.0447408789768815e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:38:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.01879281736910343 norm:2.6596531824907288e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:38:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.01878272369503975 norm:2.3929796952870674e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:39:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.018770765513181686 norm:2.19664943870157e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:40:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.018763547763228416 norm:2.0208946807542816e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:41:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.018753904849290848 norm:1.8397964595351368e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:42:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.018747685477137566 norm:1.756908568495419e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:43:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.018742205575108528 norm:1.619998693058733e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:43:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.01873798668384552 norm:1.5367577361757867e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:44:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.01873372495174408 norm:1.4742080566065852e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:45:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.0187318567186594 norm:1.4242651559470687e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:46:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.018729425966739655 norm:1.3531865079130512e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:47:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.018728505820035934 norm:1.321481067861896e-05 max memory_allocated 29271.87548828125 
[2025-02-18 23:47:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 23:48:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.02110457420349121 norm:0.00022785866167396307 max memory_allocated 29272.06298828125 
[2025-02-18 23:49:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.02083401009440422 norm:0.00011364654346834868 max memory_allocated 29272.06298828125 
[2025-02-18 23:49:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.020723626017570496 norm:7.681114948354661e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:50:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.0206567645072937 norm:5.598978168563917e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:51:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.02061212994158268 norm:4.374572745291516e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:52:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.020580213516950607 norm:3.567181920516305e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:53:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.020557716488838196 norm:2.992232475662604e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:54:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.020541854202747345 norm:2.5584098693798296e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:54:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.02053450234234333 norm:2.0652954844990745e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:55:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.02052875980734825 norm:1.70670791703742e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:56:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.02052466571331024 norm:1.4722626474394929e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:57:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.020521651953458786 norm:1.3074884009256493e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:58:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.020518824458122253 norm:1.1941550837946124e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:58:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.0205170176923275 norm:1.1066389561165124e-05 max memory_allocated 29272.06298828125 
[2025-02-18 23:59:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.02051440253853798 norm:1.0551195373409428e-05 max memory_allocated 29272.06298828125 
[2025-02-19 00:00:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.020512191578745842 norm:1.0243579708912876e-05 max memory_allocated 29272.06298828125 
[2025-02-19 00:01:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.020511113107204437 norm:1.0035558261733968e-05 max memory_allocated 29272.06298828125 
[2025-02-19 00:02:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.020509056746959686 norm:9.872153896139935e-06 max memory_allocated 29272.06298828125 
[2025-02-19 00:03:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.02050892636179924 norm:9.697373570816126e-06 max memory_allocated 29272.06298828125 
[2025-02-19 00:03:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.02050628885626793 norm:9.69168195297243e-06 max memory_allocated 29272.06298828125 
[2025-02-19 00:04:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-19 00:04:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.022896723821759224 norm:0.0002251574187539518 max memory_allocated 29272.25048828125 
[2025-02-19 00:05:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.02267354167997837 norm:0.00012151279952377081 max memory_allocated 29272.25048828125 
[2025-02-19 00:06:37 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.022553585469722748 norm:7.99339686636813e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:07:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.022478407248854637 norm:5.918129318160936e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:08:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.022428171709179878 norm:4.6580578782595694e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:09:04 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.022397225722670555 norm:3.813817602349445e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:09:53 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.022375769913196564 norm:3.167511022184044e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:10:42 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.02235800214111805 norm:2.6031328161479905e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:11:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.02234215848147869 norm:2.196821333200205e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:12:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.022332174703478813 norm:1.9091647118330002e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:13:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.022324249148368835 norm:1.6270325431833044e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:13:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.022321030497550964 norm:1.4319834917841945e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:14:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.02231989987194538 norm:1.2418017831805628e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:15:37 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.022317256778478622 norm:1.1544225344550796e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:16:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.022315338253974915 norm:1.103158592741238e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:17:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.022312065586447716 norm:1.0749175999080762e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:18:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.0223102904856205 norm:1.0601538633636665e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:18:54 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.022307731211185455 norm:1.0443050996400416e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:19:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.02230539731681347 norm:1.0340299013478216e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:20:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.022302601486444473 norm:1.0187812222284265e-05 max memory_allocated 29272.25048828125 
[2025-02-19 00:20:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-19 00:21:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.02510332688689232 norm:0.00022654228087048978 max memory_allocated 29272.43798828125 
[2025-02-19 00:22:28 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.02483564242720604 norm:0.00012230430729687214 max memory_allocated 29272.43798828125 
[2025-02-19 00:23:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.024707399308681488 norm:8.053630153881386e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:24:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.02464171312749386 norm:5.94141602050513e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:24:56 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.024606842547655106 norm:4.6349585318239406e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:25:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.02457781322300434 norm:3.699963417602703e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:26:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.024554714560508728 norm:3.0964431061875075e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:27:23 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.024540923535823822 norm:2.6948502636514604e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:28:12 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.02452734485268593 norm:2.3618287741555832e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:29:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.024515734985470772 norm:2.101742211380042e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:29:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.02450641430914402 norm:1.895237437565811e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:30:40 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.0245004054158926 norm:1.764320768415928e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:31:29 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.0244930200278759 norm:1.6678895917721093e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:32:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.024489708244800568 norm:1.5270052244886756e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:33:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.024484777823090553 norm:1.4874031876388472e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:33:57 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.024481885135173798 norm:1.411077391821891e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:34:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.024478517472743988 norm:1.3622337064589374e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:35:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.024475058540701866 norm:1.3377468349062838e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:36:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.02447044476866722 norm:1.2884389434475452e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:37:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.024468259885907173 norm:1.280027845496079e-05 max memory_allocated 29272.43798828125 
[2025-02-19 00:37:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-19 00:38:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.027836468070745468 norm:0.00026367747341282666 max memory_allocated 29272.62548828125 
[2025-02-19 00:39:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.027494873851537704 norm:0.0001604795252205804 max memory_allocated 29272.62548828125 
[2025-02-19 00:39:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.02731137163937092 norm:0.00011232317046960816 max memory_allocated 29272.62548828125 
[2025-02-19 00:40:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.027197640389204025 norm:8.462120604235679e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:41:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.027123691514134407 norm:6.693123577861115e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:42:26 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.02707323245704174 norm:5.404483454185538e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:43:15 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.027033554390072823 norm:4.4708478526445106e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:44:04 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.027008920907974243 norm:3.769878821913153e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:44:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.02698470465838909 norm:3.2311112590832636e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:45:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.026971692219376564 norm:2.788336860248819e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:46:32 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.02695668488740921 norm:2.4812659830786288e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:47:21 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.026947790756821632 norm:2.2172594981384464e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:48:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.026941491290926933 norm:1.9661529222503304e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:48:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.02693290263414383 norm:1.8083330360241234e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:49:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.026927510276436806 norm:1.6852483895490877e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:50:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.026923423632979393 norm:1.56732330651721e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:51:26 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.026922134682536125 norm:1.4806940271228086e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:52:16 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.026916643604636192 norm:1.426872859155992e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:53:05 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.02691340632736683 norm:1.3706369827559683e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:53:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.026909738779067993 norm:1.3309854693943635e-05 max memory_allocated 29272.62548828125 
[2025-02-19 00:54:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-19 00:55:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.030709242448210716 norm:0.0002503030118532479 max memory_allocated 29272.81298828125 
[2025-02-19 00:55:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.030283449217677116 norm:0.0001509190333308652 max memory_allocated 29272.81298828125 
[2025-02-19 00:56:40 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.030043194070458412 norm:0.00010733863746281713 max memory_allocated 29272.81298828125 
[2025-02-19 00:57:30 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.029887458309531212 norm:8.222537871915847e-05 max memory_allocated 29272.81298828125 
[2025-02-19 00:58:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.029781682416796684 norm:6.745370046701282e-05 max memory_allocated 29272.81298828125 
[2025-02-19 00:59:08 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.02970770187675953 norm:5.596690971287899e-05 max memory_allocated 29272.81298828125 
[2025-02-19 00:59:57 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.029654348269104958 norm:4.75252527394332e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:00:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.029612721875309944 norm:4.105820335098542e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:01:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.02958011068403721 norm:3.630393257481046e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:02:25 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.02955363132059574 norm:3.2458505302201957e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:03:14 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.029532337561249733 norm:2.9385184461716563e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:04:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.029518231749534607 norm:2.7018988475902006e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:04:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.02950265072286129 norm:2.501151720935013e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:05:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.029493574053049088 norm:2.3137719836086035e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:06:31 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.02948451042175293 norm:2.161915290344041e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:07:20 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.029477763921022415 norm:2.071363451250363e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:08:09 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.02947114408016205 norm:1.9803499526460655e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:08:58 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.029466507956385612 norm:1.913357846206054e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:09:47 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.029463699087500572 norm:1.8658967746887356e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:10:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.029462484642863274 norm:1.798689663701225e-05 max memory_allocated 29272.81298828125 
[2025-02-19 01:10:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-19 01:10:54 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:11:44 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.03381728008389473 norm:0.002779601840302348 max memory_allocated 29273.14501953125 
[2025-02-19 01:12:33 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.03331562131643295 norm:0.0024430330377072096 max memory_allocated 29273.14501953125 
[2025-02-19 01:13:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.03301431983709335 norm:0.0021310863085091114 max memory_allocated 29273.14501953125 
[2025-02-19 01:14:12 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.03279939293861389 norm:0.0018306623678654432 max memory_allocated 29273.14501953125 
[2025-02-19 01:15:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.032629091292619705 norm:0.0015035331016406417 max memory_allocated 29273.14501953125 
[2025-02-19 01:15:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.032496754080057144 norm:0.0012520472519099712 max memory_allocated 29273.14501953125 
[2025-02-19 01:16:40 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.032419946044683456 norm:0.0011202029418200254 max memory_allocated 29273.14501953125 
[2025-02-19 01:17:29 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.03235763683915138 norm:0.0010161390528082848 max memory_allocated 29273.14501953125 
[2025-02-19 01:18:18 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.032311782240867615 norm:0.0009780353866517544 max memory_allocated 29273.14501953125 
[2025-02-19 01:19:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.032305195927619934 norm:0.0009589942055754364 max memory_allocated 29273.14501953125 
[2025-02-19 01:19:57 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.03229396417737007 norm:0.0009986127261072397 max memory_allocated 29273.14501953125 
[2025-02-19 01:20:46 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.032340094447135925 norm:0.0009881476871669292 max memory_allocated 29273.14501953125 
[2025-02-19 01:21:35 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.03243780881166458 norm:0.0011276223231106997 max memory_allocated 29273.14501953125 
[2025-02-19 01:22:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.032341763377189636 norm:0.001050118706189096 max memory_allocated 29273.14501953125 
[2025-02-19 01:23:14 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.03219926357269287 norm:0.0008757598116062582 max memory_allocated 29273.14501953125 
[2025-02-19 01:24:03 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.03214891627430916 norm:0.0007464423542842269 max memory_allocated 29273.14501953125 
[2025-02-19 01:24:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.03213845565915108 norm:0.0007133382605388761 max memory_allocated 29273.14501953125 
[2025-02-19 01:25:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.03215150907635689 norm:0.0007271685753948987 max memory_allocated 29273.14501953125 
[2025-02-19 01:26:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.03215331956744194 norm:0.0007666450692340732 max memory_allocated 29273.14501953125 
[2025-02-19 01:27:20 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.03216025233268738 norm:0.0007443225476890802 max memory_allocated 29273.14501953125 
[2025-02-19 01:27:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-19 01:27:38 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:28:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.03873283043503761 norm:0.002939075231552124 max memory_allocated 29273.33251953125 
[2025-02-19 01:29:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.03781619668006897 norm:0.0023489000741392374 max memory_allocated 29273.33251953125 
[2025-02-19 01:30:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.037316225469112396 norm:0.00205695116892457 max memory_allocated 29273.33251953125 
[2025-02-19 01:30:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.036984432488679886 norm:0.0017954050563275814 max memory_allocated 29273.33251953125 
[2025-02-19 01:31:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.036731231957674026 norm:0.0015696983318775892 max memory_allocated 29273.33251953125 
[2025-02-19 01:32:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.03655197098851204 norm:0.0014525509905070066 max memory_allocated 29273.33251953125 
[2025-02-19 01:33:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.03640761226415634 norm:0.0013175267959013581 max memory_allocated 29273.33251953125 
[2025-02-19 01:34:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.03631916269659996 norm:0.00125822052359581 max memory_allocated 29273.33251953125 
[2025-02-19 01:35:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.03623224049806595 norm:0.00113640446215868 max memory_allocated 29273.33251953125 
[2025-02-19 01:35:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.036228347569704056 norm:0.0011691859690472484 max memory_allocated 29273.33251953125 
[2025-02-19 01:36:41 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.036259911954402924 norm:0.001248499727807939 max memory_allocated 29273.33251953125 
[2025-02-19 01:37:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.03737920895218849 norm:0.002744989702478051 max memory_allocated 29273.33251953125 
[2025-02-19 01:38:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.03632894158363342 norm:0.0015836571110412478 max memory_allocated 29273.33251953125 
[2025-02-19 01:39:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.03625478595495224 norm:0.0013780823210254312 max memory_allocated 29273.33251953125 
[2025-02-19 01:39:58 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.036196041852235794 norm:0.0012242007069289684 max memory_allocated 29273.33251953125 
[2025-02-19 01:40:47 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.03601178154349327 norm:0.0010544362012296915 max memory_allocated 29273.33251953125 
[2025-02-19 01:41:36 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.03595801070332527 norm:0.0009385399171151221 max memory_allocated 29273.33251953125 
[2025-02-19 01:42:26 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.03592010214924812 norm:0.0008569661295041442 max memory_allocated 29273.33251953125 
[2025-02-19 01:43:15 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.03589388355612755 norm:0.0007997200591489673 max memory_allocated 29273.33251953125 
[2025-02-19 01:44:04 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.035873107612133026 norm:0.0007535196491517127 max memory_allocated 29273.33251953125 
[2025-02-19 01:44:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-19 01:44:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 01:45:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.06643909960985184 norm:0.036383576691150665 max memory_allocated 29273.52001953125 
[2025-02-19 01:46:01 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.05558689683675766 norm:0.021573040634393692 max memory_allocated 29273.52001953125 
[2025-02-19 01:46:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.05293683707714081 norm:0.013741711154580116 max memory_allocated 29273.52001953125 
[2025-02-19 01:47:39 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.051509782671928406 norm:0.009116386994719505 max memory_allocated 29273.52001953125 
[2025-02-19 01:48:29 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.05044680833816528 norm:0.005961274728178978 max memory_allocated 29273.52001953125 
[2025-02-19 01:49:18 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.04977255314588547 norm:0.0045117647387087345 max memory_allocated 29273.52001953125 
[2025-02-19 01:50:07 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.04928082972764969 norm:0.0027302803937345743 max memory_allocated 29273.52001953125 
[2025-02-19 01:50:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.04884016141295433 norm:0.0026416191831231117 max memory_allocated 29273.52001953125 
[2025-02-19 01:51:46 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.04867904260754585 norm:0.003250501351431012 max memory_allocated 29273.52001953125 
[2025-02-19 01:52:35 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.04842865467071533 norm:0.0033414200879633427 max memory_allocated 29273.52001953125 
[2025-02-19 01:53:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.04818320274353027 norm:0.003125803777948022 max memory_allocated 29273.52001953125 
[2025-02-19 01:54:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.04805930331349373 norm:0.003078354522585869 max memory_allocated 29273.52001953125 
[2025-02-19 01:55:03 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.047834932804107666 norm:0.0027542593888938427 max memory_allocated 29273.52001953125 
[2025-02-19 01:55:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.04764150455594063 norm:0.002754743443801999 max memory_allocated 29273.52001953125 
[2025-02-19 01:56:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.047512516379356384 norm:0.002579670399427414 max memory_allocated 29273.52001953125 
[2025-02-19 01:57:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.047421738505363464 norm:0.002364181913435459 max memory_allocated 29273.52001953125 
[2025-02-19 01:58:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.04731838032603264 norm:0.0023507443256676197 max memory_allocated 29273.52001953125 
[2025-02-19 01:59:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.047196537256240845 norm:0.002150455955415964 max memory_allocated 29273.52001953125 
[2025-02-19 01:59:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.04703092575073242 norm:0.0017971383640542626 max memory_allocated 29273.52001953125 
[2025-02-19 02:00:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.04699540138244629 norm:0.0019615055061876774 max memory_allocated 29273.52001953125 
[2025-02-19 02:01:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-19 02:01:05 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 02:01:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.09629970043897629 norm:0.007867160253226757 max memory_allocated 29273.70751953125 
[2025-02-19 02:02:44 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.09144271910190582 norm:0.0065397038124501705 max memory_allocated 29273.70751953125 
[2025-02-19 02:03:33 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.08824234455823898 norm:0.006444268394261599 max memory_allocated 29273.70751953125 
[2025-02-19 02:04:23 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.08601314574480057 norm:0.006929883733391762 max memory_allocated 29273.70751953125 
[2025-02-19 02:05:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.08419138938188553 norm:0.0069826203398406506 max memory_allocated 29273.70751953125 
[2025-02-19 02:06:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.08334018290042877 norm:0.008305765688419342 max memory_allocated 29273.70751953125 
[2025-02-19 02:06:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.08241425454616547 norm:0.006296426057815552 max memory_allocated 29273.70751953125 
[2025-02-19 02:07:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.08069015294313431 norm:0.005177126731723547 max memory_allocated 29273.70751953125 
[2025-02-19 02:08:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.08008896559476852 norm:0.005981051828712225 max memory_allocated 29273.70751953125 
[2025-02-19 02:09:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.07963354140520096 norm:0.006329160649329424 max memory_allocated 29273.70751953125 
[2025-02-19 02:10:07 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.07949590682983398 norm:0.006839807610958815 max memory_allocated 29273.70751953125 
[2025-02-19 02:10:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.07909324765205383 norm:0.006674434524029493 max memory_allocated 29273.70751953125 
[2025-02-19 02:11:46 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.0787905901670456 norm:0.005930555518716574 max memory_allocated 29273.70751953125 
[2025-02-19 02:12:35 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.07804054766893387 norm:0.004992811940610409 max memory_allocated 29273.70751953125 
[2025-02-19 02:13:25 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.07766653597354889 norm:0.0058840117417275906 max memory_allocated 29273.70751953125 
[2025-02-19 02:14:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.07765264809131622 norm:0.006549282930791378 max memory_allocated 29273.70751953125 
[2025-02-19 02:15:03 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.07791311293840408 norm:0.006983071565628052 max memory_allocated 29273.70751953125 
[2025-02-19 02:15:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.0775333121418953 norm:0.005162206944078207 max memory_allocated 29273.70751953125 
[2025-02-19 02:16:42 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.0767638087272644 norm:0.005029613617807627 max memory_allocated 29273.70751953125 
[2025-02-19 02:17:31 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.07674291729927063 norm:0.005667273886501789 max memory_allocated 29273.70751953125 
[2025-02-19 02:17:45 root] (main_calibration.py 365): INFO 40072.73872447014
[2025-02-19 02:18:47 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-19 02:20:43 root] (main_calibration.py 158): INFO wikitext2 : 5.107481002807617
[2025-02-19 02:20:43 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-19 02:23:42 root] (main_calibration.py 158): INFO c4 : 6.6295623779296875
[2025-02-19 04:22:14 root] (main_calibration.py 169): INFO {'wikitext2': 5.107481002807617, 'c4': 6.6295623779296875, 'results': {'hellaswag': {'acc': 0.5898227444732125, 'acc_stderr': 0.0049086047320828115, 'acc_norm': 0.7616012746464847, 'acc_norm_stderr': 0.004252333443827118}, 'boolq': {'acc': 0.6847094801223241, 'acc_stderr': 0.008126455592662889}, 'arc_challenge': {'acc': 0.4351535836177474, 'acc_stderr': 0.014487986197186043, 'acc_norm': 0.44368600682593856, 'acc_norm_stderr': 0.014518421825670452}, 'winogrande': {'acc': 0.7032359905288083, 'acc_stderr': 0.012839239695202025}, 'arc_easy': {'acc': 0.742003367003367, 'acc_stderr': 0.008977970005203404, 'acc_norm': 0.5925925925925926, 'acc_norm_stderr': 0.010082326627832863}, 'piqa': {'acc': 0.7910772578890098, 'acc_stderr': 0.009485227030105093, 'acc_norm': 0.7905331882480957, 'acc_norm_stderr': 0.00949430297981981}}, 'versions': {'hellaswag': 0, 'boolq': 1, 'arc_challenge': 0, 'winogrande': 0, 'arc_easy': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
