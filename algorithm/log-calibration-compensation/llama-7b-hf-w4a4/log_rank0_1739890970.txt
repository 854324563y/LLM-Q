[2025-02-18 15:02:50 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation/llama-7b-hf-w4a4', save_dir='./log-calibration-compensation/quant/llama-7b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-18 15:06:24 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 15:06:24 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-18 15:06:24 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 15:06:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 15:06:31 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:07:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.07078417390584946 norm:0.04314446821808815 max memory_allocated 22557.81005859375 
[2025-02-18 15:07:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.051707517355680466 norm:0.0337088517844677 max memory_allocated 22557.81005859375 
[2025-02-18 15:08:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.04621873050928116 norm:0.02491784654557705 max memory_allocated 22557.81005859375 
[2025-02-18 15:08:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.04473208636045456 norm:0.02391374111175537 max memory_allocated 22557.81005859375 
[2025-02-18 15:09:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.042996592819690704 norm:0.020956655964255333 max memory_allocated 22557.81005859375 
[2025-02-18 15:09:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0407896563410759 norm:0.014884665608406067 max memory_allocated 22557.81005859375 
[2025-02-18 15:10:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0404210090637207 norm:0.014799226075410843 max memory_allocated 22557.81005859375 
[2025-02-18 15:10:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0399787575006485 norm:0.012569628655910492 max memory_allocated 22557.81005859375 
[2025-02-18 15:11:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.038779761642217636 norm:0.009929012507200241 max memory_allocated 22557.81005859375 
[2025-02-18 15:12:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.03809236362576485 norm:0.008381800726056099 max memory_allocated 22557.81005859375 
[2025-02-18 15:12:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.03765557333827019 norm:0.007709280587732792 max memory_allocated 22557.81005859375 
[2025-02-18 15:13:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.03748292475938797 norm:0.00713489530608058 max memory_allocated 22557.81005859375 
[2025-02-18 15:13:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.03731369972229004 norm:0.0065872445702552795 max memory_allocated 22557.81005859375 
[2025-02-18 15:14:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.037287648767232895 norm:0.006427707150578499 max memory_allocated 22557.81005859375 
[2025-02-18 15:14:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.03744031488895416 norm:0.006471547298133373 max memory_allocated 22557.81005859375 
[2025-02-18 15:15:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.037248048931360245 norm:0.006056969054043293 max memory_allocated 22557.81005859375 
[2025-02-18 15:15:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.037304751574993134 norm:0.006108476780354977 max memory_allocated 22557.81005859375 
[2025-02-18 15:16:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.037265729159116745 norm:0.005860893987119198 max memory_allocated 22557.81005859375 
[2025-02-18 15:17:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.03689972311258316 norm:0.005366489291191101 max memory_allocated 22557.81005859375 
[2025-02-18 15:17:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.03685842081904411 norm:0.005305311642587185 max memory_allocated 22557.81005859375 
[2025-02-18 15:17:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 15:17:49 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:18:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.24877242743968964 norm:0.13341961801052094 max memory_allocated 22557.98193359375 
[2025-02-18 15:18:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.17204736173152924 norm:0.06729850172996521 max memory_allocated 22557.98193359375 
[2025-02-18 15:19:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.15950115025043488 norm:0.05373850092291832 max memory_allocated 22557.98193359375 
[2025-02-18 15:20:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.14699947834014893 norm:0.04013150557875633 max memory_allocated 22557.98193359375 
[2025-02-18 15:20:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.13688093423843384 norm:0.03339932858943939 max memory_allocated 22557.98193359375 
[2025-02-18 15:21:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.12354068458080292 norm:0.02240118570625782 max memory_allocated 22557.98193359375 
[2025-02-18 15:21:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.1154254674911499 norm:0.019182328134775162 max memory_allocated 22557.98193359375 
[2025-02-18 15:22:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.11132615804672241 norm:0.01814155839383602 max memory_allocated 22557.98193359375 
[2025-02-18 15:22:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.10820771753787994 norm:0.01666567660868168 max memory_allocated 22557.98193359375 
[2025-02-18 15:23:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.10623206198215485 norm:0.016318581998348236 max memory_allocated 22557.98193359375 
[2025-02-18 15:23:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.10476040095090866 norm:0.015143321827054024 max memory_allocated 22557.98193359375 
[2025-02-18 15:24:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.10425989329814911 norm:0.015150494873523712 max memory_allocated 22557.98193359375 
[2025-02-18 15:25:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.10346309840679169 norm:0.014419305138289928 max memory_allocated 22557.98193359375 
[2025-02-18 15:25:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.10315234959125519 norm:0.014497593976557255 max memory_allocated 22557.98193359375 
[2025-02-18 15:26:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.10282831639051437 norm:0.01400411780923605 max memory_allocated 22557.98193359375 
[2025-02-18 15:26:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.102922722697258 norm:0.014010309241712093 max memory_allocated 22557.98193359375 
[2025-02-18 15:27:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.10217662155628204 norm:0.013038650155067444 max memory_allocated 22557.98193359375 
[2025-02-18 15:27:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.10184434056282043 norm:0.013018883764743805 max memory_allocated 22557.98193359375 
[2025-02-18 15:28:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.10126645117998123 norm:0.01243911124765873 max memory_allocated 22557.98193359375 
[2025-02-18 15:28:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.10122053325176239 norm:0.012470001354813576 max memory_allocated 22557.98193359375 
[2025-02-18 15:29:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 15:29:10 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:29:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.40169757604599 norm:0.058858808130025864 max memory_allocated 22558.15380859375 
[2025-02-18 15:30:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.26028332114219666 norm:0.03374333307147026 max memory_allocated 22558.15380859375 
[2025-02-18 15:30:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.21103450655937195 norm:0.022189179435372353 max memory_allocated 22558.15380859375 
[2025-02-18 15:31:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.1944897174835205 norm:0.017831185832619667 max memory_allocated 22558.15380859375 
[2025-02-18 15:31:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.18418966233730316 norm:0.015978332608938217 max memory_allocated 22558.15380859375 
[2025-02-18 15:32:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.18020431697368622 norm:0.015122887678444386 max memory_allocated 22558.15380859375 
[2025-02-18 15:33:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.17732013761997223 norm:0.014553560875356197 max memory_allocated 22558.15380859375 
[2025-02-18 15:33:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.175251305103302 norm:0.014233814552426338 max memory_allocated 22558.15380859375 
[2025-02-18 15:34:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.1731107383966446 norm:0.013550304807722569 max memory_allocated 22558.15380859375 
[2025-02-18 15:34:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.17309297621250153 norm:0.013026973232626915 max memory_allocated 22558.15380859375 
[2025-02-18 15:35:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.1715565323829651 norm:0.012020611204206944 max memory_allocated 22558.15380859375 
[2025-02-18 15:35:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.17100830376148224 norm:0.011679480783641338 max memory_allocated 22558.15380859375 
[2025-02-18 15:36:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.17033444344997406 norm:0.011341712437570095 max memory_allocated 22558.15380859375 
[2025-02-18 15:36:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.1696544885635376 norm:0.010721242986619473 max memory_allocated 22558.15380859375 
[2025-02-18 15:37:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.16962136328220367 norm:0.010245065204799175 max memory_allocated 22558.15380859375 
[2025-02-18 15:38:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.16837072372436523 norm:0.0100363465026021 max memory_allocated 22558.15380859375 
[2025-02-18 15:38:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.17056408524513245 norm:0.0201761145144701 max memory_allocated 22558.15380859375 
[2025-02-18 15:39:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.17168833315372467 norm:0.01668017916381359 max memory_allocated 22558.15380859375 
[2025-02-18 15:39:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.1706697940826416 norm:0.015254434198141098 max memory_allocated 22558.15380859375 
[2025-02-18 15:40:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.16913357377052307 norm:0.014601333066821098 max memory_allocated 22558.15380859375 
[2025-02-18 15:40:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 15:41:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.2644518315792084 norm:0.04141107201576233 max memory_allocated 22558.21044921875 
[2025-02-18 15:41:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.23622405529022217 norm:0.014100204221904278 max memory_allocated 22558.21044921875 
[2025-02-18 15:42:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.22838042676448822 norm:0.0075624315068125725 max memory_allocated 22558.21044921875 
[2025-02-18 15:42:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.2258145809173584 norm:0.005536997225135565 max memory_allocated 22558.21044921875 
[2025-02-18 15:43:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.22138406336307526 norm:0.0040886360220611095 max memory_allocated 22558.21044921875 
[2025-02-18 15:43:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.2182486653327942 norm:0.0035456225741654634 max memory_allocated 22558.21044921875 
[2025-02-18 15:44:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.2158377766609192 norm:0.0030662845820188522 max memory_allocated 22558.21044921875 
[2025-02-18 15:44:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.21581199765205383 norm:0.002758290618658066 max memory_allocated 22558.21044921875 
[2025-02-18 15:45:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.21590039134025574 norm:0.002665949519723654 max memory_allocated 22558.21044921875 
[2025-02-18 15:46:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.2136068344116211 norm:0.002405573846772313 max memory_allocated 22558.21044921875 
[2025-02-18 15:46:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.21198421716690063 norm:0.002242890652269125 max memory_allocated 22558.21044921875 
[2025-02-18 15:47:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.21161602437496185 norm:0.002236227970570326 max memory_allocated 22558.21044921875 
[2025-02-18 15:47:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.21198071539402008 norm:0.0022146976552903652 max memory_allocated 22558.21044921875 
[2025-02-18 15:48:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.21169447898864746 norm:0.0021653601434081793 max memory_allocated 22558.21044921875 
[2025-02-18 15:48:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.21149152517318726 norm:0.0021164738573133945 max memory_allocated 22558.21044921875 
[2025-02-18 15:49:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.21099179983139038 norm:0.0020524899009615183 max memory_allocated 22558.21044921875 
[2025-02-18 15:49:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.21089237928390503 norm:0.002033681608736515 max memory_allocated 22558.21044921875 
[2025-02-18 15:50:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.210643470287323 norm:0.002016467275097966 max memory_allocated 22558.21044921875 
[2025-02-18 15:51:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.21067778766155243 norm:0.0020070115569978952 max memory_allocated 22558.21044921875 
[2025-02-18 15:51:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.21053695678710938 norm:0.0020060716196894646 max memory_allocated 22558.21044921875 
[2025-02-18 15:51:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 15:52:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.3399246037006378 norm:0.046864792704582214 max memory_allocated 22558.38232421875 
[2025-02-18 15:52:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.31833720207214355 norm:0.0242110937833786 max memory_allocated 22558.38232421875 
[2025-02-18 15:53:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.30979275703430176 norm:0.0136195607483387 max memory_allocated 22558.38232421875 
[2025-02-18 15:54:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.30566734075546265 norm:0.008472535759210587 max memory_allocated 22558.38232421875 
[2025-02-18 15:54:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.3000619411468506 norm:0.005801159422844648 max memory_allocated 22558.38232421875 
[2025-02-18 15:55:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.29513031244277954 norm:0.004534593317657709 max memory_allocated 22558.38232421875 
[2025-02-18 15:55:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.2925153076648712 norm:0.00375326513312757 max memory_allocated 22558.38232421875 
[2025-02-18 15:56:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.29148513078689575 norm:0.0033418580424040556 max memory_allocated 22558.38232421875 
[2025-02-18 15:56:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.2882006764411926 norm:0.003028418170288205 max memory_allocated 22558.38232421875 
[2025-02-18 15:57:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.286758154630661 norm:0.0027276119217276573 max memory_allocated 22558.38232421875 
[2025-02-18 15:57:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.2855788469314575 norm:0.0025225519202649593 max memory_allocated 22558.38232421875 
[2025-02-18 15:58:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.2848373353481293 norm:0.0023600368294864893 max memory_allocated 22558.38232421875 
[2025-02-18 15:59:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.2842462360858917 norm:0.002308695809915662 max memory_allocated 22558.38232421875 
[2025-02-18 15:59:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.2831158936023712 norm:0.00223548524081707 max memory_allocated 22558.38232421875 
[2025-02-18 16:00:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.2823367714881897 norm:0.002181414980441332 max memory_allocated 22558.38232421875 
[2025-02-18 16:00:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.28188690543174744 norm:0.0021579056046903133 max memory_allocated 22558.38232421875 
[2025-02-18 16:01:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.28115418553352356 norm:0.0021139755845069885 max memory_allocated 22558.38232421875 
[2025-02-18 16:01:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.28071239590644836 norm:0.002097304444760084 max memory_allocated 22558.38232421875 
[2025-02-18 16:02:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.28030234575271606 norm:0.0020724886562675238 max memory_allocated 22558.38232421875 
[2025-02-18 16:03:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.27955323457717896 norm:0.0020489480812102556 max memory_allocated 22558.38232421875 
[2025-02-18 16:03:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 16:03:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.40403828024864197 norm:0.05939851328730583 max memory_allocated 22558.55419921875 
[2025-02-18 16:04:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.3754417300224304 norm:0.026133932173252106 max memory_allocated 22558.55419921875 
[2025-02-18 16:04:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.3647272288799286 norm:0.014038692228496075 max memory_allocated 22558.55419921875 
[2025-02-18 16:05:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.3595624566078186 norm:0.008358015678822994 max memory_allocated 22558.55419921875 
[2025-02-18 16:05:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.35592034459114075 norm:0.006085246801376343 max memory_allocated 22558.55419921875 
[2025-02-18 16:06:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.357083797454834 norm:0.005105442833155394 max memory_allocated 22558.55419921875 
[2025-02-18 16:07:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.3547765612602234 norm:0.004294862970709801 max memory_allocated 22558.55419921875 
[2025-02-18 16:07:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.35270002484321594 norm:0.0038749256636947393 max memory_allocated 22558.55419921875 
[2025-02-18 16:08:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.34974533319473267 norm:0.003533235052600503 max memory_allocated 22558.55419921875 
[2025-02-18 16:08:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.3472849726676941 norm:0.003268276108428836 max memory_allocated 22558.55419921875 
[2025-02-18 16:09:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.34549832344055176 norm:0.0030391886830329895 max memory_allocated 22558.55419921875 
[2025-02-18 16:09:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.3454587757587433 norm:0.0030628361273556948 max memory_allocated 22558.55419921875 
[2025-02-18 16:10:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.3457328677177429 norm:0.0030500951688736677 max memory_allocated 22558.55419921875 
[2025-02-18 16:11:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.34542739391326904 norm:0.003003050573170185 max memory_allocated 22558.55419921875 
[2025-02-18 16:11:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.3453407883644104 norm:0.003016931237652898 max memory_allocated 22558.55419921875 
[2025-02-18 16:12:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.34586188197135925 norm:0.0030005478765815496 max memory_allocated 22558.55419921875 
[2025-02-18 16:12:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.34552621841430664 norm:0.0029907680582255125 max memory_allocated 22558.55419921875 
[2025-02-18 16:13:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.3453424572944641 norm:0.00294552487321198 max memory_allocated 22558.55419921875 
[2025-02-18 16:13:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.34453248977661133 norm:0.0029149625916033983 max memory_allocated 22558.55419921875 
[2025-02-18 16:14:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.34395453333854675 norm:0.0028792833909392357 max memory_allocated 22558.55419921875 
[2025-02-18 16:14:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 16:15:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.44518038630485535 norm:0.050526734441518784 max memory_allocated 22558.72607421875 
[2025-02-18 16:15:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.42486050724983215 norm:0.024875536561012268 max memory_allocated 22558.72607421875 
[2025-02-18 16:16:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.4149482250213623 norm:0.012912113219499588 max memory_allocated 22558.72607421875 
[2025-02-18 16:16:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.41030213236808777 norm:0.007238614372909069 max memory_allocated 22558.72607421875 
[2025-02-18 16:17:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.40324896574020386 norm:0.005019439850002527 max memory_allocated 22558.72607421875 
[2025-02-18 16:17:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.39591240882873535 norm:0.003479401348158717 max memory_allocated 22558.72607421875 
[2025-02-18 16:18:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.39506301283836365 norm:0.0029793193098157644 max memory_allocated 22558.72607421875 
[2025-02-18 16:19:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.39639100432395935 norm:0.002750639570876956 max memory_allocated 22558.72607421875 
[2025-02-18 16:19:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.39720696210861206 norm:0.0025349915958940983 max memory_allocated 22558.72607421875 
[2025-02-18 16:20:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.39671623706817627 norm:0.0024740712251514196 max memory_allocated 22558.72607421875 
[2025-02-18 16:20:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.3966082036495209 norm:0.00246130651794374 max memory_allocated 22558.72607421875 
[2025-02-18 16:21:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.3967530429363251 norm:0.002409935463219881 max memory_allocated 22558.72607421875 
[2025-02-18 16:21:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.3962118923664093 norm:0.0023860579822212458 max memory_allocated 22558.72607421875 
[2025-02-18 16:22:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.3959418535232544 norm:0.0023659910075366497 max memory_allocated 22558.72607421875 
[2025-02-18 16:22:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.3950836956501007 norm:0.002385282889008522 max memory_allocated 22558.72607421875 
[2025-02-18 16:23:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.3947480320930481 norm:0.002354631433263421 max memory_allocated 22558.72607421875 
[2025-02-18 16:24:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.39490318298339844 norm:0.0023110120091587305 max memory_allocated 22558.72607421875 
[2025-02-18 16:24:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.3946840465068817 norm:0.0023266891948878765 max memory_allocated 22558.72607421875 
[2025-02-18 16:25:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.39484304189682007 norm:0.0023397207260131836 max memory_allocated 22558.72607421875 
[2025-02-18 16:25:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.39439424872398376 norm:0.0023212693631649017 max memory_allocated 22558.72607421875 
[2025-02-18 16:25:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 16:26:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.5109815001487732 norm:0.042397163808345795 max memory_allocated 22558.89794921875 
[2025-02-18 16:27:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.48472726345062256 norm:0.020488211885094643 max memory_allocated 22558.89794921875 
[2025-02-18 16:27:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.47838205099105835 norm:0.010835064575076103 max memory_allocated 22558.89794921875 
[2025-02-18 16:28:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.47216126322746277 norm:0.006293792277574539 max memory_allocated 22558.89794921875 
[2025-02-18 16:28:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.46395736932754517 norm:0.004216593690216541 max memory_allocated 22558.89794921875 
[2025-02-18 16:29:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.4599466919898987 norm:0.003548605367541313 max memory_allocated 22558.89794921875 
[2025-02-18 16:29:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.45795243978500366 norm:0.0031422264873981476 max memory_allocated 22558.89794921875 
[2025-02-18 16:30:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.4595757722854614 norm:0.002981217112392187 max memory_allocated 22558.89794921875 
[2025-02-18 16:30:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.4604473114013672 norm:0.0029021198861300945 max memory_allocated 22558.89794921875 
[2025-02-18 16:31:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.4593402147293091 norm:0.0028266783338040113 max memory_allocated 22558.89794921875 
[2025-02-18 16:32:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.4590190351009369 norm:0.002816843567416072 max memory_allocated 22558.89794921875 
[2025-02-18 16:32:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.4593467712402344 norm:0.002826607320457697 max memory_allocated 22558.89794921875 
[2025-02-18 16:33:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.4592398405075073 norm:0.0028467318043112755 max memory_allocated 22558.89794921875 
[2025-02-18 16:33:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.45844706892967224 norm:0.0028041067998856306 max memory_allocated 22558.89794921875 
[2025-02-18 16:34:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.4578797221183777 norm:0.0027885334566235542 max memory_allocated 22558.89794921875 
[2025-02-18 16:34:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.4565201699733734 norm:0.00273387529887259 max memory_allocated 22558.89794921875 
[2025-02-18 16:35:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.4558769762516022 norm:0.002731869462877512 max memory_allocated 22558.89794921875 
[2025-02-18 16:35:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.45536914467811584 norm:0.0026942116674035788 max memory_allocated 22558.89794921875 
[2025-02-18 16:36:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.4554091989994049 norm:0.0027224859222769737 max memory_allocated 22558.89794921875 
[2025-02-18 16:37:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.4552045464515686 norm:0.002727821236476302 max memory_allocated 22558.89794921875 
[2025-02-18 16:37:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 16:37:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.5494065284729004 norm:0.028563641011714935 max memory_allocated 22559.06982421875 
[2025-02-18 16:38:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.5314042568206787 norm:0.01530859898775816 max memory_allocated 22559.06982421875 
[2025-02-18 16:38:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.5254848003387451 norm:0.009635735303163528 max memory_allocated 22559.06982421875 
[2025-02-18 16:39:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.5152915120124817 norm:0.0056790560483932495 max memory_allocated 22559.06982421875 
[2025-02-18 16:40:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.5111661553382874 norm:0.003923903219401836 max memory_allocated 22559.06982421875 
[2025-02-18 16:40:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.508579432964325 norm:0.0035580878611654043 max memory_allocated 22559.06982421875 
[2025-02-18 16:41:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.5053766369819641 norm:0.0032442694064229727 max memory_allocated 22559.06982421875 
[2025-02-18 16:41:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.498663067817688 norm:0.002885422669351101 max memory_allocated 22559.06982421875 
[2025-02-18 16:42:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.49911704659461975 norm:0.00298054376617074 max memory_allocated 22559.06982421875 
[2025-02-18 16:42:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.49778831005096436 norm:0.0027501985896378756 max memory_allocated 22559.06982421875 
[2025-02-18 16:43:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.49739712476730347 norm:0.0027311912272125483 max memory_allocated 22559.06982421875 
[2025-02-18 16:43:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.49740147590637207 norm:0.002724233316257596 max memory_allocated 22559.06982421875 
[2025-02-18 16:44:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.49573028087615967 norm:0.0025924579240381718 max memory_allocated 22559.06982421875 
[2025-02-18 16:45:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.4955039918422699 norm:0.002583963330835104 max memory_allocated 22559.06982421875 
[2025-02-18 16:45:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.49482980370521545 norm:0.002570002106949687 max memory_allocated 22559.06982421875 
[2025-02-18 16:46:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.4937368631362915 norm:0.0025337834376841784 max memory_allocated 22559.06982421875 
[2025-02-18 16:46:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.49367010593414307 norm:0.0025421001482754946 max memory_allocated 22559.06982421875 
[2025-02-18 16:47:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.4936125576496124 norm:0.00253316736780107 max memory_allocated 22559.06982421875 
[2025-02-18 16:47:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.4930170774459839 norm:0.002521541900932789 max memory_allocated 22559.06982421875 
[2025-02-18 16:48:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.4929125905036926 norm:0.002510514110326767 max memory_allocated 22559.06982421875 
[2025-02-18 16:48:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 16:49:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.6133507490158081 norm:0.03650365024805069 max memory_allocated 22559.24169921875 
[2025-02-18 16:49:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.5980262160301208 norm:0.02335018292069435 max memory_allocated 22559.24169921875 
[2025-02-18 16:50:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.5873075723648071 norm:0.013151057995855808 max memory_allocated 22559.24169921875 
[2025-02-18 16:50:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.5829582810401917 norm:0.008842436596751213 max memory_allocated 22559.24169921875 
[2025-02-18 16:51:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.5781565308570862 norm:0.006373829208314419 max memory_allocated 22559.24169921875 
[2025-02-18 16:52:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.574334979057312 norm:0.005131594371050596 max memory_allocated 22559.24169921875 
[2025-02-18 16:52:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.567420482635498 norm:0.0038543068803846836 max memory_allocated 22559.24169921875 
[2025-02-18 16:53:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.5647561550140381 norm:0.003391618374735117 max memory_allocated 22559.24169921875 
[2025-02-18 16:53:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.5601954460144043 norm:0.003025126876309514 max memory_allocated 22559.24169921875 
[2025-02-18 16:54:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.5581080913543701 norm:0.0029413108713924885 max memory_allocated 22559.24169921875 
[2025-02-18 16:54:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.5554934740066528 norm:0.0028353161178529263 max memory_allocated 22559.24169921875 
[2025-02-18 16:55:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.5542530417442322 norm:0.002756999572739005 max memory_allocated 22559.24169921875 
[2025-02-18 16:55:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.554974377155304 norm:0.0027885925956070423 max memory_allocated 22559.24169921875 
[2025-02-18 16:56:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.5531574487686157 norm:0.002687669126316905 max memory_allocated 22559.24169921875 
[2025-02-18 16:57:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.5531887412071228 norm:0.002709490479901433 max memory_allocated 22559.24169921875 
[2025-02-18 16:57:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.5550505518913269 norm:0.0027670576237142086 max memory_allocated 22559.24169921875 
[2025-02-18 16:58:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.5546970367431641 norm:0.00273841037414968 max memory_allocated 22559.24169921875 
[2025-02-18 16:58:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.5538867115974426 norm:0.0027177983429282904 max memory_allocated 22559.24169921875 
[2025-02-18 16:59:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.5528639554977417 norm:0.0026879971846938133 max memory_allocated 22559.24169921875 
[2025-02-18 16:59:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.5521265864372253 norm:0.002643657848238945 max memory_allocated 22559.24169921875 
[2025-02-18 16:59:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 17:00:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.6165072321891785 norm:0.022919543087482452 max memory_allocated 22559.41357421875 
[2025-02-18 17:01:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.6025465726852417 norm:0.014481441117823124 max memory_allocated 22559.41357421875 
[2025-02-18 17:01:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.5955773591995239 norm:0.008646096102893353 max memory_allocated 22559.41357421875 
[2025-02-18 17:02:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.5931971073150635 norm:0.005786668509244919 max memory_allocated 22559.41357421875 
[2025-02-18 17:02:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.587174117565155 norm:0.004128510132431984 max memory_allocated 22559.41357421875 
[2025-02-18 17:03:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.5820112824440002 norm:0.0029021049849689007 max memory_allocated 22559.41357421875 
[2025-02-18 17:03:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.5791811347007751 norm:0.002368322340771556 max memory_allocated 22559.41357421875 
[2025-02-18 17:04:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.5777497291564941 norm:0.0020574070513248444 max memory_allocated 22559.41357421875 
[2025-02-18 17:05:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.5760338306427002 norm:0.0018340360838919878 max memory_allocated 22559.41357421875 
[2025-02-18 17:05:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.5749111175537109 norm:0.001709734438918531 max memory_allocated 22559.41357421875 
[2025-02-18 17:06:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.5734356045722961 norm:0.001626312150619924 max memory_allocated 22559.41357421875 
[2025-02-18 17:06:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.5721678137779236 norm:0.0015414234949275851 max memory_allocated 22559.41357421875 
[2025-02-18 17:07:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.5712522268295288 norm:0.001506879343651235 max memory_allocated 22559.41357421875 
[2025-02-18 17:07:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.5709213018417358 norm:0.0014929340686649084 max memory_allocated 22559.41357421875 
[2025-02-18 17:08:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.5699210166931152 norm:0.001450252952054143 max memory_allocated 22559.41357421875 
[2025-02-18 17:08:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.5694393515586853 norm:0.001428226474672556 max memory_allocated 22559.41357421875 
[2025-02-18 17:09:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.5699908137321472 norm:0.0014364292146638036 max memory_allocated 22559.41357421875 
[2025-02-18 17:10:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.5692914724349976 norm:0.00141627318225801 max memory_allocated 22559.41357421875 
[2025-02-18 17:10:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.5690950751304626 norm:0.0014011652674525976 max memory_allocated 22559.41357421875 
[2025-02-18 17:11:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.5694795250892639 norm:0.0014104126021265984 max memory_allocated 22559.41357421875 
[2025-02-18 17:11:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 17:11:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.641788899898529 norm:0.02717835269868374 max memory_allocated 22559.58544921875 
[2025-02-18 17:12:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.6256384253501892 norm:0.016807911917567253 max memory_allocated 22559.58544921875 
[2025-02-18 17:13:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.6161795258522034 norm:0.010058953426778316 max memory_allocated 22559.58544921875 
[2025-02-18 17:13:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.6135217547416687 norm:0.006923263892531395 max memory_allocated 22559.58544921875 
[2025-02-18 17:14:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.610784649848938 norm:0.004969309084117413 max memory_allocated 22559.58544921875 
[2025-02-18 17:14:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.6068682074546814 norm:0.003669578582048416 max memory_allocated 22559.58544921875 
[2025-02-18 17:15:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.6039403080940247 norm:0.0030032042413949966 max memory_allocated 22559.58544921875 
[2025-02-18 17:15:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.6027154326438904 norm:0.002707271371036768 max memory_allocated 22559.58544921875 
[2025-02-18 17:16:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.6003639698028564 norm:0.0021938749123364687 max memory_allocated 22559.58544921875 
[2025-02-18 17:17:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.597880482673645 norm:0.001920199254527688 max memory_allocated 22559.58544921875 
[2025-02-18 17:17:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.5951240062713623 norm:0.0017975999508053064 max memory_allocated 22559.58544921875 
[2025-02-18 17:18:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.5938485860824585 norm:0.0017224123002961278 max memory_allocated 22559.58544921875 
[2025-02-18 17:18:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.5927760004997253 norm:0.001655701082199812 max memory_allocated 22559.58544921875 
[2025-02-18 17:19:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.5918025970458984 norm:0.0016118722269311547 max memory_allocated 22559.58544921875 
[2025-02-18 17:19:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.5914727449417114 norm:0.001590727362781763 max memory_allocated 22559.58544921875 
[2025-02-18 17:20:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.5916863083839417 norm:0.001599220442585647 max memory_allocated 22559.58544921875 
[2025-02-18 17:20:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.5914121866226196 norm:0.0015747763682156801 max memory_allocated 22559.58544921875 
[2025-02-18 17:21:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.5914178490638733 norm:0.0015630092239007354 max memory_allocated 22559.58544921875 
[2025-02-18 17:22:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.591760516166687 norm:0.0015662070363759995 max memory_allocated 22559.58544921875 
[2025-02-18 17:22:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.5922669768333435 norm:0.0015889176866039634 max memory_allocated 22559.58544921875 
[2025-02-18 17:22:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 17:23:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.6605912446975708 norm:0.0292974840849638 max memory_allocated 22559.75732421875 
[2025-02-18 17:23:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.6508455276489258 norm:0.017581792548298836 max memory_allocated 22559.75732421875 
[2025-02-18 17:24:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.6446380019187927 norm:0.01164520438760519 max memory_allocated 22559.75732421875 
[2025-02-18 17:25:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.6378358602523804 norm:0.007481294684112072 max memory_allocated 22559.75732421875 
[2025-02-18 17:25:34 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.6331177949905396 norm:0.005399815738201141 max memory_allocated 22559.75732421875 
[2025-02-18 17:26:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.6313184499740601 norm:0.004360233899205923 max memory_allocated 22559.75732421875 
[2025-02-18 17:26:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.6263791918754578 norm:0.003485726658254862 max memory_allocated 22559.75732421875 
[2025-02-18 17:27:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.6233152747154236 norm:0.0028674376662820578 max memory_allocated 22559.75732421875 
[2025-02-18 17:27:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.6217318177223206 norm:0.0025336770340800285 max memory_allocated 22559.75732421875 
[2025-02-18 17:28:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.6220993995666504 norm:0.0024079028517007828 max memory_allocated 22559.75732421875 
[2025-02-18 17:28:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.6218860149383545 norm:0.0022871822584420443 max memory_allocated 22559.75732421875 
[2025-02-18 17:29:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.6213605999946594 norm:0.0021584772039204836 max memory_allocated 22559.75732421875 
[2025-02-18 17:30:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.6200793981552124 norm:0.002047642134130001 max memory_allocated 22559.75732421875 
[2025-02-18 17:30:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.6199246048927307 norm:0.0020424113608896732 max memory_allocated 22559.75732421875 
[2025-02-18 17:31:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.6208722591400146 norm:0.0020199897699058056 max memory_allocated 22559.75732421875 
[2025-02-18 17:31:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.6191484928131104 norm:0.0019468538230285048 max memory_allocated 22559.75732421875 
[2025-02-18 17:32:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.6186809539794922 norm:0.0019093318842351437 max memory_allocated 22559.75732421875 
[2025-02-18 17:32:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.6174144744873047 norm:0.0018641868373379111 max memory_allocated 22559.75732421875 
[2025-02-18 17:33:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.617084264755249 norm:0.0018515749834477901 max memory_allocated 22559.75732421875 
[2025-02-18 17:33:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.6171259880065918 norm:0.0018294748151674867 max memory_allocated 22559.75732421875 
[2025-02-18 17:34:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 17:34:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.6850131154060364 norm:0.02033587545156479 max memory_allocated 22559.92919921875 
[2025-02-18 17:35:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.67268306016922 norm:0.014341185800731182 max memory_allocated 22559.92919921875 
[2025-02-18 17:35:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.6626304388046265 norm:0.009436213411390781 max memory_allocated 22559.92919921875 
[2025-02-18 17:36:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.6522580981254578 norm:0.005560099612921476 max memory_allocated 22559.92919921875 
[2025-02-18 17:36:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.650231659412384 norm:0.004128025379031897 max memory_allocated 22559.92919921875 
[2025-02-18 17:37:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.6473568081855774 norm:0.0032691508531570435 max memory_allocated 22559.92919921875 
[2025-02-18 17:38:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.6449810266494751 norm:0.0027319847140461206 max memory_allocated 22559.92919921875 
[2025-02-18 17:38:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.6443880796432495 norm:0.0024799855891615152 max memory_allocated 22559.92919921875 
[2025-02-18 17:39:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.6440706849098206 norm:0.002314461162313819 max memory_allocated 22559.92919921875 
[2025-02-18 17:39:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.6440799832344055 norm:0.0022876013536006212 max memory_allocated 22559.92919921875 
[2025-02-18 17:40:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.6458741426467896 norm:0.0023474721238017082 max memory_allocated 22559.92919921875 
[2025-02-18 17:40:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.646456241607666 norm:0.002375864889472723 max memory_allocated 22559.92919921875 
[2025-02-18 17:41:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.6482428908348083 norm:0.0024926732294261456 max memory_allocated 22559.92919921875 
[2025-02-18 17:41:58 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.6477847099304199 norm:0.0024612995330244303 max memory_allocated 22559.92919921875 
[2025-02-18 17:42:32 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.648789644241333 norm:0.002507443306967616 max memory_allocated 22559.92919921875 
[2025-02-18 17:43:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.6477327942848206 norm:0.0024595854338258505 max memory_allocated 22559.92919921875 
[2025-02-18 17:43:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.6475096940994263 norm:0.0024959410075098276 max memory_allocated 22559.92919921875 
[2025-02-18 17:44:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.6459363102912903 norm:0.0024741850793361664 max memory_allocated 22559.92919921875 
[2025-02-18 17:44:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.6458020806312561 norm:0.0025161507073789835 max memory_allocated 22559.92919921875 
[2025-02-18 17:45:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.6447683572769165 norm:0.0024705063551664352 max memory_allocated 22559.92919921875 
[2025-02-18 17:45:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 17:46:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.7525966763496399 norm:0.04530264437198639 max memory_allocated 22560.10107421875 
[2025-02-18 17:46:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.7369809150695801 norm:0.028094913810491562 max memory_allocated 22560.10107421875 
[2025-02-18 17:47:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.7249874472618103 norm:0.016859058290719986 max memory_allocated 22560.10107421875 
[2025-02-18 17:47:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.7203642725944519 norm:0.011686528101563454 max memory_allocated 22560.10107421875 
[2025-02-18 17:48:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.7179626822471619 norm:0.008796501904726028 max memory_allocated 22560.10107421875 
[2025-02-18 17:48:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.7137065529823303 norm:0.006878603715449572 max memory_allocated 22560.10107421875 
[2025-02-18 17:49:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.710949182510376 norm:0.0057042739354074 max memory_allocated 22560.10107421875 
[2025-02-18 17:49:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.7088538408279419 norm:0.0048487805761396885 max memory_allocated 22560.10107421875 
[2025-02-18 17:50:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.7041718363761902 norm:0.003943039570003748 max memory_allocated 22560.10107421875 
[2025-02-18 17:51:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.7006596922874451 norm:0.0034551804419606924 max memory_allocated 22560.10107421875 
[2025-02-18 17:51:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.7002540826797485 norm:0.00317580858245492 max memory_allocated 22560.10107421875 
[2025-02-18 17:52:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.6993475556373596 norm:0.0029957653023302555 max memory_allocated 22560.10107421875 
[2025-02-18 17:52:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.6988614797592163 norm:0.0028421899769455194 max memory_allocated 22560.10107421875 
[2025-02-18 17:53:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.6987906694412231 norm:0.002729695290327072 max memory_allocated 22560.10107421875 
[2025-02-18 17:53:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.6979753971099854 norm:0.002645916538313031 max memory_allocated 22560.10107421875 
[2025-02-18 17:54:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.6974340677261353 norm:0.0025888709351420403 max memory_allocated 22560.10107421875 
[2025-02-18 17:55:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.6972116827964783 norm:0.0025389627553522587 max memory_allocated 22560.10107421875 
[2025-02-18 17:55:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.6975728273391724 norm:0.002509242855012417 max memory_allocated 22560.10107421875 
[2025-02-18 17:56:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.6970469355583191 norm:0.0024486402980983257 max memory_allocated 22560.10107421875 
[2025-02-18 17:56:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.6959165334701538 norm:0.002372428774833679 max memory_allocated 22560.10107421875 
[2025-02-18 17:56:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 17:57:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.7690982818603516 norm:0.03507310152053833 max memory_allocated 22560.27294921875 
[2025-02-18 17:58:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.7537676692008972 norm:0.01900136098265648 max memory_allocated 22560.27294921875 
[2025-02-18 17:58:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.7467809915542603 norm:0.012409082613885403 max memory_allocated 22560.27294921875 
[2025-02-18 17:59:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.7423595190048218 norm:0.008087420836091042 max memory_allocated 22560.27294921875 
[2025-02-18 17:59:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.7402825951576233 norm:0.0063900938257575035 max memory_allocated 22560.27294921875 
[2025-02-18 18:00:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.739885151386261 norm:0.005425183102488518 max memory_allocated 22560.27294921875 
[2025-02-18 18:00:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.7376596331596375 norm:0.004201173782348633 max memory_allocated 22560.27294921875 
[2025-02-18 18:01:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.7357790470123291 norm:0.003340308554470539 max memory_allocated 22560.27294921875 
[2025-02-18 18:01:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.7344186305999756 norm:0.0030575438868254423 max memory_allocated 22560.27294921875 
[2025-02-18 18:02:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.7318832874298096 norm:0.0026783470530062914 max memory_allocated 22560.27294921875 
[2025-02-18 18:03:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.7301485538482666 norm:0.0023890011943876743 max memory_allocated 22560.27294921875 
[2025-02-18 18:03:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.728794276714325 norm:0.002164586214348674 max memory_allocated 22560.27294921875 
[2025-02-18 18:04:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.7277455925941467 norm:0.0020544612780213356 max memory_allocated 22560.27294921875 
[2025-02-18 18:04:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.7260950803756714 norm:0.0018872846849262714 max memory_allocated 22560.27294921875 
[2025-02-18 18:05:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.7250766158103943 norm:0.0018214554293081164 max memory_allocated 22560.27294921875 
[2025-02-18 18:05:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.7244131565093994 norm:0.0017666248604655266 max memory_allocated 22560.27294921875 
[2025-02-18 18:06:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.7233084440231323 norm:0.001681947847828269 max memory_allocated 22560.27294921875 
[2025-02-18 18:06:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.7239262461662292 norm:0.0016676299273967743 max memory_allocated 22560.27294921875 
[2025-02-18 18:07:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.7237954139709473 norm:0.0016376827843487263 max memory_allocated 22560.27294921875 
[2025-02-18 18:08:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.7235490679740906 norm:0.0016364436596632004 max memory_allocated 22560.27294921875 
[2025-02-18 18:08:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 18:08:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.8452600836753845 norm:0.03586415946483612 max memory_allocated 22560.44482421875 
[2025-02-18 18:09:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.8347792029380798 norm:0.02419489622116089 max memory_allocated 22560.44482421875 
[2025-02-18 18:09:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.8258948922157288 norm:0.014993030577898026 max memory_allocated 22560.44482421875 
[2025-02-18 18:10:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.8186169862747192 norm:0.009166586212813854 max memory_allocated 22560.44482421875 
[2025-02-18 18:11:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.817936897277832 norm:0.006477019749581814 max memory_allocated 22560.44482421875 
[2025-02-18 18:11:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.818495512008667 norm:0.005482093896716833 max memory_allocated 22560.44482421875 
[2025-02-18 18:12:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.8188497424125671 norm:0.004890430718660355 max memory_allocated 22560.44482421875 
[2025-02-18 18:12:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.8163817524909973 norm:0.0042487033642828465 max memory_allocated 22560.44482421875 
[2025-02-18 18:13:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.8149622678756714 norm:0.003923866432160139 max memory_allocated 22560.44482421875 
[2025-02-18 18:13:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.8155046105384827 norm:0.0037109432741999626 max memory_allocated 22560.44482421875 
[2025-02-18 18:14:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.8154951333999634 norm:0.0035658790729939938 max memory_allocated 22560.44482421875 
[2025-02-18 18:14:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.8134937882423401 norm:0.00334173277951777 max memory_allocated 22560.44482421875 
[2025-02-18 18:15:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.8127413392066956 norm:0.0032381736673414707 max memory_allocated 22560.44482421875 
[2025-02-18 18:16:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.8126831650733948 norm:0.003209850750863552 max memory_allocated 22560.44482421875 
[2025-02-18 18:16:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.8118262887001038 norm:0.0031376504339277744 max memory_allocated 22560.44482421875 
[2025-02-18 18:17:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.8108319640159607 norm:0.0030829536262899637 max memory_allocated 22560.44482421875 
[2025-02-18 18:17:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.8100035190582275 norm:0.0030561278108507395 max memory_allocated 22560.44482421875 
[2025-02-18 18:18:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.8096233606338501 norm:0.003065867116674781 max memory_allocated 22560.44482421875 
[2025-02-18 18:18:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.8093091249465942 norm:0.00306598749011755 max memory_allocated 22560.44482421875 
[2025-02-18 18:19:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.8100352883338928 norm:0.003096682485193014 max memory_allocated 22560.44482421875 
[2025-02-18 18:19:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 18:20:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.949907124042511 norm:0.04942785203456879 max memory_allocated 22560.61669921875 
[2025-02-18 18:20:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.9363048076629639 norm:0.03342336788773537 max memory_allocated 22560.61669921875 
[2025-02-18 18:21:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.9313297271728516 norm:0.02349936217069626 max memory_allocated 22560.61669921875 
[2025-02-18 18:21:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.9269571304321289 norm:0.016567043960094452 max memory_allocated 22560.61669921875 
[2025-02-18 18:22:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.9225788116455078 norm:0.012259155511856079 max memory_allocated 22560.61669921875 
[2025-02-18 18:22:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.9172126054763794 norm:0.00948332343250513 max memory_allocated 22560.61669921875 
[2025-02-18 18:23:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.9125467538833618 norm:0.0076389010064303875 max memory_allocated 22560.61669921875 
[2025-02-18 18:24:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.9096204042434692 norm:0.006419128738343716 max memory_allocated 22560.61669921875 
[2025-02-18 18:24:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.9086219072341919 norm:0.005700766108930111 max memory_allocated 22560.61669921875 
[2025-02-18 18:25:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.9067121148109436 norm:0.005066464655101299 max memory_allocated 22560.61669921875 
[2025-02-18 18:25:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.9044329524040222 norm:0.004607711918652058 max memory_allocated 22560.61669921875 
[2025-02-18 18:26:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.9010298252105713 norm:0.004178391769528389 max memory_allocated 22560.61669921875 
[2025-02-18 18:26:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.8983429074287415 norm:0.003880332224071026 max memory_allocated 22560.61669921875 
[2025-02-18 18:27:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.8967999219894409 norm:0.003627174301072955 max memory_allocated 22560.61669921875 
[2025-02-18 18:27:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.8954507112503052 norm:0.003380771493539214 max memory_allocated 22560.61669921875 
[2025-02-18 18:28:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.8935388922691345 norm:0.00315291341394186 max memory_allocated 22560.61669921875 
[2025-02-18 18:29:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.89223313331604 norm:0.0029742722399532795 max memory_allocated 22560.61669921875 
[2025-02-18 18:29:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.891244113445282 norm:0.0028405736666172743 max memory_allocated 22560.61669921875 
[2025-02-18 18:30:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.8917397260665894 norm:0.0027998662553727627 max memory_allocated 22560.61669921875 
[2025-02-18 18:30:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.8914610147476196 norm:0.0027152658440172672 max memory_allocated 22560.61669921875 
[2025-02-18 18:30:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 18:31:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:1.036793828010559 norm:0.03096386417746544 max memory_allocated 22560.78857421875 
[2025-02-18 18:32:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:1.0265915393829346 norm:0.020296797156333923 max memory_allocated 22560.78857421875 
[2025-02-18 18:32:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:1.0296472311019897 norm:0.015232917852699757 max memory_allocated 22560.78857421875 
[2025-02-18 18:33:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:1.0291098356246948 norm:0.009958096779882908 max memory_allocated 22560.78857421875 
[2025-02-18 18:33:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:1.0267196893692017 norm:0.007063381373882294 max memory_allocated 22560.78857421875 
[2025-02-18 18:34:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:1.0219638347625732 norm:0.005376285873353481 max memory_allocated 22560.78857421875 
[2025-02-18 18:34:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:1.016682744026184 norm:0.004329497925937176 max memory_allocated 22560.78857421875 
[2025-02-18 18:35:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:1.0146141052246094 norm:0.003734906669706106 max memory_allocated 22560.78857421875 
[2025-02-18 18:35:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:1.0139071941375732 norm:0.0034192840103060007 max memory_allocated 22560.78857421875 
[2025-02-18 18:36:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:1.0141571760177612 norm:0.003226360771805048 max memory_allocated 22560.78857421875 
[2025-02-18 18:37:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:1.013277292251587 norm:0.0030183088965713978 max memory_allocated 22560.78857421875 
[2025-02-18 18:37:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:1.011243462562561 norm:0.0027663593646138906 max memory_allocated 22560.78857421875 
[2025-02-18 18:38:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:1.010280966758728 norm:0.0026553114876151085 max memory_allocated 22560.78857421875 
[2025-02-18 18:38:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:1.0100024938583374 norm:0.002591243712231517 max memory_allocated 22560.78857421875 
[2025-02-18 18:39:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:1.009533166885376 norm:0.0025422738399356604 max memory_allocated 22560.78857421875 
[2025-02-18 18:39:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:1.0088101625442505 norm:0.002488881815224886 max memory_allocated 22560.78857421875 
[2025-02-18 18:40:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:1.009222388267517 norm:0.0025178499054163694 max memory_allocated 22560.78857421875 
[2025-02-18 18:41:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:1.0096358060836792 norm:0.002538928296416998 max memory_allocated 22560.78857421875 
[2025-02-18 18:41:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:1.009492039680481 norm:0.0025538860354572535 max memory_allocated 22560.78857421875 
[2025-02-18 18:42:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:1.0098836421966553 norm:0.0025922032073140144 max memory_allocated 22560.78857421875 
[2025-02-18 18:42:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 18:42:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:1.1798721551895142 norm:0.020717265084385872 max memory_allocated 22560.96044921875 
[2025-02-18 18:43:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:1.1706258058547974 norm:0.013520212844014168 max memory_allocated 22560.96044921875 
[2025-02-18 18:44:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:1.1691440343856812 norm:0.010299524292349815 max memory_allocated 22560.96044921875 
[2025-02-18 18:44:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:1.1683677434921265 norm:0.007223200984299183 max memory_allocated 22560.96044921875 
[2025-02-18 18:45:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:1.166323184967041 norm:0.00547007005661726 max memory_allocated 22560.96044921875 
[2025-02-18 18:45:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:1.1641836166381836 norm:0.004596967715770006 max memory_allocated 22560.96044921875 
[2025-02-18 18:46:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:1.1622830629348755 norm:0.0039715636521577835 max memory_allocated 22560.96044921875 
[2025-02-18 18:46:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:1.1612187623977661 norm:0.003530760994181037 max memory_allocated 22560.96044921875 
[2025-02-18 18:47:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:1.160593867301941 norm:0.0031526696402579546 max memory_allocated 22560.96044921875 
[2025-02-18 18:47:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:1.1603307723999023 norm:0.0029132128693163395 max memory_allocated 22560.96044921875 
[2025-02-18 18:48:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:1.1603426933288574 norm:0.002744811587035656 max memory_allocated 22560.96044921875 
[2025-02-18 18:49:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:1.1592365503311157 norm:0.002652602270245552 max memory_allocated 22560.96044921875 
[2025-02-18 18:49:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:1.1585296392440796 norm:0.00258575938642025 max memory_allocated 22560.96044921875 
[2025-02-18 18:50:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:1.1579015254974365 norm:0.0024979652371257544 max memory_allocated 22560.96044921875 
[2025-02-18 18:50:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:1.157827377319336 norm:0.0024570084642618895 max memory_allocated 22560.96044921875 
[2025-02-18 18:51:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:1.1566247940063477 norm:0.0023863427340984344 max memory_allocated 22560.96044921875 
[2025-02-18 18:51:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:1.1544054746627808 norm:0.002381035825237632 max memory_allocated 22560.96044921875 
[2025-02-18 18:52:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:1.1509498357772827 norm:0.0038011251017451286 max memory_allocated 22560.96044921875 
[2025-02-18 18:52:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:1.1489534378051758 norm:0.005209960043430328 max memory_allocated 22560.96044921875 
[2025-02-18 18:53:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:1.1478198766708374 norm:0.005789253860712051 max memory_allocated 22560.96044921875 
[2025-02-18 18:53:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 18:54:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:1.3778307437896729 norm:0.030289165675640106 max memory_allocated 22561.13232421875 
[2025-02-18 18:54:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:1.3671454191207886 norm:0.020147019997239113 max memory_allocated 22561.13232421875 
[2025-02-18 18:55:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:1.3664822578430176 norm:0.01560303010046482 max memory_allocated 22561.13232421875 
[2025-02-18 18:55:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:1.3676197528839111 norm:0.011450364254415035 max memory_allocated 22561.13232421875 
[2025-02-18 18:56:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:1.3647332191467285 norm:0.008360485546290874 max memory_allocated 22561.13232421875 
[2025-02-18 18:57:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:1.3621940612792969 norm:0.006660284474492073 max memory_allocated 22561.13232421875 
[2025-02-18 18:57:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:1.360166072845459 norm:0.0055785467848181725 max memory_allocated 22561.13232421875 
[2025-02-18 18:58:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:1.358214259147644 norm:0.004924630280584097 max memory_allocated 22561.13232421875 
[2025-02-18 18:58:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:1.3574790954589844 norm:0.004466308280825615 max memory_allocated 22561.13232421875 
[2025-02-18 18:59:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:1.3551596403121948 norm:0.004104314837604761 max memory_allocated 22561.13232421875 
[2025-02-18 18:59:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:1.3539904356002808 norm:0.003826186992228031 max memory_allocated 22561.13232421875 
[2025-02-18 19:00:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:1.3532804250717163 norm:0.0036398647353053093 max memory_allocated 22561.13232421875 
[2025-02-18 19:00:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:1.3538517951965332 norm:0.0034929895773530006 max memory_allocated 22561.13232421875 
[2025-02-18 19:01:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:1.3540375232696533 norm:0.0034024901688098907 max memory_allocated 22561.13232421875 
[2025-02-18 19:02:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:1.3547687530517578 norm:0.0033739577047526836 max memory_allocated 22561.13232421875 
[2025-02-18 19:02:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:1.3557955026626587 norm:0.003379174042493105 max memory_allocated 22561.13232421875 
[2025-02-18 19:03:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:1.3555078506469727 norm:0.003324416233226657 max memory_allocated 22561.13232421875 
[2025-02-18 19:03:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:1.3558506965637207 norm:0.0033029159530997276 max memory_allocated 22561.13232421875 
[2025-02-18 19:04:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:1.356021523475647 norm:0.0032955268397927284 max memory_allocated 22561.13232421875 
[2025-02-18 19:04:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:1.3557490110397339 norm:0.0032868620473891497 max memory_allocated 22561.13232421875 
[2025-02-18 19:04:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 19:05:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:1.600407600402832 norm:0.0170364398509264 max memory_allocated 22561.30419921875 
[2025-02-18 19:06:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:1.620270848274231 norm:0.014229418709874153 max memory_allocated 22561.30419921875 
[2025-02-18 19:06:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:1.638446569442749 norm:0.008904378861188889 max memory_allocated 22561.30419921875 
[2025-02-18 19:07:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:1.659145712852478 norm:0.007113276515156031 max memory_allocated 22561.30419921875 
[2025-02-18 19:07:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:1.6666688919067383 norm:0.006072895135730505 max memory_allocated 22561.30419921875 
[2025-02-18 19:08:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:1.6629278659820557 norm:0.005928583908826113 max memory_allocated 22561.30419921875 
[2025-02-18 19:08:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:1.6657781600952148 norm:0.0059820436872541904 max memory_allocated 22561.30419921875 
[2025-02-18 19:09:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:1.6599717140197754 norm:0.006031223107129335 max memory_allocated 22561.30419921875 
[2025-02-18 19:10:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:1.6593704223632812 norm:0.006128453183919191 max memory_allocated 22561.30419921875 
[2025-02-18 19:10:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:1.6648112535476685 norm:0.006515612360090017 max memory_allocated 22561.30419921875 
[2025-02-18 19:11:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:1.6716639995574951 norm:0.006699482444673777 max memory_allocated 22561.30419921875 
[2025-02-18 19:11:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:1.670935034751892 norm:0.006861794274300337 max memory_allocated 22561.30419921875 
[2025-02-18 19:12:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:1.6707509756088257 norm:0.007003398146480322 max memory_allocated 22561.30419921875 
[2025-02-18 19:12:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:1.6675374507904053 norm:0.0069457171484827995 max memory_allocated 22561.30419921875 
[2025-02-18 19:13:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:1.6652216911315918 norm:0.0069144549779593945 max memory_allocated 22561.30419921875 
[2025-02-18 19:13:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:1.6637119054794312 norm:0.006921333260834217 max memory_allocated 22561.30419921875 
[2025-02-18 19:14:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:1.6630115509033203 norm:0.006843555252999067 max memory_allocated 22561.30419921875 
[2025-02-18 19:15:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:1.6630067825317383 norm:0.006866345647722483 max memory_allocated 22561.30419921875 
[2025-02-18 19:15:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:1.662584662437439 norm:0.006907016970217228 max memory_allocated 22561.30419921875 
[2025-02-18 19:16:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:1.6633412837982178 norm:0.007065001409500837 max memory_allocated 22561.30419921875 
[2025-02-18 19:16:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 19:16:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:1.8810200691223145 norm:0.01447655912488699 max memory_allocated 22561.47607421875 
[2025-02-18 19:17:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:1.8755536079406738 norm:0.01044668722897768 max memory_allocated 22561.47607421875 
[2025-02-18 19:18:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:1.8773201704025269 norm:0.00729305250570178 max memory_allocated 22561.47607421875 
[2025-02-18 19:18:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:1.8809587955474854 norm:0.0051603540778160095 max memory_allocated 22561.47607421875 
[2025-02-18 19:19:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:1.8830913305282593 norm:0.004032282158732414 max memory_allocated 22561.47607421875 
[2025-02-18 19:19:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:1.8823152780532837 norm:0.0035399198532104492 max memory_allocated 22561.47607421875 
[2025-02-18 19:20:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:1.8803032636642456 norm:0.003419559681788087 max memory_allocated 22561.47607421875 
[2025-02-18 19:20:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:1.8789256811141968 norm:0.00323013449087739 max memory_allocated 22561.47607421875 
[2025-02-18 19:21:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:1.8784253597259521 norm:0.0031761224381625652 max memory_allocated 22561.47607421875 
[2025-02-18 19:21:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:1.877243995666504 norm:0.0031224284321069717 max memory_allocated 22561.47607421875 
[2025-02-18 19:22:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:1.8757963180541992 norm:0.0031043661292642355 max memory_allocated 22561.47607421875 
[2025-02-18 19:23:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:1.875598430633545 norm:0.003122614696621895 max memory_allocated 22561.47607421875 
[2025-02-18 19:23:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:1.8756805658340454 norm:0.0031369717326015234 max memory_allocated 22561.47607421875 
[2025-02-18 19:24:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:1.8757047653198242 norm:0.00314304674975574 max memory_allocated 22561.47607421875 
[2025-02-18 19:24:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:1.876135230064392 norm:0.003210616298019886 max memory_allocated 22561.47607421875 
[2025-02-18 19:25:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:1.8756904602050781 norm:0.0031852659303694963 max memory_allocated 22561.47607421875 
[2025-02-18 19:25:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:1.8758848905563354 norm:0.0031892333645373583 max memory_allocated 22561.47607421875 
[2025-02-18 19:26:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:1.8754935264587402 norm:0.003173169447109103 max memory_allocated 22561.47607421875 
[2025-02-18 19:26:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:1.8753817081451416 norm:0.0031383796595036983 max memory_allocated 22561.47607421875 
[2025-02-18 19:27:28 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:1.8751718997955322 norm:0.0031504719518125057 max memory_allocated 22561.47607421875 
[2025-02-18 19:27:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 19:28:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:2.217242479324341 norm:0.019910449162125587 max memory_allocated 22561.64794921875 
[2025-02-18 19:28:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:2.2157585620880127 norm:0.013741102069616318 max memory_allocated 22561.64794921875 
[2025-02-18 19:29:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:2.214982032775879 norm:0.010117798112332821 max memory_allocated 22561.64794921875 
[2025-02-18 19:29:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:2.214400291442871 norm:0.007984604686498642 max memory_allocated 22561.64794921875 
[2025-02-18 19:30:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:2.2130892276763916 norm:0.0066107409074902534 max memory_allocated 22561.64794921875 
[2025-02-18 19:31:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:2.2112622261047363 norm:0.005691304337233305 max memory_allocated 22561.64794921875 
[2025-02-18 19:31:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:2.209280252456665 norm:0.004940064158290625 max memory_allocated 22561.64794921875 
[2025-02-18 19:32:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:2.2069780826568604 norm:0.0043757082894444466 max memory_allocated 22561.64794921875 
[2025-02-18 19:32:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:2.2062458992004395 norm:0.003971458412706852 max memory_allocated 22561.64794921875 
[2025-02-18 19:33:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:2.2053585052490234 norm:0.003671427257359028 max memory_allocated 22561.64794921875 
[2025-02-18 19:33:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:2.203953981399536 norm:0.00347026064991951 max memory_allocated 22561.64794921875 
[2025-02-18 19:34:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:2.2035934925079346 norm:0.0032337785232812166 max memory_allocated 22561.64794921875 
[2025-02-18 19:34:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:2.2022180557250977 norm:0.0030700021889060736 max memory_allocated 22561.64794921875 
[2025-02-18 19:35:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:2.2012412548065186 norm:0.0030017709359526634 max memory_allocated 22561.64794921875 
[2025-02-18 19:36:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:2.2008907794952393 norm:0.0028862780891358852 max memory_allocated 22561.64794921875 
[2025-02-18 19:36:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:2.2002317905426025 norm:0.0028221963439136744 max memory_allocated 22561.64794921875 
[2025-02-18 19:37:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:2.1998603343963623 norm:0.0027528866194188595 max memory_allocated 22561.64794921875 
[2025-02-18 19:37:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:2.1992483139038086 norm:0.0027086976915597916 max memory_allocated 22561.64794921875 
[2025-02-18 19:38:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:2.1993401050567627 norm:0.0027424870058894157 max memory_allocated 22561.64794921875 
[2025-02-18 19:38:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:2.1990158557891846 norm:0.0026824912056326866 max memory_allocated 22561.64794921875 
[2025-02-18 19:38:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 19:39:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:2.536557912826538 norm:0.03724168986082077 max memory_allocated 22561.81982421875 
[2025-02-18 19:40:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:2.5275111198425293 norm:0.027465900406241417 max memory_allocated 22561.81982421875 
[2025-02-18 19:40:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:2.5213961601257324 norm:0.021709607914090157 max memory_allocated 22561.81982421875 
[2025-02-18 19:41:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:2.5164644718170166 norm:0.01778329908847809 max memory_allocated 22561.81982421875 
[2025-02-18 19:41:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:2.513009548187256 norm:0.014700045809149742 max memory_allocated 22561.81982421875 
[2025-02-18 19:42:21 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:2.5109944343566895 norm:0.01269155740737915 max memory_allocated 22561.81982421875 
[2025-02-18 19:42:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:2.5083255767822266 norm:0.011295538395643234 max memory_allocated 22561.81982421875 
[2025-02-18 19:43:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:2.5067949295043945 norm:0.010111842304468155 max memory_allocated 22561.81982421875 
[2025-02-18 19:44:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:2.5027689933776855 norm:0.00899724941700697 max memory_allocated 22561.81982421875 
[2025-02-18 19:44:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:2.5019888877868652 norm:0.00792504008859396 max memory_allocated 22561.81982421875 
[2025-02-18 19:45:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:2.501094341278076 norm:0.00697885500267148 max memory_allocated 22561.81982421875 
[2025-02-18 19:45:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:2.500047445297241 norm:0.006217513233423233 max memory_allocated 22561.81982421875 
[2025-02-18 19:46:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:2.4978528022766113 norm:0.0057694027200341225 max memory_allocated 22561.81982421875 
[2025-02-18 19:46:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:2.4969558715820312 norm:0.0052584572695195675 max memory_allocated 22561.81982421875 
[2025-02-18 19:47:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:2.496811866760254 norm:0.004908483009785414 max memory_allocated 22561.81982421875 
[2025-02-18 19:47:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:2.4966647624969482 norm:0.004636215046048164 max memory_allocated 22561.81982421875 
[2025-02-18 19:48:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:2.4950928688049316 norm:0.004380723461508751 max memory_allocated 22561.81982421875 
[2025-02-18 19:49:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:2.4947915077209473 norm:0.004143824800848961 max memory_allocated 22561.81982421875 
[2025-02-18 19:49:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:2.4943411350250244 norm:0.003971097059547901 max memory_allocated 22561.81982421875 
[2025-02-18 19:50:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:2.494252920150757 norm:0.00377566902898252 max memory_allocated 22561.81982421875 
[2025-02-18 19:50:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 19:50:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:2.877948045730591 norm:0.020379403606057167 max memory_allocated 22561.99169921875 
[2025-02-18 19:51:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:2.8704426288604736 norm:0.01427450031042099 max memory_allocated 22561.99169921875 
[2025-02-18 19:52:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:2.8687639236450195 norm:0.009484251029789448 max memory_allocated 22561.99169921875 
[2025-02-18 19:52:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:2.8683948516845703 norm:0.006636659614741802 max memory_allocated 22561.99169921875 
[2025-02-18 19:53:09 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:2.8698368072509766 norm:0.004957666154950857 max memory_allocated 22561.99169921875 
[2025-02-18 19:53:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:2.871049404144287 norm:0.004190915264189243 max memory_allocated 22561.99169921875 
[2025-02-18 19:54:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:2.8696746826171875 norm:0.0035778600722551346 max memory_allocated 22561.99169921875 
[2025-02-18 19:54:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:2.8671648502349854 norm:0.0033371138852089643 max memory_allocated 22561.99169921875 
[2025-02-18 19:55:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:2.8650243282318115 norm:0.0032412363216280937 max memory_allocated 22561.99169921875 
[2025-02-18 19:55:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:2.8637006282806396 norm:0.0032186887692660093 max memory_allocated 22561.99169921875 
[2025-02-18 19:56:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:2.862783432006836 norm:0.003214488970115781 max memory_allocated 22561.99169921875 
[2025-02-18 19:57:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:2.8623335361480713 norm:0.0032238420099020004 max memory_allocated 22561.99169921875 
[2025-02-18 19:57:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:2.8621015548706055 norm:0.003222300671041012 max memory_allocated 22561.99169921875 
[2025-02-18 19:58:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:2.8623809814453125 norm:0.0032324721105396748 max memory_allocated 22561.99169921875 
[2025-02-18 19:58:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:2.8614768981933594 norm:0.003228940535336733 max memory_allocated 22561.99169921875 
[2025-02-18 19:59:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:2.8611109256744385 norm:0.0032492629252374172 max memory_allocated 22561.99169921875 
[2025-02-18 19:59:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:2.860536813735962 norm:0.0032451068982481956 max memory_allocated 22561.99169921875 
[2025-02-18 20:00:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:2.860511302947998 norm:0.003249196568503976 max memory_allocated 22561.99169921875 
[2025-02-18 20:00:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:2.860478401184082 norm:0.0032803984358906746 max memory_allocated 22561.99169921875 
[2025-02-18 20:01:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:2.8602261543273926 norm:0.0032775215804576874 max memory_allocated 22561.99169921875 
[2025-02-18 20:01:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 20:02:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:3.286529302597046 norm:0.032028306275606155 max memory_allocated 22562.16357421875 
[2025-02-18 20:02:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:3.2732415199279785 norm:0.02117762714624405 max memory_allocated 22562.16357421875 
[2025-02-18 20:03:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:3.26576828956604 norm:0.014225076884031296 max memory_allocated 22562.16357421875 
[2025-02-18 20:03:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:3.2651679515838623 norm:0.010217034257948399 max memory_allocated 22562.16357421875 
[2025-02-18 20:04:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:3.2644715309143066 norm:0.007349000778049231 max memory_allocated 22562.16357421875 
[2025-02-18 20:05:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:3.264195680618286 norm:0.005840991158038378 max memory_allocated 22562.16357421875 
[2025-02-18 20:05:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:3.2634880542755127 norm:0.0048890686593949795 max memory_allocated 22562.16357421875 
[2025-02-18 20:06:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:3.262244701385498 norm:0.004350852221250534 max memory_allocated 22562.16357421875 
[2025-02-18 20:06:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:3.2605037689208984 norm:0.003882633987814188 max memory_allocated 22562.16357421875 
[2025-02-18 20:07:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:3.2597999572753906 norm:0.003635613014921546 max memory_allocated 22562.16357421875 
[2025-02-18 20:07:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:3.2565691471099854 norm:0.003441339824348688 max memory_allocated 22562.16357421875 
[2025-02-18 20:08:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:3.2536308765411377 norm:0.003259944496676326 max memory_allocated 22562.16357421875 
[2025-02-18 20:08:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:3.252562999725342 norm:0.003155753016471863 max memory_allocated 22562.16357421875 
[2025-02-18 20:09:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:3.2513978481292725 norm:0.003093246603384614 max memory_allocated 22562.16357421875 
[2025-02-18 20:10:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:3.250927686691284 norm:0.0030487901531159878 max memory_allocated 22562.16357421875 
[2025-02-18 20:10:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:3.2510323524475098 norm:0.0030154408887028694 max memory_allocated 22562.16357421875 
[2025-02-18 20:11:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:3.2502334117889404 norm:0.0029808715917170048 max memory_allocated 22562.16357421875 
[2025-02-18 20:11:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:3.248835325241089 norm:0.0029268895741552114 max memory_allocated 22562.16357421875 
[2025-02-18 20:12:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:3.2477595806121826 norm:0.002906465670093894 max memory_allocated 22562.16357421875 
[2025-02-18 20:12:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:3.2468626499176025 norm:0.0029044051188975573 max memory_allocated 22562.16357421875 
[2025-02-18 20:13:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 20:13:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:3.6665823459625244 norm:0.029796987771987915 max memory_allocated 22562.33544921875 
[2025-02-18 20:14:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:3.648648262023926 norm:0.018485134467482567 max memory_allocated 22562.33544921875 
[2025-02-18 20:14:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:3.661264419555664 norm:0.012274239212274551 max memory_allocated 22562.33544921875 
[2025-02-18 20:15:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:3.6695125102996826 norm:0.008947736583650112 max memory_allocated 22562.33544921875 
[2025-02-18 20:15:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:3.669839859008789 norm:0.007397030480206013 max memory_allocated 22562.33544921875 
[2025-02-18 20:16:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:3.6739649772644043 norm:0.0066675287671387196 max memory_allocated 22562.33544921875 
[2025-02-18 20:16:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:3.6823863983154297 norm:0.006439087446779013 max memory_allocated 22562.33544921875 
[2025-02-18 20:17:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:3.680657386779785 norm:0.006348079536110163 max memory_allocated 22562.33544921875 
[2025-02-18 20:18:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:3.6780333518981934 norm:0.006285862997174263 max memory_allocated 22562.33544921875 
[2025-02-18 20:18:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:3.674067735671997 norm:0.006180713418871164 max memory_allocated 22562.33544921875 
[2025-02-18 20:19:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:3.6688249111175537 norm:0.006117938552051783 max memory_allocated 22562.33544921875 
[2025-02-18 20:19:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:3.6669747829437256 norm:0.0061247036792337894 max memory_allocated 22562.33544921875 
[2025-02-18 20:20:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:3.66685152053833 norm:0.006213740445673466 max memory_allocated 22562.33544921875 
[2025-02-18 20:20:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:3.665464401245117 norm:0.006291631609201431 max memory_allocated 22562.33544921875 
[2025-02-18 20:21:26 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:3.6650891304016113 norm:0.006353224162012339 max memory_allocated 22562.33544921875 
[2025-02-18 20:21:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:3.6658973693847656 norm:0.006452609784901142 max memory_allocated 22562.33544921875 
[2025-02-18 20:22:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:3.6656999588012695 norm:0.006467612460255623 max memory_allocated 22562.33544921875 
[2025-02-18 20:23:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:3.665618419647217 norm:0.006455701310187578 max memory_allocated 22562.33544921875 
[2025-02-18 20:23:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:3.6651875972747803 norm:0.006490026600658894 max memory_allocated 22562.33544921875 
[2025-02-18 20:24:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:3.6642301082611084 norm:0.006414180155843496 max memory_allocated 22562.33544921875 
[2025-02-18 20:24:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 20:24:25 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:24:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:4.149894714355469 norm:0.11908790469169617 max memory_allocated 22562.62255859375 
[2025-02-18 20:25:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:4.053923606872559 norm:0.13692504167556763 max memory_allocated 22562.62255859375 
[2025-02-18 20:26:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:4.042529106140137 norm:0.123894102871418 max memory_allocated 22562.62255859375 
[2025-02-18 20:26:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:4.035765647888184 norm:0.11524128913879395 max memory_allocated 22562.62255859375 
[2025-02-18 20:27:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:4.027996063232422 norm:0.1040000170469284 max memory_allocated 22562.62255859375 
[2025-02-18 20:27:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:4.01982307434082 norm:0.09588564187288284 max memory_allocated 22562.62255859375 
[2025-02-18 20:28:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:4.013317108154297 norm:0.08921059221029282 max memory_allocated 22562.62255859375 
[2025-02-18 20:28:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:4.007032871246338 norm:0.08335959911346436 max memory_allocated 22562.62255859375 
[2025-02-18 20:29:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:4.001570224761963 norm:0.07670824229717255 max memory_allocated 22562.62255859375 
[2025-02-18 20:30:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:3.997140884399414 norm:0.07136742025613785 max memory_allocated 22562.62255859375 
[2025-02-18 20:30:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:3.9926130771636963 norm:0.06687332689762115 max memory_allocated 22562.62255859375 
[2025-02-18 20:31:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:3.989894151687622 norm:0.06352850049734116 max memory_allocated 22562.62255859375 
[2025-02-18 20:31:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:3.9868409633636475 norm:0.06004096567630768 max memory_allocated 22562.62255859375 
[2025-02-18 20:32:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:3.985081672668457 norm:0.05896797776222229 max memory_allocated 22562.62255859375 
[2025-02-18 20:32:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:3.9851386547088623 norm:0.057898543775081635 max memory_allocated 22562.62255859375 
[2025-02-18 20:33:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:3.9827582836151123 norm:0.05573485046625137 max memory_allocated 22562.62255859375 
[2025-02-18 20:33:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:3.9819540977478027 norm:0.05405391380190849 max memory_allocated 22562.62255859375 
[2025-02-18 20:34:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:3.980536460876465 norm:0.05198501795530319 max memory_allocated 22562.62255859375 
[2025-02-18 20:35:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:3.9794793128967285 norm:0.050046224147081375 max memory_allocated 22562.62255859375 
[2025-02-18 20:35:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:3.978672504425049 norm:0.04845784977078438 max memory_allocated 22562.62255859375 
[2025-02-18 20:35:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 20:35:50 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:36:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:4.505965709686279 norm:0.11021745204925537 max memory_allocated 22562.79443359375 
[2025-02-18 20:36:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:4.467048168182373 norm:0.1050870418548584 max memory_allocated 22562.79443359375 
[2025-02-18 20:37:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:4.45641565322876 norm:0.09093977510929108 max memory_allocated 22562.79443359375 
[2025-02-18 20:38:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:4.448131084442139 norm:0.08151736855506897 max memory_allocated 22562.79443359375 
[2025-02-18 20:38:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:4.441206932067871 norm:0.07230493426322937 max memory_allocated 22562.79443359375 
[2025-02-18 20:39:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:4.435145378112793 norm:0.0663691908121109 max memory_allocated 22562.79443359375 
[2025-02-18 20:39:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:4.4290876388549805 norm:0.061140552163124084 max memory_allocated 22562.79443359375 
[2025-02-18 20:40:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:4.423043251037598 norm:0.05650270730257034 max memory_allocated 22562.79443359375 
[2025-02-18 20:40:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:4.418155193328857 norm:0.05240709334611893 max memory_allocated 22562.79443359375 
[2025-02-18 20:41:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:4.413959503173828 norm:0.04895428195595741 max memory_allocated 22562.79443359375 
[2025-02-18 20:41:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:4.41007137298584 norm:0.0461014062166214 max memory_allocated 22562.79443359375 
[2025-02-18 20:42:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:4.407963752746582 norm:0.04402686655521393 max memory_allocated 22562.79443359375 
[2025-02-18 20:43:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:4.405764579772949 norm:0.04303254187107086 max memory_allocated 22562.79443359375 
[2025-02-18 20:43:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:4.40331506729126 norm:0.041180599480867386 max memory_allocated 22562.79443359375 
[2025-02-18 20:44:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:4.401481628417969 norm:0.039770882576704025 max memory_allocated 22562.79443359375 
[2025-02-18 20:44:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:4.399740219116211 norm:0.038643691688776016 max memory_allocated 22562.79443359375 
[2025-02-18 20:45:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:4.398656845092773 norm:0.03740180656313896 max memory_allocated 22562.79443359375 
[2025-02-18 20:45:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:4.3978495597839355 norm:0.036287665367126465 max memory_allocated 22562.79443359375 
[2025-02-18 20:46:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:4.397165298461914 norm:0.0356183759868145 max memory_allocated 22562.79443359375 
[2025-02-18 20:47:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:4.396511554718018 norm:0.035009246319532394 max memory_allocated 22562.79443359375 
[2025-02-18 20:47:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 20:47:15 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:47:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:5.594181537628174 norm:0.1260087788105011 max memory_allocated 22562.96630859375 
[2025-02-18 20:48:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:5.534069538116455 norm:0.11961673945188522 max memory_allocated 22562.96630859375 
[2025-02-18 20:48:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:5.510358810424805 norm:0.0976792573928833 max memory_allocated 22562.96630859375 
[2025-02-18 20:49:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:5.4941816329956055 norm:0.08220596611499786 max memory_allocated 22562.96630859375 
[2025-02-18 20:50:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:5.480385780334473 norm:0.07192140072584152 max memory_allocated 22562.96630859375 
[2025-02-18 20:50:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:5.469780921936035 norm:0.06464977562427521 max memory_allocated 22562.96630859375 
[2025-02-18 20:51:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:5.456198692321777 norm:0.061116062104701996 max memory_allocated 22562.96630859375 
[2025-02-18 20:51:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:5.432163238525391 norm:0.060925692319869995 max memory_allocated 22562.96630859375 
[2025-02-18 20:52:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:5.4201836585998535 norm:0.05948962643742561 max memory_allocated 22562.96630859375 
[2025-02-18 20:52:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:5.409218788146973 norm:0.05845576524734497 max memory_allocated 22562.96630859375 
[2025-02-18 20:53:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:5.40952205657959 norm:0.05813927575945854 max memory_allocated 22562.96630859375 
[2025-02-18 20:53:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:5.405783176422119 norm:0.057079438120126724 max memory_allocated 22562.96630859375 
[2025-02-18 20:54:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:5.392963409423828 norm:0.057816147804260254 max memory_allocated 22562.96630859375 
[2025-02-18 20:55:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:5.396997451782227 norm:0.05972534790635109 max memory_allocated 22562.96630859375 
[2025-02-18 20:55:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:5.384833812713623 norm:0.054775889962911606 max memory_allocated 22562.96630859375 
[2025-02-18 20:56:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:5.390939712524414 norm:0.055623363703489304 max memory_allocated 22562.96630859375 
[2025-02-18 20:56:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:5.371819972991943 norm:0.05806499719619751 max memory_allocated 22562.96630859375 
[2025-02-18 20:57:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:5.364195823669434 norm:0.05331885814666748 max memory_allocated 22562.96630859375 
[2025-02-18 20:57:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:5.36402702331543 norm:0.049868036061525345 max memory_allocated 22562.96630859375 
[2025-02-18 20:58:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:5.3684306144714355 norm:0.049719661474227905 max memory_allocated 22562.96630859375 
[2025-02-18 20:58:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 20:58:39 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:59:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:10.199416160583496 norm:0.4457685351371765 max memory_allocated 22563.13818359375 
[2025-02-18 20:59:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:9.889144897460938 norm:0.40372559428215027 max memory_allocated 22563.13818359375 
[2025-02-18 21:00:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:9.777139663696289 norm:0.3617290258407593 max memory_allocated 22563.13818359375 
[2025-02-18 21:00:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:9.695199012756348 norm:0.3288841247558594 max memory_allocated 22563.13818359375 
[2025-02-18 21:01:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:9.645261764526367 norm:0.2942626476287842 max memory_allocated 22563.13818359375 
[2025-02-18 21:02:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:9.597877502441406 norm:0.2666904926300049 max memory_allocated 22563.13818359375 
[2025-02-18 21:02:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:9.555278778076172 norm:0.24081318080425262 max memory_allocated 22563.13818359375 
[2025-02-18 21:03:08 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:9.51842212677002 norm:0.21714231371879578 max memory_allocated 22563.13818359375 
[2025-02-18 21:03:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:9.492776870727539 norm:0.19795657694339752 max memory_allocated 22563.13818359375 
[2025-02-18 21:04:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:9.460203170776367 norm:0.18518760800361633 max memory_allocated 22563.13818359375 
[2025-02-18 21:04:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:9.416236877441406 norm:0.17720414698123932 max memory_allocated 22563.13818359375 
[2025-02-18 21:05:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:9.389631271362305 norm:0.16632671654224396 max memory_allocated 22563.13818359375 
[2025-02-18 21:05:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:9.368958473205566 norm:0.1605413258075714 max memory_allocated 22563.13818359375 
[2025-02-18 21:06:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:9.353289604187012 norm:0.15377391874790192 max memory_allocated 22563.13818359375 
[2025-02-18 21:07:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:9.337406158447266 norm:0.14902466535568237 max memory_allocated 22563.13818359375 
[2025-02-18 21:07:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:9.32725715637207 norm:0.1457734853029251 max memory_allocated 22563.13818359375 
[2025-02-18 21:08:10 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:9.314155578613281 norm:0.14179712533950806 max memory_allocated 22563.13818359375 
[2025-02-18 21:08:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:9.300902366638184 norm:0.13853535056114197 max memory_allocated 22563.13818359375 
[2025-02-18 21:09:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:9.279438018798828 norm:0.13709621131420135 max memory_allocated 22563.13818359375 
[2025-02-18 21:09:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:9.269708633422852 norm:0.13622115552425385 max memory_allocated 22563.13818359375 
[2025-02-18 21:10:01 root] (main_calibration.py 365): INFO 21817.046661376953
[2025-02-18 21:10:31 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-18 21:11:41 root] (main_calibration.py 158): INFO wikitext2 : 11.662426948547363
[2025-02-18 21:11:41 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-18 21:13:30 root] (main_calibration.py 158): INFO c4 : 16.114267349243164
[2025-02-18 22:49:09 root] (main_calibration.py 169): INFO {'wikitext2': 11.662426948547363, 'c4': 16.114267349243164, 'results': {'piqa': {'acc': 0.6245919477693145, 'acc_stderr': 0.011297839589776662, 'acc_norm': 0.6338411316648531, 'acc_norm_stderr': 0.011240106070308464}, 'arc_easy': {'acc': 0.3977272727272727, 'acc_stderr': 0.010042861602178061, 'acc_norm': 0.36195286195286197, 'acc_norm_stderr': 0.009860991466688502}, 'winogrande': {'acc': 0.4964483030781373, 'acc_stderr': 0.014052131146915853}, 'arc_challenge': {'acc': 0.2380546075085324, 'acc_stderr': 0.012445770028026212, 'acc_norm': 0.2781569965870307, 'acc_norm_stderr': 0.013094469919538804}, 'hellaswag': {'acc': 0.388070105556662, 'acc_stderr': 0.004863147544177515, 'acc_norm': 0.49044015136427005, 'acc_norm_stderr': 0.0049888692887868746}, 'boolq': {'acc': 0.5651376146788991, 'acc_stderr': 0.008670528471841556}}, 'versions': {'piqa': 0, 'arc_easy': 0, 'winogrande': 0, 'arc_challenge': 0, 'hellaswag': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
