[2025-02-18 15:02:35 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation/Llama-2-7b-hf-w4a4', save_dir='./log-calibration-compensation/quant/Llama-2-7b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-18 15:05:09 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 15:05:09 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-18 15:05:09 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 15:05:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 15:05:15 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:05:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.05821835994720459 norm:0.0524008646607399 max memory_allocated 22560.81005859375 
[2025-02-18 15:06:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.036786675453186035 norm:0.035248562693595886 max memory_allocated 22560.81005859375 
[2025-02-18 15:06:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.030663195997476578 norm:0.023847799748182297 max memory_allocated 22560.81005859375 
[2025-02-18 15:07:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.02807185798883438 norm:0.020313721150159836 max memory_allocated 22560.81005859375 
[2025-02-18 15:07:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.028818102553486824 norm:0.021198486909270287 max memory_allocated 22560.81005859375 
[2025-02-18 15:08:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.026632627472281456 norm:0.012891532853245735 max memory_allocated 22560.81005859375 
[2025-02-18 15:09:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.02406282164156437 norm:0.009283646009862423 max memory_allocated 22560.81005859375 
[2025-02-18 15:09:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.023445723578333855 norm:0.008458945900201797 max memory_allocated 22560.81005859375 
[2025-02-18 15:10:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.023425303399562836 norm:0.007818344980478287 max memory_allocated 22560.81005859375 
[2025-02-18 15:10:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.024133959785103798 norm:0.008460372686386108 max memory_allocated 22560.81005859375 
[2025-02-18 15:11:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.023922042921185493 norm:0.006616833619773388 max memory_allocated 22560.81005859375 
[2025-02-18 15:11:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.026862548664212227 norm:0.013820428401231766 max memory_allocated 22560.81005859375 
[2025-02-18 15:12:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.025649718940258026 norm:0.006956196390092373 max memory_allocated 22560.81005859375 
[2025-02-18 15:12:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.02412109076976776 norm:0.005226104520261288 max memory_allocated 22560.81005859375 
[2025-02-18 15:13:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.024470603093504906 norm:0.006639618426561356 max memory_allocated 22560.81005859375 
[2025-02-18 15:14:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.024887263774871826 norm:0.006374657154083252 max memory_allocated 22560.81005859375 
[2025-02-18 15:14:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.02549249865114689 norm:0.0068749720230698586 max memory_allocated 22560.81005859375 
[2025-02-18 15:15:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0248856283724308 norm:0.005497862119227648 max memory_allocated 22560.81005859375 
[2025-02-18 15:15:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.02521563321352005 norm:0.005984565243124962 max memory_allocated 22560.81005859375 
[2025-02-18 15:16:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.023489028215408325 norm:0.003781306091696024 max memory_allocated 22560.81005859375 
[2025-02-18 15:16:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 15:16:27 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:17:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.2540929317474365 norm:0.1018027737736702 max memory_allocated 22560.98193359375 
[2025-02-18 15:17:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.19537915289402008 norm:0.07587338984012604 max memory_allocated 22560.98193359375 
[2025-02-18 15:18:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.1776864230632782 norm:0.06799362599849701 max memory_allocated 22560.98193359375 
[2025-02-18 15:18:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.16454820334911346 norm:0.05924414098262787 max memory_allocated 22560.98193359375 
[2025-02-18 15:19:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.15782125294208527 norm:0.056650735437870026 max memory_allocated 22560.98193359375 
[2025-02-18 15:19:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.14866343140602112 norm:0.052109040319919586 max memory_allocated 22560.98193359375 
[2025-02-18 15:20:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.1454063057899475 norm:0.049834590405225754 max memory_allocated 22560.98193359375 
[2025-02-18 15:20:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.14317163825035095 norm:0.046929385513067245 max memory_allocated 22560.98193359375 
[2025-02-18 15:21:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.1414688229560852 norm:0.04392215237021446 max memory_allocated 22560.98193359375 
[2025-02-18 15:21:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.13977296650409698 norm:0.04077460989356041 max memory_allocated 22560.98193359375 
[2025-02-18 15:22:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.1378517895936966 norm:0.03821025788784027 max memory_allocated 22560.98193359375 
[2025-02-18 15:23:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.13669298589229584 norm:0.037778448313474655 max memory_allocated 22560.98193359375 
[2025-02-18 15:23:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.13445203006267548 norm:0.034369297325611115 max memory_allocated 22560.98193359375 
[2025-02-18 15:24:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.13392138481140137 norm:0.03152588754892349 max memory_allocated 22560.98193359375 
[2025-02-18 15:24:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.13246525824069977 norm:0.029293116182088852 max memory_allocated 22560.98193359375 
[2025-02-18 15:25:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.13229066133499146 norm:0.027584824711084366 max memory_allocated 22560.98193359375 
[2025-02-18 15:25:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.13189701735973358 norm:0.0266221072524786 max memory_allocated 22560.98193359375 
[2025-02-18 15:26:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.13048148155212402 norm:0.024228015914559364 max memory_allocated 22560.98193359375 
[2025-02-18 15:26:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.1297985166311264 norm:0.02320004254579544 max memory_allocated 22560.98193359375 
[2025-02-18 15:27:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.12832628190517426 norm:0.02159116044640541 max memory_allocated 22560.98193359375 
[2025-02-18 15:27:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 15:27:40 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 15:28:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.2015870213508606 norm:0.04632898420095444 max memory_allocated 22561.15380859375 
[2025-02-18 15:28:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.18545252084732056 norm:0.03504306077957153 max memory_allocated 22561.15380859375 
[2025-02-18 15:29:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.18275785446166992 norm:0.0285390242934227 max memory_allocated 22561.15380859375 
[2025-02-18 15:29:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.17946165800094604 norm:0.02162247709929943 max memory_allocated 22561.15380859375 
[2025-02-18 15:30:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.17603161931037903 norm:0.015629950910806656 max memory_allocated 22561.15380859375 
[2025-02-18 15:30:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.17485003173351288 norm:0.011796464212238789 max memory_allocated 22561.15380859375 
[2025-02-18 15:31:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.174136221408844 norm:0.009440198540687561 max memory_allocated 22561.15380859375 
[2025-02-18 15:32:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.17342862486839294 norm:0.00813246238976717 max memory_allocated 22561.15380859375 
[2025-02-18 15:32:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.17170101404190063 norm:0.0072355312295258045 max memory_allocated 22561.15380859375 
[2025-02-18 15:33:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.16961273550987244 norm:0.006505608092993498 max memory_allocated 22561.15380859375 
[2025-02-18 15:33:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.1691371351480484 norm:0.006093395408242941 max memory_allocated 22561.15380859375 
[2025-02-18 15:34:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.16949275135993958 norm:0.005926193669438362 max memory_allocated 22561.15380859375 
[2025-02-18 15:34:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.16937285661697388 norm:0.005811352748423815 max memory_allocated 22561.15380859375 
[2025-02-18 15:35:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.1695897877216339 norm:0.005739381071180105 max memory_allocated 22561.15380859375 
[2025-02-18 15:35:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.16931341588497162 norm:0.005602866876870394 max memory_allocated 22561.15380859375 
[2025-02-18 15:36:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.16894125938415527 norm:0.00547865591943264 max memory_allocated 22561.15380859375 
[2025-02-18 15:37:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.16895179450511932 norm:0.005271920468658209 max memory_allocated 22561.15380859375 
[2025-02-18 15:37:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.16895484924316406 norm:0.005165525246411562 max memory_allocated 22561.15380859375 
[2025-02-18 15:38:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.1690273880958557 norm:0.0050916592590510845 max memory_allocated 22561.15380859375 
[2025-02-18 15:38:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.16921034455299377 norm:0.005091681610792875 max memory_allocated 22561.15380859375 
[2025-02-18 15:38:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 15:39:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.25287681818008423 norm:0.018123306334018707 max memory_allocated 22561.21044921875 
[2025-02-18 15:40:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.23408818244934082 norm:0.006593682803213596 max memory_allocated 22561.21044921875 
[2025-02-18 15:40:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.22818708419799805 norm:0.003816095646470785 max memory_allocated 22561.21044921875 
[2025-02-18 15:41:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.22586438059806824 norm:0.002555279992520809 max memory_allocated 22561.21044921875 
[2025-02-18 15:41:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.2247786819934845 norm:0.0020010732114315033 max memory_allocated 22561.21044921875 
[2025-02-18 15:42:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.2225988507270813 norm:0.0016596668865531683 max memory_allocated 22561.21044921875 
[2025-02-18 15:42:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.22165311872959137 norm:0.0015168486861512065 max memory_allocated 22561.21044921875 
[2025-02-18 15:43:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.22031570971012115 norm:0.001468597212806344 max memory_allocated 22561.21044921875 
[2025-02-18 15:43:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.21920661628246307 norm:0.0013454246800392866 max memory_allocated 22561.21044921875 
[2025-02-18 15:44:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.21917696297168732 norm:0.0012755586067214608 max memory_allocated 22561.21044921875 
[2025-02-18 15:45:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.21941567957401276 norm:0.001244917744770646 max memory_allocated 22561.21044921875 
[2025-02-18 15:45:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.21951834857463837 norm:0.0012136754812672734 max memory_allocated 22561.21044921875 
[2025-02-18 15:46:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.21863360702991486 norm:0.0011823514942079782 max memory_allocated 22561.21044921875 
[2025-02-18 15:46:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.21810387074947357 norm:0.0011634321417659521 max memory_allocated 22561.21044921875 
[2025-02-18 15:47:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.21778994798660278 norm:0.001149744843132794 max memory_allocated 22561.21044921875 
[2025-02-18 15:47:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.2179918885231018 norm:0.0011627842904999852 max memory_allocated 22561.21044921875 
[2025-02-18 15:48:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.21781781315803528 norm:0.0011535899247974157 max memory_allocated 22561.21044921875 
[2025-02-18 15:48:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.2177966833114624 norm:0.001161421649158001 max memory_allocated 22561.21044921875 
[2025-02-18 15:49:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.2179398536682129 norm:0.0011573850642889738 max memory_allocated 22561.21044921875 
[2025-02-18 15:49:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.21777954697608948 norm:0.0011563654989004135 max memory_allocated 22561.21044921875 
[2025-02-18 15:50:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 15:50:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.3303413987159729 norm:0.041546665132045746 max memory_allocated 22561.38232421875 
[2025-02-18 15:51:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.30487191677093506 norm:0.017278317362070084 max memory_allocated 22561.38232421875 
[2025-02-18 15:51:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.2935030162334442 norm:0.010181008838117123 max memory_allocated 22561.38232421875 
[2025-02-18 15:52:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.2875748872756958 norm:0.006284378003329039 max memory_allocated 22561.38232421875 
[2025-02-18 15:52:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.282600075006485 norm:0.0044152396731078625 max memory_allocated 22561.38232421875 
[2025-02-18 15:53:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.2791695296764374 norm:0.003342648269608617 max memory_allocated 22561.38232421875 
[2025-02-18 15:54:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.278683066368103 norm:0.0028207285795360804 max memory_allocated 22561.38232421875 
[2025-02-18 15:54:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.2778796851634979 norm:0.002497744280844927 max memory_allocated 22561.38232421875 
[2025-02-18 15:55:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.2768436074256897 norm:0.00219279364682734 max memory_allocated 22561.38232421875 
[2025-02-18 15:55:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.27567610144615173 norm:0.002041471656411886 max memory_allocated 22561.38232421875 
[2025-02-18 15:56:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.2755286395549774 norm:0.0018694697646424174 max memory_allocated 22561.38232421875 
[2025-02-18 15:56:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.27582842111587524 norm:0.0017705177888274193 max memory_allocated 22561.38232421875 
[2025-02-18 15:57:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.2755940854549408 norm:0.0017087949672713876 max memory_allocated 22561.38232421875 
[2025-02-18 15:57:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.2752625346183777 norm:0.0016691719647496939 max memory_allocated 22561.38232421875 
[2025-02-18 15:58:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.274618536233902 norm:0.0016176500357687473 max memory_allocated 22561.38232421875 
[2025-02-18 15:59:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.2737557888031006 norm:0.0015731097664684057 max memory_allocated 22561.38232421875 
[2025-02-18 15:59:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.27295517921447754 norm:0.0015451248036697507 max memory_allocated 22561.38232421875 
[2025-02-18 16:00:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.2727227210998535 norm:0.0014896345091983676 max memory_allocated 22561.38232421875 
[2025-02-18 16:00:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.2726549208164215 norm:0.0014803570229560137 max memory_allocated 22561.38232421875 
[2025-02-18 16:01:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.2724316418170929 norm:0.0014546581078320742 max memory_allocated 22561.38232421875 
[2025-02-18 16:01:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 16:01:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.3539896011352539 norm:0.014605476520955563 max memory_allocated 22561.55419921875 
[2025-02-18 16:02:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.3391629755496979 norm:0.007152661215513945 max memory_allocated 22561.55419921875 
[2025-02-18 16:03:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.3360905051231384 norm:0.0041319867596030235 max memory_allocated 22561.55419921875 
[2025-02-18 16:03:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.3318065404891968 norm:0.002849457785487175 max memory_allocated 22561.55419921875 
[2025-02-18 16:04:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.3280579149723053 norm:0.002188572194427252 max memory_allocated 22561.55419921875 
[2025-02-18 16:04:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.32512155175209045 norm:0.0019004998030140996 max memory_allocated 22561.55419921875 
[2025-02-18 16:05:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.32385367155075073 norm:0.0018036200199276209 max memory_allocated 22561.55419921875 
[2025-02-18 16:05:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.3220612406730652 norm:0.0017053448827937245 max memory_allocated 22561.55419921875 
[2025-02-18 16:06:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.3218936324119568 norm:0.0016968545969575644 max memory_allocated 22561.55419921875 
[2025-02-18 16:06:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.32324641942977905 norm:0.0017641301965340972 max memory_allocated 22561.55419921875 
[2025-02-18 16:07:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.32332924008369446 norm:0.0017621947918087244 max memory_allocated 22561.55419921875 
[2025-02-18 16:08:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.3228914737701416 norm:0.0017357402248308063 max memory_allocated 22561.55419921875 
[2025-02-18 16:08:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.32244864106178284 norm:0.0017095580697059631 max memory_allocated 22561.55419921875 
[2025-02-18 16:09:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.3218931257724762 norm:0.0016856122529134154 max memory_allocated 22561.55419921875 
[2025-02-18 16:09:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.32078850269317627 norm:0.0016681378474459052 max memory_allocated 22561.55419921875 
[2025-02-18 16:10:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.3203710913658142 norm:0.001640638685785234 max memory_allocated 22561.55419921875 
[2025-02-18 16:10:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.3203940689563751 norm:0.0016467737732455134 max memory_allocated 22561.55419921875 
[2025-02-18 16:11:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.3199838697910309 norm:0.0016445635119453073 max memory_allocated 22561.55419921875 
[2025-02-18 16:11:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.3192458748817444 norm:0.0016337581910192966 max memory_allocated 22561.55419921875 
[2025-02-18 16:12:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.31880420446395874 norm:0.001641751965507865 max memory_allocated 22561.55419921875 
[2025-02-18 16:12:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 16:13:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.42849433422088623 norm:0.03660885617136955 max memory_allocated 22561.72607421875 
[2025-02-18 16:13:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.3950621783733368 norm:0.014674888923764229 max memory_allocated 22561.72607421875 
[2025-02-18 16:14:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.3907870054244995 norm:0.0094219371676445 max memory_allocated 22561.72607421875 
[2025-02-18 16:14:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.3872801959514618 norm:0.006778442766517401 max memory_allocated 22561.72607421875 
[2025-02-18 16:15:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.38221874833106995 norm:0.005148349329829216 max memory_allocated 22561.72607421875 
[2025-02-18 16:15:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.37768009305000305 norm:0.00407251575961709 max memory_allocated 22561.72607421875 
[2025-02-18 16:16:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.3780268132686615 norm:0.003650512546300888 max memory_allocated 22561.72607421875 
[2025-02-18 16:17:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.3785552382469177 norm:0.0033751768060028553 max memory_allocated 22561.72607421875 
[2025-02-18 16:17:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.3808111846446991 norm:0.003415055572986603 max memory_allocated 22561.72607421875 
[2025-02-18 16:18:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.38039422035217285 norm:0.003157279919832945 max memory_allocated 22561.72607421875 
[2025-02-18 16:18:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.37817877531051636 norm:0.0030644775833934546 max memory_allocated 22561.72607421875 
[2025-02-18 16:19:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.37768614292144775 norm:0.0030564565677195787 max memory_allocated 22561.72607421875 
[2025-02-18 16:19:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.3772202432155609 norm:0.0029980193357914686 max memory_allocated 22561.72607421875 
[2025-02-18 16:20:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.37715208530426025 norm:0.002972421469166875 max memory_allocated 22561.72607421875 
[2025-02-18 16:20:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.3767874836921692 norm:0.0029772804118692875 max memory_allocated 22561.72607421875 
[2025-02-18 16:21:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.3759809136390686 norm:0.002930241636931896 max memory_allocated 22561.72607421875 
[2025-02-18 16:22:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.37601616978645325 norm:0.0029407606925815344 max memory_allocated 22561.72607421875 
[2025-02-18 16:22:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.37584298849105835 norm:0.0029275952838361263 max memory_allocated 22561.72607421875 
[2025-02-18 16:23:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.3761860430240631 norm:0.0029251486994326115 max memory_allocated 22561.72607421875 
[2025-02-18 16:23:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.3760101795196533 norm:0.0029066922143101692 max memory_allocated 22561.72607421875 
[2025-02-18 16:23:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 16:24:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.485147625207901 norm:0.03621635213494301 max memory_allocated 22561.89794921875 
[2025-02-18 16:25:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.4402051568031311 norm:0.01189965195953846 max memory_allocated 22561.89794921875 
[2025-02-18 16:25:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.4300791025161743 norm:0.007185901515185833 max memory_allocated 22561.89794921875 
[2025-02-18 16:26:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.4238378405570984 norm:0.004530884325504303 max memory_allocated 22561.89794921875 
[2025-02-18 16:26:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.4195566773414612 norm:0.003359032329171896 max memory_allocated 22561.89794921875 
[2025-02-18 16:27:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.4165981709957123 norm:0.002871867036446929 max memory_allocated 22561.89794921875 
[2025-02-18 16:27:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.4155219793319702 norm:0.00280510145239532 max memory_allocated 22561.89794921875 
[2025-02-18 16:28:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.4143718183040619 norm:0.0026093728374689817 max memory_allocated 22561.89794921875 
[2025-02-18 16:28:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.41207218170166016 norm:0.0024987608194351196 max memory_allocated 22561.89794921875 
[2025-02-18 16:29:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.41192126274108887 norm:0.002509613521397114 max memory_allocated 22561.89794921875 
[2025-02-18 16:29:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.4121389389038086 norm:0.002500954084098339 max memory_allocated 22561.89794921875 
[2025-02-18 16:30:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.41021662950515747 norm:0.002410267014056444 max memory_allocated 22561.89794921875 
[2025-02-18 16:31:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.40873780846595764 norm:0.002319125924259424 max memory_allocated 22561.89794921875 
[2025-02-18 16:31:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.40819045901298523 norm:0.002289966680109501 max memory_allocated 22561.89794921875 
[2025-02-18 16:32:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.40870654582977295 norm:0.002294653095304966 max memory_allocated 22561.89794921875 
[2025-02-18 16:32:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.4096231460571289 norm:0.0023127116728574038 max memory_allocated 22561.89794921875 
[2025-02-18 16:33:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.40938547253608704 norm:0.0023353234864771366 max memory_allocated 22561.89794921875 
[2025-02-18 16:33:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.4099784791469574 norm:0.0023435039911419153 max memory_allocated 22561.89794921875 
[2025-02-18 16:34:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.41038408875465393 norm:0.0023849313147366047 max memory_allocated 22561.89794921875 
[2025-02-18 16:34:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.4105771481990814 norm:0.0024215453304350376 max memory_allocated 22561.89794921875 
[2025-02-18 16:35:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 16:35:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.4831357002258301 norm:0.01912381872534752 max memory_allocated 22562.06982421875 
[2025-02-18 16:36:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.45852696895599365 norm:0.007385721895843744 max memory_allocated 22562.06982421875 
[2025-02-18 16:36:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.45171061158180237 norm:0.00467139296233654 max memory_allocated 22562.06982421875 
[2025-02-18 16:37:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.4498617649078369 norm:0.0035357768647372723 max memory_allocated 22562.06982421875 
[2025-02-18 16:37:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.44685986638069153 norm:0.0026816544122993946 max memory_allocated 22562.06982421875 
[2025-02-18 16:38:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.44529929757118225 norm:0.0022610679734498262 max memory_allocated 22562.06982421875 
[2025-02-18 16:39:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.4415319263935089 norm:0.0020204726606607437 max memory_allocated 22562.06982421875 
[2025-02-18 16:39:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.4401998221874237 norm:0.001921934774145484 max memory_allocated 22562.06982421875 
[2025-02-18 16:40:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.43935179710388184 norm:0.0018916523549705744 max memory_allocated 22562.06982421875 
[2025-02-18 16:40:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.43763330578804016 norm:0.001800261321477592 max memory_allocated 22562.06982421875 
[2025-02-18 16:41:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.43752264976501465 norm:0.0018076507840305567 max memory_allocated 22562.06982421875 
[2025-02-18 16:41:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.439879834651947 norm:0.001923330593854189 max memory_allocated 22562.06982421875 
[2025-02-18 16:42:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.4402669668197632 norm:0.0019438586896285415 max memory_allocated 22562.06982421875 
[2025-02-18 16:42:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.43945831060409546 norm:0.0018914347747340798 max memory_allocated 22562.06982421875 
[2025-02-18 16:43:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.43769630789756775 norm:0.0018555682618170977 max memory_allocated 22562.06982421875 
[2025-02-18 16:44:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.4365658164024353 norm:0.001804983476176858 max memory_allocated 22562.06982421875 
[2025-02-18 16:44:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.4366512894630432 norm:0.001821804791688919 max memory_allocated 22562.06982421875 
[2025-02-18 16:45:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.4363475441932678 norm:0.0018406347371637821 max memory_allocated 22562.06982421875 
[2025-02-18 16:45:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.4355121850967407 norm:0.0018261164659634233 max memory_allocated 22562.06982421875 
[2025-02-18 16:46:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.43571943044662476 norm:0.0018159737810492516 max memory_allocated 22562.06982421875 
[2025-02-18 16:46:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 16:46:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.5345147252082825 norm:0.03307661414146423 max memory_allocated 22562.24169921875 
[2025-02-18 16:47:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.4959813952445984 norm:0.01310834102332592 max memory_allocated 22562.24169921875 
[2025-02-18 16:48:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.4785109758377075 norm:0.006615911144763231 max memory_allocated 22562.24169921875 
[2025-02-18 16:48:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.4727977514266968 norm:0.004330343566834927 max memory_allocated 22562.24169921875 
[2025-02-18 16:49:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.46949562430381775 norm:0.0034062715712934732 max memory_allocated 22562.24169921875 
[2025-02-18 16:49:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.4664655029773712 norm:0.0028846547938883305 max memory_allocated 22562.24169921875 
[2025-02-18 16:50:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.4661826491355896 norm:0.0028521958738565445 max memory_allocated 22562.24169921875 
[2025-02-18 16:50:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.4645795226097107 norm:0.002416122704744339 max memory_allocated 22562.24169921875 
[2025-02-18 16:51:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.4613601267337799 norm:0.002051325049251318 max memory_allocated 22562.24169921875 
[2025-02-18 16:51:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.4598659873008728 norm:0.001884794794023037 max memory_allocated 22562.24169921875 
[2025-02-18 16:52:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.4577309489250183 norm:0.0017228566575795412 max memory_allocated 22562.24169921875 
[2025-02-18 16:53:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.45673197507858276 norm:0.0017039377707988024 max memory_allocated 22562.24169921875 
[2025-02-18 16:53:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.45667096972465515 norm:0.0016151603776961565 max memory_allocated 22562.24169921875 
[2025-02-18 16:54:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.457263320684433 norm:0.001631808583624661 max memory_allocated 22562.24169921875 
[2025-02-18 16:54:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.457173228263855 norm:0.0016167842550203204 max memory_allocated 22562.24169921875 
[2025-02-18 16:55:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.45745086669921875 norm:0.0015584281645715237 max memory_allocated 22562.24169921875 
[2025-02-18 16:55:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.4585816562175751 norm:0.00156106473878026 max memory_allocated 22562.24169921875 
[2025-02-18 16:56:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.45883914828300476 norm:0.0014807733241468668 max memory_allocated 22562.24169921875 
[2025-02-18 16:56:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.45855429768562317 norm:0.0014878257643431425 max memory_allocated 22562.24169921875 
[2025-02-18 16:57:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.4601283669471741 norm:0.001509709982201457 max memory_allocated 22562.24169921875 
[2025-02-18 16:57:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 16:58:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.5338072776794434 norm:0.03231681138277054 max memory_allocated 22562.41357421875 
[2025-02-18 16:58:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.5051828026771545 norm:0.014560947194695473 max memory_allocated 22562.41357421875 
[2025-02-18 16:59:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.49550044536590576 norm:0.00952175259590149 max memory_allocated 22562.41357421875 
[2025-02-18 16:59:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.49008551239967346 norm:0.0074734557420015335 max memory_allocated 22562.41357421875 
[2025-02-18 17:00:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.4863319993019104 norm:0.00572133669629693 max memory_allocated 22562.41357421875 
[2025-02-18 17:00:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.48474809527397156 norm:0.004496443551033735 max memory_allocated 22562.41357421875 
[2025-02-18 17:01:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.48206251859664917 norm:0.0035032976884394884 max memory_allocated 22562.41357421875 
[2025-02-18 17:02:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.48020610213279724 norm:0.0030504565220326185 max memory_allocated 22562.41357421875 
[2025-02-18 17:02:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.4780197739601135 norm:0.0025107876863330603 max memory_allocated 22562.41357421875 
[2025-02-18 17:03:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.4760999381542206 norm:0.0022064982913434505 max memory_allocated 22562.41357421875 
[2025-02-18 17:03:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.4745433032512665 norm:0.0020173999946564436 max memory_allocated 22562.41357421875 
[2025-02-18 17:04:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.4737266004085541 norm:0.0019212940242141485 max memory_allocated 22562.41357421875 
[2025-02-18 17:04:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.4733535349369049 norm:0.001842502155341208 max memory_allocated 22562.41357421875 
[2025-02-18 17:05:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.473233699798584 norm:0.0016676606610417366 max memory_allocated 22562.41357421875 
[2025-02-18 17:05:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.4714045524597168 norm:0.0015460299327969551 max memory_allocated 22562.41357421875 
[2025-02-18 17:06:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.47111624479293823 norm:0.0014747127424925566 max memory_allocated 22562.41357421875 
[2025-02-18 17:07:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.4702233076095581 norm:0.0013880643527954817 max memory_allocated 22562.41357421875 
[2025-02-18 17:07:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.4702460765838623 norm:0.0013635378563776612 max memory_allocated 22562.41357421875 
[2025-02-18 17:08:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.46966180205345154 norm:0.001346064847894013 max memory_allocated 22562.41357421875 
[2025-02-18 17:08:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.4695017635822296 norm:0.0013468563556671143 max memory_allocated 22562.41357421875 
[2025-02-18 17:08:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 17:09:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.5374543070793152 norm:0.01854155957698822 max memory_allocated 22562.58544921875 
[2025-02-18 17:10:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.5101447105407715 norm:0.008515230379998684 max memory_allocated 22562.58544921875 
[2025-02-18 17:10:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.4968932867050171 norm:0.0045504882000386715 max memory_allocated 22562.58544921875 
[2025-02-18 17:11:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.4931603670120239 norm:0.0032503127586096525 max memory_allocated 22562.58544921875 
[2025-02-18 17:11:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.49151983857154846 norm:0.0025884935166686773 max memory_allocated 22562.58544921875 
[2025-02-18 17:12:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.48960450291633606 norm:0.002173012588173151 max memory_allocated 22562.58544921875 
[2025-02-18 17:12:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.4892334043979645 norm:0.0020583639852702618 max memory_allocated 22562.58544921875 
[2025-02-18 17:13:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.48638877272605896 norm:0.0017648751381784678 max memory_allocated 22562.58544921875 
[2025-02-18 17:13:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.4850812554359436 norm:0.0016323489835485816 max memory_allocated 22562.58544921875 
[2025-02-18 17:14:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.4856603741645813 norm:0.0015945483464747667 max memory_allocated 22562.58544921875 
[2025-02-18 17:15:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.48692432045936584 norm:0.0015967964427545667 max memory_allocated 22562.58544921875 
[2025-02-18 17:15:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.4861324727535248 norm:0.001480063539929688 max memory_allocated 22562.58544921875 
[2025-02-18 17:16:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.48731109499931335 norm:0.0015179095789790154 max memory_allocated 22562.58544921875 
[2025-02-18 17:16:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.48766645789146423 norm:0.0015188443940132856 max memory_allocated 22562.58544921875 
[2025-02-18 17:17:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.4883285164833069 norm:0.0015288506401702762 max memory_allocated 22562.58544921875 
[2025-02-18 17:17:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.4894771873950958 norm:0.0015777493827044964 max memory_allocated 22562.58544921875 
[2025-02-18 17:18:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.4892697036266327 norm:0.0015393180074170232 max memory_allocated 22562.58544921875 
[2025-02-18 17:18:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.4886957108974457 norm:0.0014752061106264591 max memory_allocated 22562.58544921875 
[2025-02-18 17:19:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.487403929233551 norm:0.0014209337532520294 max memory_allocated 22562.58544921875 
[2025-02-18 17:19:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.4880698025226593 norm:0.001413076650351286 max memory_allocated 22562.58544921875 
[2025-02-18 17:20:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 17:20:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.5368257761001587 norm:0.010157745331525803 max memory_allocated 22562.75732421875 
[2025-02-18 17:21:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.5174261331558228 norm:0.005039171781390905 max memory_allocated 22562.75732421875 
[2025-02-18 17:21:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.5095648765563965 norm:0.0030067365150898695 max memory_allocated 22562.75732421875 
[2025-02-18 17:22:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.5058295726776123 norm:0.002123418264091015 max memory_allocated 22562.75732421875 
[2025-02-18 17:22:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.5045866966247559 norm:0.0017356687458232045 max memory_allocated 22562.75732421875 
[2025-02-18 17:23:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.5028789639472961 norm:0.001474392251111567 max memory_allocated 22562.75732421875 
[2025-02-18 17:24:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.5032379031181335 norm:0.0014561355346813798 max memory_allocated 22562.75732421875 
[2025-02-18 17:24:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.5046918988227844 norm:0.0014721342595294118 max memory_allocated 22562.75732421875 
[2025-02-18 17:25:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.5024391412734985 norm:0.0013752869563177228 max memory_allocated 22562.75732421875 
[2025-02-18 17:25:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.5010703206062317 norm:0.0013212300837039948 max memory_allocated 22562.75732421875 
[2025-02-18 17:26:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.5011062026023865 norm:0.0012862356379628181 max memory_allocated 22562.75732421875 
[2025-02-18 17:26:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.5017824172973633 norm:0.001307026599533856 max memory_allocated 22562.75732421875 
[2025-02-18 17:27:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.501427173614502 norm:0.0012840344570577145 max memory_allocated 22562.75732421875 
[2025-02-18 17:27:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.5026435852050781 norm:0.0013030162081122398 max memory_allocated 22562.75732421875 
[2025-02-18 17:28:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.503780722618103 norm:0.0013382985489442945 max memory_allocated 22562.75732421875 
[2025-02-18 17:29:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.5035742521286011 norm:0.0013347403146326542 max memory_allocated 22562.75732421875 
[2025-02-18 17:29:34 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.5041385889053345 norm:0.0013487666146829724 max memory_allocated 22562.75732421875 
[2025-02-18 17:30:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.5050117373466492 norm:0.0013759734574705362 max memory_allocated 22562.75732421875 
[2025-02-18 17:30:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.5052707195281982 norm:0.0013798584695905447 max memory_allocated 22562.75732421875 
[2025-02-18 17:31:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.5053241848945618 norm:0.0013947529951110482 max memory_allocated 22562.75732421875 
[2025-02-18 17:31:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 17:31:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.5824425220489502 norm:0.04506140947341919 max memory_allocated 22562.92919921875 
[2025-02-18 17:32:32 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.5437564849853516 norm:0.01951465755701065 max memory_allocated 22562.92919921875 
[2025-02-18 17:33:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.5310232043266296 norm:0.011973902583122253 max memory_allocated 22562.92919921875 
[2025-02-18 17:33:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.5289991497993469 norm:0.008785044774413109 max memory_allocated 22562.92919921875 
[2025-02-18 17:34:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.526295006275177 norm:0.0059309848584234715 max memory_allocated 22562.92919921875 
[2025-02-18 17:34:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.5220983028411865 norm:0.0044134389609098434 max memory_allocated 22562.92919921875 
[2025-02-18 17:35:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.5188663005828857 norm:0.0035643763840198517 max memory_allocated 22562.92919921875 
[2025-02-18 17:35:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.5161836743354797 norm:0.0031019095331430435 max memory_allocated 22562.92919921875 
[2025-02-18 17:36:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.5154680609703064 norm:0.0028236880898475647 max memory_allocated 22562.92919921875 
[2025-02-18 17:36:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.5152334570884705 norm:0.0026280118618160486 max memory_allocated 22562.92919921875 
[2025-02-18 17:37:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.514108419418335 norm:0.0024193814024329185 max memory_allocated 22562.92919921875 
[2025-02-18 17:38:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.5131411552429199 norm:0.002311437390744686 max memory_allocated 22562.92919921875 
[2025-02-18 17:38:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.5131746530532837 norm:0.0021922816522419453 max memory_allocated 22562.92919921875 
[2025-02-18 17:39:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.5131546258926392 norm:0.002087946981191635 max memory_allocated 22562.92919921875 
[2025-02-18 17:39:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.5112654566764832 norm:0.002019331092014909 max memory_allocated 22562.92919921875 
[2025-02-18 17:40:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.5121884346008301 norm:0.002015487989410758 max memory_allocated 22562.92919921875 
[2025-02-18 17:40:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.5132213234901428 norm:0.0020637239795178175 max memory_allocated 22562.92919921875 
[2025-02-18 17:41:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.5128400921821594 norm:0.0020544647704809904 max memory_allocated 22562.92919921875 
[2025-02-18 17:41:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.5127536654472351 norm:0.001999231521040201 max memory_allocated 22562.92919921875 
[2025-02-18 17:42:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.5129184126853943 norm:0.001964214025065303 max memory_allocated 22562.92919921875 
[2025-02-18 17:42:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 17:43:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.5501037836074829 norm:0.013308169320225716 max memory_allocated 22563.10107421875 
[2025-02-18 17:43:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.5290650725364685 norm:0.006692853756248951 max memory_allocated 22563.10107421875 
[2025-02-18 17:44:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.5219818949699402 norm:0.004427923355251551 max memory_allocated 22563.10107421875 
[2025-02-18 17:44:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.5164825916290283 norm:0.002779088681563735 max memory_allocated 22563.10107421875 
[2025-02-18 17:45:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.5143098831176758 norm:0.0021961857564747334 max memory_allocated 22563.10107421875 
[2025-02-18 17:45:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.5139471888542175 norm:0.001962770242244005 max memory_allocated 22563.10107421875 
[2025-02-18 17:46:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.5136895179748535 norm:0.00182317104190588 max memory_allocated 22563.10107421875 
[2025-02-18 17:47:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.5125928521156311 norm:0.0016823704354465008 max memory_allocated 22563.10107421875 
[2025-02-18 17:47:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.5113635063171387 norm:0.001537830801680684 max memory_allocated 22563.10107421875 
[2025-02-18 17:48:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.5104686617851257 norm:0.0015506112249568105 max memory_allocated 22563.10107421875 
[2025-02-18 17:48:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.5110624432563782 norm:0.0014840265503153205 max memory_allocated 22563.10107421875 
[2025-02-18 17:49:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.5128716230392456 norm:0.0015063248574733734 max memory_allocated 22563.10107421875 
[2025-02-18 17:49:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.5133002996444702 norm:0.0015668629202991724 max memory_allocated 22563.10107421875 
[2025-02-18 17:50:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.5131762623786926 norm:0.0016123930690810084 max memory_allocated 22563.10107421875 
[2025-02-18 17:50:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.5139345526695251 norm:0.001640400616452098 max memory_allocated 22563.10107421875 
[2025-02-18 17:51:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.5135315656661987 norm:0.001602704869583249 max memory_allocated 22563.10107421875 
[2025-02-18 17:52:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.5140601396560669 norm:0.0016055983724072576 max memory_allocated 22563.10107421875 
[2025-02-18 17:52:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.5146655440330505 norm:0.001618837472051382 max memory_allocated 22563.10107421875 
[2025-02-18 17:53:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.5136035680770874 norm:0.0016069563571363688 max memory_allocated 22563.10107421875 
[2025-02-18 17:53:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.5135127305984497 norm:0.0015973836416378617 max memory_allocated 22563.10107421875 
[2025-02-18 17:53:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 17:54:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.5831973552703857 norm:0.031432900577783585 max memory_allocated 22563.27294921875 
[2025-02-18 17:55:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.5546934604644775 norm:0.015220267698168755 max memory_allocated 22563.27294921875 
[2025-02-18 17:55:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.5405195951461792 norm:0.00912218727171421 max memory_allocated 22563.27294921875 
[2025-02-18 17:56:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.5334298014640808 norm:0.005901908967643976 max memory_allocated 22563.27294921875 
[2025-02-18 17:56:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.5312595367431641 norm:0.004585924558341503 max memory_allocated 22563.27294921875 
[2025-02-18 17:57:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.5271872282028198 norm:0.003576018614694476 max memory_allocated 22563.27294921875 
[2025-02-18 17:57:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.5257050395011902 norm:0.0029971746262162924 max memory_allocated 22563.27294921875 
[2025-02-18 17:58:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.5247175097465515 norm:0.002546468051150441 max memory_allocated 22563.27294921875 
[2025-02-18 17:58:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.5240846872329712 norm:0.002286692149937153 max memory_allocated 22563.27294921875 
[2025-02-18 17:59:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.5231614112854004 norm:0.002161462791264057 max memory_allocated 22563.27294921875 
[2025-02-18 17:59:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.5216731429100037 norm:0.0020016583148390055 max memory_allocated 22563.27294921875 
[2025-02-18 18:00:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.5204636454582214 norm:0.001865028403699398 max memory_allocated 22563.27294921875 
[2025-02-18 18:01:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.5203492045402527 norm:0.0017670353408902884 max memory_allocated 22563.27294921875 
[2025-02-18 18:01:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.5199571847915649 norm:0.0016809386434033513 max memory_allocated 22563.27294921875 
[2025-02-18 18:02:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.5193822383880615 norm:0.0016227196902036667 max memory_allocated 22563.27294921875 
[2025-02-18 18:02:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.5190422534942627 norm:0.0015585763612762094 max memory_allocated 22563.27294921875 
[2025-02-18 18:03:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.5193745493888855 norm:0.0015440636780112982 max memory_allocated 22563.27294921875 
[2025-02-18 18:03:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.5187668204307556 norm:0.0014973318902775645 max memory_allocated 22563.27294921875 
[2025-02-18 18:04:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.5179634094238281 norm:0.001451576128602028 max memory_allocated 22563.27294921875 
[2025-02-18 18:04:57 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.5192097425460815 norm:0.0014835868496447802 max memory_allocated 22563.27294921875 
[2025-02-18 18:05:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 18:05:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.5927391052246094 norm:0.03363475948572159 max memory_allocated 22563.44482421875 
[2025-02-18 18:06:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.5670036673545837 norm:0.017684470862150192 max memory_allocated 22563.44482421875 
[2025-02-18 18:06:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.5506502389907837 norm:0.010215339250862598 max memory_allocated 22563.44482421875 
[2025-02-18 18:07:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.5399399995803833 norm:0.006520469672977924 max memory_allocated 22563.44482421875 
[2025-02-18 18:07:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.5348966121673584 norm:0.004783140029758215 max memory_allocated 22563.44482421875 
[2025-02-18 18:08:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.5321352481842041 norm:0.003826070809736848 max memory_allocated 22563.44482421875 
[2025-02-18 18:09:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.5298095941543579 norm:0.0030993344262242317 max memory_allocated 22563.44482421875 
[2025-02-18 18:09:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.5290755033493042 norm:0.002745040925219655 max memory_allocated 22563.44482421875 
[2025-02-18 18:10:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.5264953374862671 norm:0.002395742340013385 max memory_allocated 22563.44482421875 
[2025-02-18 18:10:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.5257869362831116 norm:0.0022136226762086153 max memory_allocated 22563.44482421875 
[2025-02-18 18:11:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.5255837440490723 norm:0.002079736441373825 max memory_allocated 22563.44482421875 
[2025-02-18 18:11:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.5253200531005859 norm:0.0019639157690107822 max memory_allocated 22563.44482421875 
[2025-02-18 18:12:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.5257455110549927 norm:0.0019290292402729392 max memory_allocated 22563.44482421875 
[2025-02-18 18:12:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.5250112414360046 norm:0.0018364028073847294 max memory_allocated 22563.44482421875 
[2025-02-18 18:13:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.5246882438659668 norm:0.0017556289676576853 max memory_allocated 22563.44482421875 
[2025-02-18 18:13:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.5249095559120178 norm:0.0017444577533751726 max memory_allocated 22563.44482421875 
[2025-02-18 18:14:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.5263032913208008 norm:0.0017634115647524595 max memory_allocated 22563.44482421875 
[2025-02-18 18:15:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.5268741846084595 norm:0.0017312249401584268 max memory_allocated 22563.44482421875 
[2025-02-18 18:15:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.5266666412353516 norm:0.0016806266503408551 max memory_allocated 22563.44482421875 
[2025-02-18 18:16:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.5269619822502136 norm:0.0016719523118808866 max memory_allocated 22563.44482421875 
[2025-02-18 18:16:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 18:16:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.590510904788971 norm:0.025358032435178757 max memory_allocated 22563.61669921875 
[2025-02-18 18:17:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.5680379867553711 norm:0.013567207381129265 max memory_allocated 22563.61669921875 
[2025-02-18 18:18:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.5599735379219055 norm:0.009137102402746677 max memory_allocated 22563.61669921875 
[2025-02-18 18:18:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.5532381534576416 norm:0.005972799379378557 max memory_allocated 22563.61669921875 
[2025-02-18 18:19:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.5501437783241272 norm:0.004472491331398487 max memory_allocated 22563.61669921875 
[2025-02-18 18:19:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.5491613149642944 norm:0.003695848397910595 max memory_allocated 22563.61669921875 
[2025-02-18 18:20:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.5482789278030396 norm:0.003143750596791506 max memory_allocated 22563.61669921875 
[2025-02-18 18:20:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.5489106178283691 norm:0.0026208425406366587 max memory_allocated 22563.61669921875 
[2025-02-18 18:21:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.54774409532547 norm:0.0023022573441267014 max memory_allocated 22563.61669921875 
[2025-02-18 18:21:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.5469916462898254 norm:0.0022231589537113905 max memory_allocated 22563.61669921875 
[2025-02-18 18:22:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.5473071336746216 norm:0.0023245965130627155 max memory_allocated 22563.61669921875 
[2025-02-18 18:23:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.5457058548927307 norm:0.0026419470086693764 max memory_allocated 22563.61669921875 
[2025-02-18 18:23:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.5446814298629761 norm:0.0031174279283732176 max memory_allocated 22563.61669921875 
[2025-02-18 18:24:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.5441835522651672 norm:0.0032286844216287136 max memory_allocated 22563.61669921875 
[2025-02-18 18:24:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.5446969270706177 norm:0.0036203942727297544 max memory_allocated 22563.61669921875 
[2025-02-18 18:25:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.5442101359367371 norm:0.0037368987686932087 max memory_allocated 22563.61669921875 
[2025-02-18 18:25:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.5433170795440674 norm:0.0033003208227455616 max memory_allocated 22563.61669921875 
[2025-02-18 18:26:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.5422964096069336 norm:0.003245048923417926 max memory_allocated 22563.61669921875 
[2025-02-18 18:26:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.5420467257499695 norm:0.00312773953191936 max memory_allocated 22563.61669921875 
[2025-02-18 18:27:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.5420036315917969 norm:0.0030662284698337317 max memory_allocated 22563.61669921875 
[2025-02-18 18:27:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 18:28:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.6533060073852539 norm:0.05249498412013054 max memory_allocated 22563.78857421875 
[2025-02-18 18:28:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.6270599365234375 norm:0.025806933641433716 max memory_allocated 22563.78857421875 
[2025-02-18 18:29:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.6146968603134155 norm:0.014803119003772736 max memory_allocated 22563.78857421875 
[2025-02-18 18:29:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.608664333820343 norm:0.010469349101185799 max memory_allocated 22563.78857421875 
[2025-02-18 18:30:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.6133788824081421 norm:0.008350369520485401 max memory_allocated 22563.78857421875 
[2025-02-18 18:30:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.6159138679504395 norm:0.006681668106466532 max memory_allocated 22563.78857421875 
[2025-02-18 18:31:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.6207036375999451 norm:0.005829342640936375 max memory_allocated 22563.78857421875 
[2025-02-18 18:32:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.6216750144958496 norm:0.004971161484718323 max memory_allocated 22563.78857421875 
[2025-02-18 18:32:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.6191271543502808 norm:0.004607002250850201 max memory_allocated 22563.78857421875 
[2025-02-18 18:33:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.6173035502433777 norm:0.004493390675634146 max memory_allocated 22563.78857421875 
[2025-02-18 18:33:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.6160618662834167 norm:0.004133824724704027 max memory_allocated 22563.78857421875 
[2025-02-18 18:34:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.6153559684753418 norm:0.003999773412942886 max memory_allocated 22563.78857421875 
[2025-02-18 18:34:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.6155864596366882 norm:0.003909497521817684 max memory_allocated 22563.78857421875 
[2025-02-18 18:35:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.6159034371376038 norm:0.003919739276170731 max memory_allocated 22563.78857421875 
[2025-02-18 18:35:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.614891529083252 norm:0.0039134398102760315 max memory_allocated 22563.78857421875 
[2025-02-18 18:36:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.6147504448890686 norm:0.003835541196167469 max memory_allocated 22563.78857421875 
[2025-02-18 18:36:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.6131840944290161 norm:0.0037291226908564568 max memory_allocated 22563.78857421875 
[2025-02-18 18:37:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.613657534122467 norm:0.00369437993504107 max memory_allocated 22563.78857421875 
[2025-02-18 18:38:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.6133020520210266 norm:0.003698508720844984 max memory_allocated 22563.78857421875 
[2025-02-18 18:38:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.6130405068397522 norm:0.00368515239097178 max memory_allocated 22563.78857421875 
[2025-02-18 18:38:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 18:39:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.6835447549819946 norm:0.030346659943461418 max memory_allocated 22563.96044921875 
[2025-02-18 18:39:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.6673532128334045 norm:0.016775280237197876 max memory_allocated 22563.96044921875 
[2025-02-18 18:40:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.6646719574928284 norm:0.012320799753069878 max memory_allocated 22563.96044921875 
[2025-02-18 18:41:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.6617653369903564 norm:0.009290945716202259 max memory_allocated 22563.96044921875 
[2025-02-18 18:41:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.6606267094612122 norm:0.006492093205451965 max memory_allocated 22563.96044921875 
[2025-02-18 18:42:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.6610467433929443 norm:0.005023358389735222 max memory_allocated 22563.96044921875 
[2025-02-18 18:42:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.6602489352226257 norm:0.00392188923433423 max memory_allocated 22563.96044921875 
[2025-02-18 18:43:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.6572839617729187 norm:0.0032569565810263157 max memory_allocated 22563.96044921875 
[2025-02-18 18:43:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.6584540009498596 norm:0.0030849731992930174 max memory_allocated 22563.96044921875 
[2025-02-18 18:44:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.6570050716400146 norm:0.0028778649866580963 max memory_allocated 22563.96044921875 
[2025-02-18 18:44:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.6553934812545776 norm:0.002695046365261078 max memory_allocated 22563.96044921875 
[2025-02-18 18:45:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.6541668176651001 norm:0.0025623333640396595 max memory_allocated 22563.96044921875 
[2025-02-18 18:46:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.6531664133071899 norm:0.002511094557121396 max memory_allocated 22563.96044921875 
[2025-02-18 18:46:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.6522639989852905 norm:0.0024415371008217335 max memory_allocated 22563.96044921875 
[2025-02-18 18:47:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.651026725769043 norm:0.002392388880252838 max memory_allocated 22563.96044921875 
[2025-02-18 18:47:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.6500909924507141 norm:0.0023447745479643345 max memory_allocated 22563.96044921875 
[2025-02-18 18:48:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.650059163570404 norm:0.002314177341759205 max memory_allocated 22563.96044921875 
[2025-02-18 18:48:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.649921715259552 norm:0.002283938694745302 max memory_allocated 22563.96044921875 
[2025-02-18 18:49:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.6502801179885864 norm:0.002330625895410776 max memory_allocated 22563.96044921875 
[2025-02-18 18:49:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.6506745219230652 norm:0.0023564642760902643 max memory_allocated 22563.96044921875 
[2025-02-18 18:50:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 18:50:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.7382315397262573 norm:0.02602681890130043 max memory_allocated 22564.13232421875 
[2025-02-18 18:51:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.7236114740371704 norm:0.015787068754434586 max memory_allocated 22564.13232421875 
[2025-02-18 18:51:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.7137066125869751 norm:0.01007234025746584 max memory_allocated 22564.13232421875 
[2025-02-18 18:52:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.7116241455078125 norm:0.00781918317079544 max memory_allocated 22564.13232421875 
[2025-02-18 18:52:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.7090219855308533 norm:0.006289210170507431 max memory_allocated 22564.13232421875 
[2025-02-18 18:53:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.708442211151123 norm:0.0052440473809838295 max memory_allocated 22564.13232421875 
[2025-02-18 18:53:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.7074242830276489 norm:0.004350641276687384 max memory_allocated 22564.13232421875 
[2025-02-18 18:54:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.7054392695426941 norm:0.003455519210547209 max memory_allocated 22564.13232421875 
[2025-02-18 18:55:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.7055639028549194 norm:0.0030033565126359463 max memory_allocated 22564.13232421875 
[2025-02-18 18:55:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.703304648399353 norm:0.002624767366796732 max memory_allocated 22564.13232421875 
[2025-02-18 18:56:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.7023617625236511 norm:0.0024049989879131317 max memory_allocated 22564.13232421875 
[2025-02-18 18:56:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.7025977373123169 norm:0.0022497735917568207 max memory_allocated 22564.13232421875 
[2025-02-18 18:57:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.7022450566291809 norm:0.0021388991735875607 max memory_allocated 22564.13232421875 
[2025-02-18 18:57:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.7010586261749268 norm:0.002033054828643799 max memory_allocated 22564.13232421875 
[2025-02-18 18:58:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.7005143165588379 norm:0.0019487817771732807 max memory_allocated 22564.13232421875 
[2025-02-18 18:58:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.7004294395446777 norm:0.0018934507388621569 max memory_allocated 22564.13232421875 
[2025-02-18 18:59:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.7000600099563599 norm:0.001843924866989255 max memory_allocated 22564.13232421875 
[2025-02-18 19:00:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.6996631622314453 norm:0.0017965675797313452 max memory_allocated 22564.13232421875 
[2025-02-18 19:00:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.6989530920982361 norm:0.001752863870933652 max memory_allocated 22564.13232421875 
[2025-02-18 19:01:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.6990693807601929 norm:0.0017435868503525853 max memory_allocated 22564.13232421875 
[2025-02-18 19:01:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 19:01:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.7914162874221802 norm:0.022713150829076767 max memory_allocated 22564.30419921875 
[2025-02-18 19:02:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.774227499961853 norm:0.01101367175579071 max memory_allocated 22564.30419921875 
[2025-02-18 19:02:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.7662200927734375 norm:0.007104441057890654 max memory_allocated 22564.30419921875 
[2025-02-18 19:03:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.7623542547225952 norm:0.005358900874853134 max memory_allocated 22564.30419921875 
[2025-02-18 19:04:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.7600598931312561 norm:0.004063091706484556 max memory_allocated 22564.30419921875 
[2025-02-18 19:04:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.7591390609741211 norm:0.0031114020384848118 max memory_allocated 22564.30419921875 
[2025-02-18 19:05:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.7571688890457153 norm:0.00243014027364552 max memory_allocated 22564.30419921875 
[2025-02-18 19:05:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.7568193078041077 norm:0.002068402711302042 max memory_allocated 22564.30419921875 
[2025-02-18 19:06:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.7559284567832947 norm:0.0017697247676551342 max memory_allocated 22564.30419921875 
[2025-02-18 19:06:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.7552206516265869 norm:0.0015578994061797857 max memory_allocated 22564.30419921875 
[2025-02-18 19:07:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.755244255065918 norm:0.001450960524380207 max memory_allocated 22564.30419921875 
[2025-02-18 19:07:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.7552834749221802 norm:0.00136945687700063 max memory_allocated 22564.30419921875 
[2025-02-18 19:08:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.7545892000198364 norm:0.001307281432673335 max memory_allocated 22564.30419921875 
[2025-02-18 19:09:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.7545647621154785 norm:0.0013069553533568978 max memory_allocated 22564.30419921875 
[2025-02-18 19:09:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.7550157308578491 norm:0.0012997642625123262 max memory_allocated 22564.30419921875 
[2025-02-18 19:10:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.7542893886566162 norm:0.001272861147299409 max memory_allocated 22564.30419921875 
[2025-02-18 19:10:43 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.7540607452392578 norm:0.001263312646187842 max memory_allocated 22564.30419921875 
[2025-02-18 19:11:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.7534360289573669 norm:0.0012479971628636122 max memory_allocated 22564.30419921875 
[2025-02-18 19:11:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.7534813284873962 norm:0.0012481939047574997 max memory_allocated 22564.30419921875 
[2025-02-18 19:12:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.7535361647605896 norm:0.001262238365598023 max memory_allocated 22564.30419921875 
[2025-02-18 19:12:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 19:13:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.8827205896377563 norm:0.025871098041534424 max memory_allocated 22564.47607421875 
[2025-02-18 19:13:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.8637955188751221 norm:0.01338566280901432 max memory_allocated 22564.47607421875 
[2025-02-18 19:14:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.859636664390564 norm:0.009711806662380695 max memory_allocated 22564.47607421875 
[2025-02-18 19:14:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.858493447303772 norm:0.007421411573886871 max memory_allocated 22564.47607421875 
[2025-02-18 19:15:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.8586042523384094 norm:0.005305137950927019 max memory_allocated 22564.47607421875 
[2025-02-18 19:15:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.8562343716621399 norm:0.0036904546432197094 max memory_allocated 22564.47607421875 
[2025-02-18 19:16:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.8527573347091675 norm:0.002742322627454996 max memory_allocated 22564.47607421875 
[2025-02-18 19:17:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.8529728651046753 norm:0.002488703466951847 max memory_allocated 22564.47607421875 
[2025-02-18 19:17:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.853536069393158 norm:0.002413482405245304 max memory_allocated 22564.47607421875 
[2025-02-18 19:18:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.853374183177948 norm:0.0023878165520727634 max memory_allocated 22564.47607421875 
[2025-02-18 19:18:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.8526800274848938 norm:0.002370016649365425 max memory_allocated 22564.47607421875 
[2025-02-18 19:19:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.8524435758590698 norm:0.0024102714378386736 max memory_allocated 22564.47607421875 
[2025-02-18 19:19:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.8522010445594788 norm:0.0023929160088300705 max memory_allocated 22564.47607421875 
[2025-02-18 19:20:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.8518401980400085 norm:0.0024156260769814253 max memory_allocated 22564.47607421875 
[2025-02-18 19:20:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.8516124486923218 norm:0.0024277593474835157 max memory_allocated 22564.47607421875 
[2025-02-18 19:21:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.8523482084274292 norm:0.002438370604068041 max memory_allocated 22564.47607421875 
[2025-02-18 19:21:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.8520757555961609 norm:0.002465060679242015 max memory_allocated 22564.47607421875 
[2025-02-18 19:22:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.8518888354301453 norm:0.0024804219137877226 max memory_allocated 22564.47607421875 
[2025-02-18 19:23:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.8515251874923706 norm:0.002476714551448822 max memory_allocated 22564.47607421875 
[2025-02-18 19:23:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.8516844511032104 norm:0.0024765790440142155 max memory_allocated 22564.47607421875 
[2025-02-18 19:23:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 19:24:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.9664027094841003 norm:0.014250269159674644 max memory_allocated 22564.64794921875 
[2025-02-18 19:24:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.9559999704360962 norm:0.007874679751694202 max memory_allocated 22564.64794921875 
[2025-02-18 19:25:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.9536531567573547 norm:0.005634706001728773 max memory_allocated 22564.64794921875 
[2025-02-18 19:26:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.9507147073745728 norm:0.003861982375383377 max memory_allocated 22564.64794921875 
[2025-02-18 19:26:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.9487316608428955 norm:0.0026292894035577774 max memory_allocated 22564.64794921875 
[2025-02-18 19:27:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.9474718570709229 norm:0.0019185784040018916 max memory_allocated 22564.64794921875 
[2025-02-18 19:27:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.9466647505760193 norm:0.0016297713154926896 max memory_allocated 22564.64794921875 
[2025-02-18 19:28:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.9453707933425903 norm:0.0014575724489986897 max memory_allocated 22564.64794921875 
[2025-02-18 19:28:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.9449275135993958 norm:0.0013740462018176913 max memory_allocated 22564.64794921875 
[2025-02-18 19:29:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.9449590444564819 norm:0.0013001972110942006 max memory_allocated 22564.64794921875 
[2025-02-18 19:29:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.9445429444313049 norm:0.0012370952172204852 max memory_allocated 22564.64794921875 
[2025-02-18 19:30:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.9447094202041626 norm:0.001234771334566176 max memory_allocated 22564.64794921875 
[2025-02-18 19:31:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.9445739984512329 norm:0.0011877536308020353 max memory_allocated 22564.64794921875 
[2025-02-18 19:31:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.9442553520202637 norm:0.0011731430422514677 max memory_allocated 22564.64794921875 
[2025-02-18 19:32:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.9436814785003662 norm:0.0011516809463500977 max memory_allocated 22564.64794921875 
[2025-02-18 19:32:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.9436356425285339 norm:0.0011461073299869895 max memory_allocated 22564.64794921875 
[2025-02-18 19:33:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.9436689615249634 norm:0.0011439002119004726 max memory_allocated 22564.64794921875 
[2025-02-18 19:33:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.9436571002006531 norm:0.0011400077491998672 max memory_allocated 22564.64794921875 
[2025-02-18 19:34:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.9436164498329163 norm:0.001151817967183888 max memory_allocated 22564.64794921875 
[2025-02-18 19:34:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.9434564709663391 norm:0.0011502644047141075 max memory_allocated 22564.64794921875 
[2025-02-18 19:35:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 19:35:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:1.0901446342468262 norm:0.01928282156586647 max memory_allocated 22564.81982421875 
[2025-02-18 19:36:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:1.072685718536377 norm:0.009292008355259895 max memory_allocated 22564.81982421875 
[2025-02-18 19:36:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:1.067468523979187 norm:0.006018267944455147 max memory_allocated 22564.81982421875 
[2025-02-18 19:37:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:1.0658096075057983 norm:0.004404154140502214 max memory_allocated 22564.81982421875 
[2025-02-18 19:37:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:1.0649147033691406 norm:0.003509165719151497 max memory_allocated 22564.81982421875 
[2025-02-18 19:38:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:1.063888669013977 norm:0.0028434162959456444 max memory_allocated 22564.81982421875 
[2025-02-18 19:38:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:1.062052607536316 norm:0.002286783652380109 max memory_allocated 22564.81982421875 
[2025-02-18 19:39:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:1.0614032745361328 norm:0.0020542843267321587 max memory_allocated 22564.81982421875 
[2025-02-18 19:40:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:1.0610332489013672 norm:0.0018974121194332838 max memory_allocated 22564.81982421875 
[2025-02-18 19:40:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:1.0577062368392944 norm:0.0017509341705590487 max memory_allocated 22564.81982421875 
[2025-02-18 19:41:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:1.0569156408309937 norm:0.001645898213610053 max memory_allocated 22564.81982421875 
[2025-02-18 19:41:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:1.0571705102920532 norm:0.001588275539688766 max memory_allocated 22564.81982421875 
[2025-02-18 19:42:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:1.056522250175476 norm:0.0015137087320908904 max memory_allocated 22564.81982421875 
[2025-02-18 19:42:49 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:1.0563833713531494 norm:0.0014380875509232283 max memory_allocated 22564.81982421875 
[2025-02-18 19:43:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:1.0559468269348145 norm:0.0013776390114799142 max memory_allocated 22564.81982421875 
[2025-02-18 19:43:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:1.055665373802185 norm:0.0013419629540294409 max memory_allocated 22564.81982421875 
[2025-02-18 19:44:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:1.0556663274765015 norm:0.0013305610045790672 max memory_allocated 22564.81982421875 
[2025-02-18 19:45:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:1.055572271347046 norm:0.0013190102763473988 max memory_allocated 22564.81982421875 
[2025-02-18 19:45:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:1.0555170774459839 norm:0.0012920935405418277 max memory_allocated 22564.81982421875 
[2025-02-18 19:46:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:1.0552256107330322 norm:0.0012859845301136374 max memory_allocated 22564.81982421875 
[2025-02-18 19:46:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 19:46:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:1.2325271368026733 norm:0.030603885650634766 max memory_allocated 22564.99169921875 
[2025-02-18 19:47:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:1.2224459648132324 norm:0.020186565816402435 max memory_allocated 22564.99169921875 
[2025-02-18 19:47:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:1.2217687368392944 norm:0.015636151656508446 max memory_allocated 22564.99169921875 
[2025-02-18 19:48:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:1.2213964462280273 norm:0.011928518302738667 max memory_allocated 22564.99169921875 
[2025-02-18 19:49:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:1.2225029468536377 norm:0.009563236497342587 max memory_allocated 22564.99169921875 
[2025-02-18 19:49:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:1.2221683263778687 norm:0.007899201475083828 max memory_allocated 22564.99169921875 
[2025-02-18 19:50:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:1.2207939624786377 norm:0.006726567167788744 max memory_allocated 22564.99169921875 
[2025-02-18 19:50:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:1.2203270196914673 norm:0.005863346625119448 max memory_allocated 22564.99169921875 
[2025-02-18 19:51:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:1.2190444469451904 norm:0.005171231925487518 max memory_allocated 22564.99169921875 
[2025-02-18 19:51:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:1.2176332473754883 norm:0.004649695940315723 max memory_allocated 22564.99169921875 
[2025-02-18 19:52:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:1.2170569896697998 norm:0.0043268874287605286 max memory_allocated 22564.99169921875 
[2025-02-18 19:52:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:1.2158024311065674 norm:0.003985388670116663 max memory_allocated 22564.99169921875 
[2025-02-18 19:53:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:1.2142914533615112 norm:0.003670020494610071 max memory_allocated 22564.99169921875 
[2025-02-18 19:54:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:1.2130563259124756 norm:0.0034509925171732903 max memory_allocated 22564.99169921875 
[2025-02-18 19:54:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:1.2114148139953613 norm:0.0032392956782132387 max memory_allocated 22564.99169921875 
[2025-02-18 19:55:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:1.2103066444396973 norm:0.0030766241252422333 max memory_allocated 22564.99169921875 
[2025-02-18 19:55:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:1.2089369297027588 norm:0.002922856481745839 max memory_allocated 22564.99169921875 
[2025-02-18 19:56:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:1.208277940750122 norm:0.0028086029924452305 max memory_allocated 22564.99169921875 
[2025-02-18 19:56:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:1.207676887512207 norm:0.002684570848941803 max memory_allocated 22564.99169921875 
[2025-02-18 19:57:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:1.2067348957061768 norm:0.002591339871287346 max memory_allocated 22564.99169921875 
[2025-02-18 19:57:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 19:58:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:1.3924713134765625 norm:0.014931789599359035 max memory_allocated 22565.16357421875 
[2025-02-18 19:58:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:1.3800928592681885 norm:0.009114007465541363 max memory_allocated 22565.16357421875 
[2025-02-18 19:59:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:1.371756672859192 norm:0.006279467139393091 max memory_allocated 22565.16357421875 
[2025-02-18 19:59:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:1.3655775785446167 norm:0.004489933140575886 max memory_allocated 22565.16357421875 
[2025-02-18 20:00:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:1.363049864768982 norm:0.0034102299250662327 max memory_allocated 22565.16357421875 
[2025-02-18 20:00:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:1.3635790348052979 norm:0.002786985831335187 max memory_allocated 22565.16357421875 
[2025-02-18 20:01:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:1.3643381595611572 norm:0.0025198496878147125 max memory_allocated 22565.16357421875 
[2025-02-18 20:01:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:1.3631558418273926 norm:0.0022523982916027308 max memory_allocated 22565.16357421875 
[2025-02-18 20:02:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:1.3639163970947266 norm:0.0020337975583970547 max memory_allocated 22565.16357421875 
[2025-02-18 20:03:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:1.3656892776489258 norm:0.0020054380875080824 max memory_allocated 22565.16357421875 
[2025-02-18 20:03:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:1.366481065750122 norm:0.0020513287745416164 max memory_allocated 22565.16357421875 
[2025-02-18 20:04:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:1.3672759532928467 norm:0.0020778796169906855 max memory_allocated 22565.16357421875 
[2025-02-18 20:04:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:1.3665751218795776 norm:0.0020851115696132183 max memory_allocated 22565.16357421875 
[2025-02-18 20:05:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:1.3656046390533447 norm:0.0020761245395988226 max memory_allocated 22565.16357421875 
[2025-02-18 20:05:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:1.3648855686187744 norm:0.002125826897099614 max memory_allocated 22565.16357421875 
[2025-02-18 20:06:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:1.3653491735458374 norm:0.0021079801954329014 max memory_allocated 22565.16357421875 
[2025-02-18 20:06:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:1.3656282424926758 norm:0.00215735100209713 max memory_allocated 22565.16357421875 
[2025-02-18 20:07:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:1.3665663003921509 norm:0.0022938421461731195 max memory_allocated 22565.16357421875 
[2025-02-18 20:08:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:1.3675024509429932 norm:0.0023724245838820934 max memory_allocated 22565.16357421875 
[2025-02-18 20:08:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:1.3679046630859375 norm:0.002489627804607153 max memory_allocated 22565.16357421875 
[2025-02-18 20:08:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 20:09:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:1.5967622995376587 norm:0.028092944994568825 max memory_allocated 22565.33544921875 
[2025-02-18 20:09:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:1.5778930187225342 norm:0.015906628221273422 max memory_allocated 22565.33544921875 
[2025-02-18 20:10:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:1.5691983699798584 norm:0.011493562720716 max memory_allocated 22565.33544921875 
[2025-02-18 20:11:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:1.56502103805542 norm:0.008080365136265755 max memory_allocated 22565.33544921875 
[2025-02-18 20:11:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:1.5617377758026123 norm:0.006096768658608198 max memory_allocated 22565.33544921875 
[2025-02-18 20:12:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:1.5581824779510498 norm:0.0042653270065784454 max memory_allocated 22565.33544921875 
[2025-02-18 20:12:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:1.5563881397247314 norm:0.003097238251939416 max memory_allocated 22565.33544921875 
[2025-02-18 20:13:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:1.5556023120880127 norm:0.0024978555738925934 max memory_allocated 22565.33544921875 
[2025-02-18 20:13:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:1.554234266281128 norm:0.002063494175672531 max memory_allocated 22565.33544921875 
[2025-02-18 20:14:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:1.5535719394683838 norm:0.001843961770646274 max memory_allocated 22565.33544921875 
[2025-02-18 20:14:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:1.5536879301071167 norm:0.0017669883091002703 max memory_allocated 22565.33544921875 
[2025-02-18 20:15:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:1.5528936386108398 norm:0.001754324184730649 max memory_allocated 22565.33544921875 
[2025-02-18 20:16:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:1.5524327754974365 norm:0.0017073117196559906 max memory_allocated 22565.33544921875 
[2025-02-18 20:16:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:1.5518158674240112 norm:0.0016793712275102735 max memory_allocated 22565.33544921875 
[2025-02-18 20:17:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:1.5521587133407593 norm:0.0016575480112805963 max memory_allocated 22565.33544921875 
[2025-02-18 20:17:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:1.5526423454284668 norm:0.0016708815237507224 max memory_allocated 22565.33544921875 
[2025-02-18 20:18:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:1.5525848865509033 norm:0.0016715496312826872 max memory_allocated 22565.33544921875 
[2025-02-18 20:18:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:1.5518786907196045 norm:0.0016678242245689034 max memory_allocated 22565.33544921875 
[2025-02-18 20:19:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:1.5518975257873535 norm:0.0016617390792816877 max memory_allocated 22565.33544921875 
[2025-02-18 20:19:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:1.551600456237793 norm:0.001651332131586969 max memory_allocated 22565.33544921875 
[2025-02-18 20:20:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 20:20:04 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:20:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:1.7804977893829346 norm:0.06859748810529709 max memory_allocated 22565.62255859375 
[2025-02-18 20:21:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:1.7443562746047974 norm:0.06722647696733475 max memory_allocated 22565.62255859375 
[2025-02-18 20:21:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:1.7349910736083984 norm:0.05835428088903427 max memory_allocated 22565.62255859375 
[2025-02-18 20:22:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:1.7290921211242676 norm:0.052346423268318176 max memory_allocated 22565.62255859375 
[2025-02-18 20:22:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:1.7245062589645386 norm:0.046997807919979095 max memory_allocated 22565.62255859375 
[2025-02-18 20:23:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:1.7208046913146973 norm:0.042428240180015564 max memory_allocated 22565.62255859375 
[2025-02-18 20:23:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:1.7175506353378296 norm:0.03805353119969368 max memory_allocated 22565.62255859375 
[2025-02-18 20:24:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:1.7148832082748413 norm:0.03513244539499283 max memory_allocated 22565.62255859375 
[2025-02-18 20:25:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:1.7125370502471924 norm:0.032286956906318665 max memory_allocated 22565.62255859375 
[2025-02-18 20:25:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:1.7111941576004028 norm:0.030478179454803467 max memory_allocated 22565.62255859375 
[2025-02-18 20:26:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:1.7099783420562744 norm:0.02854415401816368 max memory_allocated 22565.62255859375 
[2025-02-18 20:26:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:1.708720088005066 norm:0.026668282225728035 max memory_allocated 22565.62255859375 
[2025-02-18 20:27:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:1.7082213163375854 norm:0.025601936504244804 max memory_allocated 22565.62255859375 
[2025-02-18 20:27:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:1.707611322402954 norm:0.02455206587910652 max memory_allocated 22565.62255859375 
[2025-02-18 20:28:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:1.7070597410202026 norm:0.023688659071922302 max memory_allocated 22565.62255859375 
[2025-02-18 20:28:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:1.706428050994873 norm:0.022760476917028427 max memory_allocated 22565.62255859375 
[2025-02-18 20:29:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:1.7059400081634521 norm:0.022119931876659393 max memory_allocated 22565.62255859375 
[2025-02-18 20:30:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:1.7052855491638184 norm:0.021593578159809113 max memory_allocated 22565.62255859375 
[2025-02-18 20:30:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:1.705024242401123 norm:0.02114567905664444 max memory_allocated 22565.62255859375 
[2025-02-18 20:31:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:1.7048420906066895 norm:0.020556051284074783 max memory_allocated 22565.62255859375 
[2025-02-18 20:31:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 20:31:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:31:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:1.9501594305038452 norm:0.050210610032081604 max memory_allocated 22565.79443359375 
[2025-02-18 20:32:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:1.9247205257415771 norm:0.0429207906126976 max memory_allocated 22565.79443359375 
[2025-02-18 20:33:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:1.914089322090149 norm:0.036353424191474915 max memory_allocated 22565.79443359375 
[2025-02-18 20:33:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:1.9079829454421997 norm:0.03157758712768555 max memory_allocated 22565.79443359375 
[2025-02-18 20:34:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:1.9034693241119385 norm:0.027817485854029655 max memory_allocated 22565.79443359375 
[2025-02-18 20:34:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:1.8997690677642822 norm:0.02425481006503105 max memory_allocated 22565.79443359375 
[2025-02-18 20:35:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:1.896315336227417 norm:0.021460749208927155 max memory_allocated 22565.79443359375 
[2025-02-18 20:35:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:1.8938888311386108 norm:0.01965905725955963 max memory_allocated 22565.79443359375 
[2025-02-18 20:36:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:1.891470193862915 norm:0.018213283270597458 max memory_allocated 22565.79443359375 
[2025-02-18 20:36:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:1.8905137777328491 norm:0.017721161246299744 max memory_allocated 22565.79443359375 
[2025-02-18 20:37:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:1.8890316486358643 norm:0.017049893736839294 max memory_allocated 22565.79443359375 
[2025-02-18 20:38:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:1.8878661394119263 norm:0.016268257051706314 max memory_allocated 22565.79443359375 
[2025-02-18 20:38:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:1.8867000341415405 norm:0.01579439267516136 max memory_allocated 22565.79443359375 
[2025-02-18 20:39:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:1.8860573768615723 norm:0.015570251271128654 max memory_allocated 22565.79443359375 
[2025-02-18 20:39:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:1.8854525089263916 norm:0.015063837170600891 max memory_allocated 22565.79443359375 
[2025-02-18 20:40:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:1.884572982788086 norm:0.014576932415366173 max memory_allocated 22565.79443359375 
[2025-02-18 20:40:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:1.8839316368103027 norm:0.014187056571245193 max memory_allocated 22565.79443359375 
[2025-02-18 20:41:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:1.8834023475646973 norm:0.01373184472322464 max memory_allocated 22565.79443359375 
[2025-02-18 20:41:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:1.883015513420105 norm:0.013601028360426426 max memory_allocated 22565.79443359375 
[2025-02-18 20:42:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:1.8824610710144043 norm:0.013410523533821106 max memory_allocated 22565.79443359375 
[2025-02-18 20:42:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 20:42:40 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:43:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:3.239500045776367 norm:0.15535785257816315 max memory_allocated 22565.96630859375 
[2025-02-18 20:43:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:3.2481722831726074 norm:0.3211504817008972 max memory_allocated 22565.96630859375 
[2025-02-18 20:44:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:2.9831833839416504 norm:0.1792753040790558 max memory_allocated 22565.96630859375 
[2025-02-18 20:44:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:2.7834699153900146 norm:0.11184795200824738 max memory_allocated 22565.96630859375 
[2025-02-18 20:45:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:2.7074201107025146 norm:0.09074287861585617 max memory_allocated 22565.96630859375 
[2025-02-18 20:45:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:2.685063123703003 norm:0.08942662179470062 max memory_allocated 22565.96630859375 
[2025-02-18 20:46:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:2.654716968536377 norm:0.0786602720618248 max memory_allocated 22565.96630859375 
[2025-02-18 20:47:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:2.6440024375915527 norm:0.07964558899402618 max memory_allocated 22565.96630859375 
[2025-02-18 20:47:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:2.643990993499756 norm:0.07127037644386292 max memory_allocated 22565.96630859375 
[2025-02-18 20:48:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:2.6363253593444824 norm:0.06337112933397293 max memory_allocated 22565.96630859375 
[2025-02-18 20:48:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:2.645312547683716 norm:0.06523320823907852 max memory_allocated 22565.96630859375 
[2025-02-18 20:49:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:2.624134063720703 norm:0.06690160930156708 max memory_allocated 22565.96630859375 
[2025-02-18 20:49:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:2.6014819145202637 norm:0.0745718702673912 max memory_allocated 22565.96630859375 
[2025-02-18 20:50:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:2.5979862213134766 norm:0.08524413406848907 max memory_allocated 22565.96630859375 
[2025-02-18 20:50:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:2.599252223968506 norm:0.09086992591619492 max memory_allocated 22565.96630859375 
[2025-02-18 20:51:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:2.602374315261841 norm:0.09674225002527237 max memory_allocated 22565.96630859375 
[2025-02-18 20:52:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:2.607447385787964 norm:0.10991358757019043 max memory_allocated 22565.96630859375 
[2025-02-18 20:52:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:2.5936150550842285 norm:0.1031714379787445 max memory_allocated 22565.96630859375 
[2025-02-18 20:53:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:2.591093063354492 norm:0.09425228089094162 max memory_allocated 22565.96630859375 
[2025-02-18 20:53:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:2.5967814922332764 norm:0.09460656344890594 max memory_allocated 22565.96630859375 
[2025-02-18 20:53:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 20:53:56 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-18 20:54:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:5.02311897277832 norm:0.27039656043052673 max memory_allocated 22566.13818359375 
[2025-02-18 20:55:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:4.880231857299805 norm:0.2476925551891327 max memory_allocated 22566.13818359375 
[2025-02-18 20:55:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:4.810916423797607 norm:0.21820363402366638 max memory_allocated 22566.13818359375 
[2025-02-18 20:56:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:4.769095420837402 norm:0.1872694492340088 max memory_allocated 22566.13818359375 
[2025-02-18 20:56:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:4.743524074554443 norm:0.16775870323181152 max memory_allocated 22566.13818359375 
[2025-02-18 20:57:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:4.719465255737305 norm:0.14900584518909454 max memory_allocated 22566.13818359375 
[2025-02-18 20:57:48 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:4.688961505889893 norm:0.135386660695076 max memory_allocated 22566.13818359375 
[2025-02-18 20:58:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:4.66823673248291 norm:0.12421510368585587 max memory_allocated 22566.13818359375 
[2025-02-18 20:58:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:4.650458812713623 norm:0.11359748989343643 max memory_allocated 22566.13818359375 
[2025-02-18 20:59:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:4.642032623291016 norm:0.10614679753780365 max memory_allocated 22566.13818359375 
[2025-02-18 21:00:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:4.632382392883301 norm:0.1006728932261467 max memory_allocated 22566.13818359375 
[2025-02-18 21:00:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:4.614062786102295 norm:0.09729515016078949 max memory_allocated 22566.13818359375 
[2025-02-18 21:01:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:4.59503173828125 norm:0.09455282986164093 max memory_allocated 22566.13818359375 
[2025-02-18 21:01:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:4.572320938110352 norm:0.09241673350334167 max memory_allocated 22566.13818359375 
[2025-02-18 21:02:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:4.540258407592773 norm:0.09267568588256836 max memory_allocated 22566.13818359375 
[2025-02-18 21:02:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:4.519520282745361 norm:0.09010158479213715 max memory_allocated 22566.13818359375 
[2025-02-18 21:03:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:4.506927967071533 norm:0.08872204273939133 max memory_allocated 22566.13818359375 
[2025-02-18 21:03:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:4.504208087921143 norm:0.08259039372205734 max memory_allocated 22566.13818359375 
[2025-02-18 21:04:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:4.492169380187988 norm:0.08445851504802704 max memory_allocated 22566.13818359375 
[2025-02-18 21:04:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:4.474319934844971 norm:0.08429551124572754 max memory_allocated 22566.13818359375 
[2025-02-18 21:05:09 root] (main_calibration.py 365): INFO 21600.19873690605
[2025-02-18 21:05:41 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-18 21:06:50 root] (main_calibration.py 158): INFO wikitext2 : 11.391595840454102
[2025-02-18 21:06:50 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-18 21:08:38 root] (main_calibration.py 158): INFO c4 : 16.164443969726562
[2025-02-18 22:46:47 root] (main_calibration.py 169): INFO {'wikitext2': 11.391595840454102, 'c4': 16.164443969726562, 'results': {'boolq': {'acc': 0.5850152905198777, 'acc_stderr': 0.008617716361921565}, 'hellaswag': {'acc': 0.4082852021509659, 'acc_stderr': 0.004905119039849456, 'acc_norm': 0.5256920932085242, 'acc_norm_stderr': 0.004983189711208511}, 'arc_easy': {'acc': 0.4659090909090909, 'acc_stderr': 0.010235908103438688, 'acc_norm': 0.398989898989899, 'acc_norm_stderr': 0.010048240683798774}, 'winogrande': {'acc': 0.5280189423835833, 'acc_stderr': 0.01403040421340578}, 'arc_challenge': {'acc': 0.27474402730375425, 'acc_stderr': 0.013044617212771227, 'acc_norm': 0.30802047781569963, 'acc_norm_stderr': 0.013491429517292038}, 'piqa': {'acc': 0.6512513601741022, 'acc_stderr': 0.011119263056159592, 'acc_norm': 0.6409140369967355, 'acc_norm_stderr': 0.011192949073844112}}, 'versions': {'boolq': 1, 'hellaswag': 0, 'arc_easy': 0, 'winogrande': 0, 'arc_challenge': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
