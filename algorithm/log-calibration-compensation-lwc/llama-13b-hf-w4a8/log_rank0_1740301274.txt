[2025-02-23 09:01:14 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-13b-hf-w4a8', save_dir='./log-calibration-compensation-lwc/quant/llama-13b-hf-w4a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-23 09:03:36 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-23 09:03:36 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-23 09:03:36 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-23 09:03:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-23 09:03:45 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:04:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.014213670045137405 norm:0.013292423449456692 max memory_allocated 29268.02001953125 
[2025-02-23 09:05:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.008149665780365467 norm:0.006702549755573273 max memory_allocated 29268.02001953125 
[2025-02-23 09:06:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0054402160458266735 norm:0.0042076692916452885 max memory_allocated 29268.02001953125 
[2025-02-23 09:07:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.004682764410972595 norm:0.0033373567275702953 max memory_allocated 29268.02001953125 
[2025-02-23 09:07:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0043493518605828285 norm:0.002677149837836623 max memory_allocated 29268.02001953125 
[2025-02-23 09:08:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0042071351781487465 norm:0.0023804297670722008 max memory_allocated 29268.02001953125 
[2025-02-23 09:09:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.004064599052071571 norm:0.0019943213555961847 max memory_allocated 29268.02001953125 
[2025-02-23 09:10:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.003916154615581036 norm:0.0017069490859284997 max memory_allocated 29268.02001953125 
[2025-02-23 09:11:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0038881665095686913 norm:0.0015525469789281487 max memory_allocated 29268.02001953125 
[2025-02-23 09:12:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0038229820784181356 norm:0.0014008195139467716 max memory_allocated 29268.02001953125 
[2025-02-23 09:12:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.003785736160352826 norm:0.001220134668983519 max memory_allocated 29268.02001953125 
[2025-02-23 09:13:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0037443151231855154 norm:0.0011196478735655546 max memory_allocated 29268.02001953125 
[2025-02-23 09:14:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.003700228640809655 norm:0.0009943090844899416 max memory_allocated 29268.02001953125 
[2025-02-23 09:15:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0036838268861174583 norm:0.0009225135436281562 max memory_allocated 29268.02001953125 
[2025-02-23 09:16:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.003670495469123125 norm:0.0008484737481921911 max memory_allocated 29268.02001953125 
[2025-02-23 09:17:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0036776228807866573 norm:0.0008163611637428403 max memory_allocated 29268.02001953125 
[2025-02-23 09:17:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.003685416653752327 norm:0.0007466122042387724 max memory_allocated 29268.02001953125 
[2025-02-23 09:18:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0036422284319996834 norm:0.0007356579881161451 max memory_allocated 29268.02001953125 
[2025-02-23 09:19:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0035981799010187387 norm:0.0007135776104405522 max memory_allocated 29268.02001953125 
[2025-02-23 09:20:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.003635932458564639 norm:0.0006537723820656538 max memory_allocated 29268.02001953125 
[2025-02-23 09:20:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-23 09:20:45 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:21:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.024246234446763992 norm:0.01596938632428646 max memory_allocated 29268.02001953125 
[2025-02-23 09:22:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.01399203296750784 norm:0.008494128473103046 max memory_allocated 29268.02001953125 
[2025-02-23 09:23:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.009921845048666 norm:0.005632244981825352 max memory_allocated 29268.02001953125 
[2025-02-23 09:24:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.008728880435228348 norm:0.004519566893577576 max memory_allocated 29268.02001953125 
[2025-02-23 09:24:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.008251478895545006 norm:0.0038987877778708935 max memory_allocated 29268.02001953125 
[2025-02-23 09:25:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.007971112616360188 norm:0.003550910856574774 max memory_allocated 29268.02001953125 
[2025-02-23 09:26:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.007774427533149719 norm:0.003217448713257909 max memory_allocated 29268.02001953125 
[2025-02-23 09:27:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.007598212920129299 norm:0.0029720189049839973 max memory_allocated 29268.02001953125 
[2025-02-23 09:28:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.007443018723279238 norm:0.0027167159132659435 max memory_allocated 29268.02001953125 
[2025-02-23 09:29:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.007295301649719477 norm:0.0025052777491509914 max memory_allocated 29268.02001953125 
[2025-02-23 09:29:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.007194028235971928 norm:0.0022598940413445234 max memory_allocated 29268.02001953125 
[2025-02-23 09:30:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.007078008726239204 norm:0.002072326373308897 max memory_allocated 29268.02001953125 
[2025-02-23 09:31:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.006987171247601509 norm:0.0018856272799894214 max memory_allocated 29268.02001953125 
[2025-02-23 09:32:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.006899808533489704 norm:0.0016891085542738438 max memory_allocated 29268.02001953125 
[2025-02-23 09:33:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.006840604357421398 norm:0.0015464222524315119 max memory_allocated 29268.02001953125 
[2025-02-23 09:34:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.006804189644753933 norm:0.0013960532378405333 max memory_allocated 29268.02001953125 
[2025-02-23 09:35:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.006763746961951256 norm:0.0012796177761629224 max memory_allocated 29268.02001953125 
[2025-02-23 09:35:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.006736463867127895 norm:0.0011839158833026886 max memory_allocated 29268.02001953125 
[2025-02-23 09:36:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.006748153828084469 norm:0.0011924341088160872 max memory_allocated 29268.02001953125 
[2025-02-23 09:37:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0067778704687952995 norm:0.001193607342429459 max memory_allocated 29268.02001953125 
[2025-02-23 09:37:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-23 09:37:49 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:38:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.023080650717020035 norm:0.008121022023260593 max memory_allocated 29268.39501953125 
[2025-02-23 09:39:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.01850769855082035 norm:0.006988564506173134 max memory_allocated 29268.39501953125 
[2025-02-23 09:40:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.015605968423187733 norm:0.005225917790085077 max memory_allocated 29268.39501953125 
[2025-02-23 09:41:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.014687451533973217 norm:0.004647109657526016 max memory_allocated 29268.39501953125 
[2025-02-23 09:42:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.014097319915890694 norm:0.004234881140291691 max memory_allocated 29268.39501953125 
[2025-02-23 09:42:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.013774157501757145 norm:0.003761960193514824 max memory_allocated 29268.39501953125 
[2025-02-23 09:43:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.013573716394603252 norm:0.0037612076848745346 max memory_allocated 29268.39501953125 
[2025-02-23 09:44:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.013320882804691792 norm:0.0036259014159440994 max memory_allocated 29268.39501953125 
[2025-02-23 09:45:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.013274009339511395 norm:0.003760389517992735 max memory_allocated 29268.39501953125 
[2025-02-23 09:46:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.013047139160335064 norm:0.003412520745769143 max memory_allocated 29268.39501953125 
[2025-02-23 09:47:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.01329459436237812 norm:0.003550827968865633 max memory_allocated 29268.39501953125 
[2025-02-23 09:47:54 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.012952382676303387 norm:0.003319102805107832 max memory_allocated 29268.39501953125 
[2025-02-23 09:48:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.013200579211115837 norm:0.0033298612106591463 max memory_allocated 29268.39501953125 
[2025-02-23 09:49:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.013143012300133705 norm:0.0031715300865471363 max memory_allocated 29268.39501953125 
[2025-02-23 09:50:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.013124221004545689 norm:0.0032143862918019295 max memory_allocated 29268.39501953125 
[2025-02-23 09:51:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.01301793847233057 norm:0.003007070627063513 max memory_allocated 29268.39501953125 
[2025-02-23 09:52:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.013028543442487717 norm:0.0029509610030800104 max memory_allocated 29268.39501953125 
[2025-02-23 09:52:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.013175920583307743 norm:0.003193308599293232 max memory_allocated 29268.39501953125 
[2025-02-23 09:53:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.01304684765636921 norm:0.0029554027132689953 max memory_allocated 29268.39501953125 
[2025-02-23 09:54:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.013103976845741272 norm:0.0029453823808580637 max memory_allocated 29268.39501953125 
[2025-02-23 09:54:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-23 09:55:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.02907053381204605 norm:0.0024667505640536547 max memory_allocated 29268.43798828125 
[2025-02-23 09:56:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.021685447543859482 norm:0.0008205414633266628 max memory_allocated 29268.43798828125 
[2025-02-23 09:57:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.017788443714380264 norm:0.0004412092675920576 max memory_allocated 29268.43798828125 
[2025-02-23 09:58:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.01641896553337574 norm:0.0002883942797780037 max memory_allocated 29268.43798828125 
[2025-02-23 09:59:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.01579911634325981 norm:0.00026505961432121694 max memory_allocated 29268.43798828125 
[2025-02-23 09:59:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.015307559631764889 norm:0.00018953587277792394 max memory_allocated 29268.43798828125 
[2025-02-23 10:00:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.01498149149119854 norm:0.00016469665570184588 max memory_allocated 29268.43798828125 
[2025-02-23 10:01:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.014799464493989944 norm:0.0001445179368602112 max memory_allocated 29268.43798828125 
[2025-02-23 10:02:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.014709776267409325 norm:0.0001498681667726487 max memory_allocated 29268.43798828125 
[2025-02-23 10:03:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.014645133167505264 norm:0.0001292349916184321 max memory_allocated 29268.43798828125 
[2025-02-23 10:04:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.014574497006833553 norm:0.00012033736857119948 max memory_allocated 29268.43798828125 
[2025-02-23 10:04:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.014555689878761768 norm:0.0001230951165780425 max memory_allocated 29268.43798828125 
[2025-02-23 10:05:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.014507358893752098 norm:0.00011728384561138228 max memory_allocated 29268.43798828125 
[2025-02-23 10:06:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.014557338319718838 norm:0.0001583382982062176 max memory_allocated 29268.43798828125 
[2025-02-23 10:07:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.014534172601997852 norm:0.00010466821549925953 max memory_allocated 29268.43798828125 
[2025-02-23 10:08:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.014516618102788925 norm:9.617326577426866e-05 max memory_allocated 29268.43798828125 
[2025-02-23 10:09:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.014524522237479687 norm:9.982543997466564e-05 max memory_allocated 29268.43798828125 
[2025-02-23 10:09:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.01450173556804657 norm:0.00010428774112369865 max memory_allocated 29268.43798828125 
[2025-02-23 10:10:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.014504560269415379 norm:0.0001099269647966139 max memory_allocated 29268.43798828125 
[2025-02-23 10:11:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.014507360756397247 norm:0.00011536967940628529 max memory_allocated 29268.43798828125 
[2025-02-23 10:11:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-23 10:12:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.03687598928809166 norm:0.0058677890338003635 max memory_allocated 29268.43798828125 
[2025-02-23 10:13:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0271092988550663 norm:0.0015876441029831767 max memory_allocated 29268.43798828125 
[2025-02-23 10:14:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.022245939821004868 norm:0.0009096248541027308 max memory_allocated 29268.43798828125 
[2025-02-23 10:15:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.020613890141248703 norm:0.0006406342145055532 max memory_allocated 29268.43798828125 
[2025-02-23 10:16:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.019696464762091637 norm:0.0005173935787752271 max memory_allocated 29268.43798828125 
[2025-02-23 10:16:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.019093036651611328 norm:0.0004140262317378074 max memory_allocated 29268.43798828125 
[2025-02-23 10:17:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.018743576481938362 norm:0.000361263460945338 max memory_allocated 29268.43798828125 
[2025-02-23 10:18:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.018564114347100258 norm:0.00030320859514176846 max memory_allocated 29268.43798828125 
[2025-02-23 10:19:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.018448568880558014 norm:0.0002719215990509838 max memory_allocated 29268.43798828125 
[2025-02-23 10:20:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.018375970423221588 norm:0.00024663438671268523 max memory_allocated 29268.43798828125 
[2025-02-23 10:21:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.01831219531595707 norm:0.0002213451953139156 max memory_allocated 29268.43798828125 
[2025-02-23 10:22:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.01824984885752201 norm:0.00019513737061060965 max memory_allocated 29268.43798828125 
[2025-02-23 10:22:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.01826431229710579 norm:0.00018310440646018833 max memory_allocated 29268.43798828125 
[2025-02-23 10:23:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.018290024250745773 norm:0.00021057597768958658 max memory_allocated 29268.43798828125 
[2025-02-23 10:24:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.01823101006448269 norm:0.00015044110477901995 max memory_allocated 29268.43798828125 
[2025-02-23 10:25:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.018167009577155113 norm:0.0001262905861949548 max memory_allocated 29268.43798828125 
[2025-02-23 10:26:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.018144628033041954 norm:0.00012470920046325773 max memory_allocated 29268.43798828125 
[2025-02-23 10:27:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.01812993362545967 norm:0.00012618774781003594 max memory_allocated 29268.43798828125 
[2025-02-23 10:27:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.018118273466825485 norm:0.00012488767970353365 max memory_allocated 29268.43798828125 
[2025-02-23 10:28:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.01809418760240078 norm:0.00012529852392617613 max memory_allocated 29268.43798828125 
[2025-02-23 10:28:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-23 10:29:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.04128621146082878 norm:0.003958775196224451 max memory_allocated 29268.81298828125 
[2025-02-23 10:30:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.030125737190246582 norm:0.0011164348106831312 max memory_allocated 29268.81298828125 
[2025-02-23 10:31:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.024889929220080376 norm:0.000626337539870292 max memory_allocated 29268.81298828125 
[2025-02-23 10:32:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.023139504715800285 norm:0.00044310366502031684 max memory_allocated 29268.81298828125 
[2025-02-23 10:33:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.022092022001743317 norm:0.0003434523241594434 max memory_allocated 29268.81298828125 
[2025-02-23 10:34:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.021472755819559097 norm:0.00028619286604225636 max memory_allocated 29268.81298828125 
[2025-02-23 10:34:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.021115142852067947 norm:0.00024540789308957756 max memory_allocated 29268.81298828125 
[2025-02-23 10:35:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.020921602845191956 norm:0.00023278771550394595 max memory_allocated 29268.81298828125 
[2025-02-23 10:36:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.020784612745046616 norm:0.00018685297982301563 max memory_allocated 29268.81298828125 
[2025-02-23 10:37:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.020687974989414215 norm:0.0001618325331946835 max memory_allocated 29268.81298828125 
[2025-02-23 10:38:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.02062949724495411 norm:0.00015163965872488916 max memory_allocated 29268.81298828125 
[2025-02-23 10:39:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.020602229982614517 norm:0.00017193281382787973 max memory_allocated 29268.81298828125 
[2025-02-23 10:39:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.020532431080937386 norm:0.0001496358308941126 max memory_allocated 29268.81298828125 
[2025-02-23 10:40:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.02053472213447094 norm:0.00015297462232410908 max memory_allocated 29268.81298828125 
[2025-02-23 10:41:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.02048361673951149 norm:0.00012278604845050722 max memory_allocated 29268.81298828125 
[2025-02-23 10:42:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.020497070625424385 norm:0.0001527568674646318 max memory_allocated 29268.81298828125 
[2025-02-23 10:43:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.020445698872208595 norm:0.00012047222116962075 max memory_allocated 29268.81298828125 
[2025-02-23 10:44:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.02042102813720703 norm:0.00011267342779319733 max memory_allocated 29268.81298828125 
[2025-02-23 10:44:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.020397137850522995 norm:0.00011209708463866264 max memory_allocated 29268.81298828125 
[2025-02-23 10:45:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0204066950827837 norm:0.00011628750507952645 max memory_allocated 29268.81298828125 
[2025-02-23 10:45:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-23 10:46:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.04640837013721466 norm:0.001067504519596696 max memory_allocated 29269.00048828125 
[2025-02-23 10:47:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.036113180220127106 norm:0.0004348888760432601 max memory_allocated 29269.00048828125 
[2025-02-23 10:48:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.03012138418853283 norm:0.0003249107685405761 max memory_allocated 29269.00048828125 
[2025-02-23 10:49:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.028113137930631638 norm:0.00030749785946682096 max memory_allocated 29269.00048828125 
[2025-02-23 10:50:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.027086514979600906 norm:0.00029264288605190814 max memory_allocated 29269.00048828125 
[2025-02-23 10:51:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.026408594101667404 norm:0.00023427237465512007 max memory_allocated 29269.00048828125 
[2025-02-23 10:51:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.026092026382684708 norm:0.0002884301356971264 max memory_allocated 29269.00048828125 
[2025-02-23 10:52:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.025880686938762665 norm:0.00023835046158637851 max memory_allocated 29269.00048828125 
[2025-02-23 10:53:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.025712531059980392 norm:0.00020789369591511786 max memory_allocated 29269.00048828125 
[2025-02-23 10:54:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.025605440139770508 norm:0.00018263934180140495 max memory_allocated 29269.00048828125 
[2025-02-23 10:55:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.02553275041282177 norm:0.00018627835379447788 max memory_allocated 29269.00048828125 
[2025-02-23 10:56:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.025482360273599625 norm:0.0001673999649938196 max memory_allocated 29269.00048828125 
[2025-02-23 10:56:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.025468293577432632 norm:0.00016938168846536428 max memory_allocated 29269.00048828125 
[2025-02-23 10:57:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.02538342960178852 norm:0.0001493798044975847 max memory_allocated 29269.00048828125 
[2025-02-23 10:58:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.025404907763004303 norm:0.00015335381613112986 max memory_allocated 29269.00048828125 
[2025-02-23 10:59:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.025421680882573128 norm:0.00014735801960341632 max memory_allocated 29269.00048828125 
[2025-02-23 11:00:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.02566932886838913 norm:0.00029041810194030404 max memory_allocated 29269.00048828125 
[2025-02-23 11:01:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.025407930836081505 norm:0.00019126907864119858 max memory_allocated 29269.00048828125 
[2025-02-23 11:01:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.02544206567108631 norm:0.00018332843319512904 max memory_allocated 29269.00048828125 
[2025-02-23 11:02:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.02541198395192623 norm:0.00017307222879026085 max memory_allocated 29269.00048828125 
[2025-02-23 11:03:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-23 11:03:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.04426119849085808 norm:0.0005140721914358437 max memory_allocated 29269.18798828125 
[2025-02-23 11:04:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.03626686707139015 norm:0.0002366659027757123 max memory_allocated 29269.18798828125 
[2025-02-23 11:05:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.031260404735803604 norm:0.00017446198035031557 max memory_allocated 29269.18798828125 
[2025-02-23 11:06:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.02948661334812641 norm:0.0001443129003746435 max memory_allocated 29269.18798828125 
[2025-02-23 11:07:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.028594529256224632 norm:0.00012635334860533476 max memory_allocated 29269.18798828125 
[2025-02-23 11:08:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.028060447424650192 norm:0.00012081426393706352 max memory_allocated 29269.18798828125 
[2025-02-23 11:08:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.027767295017838478 norm:0.00011091382475569844 max memory_allocated 29269.18798828125 
[2025-02-23 11:09:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.027620792388916016 norm:0.00010774001566460356 max memory_allocated 29269.18798828125 
[2025-02-23 11:10:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.027516653761267662 norm:9.777021477930248e-05 max memory_allocated 29269.18798828125 
[2025-02-23 11:11:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.02747594192624092 norm:0.00011143297888338566 max memory_allocated 29269.18798828125 
[2025-02-23 11:12:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.027437806129455566 norm:9.877670527203009e-05 max memory_allocated 29269.18798828125 
[2025-02-23 11:13:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.027397671714425087 norm:9.782863344298676e-05 max memory_allocated 29269.18798828125 
[2025-02-23 11:13:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.027368074283003807 norm:0.00010027292591985315 max memory_allocated 29269.18798828125 
[2025-02-23 11:14:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.027341004461050034 norm:9.046968625625595e-05 max memory_allocated 29269.18798828125 
[2025-02-23 11:15:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.027305420488119125 norm:9.875705291051418e-05 max memory_allocated 29269.18798828125 
[2025-02-23 11:16:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.02727404236793518 norm:9.845905879046768e-05 max memory_allocated 29269.18798828125 
[2025-02-23 11:17:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.02726871706545353 norm:0.00010116588236996904 max memory_allocated 29269.18798828125 
[2025-02-23 11:18:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.02726796455681324 norm:9.656748443376273e-05 max memory_allocated 29269.18798828125 
[2025-02-23 11:19:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.027255820110440254 norm:9.066497295862064e-05 max memory_allocated 29269.18798828125 
[2025-02-23 11:19:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.027253735810518265 norm:0.00010094798199133947 max memory_allocated 29269.18798828125 
[2025-02-23 11:20:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-23 11:20:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.04794049263000488 norm:0.0006851385114714503 max memory_allocated 29269.37548828125 
[2025-02-23 11:21:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.03938908502459526 norm:0.0003098691813647747 max memory_allocated 29269.37548828125 
[2025-02-23 11:22:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.03440612182021141 norm:0.0002127346961060539 max memory_allocated 29269.37548828125 
[2025-02-23 11:23:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.032549601048231125 norm:0.0001983426627703011 max memory_allocated 29269.37548828125 
[2025-02-23 11:24:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.031530119478702545 norm:0.00017155382374767214 max memory_allocated 29269.37548828125 
[2025-02-23 11:25:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.030956298112869263 norm:0.0001604049903107807 max memory_allocated 29269.37548828125 
[2025-02-23 11:26:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.030598385259509087 norm:0.00013877043966203928 max memory_allocated 29269.37548828125 
[2025-02-23 11:26:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.030426181852817535 norm:0.00013141907402314246 max memory_allocated 29269.37548828125 
[2025-02-23 11:27:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.03033513016998768 norm:0.00012881771544925869 max memory_allocated 29269.37548828125 
[2025-02-23 11:28:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.030261697247624397 norm:0.00012141868501203135 max memory_allocated 29269.37548828125 
[2025-02-23 11:29:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.030207356438040733 norm:0.0001176234072772786 max memory_allocated 29269.37548828125 
[2025-02-23 11:30:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.030193764716386795 norm:0.00011840500519610941 max memory_allocated 29269.37548828125 
[2025-02-23 11:31:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.030180897563695908 norm:0.00011987829930149019 max memory_allocated 29269.37548828125 
[2025-02-23 11:31:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.03016088157892227 norm:0.0001204920990858227 max memory_allocated 29269.37548828125 
[2025-02-23 11:32:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.030140839517116547 norm:0.00012206586688989773 max memory_allocated 29269.37548828125 
[2025-02-23 11:33:32 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.030115904286503792 norm:0.00011737914610421285 max memory_allocated 29269.37548828125 
[2025-02-23 11:34:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.030113501474261284 norm:0.0001126503266277723 max memory_allocated 29269.37548828125 
[2025-02-23 11:35:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.03012208640575409 norm:0.00011275412543909624 max memory_allocated 29269.37548828125 
[2025-02-23 11:36:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.03008945845067501 norm:0.00011486830044304952 max memory_allocated 29269.37548828125 
[2025-02-23 11:36:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.030066125094890594 norm:0.00011483255366329104 max memory_allocated 29269.37548828125 
[2025-02-23 11:37:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-23 11:38:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.048850808292627335 norm:0.0005049728788435459 max memory_allocated 29269.56298828125 
[2025-02-23 11:38:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.04147104546427727 norm:0.0002548886404838413 max memory_allocated 29269.56298828125 
[2025-02-23 11:39:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.03647150099277496 norm:0.00018332747276872396 max memory_allocated 29269.56298828125 
[2025-02-23 11:40:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.03470893204212189 norm:0.0001434760633856058 max memory_allocated 29269.56298828125 
[2025-02-23 11:41:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0338904894888401 norm:0.000130412561702542 max memory_allocated 29269.56298828125 
[2025-02-23 11:42:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.03337535262107849 norm:0.00011588523193495348 max memory_allocated 29269.56298828125 
[2025-02-23 11:43:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.033094100654125214 norm:0.0001023629229166545 max memory_allocated 29269.56298828125 
[2025-02-23 11:43:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.03293085843324661 norm:9.513050463283435e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:44:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.03286457061767578 norm:9.013880480779335e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:45:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.032815515995025635 norm:8.762604556977749e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:46:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.03276605159044266 norm:8.311535930261016e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:47:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.03270909935235977 norm:8.279651956399903e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:48:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.03268321603536606 norm:8.217929280363023e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:48:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.03265434503555298 norm:7.961748633533716e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:49:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.032644737511873245 norm:8.013863407541066e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:50:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.032624196261167526 norm:8.033778431126848e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:51:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.03262893110513687 norm:8.454259659629315e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:52:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.032620713114738464 norm:8.189002983272076e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:53:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.03261445462703705 norm:8.017662185011432e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:53:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.03261728584766388 norm:8.390875154873356e-05 max memory_allocated 29269.56298828125 
[2025-02-23 11:54:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-23 11:55:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.051643434911966324 norm:0.0006021264125593007 max memory_allocated 29269.75048828125 
[2025-02-23 11:55:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.044138822704553604 norm:0.0002754892338998616 max memory_allocated 29269.75048828125 
[2025-02-23 11:56:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.03919873386621475 norm:0.00018207474204245955 max memory_allocated 29269.75048828125 
[2025-02-23 11:57:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.037388067692518234 norm:0.00013841374311596155 max memory_allocated 29269.75048828125 
[2025-02-23 11:58:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.036539476364851 norm:0.00012309078010730445 max memory_allocated 29269.75048828125 
[2025-02-23 11:59:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.03606490418314934 norm:0.00010763572208816186 max memory_allocated 29269.75048828125 
[2025-02-23 12:00:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.03578740358352661 norm:0.00010162017133552581 max memory_allocated 29269.75048828125 
[2025-02-23 12:00:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.03564083203673363 norm:9.526569920126349e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:01:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.035563673824071884 norm:8.840975351631641e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:02:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.03550577908754349 norm:8.569667988922447e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:03:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.03544533997774124 norm:8.298457396449521e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:04:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.03539437800645828 norm:7.928181003080681e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:05:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.03537759184837341 norm:7.704590098001063e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:05:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.035354193300008774 norm:7.613143679918721e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:06:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.035344600677490234 norm:7.459077460225672e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:07:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.03531697392463684 norm:7.530181756010279e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:08:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.03531670942902565 norm:7.423108763759956e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:09:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.03530661389231682 norm:7.440366607625037e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:10:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.035287532955408096 norm:7.680431008338928e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:11:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.03529553860425949 norm:7.410188118228689e-05 max memory_allocated 29269.75048828125 
[2025-02-23 12:11:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-23 12:12:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.055148862302303314 norm:0.0005277329473756254 max memory_allocated 29269.93798828125 
[2025-02-23 12:12:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.04783705621957779 norm:0.0002903399581555277 max memory_allocated 29269.93798828125 
[2025-02-23 12:13:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.04240752384066582 norm:0.0001752028038026765 max memory_allocated 29269.93798828125 
[2025-02-23 12:14:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.04060670733451843 norm:0.00013068542466498911 max memory_allocated 29269.93798828125 
[2025-02-23 12:15:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.03979972004890442 norm:0.00011315455049043521 max memory_allocated 29269.93798828125 
[2025-02-23 12:16:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.03932352364063263 norm:9.949941886588931e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:17:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.039042290300130844 norm:9.136475273407996e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:18:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.03889969736337662 norm:8.601743320468813e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:18:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.03881409019231796 norm:8.383191743632779e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:19:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.03875204920768738 norm:8.025163697311655e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:20:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.03870173543691635 norm:7.646735321031883e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:21:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.03865015506744385 norm:7.370306411758065e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:22:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.03863214701414108 norm:7.317432027775794e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:23:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.038631584495306015 norm:7.364648627117276e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:23:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.038592275232076645 norm:7.425215881085023e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:24:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.03857303410768509 norm:7.284800085471943e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:25:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.038559440523386 norm:7.230589108075947e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:26:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.03853675350546837 norm:7.146514690248296e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:27:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.038523703813552856 norm:7.160504901548848e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:28:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.038536153733730316 norm:7.11142347427085e-05 max memory_allocated 29269.93798828125 
[2025-02-23 12:28:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-23 12:29:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.058623913675546646 norm:0.0004543063114397228 max memory_allocated 29270.12548828125 
[2025-02-23 12:30:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.05122559517621994 norm:0.0002565156319178641 max memory_allocated 29270.12548828125 
[2025-02-23 12:30:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.04550531134009361 norm:0.00016783722094260156 max memory_allocated 29270.12548828125 
[2025-02-23 12:31:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.04367651045322418 norm:0.00013193937775213271 max memory_allocated 29270.12548828125 
[2025-02-23 12:32:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.04287903755903244 norm:0.00011523204739205539 max memory_allocated 29270.12548828125 
[2025-02-23 12:33:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.04239878058433533 norm:0.00010994917101925239 max memory_allocated 29270.12548828125 
[2025-02-23 12:34:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.04209556430578232 norm:9.964672790374607e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:35:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.041937924921512604 norm:9.59183307713829e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:35:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.04183988273143768 norm:9.044307807926089e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:36:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.04178236797451973 norm:8.72947130119428e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:37:34 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.04173329100012779 norm:8.635252743260935e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:38:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.04170249029994011 norm:8.534367952961475e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:39:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.04167015850543976 norm:8.435499330516905e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:40:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.041655704379081726 norm:8.258226444013417e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:40:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.04162014275789261 norm:8.11432910268195e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:41:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.04162062704563141 norm:8.063695713644847e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:42:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.04160091280937195 norm:8.042786794248968e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:43:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.041591793298721313 norm:7.996729982551187e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:44:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.04159077629446983 norm:8.031538163777441e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:45:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.041567690670490265 norm:8.036881627049297e-05 max memory_allocated 29270.12548828125 
[2025-02-23 12:45:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-23 12:46:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.06155839189887047 norm:0.0004140701494179666 max memory_allocated 29270.31298828125 
[2025-02-23 12:47:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.05421989783644676 norm:0.00023941324616316706 max memory_allocated 29270.31298828125 
[2025-02-23 12:47:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.048728570342063904 norm:0.00017489878518972546 max memory_allocated 29270.31298828125 
[2025-02-23 12:48:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.04677913710474968 norm:0.00013756229600403458 max memory_allocated 29270.31298828125 
[2025-02-23 12:49:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.04586201161146164 norm:0.00012199740012874827 max memory_allocated 29270.31298828125 
[2025-02-23 12:50:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.04531663656234741 norm:0.00011267123045399785 max memory_allocated 29270.31298828125 
[2025-02-23 12:51:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.04502210021018982 norm:0.00010590736201265827 max memory_allocated 29270.31298828125 
[2025-02-23 12:52:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.04482707381248474 norm:9.76770170382224e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:52:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.04469464346766472 norm:9.464183676755056e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:53:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.044625841081142426 norm:9.143470379058272e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:54:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.04454408958554268 norm:8.605512994108722e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:55:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.04449955374002457 norm:8.533473737770692e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:56:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0444403737783432 norm:8.083184366114438e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:57:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.04442630335688591 norm:8.073580102063715e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:57:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.04439286142587662 norm:8.080734551185742e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:58:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.04436326026916504 norm:7.87815879448317e-05 max memory_allocated 29270.31298828125 
[2025-02-23 12:59:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.04435838758945465 norm:8.018493826966733e-05 max memory_allocated 29270.31298828125 
[2025-02-23 13:00:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.04434863105416298 norm:8.17049149191007e-05 max memory_allocated 29270.31298828125 
[2025-02-23 13:01:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.044330909848213196 norm:7.958407513797283e-05 max memory_allocated 29270.31298828125 
[2025-02-23 13:02:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0443180613219738 norm:7.92331193224527e-05 max memory_allocated 29270.31298828125 
[2025-02-23 13:02:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-23 13:03:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.06567501276731491 norm:0.0005497266538441181 max memory_allocated 29270.50048828125 
[2025-02-23 13:04:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.057711437344551086 norm:0.0002850840101018548 max memory_allocated 29270.50048828125 
[2025-02-23 13:04:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.05166516453027725 norm:0.00017231942911166698 max memory_allocated 29270.50048828125 
[2025-02-23 13:05:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.04971538111567497 norm:0.0001419319014530629 max memory_allocated 29270.50048828125 
[2025-02-23 13:06:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.04876929521560669 norm:0.00012511925888247788 max memory_allocated 29270.50048828125 
[2025-02-23 13:07:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.04819139838218689 norm:0.00011260214523645118 max memory_allocated 29270.50048828125 
[2025-02-23 13:08:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.04788323491811752 norm:0.00010566994023974985 max memory_allocated 29270.50048828125 
[2025-02-23 13:09:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.04770283028483391 norm:9.89051623037085e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:09:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.04760546237230301 norm:9.33033661567606e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:10:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.04752499237656593 norm:8.638868166599423e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:11:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0474584624171257 norm:8.336083556059748e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:12:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.04740625247359276 norm:8.340169733855873e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:13:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.04737516865134239 norm:8.228855585912243e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:14:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.04731959104537964 norm:8.067135058809072e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:14:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.047304779291152954 norm:8.112512296065688e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:15:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.04728110507130623 norm:8.178954158211127e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:16:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.047244295477867126 norm:8.081558189587668e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:17:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.047227274626493454 norm:7.93097133282572e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:18:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.047208335250616074 norm:7.942917727632448e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:19:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.04719594493508339 norm:7.952765736263245e-05 max memory_allocated 29270.50048828125 
[2025-02-23 13:19:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-23 13:20:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.06877003610134125 norm:0.0006452722591347992 max memory_allocated 29270.68798828125 
[2025-02-23 13:21:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.06114896759390831 norm:0.00028840178856626153 max memory_allocated 29270.68798828125 
[2025-02-23 13:22:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.055979661643505096 norm:0.00021099374862387776 max memory_allocated 29270.68798828125 
[2025-02-23 13:22:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.05402844026684761 norm:0.00017306672816630453 max memory_allocated 29270.68798828125 
[2025-02-23 13:23:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.05291302502155304 norm:0.00015000284474808723 max memory_allocated 29270.68798828125 
[2025-02-23 13:24:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.052287034690380096 norm:0.00013495406892616302 max memory_allocated 29270.68798828125 
[2025-02-23 13:25:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.05195941776037216 norm:0.00012488188804127276 max memory_allocated 29270.68798828125 
[2025-02-23 13:26:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.05175037682056427 norm:0.00011822608939837664 max memory_allocated 29270.68798828125 
[2025-02-23 13:27:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.05161597952246666 norm:0.000109059241367504 max memory_allocated 29270.68798828125 
[2025-02-23 13:27:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.05153270810842514 norm:0.0001020972995320335 max memory_allocated 29270.68798828125 
[2025-02-23 13:28:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.051458463072776794 norm:9.650650463299826e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:29:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.05142245069146156 norm:9.167072857962921e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:30:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.05137744918465614 norm:9.283899271395057e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:31:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.051316261291503906 norm:9.027175838127732e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:32:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.051272131502628326 norm:8.699153841007501e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:32:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.05125466734170914 norm:8.502741547999904e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:33:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.05122821033000946 norm:8.467884617857635e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:34:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.05121519789099693 norm:8.285992225864902e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:35:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.05118658021092415 norm:8.077890379354358e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:36:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.05116674676537514 norm:8.110136695904657e-05 max memory_allocated 29270.68798828125 
[2025-02-23 13:36:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-23 13:37:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.06980162113904953 norm:0.0003017453127540648 max memory_allocated 29270.87548828125 
[2025-02-23 13:38:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.06370523571968079 norm:0.00016786568448878825 max memory_allocated 29270.87548828125 
[2025-02-23 13:39:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0588306225836277 norm:0.00011734047438949347 max memory_allocated 29270.87548828125 
[2025-02-23 13:39:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.057171352207660675 norm:0.00010816460417117923 max memory_allocated 29270.87548828125 
[2025-02-23 13:40:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.056210700422525406 norm:9.731124737299979e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:41:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.055655963718891144 norm:8.767080726101995e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:42:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.05540722981095314 norm:8.095178054645658e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:43:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.05527399852871895 norm:7.900546916062012e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:44:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.055186498910188675 norm:7.472051947843283e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:44:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.05512724071741104 norm:7.556380296591669e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:45:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.055070266127586365 norm:7.275022653630003e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:46:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.055018745362758636 norm:7.185306458268315e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:47:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.05500626191496849 norm:7.182371336966753e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:48:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.05495791882276535 norm:7.147472206270322e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:49:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.05491698533296585 norm:7.085615652613342e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:49:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.054890673607587814 norm:7.093049498507753e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:50:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.054870154708623886 norm:7.21841279300861e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:51:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.054857492446899414 norm:7.06999417161569e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:52:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.05484171211719513 norm:7.055559399304911e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:53:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.05482007563114166 norm:6.947985821170732e-05 max memory_allocated 29270.87548828125 
[2025-02-23 13:53:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-23 13:54:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.07687979936599731 norm:0.0003517076256684959 max memory_allocated 29271.06298828125 
[2025-02-23 13:55:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.06986847519874573 norm:0.000199882808374241 max memory_allocated 29271.06298828125 
[2025-02-23 13:56:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.06418105959892273 norm:0.0001396018051309511 max memory_allocated 29271.06298828125 
[2025-02-23 13:56:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.06229054555296898 norm:0.00012426316970959306 max memory_allocated 29271.06298828125 
[2025-02-23 13:57:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.061268214136362076 norm:0.0001113545658881776 max memory_allocated 29271.06298828125 
[2025-02-23 13:58:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.060726433992385864 norm:0.00010394598939456046 max memory_allocated 29271.06298828125 
[2025-02-23 13:59:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.060465987771749496 norm:9.979895548895001e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:00:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0603172741830349 norm:9.459869033889845e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:01:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.06022047996520996 norm:8.94928234629333e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:01:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.06012669578194618 norm:8.31834549899213e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:02:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.0600629486143589 norm:7.938248745631427e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:03:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.060005445033311844 norm:7.821939652785659e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:04:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.059975516051054 norm:7.886523962952197e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:05:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.059932202100753784 norm:7.558368088211864e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:06:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.059879183769226074 norm:7.519638893427327e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:06:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.05984267219901085 norm:7.381843897746876e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:07:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.05980866774916649 norm:7.465820090146735e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:08:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.059801407158374786 norm:7.4685929575935e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:09:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.059790000319480896 norm:7.505226676585153e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:10:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.05976660177111626 norm:7.382265175692737e-05 max memory_allocated 29271.06298828125 
[2025-02-23 14:10:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-23 14:11:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.08504485338926315 norm:0.0005687437951564789 max memory_allocated 29271.25048828125 
[2025-02-23 14:12:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.07730206847190857 norm:0.0003152606659568846 max memory_allocated 29271.25048828125 
[2025-02-23 14:13:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.07117564231157303 norm:0.00018295133486390114 max memory_allocated 29271.25048828125 
[2025-02-23 14:13:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.06920033693313599 norm:0.00015668640844523907 max memory_allocated 29271.25048828125 
[2025-02-23 14:14:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.06808936595916748 norm:0.0001373334671370685 max memory_allocated 29271.25048828125 
[2025-02-23 14:15:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.06753440201282501 norm:0.00012574279389809817 max memory_allocated 29271.25048828125 
[2025-02-23 14:16:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.06729939579963684 norm:0.00011777203326346353 max memory_allocated 29271.25048828125 
[2025-02-23 14:17:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.06713022291660309 norm:0.00011157251719851047 max memory_allocated 29271.25048828125 
[2025-02-23 14:18:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.06701993942260742 norm:0.000104406317404937 max memory_allocated 29271.25048828125 
[2025-02-23 14:18:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.06691135466098785 norm:9.920571756083518e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:19:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.06681650876998901 norm:9.439205314265564e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:20:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.06672881543636322 norm:9.25628119148314e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:21:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.06665769219398499 norm:9.158444299828261e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:22:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.06659390032291412 norm:8.806534606264904e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:23:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.06655976176261902 norm:8.527185127604753e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:24:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.06654031574726105 norm:8.530401100870222e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:24:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.06653527170419693 norm:8.640997111797333e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:25:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.06651695817708969 norm:8.484747377224267e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:26:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.06649233400821686 norm:8.314983278978616e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:27:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.06646804511547089 norm:8.205961785279214e-05 max memory_allocated 29271.25048828125 
[2025-02-23 14:27:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-23 14:28:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.09292278438806534 norm:0.00038582200068049133 max memory_allocated 29271.43798828125 
[2025-02-23 14:29:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.08552078157663345 norm:0.00023304969363380224 max memory_allocated 29271.43798828125 
[2025-02-23 14:30:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.07901781797409058 norm:0.00015467712364625186 max memory_allocated 29271.43798828125 
[2025-02-23 14:31:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.07688259333372116 norm:0.00013106188271194696 max memory_allocated 29271.43798828125 
[2025-02-23 14:31:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.07579497247934341 norm:0.0001227743923664093 max memory_allocated 29271.43798828125 
[2025-02-23 14:32:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.07529239356517792 norm:0.00011372736480552703 max memory_allocated 29271.43798828125 
[2025-02-23 14:33:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.07506851106882095 norm:0.00010547931015025824 max memory_allocated 29271.43798828125 
[2025-02-23 14:34:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.07490096241235733 norm:9.720330126583576e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:35:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0747961476445198 norm:9.751004108693451e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:36:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.07469464838504791 norm:9.745185525389388e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:36:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.07463930547237396 norm:9.621931531000882e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:37:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.07457907497882843 norm:9.322298865299672e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:38:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.07454562932252884 norm:9.284538100473583e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:39:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.07449774444103241 norm:8.855685155140236e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:40:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.07444790005683899 norm:8.776475442573428e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:41:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.07440052926540375 norm:8.686906221555546e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:41:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.07435958087444305 norm:8.406932465732098e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:42:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.07433459162712097 norm:8.460260869469494e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:43:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.07430461049079895 norm:8.437870565103367e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:44:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0742834284901619 norm:8.421346865361556e-05 max memory_allocated 29271.43798828125 
[2025-02-23 14:44:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-23 14:45:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.1042398065328598 norm:0.0006795792724005878 max memory_allocated 29271.62548828125 
[2025-02-23 14:46:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.09499597549438477 norm:0.000361414160579443 max memory_allocated 29271.62548828125 
[2025-02-23 14:47:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.08744194358587265 norm:0.00020543538266792893 max memory_allocated 29271.62548828125 
[2025-02-23 14:48:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.08513786643743515 norm:0.00016368136857636273 max memory_allocated 29271.62548828125 
[2025-02-23 14:48:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.08395465463399887 norm:0.00014651284436695278 max memory_allocated 29271.62548828125 
[2025-02-23 14:49:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0834655910730362 norm:0.0001323489414062351 max memory_allocated 29271.62548828125 
[2025-02-23 14:50:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.08321791887283325 norm:0.00012327115109656006 max memory_allocated 29271.62548828125 
[2025-02-23 14:51:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.0830637663602829 norm:0.00011597802222240716 max memory_allocated 29271.62548828125 
[2025-02-23 14:52:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.08289675414562225 norm:0.00010776784620247781 max memory_allocated 29271.62548828125 
[2025-02-23 14:53:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.08277756720781326 norm:0.00010525420657359064 max memory_allocated 29271.62548828125 
[2025-02-23 14:53:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.08267509937286377 norm:0.00010141763050341979 max memory_allocated 29271.62548828125 
[2025-02-23 14:54:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.08259887993335724 norm:9.968246013158932e-05 max memory_allocated 29271.62548828125 
[2025-02-23 14:55:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.08254772424697876 norm:9.568202221998945e-05 max memory_allocated 29271.62548828125 
[2025-02-23 14:56:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.08250122517347336 norm:9.508356015430763e-05 max memory_allocated 29271.62548828125 
[2025-02-23 14:57:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.08243318647146225 norm:9.187773684971035e-05 max memory_allocated 29271.62548828125 
[2025-02-23 14:58:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.08239752799272537 norm:9.135204891208559e-05 max memory_allocated 29271.62548828125 
[2025-02-23 14:58:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.08236722648143768 norm:8.960682316683233e-05 max memory_allocated 29271.62548828125 
[2025-02-23 14:59:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.08231295645236969 norm:8.61378648551181e-05 max memory_allocated 29271.62548828125 
[2025-02-23 15:00:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.08227698504924774 norm:8.424094266956672e-05 max memory_allocated 29271.62548828125 
[2025-02-23 15:01:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.08223260939121246 norm:8.389749564230442e-05 max memory_allocated 29271.62548828125 
[2025-02-23 15:01:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-23 15:02:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.11733057349920273 norm:0.0005824792315252125 max memory_allocated 29271.81298828125 
[2025-02-23 15:03:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.1072775200009346 norm:0.00028428505174815655 max memory_allocated 29271.81298828125 
[2025-02-23 15:04:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.09884636849164963 norm:0.00020108248281758279 max memory_allocated 29271.81298828125 
[2025-02-23 15:05:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.09646287560462952 norm:0.00017529301112517715 max memory_allocated 29271.81298828125 
[2025-02-23 15:05:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.09520207345485687 norm:0.00015286436246242374 max memory_allocated 29271.81298828125 
[2025-02-23 15:06:43 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.09471601247787476 norm:0.0001451920106774196 max memory_allocated 29271.81298828125 
[2025-02-23 15:07:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.09443020820617676 norm:0.0001321122981607914 max memory_allocated 29271.81298828125 
[2025-02-23 15:08:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.09423243999481201 norm:0.00012632379366550595 max memory_allocated 29271.81298828125 
[2025-02-23 15:09:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.09407264739274979 norm:0.00012105724454158917 max memory_allocated 29271.81298828125 
[2025-02-23 15:10:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.09392816573381424 norm:0.00011629665095824748 max memory_allocated 29271.81298828125 
[2025-02-23 15:10:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.09379589557647705 norm:0.00010919028136413544 max memory_allocated 29271.81298828125 
[2025-02-23 15:11:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.09369322657585144 norm:0.00010501538054086268 max memory_allocated 29271.81298828125 
[2025-02-23 15:12:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.09362567216157913 norm:0.00010291529179085046 max memory_allocated 29271.81298828125 
[2025-02-23 15:13:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.09354931861162186 norm:0.00010032291174866259 max memory_allocated 29271.81298828125 
[2025-02-23 15:14:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.09350434690713882 norm:0.00010044934606412426 max memory_allocated 29271.81298828125 
[2025-02-23 15:15:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.09346118569374084 norm:9.939895244315267e-05 max memory_allocated 29271.81298828125 
[2025-02-23 15:15:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.09341675043106079 norm:9.753082122188061e-05 max memory_allocated 29271.81298828125 
[2025-02-23 15:16:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.09335698932409286 norm:9.637489711167291e-05 max memory_allocated 29271.81298828125 
[2025-02-23 15:17:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.09331264346837997 norm:9.42522456170991e-05 max memory_allocated 29271.81298828125 
[2025-02-23 15:18:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.09328342974185944 norm:9.394504741067067e-05 max memory_allocated 29271.81298828125 
[2025-02-23 15:18:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-23 15:19:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.1319035291671753 norm:0.0005244591739028692 max memory_allocated 29272.00048828125 
[2025-02-23 15:20:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.12148398160934448 norm:0.000309517839923501 max memory_allocated 29272.00048828125 
[2025-02-23 15:21:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.11246007680892944 norm:0.00020409452554304153 max memory_allocated 29272.00048828125 
[2025-02-23 15:22:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.10985607653856277 norm:0.00019321890431456268 max memory_allocated 29272.00048828125 
[2025-02-23 15:22:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.10861567407846451 norm:0.0001813518174458295 max memory_allocated 29272.00048828125 
[2025-02-23 15:23:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.10816580802202225 norm:0.00017542182467877865 max memory_allocated 29272.00048828125 
[2025-02-23 15:24:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.10787704586982727 norm:0.0001659584668232128 max memory_allocated 29272.00048828125 
[2025-02-23 15:25:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.10767663270235062 norm:0.00015972617256920785 max memory_allocated 29272.00048828125 
[2025-02-23 15:26:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.10747881233692169 norm:0.00015627959510311484 max memory_allocated 29272.00048828125 
[2025-02-23 15:27:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.107354536652565 norm:0.00015353411436080933 max memory_allocated 29272.00048828125 
[2025-02-23 15:27:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.10724001377820969 norm:0.00014986030873842537 max memory_allocated 29272.00048828125 
[2025-02-23 15:28:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.10713069140911102 norm:0.0001445244997739792 max memory_allocated 29272.00048828125 
[2025-02-23 15:29:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.10701262950897217 norm:0.00014251124230213463 max memory_allocated 29272.00048828125 
[2025-02-23 15:30:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.1069168969988823 norm:0.00014196161646395922 max memory_allocated 29272.00048828125 
[2025-02-23 15:31:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.10682715475559235 norm:0.00013894904986955225 max memory_allocated 29272.00048828125 
[2025-02-23 15:32:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.106753408908844 norm:0.00014016467321198434 max memory_allocated 29272.00048828125 
[2025-02-23 15:33:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.10670043528079987 norm:0.00013743095041718334 max memory_allocated 29272.00048828125 
[2025-02-23 15:33:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.10666523873806 norm:0.00013723387382924557 max memory_allocated 29272.00048828125 
[2025-02-23 15:34:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.1066044345498085 norm:0.00013431372644845396 max memory_allocated 29272.00048828125 
[2025-02-23 15:35:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.1065698117017746 norm:0.00013396117719821632 max memory_allocated 29272.00048828125 
[2025-02-23 15:35:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-23 15:36:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.15008683502674103 norm:0.0006882029701955616 max memory_allocated 29272.18798828125 
[2025-02-23 15:37:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.1386522799730301 norm:0.00038917834172025323 max memory_allocated 29272.18798828125 
[2025-02-23 15:38:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.1289585679769516 norm:0.00023977449745871127 max memory_allocated 29272.18798828125 
[2025-02-23 15:39:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.1260550320148468 norm:0.00022319071285892278 max memory_allocated 29272.18798828125 
[2025-02-23 15:40:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.12472789734601974 norm:0.0002049153408734128 max memory_allocated 29272.18798828125 
[2025-02-23 15:40:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.12422949075698853 norm:0.00019115812028758228 max memory_allocated 29272.18798828125 
[2025-02-23 15:41:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.12393045425415039 norm:0.00018251135770697147 max memory_allocated 29272.18798828125 
[2025-02-23 15:42:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.12369044870138168 norm:0.00017200376896653324 max memory_allocated 29272.18798828125 
[2025-02-23 15:43:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.12349772453308105 norm:0.00016485890955664217 max memory_allocated 29272.18798828125 
[2025-02-23 15:44:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.12333591282367706 norm:0.0001638781395740807 max memory_allocated 29272.18798828125 
[2025-02-23 15:45:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.12315435707569122 norm:0.0001551128225401044 max memory_allocated 29272.18798828125 
[2025-02-23 15:45:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.12302634119987488 norm:0.00015404869918711483 max memory_allocated 29272.18798828125 
[2025-02-23 15:46:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.12290970981121063 norm:0.00014928124437574297 max memory_allocated 29272.18798828125 
[2025-02-23 15:47:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.12278690934181213 norm:0.00014560326235368848 max memory_allocated 29272.18798828125 
[2025-02-23 15:48:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.12270022183656693 norm:0.00014282335177995265 max memory_allocated 29272.18798828125 
[2025-02-23 15:49:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.12262129038572311 norm:0.0001406755909556523 max memory_allocated 29272.18798828125 
[2025-02-23 15:50:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.12255273759365082 norm:0.0001388803357258439 max memory_allocated 29272.18798828125 
[2025-02-23 15:50:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.1225259006023407 norm:0.00013729215424973518 max memory_allocated 29272.18798828125 
[2025-02-23 15:51:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.12247820943593979 norm:0.00013426422083284706 max memory_allocated 29272.18798828125 
[2025-02-23 15:52:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.12243051081895828 norm:0.00013343006139621139 max memory_allocated 29272.18798828125 
[2025-02-23 15:52:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-23 15:53:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.16565023362636566 norm:0.0004940753569826484 max memory_allocated 29272.37548828125 
[2025-02-23 15:54:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.1546846479177475 norm:0.00029977207304909825 max memory_allocated 29272.37548828125 
[2025-02-23 15:55:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.14481410384178162 norm:0.0002084489824483171 max memory_allocated 29272.37548828125 
[2025-02-23 15:56:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.14174440503120422 norm:0.00018514656403567642 max memory_allocated 29272.37548828125 
[2025-02-23 15:57:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.14044791460037231 norm:0.00017452322936151177 max memory_allocated 29272.37548828125 
[2025-02-23 15:57:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.13996490836143494 norm:0.00016203909763135016 max memory_allocated 29272.37548828125 
[2025-02-23 15:58:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.13966430723667145 norm:0.0001470231218263507 max memory_allocated 29272.37548828125 
[2025-02-23 15:59:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.13943816721439362 norm:0.00014149121125228703 max memory_allocated 29272.37548828125 
[2025-02-23 16:00:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.13926304876804352 norm:0.00014158562407828867 max memory_allocated 29272.37548828125 
[2025-02-23 16:01:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.13909883797168732 norm:0.00014396377082448453 max memory_allocated 29272.37548828125 
[2025-02-23 16:02:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.13895025849342346 norm:0.00013684661826118827 max memory_allocated 29272.37548828125 
[2025-02-23 16:02:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.13882844150066376 norm:0.0001338834990747273 max memory_allocated 29272.37548828125 
[2025-02-23 16:03:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.13871946930885315 norm:0.0001377677544951439 max memory_allocated 29272.37548828125 
[2025-02-23 16:04:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.13860347867012024 norm:0.00013733685773331672 max memory_allocated 29272.37548828125 
[2025-02-23 16:05:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.13853394985198975 norm:0.0001327952486462891 max memory_allocated 29272.37548828125 
[2025-02-23 16:06:21 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.13845452666282654 norm:0.00012894968676846474 max memory_allocated 29272.37548828125 
[2025-02-23 16:07:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.13837124407291412 norm:0.0001278333511436358 max memory_allocated 29272.37548828125 
[2025-02-23 16:08:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.13829383254051208 norm:0.00012544819037429988 max memory_allocated 29272.37548828125 
[2025-02-23 16:08:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.13825225830078125 norm:0.00012580250040628016 max memory_allocated 29272.37548828125 
[2025-02-23 16:09:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.13818441331386566 norm:0.0001246388565050438 max memory_allocated 29272.37548828125 
[2025-02-23 16:09:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-23 16:10:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.1871916651725769 norm:0.0006184853846207261 max memory_allocated 29272.56298828125 
[2025-02-23 16:11:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.17469675838947296 norm:0.00035311083775013685 max memory_allocated 29272.56298828125 
[2025-02-23 16:12:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.16336388885974884 norm:0.00022381902090273798 max memory_allocated 29272.56298828125 
[2025-02-23 16:13:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.16017039120197296 norm:0.0002019257954088971 max memory_allocated 29272.56298828125 
[2025-02-23 16:14:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.15889999270439148 norm:0.00018740408995654434 max memory_allocated 29272.56298828125 
[2025-02-23 16:15:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.15843811631202698 norm:0.000177799302036874 max memory_allocated 29272.56298828125 
[2025-02-23 16:15:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.1581021249294281 norm:0.00016946459072642028 max memory_allocated 29272.56298828125 
[2025-02-23 16:16:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.15783147513866425 norm:0.00016691342170815915 max memory_allocated 29272.56298828125 
[2025-02-23 16:17:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.1576014906167984 norm:0.00016078224871307611 max memory_allocated 29272.56298828125 
[2025-02-23 16:18:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.157391756772995 norm:0.00015569268725812435 max memory_allocated 29272.56298828125 
[2025-02-23 16:19:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.1572355329990387 norm:0.00015140204050112516 max memory_allocated 29272.56298828125 
[2025-02-23 16:20:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.15712091326713562 norm:0.00015428299957420677 max memory_allocated 29272.56298828125 
[2025-02-23 16:20:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.15697059035301208 norm:0.00014757497410755605 max memory_allocated 29272.56298828125 
[2025-02-23 16:21:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.15690523386001587 norm:0.00014726886001881212 max memory_allocated 29272.56298828125 
[2025-02-23 16:22:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.15680329501628876 norm:0.00014287586964201182 max memory_allocated 29272.56298828125 
[2025-02-23 16:23:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.15669424831867218 norm:0.0001381172041874379 max memory_allocated 29272.56298828125 
[2025-02-23 16:24:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.15662021934986115 norm:0.00013992935419082642 max memory_allocated 29272.56298828125 
[2025-02-23 16:25:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.15654678642749786 norm:0.000137019669637084 max memory_allocated 29272.56298828125 
[2025-02-23 16:25:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.15648648142814636 norm:0.00014090188778936863 max memory_allocated 29272.56298828125 
[2025-02-23 16:26:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.1564340740442276 norm:0.00013557850616052747 max memory_allocated 29272.56298828125 
[2025-02-23 16:27:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-23 16:27:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.20619161427021027 norm:0.000527838827110827 max memory_allocated 29272.75048828125 
[2025-02-23 16:28:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.1935925930738449 norm:0.0003118674212601036 max memory_allocated 29272.75048828125 
[2025-02-23 16:29:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.18191277980804443 norm:0.0001987030846066773 max memory_allocated 29272.75048828125 
[2025-02-23 16:30:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.1785372793674469 norm:0.00017554251826368272 max memory_allocated 29272.75048828125 
[2025-02-23 16:31:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.1773211508989334 norm:0.0001621352566871792 max memory_allocated 29272.75048828125 
[2025-02-23 16:32:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.17686936259269714 norm:0.0001550383458379656 max memory_allocated 29272.75048828125 
[2025-02-23 16:32:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.17659513652324677 norm:0.00014828649000264704 max memory_allocated 29272.75048828125 
[2025-02-23 16:33:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.17635706067085266 norm:0.0001435153972124681 max memory_allocated 29272.75048828125 
[2025-02-23 16:34:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.17612427473068237 norm:0.00013848427624907345 max memory_allocated 29272.75048828125 
[2025-02-23 16:35:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.1759457141160965 norm:0.00013664481230080128 max memory_allocated 29272.75048828125 
[2025-02-23 16:36:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.17578662931919098 norm:0.00013457032036967576 max memory_allocated 29272.75048828125 
[2025-02-23 16:37:09 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.1756497025489807 norm:0.00013161072274670005 max memory_allocated 29272.75048828125 
[2025-02-23 16:37:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.17553043365478516 norm:0.00013028083776589483 max memory_allocated 29272.75048828125 
[2025-02-23 16:38:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.17541101574897766 norm:0.00012695803889073431 max memory_allocated 29272.75048828125 
[2025-02-23 16:39:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.1753089427947998 norm:0.00012584051000885665 max memory_allocated 29272.75048828125 
[2025-02-23 16:40:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.17521919310092926 norm:0.00012399230035953224 max memory_allocated 29272.75048828125 
[2025-02-23 16:41:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.1751476228237152 norm:0.00012255783076398075 max memory_allocated 29272.75048828125 
[2025-02-23 16:42:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.17506170272827148 norm:0.00012092811812181026 max memory_allocated 29272.75048828125 
[2025-02-23 16:43:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.17500042915344238 norm:0.00011950583575526252 max memory_allocated 29272.75048828125 
[2025-02-23 16:43:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.17492465674877167 norm:0.00011950543557759374 max memory_allocated 29272.75048828125 
[2025-02-23 16:44:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-23 16:45:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.23282866179943085 norm:0.0005387927521951497 max memory_allocated 29272.93798828125 
[2025-02-23 16:45:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.21834275126457214 norm:0.0003411152574699372 max memory_allocated 29272.93798828125 
[2025-02-23 16:46:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.20465052127838135 norm:0.00022525845270138234 max memory_allocated 29272.93798828125 
[2025-02-23 16:47:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.20092973113059998 norm:0.00020165875321254134 max memory_allocated 29272.93798828125 
[2025-02-23 16:48:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.19968634843826294 norm:0.00018694711616262794 max memory_allocated 29272.93798828125 
[2025-02-23 16:49:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.19918334484100342 norm:0.00017853235476650298 max memory_allocated 29272.93798828125 
[2025-02-23 16:50:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.19882507622241974 norm:0.00017158522678073496 max memory_allocated 29272.93798828125 
[2025-02-23 16:50:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.198555126786232 norm:0.00016364538169000298 max memory_allocated 29272.93798828125 
[2025-02-23 16:51:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.1982942819595337 norm:0.00015606496890541166 max memory_allocated 29272.93798828125 
[2025-02-23 16:52:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.19808512926101685 norm:0.00015261326916515827 max memory_allocated 29272.93798828125 
[2025-02-23 16:53:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.19789642095565796 norm:0.00015310909657273442 max memory_allocated 29272.93798828125 
[2025-02-23 16:54:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.19772574305534363 norm:0.0001500767539255321 max memory_allocated 29272.93798828125 
[2025-02-23 16:55:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.19756901264190674 norm:0.00014603919407818466 max memory_allocated 29272.93798828125 
[2025-02-23 16:55:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.19744667410850525 norm:0.00014351599384099245 max memory_allocated 29272.93798828125 
[2025-02-23 16:56:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.1973339170217514 norm:0.0001454601442674175 max memory_allocated 29272.93798828125 
[2025-02-23 16:57:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.19723942875862122 norm:0.00014255692076403648 max memory_allocated 29272.93798828125 
[2025-02-23 16:58:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.19717174768447876 norm:0.00014025226118974388 max memory_allocated 29272.93798828125 
[2025-02-23 16:59:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.1971026510000229 norm:0.00013889296678826213 max memory_allocated 29272.93798828125 
[2025-02-23 17:00:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.19703838229179382 norm:0.00014173699310049415 max memory_allocated 29272.93798828125 
[2025-02-23 17:00:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.19699576497077942 norm:0.00014099648979026824 max memory_allocated 29272.93798828125 
[2025-02-23 17:01:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-23 17:02:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.25808560848236084 norm:0.0008230357198044658 max memory_allocated 29273.12548828125 
[2025-02-23 17:02:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.24198976159095764 norm:0.0004255405510775745 max memory_allocated 29273.12548828125 
[2025-02-23 17:03:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.22749318182468414 norm:0.00035107030998915434 max memory_allocated 29273.12548828125 
[2025-02-23 17:04:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.22344620525836945 norm:0.00027146347565576434 max memory_allocated 29273.12548828125 
[2025-02-23 17:05:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.22214533388614655 norm:0.0002613024553284049 max memory_allocated 29273.12548828125 
[2025-02-23 17:06:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.22168506681919098 norm:0.0002409605513093993 max memory_allocated 29273.12548828125 
[2025-02-23 17:07:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.22126679122447968 norm:0.00022687262389808893 max memory_allocated 29273.12548828125 
[2025-02-23 17:07:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.22098678350448608 norm:0.00023900711676105857 max memory_allocated 29273.12548828125 
[2025-02-23 17:08:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.22076264023780823 norm:0.0002254027931485325 max memory_allocated 29273.12548828125 
[2025-02-23 17:09:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.2205602377653122 norm:0.00026305890060029924 max memory_allocated 29273.12548828125 
[2025-02-23 17:10:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.2203652560710907 norm:0.00028819291037507355 max memory_allocated 29273.12548828125 
[2025-02-23 17:11:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.22019818425178528 norm:0.0002561263390816748 max memory_allocated 29273.12548828125 
[2025-02-23 17:12:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.22005505859851837 norm:0.00022791323135606945 max memory_allocated 29273.12548828125 
[2025-02-23 17:13:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.21989582479000092 norm:0.00021308664872776717 max memory_allocated 29273.12548828125 
[2025-02-23 17:13:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.21975575387477875 norm:0.00021087862842250615 max memory_allocated 29273.12548828125 
[2025-02-23 17:14:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.21967454254627228 norm:0.00022100901696830988 max memory_allocated 29273.12548828125 
[2025-02-23 17:15:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.21964143216609955 norm:0.00021428518812172115 max memory_allocated 29273.12548828125 
[2025-02-23 17:16:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.2195172756910324 norm:0.00021121914323884994 max memory_allocated 29273.12548828125 
[2025-02-23 17:17:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.21943449974060059 norm:0.00024159293388947845 max memory_allocated 29273.12548828125 
[2025-02-23 17:18:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.21936160326004028 norm:0.00020440710068214685 max memory_allocated 29273.12548828125 
[2025-02-23 17:18:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-23 17:19:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.2784939408302307 norm:0.0004552045138552785 max memory_allocated 29273.31298828125 
[2025-02-23 17:20:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.26366186141967773 norm:0.00029056199127808213 max memory_allocated 29273.31298828125 
[2025-02-23 17:20:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.2491646260023117 norm:0.00018212040595244616 max memory_allocated 29273.31298828125 
[2025-02-23 17:21:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.24526673555374146 norm:0.00015594555588904768 max memory_allocated 29273.31298828125 
[2025-02-23 17:22:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.24415701627731323 norm:0.000150282314280048 max memory_allocated 29273.31298828125 
[2025-02-23 17:23:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.24373486638069153 norm:0.00014921510592103004 max memory_allocated 29273.31298828125 
[2025-02-23 17:24:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.24341510236263275 norm:0.00014193532115314156 max memory_allocated 29273.31298828125 
[2025-02-23 17:25:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.2431642860174179 norm:0.00013874945580027997 max memory_allocated 29273.31298828125 
[2025-02-23 17:25:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.24292495846748352 norm:0.00013641227269545197 max memory_allocated 29273.31298828125 
[2025-02-23 17:26:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.2427271008491516 norm:0.00013583328109234571 max memory_allocated 29273.31298828125 
[2025-02-23 17:27:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.24255278706550598 norm:0.00013565653352998197 max memory_allocated 29273.31298828125 
[2025-02-23 17:28:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.24237841367721558 norm:0.00013279753329697996 max memory_allocated 29273.31298828125 
[2025-02-23 17:29:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.24223318696022034 norm:0.0001328025246039033 max memory_allocated 29273.31298828125 
[2025-02-23 17:30:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.2420971542596817 norm:0.00012998651072848588 max memory_allocated 29273.31298828125 
[2025-02-23 17:30:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.24199031293392181 norm:0.00012954654812347144 max memory_allocated 29273.31298828125 
[2025-02-23 17:31:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.24188032746315002 norm:0.00012887203774880618 max memory_allocated 29273.31298828125 
[2025-02-23 17:32:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.24178999662399292 norm:0.00012813566718250513 max memory_allocated 29273.31298828125 
[2025-02-23 17:33:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.2417169213294983 norm:0.00012870800856035203 max memory_allocated 29273.31298828125 
[2025-02-23 17:34:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.24161292612552643 norm:0.0001266097679035738 max memory_allocated 29273.31298828125 
[2025-02-23 17:35:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.24152161180973053 norm:0.00012517270806711167 max memory_allocated 29273.31298828125 
[2025-02-23 17:35:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-23 17:36:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.30648690462112427 norm:0.0005580995930358768 max memory_allocated 29273.50048828125 
[2025-02-23 17:37:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.29082924127578735 norm:0.00035194199881516397 max memory_allocated 29273.50048828125 
[2025-02-23 17:37:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.2756062150001526 norm:0.00022968828852754086 max memory_allocated 29273.50048828125 
[2025-02-23 17:38:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.27148449420928955 norm:0.00020792771829292178 max memory_allocated 29273.50048828125 
[2025-02-23 17:39:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.27036285400390625 norm:0.00018844772421289235 max memory_allocated 29273.50048828125 
[2025-02-23 17:40:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.26987355947494507 norm:0.00017826365365181118 max memory_allocated 29273.50048828125 
[2025-02-23 17:41:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.26950299739837646 norm:0.00016954740567598492 max memory_allocated 29273.50048828125 
[2025-02-23 17:42:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.2692003846168518 norm:0.00016701314598321915 max memory_allocated 29273.50048828125 
[2025-02-23 17:42:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.26895636320114136 norm:0.00018315798661205918 max memory_allocated 29273.50048828125 
[2025-02-23 17:43:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.26872873306274414 norm:0.00016935575695242733 max memory_allocated 29273.50048828125 
[2025-02-23 17:44:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.2685118317604065 norm:0.0001616182562429458 max memory_allocated 29273.50048828125 
[2025-02-23 17:45:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.26831573247909546 norm:0.0001577549846842885 max memory_allocated 29273.50048828125 
[2025-02-23 17:46:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.2681618332862854 norm:0.00016010670515242964 max memory_allocated 29273.50048828125 
[2025-02-23 17:47:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.26801979541778564 norm:0.00015896289551164955 max memory_allocated 29273.50048828125 
[2025-02-23 17:48:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.26790258288383484 norm:0.00015827550669200718 max memory_allocated 29273.50048828125 
[2025-02-23 17:48:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.26781103014945984 norm:0.00015663637896068394 max memory_allocated 29273.50048828125 
[2025-02-23 17:49:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.2677190899848938 norm:0.00015781872207298875 max memory_allocated 29273.50048828125 
[2025-02-23 17:50:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.2676319479942322 norm:0.00016069176490418613 max memory_allocated 29273.50048828125 
[2025-02-23 17:51:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.2675614655017853 norm:0.00015766169235575944 max memory_allocated 29273.50048828125 
[2025-02-23 17:52:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.26746493577957153 norm:0.0001559863449074328 max memory_allocated 29273.50048828125 
[2025-02-23 17:52:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-23 17:53:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.3346790671348572 norm:0.00043002713937312365 max memory_allocated 29273.68798828125 
[2025-02-23 17:54:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.3182321786880493 norm:0.00028722520801238716 max memory_allocated 29273.68798828125 
[2025-02-23 17:55:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.30249327421188354 norm:0.00020017831411678344 max memory_allocated 29273.68798828125 
[2025-02-23 17:55:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.2981263995170593 norm:0.00018222283688373864 max memory_allocated 29273.68798828125 
[2025-02-23 17:56:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.29703712463378906 norm:0.00017520820256322622 max memory_allocated 29273.68798828125 
[2025-02-23 17:57:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.2965543270111084 norm:0.00016659198445267975 max memory_allocated 29273.68798828125 
[2025-02-23 17:58:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.29615098237991333 norm:0.0001603219861863181 max memory_allocated 29273.68798828125 
[2025-02-23 17:59:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.2958030700683594 norm:0.00015901669394224882 max memory_allocated 29273.68798828125 
[2025-02-23 18:00:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.2955021262168884 norm:0.0001516961638117209 max memory_allocated 29273.68798828125 
[2025-02-23 18:00:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.2952479124069214 norm:0.0001500387879787013 max memory_allocated 29273.68798828125 
[2025-02-23 18:01:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.2950429618358612 norm:0.00014981992717366666 max memory_allocated 29273.68798828125 
[2025-02-23 18:02:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.29485955834388733 norm:0.00014830402506049722 max memory_allocated 29273.68798828125 
[2025-02-23 18:03:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.29473352432250977 norm:0.00014925847062841058 max memory_allocated 29273.68798828125 
[2025-02-23 18:04:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.29460224509239197 norm:0.0001459399936720729 max memory_allocated 29273.68798828125 
[2025-02-23 18:05:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.294468492269516 norm:0.0001461329375160858 max memory_allocated 29273.68798828125 
[2025-02-23 18:05:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.2943330705165863 norm:0.00014690947136841714 max memory_allocated 29273.68798828125 
[2025-02-23 18:06:48 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.29422521591186523 norm:0.00014172389637678862 max memory_allocated 29273.68798828125 
[2025-02-23 18:07:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.29414767026901245 norm:0.00014703170745633543 max memory_allocated 29273.68798828125 
[2025-02-23 18:08:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.2940834164619446 norm:0.00015427885227836668 max memory_allocated 29273.68798828125 
[2025-02-23 18:09:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.29399678111076355 norm:0.00015586731024086475 max memory_allocated 29273.68798828125 
[2025-02-23 18:09:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-23 18:10:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.362444669008255 norm:0.0005070619517937303 max memory_allocated 29273.87548828125 
[2025-02-23 18:11:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.3457372188568115 norm:0.0003245784027967602 max memory_allocated 29273.87548828125 
[2025-02-23 18:12:08 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.32937100529670715 norm:0.00021397980162873864 max memory_allocated 29273.87548828125 
[2025-02-23 18:12:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.3251006603240967 norm:0.00018635856395121664 max memory_allocated 29273.87548828125 
[2025-02-23 18:13:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.3240921199321747 norm:0.0001719960564514622 max memory_allocated 29273.87548828125 
[2025-02-23 18:14:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.3235872983932495 norm:0.00016293118824250996 max memory_allocated 29273.87548828125 
[2025-02-23 18:15:30 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.3231891393661499 norm:0.00015610548143740743 max memory_allocated 29273.87548828125 
[2025-02-23 18:16:20 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.32286885380744934 norm:0.0001513509632786736 max memory_allocated 29273.87548828125 
[2025-02-23 18:17:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.32258111238479614 norm:0.00014618948625866324 max memory_allocated 29273.87548828125 
[2025-02-23 18:18:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.32231056690216064 norm:0.0001421270426362753 max memory_allocated 29273.87548828125 
[2025-02-23 18:18:51 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.32208526134490967 norm:0.0001391315890941769 max memory_allocated 29273.87548828125 
[2025-02-23 18:19:41 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.32191336154937744 norm:0.00013827123621013016 max memory_allocated 29273.87548828125 
[2025-02-23 18:20:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.321774423122406 norm:0.00013641342229675502 max memory_allocated 29273.87548828125 
[2025-02-23 18:21:22 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.3216947615146637 norm:0.00013827471411786973 max memory_allocated 29273.87548828125 
[2025-02-23 18:22:12 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.32156622409820557 norm:0.00013840034080203623 max memory_allocated 29273.87548828125 
[2025-02-23 18:23:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.32145383954048157 norm:0.00013857308658771217 max memory_allocated 29273.87548828125 
[2025-02-23 18:23:53 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.3213285803794861 norm:0.00013839587336406112 max memory_allocated 29273.87548828125 
[2025-02-23 18:24:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.32119035720825195 norm:0.00013671867782250047 max memory_allocated 29273.87548828125 
[2025-02-23 18:25:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.3211180865764618 norm:0.00013361035962589085 max memory_allocated 29273.87548828125 
[2025-02-23 18:26:24 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.3210117518901825 norm:0.00013347658386919647 max memory_allocated 29273.87548828125 
[2025-02-23 18:26:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-23 18:27:33 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.3970368206501007 norm:0.0006187741528265178 max memory_allocated 29274.06298828125 
[2025-02-23 18:28:23 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.3791007995605469 norm:0.000383045437047258 max memory_allocated 29274.06298828125 
[2025-02-23 18:29:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.362088680267334 norm:0.0002660246391315013 max memory_allocated 29274.06298828125 
[2025-02-23 18:30:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.3576831519603729 norm:0.00022241588158067316 max memory_allocated 29274.06298828125 
[2025-02-23 18:30:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.3565816879272461 norm:0.00020376902830321342 max memory_allocated 29274.06298828125 
[2025-02-23 18:31:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.3560025095939636 norm:0.000201249320525676 max memory_allocated 29274.06298828125 
[2025-02-23 18:32:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.3555811941623688 norm:0.00019762999727390707 max memory_allocated 29274.06298828125 
[2025-02-23 18:33:26 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.35522714257240295 norm:0.0001886069221654907 max memory_allocated 29274.06298828125 
[2025-02-23 18:34:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.35490861535072327 norm:0.00018776838260237128 max memory_allocated 29274.06298828125 
[2025-02-23 18:35:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.3546542823314667 norm:0.00018961139721795917 max memory_allocated 29274.06298828125 
[2025-02-23 18:35:57 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.35442614555358887 norm:0.00018886211910285056 max memory_allocated 29274.06298828125 
[2025-02-23 18:36:48 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.35422492027282715 norm:0.00018415623344480991 max memory_allocated 29274.06298828125 
[2025-02-23 18:37:38 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.3540302813053131 norm:0.00018341149552725255 max memory_allocated 29274.06298828125 
[2025-02-23 18:38:28 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.3538885712623596 norm:0.0001800540048861876 max memory_allocated 29274.06298828125 
[2025-02-23 18:39:19 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.353719562292099 norm:0.00017952958296518773 max memory_allocated 29274.06298828125 
[2025-02-23 18:40:09 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.35359030961990356 norm:0.0001792120310710743 max memory_allocated 29274.06298828125 
[2025-02-23 18:41:00 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.3534759283065796 norm:0.00018030907085631043 max memory_allocated 29274.06298828125 
[2025-02-23 18:41:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.3533782362937927 norm:0.00017779662448447198 max memory_allocated 29274.06298828125 
[2025-02-23 18:42:41 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.35328614711761475 norm:0.00017416114860679954 max memory_allocated 29274.06298828125 
[2025-02-23 18:43:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.35315877199172974 norm:0.00017364085942972451 max memory_allocated 29274.06298828125 
[2025-02-23 18:43:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-23 18:44:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.4413397014141083 norm:0.0009330342290922999 max memory_allocated 29274.25048828125 
[2025-02-23 18:45:30 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.4205246865749359 norm:0.0005564602906815708 max memory_allocated 29274.25048828125 
[2025-02-23 18:46:21 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.4006439447402954 norm:0.00030381628312170506 max memory_allocated 29274.25048828125 
[2025-02-23 18:47:11 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.3958961069583893 norm:0.00025530788116157055 max memory_allocated 29274.25048828125 
[2025-02-23 18:48:01 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.3948312997817993 norm:0.00024272050359286368 max memory_allocated 29274.25048828125 
[2025-02-23 18:48:52 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.3942386507987976 norm:0.0002343180967727676 max memory_allocated 29274.25048828125 
[2025-02-23 18:49:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.39380377531051636 norm:0.0002259242901345715 max memory_allocated 29274.25048828125 
[2025-02-23 18:50:32 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.393406480550766 norm:0.00022005135542713106 max memory_allocated 29274.25048828125 
[2025-02-23 18:51:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.39306315779685974 norm:0.00021052334341220558 max memory_allocated 29274.25048828125 
[2025-02-23 18:52:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.39282968640327454 norm:0.00020736976875923574 max memory_allocated 29274.25048828125 
[2025-02-23 18:53:04 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.39260613918304443 norm:0.00020457703794818372 max memory_allocated 29274.25048828125 
[2025-02-23 18:53:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.39241570234298706 norm:0.00020684508490376174 max memory_allocated 29274.25048828125 
[2025-02-23 18:54:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.3922433853149414 norm:0.00020543474238365889 max memory_allocated 29274.25048828125 
[2025-02-23 18:55:35 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.39206457138061523 norm:0.00020637782290577888 max memory_allocated 29274.25048828125 
[2025-02-23 18:56:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.3918997347354889 norm:0.00020379142370074987 max memory_allocated 29274.25048828125 
[2025-02-23 18:57:16 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.3917422890663147 norm:0.00020006562408525497 max memory_allocated 29274.25048828125 
[2025-02-23 18:58:06 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.3916082978248596 norm:0.00019818109285552055 max memory_allocated 29274.25048828125 
[2025-02-23 18:58:56 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.3915339708328247 norm:0.0001971721649169922 max memory_allocated 29274.25048828125 
[2025-02-23 18:59:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.3914458751678467 norm:0.00019548613636288792 max memory_allocated 29274.25048828125 
[2025-02-23 19:00:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.3913411796092987 norm:0.00019420043099671602 max memory_allocated 29274.25048828125 
[2025-02-23 19:00:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-23 19:01:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.4838047921657562 norm:0.0009668671991676092 max memory_allocated 29274.43798828125 
[2025-02-23 19:02:37 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.4614022374153137 norm:0.0006013497477397323 max memory_allocated 29274.43798828125 
[2025-02-23 19:03:27 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.44060438871383667 norm:0.00035906003904528916 max memory_allocated 29274.43798828125 
[2025-02-23 19:04:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.4359336495399475 norm:0.00031334199593402445 max memory_allocated 29274.43798828125 
[2025-02-23 19:05:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.43472719192504883 norm:0.0002952341747004539 max memory_allocated 29274.43798828125 
[2025-02-23 19:05:58 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.4340021312236786 norm:0.0002777108456939459 max memory_allocated 29274.43798828125 
[2025-02-23 19:06:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.43341630697250366 norm:0.00026644012541510165 max memory_allocated 29274.43798828125 
[2025-02-23 19:07:39 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.43297433853149414 norm:0.0002634309639688581 max memory_allocated 29274.43798828125 
[2025-02-23 19:08:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.43257468938827515 norm:0.0002574837999418378 max memory_allocated 29274.43798828125 
[2025-02-23 19:09:20 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.4322158098220825 norm:0.0002591987722553313 max memory_allocated 29274.43798828125 
[2025-02-23 19:10:10 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.4319785237312317 norm:0.000256254366831854 max memory_allocated 29274.43798828125 
[2025-02-23 19:11:01 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.4317469596862793 norm:0.0002493948268238455 max memory_allocated 29274.43798828125 
[2025-02-23 19:11:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.4315526485443115 norm:0.00025106334942393005 max memory_allocated 29274.43798828125 
[2025-02-23 19:12:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.4313715696334839 norm:0.0002512556384317577 max memory_allocated 29274.43798828125 
[2025-02-23 19:13:32 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.43115320801734924 norm:0.00024510204093530774 max memory_allocated 29274.43798828125 
[2025-02-23 19:14:22 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.43098464608192444 norm:0.000246465380769223 max memory_allocated 29274.43798828125 
[2025-02-23 19:15:12 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.4308542311191559 norm:0.00024402476265095174 max memory_allocated 29274.43798828125 
[2025-02-23 19:16:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.4307551383972168 norm:0.0002446373400744051 max memory_allocated 29274.43798828125 
[2025-02-23 19:16:53 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.43064698576927185 norm:0.00024296678020618856 max memory_allocated 29274.43798828125 
[2025-02-23 19:17:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.4305119514465332 norm:0.00023667105415370315 max memory_allocated 29274.43798828125 
[2025-02-23 19:17:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-23 19:18:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 19:18:53 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.5351390838623047 norm:0.014188641682267189 max memory_allocated 29274.77001953125 
[2025-02-23 19:19:43 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.5091436505317688 norm:0.010835696943104267 max memory_allocated 29274.77001953125 
[2025-02-23 19:20:34 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.48529160022735596 norm:0.008109606802463531 max memory_allocated 29274.77001953125 
[2025-02-23 19:21:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.4803076982498169 norm:0.006782884243875742 max memory_allocated 29274.77001953125 
[2025-02-23 19:22:15 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.4787498116493225 norm:0.005546963773667812 max memory_allocated 29274.77001953125 
[2025-02-23 19:23:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.4777137041091919 norm:0.0045788101851940155 max memory_allocated 29274.77001953125 
[2025-02-23 19:23:56 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.47709453105926514 norm:0.004197429399937391 max memory_allocated 29274.77001953125 
[2025-02-23 19:24:47 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.4766661524772644 norm:0.004190518520772457 max memory_allocated 29274.77001953125 
[2025-02-23 19:25:37 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.4762900769710541 norm:0.003949109930545092 max memory_allocated 29274.77001953125 
[2025-02-23 19:26:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.4759314954280853 norm:0.003840521676465869 max memory_allocated 29274.77001953125 
[2025-02-23 19:27:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.47550106048583984 norm:0.003683917922899127 max memory_allocated 29274.77001953125 
[2025-02-23 19:28:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.47525662183761597 norm:0.003540501929819584 max memory_allocated 29274.77001953125 
[2025-02-23 19:29:00 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.47494104504585266 norm:0.0034849606454372406 max memory_allocated 29274.77001953125 
[2025-02-23 19:29:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.4747293293476105 norm:0.0033627927768975496 max memory_allocated 29274.77001953125 
[2025-02-23 19:30:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.47450923919677734 norm:0.0033516990952193737 max memory_allocated 29274.77001953125 
[2025-02-23 19:31:32 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.4743734300136566 norm:0.003245333908125758 max memory_allocated 29274.77001953125 
[2025-02-23 19:32:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.4741279184818268 norm:0.003183629596605897 max memory_allocated 29274.77001953125 
[2025-02-23 19:33:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.47400903701782227 norm:0.00313384598121047 max memory_allocated 29274.77001953125 
[2025-02-23 19:34:04 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.47377702593803406 norm:0.003103983122855425 max memory_allocated 29274.77001953125 
[2025-02-23 19:34:54 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.47373154759407043 norm:0.0030457456596195698 max memory_allocated 29274.77001953125 
[2025-02-23 19:35:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-23 19:35:13 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 19:36:03 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.6148442625999451 norm:0.020775865763425827 max memory_allocated 29274.95751953125 
[2025-02-23 19:36:54 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.5785979628562927 norm:0.014657897874712944 max memory_allocated 29274.95751953125 
[2025-02-23 19:37:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.5475875735282898 norm:0.010328960604965687 max memory_allocated 29274.95751953125 
[2025-02-23 19:38:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.5409695506095886 norm:0.008575567044317722 max memory_allocated 29274.95751953125 
[2025-02-23 19:39:26 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.539125382900238 norm:0.007091518957167864 max memory_allocated 29274.95751953125 
[2025-02-23 19:40:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.5377367734909058 norm:0.005887553095817566 max memory_allocated 29274.95751953125 
[2025-02-23 19:41:07 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.5367243885993958 norm:0.005059013143181801 max memory_allocated 29274.95751953125 
[2025-02-23 19:41:58 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.5361171364784241 norm:0.0049324422143399715 max memory_allocated 29274.95751953125 
[2025-02-23 19:42:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.5355070233345032 norm:0.0048132650554180145 max memory_allocated 29274.95751953125 
[2025-02-23 19:43:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.5350193977355957 norm:0.004435320384800434 max memory_allocated 29274.95751953125 
[2025-02-23 19:44:29 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.534633219242096 norm:0.00428824732080102 max memory_allocated 29274.95751953125 
[2025-02-23 19:45:20 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.5342848896980286 norm:0.0041328310035169125 max memory_allocated 29274.95751953125 
[2025-02-23 19:46:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.5338714122772217 norm:0.003941347822546959 max memory_allocated 29274.95751953125 
[2025-02-23 19:47:01 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.5336222052574158 norm:0.0038296629209071398 max memory_allocated 29274.95751953125 
[2025-02-23 19:47:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.5334614515304565 norm:0.0037816131953150034 max memory_allocated 29274.95751953125 
[2025-02-23 19:48:42 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.5333499908447266 norm:0.003687892574816942 max memory_allocated 29274.95751953125 
[2025-02-23 19:49:32 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.5332280397415161 norm:0.003671603975817561 max memory_allocated 29274.95751953125 
[2025-02-23 19:50:22 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.5330818295478821 norm:0.0037376144900918007 max memory_allocated 29274.95751953125 
[2025-02-23 19:51:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.5328717827796936 norm:0.0037086335942149162 max memory_allocated 29274.95751953125 
[2025-02-23 19:52:03 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.5327364802360535 norm:0.003653530031442642 max memory_allocated 29274.95751953125 
[2025-02-23 19:52:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-23 19:52:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 19:53:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.7518796920776367 norm:0.03521941602230072 max memory_allocated 29275.14501953125 
[2025-02-23 19:54:03 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.7048519849777222 norm:0.022829126566648483 max memory_allocated 29275.14501953125 
[2025-02-23 19:54:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.669508695602417 norm:0.016751134768128395 max memory_allocated 29275.14501953125 
[2025-02-23 19:55:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.6604309678077698 norm:0.014360415749251842 max memory_allocated 29275.14501953125 
[2025-02-23 19:56:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.6570070385932922 norm:0.012675910256803036 max memory_allocated 29275.14501953125 
[2025-02-23 19:57:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.6546675562858582 norm:0.0111180879175663 max memory_allocated 29275.14501953125 
[2025-02-23 19:58:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.6529792547225952 norm:0.010112586431205273 max memory_allocated 29275.14501953125 
[2025-02-23 19:59:05 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.6515142917633057 norm:0.008963148109614849 max memory_allocated 29275.14501953125 
[2025-02-23 19:59:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.6503012180328369 norm:0.008068892173469067 max memory_allocated 29275.14501953125 
[2025-02-23 20:00:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.6491696238517761 norm:0.007767410948872566 max memory_allocated 29275.14501953125 
[2025-02-23 20:01:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.6488885879516602 norm:0.008116135373711586 max memory_allocated 29275.14501953125 
[2025-02-23 20:02:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.648629367351532 norm:0.007865832187235355 max memory_allocated 29275.14501953125 
[2025-02-23 20:03:18 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.6477386951446533 norm:0.007330002263188362 max memory_allocated 29275.14501953125 
[2025-02-23 20:04:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.6473052501678467 norm:0.007114142179489136 max memory_allocated 29275.14501953125 
[2025-02-23 20:04:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.647210955619812 norm:0.007133017759770155 max memory_allocated 29275.14501953125 
[2025-02-23 20:05:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.6472285985946655 norm:0.007138635031878948 max memory_allocated 29275.14501953125 
[2025-02-23 20:06:40 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.6467629075050354 norm:0.007053361739963293 max memory_allocated 29275.14501953125 
[2025-02-23 20:07:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.6458447575569153 norm:0.006394227501004934 max memory_allocated 29275.14501953125 
[2025-02-23 20:08:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.645901620388031 norm:0.006202147342264652 max memory_allocated 29275.14501953125 
[2025-02-23 20:09:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.6456292271614075 norm:0.006115231662988663 max memory_allocated 29275.14501953125 
[2025-02-23 20:09:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-23 20:09:30 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 20:10:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:1.2042417526245117 norm:0.09946240484714508 max memory_allocated 29275.33251953125 
[2025-02-23 20:11:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:1.1075271368026733 norm:0.07007897645235062 max memory_allocated 29275.33251953125 
[2025-02-23 20:12:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:1.0316579341888428 norm:0.04121022671461105 max memory_allocated 29275.33251953125 
[2025-02-23 20:12:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:1.012880802154541 norm:0.038468241691589355 max memory_allocated 29275.33251953125 
[2025-02-23 20:13:42 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:1.0042027235031128 norm:0.03606037795543671 max memory_allocated 29275.33251953125 
[2025-02-23 20:14:33 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.9986016750335693 norm:0.034530725330114365 max memory_allocated 29275.33251953125 
[2025-02-23 20:15:23 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.9947463274002075 norm:0.032483283430337906 max memory_allocated 29275.33251953125 
[2025-02-23 20:16:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.9913074970245361 norm:0.03117472678422928 max memory_allocated 29275.33251953125 
[2025-02-23 20:17:05 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.9897185564041138 norm:0.030683301389217377 max memory_allocated 29275.33251953125 
[2025-02-23 20:17:56 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.9884417057037354 norm:0.03217598795890808 max memory_allocated 29275.33251953125 
[2025-02-23 20:18:46 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.98689866065979 norm:0.029407881200313568 max memory_allocated 29275.33251953125 
[2025-02-23 20:19:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.9852863550186157 norm:0.02930266223847866 max memory_allocated 29275.33251953125 
[2025-02-23 20:20:27 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.9845444560050964 norm:0.02834484912455082 max memory_allocated 29275.33251953125 
[2025-02-23 20:21:17 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.9819507002830505 norm:0.02678009867668152 max memory_allocated 29275.33251953125 
[2025-02-23 20:22:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.9802801012992859 norm:0.02548571303486824 max memory_allocated 29275.33251953125 
[2025-02-23 20:22:59 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.97858065366745 norm:0.0247575081884861 max memory_allocated 29275.33251953125 
[2025-02-23 20:23:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.9780197739601135 norm:0.025113612413406372 max memory_allocated 29275.33251953125 
[2025-02-23 20:24:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.9777334928512573 norm:0.024939510971307755 max memory_allocated 29275.33251953125 
[2025-02-23 20:25:32 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.97792649269104 norm:0.025662969797849655 max memory_allocated 29275.33251953125 
[2025-02-23 20:26:23 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.9778653383255005 norm:0.025218982249498367 max memory_allocated 29275.33251953125 
[2025-02-23 20:26:38 root] (main_calibration.py 365): INFO 40981.57788825035
[2025-02-23 20:27:39 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-23 20:29:37 root] (main_calibration.py 158): INFO wikitext2 : 5.225572109222412
[2025-02-23 20:29:37 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-23 20:32:43 root] (main_calibration.py 158): INFO c4 : 6.777667045593262
[2025-02-23 22:24:27 root] (main_calibration.py 169): INFO {'wikitext2': 5.225572109222412, 'c4': 6.777667045593262, 'results': {'winogrande': {'acc': 0.6906077348066298, 'acc_stderr': 0.012991329330823006}, 'piqa': {'acc': 0.7867247007616975, 'acc_stderr': 0.009557121225861338, 'acc_norm': 0.7867247007616975, 'acc_norm_stderr': 0.009557121225861345}, 'boolq': {'acc': 0.7009174311926606, 'acc_stderr': 0.008007953102490839}, 'hellaswag': {'acc': 0.584744074885481, 'acc_stderr': 0.004917590378138209, 'acc_norm': 0.7524397530372435, 'acc_norm_stderr': 0.0043071285732852365}, 'arc_challenge': {'acc': 0.4257679180887372, 'acc_stderr': 0.014449464278868809, 'acc_norm': 0.4180887372013652, 'acc_norm_stderr': 0.014413988396996074}, 'arc_easy': {'acc': 0.7407407407407407, 'acc_stderr': 0.008992251535805523, 'acc_norm': 0.5896464646464646, 'acc_norm_stderr': 0.010093531255765453}}, 'versions': {'winogrande': 0, 'piqa': 0, 'boolq': 1, 'hellaswag': 0, 'arc_challenge': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
