[2025-03-09 10:21:14 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-13b-hf-w4a7', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-09 10:24:11 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-09 10:24:11 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-09 10:24:12 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-09 10:24:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-09 10:24:17 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:25:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.018362250179052353 norm:0.015908988192677498 max memory_allocated 29268.02001953125 
[2025-03-09 10:25:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.01127156987786293 norm:0.008248645812273026 max memory_allocated 29268.02001953125 
[2025-03-09 10:26:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.007955502718687057 norm:0.005035084206610918 max memory_allocated 29268.02001953125 
[2025-03-09 10:27:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.006937077268958092 norm:0.0038639940321445465 max memory_allocated 29268.02001953125 
[2025-03-09 10:28:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.006615808233618736 norm:0.003329660976305604 max memory_allocated 29268.02001953125 
[2025-03-09 10:28:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.006438014097511768 norm:0.002902564126998186 max memory_allocated 29268.02001953125 
[2025-03-09 10:29:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.006210872903466225 norm:0.002478716429322958 max memory_allocated 29268.02001953125 
[2025-03-09 10:30:19 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.006095013581216335 norm:0.0021875882521271706 max memory_allocated 29268.02001953125 
[2025-03-09 10:31:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.005943511612713337 norm:0.0018959096632897854 max memory_allocated 29268.02001953125 
[2025-03-09 10:31:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00593848992139101 norm:0.0017437600763514638 max memory_allocated 29268.02001953125 
[2025-03-09 10:32:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.005936348810791969 norm:0.0016281312564387918 max memory_allocated 29268.02001953125 
[2025-03-09 10:33:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.005857500247657299 norm:0.0014349897392094135 max memory_allocated 29268.02001953125 
[2025-03-09 10:34:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.005787070840597153 norm:0.0012617144966498017 max memory_allocated 29268.02001953125 
[2025-03-09 10:34:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.005846569314599037 norm:0.0012136056320741773 max memory_allocated 29268.02001953125 
[2025-03-09 10:35:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.005833654664456844 norm:0.0010955428006127477 max memory_allocated 29268.02001953125 
[2025-03-09 10:36:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.005703330971300602 norm:0.0010147633729502559 max memory_allocated 29268.02001953125 
[2025-03-09 10:37:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.005880171433091164 norm:0.0010283502051606774 max memory_allocated 29268.02001953125 
[2025-03-09 10:37:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.005710006691515446 norm:0.0009094196138903499 max memory_allocated 29268.02001953125 
[2025-03-09 10:38:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.005715665873140097 norm:0.0009006431209854782 max memory_allocated 29268.02001953125 
[2025-03-09 10:39:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.005721070803701878 norm:0.0008443202823400497 max memory_allocated 29268.02001953125 
[2025-03-09 10:39:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-09 10:39:38 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:40:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.02922755479812622 norm:0.016830209642648697 max memory_allocated 29268.02001953125 
[2025-03-09 10:41:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0179155170917511 norm:0.009459588676691055 max memory_allocated 29268.02001953125 
[2025-03-09 10:41:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.013297039084136486 norm:0.00594218960031867 max memory_allocated 29268.02001953125 
[2025-03-09 10:42:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.011937801726162434 norm:0.004900700878351927 max memory_allocated 29268.02001953125 
[2025-03-09 10:43:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.01143459603190422 norm:0.004226965364068747 max memory_allocated 29268.02001953125 
[2025-03-09 10:44:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.011149514466524124 norm:0.0038015246391296387 max memory_allocated 29268.02001953125 
[2025-03-09 10:44:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.010929176583886147 norm:0.003496783785521984 max memory_allocated 29268.02001953125 
[2025-03-09 10:45:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.010745878331363201 norm:0.003215049859136343 max memory_allocated 29268.02001953125 
[2025-03-09 10:46:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.010593528859317303 norm:0.0029532883781939745 max memory_allocated 29268.02001953125 
[2025-03-09 10:47:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.010452834889292717 norm:0.002733896253630519 max memory_allocated 29268.02001953125 
[2025-03-09 10:47:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.01035698689520359 norm:0.0024958294816315174 max memory_allocated 29268.02001953125 
[2025-03-09 10:48:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.010229181498289108 norm:0.0022580898366868496 max memory_allocated 29268.02001953125 
[2025-03-09 10:49:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.010112248361110687 norm:0.002050078706815839 max memory_allocated 29268.02001953125 
[2025-03-09 10:50:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.010053759440779686 norm:0.001877964474260807 max memory_allocated 29268.02001953125 
[2025-03-09 10:50:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.009997849352657795 norm:0.0017175145912915468 max memory_allocated 29268.02001953125 
[2025-03-09 10:51:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.009964868426322937 norm:0.0015762937255203724 max memory_allocated 29268.02001953125 
[2025-03-09 10:52:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.00992556381970644 norm:0.0014220234006643295 max memory_allocated 29268.02001953125 
[2025-03-09 10:53:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.009911294095218182 norm:0.0012996022123843431 max memory_allocated 29268.02001953125 
[2025-03-09 10:53:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.00989553052932024 norm:0.001281274133361876 max memory_allocated 29268.02001953125 
[2025-03-09 10:54:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.009960168972611427 norm:0.0013299371348693967 max memory_allocated 29268.02001953125 
[2025-03-09 10:54:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-09 10:54:58 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:55:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.029705844819545746 norm:0.008097133599221706 max memory_allocated 29268.02001953125 
[2025-03-09 10:56:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.02443714067339897 norm:0.007696171756833792 max memory_allocated 29268.02001953125 
[2025-03-09 10:57:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.02121490240097046 norm:0.00543231051415205 max memory_allocated 29268.02001953125 
[2025-03-09 10:57:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.02020346000790596 norm:0.004912072792649269 max memory_allocated 29268.02001953125 
[2025-03-09 10:58:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.019915251061320305 norm:0.004759281873703003 max memory_allocated 29268.02001953125 
[2025-03-09 10:59:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.01913512498140335 norm:0.004517420660704374 max memory_allocated 29268.02001953125 
[2025-03-09 11:00:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.018922284245491028 norm:0.004330350551754236 max memory_allocated 29268.02001953125 
[2025-03-09 11:01:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.018594039604067802 norm:0.004040516447275877 max memory_allocated 29268.02001953125 
[2025-03-09 11:01:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.018426302820444107 norm:0.004010477103292942 max memory_allocated 29268.02001953125 
[2025-03-09 11:02:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.018557671457529068 norm:0.004149203654378653 max memory_allocated 29268.02001953125 
[2025-03-09 11:03:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.018583467230200768 norm:0.004087327979505062 max memory_allocated 29268.02001953125 
[2025-03-09 11:04:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.018407123163342476 norm:0.0038166088052093983 max memory_allocated 29268.02001953125 
[2025-03-09 11:04:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.018289431929588318 norm:0.0036199335008859634 max memory_allocated 29268.02001953125 
[2025-03-09 11:05:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.01826412044465542 norm:0.0036435772199183702 max memory_allocated 29268.02001953125 
[2025-03-09 11:06:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.018207784742116928 norm:0.003462701803073287 max memory_allocated 29268.02001953125 
[2025-03-09 11:07:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.01833464577794075 norm:0.0036724861711263657 max memory_allocated 29268.02001953125 
[2025-03-09 11:07:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.018174778670072556 norm:0.003310268046334386 max memory_allocated 29268.02001953125 
[2025-03-09 11:08:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.018291771411895752 norm:0.0033266721293330193 max memory_allocated 29268.02001953125 
[2025-03-09 11:09:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.018305080011487007 norm:0.003328559920191765 max memory_allocated 29268.02001953125 
[2025-03-09 11:10:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.018325086683034897 norm:0.003174212295562029 max memory_allocated 29268.02001953125 
[2025-03-09 11:10:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-09 11:11:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.03874444216489792 norm:0.0052404217422008514 max memory_allocated 29268.43798828125 
[2025-03-09 11:11:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.02832057885825634 norm:0.0015562532935291529 max memory_allocated 29268.43798828125 
[2025-03-09 11:12:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.023387350142002106 norm:0.0007990848389454186 max memory_allocated 29268.43798828125 
[2025-03-09 11:13:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.021830344572663307 norm:0.0005576087278313935 max memory_allocated 29268.43798828125 
[2025-03-09 11:14:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.021044593304395676 norm:0.0004487536789383739 max memory_allocated 29268.43798828125 
[2025-03-09 11:14:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.020544787868857384 norm:0.00040569048724137247 max memory_allocated 29268.43798828125 
[2025-03-09 11:15:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.020156748592853546 norm:0.0002926186425611377 max memory_allocated 29268.43798828125 
[2025-03-09 11:16:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.019909914582967758 norm:0.0002770282153505832 max memory_allocated 29268.43798828125 
[2025-03-09 11:17:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.019762644544243813 norm:0.00023637997219339013 max memory_allocated 29268.43798828125 
[2025-03-09 11:17:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.01971384882926941 norm:0.00024788297014310956 max memory_allocated 29268.43798828125 
[2025-03-09 11:18:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.019678039476275444 norm:0.000210514641366899 max memory_allocated 29268.43798828125 
[2025-03-09 11:19:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.019579092040657997 norm:0.00017294551071245223 max memory_allocated 29268.43798828125 
[2025-03-09 11:20:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.019541941583156586 norm:0.00017916398064699024 max memory_allocated 29268.43798828125 
[2025-03-09 11:20:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.019520437344908714 norm:0.0001688820484559983 max memory_allocated 29268.43798828125 
[2025-03-09 11:21:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.01948639750480652 norm:0.0001578802039148286 max memory_allocated 29268.43798828125 
[2025-03-09 11:22:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.01953393779695034 norm:0.0001622780691832304 max memory_allocated 29268.43798828125 
[2025-03-09 11:23:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.01959150657057762 norm:0.00017805503739509732 max memory_allocated 29268.43798828125 
[2025-03-09 11:23:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.019553273916244507 norm:0.00014351733261719346 max memory_allocated 29268.43798828125 
[2025-03-09 11:24:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.01952720247209072 norm:0.00014477659715339541 max memory_allocated 29268.43798828125 
[2025-03-09 11:25:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0195261649787426 norm:0.00016040171612985432 max memory_allocated 29268.43798828125 
[2025-03-09 11:25:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-09 11:26:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.05036863312125206 norm:0.012201696634292603 max memory_allocated 29268.62548828125 
[2025-03-09 11:27:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.03579320013523102 norm:0.0029936928767710924 max memory_allocated 29268.62548828125 
[2025-03-09 11:27:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.02869323641061783 norm:0.0013578783255070448 max memory_allocated 29268.62548828125 
[2025-03-09 11:28:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.026732072234153748 norm:0.0009585315128788352 max memory_allocated 29268.62548828125 
[2025-03-09 11:29:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.025687873363494873 norm:0.0006928020156919956 max memory_allocated 29268.62548828125 
[2025-03-09 11:30:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.024994082748889923 norm:0.0006138042663224041 max memory_allocated 29268.62548828125 
[2025-03-09 11:30:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.02466582879424095 norm:0.0005693173152394593 max memory_allocated 29268.62548828125 
[2025-03-09 11:31:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.02439700812101364 norm:0.0004593499470502138 max memory_allocated 29268.62548828125 
[2025-03-09 11:32:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.024238722398877144 norm:0.0004116259515285492 max memory_allocated 29268.62548828125 
[2025-03-09 11:33:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.02415509521961212 norm:0.00038916891207918525 max memory_allocated 29268.62548828125 
[2025-03-09 11:33:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.02414703369140625 norm:0.00036801546229980886 max memory_allocated 29268.62548828125 
[2025-03-09 11:34:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.02408098243176937 norm:0.0003048715880140662 max memory_allocated 29268.62548828125 
[2025-03-09 11:35:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.02404588647186756 norm:0.0002959144185297191 max memory_allocated 29268.62548828125 
[2025-03-09 11:36:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.024123238399624825 norm:0.00027460005367174745 max memory_allocated 29268.62548828125 
[2025-03-09 11:36:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.02406253293156624 norm:0.0002625731867738068 max memory_allocated 29268.62548828125 
[2025-03-09 11:37:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.024061216041445732 norm:0.00024378244415856898 max memory_allocated 29268.62548828125 
[2025-03-09 11:38:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.024068281054496765 norm:0.00024063579621724784 max memory_allocated 29268.62548828125 
[2025-03-09 11:39:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.024050671607255936 norm:0.00020322715863585472 max memory_allocated 29268.62548828125 
[2025-03-09 11:39:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.024014335125684738 norm:0.00019362899183761328 max memory_allocated 29268.62548828125 
[2025-03-09 11:40:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.02398449182510376 norm:0.00018114007252734154 max memory_allocated 29268.62548828125 
[2025-03-09 11:40:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-09 11:41:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.062381718307733536 norm:0.015031683258712292 max memory_allocated 29268.81298828125 
[2025-03-09 11:42:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.04115155711770058 norm:0.0031914261635392904 max memory_allocated 29268.81298828125 
[2025-03-09 11:43:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.03253823518753052 norm:0.0013278963742777705 max memory_allocated 29268.81298828125 
[2025-03-09 11:43:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.030218428000807762 norm:0.0008401766535826027 max memory_allocated 29268.81298828125 
[2025-03-09 11:44:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.029080312699079514 norm:0.0006917202263139188 max memory_allocated 29268.81298828125 
[2025-03-09 11:45:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.02834140881896019 norm:0.0006070585222914815 max memory_allocated 29268.81298828125 
[2025-03-09 11:46:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.027852864935994148 norm:0.0005238759331405163 max memory_allocated 29268.81298828125 
[2025-03-09 11:46:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.027592185884714127 norm:0.0004832406993955374 max memory_allocated 29268.81298828125 
[2025-03-09 11:47:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.027438301593065262 norm:0.0004327970091253519 max memory_allocated 29268.81298828125 
[2025-03-09 11:48:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.027291756123304367 norm:0.0003831740468740463 max memory_allocated 29268.81298828125 
[2025-03-09 11:49:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.027180172502994537 norm:0.0003522810875438154 max memory_allocated 29268.81298828125 
[2025-03-09 11:49:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.027104884386062622 norm:0.00035083602415397763 max memory_allocated 29268.81298828125 
[2025-03-09 11:50:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.027014458552002907 norm:0.0002855204220395535 max memory_allocated 29268.81298828125 
[2025-03-09 11:51:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.02695334330201149 norm:0.0002672328264452517 max memory_allocated 29268.81298828125 
[2025-03-09 11:52:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.026920899748802185 norm:0.00027777947252616286 max memory_allocated 29268.81298828125 
[2025-03-09 11:52:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.026924967765808105 norm:0.00024997766013257205 max memory_allocated 29268.81298828125 
[2025-03-09 11:53:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.02692567929625511 norm:0.0002504982112441212 max memory_allocated 29268.81298828125 
[2025-03-09 11:54:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.026870155707001686 norm:0.0002245101786684245 max memory_allocated 29268.81298828125 
[2025-03-09 11:55:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.02684260904788971 norm:0.00022209454618860036 max memory_allocated 29268.81298828125 
[2025-03-09 11:55:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.02682523801922798 norm:0.00021084534819237888 max memory_allocated 29268.81298828125 
[2025-03-09 11:56:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-09 11:56:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.05996863916516304 norm:0.0031418269500136375 max memory_allocated 29269.00048828125 
[2025-03-09 11:57:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.04563943296670914 norm:0.0009713283507153392 max memory_allocated 29269.00048828125 
[2025-03-09 11:58:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.038065165281295776 norm:0.0005391141166910529 max memory_allocated 29269.00048828125 
[2025-03-09 11:59:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.03559006005525589 norm:0.0004401140904519707 max memory_allocated 29269.00048828125 
[2025-03-09 11:59:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.03438737243413925 norm:0.0003898283466696739 max memory_allocated 29269.00048828125 
[2025-03-09 12:00:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.03369373828172684 norm:0.0003328102466184646 max memory_allocated 29269.00048828125 
[2025-03-09 12:01:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.03340626135468483 norm:0.0003156591847073287 max memory_allocated 29269.00048828125 
[2025-03-09 12:02:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.03319921717047691 norm:0.0002958569675683975 max memory_allocated 29269.00048828125 
[2025-03-09 12:02:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.03308076411485672 norm:0.00028694485081359744 max memory_allocated 29269.00048828125 
[2025-03-09 12:03:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.03297713026404381 norm:0.0002796310873236507 max memory_allocated 29269.00048828125 
[2025-03-09 12:04:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.032848700881004333 norm:0.0002874771016649902 max memory_allocated 29269.00048828125 
[2025-03-09 12:05:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.03273661434650421 norm:0.000272276607574895 max memory_allocated 29269.00048828125 
[2025-03-09 12:05:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.03271174803376198 norm:0.00029939276282675564 max memory_allocated 29269.00048828125 
[2025-03-09 12:06:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0326634906232357 norm:0.00028335704701021314 max memory_allocated 29269.00048828125 
[2025-03-09 12:07:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.03262560814619064 norm:0.0002556053514126688 max memory_allocated 29269.00048828125 
[2025-03-09 12:08:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.03262558951973915 norm:0.0002520861162338406 max memory_allocated 29269.00048828125 
[2025-03-09 12:08:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.032628148794174194 norm:0.00028144169482402503 max memory_allocated 29269.00048828125 
[2025-03-09 12:09:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.03256816416978836 norm:0.0002462767588440329 max memory_allocated 29269.00048828125 
[2025-03-09 12:10:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.03255695104598999 norm:0.00024732507881708443 max memory_allocated 29269.00048828125 
[2025-03-09 12:11:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.03257112577557564 norm:0.0002146320475731045 max memory_allocated 29269.00048828125 
[2025-03-09 12:11:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-09 12:12:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.061622995883226395 norm:0.0030536013655364513 max memory_allocated 29269.18798828125 
[2025-03-09 12:12:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.04843802750110626 norm:0.0009897974086925387 max memory_allocated 29269.18798828125 
[2025-03-09 12:13:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.04021497443318367 norm:0.00041952921310439706 max memory_allocated 29269.18798828125 
[2025-03-09 12:14:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.03774939104914665 norm:0.00028129576821811497 max memory_allocated 29269.18798828125 
[2025-03-09 12:15:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.03661351650953293 norm:0.00023925919958855957 max memory_allocated 29269.18798828125 
[2025-03-09 12:15:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.03593278303742409 norm:0.00022115762112662196 max memory_allocated 29269.18798828125 
[2025-03-09 12:16:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.035492610186338425 norm:0.00020342660718597472 max memory_allocated 29269.18798828125 
[2025-03-09 12:17:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.035338208079338074 norm:0.00020248786313459277 max memory_allocated 29269.18798828125 
[2025-03-09 12:18:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.03524423763155937 norm:0.00019559270003810525 max memory_allocated 29269.18798828125 
[2025-03-09 12:18:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.035126302391290665 norm:0.00017281741020269692 max memory_allocated 29269.18798828125 
[2025-03-09 12:19:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.035058945417404175 norm:0.00016530427092220634 max memory_allocated 29269.18798828125 
[2025-03-09 12:20:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.03500347584486008 norm:0.0001607052981853485 max memory_allocated 29269.18798828125 
[2025-03-09 12:21:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.03499238193035126 norm:0.00016050557314883918 max memory_allocated 29269.18798828125 
[2025-03-09 12:21:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.03495875746011734 norm:0.00015747969155199826 max memory_allocated 29269.18798828125 
[2025-03-09 12:22:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.034960728138685226 norm:0.00015215603343676776 max memory_allocated 29269.18798828125 
[2025-03-09 12:23:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.03493547812104225 norm:0.0001437524042557925 max memory_allocated 29269.18798828125 
[2025-03-09 12:24:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0349072702229023 norm:0.00014277786249294877 max memory_allocated 29269.18798828125 
[2025-03-09 12:24:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0348738357424736 norm:0.00013751359074376523 max memory_allocated 29269.18798828125 
[2025-03-09 12:25:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.03487800061702728 norm:0.00013878352183382958 max memory_allocated 29269.18798828125 
[2025-03-09 12:26:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.03488164395093918 norm:0.0001452052383683622 max memory_allocated 29269.18798828125 
[2025-03-09 12:26:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-09 12:27:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0640491172671318 norm:0.0021937512792646885 max memory_allocated 29269.37548828125 
[2025-03-09 12:28:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.050823695957660675 norm:0.0007766220369376242 max memory_allocated 29269.37548828125 
[2025-03-09 12:28:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.04360389709472656 norm:0.0003694699262268841 max memory_allocated 29269.37548828125 
[2025-03-09 12:29:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0411933995783329 norm:0.0002851393655873835 max memory_allocated 29269.37548828125 
[2025-03-09 12:30:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0400552973151207 norm:0.00025552528677508235 max memory_allocated 29269.37548828125 
[2025-03-09 12:31:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.03930153697729111 norm:0.00021802273113280535 max memory_allocated 29269.37548828125 
[2025-03-09 12:32:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.03889983519911766 norm:0.00021071964874863625 max memory_allocated 29269.37548828125 
[2025-03-09 12:32:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.03865059092640877 norm:0.00019941775826737285 max memory_allocated 29269.37548828125 
[2025-03-09 12:33:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.03850026801228523 norm:0.00017944147111847997 max memory_allocated 29269.37548828125 
[2025-03-09 12:34:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.038428835570812225 norm:0.00016969069838523865 max memory_allocated 29269.37548828125 
[2025-03-09 12:35:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.038424164056777954 norm:0.00017512460181023926 max memory_allocated 29269.37548828125 
[2025-03-09 12:35:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.038416795432567596 norm:0.0001609538885531947 max memory_allocated 29269.37548828125 
[2025-03-09 12:36:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.038385502994060516 norm:0.00015032166265882552 max memory_allocated 29269.37548828125 
[2025-03-09 12:37:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.03835367411375046 norm:0.00015237803745549172 max memory_allocated 29269.37548828125 
[2025-03-09 12:38:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.03834155201911926 norm:0.0001537524804007262 max memory_allocated 29269.37548828125 
[2025-03-09 12:38:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.03832098841667175 norm:0.00015397592505905777 max memory_allocated 29269.37548828125 
[2025-03-09 12:39:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.038278982043266296 norm:0.00014813028974458575 max memory_allocated 29269.37548828125 
[2025-03-09 12:40:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.03829135000705719 norm:0.0001545962004456669 max memory_allocated 29269.37548828125 
[2025-03-09 12:41:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.03829682618379593 norm:0.00014766516687814146 max memory_allocated 29269.37548828125 
[2025-03-09 12:41:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.03829566016793251 norm:0.0001525991247035563 max memory_allocated 29269.37548828125 
[2025-03-09 12:41:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-09 12:42:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.06613414734601974 norm:0.001982680754736066 max memory_allocated 29269.56298828125 
[2025-03-09 12:43:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.053764570504426956 norm:0.0006305078277364373 max memory_allocated 29269.56298828125 
[2025-03-09 12:44:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.04624754935503006 norm:0.0002853222540579736 max memory_allocated 29269.56298828125 
[2025-03-09 12:45:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.04399329796433449 norm:0.0002175960544263944 max memory_allocated 29269.56298828125 
[2025-03-09 12:45:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.04294092208147049 norm:0.0002016739163082093 max memory_allocated 29269.56298828125 
[2025-03-09 12:46:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.042354293167591095 norm:0.0001931147853611037 max memory_allocated 29269.56298828125 
[2025-03-09 12:47:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.042017191648483276 norm:0.0001783567131496966 max memory_allocated 29269.56298828125 
[2025-03-09 12:48:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.04184402525424957 norm:0.00016645384312141687 max memory_allocated 29269.56298828125 
[2025-03-09 12:48:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.04171071946620941 norm:0.0001628081372473389 max memory_allocated 29269.56298828125 
[2025-03-09 12:49:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.04161490872502327 norm:0.0001537781354272738 max memory_allocated 29269.56298828125 
[2025-03-09 12:50:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.04157072305679321 norm:0.00014295923756435513 max memory_allocated 29269.56298828125 
[2025-03-09 12:51:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0415409654378891 norm:0.00013929630222264677 max memory_allocated 29269.56298828125 
[2025-03-09 12:51:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.04148690775036812 norm:0.00012524289195425808 max memory_allocated 29269.56298828125 
[2025-03-09 12:52:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.041467100381851196 norm:0.0001307296333834529 max memory_allocated 29269.56298828125 
[2025-03-09 12:53:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0414687804877758 norm:0.0001312837121076882 max memory_allocated 29269.56298828125 
[2025-03-09 12:54:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.041456643491983414 norm:0.00013280900020617992 max memory_allocated 29269.56298828125 
[2025-03-09 12:54:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.04143747687339783 norm:0.0001335971464868635 max memory_allocated 29269.56298828125 
[2025-03-09 12:55:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.041426174342632294 norm:0.0001312164677074179 max memory_allocated 29269.56298828125 
[2025-03-09 12:56:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.04142718017101288 norm:0.00013347543426789343 max memory_allocated 29269.56298828125 
[2025-03-09 12:57:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.04143229126930237 norm:0.00013327282795216888 max memory_allocated 29269.56298828125 
[2025-03-09 12:57:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-09 12:58:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.06935988366603851 norm:0.0029218916315585375 max memory_allocated 29269.75048828125 
[2025-03-09 12:58:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.05727729946374893 norm:0.0009563813218846917 max memory_allocated 29269.75048828125 
[2025-03-09 12:59:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0496930256485939 norm:0.0003942821640521288 max memory_allocated 29269.75048828125 
[2025-03-09 13:00:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.04730711877346039 norm:0.00027096562553197145 max memory_allocated 29269.75048828125 
[2025-03-09 13:01:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.04628051444888115 norm:0.00024279789067804813 max memory_allocated 29269.75048828125 
[2025-03-09 13:01:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.045695528388023376 norm:0.0002229818783234805 max memory_allocated 29269.75048828125 
[2025-03-09 13:02:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.04535016790032387 norm:0.00020914831839036196 max memory_allocated 29269.75048828125 
[2025-03-09 13:03:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.04517336189746857 norm:0.00019132267334498465 max memory_allocated 29269.75048828125 
[2025-03-09 13:04:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0450572669506073 norm:0.00017641010344959795 max memory_allocated 29269.75048828125 
[2025-03-09 13:04:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.04499271884560585 norm:0.00016522244550287724 max memory_allocated 29269.75048828125 
[2025-03-09 13:05:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.044960904866456985 norm:0.0001614725188119337 max memory_allocated 29269.75048828125 
[2025-03-09 13:06:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.04491578787565231 norm:0.00015177062596194446 max memory_allocated 29269.75048828125 
[2025-03-09 13:07:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.04483690485358238 norm:0.00014216479030437768 max memory_allocated 29269.75048828125 
[2025-03-09 13:07:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.04481123015284538 norm:0.00014134400407783687 max memory_allocated 29269.75048828125 
[2025-03-09 13:08:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.044814761728048325 norm:0.00014127411122899503 max memory_allocated 29269.75048828125 
[2025-03-09 13:09:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.04484580084681511 norm:0.00013591305469162762 max memory_allocated 29269.75048828125 
[2025-03-09 13:10:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.04484180733561516 norm:0.00013414036948233843 max memory_allocated 29269.75048828125 
[2025-03-09 13:10:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.044834088534116745 norm:0.00013133123866282403 max memory_allocated 29269.75048828125 
[2025-03-09 13:11:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.044808611273765564 norm:0.00012653895828407258 max memory_allocated 29269.75048828125 
[2025-03-09 13:12:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.044824160635471344 norm:0.00012654140300583094 max memory_allocated 29269.75048828125 
[2025-03-09 13:12:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-09 13:13:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.07202674448490143 norm:0.001616993802599609 max memory_allocated 29269.93798828125 
[2025-03-09 13:14:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.06097335368394852 norm:0.0006465433398261666 max memory_allocated 29269.93798828125 
[2025-03-09 13:14:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.05347423255443573 norm:0.00030485852039419115 max memory_allocated 29269.93798828125 
[2025-03-09 13:15:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.051245518028736115 norm:0.0002179239527322352 max memory_allocated 29269.93798828125 
[2025-03-09 13:16:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.050233952701091766 norm:0.00019376208365429193 max memory_allocated 29269.93798828125 
[2025-03-09 13:17:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.04971274733543396 norm:0.00017915490025188774 max memory_allocated 29269.93798828125 
[2025-03-09 13:17:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.04935254901647568 norm:0.00016060221241787076 max memory_allocated 29269.93798828125 
[2025-03-09 13:18:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.04914262518286705 norm:0.0001457148464396596 max memory_allocated 29269.93798828125 
[2025-03-09 13:19:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0490233413875103 norm:0.00013579369988292456 max memory_allocated 29269.93798828125 
[2025-03-09 13:20:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.04892628267407417 norm:0.00013171962928026915 max memory_allocated 29269.93798828125 
[2025-03-09 13:20:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.04890421777963638 norm:0.00012685172259807587 max memory_allocated 29269.93798828125 
[2025-03-09 13:21:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.048870667815208435 norm:0.0001244473533006385 max memory_allocated 29269.93798828125 
[2025-03-09 13:22:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.048822175711393356 norm:0.00011845535482279956 max memory_allocated 29269.93798828125 
[2025-03-09 13:23:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.04879022389650345 norm:0.00011821674706880003 max memory_allocated 29269.93798828125 
[2025-03-09 13:23:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.04875759035348892 norm:0.00011238206934649497 max memory_allocated 29269.93798828125 
[2025-03-09 13:24:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.04875662177801132 norm:0.00011751963756978512 max memory_allocated 29269.93798828125 
[2025-03-09 13:25:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.04878133535385132 norm:0.00011842706589959562 max memory_allocated 29269.93798828125 
[2025-03-09 13:26:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0487697534263134 norm:0.00011559631821000949 max memory_allocated 29269.93798828125 
[2025-03-09 13:26:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.048738330602645874 norm:0.00011460354289738461 max memory_allocated 29269.93798828125 
[2025-03-09 13:27:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.04872944951057434 norm:0.00011317654571030289 max memory_allocated 29269.93798828125 
[2025-03-09 13:27:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-09 13:28:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.07506699860095978 norm:0.0015422311844304204 max memory_allocated 29270.12548828125 
[2025-03-09 13:29:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.06462540477514267 norm:0.000601342529989779 max memory_allocated 29270.12548828125 
[2025-03-09 13:30:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.05718069151043892 norm:0.0003096719447057694 max memory_allocated 29270.12548828125 
[2025-03-09 13:30:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.05485401675105095 norm:0.00021882244618609548 max memory_allocated 29270.12548828125 
[2025-03-09 13:31:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.05383380874991417 norm:0.00019004259957000613 max memory_allocated 29270.12548828125 
[2025-03-09 13:32:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.05329098924994469 norm:0.00017202182789333165 max memory_allocated 29270.12548828125 
[2025-03-09 13:33:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0529547780752182 norm:0.0001475247263442725 max memory_allocated 29270.12548828125 
[2025-03-09 13:33:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0527651384472847 norm:0.00013712470536120236 max memory_allocated 29270.12548828125 
[2025-03-09 13:34:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.05265824496746063 norm:0.00013354883412830532 max memory_allocated 29270.12548828125 
[2025-03-09 13:35:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.052592188119888306 norm:0.0001346539647784084 max memory_allocated 29270.12548828125 
[2025-03-09 13:36:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.05252588540315628 norm:0.0001292448869207874 max memory_allocated 29270.12548828125 
[2025-03-09 13:37:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.05247972905635834 norm:0.00012386130401864648 max memory_allocated 29270.12548828125 
[2025-03-09 13:37:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.05245748162269592 norm:0.0001228197361342609 max memory_allocated 29270.12548828125 
[2025-03-09 13:38:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.05242412909865379 norm:0.00011980408453382552 max memory_allocated 29270.12548828125 
[2025-03-09 13:39:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.052386652678251266 norm:0.00011588944471441209 max memory_allocated 29270.12548828125 
[2025-03-09 13:40:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.052396878600120544 norm:0.00011621379235293716 max memory_allocated 29270.12548828125 
[2025-03-09 13:40:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.052377860993146896 norm:0.00011725009244401008 max memory_allocated 29270.12548828125 
[2025-03-09 13:41:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.052383262664079666 norm:0.00011556340905372053 max memory_allocated 29270.12548828125 
[2025-03-09 13:42:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.05236608535051346 norm:0.00011730071855708957 max memory_allocated 29270.12548828125 
[2025-03-09 13:43:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.05237860977649689 norm:0.0001155229692813009 max memory_allocated 29270.12548828125 
[2025-03-09 13:43:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-09 13:44:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.07806184887886047 norm:0.001280401018448174 max memory_allocated 29270.31298828125 
[2025-03-09 13:44:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0679948627948761 norm:0.0005339963245205581 max memory_allocated 29270.31298828125 
[2025-03-09 13:45:32 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.06093239784240723 norm:0.00026778888422995806 max memory_allocated 29270.31298828125 
[2025-03-09 13:46:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.05869636684656143 norm:0.0002157148119295016 max memory_allocated 29270.31298828125 
[2025-03-09 13:47:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.05761237442493439 norm:0.00018667563563212752 max memory_allocated 29270.31298828125 
[2025-03-09 13:47:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.05698561668395996 norm:0.0001685326569713652 max memory_allocated 29270.31298828125 
[2025-03-09 13:48:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.05665821209549904 norm:0.00015794270439073443 max memory_allocated 29270.31298828125 
[2025-03-09 13:49:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.05639556422829628 norm:0.0001435662852600217 max memory_allocated 29270.31298828125 
[2025-03-09 13:50:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.05626891925930977 norm:0.00014171726070344448 max memory_allocated 29270.31298828125 
[2025-03-09 13:50:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.05617755651473999 norm:0.00013644390855915844 max memory_allocated 29270.31298828125 
[2025-03-09 13:51:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.05612020194530487 norm:0.0001313447573920712 max memory_allocated 29270.31298828125 
[2025-03-09 13:52:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.056065477430820465 norm:0.00012796276132576168 max memory_allocated 29270.31298828125 
[2025-03-09 13:53:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.05602668970823288 norm:0.00012240634532645345 max memory_allocated 29270.31298828125 
[2025-03-09 13:53:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.05600563436746597 norm:0.00011992116196779534 max memory_allocated 29270.31298828125 
[2025-03-09 13:54:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.055995188653469086 norm:0.00012020561553072184 max memory_allocated 29270.31298828125 
[2025-03-09 13:55:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.056017834693193436 norm:0.00012411727220751345 max memory_allocated 29270.31298828125 
[2025-03-09 13:56:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.05600525066256523 norm:0.0001238049881067127 max memory_allocated 29270.31298828125 
[2025-03-09 13:56:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.05599244683980942 norm:0.00012053410318912938 max memory_allocated 29270.31298828125 
[2025-03-09 13:57:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.055956125259399414 norm:0.00011923810234293342 max memory_allocated 29270.31298828125 
[2025-03-09 13:58:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.05591365322470665 norm:0.00011599715799093246 max memory_allocated 29270.31298828125 
[2025-03-09 13:58:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-09 13:59:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.08665256947278976 norm:0.0021138053853064775 max memory_allocated 29270.50048828125 
[2025-03-09 14:00:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.07413528859615326 norm:0.0008423083345405757 max memory_allocated 29270.50048828125 
[2025-03-09 14:00:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.06538083404302597 norm:0.00038133544148877263 max memory_allocated 29270.50048828125 
[2025-03-09 14:01:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.06271777302026749 norm:0.0002559950517024845 max memory_allocated 29270.50048828125 
[2025-03-09 14:02:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.06153741106390953 norm:0.00022692809579893947 max memory_allocated 29270.50048828125 
[2025-03-09 14:03:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.06088044494390488 norm:0.0002117637195624411 max memory_allocated 29270.50048828125 
[2025-03-09 14:03:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.06045713648200035 norm:0.0001846583472797647 max memory_allocated 29270.50048828125 
[2025-03-09 14:04:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.06015613302588463 norm:0.00016115771722979844 max memory_allocated 29270.50048828125 
[2025-03-09 14:05:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.06001322716474533 norm:0.00015225161041598767 max memory_allocated 29270.50048828125 
[2025-03-09 14:06:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.059925664216279984 norm:0.00014708495291415602 max memory_allocated 29270.50048828125 
[2025-03-09 14:06:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.059869371354579926 norm:0.00014331376587506384 max memory_allocated 29270.50048828125 
[2025-03-09 14:07:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.05981723591685295 norm:0.00013867275265511125 max memory_allocated 29270.50048828125 
[2025-03-09 14:08:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.059769950807094574 norm:0.0001295710972044617 max memory_allocated 29270.50048828125 
[2025-03-09 14:09:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.05975273251533508 norm:0.00012852600775659084 max memory_allocated 29270.50048828125 
[2025-03-09 14:09:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.05974842980504036 norm:0.0001275018003070727 max memory_allocated 29270.50048828125 
[2025-03-09 14:10:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.05969513580203056 norm:0.00012536907161120325 max memory_allocated 29270.50048828125 
[2025-03-09 14:11:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.05966576188802719 norm:0.00012565741781145334 max memory_allocated 29270.50048828125 
[2025-03-09 14:12:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.05963229760527611 norm:0.00012161038466729224 max memory_allocated 29270.50048828125 
[2025-03-09 14:12:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.05962076038122177 norm:0.00011985400487901643 max memory_allocated 29270.50048828125 
[2025-03-09 14:13:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.05962769687175751 norm:0.00011642469326034188 max memory_allocated 29270.50048828125 
[2025-03-09 14:13:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-09 14:14:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.09634295105934143 norm:0.005700563546270132 max memory_allocated 29270.68798828125 
[2025-03-09 14:15:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.08110621571540833 norm:0.001861202297732234 max memory_allocated 29270.68798828125 
[2025-03-09 14:16:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.07148008793592453 norm:0.0006236121407710016 max memory_allocated 29270.68798828125 
[2025-03-09 14:16:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.06874645501375198 norm:0.00040477930451743305 max memory_allocated 29270.68798828125 
[2025-03-09 14:17:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.06730371713638306 norm:0.00035356663283891976 max memory_allocated 29270.68798828125 
[2025-03-09 14:18:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.06648234277963638 norm:0.0003312577900942415 max memory_allocated 29270.68798828125 
[2025-03-09 14:19:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0659937858581543 norm:0.0002973617520183325 max memory_allocated 29270.68798828125 
[2025-03-09 14:19:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0657075047492981 norm:0.0002802453236654401 max memory_allocated 29270.68798828125 
[2025-03-09 14:20:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.06551746279001236 norm:0.0002596899284981191 max memory_allocated 29270.68798828125 
[2025-03-09 14:21:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.06532871723175049 norm:0.00022585106489714235 max memory_allocated 29270.68798828125 
[2025-03-09 14:22:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.06521707028150558 norm:0.00021514952823054045 max memory_allocated 29270.68798828125 
[2025-03-09 14:22:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.065144382417202 norm:0.00020493096963036805 max memory_allocated 29270.68798828125 
[2025-03-09 14:23:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.06508374959230423 norm:0.00019994087051600218 max memory_allocated 29270.68798828125 
[2025-03-09 14:24:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0650489553809166 norm:0.00018513630493544042 max memory_allocated 29270.68798828125 
[2025-03-09 14:25:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.06494520604610443 norm:0.00017284703790210187 max memory_allocated 29270.68798828125 
[2025-03-09 14:25:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.06490523368120193 norm:0.00016857724403962493 max memory_allocated 29270.68798828125 
[2025-03-09 14:26:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.06488429009914398 norm:0.0001660660345805809 max memory_allocated 29270.68798828125 
[2025-03-09 14:27:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.06488430500030518 norm:0.00016289578343275934 max memory_allocated 29270.68798828125 
[2025-03-09 14:28:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0648682564496994 norm:0.00015615412849001586 max memory_allocated 29270.68798828125 
[2025-03-09 14:28:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.0648125559091568 norm:0.0001522415259387344 max memory_allocated 29270.68798828125 
[2025-03-09 14:29:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-09 14:29:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.09225024282932281 norm:0.0023365160450339317 max memory_allocated 29270.87548828125 
[2025-03-09 14:30:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.08128956705331802 norm:0.0007268271292559803 max memory_allocated 29270.87548828125 
[2025-03-09 14:31:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0742877945303917 norm:0.0002733216970227659 max memory_allocated 29270.87548828125 
[2025-03-09 14:32:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.07225063443183899 norm:0.000194087260751985 max memory_allocated 29270.87548828125 
[2025-03-09 14:32:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.07107686251401901 norm:0.0001792500406736508 max memory_allocated 29270.87548828125 
[2025-03-09 14:33:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.07041920721530914 norm:0.0001730961084831506 max memory_allocated 29270.87548828125 
[2025-03-09 14:34:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.07005752623081207 norm:0.00015168075333349407 max memory_allocated 29270.87548828125 
[2025-03-09 14:35:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.06986194849014282 norm:0.00014106789603829384 max memory_allocated 29270.87548828125 
[2025-03-09 14:35:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.06979207694530487 norm:0.00013751215010415763 max memory_allocated 29270.87548828125 
[2025-03-09 14:36:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.06972196698188782 norm:0.00013165874406695366 max memory_allocated 29270.87548828125 
[2025-03-09 14:37:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.06966273486614227 norm:0.00012772480840794742 max memory_allocated 29270.87548828125 
[2025-03-09 14:38:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.06963291019201279 norm:0.0001272758818231523 max memory_allocated 29270.87548828125 
[2025-03-09 14:38:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.06959167867898941 norm:0.00012078917643520981 max memory_allocated 29270.87548828125 
[2025-03-09 14:39:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.06954912841320038 norm:0.00011916890071006492 max memory_allocated 29270.87548828125 
[2025-03-09 14:40:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.06953558325767517 norm:0.00011649850057438016 max memory_allocated 29270.87548828125 
[2025-03-09 14:41:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.0695153996348381 norm:0.00011616675328696147 max memory_allocated 29270.87548828125 
[2025-03-09 14:41:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.06949356198310852 norm:0.00011493233614601195 max memory_allocated 29270.87548828125 
[2025-03-09 14:42:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.0694764256477356 norm:0.00011220484157092869 max memory_allocated 29270.87548828125 
[2025-03-09 14:43:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.06945061683654785 norm:0.00011186745541635901 max memory_allocated 29270.87548828125 
[2025-03-09 14:44:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.06943558901548386 norm:0.00011224462650716305 max memory_allocated 29270.87548828125 
[2025-03-09 14:44:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-09 14:45:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.09909778088331223 norm:0.001984217669814825 max memory_allocated 29271.06298828125 
[2025-03-09 14:45:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0883077010512352 norm:0.0006757001392543316 max memory_allocated 29271.06298828125 
[2025-03-09 14:46:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.08079379051923752 norm:0.0002954216906800866 max memory_allocated 29271.06298828125 
[2025-03-09 14:47:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.07847632467746735 norm:0.00021221228234935552 max memory_allocated 29271.06298828125 
[2025-03-09 14:48:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.07729541510343552 norm:0.00019855948630720377 max memory_allocated 29271.06298828125 
[2025-03-09 14:48:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.07666344195604324 norm:0.00018640400958247483 max memory_allocated 29271.06298828125 
[2025-03-09 14:49:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.07632806152105331 norm:0.00016813514230307192 max memory_allocated 29271.06298828125 
[2025-03-09 14:50:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.07615016400814056 norm:0.0001504057290730998 max memory_allocated 29271.06298828125 
[2025-03-09 14:51:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.07602882385253906 norm:0.00014590795035474002 max memory_allocated 29271.06298828125 
[2025-03-09 14:51:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.07598365843296051 norm:0.0001434309670003131 max memory_allocated 29271.06298828125 
[2025-03-09 14:52:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.07590112090110779 norm:0.0001334518747171387 max memory_allocated 29271.06298828125 
[2025-03-09 14:53:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.07579458504915237 norm:0.00012535482528619468 max memory_allocated 29271.06298828125 
[2025-03-09 14:54:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.07574789971113205 norm:0.00012295771739445627 max memory_allocated 29271.06298828125 
[2025-03-09 14:54:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.07571670413017273 norm:0.00011913952766917646 max memory_allocated 29271.06298828125 
[2025-03-09 14:55:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.07571632415056229 norm:0.00012394263467285782 max memory_allocated 29271.06298828125 
[2025-03-09 14:56:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.07565513998270035 norm:0.00011463050759630278 max memory_allocated 29271.06298828125 
[2025-03-09 14:57:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.07563264667987823 norm:0.00011360811913618818 max memory_allocated 29271.06298828125 
[2025-03-09 14:57:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.07558928430080414 norm:0.00011000241647707298 max memory_allocated 29271.06298828125 
[2025-03-09 14:58:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.0755872130393982 norm:0.00011065216676797718 max memory_allocated 29271.06298828125 
[2025-03-09 14:59:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.07556285709142685 norm:0.00010875773295992985 max memory_allocated 29271.06298828125 
[2025-03-09 14:59:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-09 15:00:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.11181651055812836 norm:0.0030515273101627827 max memory_allocated 29271.25048828125 
[2025-03-09 15:01:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0985357016324997 norm:0.0010306906187906861 max memory_allocated 29271.25048828125 
[2025-03-09 15:01:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.09003697335720062 norm:0.0004986986168660223 max memory_allocated 29271.25048828125 
[2025-03-09 15:02:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.08751510828733444 norm:0.0003844490274786949 max memory_allocated 29271.25048828125 
[2025-03-09 15:03:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.08616539090871811 norm:0.0003303999546915293 max memory_allocated 29271.25048828125 
[2025-03-09 15:04:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.08538847416639328 norm:0.00025434268172830343 max memory_allocated 29271.25048828125 
[2025-03-09 15:04:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.08505771309137344 norm:0.00022241234546527267 max memory_allocated 29271.25048828125 
[2025-03-09 15:05:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.08483325690031052 norm:0.00020564459555316716 max memory_allocated 29271.25048828125 
[2025-03-09 15:06:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.08467813581228256 norm:0.0001918810303322971 max memory_allocated 29271.25048828125 
[2025-03-09 15:07:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.08455996215343475 norm:0.0001822679623728618 max memory_allocated 29271.25048828125 
[2025-03-09 15:07:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.08446803689002991 norm:0.00017091062909457833 max memory_allocated 29271.25048828125 
[2025-03-09 15:08:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.08436029404401779 norm:0.00016056581807788461 max memory_allocated 29271.25048828125 
[2025-03-09 15:09:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.08429271727800369 norm:0.00015651818830519915 max memory_allocated 29271.25048828125 
[2025-03-09 15:10:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.08425457030534744 norm:0.00015273016470018774 max memory_allocated 29271.25048828125 
[2025-03-09 15:10:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.08422703295946121 norm:0.00014629024371970445 max memory_allocated 29271.25048828125 
[2025-03-09 15:11:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.08419642597436905 norm:0.0001444294030079618 max memory_allocated 29271.25048828125 
[2025-03-09 15:12:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.08417151123285294 norm:0.00013974760076962411 max memory_allocated 29271.25048828125 
[2025-03-09 15:13:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.08412337303161621 norm:0.00013567888527177274 max memory_allocated 29271.25048828125 
[2025-03-09 15:13:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.08408244699239731 norm:0.0001351326354779303 max memory_allocated 29271.25048828125 
[2025-03-09 15:14:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.08406439423561096 norm:0.00013108481653034687 max memory_allocated 29271.25048828125 
[2025-03-09 15:14:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-09 15:15:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.11805305629968643 norm:0.0020887432619929314 max memory_allocated 29271.43798828125 
[2025-03-09 15:16:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.10733548551797867 norm:0.0007167657604441047 max memory_allocated 29271.43798828125 
[2025-03-09 15:17:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.09935734421014786 norm:0.00032447505509480834 max memory_allocated 29271.43798828125 
[2025-03-09 15:17:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.09689999371767044 norm:0.00023520928516518325 max memory_allocated 29271.43798828125 
[2025-03-09 15:18:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.0956067144870758 norm:0.00020778103498741984 max memory_allocated 29271.43798828125 
[2025-03-09 15:19:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.09504858404397964 norm:0.0002002434921450913 max memory_allocated 29271.43798828125 
[2025-03-09 15:20:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.09474938362836838 norm:0.00018150456889998168 max memory_allocated 29271.43798828125 
[2025-03-09 15:20:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.09454946964979172 norm:0.00017082209524232894 max memory_allocated 29271.43798828125 
[2025-03-09 15:21:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.09440476447343826 norm:0.0001579353993292898 max memory_allocated 29271.43798828125 
[2025-03-09 15:22:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.09430961310863495 norm:0.00014892638137098402 max memory_allocated 29271.43798828125 
[2025-03-09 15:23:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.09423882514238358 norm:0.00014594130334444344 max memory_allocated 29271.43798828125 
[2025-03-09 15:23:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.09416142106056213 norm:0.00013962111552245915 max memory_allocated 29271.43798828125 
[2025-03-09 15:24:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.09411196410655975 norm:0.0001358565641567111 max memory_allocated 29271.43798828125 
[2025-03-09 15:25:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.09407638013362885 norm:0.00013253717042971402 max memory_allocated 29271.43798828125 
[2025-03-09 15:26:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.09407156705856323 norm:0.00013205475988797843 max memory_allocated 29271.43798828125 
[2025-03-09 15:27:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.09405145794153214 norm:0.00013108697021380067 max memory_allocated 29271.43798828125 
[2025-03-09 15:27:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.09399902820587158 norm:0.0001263310550712049 max memory_allocated 29271.43798828125 
[2025-03-09 15:28:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.09396060556173325 norm:0.00012568985403049737 max memory_allocated 29271.43798828125 
[2025-03-09 15:29:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.09394590556621552 norm:0.00012588109530042857 max memory_allocated 29271.43798828125 
[2025-03-09 15:30:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.09394467622041702 norm:0.00012434471864253283 max memory_allocated 29271.43798828125 
[2025-03-09 15:30:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-09 15:31:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.13388003408908844 norm:0.003104043658822775 max memory_allocated 29271.62548828125 
[2025-03-09 15:31:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.12006814032793045 norm:0.0012127290247008204 max memory_allocated 29271.62548828125 
[2025-03-09 15:32:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.11031774431467056 norm:0.0005244076019152999 max memory_allocated 29271.62548828125 
[2025-03-09 15:33:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.10752522945404053 norm:0.00037696410436183214 max memory_allocated 29271.62548828125 
[2025-03-09 15:34:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.10611207783222198 norm:0.0003211389703210443 max memory_allocated 29271.62548828125 
[2025-03-09 15:34:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.10550439357757568 norm:0.0002771067665889859 max memory_allocated 29271.62548828125 
[2025-03-09 15:35:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.10516214370727539 norm:0.00024755814229138196 max memory_allocated 29271.62548828125 
[2025-03-09 15:36:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.1049237921833992 norm:0.0002125138562405482 max memory_allocated 29271.62548828125 
[2025-03-09 15:37:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.1047370433807373 norm:0.00018920190632343292 max memory_allocated 29271.62548828125 
[2025-03-09 15:37:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.10461872816085815 norm:0.00018110961536876857 max memory_allocated 29271.62548828125 
[2025-03-09 15:38:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.10453274101018906 norm:0.00017008242139127105 max memory_allocated 29271.62548828125 
[2025-03-09 15:39:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.1044371947646141 norm:0.0001612742489669472 max memory_allocated 29271.62548828125 
[2025-03-09 15:40:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.10437706112861633 norm:0.00015535447164438665 max memory_allocated 29271.62548828125 
[2025-03-09 15:40:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.10430549085140228 norm:0.00014885314158163965 max memory_allocated 29271.62548828125 
[2025-03-09 15:41:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.10426004230976105 norm:0.00014447332068812102 max memory_allocated 29271.62548828125 
[2025-03-09 15:42:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.10420511662960052 norm:0.00013984888209961355 max memory_allocated 29271.62548828125 
[2025-03-09 15:43:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.1041506677865982 norm:0.00013630898320116103 max memory_allocated 29271.62548828125 
[2025-03-09 15:43:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.10412701219320297 norm:0.00013022925122641027 max memory_allocated 29271.62548828125 
[2025-03-09 15:44:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.1040610820055008 norm:0.0001244865416083485 max memory_allocated 29271.62548828125 
[2025-03-09 15:45:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.10402099788188934 norm:0.00012483473983593285 max memory_allocated 29271.62548828125 
[2025-03-09 15:45:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-09 15:46:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.14836697280406952 norm:0.003389643970876932 max memory_allocated 29271.81298828125 
[2025-03-09 15:47:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.13446353375911713 norm:0.0011824565008282661 max memory_allocated 29271.81298828125 
[2025-03-09 15:47:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.12424295395612717 norm:0.0004954123287461698 max memory_allocated 29271.81298828125 
[2025-03-09 15:48:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.12127361446619034 norm:0.00030715318280272186 max memory_allocated 29271.81298828125 
[2025-03-09 15:49:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.1198866218328476 norm:0.0002805213734973222 max memory_allocated 29271.81298828125 
[2025-03-09 15:50:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.11938199400901794 norm:0.00026780247571878135 max memory_allocated 29271.81298828125 
[2025-03-09 15:50:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.119090236723423 norm:0.00024213407596107572 max memory_allocated 29271.81298828125 
[2025-03-09 15:51:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.11885018646717072 norm:0.00021735946938861161 max memory_allocated 29271.81298828125 
[2025-03-09 15:52:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.11865048855543137 norm:0.0002091724018100649 max memory_allocated 29271.81298828125 
[2025-03-09 15:53:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.11851053684949875 norm:0.00019330563372932374 max memory_allocated 29271.81298828125 
[2025-03-09 15:53:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.11841048300266266 norm:0.00018646748503670096 max memory_allocated 29271.81298828125 
[2025-03-09 15:54:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.11830205470323563 norm:0.00017382919031661004 max memory_allocated 29271.81298828125 
[2025-03-09 15:55:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.11822520196437836 norm:0.0001685923052718863 max memory_allocated 29271.81298828125 
[2025-03-09 15:56:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.1181407943367958 norm:0.00015653994341846555 max memory_allocated 29271.81298828125 
[2025-03-09 15:56:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.11806948482990265 norm:0.0001484085078118369 max memory_allocated 29271.81298828125 
[2025-03-09 15:57:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.1180296391248703 norm:0.0001441670028725639 max memory_allocated 29271.81298828125 
[2025-03-09 15:58:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.11799483746290207 norm:0.0001428250689059496 max memory_allocated 29271.81298828125 
[2025-03-09 15:59:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.11796106398105621 norm:0.00013669203326571733 max memory_allocated 29271.81298828125 
[2025-03-09 15:59:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.11793395131826401 norm:0.0001347069483017549 max memory_allocated 29271.81298828125 
[2025-03-09 16:00:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.11791474372148514 norm:0.00013341050362214446 max memory_allocated 29271.81298828125 
[2025-03-09 16:00:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-09 16:01:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.16371291875839233 norm:0.001655292697250843 max memory_allocated 29272.00048828125 
[2025-03-09 16:02:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.1509261429309845 norm:0.0006360361003316939 max memory_allocated 29272.00048828125 
[2025-03-09 16:03:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.1407785564661026 norm:0.00031198817305266857 max memory_allocated 29272.00048828125 
[2025-03-09 16:03:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.13794218003749847 norm:0.0002672937698662281 max memory_allocated 29272.00048828125 
[2025-03-09 16:04:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.13659514486789703 norm:0.0002498619141988456 max memory_allocated 29272.00048828125 
[2025-03-09 16:05:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.13608117401599884 norm:0.00022991611331235617 max memory_allocated 29272.00048828125 
[2025-03-09 16:06:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.135768324136734 norm:0.00021431614004541188 max memory_allocated 29272.00048828125 
[2025-03-09 16:06:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.13550877571105957 norm:0.0002012513723457232 max memory_allocated 29272.00048828125 
[2025-03-09 16:07:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.13532425463199615 norm:0.00019488667021505535 max memory_allocated 29272.00048828125 
[2025-03-09 16:08:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.13518689572811127 norm:0.00018715330224949867 max memory_allocated 29272.00048828125 
[2025-03-09 16:09:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.13509222865104675 norm:0.00017937137454282492 max memory_allocated 29272.00048828125 
[2025-03-09 16:09:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.13499298691749573 norm:0.00017286650836467743 max memory_allocated 29272.00048828125 
[2025-03-09 16:10:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.13492244482040405 norm:0.00016879722534213215 max memory_allocated 29272.00048828125 
[2025-03-09 16:11:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.13487361371517181 norm:0.00016795644478406757 max memory_allocated 29272.00048828125 
[2025-03-09 16:12:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.13481226563453674 norm:0.00016704617883078754 max memory_allocated 29272.00048828125 
[2025-03-09 16:12:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.13475440442562103 norm:0.00016599585069343448 max memory_allocated 29272.00048828125 
[2025-03-09 16:13:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.1347295194864273 norm:0.00016269952175207436 max memory_allocated 29272.00048828125 
[2025-03-09 16:14:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.1346953958272934 norm:0.00016406981740146875 max memory_allocated 29272.00048828125 
[2025-03-09 16:15:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.13465119898319244 norm:0.00016038001922424883 max memory_allocated 29272.00048828125 
[2025-03-09 16:15:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.1346059888601303 norm:0.00015922912280075252 max memory_allocated 29272.00048828125 
[2025-03-09 16:16:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-09 16:16:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.18672434985637665 norm:0.002220151014626026 max memory_allocated 29272.18798828125 
[2025-03-09 16:17:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.17244310677051544 norm:0.0008833983447402716 max memory_allocated 29272.18798828125 
[2025-03-09 16:18:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.16163569688796997 norm:0.00047247056500054896 max memory_allocated 29272.18798828125 
[2025-03-09 16:19:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.15850260853767395 norm:0.0003614249581005424 max memory_allocated 29272.18798828125 
[2025-03-09 16:19:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.15697945654392242 norm:0.0003195733588654548 max memory_allocated 29272.18798828125 
[2025-03-09 16:20:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.15642428398132324 norm:0.000284108828054741 max memory_allocated 29272.18798828125 
[2025-03-09 16:21:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.15610472857952118 norm:0.00026467334828339517 max memory_allocated 29272.18798828125 
[2025-03-09 16:22:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.15590685606002808 norm:0.0002546531322877854 max memory_allocated 29272.18798828125 
[2025-03-09 16:22:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.15568894147872925 norm:0.00023731080000288785 max memory_allocated 29272.18798828125 
[2025-03-09 16:23:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.15549787878990173 norm:0.000222299451706931 max memory_allocated 29272.18798828125 
[2025-03-09 16:24:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.15537217259407043 norm:0.00021084169566165656 max memory_allocated 29272.18798828125 
[2025-03-09 16:25:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.15524382889270782 norm:0.0002000668173423037 max memory_allocated 29272.18798828125 
[2025-03-09 16:25:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.15516838431358337 norm:0.0001934006140800193 max memory_allocated 29272.18798828125 
[2025-03-09 16:26:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.15505565702915192 norm:0.00018361888942308724 max memory_allocated 29272.18798828125 
[2025-03-09 16:27:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.15496976673603058 norm:0.0001773557742126286 max memory_allocated 29272.18798828125 
[2025-03-09 16:28:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.15490862727165222 norm:0.0001764158223522827 max memory_allocated 29272.18798828125 
[2025-03-09 16:28:59 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.1548783928155899 norm:0.00017439358634874225 max memory_allocated 29272.18798828125 
[2025-03-09 16:29:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.15485326945781708 norm:0.0001689345226623118 max memory_allocated 29272.18798828125 
[2025-03-09 16:30:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.15480758249759674 norm:0.00016649939061608166 max memory_allocated 29272.18798828125 
[2025-03-09 16:31:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.15478810667991638 norm:0.00016311663785018027 max memory_allocated 29272.18798828125 
[2025-03-09 16:31:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-09 16:32:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.20399285852909088 norm:0.0015456342371180654 max memory_allocated 29272.37548828125 
[2025-03-09 16:33:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.19164448976516724 norm:0.0006705320556648076 max memory_allocated 29272.37548828125 
[2025-03-09 16:33:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.18113991618156433 norm:0.0004103470710106194 max memory_allocated 29272.37548828125 
[2025-03-09 16:34:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.17781898379325867 norm:0.0002881266118492931 max memory_allocated 29272.37548828125 
[2025-03-09 16:35:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.1763472557067871 norm:0.00023946927103679627 max memory_allocated 29272.37548828125 
[2025-03-09 16:36:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.17584362626075745 norm:0.00021940353326499462 max memory_allocated 29272.37548828125 
[2025-03-09 16:36:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.17556531727313995 norm:0.00021026082686148584 max memory_allocated 29272.37548828125 
[2025-03-09 16:37:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.17534959316253662 norm:0.00019607300055213273 max memory_allocated 29272.37548828125 
[2025-03-09 16:38:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.1751931607723236 norm:0.00018828618340194225 max memory_allocated 29272.37548828125 
[2025-03-09 16:39:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.17504972219467163 norm:0.00018415978411212564 max memory_allocated 29272.37548828125 
[2025-03-09 16:39:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.17492537200450897 norm:0.0001817250595195219 max memory_allocated 29272.37548828125 
[2025-03-09 16:40:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.17484718561172485 norm:0.00017236475832760334 max memory_allocated 29272.37548828125 
[2025-03-09 16:41:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.17477183043956757 norm:0.00016774883260950446 max memory_allocated 29272.37548828125 
[2025-03-09 16:42:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.17470189929008484 norm:0.0001620258262846619 max memory_allocated 29272.37548828125 
[2025-03-09 16:42:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.1746424436569214 norm:0.00016451643023174256 max memory_allocated 29272.37548828125 
[2025-03-09 16:43:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.17458446323871613 norm:0.00016257249808404595 max memory_allocated 29272.37548828125 
[2025-03-09 16:44:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.17455078661441803 norm:0.0001584165293024853 max memory_allocated 29272.37548828125 
[2025-03-09 16:45:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.17452223598957062 norm:0.00016171809693332762 max memory_allocated 29272.37548828125 
[2025-03-09 16:45:49 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.17448416352272034 norm:0.00016138682258315384 max memory_allocated 29272.37548828125 
[2025-03-09 16:46:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.174427330493927 norm:0.0001630789047339931 max memory_allocated 29272.37548828125 
[2025-03-09 16:46:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-09 16:47:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.23204530775547028 norm:0.002440929412841797 max memory_allocated 29272.56298828125 
[2025-03-09 16:48:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.21670684218406677 norm:0.0008388423593714833 max memory_allocated 29272.56298828125 
[2025-03-09 16:49:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.20431913435459137 norm:0.0004208402242511511 max memory_allocated 29272.56298828125 
[2025-03-09 16:49:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.200806125998497 norm:0.00030674057779833674 max memory_allocated 29272.56298828125 
[2025-03-09 16:50:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.19940945506095886 norm:0.00029150579939596355 max memory_allocated 29272.56298828125 
[2025-03-09 16:51:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.19891005754470825 norm:0.00027557992143556476 max memory_allocated 29272.56298828125 
[2025-03-09 16:52:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.19859077036380768 norm:0.0002547661424614489 max memory_allocated 29272.56298828125 
[2025-03-09 16:52:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.19831128418445587 norm:0.00024180258333217353 max memory_allocated 29272.56298828125 
[2025-03-09 16:53:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.19808872044086456 norm:0.00022903231729287654 max memory_allocated 29272.56298828125 
[2025-03-09 16:54:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.1979001760482788 norm:0.00021446663595270365 max memory_allocated 29272.56298828125 
[2025-03-09 16:55:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.1977657675743103 norm:0.00020962132839486003 max memory_allocated 29272.56298828125 
[2025-03-09 16:55:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.1976238638162613 norm:0.00020034571934957057 max memory_allocated 29272.56298828125 
[2025-03-09 16:56:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.1975136548280716 norm:0.00019673451606649905 max memory_allocated 29272.56298828125 
[2025-03-09 16:57:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.1974206566810608 norm:0.0001901583600556478 max memory_allocated 29272.56298828125 
[2025-03-09 16:58:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.1973404884338379 norm:0.00018570655083749443 max memory_allocated 29272.56298828125 
[2025-03-09 16:58:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.19728827476501465 norm:0.00018253675079904497 max memory_allocated 29272.56298828125 
[2025-03-09 16:59:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.19725564122200012 norm:0.00017956331430468708 max memory_allocated 29272.56298828125 
[2025-03-09 17:00:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.19720491766929626 norm:0.00017692918481770903 max memory_allocated 29272.56298828125 
[2025-03-09 17:01:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.19717399775981903 norm:0.00017566853784956038 max memory_allocated 29272.56298828125 
[2025-03-09 17:01:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.19712689518928528 norm:0.00017639363068155944 max memory_allocated 29272.56298828125 
[2025-03-09 17:02:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-09 17:02:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.25244140625 norm:0.0011063497513532639 max memory_allocated 29272.75048828125 
[2025-03-09 17:03:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.2388876974582672 norm:0.0005232148105278611 max memory_allocated 29272.75048828125 
[2025-03-09 17:04:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.22686222195625305 norm:0.00032235850812867284 max memory_allocated 29272.75048828125 
[2025-03-09 17:05:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.22340865433216095 norm:0.00025791171356104314 max memory_allocated 29272.75048828125 
[2025-03-09 17:05:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.22211496531963348 norm:0.00022567679116036743 max memory_allocated 29272.75048828125 
[2025-03-09 17:06:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.22168800234794617 norm:0.00020584843878168613 max memory_allocated 29272.75048828125 
[2025-03-09 17:07:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.22140085697174072 norm:0.00019142018572892994 max memory_allocated 29272.75048828125 
[2025-03-09 17:08:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.22116830945014954 norm:0.0001811764668673277 max memory_allocated 29272.75048828125 
[2025-03-09 17:08:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.22097346186637878 norm:0.00017202904564328492 max memory_allocated 29272.75048828125 
[2025-03-09 17:09:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.22081464529037476 norm:0.00016649786266498268 max memory_allocated 29272.75048828125 
[2025-03-09 17:10:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.22069019079208374 norm:0.0001596581278136 max memory_allocated 29272.75048828125 
[2025-03-09 17:11:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.22059032320976257 norm:0.0001567271538078785 max memory_allocated 29272.75048828125 
[2025-03-09 17:11:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.2204907089471817 norm:0.00015374876966234297 max memory_allocated 29272.75048828125 
[2025-03-09 17:12:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.2204243242740631 norm:0.00014838369679637253 max memory_allocated 29272.75048828125 
[2025-03-09 17:13:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.2203446328639984 norm:0.0001442429784219712 max memory_allocated 29272.75048828125 
[2025-03-09 17:14:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.2202969193458557 norm:0.0001435781887266785 max memory_allocated 29272.75048828125 
[2025-03-09 17:14:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.2202531099319458 norm:0.0001437619503121823 max memory_allocated 29272.75048828125 
[2025-03-09 17:15:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.22022280097007751 norm:0.00014186739281285554 max memory_allocated 29272.75048828125 
[2025-03-09 17:16:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.22016896307468414 norm:0.00013985548866912723 max memory_allocated 29272.75048828125 
[2025-03-09 17:17:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.22010858356952667 norm:0.00013952871086075902 max memory_allocated 29272.75048828125 
[2025-03-09 17:17:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-09 17:18:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.2840055823326111 norm:0.0010533345630392432 max memory_allocated 29272.93798828125 
[2025-03-09 17:19:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.2688997983932495 norm:0.0005446688155643642 max memory_allocated 29272.93798828125 
[2025-03-09 17:19:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.2547965347766876 norm:0.0003416153194848448 max memory_allocated 29272.93798828125 
[2025-03-09 17:20:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.2509917616844177 norm:0.0002836149651557207 max memory_allocated 29272.93798828125 
[2025-03-09 17:21:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.24962356686592102 norm:0.00025617267237976193 max memory_allocated 29272.93798828125 
[2025-03-09 17:22:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.24909690022468567 norm:0.0002335975004825741 max memory_allocated 29272.93798828125 
[2025-03-09 17:22:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.24875959753990173 norm:0.00021736959752161056 max memory_allocated 29272.93798828125 
[2025-03-09 17:23:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.24849005043506622 norm:0.00020481616957113147 max memory_allocated 29272.93798828125 
[2025-03-09 17:24:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.24828097224235535 norm:0.00019613142649177462 max memory_allocated 29272.93798828125 
[2025-03-09 17:25:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.24809864163398743 norm:0.00018879906565416604 max memory_allocated 29272.93798828125 
[2025-03-09 17:25:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.24796202778816223 norm:0.00018034930690191686 max memory_allocated 29272.93798828125 
[2025-03-09 17:26:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.24779705703258514 norm:0.00017488897719886154 max memory_allocated 29272.93798828125 
[2025-03-09 17:27:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.2476523369550705 norm:0.00017154695524368435 max memory_allocated 29272.93798828125 
[2025-03-09 17:28:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.24756202101707458 norm:0.00017100441618822515 max memory_allocated 29272.93798828125 
[2025-03-09 17:28:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.2474658489227295 norm:0.00016728066839277744 max memory_allocated 29272.93798828125 
[2025-03-09 17:29:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.2473842352628708 norm:0.00016586646961513907 max memory_allocated 29272.93798828125 
[2025-03-09 17:30:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.2473289668560028 norm:0.00016277209215331823 max memory_allocated 29272.93798828125 
[2025-03-09 17:31:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.24728071689605713 norm:0.0001656645763432607 max memory_allocated 29272.93798828125 
[2025-03-09 17:31:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.2472354620695114 norm:0.0001651828206377104 max memory_allocated 29272.93798828125 
[2025-03-09 17:32:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.2471972107887268 norm:0.00016623329429421574 max memory_allocated 29272.93798828125 
[2025-03-09 17:32:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-09 17:33:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.31909510493278503 norm:0.004044331144541502 max memory_allocated 29273.12548828125 
[2025-03-09 17:34:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.2988506853580475 norm:0.0013241390697658062 max memory_allocated 29273.12548828125 
[2025-03-09 17:35:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.28316235542297363 norm:0.0006316971266642213 max memory_allocated 29273.12548828125 
[2025-03-09 17:35:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.27892985939979553 norm:0.0005267960950732231 max memory_allocated 29273.12548828125 
[2025-03-09 17:36:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.27749574184417725 norm:0.00043585337698459625 max memory_allocated 29273.12548828125 
[2025-03-09 17:37:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.27695024013519287 norm:0.0003785564040299505 max memory_allocated 29273.12548828125 
[2025-03-09 17:38:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.27658113837242126 norm:0.0003716887440532446 max memory_allocated 29273.12548828125 
[2025-03-09 17:38:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.27627506852149963 norm:0.0003654816828202456 max memory_allocated 29273.12548828125 
[2025-03-09 17:39:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.27603623270988464 norm:0.000345370703143999 max memory_allocated 29273.12548828125 
[2025-03-09 17:40:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.2758603096008301 norm:0.00033062489819712937 max memory_allocated 29273.12548828125 
[2025-03-09 17:41:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.2756586968898773 norm:0.00031093612778931856 max memory_allocated 29273.12548828125 
[2025-03-09 17:41:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.2754751443862915 norm:0.00030748487915843725 max memory_allocated 29273.12548828125 
[2025-03-09 17:42:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.27532848715782166 norm:0.0003077296423725784 max memory_allocated 29273.12548828125 
[2025-03-09 17:43:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.2752429246902466 norm:0.0002981854195240885 max memory_allocated 29273.12548828125 
[2025-03-09 17:44:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.27510473132133484 norm:0.00028592749731615186 max memory_allocated 29273.12548828125 
[2025-03-09 17:44:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.2750694751739502 norm:0.00026205313042737544 max memory_allocated 29273.12548828125 
[2025-03-09 17:45:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.2750089466571808 norm:0.0002895419020205736 max memory_allocated 29273.12548828125 
[2025-03-09 17:46:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.2749064564704895 norm:0.0002654142153915018 max memory_allocated 29273.12548828125 
[2025-03-09 17:47:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.27484962344169617 norm:0.0002910076000262052 max memory_allocated 29273.12548828125 
[2025-03-09 17:47:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.274803102016449 norm:0.00025626420392654836 max memory_allocated 29273.12548828125 
[2025-03-09 17:48:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-09 17:48:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.3403196632862091 norm:0.0011423483956605196 max memory_allocated 29273.31298828125 
[2025-03-09 17:49:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.324537068605423 norm:0.0005013975314795971 max memory_allocated 29273.31298828125 
[2025-03-09 17:50:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.30956530570983887 norm:0.00032369690597988665 max memory_allocated 29273.31298828125 
[2025-03-09 17:51:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.305539071559906 norm:0.00025872798869386315 max memory_allocated 29273.31298828125 
[2025-03-09 17:52:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.30433180928230286 norm:0.00023528833116870373 max memory_allocated 29273.31298828125 
[2025-03-09 17:52:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.30386435985565186 norm:0.00021337889484129846 max memory_allocated 29273.31298828125 
[2025-03-09 17:53:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.3035713732242584 norm:0.00019941493519581854 max memory_allocated 29273.31298828125 
[2025-03-09 17:54:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.3033520579338074 norm:0.0001877190952654928 max memory_allocated 29273.31298828125 
[2025-03-09 17:55:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.3031415343284607 norm:0.0001738389692036435 max memory_allocated 29273.31298828125 
[2025-03-09 17:55:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.30299288034439087 norm:0.0001709505304461345 max memory_allocated 29273.31298828125 
[2025-03-09 17:56:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.3028374910354614 norm:0.00016835903807077557 max memory_allocated 29273.31298828125 
[2025-03-09 17:57:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.302698016166687 norm:0.00016412917466368526 max memory_allocated 29273.31298828125 
[2025-03-09 17:58:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.30259495973587036 norm:0.00015935067494865507 max memory_allocated 29273.31298828125 
[2025-03-09 17:58:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.3024827539920807 norm:0.00015654480375815183 max memory_allocated 29273.31298828125 
[2025-03-09 17:59:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.3023802936077118 norm:0.00015345154679380357 max memory_allocated 29273.31298828125 
[2025-03-09 18:00:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.3022916913032532 norm:0.00015485906624235213 max memory_allocated 29273.31298828125 
[2025-03-09 18:01:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.3022400140762329 norm:0.00015448933118022978 max memory_allocated 29273.31298828125 
[2025-03-09 18:01:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.3021739721298218 norm:0.00015370489563792944 max memory_allocated 29273.31298828125 
[2025-03-09 18:02:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.30211564898490906 norm:0.00015235146565828472 max memory_allocated 29273.31298828125 
[2025-03-09 18:03:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.30204248428344727 norm:0.00015403104771394283 max memory_allocated 29273.31298828125 
[2025-03-09 18:03:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-09 18:04:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.3739418685436249 norm:0.001139731495641172 max memory_allocated 29273.50048828125 
[2025-03-09 18:05:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.357754111289978 norm:0.0006487886421382427 max memory_allocated 29273.50048828125 
[2025-03-09 18:05:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.3421410322189331 norm:0.0004289057687856257 max memory_allocated 29273.50048828125 
[2025-03-09 18:06:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.3377748131752014 norm:0.0003380420967005193 max memory_allocated 29273.50048828125 
[2025-03-09 18:07:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.3365601599216461 norm:0.0002846387214958668 max memory_allocated 29273.50048828125 
[2025-03-09 18:08:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.33604758977890015 norm:0.0002677536685951054 max memory_allocated 29273.50048828125 
[2025-03-09 18:08:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.3357177972793579 norm:0.00025386956986039877 max memory_allocated 29273.50048828125 
[2025-03-09 18:09:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.33543339371681213 norm:0.00023260504531208426 max memory_allocated 29273.50048828125 
[2025-03-09 18:10:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.3351910710334778 norm:0.00021833223581779748 max memory_allocated 29273.50048828125 
[2025-03-09 18:11:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.33497974276542664 norm:0.00022227510635275394 max memory_allocated 29273.50048828125 
[2025-03-09 18:11:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.3348225951194763 norm:0.0002209391095675528 max memory_allocated 29273.50048828125 
[2025-03-09 18:12:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.3346696197986603 norm:0.00022951171558815986 max memory_allocated 29273.50048828125 
[2025-03-09 18:13:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.3346119523048401 norm:0.00020075778593309224 max memory_allocated 29273.50048828125 
[2025-03-09 18:14:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.33450135588645935 norm:0.00019295666425023228 max memory_allocated 29273.50048828125 
[2025-03-09 18:14:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.3343897759914398 norm:0.00019048737885896116 max memory_allocated 29273.50048828125 
[2025-03-09 18:15:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.3342798352241516 norm:0.00018800876569002867 max memory_allocated 29273.50048828125 
[2025-03-09 18:16:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.3341851830482483 norm:0.0001824016944738105 max memory_allocated 29273.50048828125 
[2025-03-09 18:17:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.33415400981903076 norm:0.0001869652624009177 max memory_allocated 29273.50048828125 
[2025-03-09 18:17:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.33407652378082275 norm:0.00018241105135530233 max memory_allocated 29273.50048828125 
[2025-03-09 18:18:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.3340076208114624 norm:0.00018034766253549606 max memory_allocated 29273.50048828125 
[2025-03-09 18:18:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-09 18:19:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.40811532735824585 norm:0.0009569678804837167 max memory_allocated 29273.68798828125 
[2025-03-09 18:20:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.39104995131492615 norm:0.00047139503294602036 max memory_allocated 29273.68798828125 
[2025-03-09 18:21:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.3749423027038574 norm:0.0003316231304779649 max memory_allocated 29273.68798828125 
[2025-03-09 18:21:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.370417982339859 norm:0.0002788302372209728 max memory_allocated 29273.68798828125 
[2025-03-09 18:22:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.3692159652709961 norm:0.00023934862110763788 max memory_allocated 29273.68798828125 
[2025-03-09 18:23:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.3686962127685547 norm:0.0002161668671760708 max memory_allocated 29273.68798828125 
[2025-03-09 18:24:10 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.3683320879936218 norm:0.00020684467745013535 max memory_allocated 29273.68798828125 
[2025-03-09 18:24:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.3680659234523773 norm:0.00019646325381472707 max memory_allocated 29273.68798828125 
[2025-03-09 18:25:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.36782306432724 norm:0.00018903263844549656 max memory_allocated 29273.68798828125 
[2025-03-09 18:26:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.3676089346408844 norm:0.00018543138867244124 max memory_allocated 29273.68798828125 
[2025-03-09 18:27:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.3674425184726715 norm:0.00017840504006017 max memory_allocated 29273.68798828125 
[2025-03-09 18:27:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.3672783076763153 norm:0.00017701979959383607 max memory_allocated 29273.68798828125 
[2025-03-09 18:28:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.36717066168785095 norm:0.00017431144078727812 max memory_allocated 29273.68798828125 
[2025-03-09 18:29:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.3670889735221863 norm:0.00016813009278848767 max memory_allocated 29273.68798828125 
[2025-03-09 18:30:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.36699944734573364 norm:0.0001714910176815465 max memory_allocated 29273.68798828125 
[2025-03-09 18:30:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.3669453263282776 norm:0.00016523577505722642 max memory_allocated 29273.68798828125 
[2025-03-09 18:31:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.3669074475765228 norm:0.00017211736121680588 max memory_allocated 29273.68798828125 
[2025-03-09 18:32:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.3668000400066376 norm:0.00017498536908533424 max memory_allocated 29273.68798828125 
[2025-03-09 18:33:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.36673256754875183 norm:0.00016765391046646982 max memory_allocated 29273.68798828125 
[2025-03-09 18:33:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.3666694164276123 norm:0.0001697928091743961 max memory_allocated 29273.68798828125 
[2025-03-09 18:34:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-03-09 18:35:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.4441344141960144 norm:0.001938376110047102 max memory_allocated 29273.87548828125 
[2025-03-09 18:35:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.4253333508968353 norm:0.0006493729306384921 max memory_allocated 29273.87548828125 
[2025-03-09 18:36:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.4082857072353363 norm:0.0004090574220754206 max memory_allocated 29273.87548828125 
[2025-03-09 18:37:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.4037398099899292 norm:0.00034567498369142413 max memory_allocated 29273.87548828125 
[2025-03-09 18:38:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.40258267521858215 norm:0.0003019716823473573 max memory_allocated 29273.87548828125 
[2025-03-09 18:38:46 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.4020291566848755 norm:0.0002675839641597122 max memory_allocated 29273.87548828125 
[2025-03-09 18:39:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.4016602635383606 norm:0.00024225488596130162 max memory_allocated 29273.87548828125 
[2025-03-09 18:40:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.40135759115219116 norm:0.00022256336524151266 max memory_allocated 29273.87548828125 
[2025-03-09 18:41:02 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.40107524394989014 norm:0.00020747336384374648 max memory_allocated 29273.87548828125 
[2025-03-09 18:41:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.4008690416812897 norm:0.00019982270896434784 max memory_allocated 29273.87548828125 
[2025-03-09 18:42:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.4007003605365753 norm:0.00018995284335687757 max memory_allocated 29273.87548828125 
[2025-03-09 18:43:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.4005730152130127 norm:0.0001809145323932171 max memory_allocated 29273.87548828125 
[2025-03-09 18:44:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.40044546127319336 norm:0.0001757012214511633 max memory_allocated 29273.87548828125 
[2025-03-09 18:44:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.40032869577407837 norm:0.00016806964413262904 max memory_allocated 29273.87548828125 
[2025-03-09 18:45:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.40021175146102905 norm:0.00016650769975967705 max memory_allocated 29273.87548828125 
[2025-03-09 18:46:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.40010255575180054 norm:0.0001645474840188399 max memory_allocated 29273.87548828125 
[2025-03-09 18:47:04 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.39999401569366455 norm:0.00016127580602187663 max memory_allocated 29273.87548828125 
[2025-03-09 18:47:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.3999174237251282 norm:0.00016082941147033125 max memory_allocated 29273.87548828125 
[2025-03-09 18:48:34 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.39985719323158264 norm:0.00016020037583075464 max memory_allocated 29273.87548828125 
[2025-03-09 18:49:19 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.3997879922389984 norm:0.00015724847617093474 max memory_allocated 29273.87548828125 
[2025-03-09 18:49:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-03-09 18:50:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.4843025803565979 norm:0.001269069965928793 max memory_allocated 29274.06298828125 
[2025-03-09 18:51:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.46570003032684326 norm:0.0006578263128176332 max memory_allocated 29274.06298828125 
[2025-03-09 18:51:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.44821470975875854 norm:0.0004482043441385031 max memory_allocated 29274.06298828125 
[2025-03-09 18:52:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.44358718395233154 norm:0.00035959455999545753 max memory_allocated 29274.06298828125 
[2025-03-09 18:53:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.442399263381958 norm:0.00030045400490053 max memory_allocated 29274.06298828125 
[2025-03-09 18:54:06 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.4418228268623352 norm:0.00027736517949961126 max memory_allocated 29274.06298828125 
[2025-03-09 18:54:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.44138115644454956 norm:0.000257457431871444 max memory_allocated 29274.06298828125 
[2025-03-09 18:55:37 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.44100651144981384 norm:0.00024643802316859365 max memory_allocated 29274.06298828125 
[2025-03-09 18:56:22 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.440743625164032 norm:0.00023521191906183958 max memory_allocated 29274.06298828125 
[2025-03-09 18:57:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.4405311644077301 norm:0.00022564947721548378 max memory_allocated 29274.06298828125 
[2025-03-09 18:57:53 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.4403286874294281 norm:0.0002165329351555556 max memory_allocated 29274.06298828125 
[2025-03-09 18:58:38 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.4401778280735016 norm:0.00021125137573108077 max memory_allocated 29274.06298828125 
[2025-03-09 18:59:23 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.4400741457939148 norm:0.00022053842258173972 max memory_allocated 29274.06298828125 
[2025-03-09 19:00:08 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.4399436414241791 norm:0.00020926928846165538 max memory_allocated 29274.06298828125 
[2025-03-09 19:00:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.43982943892478943 norm:0.0002143617457477376 max memory_allocated 29274.06298828125 
[2025-03-09 19:01:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.43971818685531616 norm:0.00020538337412290275 max memory_allocated 29274.06298828125 
[2025-03-09 19:02:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.43962135910987854 norm:0.00020546794985421002 max memory_allocated 29274.06298828125 
[2025-03-09 19:03:09 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.4395112693309784 norm:0.00020442818640731275 max memory_allocated 29274.06298828125 
[2025-03-09 19:03:55 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.4394742548465729 norm:0.00021707869018428028 max memory_allocated 29274.06298828125 
[2025-03-09 19:04:40 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.4393780529499054 norm:0.0002194329717895016 max memory_allocated 29274.06298828125 
[2025-03-09 19:04:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-03-09 19:05:41 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.5390318036079407 norm:0.001968227094039321 max memory_allocated 29274.25048828125 
[2025-03-09 19:06:26 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.5165489912033081 norm:0.0010357381543144584 max memory_allocated 29274.25048828125 
[2025-03-09 19:07:11 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.4957813322544098 norm:0.0005906406440772116 max memory_allocated 29274.25048828125 
[2025-03-09 19:07:56 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.4906187653541565 norm:0.00044230432831682265 max memory_allocated 29274.25048828125 
[2025-03-09 19:08:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.48934701085090637 norm:0.0003801425045821816 max memory_allocated 29274.25048828125 
[2025-03-09 19:09:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.48869314789772034 norm:0.00034552349825389683 max memory_allocated 29274.25048828125 
[2025-03-09 19:10:12 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.4881684184074402 norm:0.00032386096427217126 max memory_allocated 29274.25048828125 
[2025-03-09 19:10:57 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.48776012659072876 norm:0.00029723672196269035 max memory_allocated 29274.25048828125 
[2025-03-09 19:11:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.4873875379562378 norm:0.00030098319984972477 max memory_allocated 29274.25048828125 
[2025-03-09 19:12:28 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.48713892698287964 norm:0.00029109587194398046 max memory_allocated 29274.25048828125 
[2025-03-09 19:13:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.48685988783836365 norm:0.0002756869653239846 max memory_allocated 29274.25048828125 
[2025-03-09 19:13:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.4866419732570648 norm:0.0002631798852235079 max memory_allocated 29274.25048828125 
[2025-03-09 19:14:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.4864845275878906 norm:0.0002560312277637422 max memory_allocated 29274.25048828125 
[2025-03-09 19:15:28 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.4863557517528534 norm:0.0002516804088372737 max memory_allocated 29274.25048828125 
[2025-03-09 19:16:14 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.4862225651741028 norm:0.0002542938163969666 max memory_allocated 29274.25048828125 
[2025-03-09 19:16:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.4860779941082001 norm:0.0002595831174403429 max memory_allocated 29274.25048828125 
[2025-03-09 19:17:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.4860050678253174 norm:0.00025538605405017734 max memory_allocated 29274.25048828125 
[2025-03-09 19:18:29 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.4859400689601898 norm:0.0002518949331715703 max memory_allocated 29274.25048828125 
[2025-03-09 19:19:15 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.48587340116500854 norm:0.00025453875423409045 max memory_allocated 29274.25048828125 
[2025-03-09 19:20:00 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.48580190539360046 norm:0.00024885497987270355 max memory_allocated 29274.25048828125 
[2025-03-09 19:20:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-03-09 19:21:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.5908176898956299 norm:0.001880680792964995 max memory_allocated 29274.43798828125 
[2025-03-09 19:21:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.5668224692344666 norm:0.0010124077089130878 max memory_allocated 29274.43798828125 
[2025-03-09 19:22:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.5449920892715454 norm:0.0006115680444054306 max memory_allocated 29274.43798828125 
[2025-03-09 19:23:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.539768636226654 norm:0.0004865146183874458 max memory_allocated 29274.43798828125 
[2025-03-09 19:24:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.5383448600769043 norm:0.00043862484744749963 max memory_allocated 29274.43798828125 
[2025-03-09 19:24:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.5375279784202576 norm:0.00042441883124411106 max memory_allocated 29274.43798828125 
[2025-03-09 19:25:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.5369420051574707 norm:0.00039206843939609826 max memory_allocated 29274.43798828125 
[2025-03-09 19:26:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.536466658115387 norm:0.00037865302874706686 max memory_allocated 29274.43798828125 
[2025-03-09 19:27:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.5360508561134338 norm:0.0003626233374234289 max memory_allocated 29274.43798828125 
[2025-03-09 19:27:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.5358073711395264 norm:0.0003511835529934615 max memory_allocated 29274.43798828125 
[2025-03-09 19:28:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.535487711429596 norm:0.0003392958897165954 max memory_allocated 29274.43798828125 
[2025-03-09 19:29:20 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.5352265238761902 norm:0.0003346223966218531 max memory_allocated 29274.43798828125 
[2025-03-09 19:30:05 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.534965991973877 norm:0.00032373814610764384 max memory_allocated 29274.43798828125 
[2025-03-09 19:30:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.5347651243209839 norm:0.000319006503559649 max memory_allocated 29274.43798828125 
[2025-03-09 19:31:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.534591555595398 norm:0.0003048477810807526 max memory_allocated 29274.43798828125 
[2025-03-09 19:32:21 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.5344798564910889 norm:0.00031356699764728546 max memory_allocated 29274.43798828125 
[2025-03-09 19:33:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.5342909097671509 norm:0.0003021555603481829 max memory_allocated 29274.43798828125 
[2025-03-09 19:33:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.5342352986335754 norm:0.0003028463397640735 max memory_allocated 29274.43798828125 
[2025-03-09 19:34:37 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.5341427326202393 norm:0.00030019425321370363 max memory_allocated 29274.43798828125 
[2025-03-09 19:35:22 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.5341007113456726 norm:0.0003017646668013185 max memory_allocated 29274.43798828125 
[2025-03-09 19:35:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-03-09 19:35:38 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 19:36:23 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.6510260701179504 norm:0.015281064435839653 max memory_allocated 29274.77001953125 
[2025-03-09 19:37:08 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.624103307723999 norm:0.011694753542542458 max memory_allocated 29274.77001953125 
[2025-03-09 19:37:54 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.599549412727356 norm:0.008810702711343765 max memory_allocated 29274.77001953125 
[2025-03-09 19:38:39 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.5940768718719482 norm:0.007334871683269739 max memory_allocated 29274.77001953125 
[2025-03-09 19:39:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.5923656225204468 norm:0.005950752645730972 max memory_allocated 29274.77001953125 
[2025-03-09 19:40:10 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.5912421941757202 norm:0.00490739056840539 max memory_allocated 29274.77001953125 
[2025-03-09 19:40:55 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.5905213356018066 norm:0.004510523285716772 max memory_allocated 29274.77001953125 
[2025-03-09 19:41:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.5901020169258118 norm:0.004514649510383606 max memory_allocated 29274.77001953125 
[2025-03-09 19:42:26 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.5897458791732788 norm:0.004332090262323618 max memory_allocated 29274.77001953125 
[2025-03-09 19:43:12 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.5893400311470032 norm:0.0042658098973333836 max memory_allocated 29274.77001953125 
[2025-03-09 19:43:57 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.5889663696289062 norm:0.004140947945415974 max memory_allocated 29274.77001953125 
[2025-03-09 19:44:43 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.5886812210083008 norm:0.0038370657712221146 max memory_allocated 29274.77001953125 
[2025-03-09 19:45:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.5883023738861084 norm:0.003761081723496318 max memory_allocated 29274.77001953125 
[2025-03-09 19:46:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.5881179571151733 norm:0.003690354060381651 max memory_allocated 29274.77001953125 
[2025-03-09 19:46:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.5878726243972778 norm:0.003577600698918104 max memory_allocated 29274.77001953125 
[2025-03-09 19:47:44 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.5876365303993225 norm:0.0034228486474603415 max memory_allocated 29274.77001953125 
[2025-03-09 19:48:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.5875089168548584 norm:0.0033869463950395584 max memory_allocated 29274.77001953125 
[2025-03-09 19:49:15 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.5873804688453674 norm:0.0032835425809025764 max memory_allocated 29274.77001953125 
[2025-03-09 19:50:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.5873019695281982 norm:0.00317757623270154 max memory_allocated 29274.77001953125 
[2025-03-09 19:50:46 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.5871546268463135 norm:0.0032118477392941713 max memory_allocated 29274.77001953125 
[2025-03-09 19:50:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-03-09 19:51:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 19:51:47 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.7484456896781921 norm:0.023763952776789665 max memory_allocated 29274.95751953125 
[2025-03-09 19:52:32 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.7095208764076233 norm:0.01604556478559971 max memory_allocated 29274.95751953125 
[2025-03-09 19:53:18 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.6772534847259521 norm:0.011693467386066914 max memory_allocated 29274.95751953125 
[2025-03-09 19:54:03 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.6699774265289307 norm:0.009794610552489758 max memory_allocated 29274.95751953125 
[2025-03-09 19:54:49 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.6677613854408264 norm:0.008057069033384323 max memory_allocated 29274.95751953125 
[2025-03-09 19:55:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.666402280330658 norm:0.006755118723958731 max memory_allocated 29274.95751953125 
[2025-03-09 19:56:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.6653693318367004 norm:0.005793711636215448 max memory_allocated 29274.95751953125 
[2025-03-09 19:57:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.6644853353500366 norm:0.005352238658815622 max memory_allocated 29274.95751953125 
[2025-03-09 19:57:50 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.663986325263977 norm:0.0053541939705610275 max memory_allocated 29274.95751953125 
[2025-03-09 19:58:36 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.6635231971740723 norm:0.005373507738113403 max memory_allocated 29274.95751953125 
[2025-03-09 19:59:21 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.6629841327667236 norm:0.005077745765447617 max memory_allocated 29274.95751953125 
[2025-03-09 20:00:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.6624531149864197 norm:0.004607930779457092 max memory_allocated 29274.95751953125 
[2025-03-09 20:00:52 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.662111222743988 norm:0.004606154747307301 max memory_allocated 29274.95751953125 
[2025-03-09 20:01:37 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.6621212959289551 norm:0.004810424521565437 max memory_allocated 29274.95751953125 
[2025-03-09 20:02:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.6616482734680176 norm:0.004556912463158369 max memory_allocated 29274.95751953125 
[2025-03-09 20:03:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.6614249348640442 norm:0.004317890387028456 max memory_allocated 29274.95751953125 
[2025-03-09 20:03:53 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.6614862680435181 norm:0.004404704552143812 max memory_allocated 29274.95751953125 
[2025-03-09 20:04:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.6612534523010254 norm:0.004675410687923431 max memory_allocated 29274.95751953125 
[2025-03-09 20:05:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.6612568497657776 norm:0.004849949385970831 max memory_allocated 29274.95751953125 
[2025-03-09 20:06:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.6611310839653015 norm:0.004781126044690609 max memory_allocated 29274.95751953125 
[2025-03-09 20:06:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-03-09 20:06:25 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 20:07:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.9694989323616028 norm:0.11669309437274933 max memory_allocated 29275.14501953125 
[2025-03-09 20:07:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.8865377306938171 norm:0.0705152079463005 max memory_allocated 29275.14501953125 
[2025-03-09 20:08:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.8427836298942566 norm:0.04139404743909836 max memory_allocated 29275.14501953125 
[2025-03-09 20:09:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.8275208473205566 norm:0.03244315832853317 max memory_allocated 29275.14501953125 
[2025-03-09 20:10:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.8203898072242737 norm:0.028215378522872925 max memory_allocated 29275.14501953125 
[2025-03-09 20:10:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.8153669834136963 norm:0.025663848966360092 max memory_allocated 29275.14501953125 
[2025-03-09 20:11:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.8118871450424194 norm:0.022524133324623108 max memory_allocated 29275.14501953125 
[2025-03-09 20:12:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.8095250129699707 norm:0.020789209753274918 max memory_allocated 29275.14501953125 
[2025-03-09 20:13:14 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.8076760768890381 norm:0.01897677592933178 max memory_allocated 29275.14501953125 
[2025-03-09 20:13:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.8061804175376892 norm:0.018312402069568634 max memory_allocated 29275.14501953125 
[2025-03-09 20:14:44 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.8051390647888184 norm:0.017906855791807175 max memory_allocated 29275.14501953125 
[2025-03-09 20:15:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.8039107322692871 norm:0.016257500275969505 max memory_allocated 29275.14501953125 
[2025-03-09 20:16:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.8028407692909241 norm:0.015800509601831436 max memory_allocated 29275.14501953125 
[2025-03-09 20:17:01 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.8020877838134766 norm:0.015374219045042992 max memory_allocated 29275.14501953125 
[2025-03-09 20:17:46 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.8012734055519104 norm:0.014235863462090492 max memory_allocated 29275.14501953125 
[2025-03-09 20:18:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.8004810810089111 norm:0.013623546808958054 max memory_allocated 29275.14501953125 
[2025-03-09 20:19:17 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.8000650405883789 norm:0.012518681585788727 max memory_allocated 29275.14501953125 
[2025-03-09 20:20:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.7996242046356201 norm:0.01213871967047453 max memory_allocated 29275.14501953125 
[2025-03-09 20:20:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.7990827560424805 norm:0.011708226054906845 max memory_allocated 29275.14501953125 
[2025-03-09 20:21:33 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.7989188432693481 norm:0.011193504557013512 max memory_allocated 29275.14501953125 
[2025-03-09 20:21:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-03-09 20:21:49 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 20:22:34 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:1.4905638694763184 norm:0.1069251000881195 max memory_allocated 29275.33251953125 
[2025-03-09 20:23:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:1.3841781616210938 norm:0.08009179681539536 max memory_allocated 29275.33251953125 
[2025-03-09 20:24:05 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:1.2990987300872803 norm:0.053811341524124146 max memory_allocated 29275.33251953125 
[2025-03-09 20:24:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:1.2762157917022705 norm:0.04950568079948425 max memory_allocated 29275.33251953125 
[2025-03-09 20:25:36 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:1.265390396118164 norm:0.04945915937423706 max memory_allocated 29275.33251953125 
[2025-03-09 20:26:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:1.2568814754486084 norm:0.04537443816661835 max memory_allocated 29275.33251953125 
[2025-03-09 20:27:06 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:1.2507203817367554 norm:0.04190698266029358 max memory_allocated 29275.33251953125 
[2025-03-09 20:27:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:1.2463369369506836 norm:0.0386020690202713 max memory_allocated 29275.33251953125 
[2025-03-09 20:28:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:1.2460031509399414 norm:0.04211145639419556 max memory_allocated 29275.33251953125 
[2025-03-09 20:29:23 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:1.2459816932678223 norm:0.04429147019982338 max memory_allocated 29275.33251953125 
[2025-03-09 20:30:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:1.241257667541504 norm:0.038330867886543274 max memory_allocated 29275.33251953125 
[2025-03-09 20:30:53 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:1.2371671199798584 norm:0.03594152629375458 max memory_allocated 29275.33251953125 
[2025-03-09 20:31:39 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:1.2359949350357056 norm:0.036360014230012894 max memory_allocated 29275.33251953125 
[2025-03-09 20:32:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:1.2343796491622925 norm:0.03580871969461441 max memory_allocated 29275.33251953125 
[2025-03-09 20:33:10 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:1.2330341339111328 norm:0.03386017680168152 max memory_allocated 29275.33251953125 
[2025-03-09 20:33:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:1.2321045398712158 norm:0.034421540796756744 max memory_allocated 29275.33251953125 
[2025-03-09 20:34:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:1.2308683395385742 norm:0.033008918166160583 max memory_allocated 29275.33251953125 
[2025-03-09 20:35:26 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:1.230698585510254 norm:0.03381842374801636 max memory_allocated 29275.33251953125 
[2025-03-09 20:36:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:1.2304441928863525 norm:0.03370116278529167 max memory_allocated 29275.33251953125 
[2025-03-09 20:36:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:1.2299158573150635 norm:0.03489406779408455 max memory_allocated 29275.33251953125 
[2025-03-09 20:37:10 root] (main_calibration.py 365): INFO 36778.617000341415
[2025-03-09 20:37:37 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-09 20:39:20 root] (main_calibration.py 158): INFO wikitext2 : 5.273362636566162
[2025-03-09 20:39:20 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-09 20:41:58 root] (main_calibration.py 158): INFO c4 : 6.822667121887207
[2025-03-09 22:39:52 root] (main_calibration.py 169): INFO {'wikitext2': 5.273362636566162, 'c4': 6.822667121887207, 'results': {'arc_easy': {'acc': 0.7335858585858586, 'acc_stderr': 0.009071357971078681, 'acc_norm': 0.5812289562289562, 'acc_norm_stderr': 0.010123487160167817}, 'arc_challenge': {'acc': 0.42150170648464164, 'acc_stderr': 0.014430197069326025, 'acc_norm': 0.4334470989761092, 'acc_norm_stderr': 0.014481376224558896}, 'piqa': {'acc': 0.7889009793253536, 'acc_stderr': 0.00952137737873414, 'acc_norm': 0.7829162132752993, 'acc_norm_stderr': 0.009618708415756787}, 'boolq': {'acc': 0.6737003058103975, 'acc_stderr': 0.008200385052427126}, 'winogrande': {'acc': 0.6779794790844514, 'acc_stderr': 0.013132070202071064}, 'hellaswag': {'acc': 0.5819557857000598, 'acc_stderr': 0.004922294797766666, 'acc_norm': 0.7529376618203545, 'acc_norm_stderr': 0.004304218408635192}}, 'versions': {'arc_easy': 0, 'arc_challenge': 0, 'piqa': 0, 'boolq': 1, 'winogrande': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
