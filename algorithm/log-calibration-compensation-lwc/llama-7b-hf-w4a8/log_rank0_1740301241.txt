[2025-02-23 09:00:41 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-7b-hf-w4a8', save_dir='./log-calibration-compensation-lwc/quant/llama-7b-hf-w4a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-23 09:01:54 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-23 09:01:54 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-23 09:01:54 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-23 09:01:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-23 09:02:01 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:02:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.012500926852226257 norm:0.015006406232714653 max memory_allocated 22559.10693359375 
[2025-02-23 09:03:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.006545837037265301 norm:0.007154306396842003 max memory_allocated 22559.10693359375 
[2025-02-23 09:03:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.004570499993860722 norm:0.004885777365416288 max memory_allocated 22559.10693359375 
[2025-02-23 09:04:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0038662455044686794 norm:0.0038136618677526712 max memory_allocated 22559.10693359375 
[2025-02-23 09:04:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.003605949692428112 norm:0.003196170087903738 max memory_allocated 22559.10693359375 
[2025-02-23 09:05:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.00338888936676085 norm:0.0027253334410488605 max memory_allocated 22559.10693359375 
[2025-02-23 09:05:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.00326749961823225 norm:0.0022839107550680637 max memory_allocated 22559.10693359375 
[2025-02-23 09:06:27 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.003223356558009982 norm:0.002065628068521619 max memory_allocated 22559.10693359375 
[2025-02-23 09:07:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.00318391271866858 norm:0.001849874621257186 max memory_allocated 22559.10693359375 
[2025-02-23 09:07:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00310995290055871 norm:0.0016401282045990229 max memory_allocated 22559.10693359375 
[2025-02-23 09:08:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0030914642848074436 norm:0.001456184545531869 max memory_allocated 22559.10693359375 
[2025-02-23 09:08:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0030943425372242928 norm:0.0012821683194488287 max memory_allocated 22559.10693359375 
[2025-02-23 09:09:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0030452264472842216 norm:0.0011338868644088507 max memory_allocated 22559.10693359375 
[2025-02-23 09:09:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0030097635462880135 norm:0.001053285668604076 max memory_allocated 22559.10693359375 
[2025-02-23 09:10:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0029927713330835104 norm:0.0009507080540060997 max memory_allocated 22559.10693359375 
[2025-02-23 09:10:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0029949299059808254 norm:0.0008888167212717235 max memory_allocated 22559.10693359375 
[2025-02-23 09:11:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.002948316978290677 norm:0.0007913696463219821 max memory_allocated 22559.10693359375 
[2025-02-23 09:12:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.002936518518254161 norm:0.0007671600906178355 max memory_allocated 22559.10693359375 
[2025-02-23 09:12:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0029346703086048365 norm:0.0007397345034405589 max memory_allocated 22559.10693359375 
[2025-02-23 09:13:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0029422049410641193 norm:0.0007159695960581303 max memory_allocated 22559.10693359375 
[2025-02-23 09:13:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-23 09:13:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:13:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.022198420017957687 norm:0.01905227079987526 max memory_allocated 22559.27880859375 
[2025-02-23 09:14:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.013072758913040161 norm:0.011754175648093224 max memory_allocated 22559.27880859375 
[2025-02-23 09:15:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.009324909187853336 norm:0.006944298278540373 max memory_allocated 22559.27880859375 
[2025-02-23 09:15:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0083233043551445 norm:0.005007004365324974 max memory_allocated 22559.27880859375 
[2025-02-23 09:16:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.007923971861600876 norm:0.004436575807631016 max memory_allocated 22559.27880859375 
[2025-02-23 09:16:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.007640780881047249 norm:0.003825010498985648 max memory_allocated 22559.27880859375 
[2025-02-23 09:17:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.00742785818874836 norm:0.00346339400857687 max memory_allocated 22559.27880859375 
[2025-02-23 09:17:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.007306095212697983 norm:0.003178864950314164 max memory_allocated 22559.27880859375 
[2025-02-23 09:18:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.007170650642365217 norm:0.0029524751007556915 max memory_allocated 22559.27880859375 
[2025-02-23 09:18:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.007052550092339516 norm:0.0026784460060298443 max memory_allocated 22559.27880859375 
[2025-02-23 09:19:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.006942985579371452 norm:0.002445568097755313 max memory_allocated 22559.27880859375 
[2025-02-23 09:20:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0068405428901314735 norm:0.002246796851977706 max memory_allocated 22559.27880859375 
[2025-02-23 09:20:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.006797075271606445 norm:0.0020257302094250917 max memory_allocated 22559.27880859375 
[2025-02-23 09:21:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.006756902672350407 norm:0.001845112070441246 max memory_allocated 22559.27880859375 
[2025-02-23 09:21:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.006734671536833048 norm:0.001690058154053986 max memory_allocated 22559.27880859375 
[2025-02-23 09:22:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.006727868225425482 norm:0.0014912738697603345 max memory_allocated 22559.27880859375 
[2025-02-23 09:22:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.00669765705242753 norm:0.0013535602483898401 max memory_allocated 22559.27880859375 
[2025-02-23 09:23:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.006672448944300413 norm:0.0011941988486796618 max memory_allocated 22559.27880859375 
[2025-02-23 09:24:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.006667331326752901 norm:0.0010750609217211604 max memory_allocated 22559.27880859375 
[2025-02-23 09:24:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.006668235175311565 norm:0.0010511439759284258 max memory_allocated 22559.27880859375 
[2025-02-23 09:24:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-23 09:24:46 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:25:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.023813532665371895 norm:0.01075196173042059 max memory_allocated 22559.45068359375 
[2025-02-23 09:25:54 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.021444333717226982 norm:0.009903578087687492 max memory_allocated 22559.45068359375 
[2025-02-23 09:26:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.016701241955161095 norm:0.008061661384999752 max memory_allocated 22559.45068359375 
[2025-02-23 09:27:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.015437392517924309 norm:0.006015555467456579 max memory_allocated 22559.45068359375 
[2025-02-23 09:27:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.014907801523804665 norm:0.005236701108515263 max memory_allocated 22559.45068359375 
[2025-02-23 09:28:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.014424462802708149 norm:0.004749909043312073 max memory_allocated 22559.45068359375 
[2025-02-23 09:28:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.014148100279271603 norm:0.004298916086554527 max memory_allocated 22559.45068359375 
[2025-02-23 09:29:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.014097634702920914 norm:0.004196823574602604 max memory_allocated 22559.45068359375 
[2025-02-23 09:29:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.014022842980921268 norm:0.0040329135954380035 max memory_allocated 22559.45068359375 
[2025-02-23 09:30:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.013966518454253674 norm:0.0037528073880821466 max memory_allocated 22559.45068359375 
[2025-02-23 09:30:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.01405464205890894 norm:0.0038227143231779337 max memory_allocated 22559.45068359375 
[2025-02-23 09:31:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.013952263630926609 norm:0.0035635954700410366 max memory_allocated 22559.45068359375 
[2025-02-23 09:32:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.014015182852745056 norm:0.0035200822167098522 max memory_allocated 22559.45068359375 
[2025-02-23 09:32:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.014128192327916622 norm:0.0035000289790332317 max memory_allocated 22559.45068359375 
[2025-02-23 09:33:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.014108647592365742 norm:0.003358188783749938 max memory_allocated 22559.45068359375 
[2025-02-23 09:33:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.01417672447860241 norm:0.003321803640574217 max memory_allocated 22559.45068359375 
[2025-02-23 09:34:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.01404415164142847 norm:0.003213627031072974 max memory_allocated 22559.45068359375 
[2025-02-23 09:34:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.014074914157390594 norm:0.003134544938802719 max memory_allocated 22559.45068359375 
[2025-02-23 09:35:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.01463171187788248 norm:0.0052256216295063496 max memory_allocated 22559.45068359375 
[2025-02-23 09:35:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.014015286229550838 norm:0.0035081994719803333 max memory_allocated 22559.45068359375 
[2025-02-23 09:36:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-23 09:36:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.026957526803016663 norm:0.003436387050896883 max memory_allocated 22559.50732421875 
[2025-02-23 09:37:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.020108558237552643 norm:0.0008265784126706421 max memory_allocated 22559.50732421875 
[2025-02-23 09:37:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.016783466562628746 norm:0.0004036605532746762 max memory_allocated 22559.50732421875 
[2025-02-23 09:38:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.015635225921869278 norm:0.0002687150554265827 max memory_allocated 22559.50732421875 
[2025-02-23 09:38:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.01496373862028122 norm:0.00022170390002429485 max memory_allocated 22559.50732421875 
[2025-02-23 09:39:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.014504061080515385 norm:0.00016683225112501532 max memory_allocated 22559.50732421875 
[2025-02-23 09:40:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.014344350434839725 norm:0.0001549133303342387 max memory_allocated 22559.50732421875 
[2025-02-23 09:40:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.014297114685177803 norm:0.00014978925173636526 max memory_allocated 22559.50732421875 
[2025-02-23 09:41:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.014234175905585289 norm:0.0001385487848892808 max memory_allocated 22559.50732421875 
[2025-02-23 09:41:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.014175722375512123 norm:0.00011378026101738214 max memory_allocated 22559.50732421875 
[2025-02-23 09:42:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.014147189445793629 norm:0.00010753508831840008 max memory_allocated 22559.50732421875 
[2025-02-23 09:42:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.014144564978778362 norm:0.00010369535448262468 max memory_allocated 22559.50732421875 
[2025-02-23 09:43:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.014143222011625767 norm:9.902744932333007e-05 max memory_allocated 22559.50732421875 
[2025-02-23 09:44:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.01414749026298523 norm:9.566668450133875e-05 max memory_allocated 22559.50732421875 
[2025-02-23 09:44:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.01411501131951809 norm:9.453991515329108e-05 max memory_allocated 22559.50732421875 
[2025-02-23 09:45:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.014138266444206238 norm:9.03822437976487e-05 max memory_allocated 22559.50732421875 
[2025-02-23 09:45:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.01419499609619379 norm:9.773104102350771e-05 max memory_allocated 22559.50732421875 
[2025-02-23 09:46:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.014162692241370678 norm:9.55086579779163e-05 max memory_allocated 22559.50732421875 
[2025-02-23 09:46:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.014128781855106354 norm:8.847934077493846e-05 max memory_allocated 22559.50732421875 
[2025-02-23 09:47:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.014133738353848457 norm:9.476232662564144e-05 max memory_allocated 22559.50732421875 
[2025-02-23 09:47:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-23 09:48:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.03368762135505676 norm:0.0021211181301623583 max memory_allocated 22559.67919921875 
[2025-02-23 09:48:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.02586163952946663 norm:0.0006562735070474446 max memory_allocated 22559.67919921875 
[2025-02-23 09:49:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.021184779703617096 norm:0.00036952801747247577 max memory_allocated 22559.67919921875 
[2025-02-23 09:49:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.019539205357432365 norm:0.00025318452389910817 max memory_allocated 22559.67919921875 
[2025-02-23 09:50:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.018771031871438026 norm:0.0002220749738626182 max memory_allocated 22559.67919921875 
[2025-02-23 09:50:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.01837523654103279 norm:0.00021064697648398578 max memory_allocated 22559.67919921875 
[2025-02-23 09:51:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.018183937296271324 norm:0.00018974758859258145 max memory_allocated 22559.67919921875 
[2025-02-23 09:52:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.018064282834529877 norm:0.00017988216131925583 max memory_allocated 22559.67919921875 
[2025-02-23 09:52:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.017960114404559135 norm:0.0001748483773553744 max memory_allocated 22559.67919921875 
[2025-02-23 09:53:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0179545059800148 norm:0.00017043124535121024 max memory_allocated 22559.67919921875 
[2025-02-23 09:53:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.018030768260359764 norm:0.00020387831318657845 max memory_allocated 22559.67919921875 
[2025-02-23 09:54:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.01798398792743683 norm:0.00016546771803405136 max memory_allocated 22559.67919921875 
[2025-02-23 09:54:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.017944104969501495 norm:0.00015480992442462593 max memory_allocated 22559.67919921875 
[2025-02-23 09:55:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.017985345795750618 norm:0.0001636148663237691 max memory_allocated 22559.67919921875 
[2025-02-23 09:55:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.017934788018465042 norm:0.0001652881474001333 max memory_allocated 22559.67919921875 
[2025-02-23 09:56:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0179157592356205 norm:0.00016241386765614152 max memory_allocated 22559.67919921875 
[2025-02-23 09:57:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.017883136868476868 norm:0.00016785631305538118 max memory_allocated 22559.67919921875 
[2025-02-23 09:57:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.017885524779558182 norm:0.00016472642892040312 max memory_allocated 22559.67919921875 
[2025-02-23 09:58:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.017866723239421844 norm:0.0001747878413880244 max memory_allocated 22559.67919921875 
[2025-02-23 09:58:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.017838291823863983 norm:0.0001683061709627509 max memory_allocated 22559.67919921875 
[2025-02-23 09:58:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-23 09:59:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.036483969539403915 norm:0.0018669052515178919 max memory_allocated 22559.85107421875 
[2025-02-23 10:00:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.028532303869724274 norm:0.0005541956634260714 max memory_allocated 22559.85107421875 
[2025-02-23 10:00:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.024019137024879456 norm:0.00031242927070707083 max memory_allocated 22559.85107421875 
[2025-02-23 10:01:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.02238401584327221 norm:0.0002279173640999943 max memory_allocated 22559.85107421875 
[2025-02-23 10:01:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.02148294448852539 norm:0.000171936335391365 max memory_allocated 22559.85107421875 
[2025-02-23 10:02:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.021083582192659378 norm:0.00018144214118365198 max memory_allocated 22559.85107421875 
[2025-02-23 10:02:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.020945506170392036 norm:0.00016750961367506534 max memory_allocated 22559.85107421875 
[2025-02-23 10:03:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.020820122212171555 norm:0.0001529329747427255 max memory_allocated 22559.85107421875 
[2025-02-23 10:03:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.020713673904538155 norm:0.00013907279935665429 max memory_allocated 22559.85107421875 
[2025-02-23 10:04:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.020669233053922653 norm:0.00013692949141841382 max memory_allocated 22559.85107421875 
[2025-02-23 10:05:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.020629439502954483 norm:0.00013489744742400944 max memory_allocated 22559.85107421875 
[2025-02-23 10:05:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.02061198651790619 norm:0.00013562498497776687 max memory_allocated 22559.85107421875 
[2025-02-23 10:06:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.020581450313329697 norm:0.00014241645112633705 max memory_allocated 22559.85107421875 
[2025-02-23 10:06:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.02054903469979763 norm:0.00014064864080864936 max memory_allocated 22559.85107421875 
[2025-02-23 10:07:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.020567765459418297 norm:0.00014086900046095252 max memory_allocated 22559.85107421875 
[2025-02-23 10:07:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.020572323352098465 norm:0.00013466356904245913 max memory_allocated 22559.85107421875 
[2025-02-23 10:08:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.020609494298696518 norm:0.00015193191939033568 max memory_allocated 22559.85107421875 
[2025-02-23 10:08:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.02057943306863308 norm:0.0001355083513772115 max memory_allocated 22559.85107421875 
[2025-02-23 10:09:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.020518198609352112 norm:0.00013547665730584413 max memory_allocated 22559.85107421875 
[2025-02-23 10:10:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.020547151565551758 norm:0.00013709490303881466 max memory_allocated 22559.85107421875 
[2025-02-23 10:10:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-23 10:10:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.04072686284780502 norm:0.0036181705072522163 max memory_allocated 22560.02294921875 
[2025-02-23 10:11:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.03191590681672096 norm:0.000823968555778265 max memory_allocated 22560.02294921875 
[2025-02-23 10:11:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.02711665816605091 norm:0.0004210357728879899 max memory_allocated 22560.02294921875 
[2025-02-23 10:12:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.02548292651772499 norm:0.0002845149429049343 max memory_allocated 22560.02294921875 
[2025-02-23 10:13:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.024559836834669113 norm:0.00019123576930724084 max memory_allocated 22560.02294921875 
[2025-02-23 10:13:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.02407112717628479 norm:0.0001909742713905871 max memory_allocated 22560.02294921875 
[2025-02-23 10:14:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.023878175765275955 norm:0.00018440274288877845 max memory_allocated 22560.02294921875 
[2025-02-23 10:14:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.023777727037668228 norm:0.00016414000128861517 max memory_allocated 22560.02294921875 
[2025-02-23 10:15:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.023846682161092758 norm:0.00020470828167162836 max memory_allocated 22560.02294921875 
[2025-02-23 10:15:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.023686163127422333 norm:0.0001547566644148901 max memory_allocated 22560.02294921875 
[2025-02-23 10:16:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.023606449365615845 norm:0.0001380823232466355 max memory_allocated 22560.02294921875 
[2025-02-23 10:16:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.023585256189107895 norm:0.00013773766113445163 max memory_allocated 22560.02294921875 
[2025-02-23 10:17:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.023570533841848373 norm:0.000138463219627738 max memory_allocated 22560.02294921875 
[2025-02-23 10:18:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.023590300232172012 norm:0.0001307882776018232 max memory_allocated 22560.02294921875 
[2025-02-23 10:18:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.023557433858513832 norm:0.0001451474236091599 max memory_allocated 22560.02294921875 
[2025-02-23 10:19:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.023550037294626236 norm:0.00015063906903378665 max memory_allocated 22560.02294921875 
[2025-02-23 10:19:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.023541204631328583 norm:0.000151915053720586 max memory_allocated 22560.02294921875 
[2025-02-23 10:20:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.023521466180682182 norm:0.00015042207087390125 max memory_allocated 22560.02294921875 
[2025-02-23 10:20:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.023528441786766052 norm:0.0001510104484623298 max memory_allocated 22560.02294921875 
[2025-02-23 10:21:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.023568827658891678 norm:0.00015256230835802853 max memory_allocated 22560.02294921875 
[2025-02-23 10:21:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-23 10:22:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0428367480635643 norm:0.001190624083392322 max memory_allocated 22560.19482421875 
[2025-02-23 10:22:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.03480619192123413 norm:0.0003818353288806975 max memory_allocated 22560.19482421875 
[2025-02-23 10:23:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.030086291953921318 norm:0.00029178999830037355 max memory_allocated 22560.19482421875 
[2025-02-23 10:23:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.028337491676211357 norm:0.00016967450210358948 max memory_allocated 22560.19482421875 
[2025-02-23 10:24:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.027470238506793976 norm:0.00015766063006594777 max memory_allocated 22560.19482421875 
[2025-02-23 10:25:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.027138806879520416 norm:0.00016917099128477275 max memory_allocated 22560.19482421875 
[2025-02-23 10:25:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.026983313262462616 norm:0.00014397937047760934 max memory_allocated 22560.19482421875 
[2025-02-23 10:26:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.02693115919828415 norm:0.00015001653810031712 max memory_allocated 22560.19482421875 
[2025-02-23 10:26:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.026800166815519333 norm:0.00012430016067810357 max memory_allocated 22560.19482421875 
[2025-02-23 10:27:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.02673598751425743 norm:0.00012625823728740215 max memory_allocated 22560.19482421875 
[2025-02-23 10:27:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.02672010473906994 norm:0.00012180283374618739 max memory_allocated 22560.19482421875 
[2025-02-23 10:28:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.026752885431051254 norm:0.0001365021598758176 max memory_allocated 22560.19482421875 
[2025-02-23 10:28:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.02673374116420746 norm:0.00012161194172222167 max memory_allocated 22560.19482421875 
[2025-02-23 10:29:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.02672366052865982 norm:0.0001332558022113517 max memory_allocated 22560.19482421875 
[2025-02-23 10:30:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.026681531220674515 norm:0.0001299185969401151 max memory_allocated 22560.19482421875 
[2025-02-23 10:30:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.026668887585401535 norm:0.0001203775973408483 max memory_allocated 22560.19482421875 
[2025-02-23 10:31:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.026666264981031418 norm:0.00012768164742738008 max memory_allocated 22560.19482421875 
[2025-02-23 10:31:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.026656463742256165 norm:0.00012314910418353975 max memory_allocated 22560.19482421875 
[2025-02-23 10:32:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.02664077654480934 norm:0.00012203682854305953 max memory_allocated 22560.19482421875 
[2025-02-23 10:32:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.026646360754966736 norm:0.0001229516346938908 max memory_allocated 22560.19482421875 
[2025-02-23 10:33:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-23 10:33:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.048374056816101074 norm:0.001118378364481032 max memory_allocated 22560.36669921875 
[2025-02-23 10:34:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.03883200138807297 norm:0.0004416051087900996 max memory_allocated 22560.36669921875 
[2025-02-23 10:34:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.033164504915475845 norm:0.00023320676700677723 max memory_allocated 22560.36669921875 
[2025-02-23 10:35:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.031200116500258446 norm:0.00018656243628356606 max memory_allocated 22560.36669921875 
[2025-02-23 10:35:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.030253149569034576 norm:0.00016035734734032303 max memory_allocated 22560.36669921875 
[2025-02-23 10:36:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.02976815029978752 norm:0.00014052697224542499 max memory_allocated 22560.36669921875 
[2025-02-23 10:36:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.029558982700109482 norm:0.0001267584739252925 max memory_allocated 22560.36669921875 
[2025-02-23 10:37:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.029450315982103348 norm:0.00012649124255403876 max memory_allocated 22560.36669921875 
[2025-02-23 10:38:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.029367949813604355 norm:0.00012596012675203383 max memory_allocated 22560.36669921875 
[2025-02-23 10:38:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.02931354008615017 norm:0.00012559990864247084 max memory_allocated 22560.36669921875 
[2025-02-23 10:39:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.029242459684610367 norm:0.0001275890099350363 max memory_allocated 22560.36669921875 
[2025-02-23 10:39:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.029234396293759346 norm:0.00011710394755937159 max memory_allocated 22560.36669921875 
[2025-02-23 10:40:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.02921943925321102 norm:0.00012643312220461667 max memory_allocated 22560.36669921875 
[2025-02-23 10:40:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.029246049001812935 norm:0.00013364379992708564 max memory_allocated 22560.36669921875 
[2025-02-23 10:41:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.02922051027417183 norm:0.00013194426719564945 max memory_allocated 22560.36669921875 
[2025-02-23 10:42:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.029197832569479942 norm:0.000126415237900801 max memory_allocated 22560.36669921875 
[2025-02-23 10:42:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.029208112508058548 norm:0.00012364705617073923 max memory_allocated 22560.36669921875 
[2025-02-23 10:43:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.02917519584298134 norm:0.00012532727851066738 max memory_allocated 22560.36669921875 
[2025-02-23 10:43:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.029165953397750854 norm:0.00012228154810145497 max memory_allocated 22560.36669921875 
[2025-02-23 10:44:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.02915523760020733 norm:0.000125509497593157 max memory_allocated 22560.36669921875 
[2025-02-23 10:44:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-23 10:45:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.05180424824357033 norm:0.0011976503301411867 max memory_allocated 22560.53857421875 
[2025-02-23 10:45:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.04179999604821205 norm:0.0004501349467318505 max memory_allocated 22560.53857421875 
[2025-02-23 10:46:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.036336395889520645 norm:0.0002780051436275244 max memory_allocated 22560.53857421875 
[2025-02-23 10:46:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.034323129802942276 norm:0.00020348196267150342 max memory_allocated 22560.53857421875 
[2025-02-23 10:47:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.033372458070516586 norm:0.0001792271650629118 max memory_allocated 22560.53857421875 
[2025-02-23 10:47:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.03288182243704796 norm:0.00015999171591829509 max memory_allocated 22560.53857421875 
[2025-02-23 10:48:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.03266698494553566 norm:0.00015034637181088328 max memory_allocated 22560.53857421875 
[2025-02-23 10:48:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.03252124413847923 norm:0.00013236698578111827 max memory_allocated 22560.53857421875 
[2025-02-23 10:49:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.03240370750427246 norm:0.00012146049266448244 max memory_allocated 22560.53857421875 
[2025-02-23 10:50:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.03231005370616913 norm:0.00011394642206141725 max memory_allocated 22560.53857421875 
[2025-02-23 10:50:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.03224410489201546 norm:0.00011744420044124126 max memory_allocated 22560.53857421875 
[2025-02-23 10:51:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.032188672572374344 norm:0.00010935647151200101 max memory_allocated 22560.53857421875 
[2025-02-23 10:51:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.03217796981334686 norm:0.00010801937605720013 max memory_allocated 22560.53857421875 
[2025-02-23 10:52:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.03218487277626991 norm:0.00011099613038823009 max memory_allocated 22560.53857421875 
[2025-02-23 10:52:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.032158538699150085 norm:0.00010728422057582065 max memory_allocated 22560.53857421875 
[2025-02-23 10:53:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.032151758670806885 norm:0.00011026934953406453 max memory_allocated 22560.53857421875 
[2025-02-23 10:53:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.03217959403991699 norm:0.00010603894770611078 max memory_allocated 22560.53857421875 
[2025-02-23 10:54:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.032152462750673294 norm:0.00011388102575438097 max memory_allocated 22560.53857421875 
[2025-02-23 10:55:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.032133426517248154 norm:0.00011477556108729914 max memory_allocated 22560.53857421875 
[2025-02-23 10:55:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.032110221683979034 norm:0.00011162118607899174 max memory_allocated 22560.53857421875 
[2025-02-23 10:55:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-23 10:56:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.05124849081039429 norm:0.0009539015591144562 max memory_allocated 22560.71044921875 
[2025-02-23 10:56:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.04371839016675949 norm:0.0004259113338775933 max memory_allocated 22560.71044921875 
[2025-02-23 10:57:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.03842366114258766 norm:0.00022260851983446628 max memory_allocated 22560.71044921875 
[2025-02-23 10:58:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.03654854744672775 norm:0.0001668938057264313 max memory_allocated 22560.71044921875 
[2025-02-23 10:58:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.035687003284692764 norm:0.0001438287436030805 max memory_allocated 22560.71044921875 
[2025-02-23 10:59:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.035315368324518204 norm:0.00012061650340911001 max memory_allocated 22560.71044921875 
[2025-02-23 10:59:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.035154737532138824 norm:0.00010824039054568857 max memory_allocated 22560.71044921875 
[2025-02-23 11:00:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.03505363315343857 norm:9.863103332463652e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:00:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.034961339086294174 norm:9.939547453541309e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:01:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.03497816622257233 norm:0.00010061882494483143 max memory_allocated 22560.71044921875 
[2025-02-23 11:01:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.03493472561240196 norm:9.114413114730269e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:02:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.03490809351205826 norm:8.721443737158552e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:03:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.03486829996109009 norm:8.655455894768238e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:03:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.03487036004662514 norm:8.882826659828424e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:04:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.034859154373407364 norm:8.899845852283761e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:04:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.03486953303217888 norm:9.349460015073419e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:05:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0348266139626503 norm:8.823565440252423e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:05:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.03480124846100807 norm:8.637612336315215e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:06:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.034826427698135376 norm:9.065322956303135e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:07:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.03482753038406372 norm:9.085285273613408e-05 max memory_allocated 22560.71044921875 
[2025-02-23 11:07:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-23 11:07:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.053968001157045364 norm:0.001753331278450787 max memory_allocated 22560.88232421875 
[2025-02-23 11:08:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.045806318521499634 norm:0.000619136611931026 max memory_allocated 22560.88232421875 
[2025-02-23 11:08:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.04023092985153198 norm:0.0002806766133289784 max memory_allocated 22560.88232421875 
[2025-02-23 11:09:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.03840961307287216 norm:0.00016670535842422396 max memory_allocated 22560.88232421875 
[2025-02-23 11:10:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.03764857351779938 norm:0.0001324850891251117 max memory_allocated 22560.88232421875 
[2025-02-23 11:10:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.03727343678474426 norm:0.00012399097613524646 max memory_allocated 22560.88232421875 
[2025-02-23 11:11:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0371018648147583 norm:0.00011374947644071653 max memory_allocated 22560.88232421875 
[2025-02-23 11:11:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.03696995601058006 norm:0.00010723869490902871 max memory_allocated 22560.88232421875 
[2025-02-23 11:12:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.036917030811309814 norm:9.663110540714115e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:12:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.03685548156499863 norm:9.885257168207318e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:13:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.036808937788009644 norm:9.335437789559364e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:13:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.036779750138521194 norm:9.321384277427569e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:14:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.03675581142306328 norm:9.302434045821428e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:15:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0367293544113636 norm:9.133118146564811e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:15:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.03670071065425873 norm:9.071321983356029e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:16:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.03668612241744995 norm:8.788418199401349e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:16:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.03667401149868965 norm:8.57636405271478e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:17:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.036658529192209244 norm:8.755006274441257e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:17:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.03663552924990654 norm:8.50941869430244e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:18:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.03663165867328644 norm:8.55910693644546e-05 max memory_allocated 22560.88232421875 
[2025-02-23 11:18:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-23 11:19:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.05589033663272858 norm:0.0008624509209766984 max memory_allocated 22561.05419921875 
[2025-02-23 11:19:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.048375360667705536 norm:0.00042009333265013993 max memory_allocated 22561.05419921875 
[2025-02-23 11:20:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.04277818277478218 norm:0.00023113584029488266 max memory_allocated 22561.05419921875 
[2025-02-23 11:20:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.040889378637075424 norm:0.00016970527940429747 max memory_allocated 22561.05419921875 
[2025-02-23 11:21:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.039969302713871 norm:0.00014587225450668484 max memory_allocated 22561.05419921875 
[2025-02-23 11:21:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.03949563950300217 norm:0.0001333461405010894 max memory_allocated 22561.05419921875 
[2025-02-23 11:22:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.039279304444789886 norm:0.00012289610458537936 max memory_allocated 22561.05419921875 
[2025-02-23 11:23:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.039150170981884 norm:0.0001177699159597978 max memory_allocated 22561.05419921875 
[2025-02-23 11:23:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.03901420906186104 norm:0.00010277876572217792 max memory_allocated 22561.05419921875 
[2025-02-23 11:24:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.038938745856285095 norm:9.722947288537398e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:24:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.03890839219093323 norm:9.991959086619318e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:25:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.038848280906677246 norm:9.334701462648809e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:25:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.03882283717393875 norm:9.160146146314219e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:26:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.03880937770009041 norm:9.184620284941047e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:27:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.03875807672739029 norm:8.935218647820875e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:27:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0387297198176384 norm:8.678676385898143e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:28:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.03874923661351204 norm:8.738747419556603e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:28:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0387398935854435 norm:8.636238635517657e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:29:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.038721535354852676 norm:8.848716242937371e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:29:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.038695722818374634 norm:8.715526200830936e-05 max memory_allocated 22561.05419921875 
[2025-02-23 11:29:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-23 11:30:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.05702611431479454 norm:0.0008258091402240098 max memory_allocated 22561.22607421875 
[2025-02-23 11:31:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.04952376335859299 norm:0.00034953371505253017 max memory_allocated 22561.22607421875 
[2025-02-23 11:31:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.04450862482190132 norm:0.00021348849986679852 max memory_allocated 22561.22607421875 
[2025-02-23 11:32:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.042700912803411484 norm:0.0001689932687440887 max memory_allocated 22561.22607421875 
[2025-02-23 11:32:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.041816454380750656 norm:0.00014033468323759735 max memory_allocated 22561.22607421875 
[2025-02-23 11:33:21 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.04134613648056984 norm:0.0001279003918170929 max memory_allocated 22561.22607421875 
[2025-02-23 11:33:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.041089389473199844 norm:0.00011371744039934129 max memory_allocated 22561.22607421875 
[2025-02-23 11:34:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.040968772023916245 norm:0.00010623673006193712 max memory_allocated 22561.22607421875 
[2025-02-23 11:35:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.04085425287485123 norm:9.947516809916124e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:35:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.04078778252005577 norm:9.506066999165341e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:36:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.040742311626672745 norm:9.115981811191887e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:36:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.04073072969913483 norm:9.129499085247517e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:37:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.04066716879606247 norm:8.568808698328212e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:37:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.04061374440789223 norm:8.4279352449812e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:38:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.040585681796073914 norm:8.383399108424783e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:38:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.040558867156505585 norm:8.491615881212056e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:39:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.040541283786296844 norm:8.696211443748325e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:40:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.040551748126745224 norm:8.467694715363905e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:40:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.04054420441389084 norm:8.567909389967099e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:41:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.04052753373980522 norm:8.550174970878288e-05 max memory_allocated 22561.22607421875 
[2025-02-23 11:41:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-23 11:41:56 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.06139034777879715 norm:0.0008721546619199216 max memory_allocated 22561.39794921875 
[2025-02-23 11:42:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.05336012318730354 norm:0.0003867529158014804 max memory_allocated 22561.39794921875 
[2025-02-23 11:43:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.047756291925907135 norm:0.00023443385725840926 max memory_allocated 22561.39794921875 
[2025-02-23 11:43:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.04583004117012024 norm:0.00017404044046998024 max memory_allocated 22561.39794921875 
[2025-02-23 11:44:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.044952116906642914 norm:0.00014880948583595455 max memory_allocated 22561.39794921875 
[2025-02-23 11:44:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.04450032487511635 norm:0.0001371153921354562 max memory_allocated 22561.39794921875 
[2025-02-23 11:45:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0442839153110981 norm:0.00012997306475881487 max memory_allocated 22561.39794921875 
[2025-02-23 11:45:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.044134385883808136 norm:0.00012166125816293061 max memory_allocated 22561.39794921875 
[2025-02-23 11:46:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.04399716854095459 norm:0.00010691739589674398 max memory_allocated 22561.39794921875 
[2025-02-23 11:46:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.043924592435359955 norm:0.00010216909140581265 max memory_allocated 22561.39794921875 
[2025-02-23 11:47:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.043890759348869324 norm:9.45577776292339e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:48:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.04381841793656349 norm:8.909928146749735e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:48:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.043767042458057404 norm:8.810673170955852e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:49:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.04374309256672859 norm:8.959596743807197e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:49:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.04370832443237305 norm:8.917385275708511e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:50:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.04368390887975693 norm:8.859686931828037e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:50:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.043678950518369675 norm:8.276705193566158e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:51:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.04366440325975418 norm:8.321047789650038e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:51:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.04365231841802597 norm:8.315668674185872e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:52:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.043664418160915375 norm:8.347212860826403e-05 max memory_allocated 22561.39794921875 
[2025-02-23 11:52:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-23 11:53:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.06322004646062851 norm:0.0006175687885843217 max memory_allocated 22561.56982421875 
[2025-02-23 11:53:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.05611613392829895 norm:0.00028086535166949034 max memory_allocated 22561.56982421875 
[2025-02-23 11:54:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.05056348443031311 norm:0.0001616888475837186 max memory_allocated 22561.56982421875 
[2025-02-23 11:54:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.048794616013765335 norm:0.00013748224591836333 max memory_allocated 22561.56982421875 
[2025-02-23 11:55:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.04796384274959564 norm:0.00014114991063252091 max memory_allocated 22561.56982421875 
[2025-02-23 11:56:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.047573961317539215 norm:0.00011379444185877219 max memory_allocated 22561.56982421875 
[2025-02-23 11:56:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.047352030873298645 norm:0.00010866869706660509 max memory_allocated 22561.56982421875 
[2025-02-23 11:57:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.047240763902664185 norm:9.846665489021689e-05 max memory_allocated 22561.56982421875 
[2025-02-23 11:57:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.047149933874607086 norm:0.00011424258991610259 max memory_allocated 22561.56982421875 
[2025-02-23 11:58:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.04711074382066727 norm:9.375678200740367e-05 max memory_allocated 22561.56982421875 
[2025-02-23 11:58:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0470382384955883 norm:9.173389116767794e-05 max memory_allocated 22561.56982421875 
[2025-02-23 11:59:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.04698216915130615 norm:8.951069321483374e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:00:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.04701242595911026 norm:9.693937317933887e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:00:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.04695924371480942 norm:8.68702627485618e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:01:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.046893585473299026 norm:8.76284102560021e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:01:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.04686678573489189 norm:8.68131683091633e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:02:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.046860527247190475 norm:9.426199540030211e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:02:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.04683128744363785 norm:8.862511458573863e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:03:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.04681025445461273 norm:8.787523984210566e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:03:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.04680055379867554 norm:8.712448470760137e-05 max memory_allocated 22561.56982421875 
[2025-02-23 12:04:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-23 12:04:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.07015661150217056 norm:0.0011681129690259695 max memory_allocated 22561.74169921875 
[2025-02-23 12:05:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.06147928535938263 norm:0.0004202241252642125 max memory_allocated 22561.74169921875 
[2025-02-23 12:05:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0553523525595665 norm:0.0002215364365838468 max memory_allocated 22561.74169921875 
[2025-02-23 12:06:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.053414009511470795 norm:0.00018445300520397723 max memory_allocated 22561.74169921875 
[2025-02-23 12:06:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.05243965610861778 norm:0.0001636239467188716 max memory_allocated 22561.74169921875 
[2025-02-23 12:07:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.05199512094259262 norm:0.00014505071158055216 max memory_allocated 22561.74169921875 
[2025-02-23 12:08:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.05172453448176384 norm:0.0001270025095436722 max memory_allocated 22561.74169921875 
[2025-02-23 12:08:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.05155900865793228 norm:0.00011459859524620697 max memory_allocated 22561.74169921875 
[2025-02-23 12:09:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.051447708159685135 norm:0.00012103074550395831 max memory_allocated 22561.74169921875 
[2025-02-23 12:09:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.05136580392718315 norm:0.0001098676075343974 max memory_allocated 22561.74169921875 
[2025-02-23 12:10:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.051288776099681854 norm:0.00010133632167708129 max memory_allocated 22561.74169921875 
[2025-02-23 12:10:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.05122867599129677 norm:9.782040433492512e-05 max memory_allocated 22561.74169921875 
[2025-02-23 12:11:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.05118471384048462 norm:0.00010023823415394872 max memory_allocated 22561.74169921875 
[2025-02-23 12:11:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.05113522335886955 norm:9.731957834446803e-05 max memory_allocated 22561.74169921875 
[2025-02-23 12:12:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.051084548234939575 norm:9.272628085454926e-05 max memory_allocated 22561.74169921875 
[2025-02-23 12:13:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.051069993525743484 norm:9.294455958297476e-05 max memory_allocated 22561.74169921875 
[2025-02-23 12:13:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.05105279013514519 norm:9.247328853234649e-05 max memory_allocated 22561.74169921875 
[2025-02-23 12:14:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.05101652815937996 norm:9.252000745618716e-05 max memory_allocated 22561.74169921875 
[2025-02-23 12:14:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.05099110305309296 norm:9.133049752563238e-05 max memory_allocated 22561.74169921875 
[2025-02-23 12:15:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.0509839691221714 norm:9.281175152864307e-05 max memory_allocated 22561.74169921875 
[2025-02-23 12:15:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-23 12:16:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.07772122323513031 norm:0.0013278850819915533 max memory_allocated 22561.91357421875 
[2025-02-23 12:16:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.06871423125267029 norm:0.0005287079839035869 max memory_allocated 22561.91357421875 
[2025-02-23 12:17:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.061850838363170624 norm:0.0002859993255697191 max memory_allocated 22561.91357421875 
[2025-02-23 12:17:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.05974053591489792 norm:0.00021078655845485628 max memory_allocated 22561.91357421875 
[2025-02-23 12:18:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.05878182500600815 norm:0.00018637123866938055 max memory_allocated 22561.91357421875 
[2025-02-23 12:18:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.05833010375499725 norm:0.00016892269195523113 max memory_allocated 22561.91357421875 
[2025-02-23 12:19:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.05809718370437622 norm:0.0001520195946795866 max memory_allocated 22561.91357421875 
[2025-02-23 12:19:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.057989656925201416 norm:0.00014179448771756142 max memory_allocated 22561.91357421875 
[2025-02-23 12:20:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.057848382741212845 norm:0.00013484024384524673 max memory_allocated 22561.91357421875 
[2025-02-23 12:21:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.057757213711738586 norm:0.0001260469580302015 max memory_allocated 22561.91357421875 
[2025-02-23 12:21:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.05765767768025398 norm:0.00012000403512502089 max memory_allocated 22561.91357421875 
[2025-02-23 12:22:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.05757984146475792 norm:0.00011551194620551541 max memory_allocated 22561.91357421875 
[2025-02-23 12:22:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.057488225400447845 norm:0.00011761711357394233 max memory_allocated 22561.91357421875 
[2025-02-23 12:23:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.05744008719921112 norm:0.00011355146125424653 max memory_allocated 22561.91357421875 
[2025-02-23 12:23:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.05741409957408905 norm:0.00011337100295349956 max memory_allocated 22561.91357421875 
[2025-02-23 12:24:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.05739340931177139 norm:0.00011227519280510023 max memory_allocated 22561.91357421875 
[2025-02-23 12:24:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.05735662952065468 norm:0.00011090927000623196 max memory_allocated 22561.91357421875 
[2025-02-23 12:25:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.05732128769159317 norm:0.00011140007700305432 max memory_allocated 22561.91357421875 
[2025-02-23 12:26:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.05732279643416405 norm:0.00011224871559534222 max memory_allocated 22561.91357421875 
[2025-02-23 12:26:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.05731629952788353 norm:0.0001114761907956563 max memory_allocated 22561.91357421875 
[2025-02-23 12:26:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-23 12:27:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.08606408536434174 norm:0.0011501265689730644 max memory_allocated 22562.08544921875 
[2025-02-23 12:27:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.07692403346300125 norm:0.00047172189806587994 max memory_allocated 22562.08544921875 
[2025-02-23 12:28:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.06992975622415543 norm:0.00027534831315279007 max memory_allocated 22562.08544921875 
[2025-02-23 12:29:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.06780874729156494 norm:0.00020448333816602826 max memory_allocated 22562.08544921875 
[2025-02-23 12:29:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.06683000922203064 norm:0.00017638658755458891 max memory_allocated 22562.08544921875 
[2025-02-23 12:30:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.06648048013448715 norm:0.00016912324645090848 max memory_allocated 22562.08544921875 
[2025-02-23 12:30:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.06622083485126495 norm:0.00014786183601245284 max memory_allocated 22562.08544921875 
[2025-02-23 12:31:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.06605193763971329 norm:0.00014175576507113874 max memory_allocated 22562.08544921875 
[2025-02-23 12:31:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.06595004349946976 norm:0.00013210458564572036 max memory_allocated 22562.08544921875 
[2025-02-23 12:32:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.065880686044693 norm:0.00012582636554725468 max memory_allocated 22562.08544921875 
[2025-02-23 12:33:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.06581497192382812 norm:0.00011937582166865468 max memory_allocated 22562.08544921875 
[2025-02-23 12:33:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.06571941822767258 norm:0.00011352825094945729 max memory_allocated 22562.08544921875 
[2025-02-23 12:34:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.065618135035038 norm:0.00011339439515722916 max memory_allocated 22562.08544921875 
[2025-02-23 12:34:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.06556765735149384 norm:0.0001131807075580582 max memory_allocated 22562.08544921875 
[2025-02-23 12:35:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.06550237536430359 norm:0.0001069006830221042 max memory_allocated 22562.08544921875 
[2025-02-23 12:35:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.06545419245958328 norm:0.0001051633880706504 max memory_allocated 22562.08544921875 
[2025-02-23 12:36:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.06540127843618393 norm:0.00010240021219942719 max memory_allocated 22562.08544921875 
[2025-02-23 12:36:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.06536760926246643 norm:0.00010481364734005183 max memory_allocated 22562.08544921875 
[2025-02-23 12:37:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.06537839025259018 norm:0.00010437385208206251 max memory_allocated 22562.08544921875 
[2025-02-23 12:38:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.06535673886537552 norm:0.00010164175182580948 max memory_allocated 22562.08544921875 
[2025-02-23 12:38:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-23 12:38:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.09747694432735443 norm:0.0011224114568904042 max memory_allocated 22562.25732421875 
[2025-02-23 12:39:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.08801031112670898 norm:0.000514692161232233 max memory_allocated 22562.25732421875 
[2025-02-23 12:39:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.08056627959012985 norm:0.00026843472733162344 max memory_allocated 22562.25732421875 
[2025-02-23 12:40:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.07835493981838226 norm:0.00021631579147651792 max memory_allocated 22562.25732421875 
[2025-02-23 12:41:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.07738795131444931 norm:0.0001876875467132777 max memory_allocated 22562.25732421875 
[2025-02-23 12:41:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.0770314633846283 norm:0.0001754004624672234 max memory_allocated 22562.25732421875 
[2025-02-23 12:42:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.07679688930511475 norm:0.00016003182099666446 max memory_allocated 22562.25732421875 
[2025-02-23 12:42:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.07656357437372208 norm:0.00015387014718726277 max memory_allocated 22562.25732421875 
[2025-02-23 12:43:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.07638543099164963 norm:0.00013224394933786243 max memory_allocated 22562.25732421875 
[2025-02-23 12:43:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.07629891484975815 norm:0.00013525616668630391 max memory_allocated 22562.25732421875 
[2025-02-23 12:44:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.07623262703418732 norm:0.00013194771599955857 max memory_allocated 22562.25732421875 
[2025-02-23 12:44:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.07612379640340805 norm:0.0001222270366270095 max memory_allocated 22562.25732421875 
[2025-02-23 12:45:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.07601642608642578 norm:0.00011991371866315603 max memory_allocated 22562.25732421875 
[2025-02-23 12:46:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.07594288140535355 norm:0.00012023004092043266 max memory_allocated 22562.25732421875 
[2025-02-23 12:46:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.07590030878782272 norm:0.00011997680849162862 max memory_allocated 22562.25732421875 
[2025-02-23 12:47:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.07585808634757996 norm:0.00011496363003971055 max memory_allocated 22562.25732421875 
[2025-02-23 12:47:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.07581518590450287 norm:0.00011379548232071102 max memory_allocated 22562.25732421875 
[2025-02-23 12:48:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.07578424364328384 norm:0.00011482223635539412 max memory_allocated 22562.25732421875 
[2025-02-23 12:48:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.0757564827799797 norm:0.00011222928878851235 max memory_allocated 22562.25732421875 
[2025-02-23 12:49:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.07572732865810394 norm:0.00011227376671740785 max memory_allocated 22562.25732421875 
[2025-02-23 12:49:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-23 12:50:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.11917992681264877 norm:0.003340404713526368 max memory_allocated 22562.42919921875 
[2025-02-23 12:50:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.105789415538311 norm:0.0010267820907756686 max memory_allocated 22562.42919921875 
[2025-02-23 12:51:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.09638627618551254 norm:0.00045652445987798274 max memory_allocated 22562.42919921875 
[2025-02-23 12:51:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.09366151690483093 norm:0.0003711140889208764 max memory_allocated 22562.42919921875 
[2025-02-23 12:52:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.09266291558742523 norm:0.0003618574992287904 max memory_allocated 22562.42919921875 
[2025-02-23 12:52:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0921451598405838 norm:0.0002947300672531128 max memory_allocated 22562.42919921875 
[2025-02-23 12:53:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.091866634786129 norm:0.0002888973103836179 max memory_allocated 22562.42919921875 
[2025-02-23 12:54:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.09157183766365051 norm:0.00024530227528885007 max memory_allocated 22562.42919921875 
[2025-02-23 12:54:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.09133079648017883 norm:0.00023357500322163105 max memory_allocated 22562.42919921875 
[2025-02-23 12:55:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.09115465730428696 norm:0.0002109713532263413 max memory_allocated 22562.42919921875 
[2025-02-23 12:55:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.09096129238605499 norm:0.00019952666480094194 max memory_allocated 22562.42919921875 
[2025-02-23 12:56:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.09080056846141815 norm:0.00018658024782780558 max memory_allocated 22562.42919921875 
[2025-02-23 12:56:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.09067251533269882 norm:0.0001827598171075806 max memory_allocated 22562.42919921875 
[2025-02-23 12:57:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.09063157439231873 norm:0.00017789661069400609 max memory_allocated 22562.42919921875 
[2025-02-23 12:57:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.090571828186512 norm:0.0001786936481948942 max memory_allocated 22562.42919921875 
[2025-02-23 12:58:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.09049539268016815 norm:0.00017830597062129527 max memory_allocated 22562.42919921875 
[2025-02-23 12:59:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.0904262363910675 norm:0.00016856443835422397 max memory_allocated 22562.42919921875 
[2025-02-23 12:59:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.09036129713058472 norm:0.0001698364649200812 max memory_allocated 22562.42919921875 
[2025-02-23 13:00:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.09035871177911758 norm:0.0001707835035631433 max memory_allocated 22562.42919921875 
[2025-02-23 13:00:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.09032119810581207 norm:0.00016149639850482345 max memory_allocated 22562.42919921875 
[2025-02-23 13:00:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-23 13:01:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.13514988124370575 norm:0.0009375734953209758 max memory_allocated 22562.60107421875 
[2025-02-23 13:02:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.12356320768594742 norm:0.00045405750279314816 max memory_allocated 22562.60107421875 
[2025-02-23 13:02:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.11376556754112244 norm:0.0003523027990013361 max memory_allocated 22562.60107421875 
[2025-02-23 13:03:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.11114080250263214 norm:0.0003315229550935328 max memory_allocated 22562.60107421875 
[2025-02-23 13:03:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.1099410206079483 norm:0.0003136041632387787 max memory_allocated 22562.60107421875 
[2025-02-23 13:04:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.10952772945165634 norm:0.0002863229310605675 max memory_allocated 22562.60107421875 
[2025-02-23 13:04:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.10925234854221344 norm:0.00029718177393078804 max memory_allocated 22562.60107421875 
[2025-02-23 13:05:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.10886767506599426 norm:0.00024334940826520324 max memory_allocated 22562.60107421875 
[2025-02-23 13:05:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.10865567624568939 norm:0.00025156044284813106 max memory_allocated 22562.60107421875 
[2025-02-23 13:06:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.1084337830543518 norm:0.00022814195835962892 max memory_allocated 22562.60107421875 
[2025-02-23 13:07:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.10829021781682968 norm:0.000230134348385036 max memory_allocated 22562.60107421875 
[2025-02-23 13:07:40 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.10816750675439835 norm:0.00023010304721537977 max memory_allocated 22562.60107421875 
[2025-02-23 13:08:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.10803551226854324 norm:0.0002332112198928371 max memory_allocated 22562.60107421875 
[2025-02-23 13:08:47 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.10791386663913727 norm:0.000231719488510862 max memory_allocated 22562.60107421875 
[2025-02-23 13:09:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.10782678425312042 norm:0.00023245449119713157 max memory_allocated 22562.60107421875 
[2025-02-23 13:09:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.10784227401018143 norm:0.00029385133530013263 max memory_allocated 22562.60107421875 
[2025-02-23 13:10:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.10771404206752777 norm:0.000233449914958328 max memory_allocated 22562.60107421875 
[2025-02-23 13:11:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.1076284721493721 norm:0.0002167842467315495 max memory_allocated 22562.60107421875 
[2025-02-23 13:11:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.10753116011619568 norm:0.0002468850580044091 max memory_allocated 22562.60107421875 
[2025-02-23 13:12:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.10750241577625275 norm:0.0002244072820758447 max memory_allocated 22562.60107421875 
[2025-02-23 13:12:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-23 13:12:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.15445999801158905 norm:0.0014992502983659506 max memory_allocated 22562.77294921875 
[2025-02-23 13:13:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.14186754822731018 norm:0.0006384831503964961 max memory_allocated 22562.77294921875 
[2025-02-23 13:14:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.13135136663913727 norm:0.00031232525361701846 max memory_allocated 22562.77294921875 
[2025-02-23 13:14:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.12843650579452515 norm:0.00026781708584167063 max memory_allocated 22562.77294921875 
[2025-02-23 13:15:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.12752413749694824 norm:0.00023183174198493361 max memory_allocated 22562.77294921875 
[2025-02-23 13:15:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.12703485786914825 norm:0.00020628435595426708 max memory_allocated 22562.77294921875 
[2025-02-23 13:16:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.1267206072807312 norm:0.00019797142886091024 max memory_allocated 22562.77294921875 
[2025-02-23 13:16:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.1264629364013672 norm:0.0001912365114549175 max memory_allocated 22562.77294921875 
[2025-02-23 13:17:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.12622813880443573 norm:0.00018106895731762052 max memory_allocated 22562.77294921875 
[2025-02-23 13:17:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.12605684995651245 norm:0.00017909060989040881 max memory_allocated 22562.77294921875 
[2025-02-23 13:18:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.1259644776582718 norm:0.00018844532314687967 max memory_allocated 22562.77294921875 
[2025-02-23 13:19:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.12580305337905884 norm:0.00018330175953451544 max memory_allocated 22562.77294921875 
[2025-02-23 13:19:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.12568816542625427 norm:0.00017834479513112456 max memory_allocated 22562.77294921875 
[2025-02-23 13:20:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.12558604776859283 norm:0.00017288934031967074 max memory_allocated 22562.77294921875 
[2025-02-23 13:20:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.12548144161701202 norm:0.00017465392011217773 max memory_allocated 22562.77294921875 
[2025-02-23 13:21:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.12537363171577454 norm:0.00016979403153527528 max memory_allocated 22562.77294921875 
[2025-02-23 13:21:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.12527607381343842 norm:0.00017071448382921517 max memory_allocated 22562.77294921875 
[2025-02-23 13:22:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.1251748502254486 norm:0.00017449544975534081 max memory_allocated 22562.77294921875 
[2025-02-23 13:22:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.12510745227336884 norm:0.00017172173829749227 max memory_allocated 22562.77294921875 
[2025-02-23 13:23:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.1250554323196411 norm:0.00017230919911526144 max memory_allocated 22562.77294921875 
[2025-02-23 13:23:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-23 13:24:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.1780734360218048 norm:0.0010051729623228312 max memory_allocated 22562.94482421875 
[2025-02-23 13:24:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.16515327990055084 norm:0.0005181312444619834 max memory_allocated 22562.94482421875 
[2025-02-23 13:25:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.15411201119422913 norm:0.0003324197605252266 max memory_allocated 22562.94482421875 
[2025-02-23 13:25:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.1510142982006073 norm:0.000290123833110556 max memory_allocated 22562.94482421875 
[2025-02-23 13:26:30 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.15008601546287537 norm:0.0002944680745713413 max memory_allocated 22562.94482421875 
[2025-02-23 13:27:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.14949975907802582 norm:0.00023953765048645437 max memory_allocated 22562.94482421875 
[2025-02-23 13:27:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.14915676414966583 norm:0.00023186834005173296 max memory_allocated 22562.94482421875 
[2025-02-23 13:28:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.14887133240699768 norm:0.00023209405480884016 max memory_allocated 22562.94482421875 
[2025-02-23 13:28:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.1486351042985916 norm:0.0002095180534524843 max memory_allocated 22562.94482421875 
[2025-02-23 13:29:18 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.1485014408826828 norm:0.00021295099577400833 max memory_allocated 22562.94482421875 
[2025-02-23 13:29:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.14829599857330322 norm:0.00020251545356586576 max memory_allocated 22562.94482421875 
[2025-02-23 13:30:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.14814333617687225 norm:0.00019357824930921197 max memory_allocated 22562.94482421875 
[2025-02-23 13:30:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.1480410248041153 norm:0.00019600553787313402 max memory_allocated 22562.94482421875 
[2025-02-23 13:31:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.14795231819152832 norm:0.00019833960686810315 max memory_allocated 22562.94482421875 
[2025-02-23 13:32:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.14781330525875092 norm:0.0001972358877537772 max memory_allocated 22562.94482421875 
[2025-02-23 13:32:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.14771243929862976 norm:0.00020088696328457445 max memory_allocated 22562.94482421875 
[2025-02-23 13:33:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.1476256549358368 norm:0.00019156781490892172 max memory_allocated 22562.94482421875 
[2025-02-23 13:33:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.14753969013690948 norm:0.00019381160382181406 max memory_allocated 22562.94482421875 
[2025-02-23 13:34:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.14754079282283783 norm:0.00019765962497331202 max memory_allocated 22562.94482421875 
[2025-02-23 13:34:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.14747430384159088 norm:0.0001917898771353066 max memory_allocated 22562.94482421875 
[2025-02-23 13:35:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-23 13:35:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.20339174568653107 norm:0.0018302000826224685 max memory_allocated 22563.11669921875 
[2025-02-23 13:36:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.18958830833435059 norm:0.000965846236795187 max memory_allocated 22563.11669921875 
[2025-02-23 13:36:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.1768202781677246 norm:0.00047544928384013474 max memory_allocated 22563.11669921875 
[2025-02-23 13:37:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.17335015535354614 norm:0.0003608866536524147 max memory_allocated 22563.11669921875 
[2025-02-23 13:37:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.17243987321853638 norm:0.0003047581994906068 max memory_allocated 22563.11669921875 
[2025-02-23 13:38:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.17202377319335938 norm:0.00030294348835013807 max memory_allocated 22563.11669921875 
[2025-02-23 13:39:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.17167295515537262 norm:0.0002590548247098923 max memory_allocated 22563.11669921875 
[2025-02-23 13:39:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.17141757905483246 norm:0.0002513520303182304 max memory_allocated 22563.11669921875 
[2025-02-23 13:40:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.17118249833583832 norm:0.0002327774272998795 max memory_allocated 22563.11669921875 
[2025-02-23 13:40:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.1709599792957306 norm:0.00021626160014420748 max memory_allocated 22563.11669921875 
[2025-02-23 13:41:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.17076554894447327 norm:0.0002059927355730906 max memory_allocated 22563.11669921875 
[2025-02-23 13:41:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.1706181913614273 norm:0.0002011867327382788 max memory_allocated 22563.11669921875 
[2025-02-23 13:42:21 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.17052319645881653 norm:0.00019851859542541206 max memory_allocated 22563.11669921875 
[2025-02-23 13:42:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.17044396698474884 norm:0.0001854672300396487 max memory_allocated 22563.11669921875 
[2025-02-23 13:43:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.1703459769487381 norm:0.00018483954772818834 max memory_allocated 22563.11669921875 
[2025-02-23 13:44:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.17024946212768555 norm:0.0001806628715712577 max memory_allocated 22563.11669921875 
[2025-02-23 13:44:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.17017242312431335 norm:0.0001802031765691936 max memory_allocated 22563.11669921875 
[2025-02-23 13:45:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.17009946703910828 norm:0.00017672977992333472 max memory_allocated 22563.11669921875 
[2025-02-23 13:45:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.17002664506435394 norm:0.00017580509302206337 max memory_allocated 22563.11669921875 
[2025-02-23 13:46:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.16998909413814545 norm:0.00017353730800095946 max memory_allocated 22563.11669921875 
[2025-02-23 13:46:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-23 13:47:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.23109357059001923 norm:0.0013533029705286026 max memory_allocated 22563.28857421875 
[2025-02-23 13:47:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.21648895740509033 norm:0.0006092074327170849 max memory_allocated 22563.28857421875 
[2025-02-23 13:48:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.20311179757118225 norm:0.0003096992149949074 max memory_allocated 22563.28857421875 
[2025-02-23 13:48:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.19973114132881165 norm:0.00026591098867356777 max memory_allocated 22563.28857421875 
[2025-02-23 13:49:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.19883552193641663 norm:0.0002465544966980815 max memory_allocated 22563.28857421875 
[2025-02-23 13:49:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.19844628870487213 norm:0.00023403263185173273 max memory_allocated 22563.28857421875 
[2025-02-23 13:50:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.19813382625579834 norm:0.0002064170694211498 max memory_allocated 22563.28857421875 
[2025-02-23 13:50:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.19782550632953644 norm:0.00019067490939050913 max memory_allocated 22563.28857421875 
[2025-02-23 13:51:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.19760560989379883 norm:0.0001796224241843447 max memory_allocated 22563.28857421875 
[2025-02-23 13:52:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.197403684258461 norm:0.0001738670835038647 max memory_allocated 22563.28857421875 
[2025-02-23 13:52:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.197249174118042 norm:0.00017643364844843745 max memory_allocated 22563.28857421875 
[2025-02-23 13:53:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.197151780128479 norm:0.00017423681856598705 max memory_allocated 22563.28857421875 
[2025-02-23 13:53:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.19703985750675201 norm:0.00017010673764161766 max memory_allocated 22563.28857421875 
[2025-02-23 13:54:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.19691230356693268 norm:0.00017006316920742393 max memory_allocated 22563.28857421875 
[2025-02-23 13:54:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.1967773288488388 norm:0.00016636615328025073 max memory_allocated 22563.28857421875 
[2025-02-23 13:55:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.19666177034378052 norm:0.00016514348681084812 max memory_allocated 22563.28857421875 
[2025-02-23 13:55:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.19659101963043213 norm:0.00016469723777845502 max memory_allocated 22563.28857421875 
[2025-02-23 13:56:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.19651946425437927 norm:0.00016540588694624603 max memory_allocated 22563.28857421875 
[2025-02-23 13:57:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.1964622437953949 norm:0.00016527778643649071 max memory_allocated 22563.28857421875 
[2025-02-23 13:57:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.19642099738121033 norm:0.00016661998233757913 max memory_allocated 22563.28857421875 
[2025-02-23 13:57:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-23 13:58:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.26352933049201965 norm:0.0020048292353749275 max memory_allocated 22563.46044921875 
[2025-02-23 13:58:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.24759915471076965 norm:0.0008308372925966978 max memory_allocated 22563.46044921875 
[2025-02-23 13:59:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.2330268919467926 norm:0.00036378277582116425 max memory_allocated 22563.46044921875 
[2025-02-23 14:00:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.2293001264333725 norm:0.0003133266291115433 max memory_allocated 22563.46044921875 
[2025-02-23 14:00:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.22840173542499542 norm:0.00026245927438139915 max memory_allocated 22563.46044921875 
[2025-02-23 14:01:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.2279856652021408 norm:0.0002564409514889121 max memory_allocated 22563.46044921875 
[2025-02-23 14:01:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.22761841118335724 norm:0.00023060638341121376 max memory_allocated 22563.46044921875 
[2025-02-23 14:02:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.2273334562778473 norm:0.00021978880977258086 max memory_allocated 22563.46044921875 
[2025-02-23 14:02:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.22707277536392212 norm:0.00021011120406910777 max memory_allocated 22563.46044921875 
[2025-02-23 14:03:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.22678647935390472 norm:0.00020349126134533435 max memory_allocated 22563.46044921875 
[2025-02-23 14:03:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.22663535177707672 norm:0.00020534059149213135 max memory_allocated 22563.46044921875 
[2025-02-23 14:04:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.2264549285173416 norm:0.00019924108346458524 max memory_allocated 22563.46044921875 
[2025-02-23 14:05:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.22628381848335266 norm:0.0001983274705708027 max memory_allocated 22563.46044921875 
[2025-02-23 14:05:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.22614961862564087 norm:0.00019775649707298726 max memory_allocated 22563.46044921875 
[2025-02-23 14:06:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.2260858416557312 norm:0.00020026681886520237 max memory_allocated 22563.46044921875 
[2025-02-23 14:06:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.2260218858718872 norm:0.00019944597443100065 max memory_allocated 22563.46044921875 
[2025-02-23 14:07:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.225976824760437 norm:0.00019783298193942755 max memory_allocated 22563.46044921875 
[2025-02-23 14:07:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.22587847709655762 norm:0.00019694690126925707 max memory_allocated 22563.46044921875 
[2025-02-23 14:08:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.22581394016742706 norm:0.00019420112948864698 max memory_allocated 22563.46044921875 
[2025-02-23 14:09:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.22572824358940125 norm:0.00019061830244027078 max memory_allocated 22563.46044921875 
[2025-02-23 14:09:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-23 14:09:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.29164057970046997 norm:0.0008775004534982145 max memory_allocated 22563.63232421875 
[2025-02-23 14:10:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.2761848568916321 norm:0.0005202481988817453 max memory_allocated 22563.63232421875 
[2025-02-23 14:10:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.262248158454895 norm:0.00033814049675129354 max memory_allocated 22563.63232421875 
[2025-02-23 14:11:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.258734792470932 norm:0.0002891035401262343 max memory_allocated 22563.63232421875 
[2025-02-23 14:12:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.25787755846977234 norm:0.00028197382926009595 max memory_allocated 22563.63232421875 
[2025-02-23 14:12:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.25741279125213623 norm:0.0002264951472170651 max memory_allocated 22563.63232421875 
[2025-02-23 14:13:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.2570289969444275 norm:0.00022029114188626409 max memory_allocated 22563.63232421875 
[2025-02-23 14:13:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.25682544708251953 norm:0.00021147157531231642 max memory_allocated 22563.63232421875 
[2025-02-23 14:14:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.25653618574142456 norm:0.00019977407646365464 max memory_allocated 22563.63232421875 
[2025-02-23 14:14:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.2563847303390503 norm:0.00020124478032812476 max memory_allocated 22563.63232421875 
[2025-02-23 14:15:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.2561555802822113 norm:0.00019840581808239222 max memory_allocated 22563.63232421875 
[2025-02-23 14:15:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.2561163604259491 norm:0.0001977579086087644 max memory_allocated 22563.63232421875 
[2025-02-23 14:16:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.25595319271087646 norm:0.0001929138379637152 max memory_allocated 22563.63232421875 
[2025-02-23 14:17:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.2558276653289795 norm:0.00018775658099912107 max memory_allocated 22563.63232421875 
[2025-02-23 14:17:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.25570428371429443 norm:0.0001900665956782177 max memory_allocated 22563.63232421875 
[2025-02-23 14:18:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.25561946630477905 norm:0.0001922199735417962 max memory_allocated 22563.63232421875 
[2025-02-23 14:18:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.2555168569087982 norm:0.000196708831936121 max memory_allocated 22563.63232421875 
[2025-02-23 14:19:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.25552162528038025 norm:0.000201226634089835 max memory_allocated 22563.63232421875 
[2025-02-23 14:19:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.2554675340652466 norm:0.00021473319793585688 max memory_allocated 22563.63232421875 
[2025-02-23 14:20:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.25538092851638794 norm:0.0001946936536114663 max memory_allocated 22563.63232421875 
[2025-02-23 14:20:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-23 14:20:36 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 14:21:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.34254348278045654 norm:0.019521664828062057 max memory_allocated 22563.91943359375 
[2025-02-23 14:21:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.3221290409564972 norm:0.015071962028741837 max memory_allocated 22563.91943359375 
[2025-02-23 14:22:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.3051377534866333 norm:0.010033838450908661 max memory_allocated 22563.91943359375 
[2025-02-23 14:22:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.30051901936531067 norm:0.008533879183232784 max memory_allocated 22563.91943359375 
[2025-02-23 14:23:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.2992381155490875 norm:0.007352650631219149 max memory_allocated 22563.91943359375 
[2025-02-23 14:23:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.29844212532043457 norm:0.006350406911224127 max memory_allocated 22563.91943359375 
[2025-02-23 14:24:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.2977581024169922 norm:0.005397738888859749 max memory_allocated 22563.91943359375 
[2025-02-23 14:25:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.29723867774009705 norm:0.004749460145831108 max memory_allocated 22563.91943359375 
[2025-02-23 14:25:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.2968943417072296 norm:0.004488012287765741 max memory_allocated 22563.91943359375 
[2025-02-23 14:26:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.2966923713684082 norm:0.004427629988640547 max memory_allocated 22563.91943359375 
[2025-02-23 14:26:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.2965368628501892 norm:0.0044181146658957005 max memory_allocated 22563.91943359375 
[2025-02-23 14:27:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.29646390676498413 norm:0.004317381419241428 max memory_allocated 22563.91943359375 
[2025-02-23 14:27:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.2961956262588501 norm:0.004201877862215042 max memory_allocated 22563.91943359375 
[2025-02-23 14:28:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.2959904372692108 norm:0.003926242236047983 max memory_allocated 22563.91943359375 
[2025-02-23 14:29:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.29577669501304626 norm:0.0039020113181322813 max memory_allocated 22563.91943359375 
[2025-02-23 14:29:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.2957742214202881 norm:0.00399853615090251 max memory_allocated 22563.91943359375 
[2025-02-23 14:30:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.2956579029560089 norm:0.003883955767378211 max memory_allocated 22563.91943359375 
[2025-02-23 14:30:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.2954895496368408 norm:0.003781896084547043 max memory_allocated 22563.91943359375 
[2025-02-23 14:31:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.29544392228126526 norm:0.00365334190428257 max memory_allocated 22563.91943359375 
[2025-02-23 14:31:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.29539671540260315 norm:0.0036299291532486677 max memory_allocated 22563.91943359375 
[2025-02-23 14:31:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-23 14:32:01 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 14:32:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.3942347466945648 norm:0.022008642554283142 max memory_allocated 22564.09130859375 
[2025-02-23 14:33:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.3717848062515259 norm:0.01566893979907036 max memory_allocated 22564.09130859375 
[2025-02-23 14:33:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.3528089225292206 norm:0.010425820015370846 max memory_allocated 22564.09130859375 
[2025-02-23 14:34:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.348368763923645 norm:0.008788651786744595 max memory_allocated 22564.09130859375 
[2025-02-23 14:34:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.3471192419528961 norm:0.007578406948596239 max memory_allocated 22564.09130859375 
[2025-02-23 14:35:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.3463713526725769 norm:0.006542284041643143 max memory_allocated 22564.09130859375 
[2025-02-23 14:35:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.34569868445396423 norm:0.005652128718793392 max memory_allocated 22564.09130859375 
[2025-02-23 14:36:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.3451552391052246 norm:0.0049118525348603725 max memory_allocated 22564.09130859375 
[2025-02-23 14:37:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.3448179066181183 norm:0.004615110345184803 max memory_allocated 22564.09130859375 
[2025-02-23 14:37:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.3448023200035095 norm:0.004852769896388054 max memory_allocated 22564.09130859375 
[2025-02-23 14:38:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.3446193337440491 norm:0.004937967285513878 max memory_allocated 22564.09130859375 
[2025-02-23 14:38:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.34437844157218933 norm:0.004404010251164436 max memory_allocated 22564.09130859375 
[2025-02-23 14:39:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.3440701365470886 norm:0.004068341571837664 max memory_allocated 22564.09130859375 
[2025-02-23 14:39:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.34383463859558105 norm:0.0038610438350588083 max memory_allocated 22564.09130859375 
[2025-02-23 14:40:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.34380191564559937 norm:0.003856776049360633 max memory_allocated 22564.09130859375 
[2025-02-23 14:41:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.3437032103538513 norm:0.003839218057692051 max memory_allocated 22564.09130859375 
[2025-02-23 14:41:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.34356966614723206 norm:0.0037768271286040545 max memory_allocated 22564.09130859375 
[2025-02-23 14:42:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.34352895617485046 norm:0.0036745688412338495 max memory_allocated 22564.09130859375 
[2025-02-23 14:42:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.3434309959411621 norm:0.0036412517074495554 max memory_allocated 22564.09130859375 
[2025-02-23 14:43:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.34350892901420593 norm:0.0035074343904852867 max memory_allocated 22564.09130859375 
[2025-02-23 14:43:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-23 14:43:27 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 14:44:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.5310629606246948 norm:0.03476608544588089 max memory_allocated 22564.26318359375 
[2025-02-23 14:44:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.488840252161026 norm:0.023464443162083626 max memory_allocated 22564.26318359375 
[2025-02-23 14:45:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.45459768176078796 norm:0.016113372519612312 max memory_allocated 22564.26318359375 
[2025-02-23 14:45:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.4470862150192261 norm:0.013640561141073704 max memory_allocated 22564.26318359375 
[2025-02-23 14:46:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.4445230960845947 norm:0.011781838722527027 max memory_allocated 22564.26318359375 
[2025-02-23 14:46:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.4428110718727112 norm:0.010548919439315796 max memory_allocated 22564.26318359375 
[2025-02-23 14:47:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.442331999540329 norm:0.009552692994475365 max memory_allocated 22564.26318359375 
[2025-02-23 14:47:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.44228705763816833 norm:0.009098952636122704 max memory_allocated 22564.26318359375 
[2025-02-23 14:48:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.44179534912109375 norm:0.009277023375034332 max memory_allocated 22564.26318359375 
[2025-02-23 14:49:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.44211632013320923 norm:0.008723457343876362 max memory_allocated 22564.26318359375 
[2025-02-23 14:49:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.44166427850723267 norm:0.008813167922198772 max memory_allocated 22564.26318359375 
[2025-02-23 14:50:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.4417499601840973 norm:0.009928216226398945 max memory_allocated 22564.26318359375 
[2025-02-23 14:50:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.44097089767456055 norm:0.009923264384269714 max memory_allocated 22564.26318359375 
[2025-02-23 14:51:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.4408377408981323 norm:0.009940506890416145 max memory_allocated 22564.26318359375 
[2025-02-23 14:51:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.4407302737236023 norm:0.010146350599825382 max memory_allocated 22564.26318359375 
[2025-02-23 14:52:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.44044411182403564 norm:0.010021484456956387 max memory_allocated 22564.26318359375 
[2025-02-23 14:52:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.44023430347442627 norm:0.010017144493758678 max memory_allocated 22564.26318359375 
[2025-02-23 14:53:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.4402013421058655 norm:0.010380932129919529 max memory_allocated 22564.26318359375 
[2025-02-23 14:54:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.43982139229774475 norm:0.010273049585521221 max memory_allocated 22564.26318359375 
[2025-02-23 14:54:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.44037574529647827 norm:0.01091225165873766 max memory_allocated 22564.26318359375 
[2025-02-23 14:54:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-23 14:54:52 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 14:55:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.9209684133529663 norm:0.08675145357847214 max memory_allocated 22564.43505859375 
[2025-02-23 14:55:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.825003981590271 norm:0.06069823354482651 max memory_allocated 22564.43505859375 
[2025-02-23 14:56:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.7550480365753174 norm:0.041567448526620865 max memory_allocated 22564.43505859375 
[2025-02-23 14:57:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.7373449802398682 norm:0.037645015865564346 max memory_allocated 22564.43505859375 
[2025-02-23 14:57:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.7295486927032471 norm:0.03308071568608284 max memory_allocated 22564.43505859375 
[2025-02-23 14:58:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.723990797996521 norm:0.03096405230462551 max memory_allocated 22564.43505859375 
[2025-02-23 14:58:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.7196930646896362 norm:0.02895173616707325 max memory_allocated 22564.43505859375 
[2025-02-23 14:59:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.7173919677734375 norm:0.02942795865237713 max memory_allocated 22564.43505859375 
[2025-02-23 14:59:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.7146223187446594 norm:0.027383731678128242 max memory_allocated 22564.43505859375 
[2025-02-23 15:00:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.7125447988510132 norm:0.02591978758573532 max memory_allocated 22564.43505859375 
[2025-02-23 15:01:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.7110967040061951 norm:0.024982238188385963 max memory_allocated 22564.43505859375 
[2025-02-23 15:01:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.7098303437232971 norm:0.02510947734117508 max memory_allocated 22564.43505859375 
[2025-02-23 15:02:08 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.7105588912963867 norm:0.026799507439136505 max memory_allocated 22564.43505859375 
[2025-02-23 15:02:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.710118293762207 norm:0.027231404557824135 max memory_allocated 22564.43505859375 
[2025-02-23 15:03:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.7108213901519775 norm:0.027757689356803894 max memory_allocated 22564.43505859375 
[2025-02-23 15:03:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.7096163630485535 norm:0.026209350675344467 max memory_allocated 22564.43505859375 
[2025-02-23 15:04:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.7083675861358643 norm:0.024599676951766014 max memory_allocated 22564.43505859375 
[2025-02-23 15:04:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.707253098487854 norm:0.023665200918912888 max memory_allocated 22564.43505859375 
[2025-02-23 15:05:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.7075732946395874 norm:0.02387404441833496 max memory_allocated 22564.43505859375 
[2025-02-23 15:06:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.7070872187614441 norm:0.02431400865316391 max memory_allocated 22564.43505859375 
[2025-02-23 15:06:13 root] (main_calibration.py 365): INFO 21858.3894302845
[2025-02-23 15:06:44 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-23 15:07:55 root] (main_calibration.py 158): INFO wikitext2 : 5.843642234802246
[2025-02-23 15:07:56 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-23 15:09:46 root] (main_calibration.py 158): INFO c4 : 7.321895599365234
[2025-02-23 16:42:33 root] (main_calibration.py 169): INFO {'wikitext2': 5.843642234802246, 'c4': 7.321895599365234, 'results': {'piqa': {'acc': 0.7818280739934712, 'acc_stderr': 0.009636081958374381, 'acc_norm': 0.7709466811751904, 'acc_norm_stderr': 0.009804509865175505}, 'boolq': {'acc': 0.7241590214067278, 'acc_stderr': 0.007816978272864554}, 'winogrande': {'acc': 0.6582478295185478, 'acc_stderr': 0.013330103018622861}, 'hellaswag': {'acc': 0.5512846046604262, 'acc_stderr': 0.0049634646577472385, 'acc_norm': 0.7146982672774348, 'acc_norm_stderr': 0.004506351723820972}, 'arc_challenge': {'acc': 0.371160409556314, 'acc_stderr': 0.014117971901142815, 'acc_norm': 0.39334470989761094, 'acc_norm_stderr': 0.014275101465693026}, 'arc_easy': {'acc': 0.6675084175084175, 'acc_stderr': 0.009666892606130115, 'acc_norm': 0.5164141414141414, 'acc_norm_stderr': 0.010254253565929307}}, 'versions': {'piqa': 0, 'boolq': 1, 'winogrande': 0, 'hellaswag': 0, 'arc_challenge': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
