[2025-03-26 12:30:36 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-7b-hf-w4a7', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-26 12:30:44 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-26 12:30:44 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-26 12:30:44 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-26 12:30:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-26 12:30:51 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 12:31:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.010605819523334503 norm:0.013461535796523094 max memory_allocated 22562.10693359375 
[2025-03-26 12:31:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0062210438773036 norm:0.007844100706279278 max memory_allocated 22562.10693359375 
[2025-03-26 12:32:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.004704018123447895 norm:0.0057758972980082035 max memory_allocated 22562.10693359375 
[2025-03-26 12:33:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0040665422566235065 norm:0.004608341958373785 max memory_allocated 22562.10693359375 
[2025-03-26 12:33:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.003822813741862774 norm:0.003788566682487726 max memory_allocated 22562.10693359375 
[2025-03-26 12:34:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0037597655318677425 norm:0.0033944761380553246 max memory_allocated 22562.10693359375 
[2025-03-26 12:34:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0037257238291203976 norm:0.0029963834676891565 max memory_allocated 22562.10693359375 
[2025-03-26 12:35:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.003594706067815423 norm:0.0025158398784697056 max memory_allocated 22562.10693359375 
[2025-03-26 12:35:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0035786652006208897 norm:0.002269701100885868 max memory_allocated 22562.10693359375 
[2025-03-26 12:36:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0036124158650636673 norm:0.0019410663517192006 max memory_allocated 22562.10693359375 
[2025-03-26 12:36:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.003516854252666235 norm:0.00178426131606102 max memory_allocated 22562.10693359375 
[2025-03-26 12:37:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.003503361949697137 norm:0.001587386941537261 max memory_allocated 22562.10693359375 
[2025-03-26 12:38:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.003479379927739501 norm:0.0014213707763701677 max memory_allocated 22562.10693359375 
[2025-03-26 12:38:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0034103919751942158 norm:0.0013343400787562132 max memory_allocated 22562.10693359375 
[2025-03-26 12:39:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0034689304884523153 norm:0.0013481222558766603 max memory_allocated 22562.10693359375 
[2025-03-26 12:39:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.003483699169009924 norm:0.0012874577660113573 max memory_allocated 22562.10693359375 
[2025-03-26 12:40:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0034251019824296236 norm:0.001136773033067584 max memory_allocated 22562.10693359375 
[2025-03-26 12:40:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.003365977667272091 norm:0.0010159226367250085 max memory_allocated 22562.10693359375 
[2025-03-26 12:41:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0034252770710736513 norm:0.0010384388733655214 max memory_allocated 22562.10693359375 
[2025-03-26 12:42:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0034185005351901054 norm:0.0010336587438359857 max memory_allocated 22562.10693359375 
[2025-03-26 12:42:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-26 12:42:15 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 12:42:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.04179774969816208 norm:0.028609376400709152 max memory_allocated 22562.27880859375 
[2025-03-26 12:43:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.030573351308703423 norm:0.0191652774810791 max memory_allocated 22562.27880859375 
[2025-03-26 12:43:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.021990599110722542 norm:0.009462838992476463 max memory_allocated 22562.27880859375 
[2025-03-26 12:44:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.020119737833738327 norm:0.009972963482141495 max memory_allocated 22562.27880859375 
[2025-03-26 12:45:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.019734108820557594 norm:0.012912968173623085 max memory_allocated 22562.27880859375 
[2025-03-26 12:45:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.019381430000066757 norm:0.008736036717891693 max memory_allocated 22562.27880859375 
[2025-03-26 12:46:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.018957890570163727 norm:0.008581063710153103 max memory_allocated 22562.27880859375 
[2025-03-26 12:46:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.019176458939909935 norm:0.008746184408664703 max memory_allocated 22562.27880859375 
[2025-03-26 12:47:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.018504412844777107 norm:0.0087051410228014 max memory_allocated 22562.27880859375 
[2025-03-26 12:47:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.018351402133703232 norm:0.00863892026245594 max memory_allocated 22562.27880859375 
[2025-03-26 12:48:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.017935456708073616 norm:0.007871191948652267 max memory_allocated 22562.27880859375 
[2025-03-26 12:48:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.017726656049489975 norm:0.0076932646334171295 max memory_allocated 22562.27880859375 
[2025-03-26 12:49:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.018361061811447144 norm:0.007711910642683506 max memory_allocated 22562.27880859375 
[2025-03-26 12:50:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.017945779487490654 norm:0.007570629008114338 max memory_allocated 22562.27880859375 
[2025-03-26 12:50:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.01807091012597084 norm:0.0076958113349974155 max memory_allocated 22562.27880859375 
[2025-03-26 12:51:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.017681732773780823 norm:0.007241119164973497 max memory_allocated 22562.27880859375 
[2025-03-26 12:51:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.01784304529428482 norm:0.007012797053903341 max memory_allocated 22562.27880859375 
[2025-03-26 12:52:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.017478937283158302 norm:0.006726717576384544 max memory_allocated 22562.27880859375 
[2025-03-26 12:52:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.017708001658320427 norm:0.006236468441784382 max memory_allocated 22562.27880859375 
[2025-03-26 12:53:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.01805363968014717 norm:0.006223658099770546 max memory_allocated 22562.27880859375 
[2025-03-26 12:53:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-26 12:53:41 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 12:54:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.037030838429927826 norm:0.018749389797449112 max memory_allocated 22562.45068359375 
[2025-03-26 12:54:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.027165774255990982 norm:0.011595374904572964 max memory_allocated 22562.45068359375 
[2025-03-26 12:55:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.02284037321805954 norm:0.007935920730233192 max memory_allocated 22562.45068359375 
[2025-03-26 12:55:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.021419569849967957 norm:0.006102708168327808 max memory_allocated 22562.45068359375 
[2025-03-26 12:56:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.020628588274121284 norm:0.005140133202075958 max memory_allocated 22562.45068359375 
[2025-03-26 12:57:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.020025789737701416 norm:0.0042269472032785416 max memory_allocated 22562.45068359375 
[2025-03-26 12:57:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.019672250375151634 norm:0.003573258174583316 max memory_allocated 22562.45068359375 
[2025-03-26 12:58:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.019452650099992752 norm:0.0030422245617955923 max memory_allocated 22562.45068359375 
[2025-03-26 12:58:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.019318269565701485 norm:0.0025645915884524584 max memory_allocated 22562.45068359375 
[2025-03-26 12:59:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.019209910184144974 norm:0.0021245491225272417 max memory_allocated 22562.45068359375 
[2025-03-26 12:59:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.01914539374411106 norm:0.001753775984980166 max memory_allocated 22562.45068359375 
[2025-03-26 13:00:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.019132565706968307 norm:0.001712236786261201 max memory_allocated 22562.45068359375 
[2025-03-26 13:01:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.01913256198167801 norm:0.0017223558388650417 max memory_allocated 22562.45068359375 
[2025-03-26 13:01:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.01910175010561943 norm:0.0016185037093237042 max memory_allocated 22562.45068359375 
[2025-03-26 13:02:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.019084185361862183 norm:0.0015177055029198527 max memory_allocated 22562.45068359375 
[2025-03-26 13:02:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.019051991403102875 norm:0.0014525270089507103 max memory_allocated 22562.45068359375 
[2025-03-26 13:03:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.01904962956905365 norm:0.0013712085783481598 max memory_allocated 22562.45068359375 
[2025-03-26 13:03:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.019077513366937637 norm:0.0013545387191697955 max memory_allocated 22562.45068359375 
[2025-03-26 13:04:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.019135378301143646 norm:0.0013182684779167175 max memory_allocated 22562.45068359375 
[2025-03-26 13:04:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.019116753712296486 norm:0.0012169646797701716 max memory_allocated 22562.45068359375 
[2025-03-26 13:05:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-26 13:05:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.04381805658340454 norm:0.006820226088166237 max memory_allocated 22562.50732421875 
[2025-03-26 13:06:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.033098455518484116 norm:0.0017081164987757802 max memory_allocated 22562.50732421875 
[2025-03-26 13:06:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.02783048152923584 norm:0.0011386843398213387 max memory_allocated 22562.50732421875 
[2025-03-26 13:07:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.026009704917669296 norm:0.0008256332948803902 max memory_allocated 22562.50732421875 
[2025-03-26 13:07:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.025050634518265724 norm:0.0006743657868355513 max memory_allocated 22562.50732421875 
[2025-03-26 13:08:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.02442796155810356 norm:0.0005444352282211185 max memory_allocated 22562.50732421875 
[2025-03-26 13:09:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.02422415092587471 norm:0.00047413044376298785 max memory_allocated 22562.50732421875 
[2025-03-26 13:09:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.024106156080961227 norm:0.00043251775787211955 max memory_allocated 22562.50732421875 
[2025-03-26 13:10:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0240015871822834 norm:0.0003899261064361781 max memory_allocated 22562.50732421875 
[2025-03-26 13:10:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.024005020037293434 norm:0.00035772216506302357 max memory_allocated 22562.50732421875 
[2025-03-26 13:11:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.023929834365844727 norm:0.0003062360337935388 max memory_allocated 22562.50732421875 
[2025-03-26 13:11:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.023877209052443504 norm:0.00029033570899628103 max memory_allocated 22562.50732421875 
[2025-03-26 13:12:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.02382492832839489 norm:0.0002757816109806299 max memory_allocated 22562.50732421875 
[2025-03-26 13:13:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.02387947030365467 norm:0.0002791969745885581 max memory_allocated 22562.50732421875 
[2025-03-26 13:13:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.023839715868234634 norm:0.0002552965597715229 max memory_allocated 22562.50732421875 
[2025-03-26 13:14:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.02384382113814354 norm:0.0002517341636121273 max memory_allocated 22562.50732421875 
[2025-03-26 13:14:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.023817043751478195 norm:0.0002576112747192383 max memory_allocated 22562.50732421875 
[2025-03-26 13:15:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.023812251165509224 norm:0.0002402740065008402 max memory_allocated 22562.50732421875 
[2025-03-26 13:15:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.023856129497289658 norm:0.00025025810464285314 max memory_allocated 22562.50732421875 
[2025-03-26 13:16:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.023857375606894493 norm:0.00024486216716468334 max memory_allocated 22562.50732421875 
[2025-03-26 13:16:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-26 13:17:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.05167873576283455 norm:0.004146833438426256 max memory_allocated 22562.67919921875 
[2025-03-26 13:17:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.03913584351539612 norm:0.001207975554279983 max memory_allocated 22562.67919921875 
[2025-03-26 13:18:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.03213028237223625 norm:0.0007108304416760802 max memory_allocated 22562.67919921875 
[2025-03-26 13:18:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.030006524175405502 norm:0.0005069777253083885 max memory_allocated 22562.67919921875 
[2025-03-26 13:19:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.02900202013552189 norm:0.00040619197534397244 max memory_allocated 22562.67919921875 
[2025-03-26 13:19:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.028498588129878044 norm:0.0003453154640737921 max memory_allocated 22562.67919921875 
[2025-03-26 13:20:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.02820814959704876 norm:0.00029784225625917315 max memory_allocated 22562.67919921875 
[2025-03-26 13:21:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.028068389743566513 norm:0.0002881925320252776 max memory_allocated 22562.67919921875 
[2025-03-26 13:21:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.027989957481622696 norm:0.00025119876954704523 max memory_allocated 22562.67919921875 
[2025-03-26 13:22:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.02799452655017376 norm:0.00027162645710632205 max memory_allocated 22562.67919921875 
[2025-03-26 13:22:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.027995314449071884 norm:0.00021253999148029834 max memory_allocated 22562.67919921875 
[2025-03-26 13:23:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.027965903282165527 norm:0.0001945949625223875 max memory_allocated 22562.67919921875 
[2025-03-26 13:23:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.027927856892347336 norm:0.00020171325013507158 max memory_allocated 22562.67919921875 
[2025-03-26 13:24:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.027954626828432083 norm:0.00021296805061865598 max memory_allocated 22562.67919921875 
[2025-03-26 13:24:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.027934852987527847 norm:0.0002053596545010805 max memory_allocated 22562.67919921875 
[2025-03-26 13:25:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.027953464537858963 norm:0.0001998304796870798 max memory_allocated 22562.67919921875 
[2025-03-26 13:26:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.027936778962612152 norm:0.00019064379739575088 max memory_allocated 22562.67919921875 
[2025-03-26 13:26:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.027914725244045258 norm:0.00018963035836350173 max memory_allocated 22562.67919921875 
[2025-03-26 13:27:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.027866344898939133 norm:0.00018113924306817353 max memory_allocated 22562.67919921875 
[2025-03-26 13:27:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.027878332883119583 norm:0.0001789998059393838 max memory_allocated 22562.67919921875 
[2025-03-26 13:27:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-26 13:28:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.05163786560297012 norm:0.006319109816104174 max memory_allocated 22562.85107421875 
[2025-03-26 13:29:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.040317993611097336 norm:0.0009642059449106455 max memory_allocated 22562.85107421875 
[2025-03-26 13:29:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.03443543612957001 norm:0.0004867481766268611 max memory_allocated 22562.85107421875 
[2025-03-26 13:30:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.032527171075344086 norm:0.00038256344851106405 max memory_allocated 22562.85107421875 
[2025-03-26 13:30:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.03162703290581703 norm:0.00032554619247093797 max memory_allocated 22562.85107421875 
[2025-03-26 13:31:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.031174711883068085 norm:0.0002950503840111196 max memory_allocated 22562.85107421875 
[2025-03-26 13:31:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.031008543446660042 norm:0.00028204292175360024 max memory_allocated 22562.85107421875 
[2025-03-26 13:32:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.030827080830931664 norm:0.0002319030463695526 max memory_allocated 22562.85107421875 
[2025-03-26 13:33:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.030746595934033394 norm:0.00024067300546448678 max memory_allocated 22562.85107421875 
[2025-03-26 13:33:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.030727284029126167 norm:0.00021520501468330622 max memory_allocated 22562.85107421875 
[2025-03-26 13:34:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.03067683055996895 norm:0.0001938089117174968 max memory_allocated 22562.85107421875 
[2025-03-26 13:34:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.03065139427781105 norm:0.00019396429706830531 max memory_allocated 22562.85107421875 
[2025-03-26 13:35:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.03063785471022129 norm:0.00020271746325306594 max memory_allocated 22562.85107421875 
[2025-03-26 13:35:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.030639147385954857 norm:0.0001823299826355651 max memory_allocated 22562.85107421875 
[2025-03-26 13:36:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.030619505792856216 norm:0.0001792245893739164 max memory_allocated 22562.85107421875 
[2025-03-26 13:36:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.03064526617527008 norm:0.0001860096090240404 max memory_allocated 22562.85107421875 
[2025-03-26 13:37:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.03066525049507618 norm:0.00017461011884734035 max memory_allocated 22562.85107421875 
[2025-03-26 13:38:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.030665863305330276 norm:0.00017346200183965266 max memory_allocated 22562.85107421875 
[2025-03-26 13:38:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.030648985877633095 norm:0.00017942459089681506 max memory_allocated 22562.85107421875 
[2025-03-26 13:39:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.030630724504590034 norm:0.0001763603650033474 max memory_allocated 22562.85107421875 
[2025-03-26 13:39:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-26 13:40:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.06260596960783005 norm:0.004217975307255983 max memory_allocated 22563.02294921875 
[2025-03-26 13:40:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.04804341495037079 norm:0.0010942594381049275 max memory_allocated 22563.02294921875 
[2025-03-26 13:41:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.03955617919564247 norm:0.0005217882571741939 max memory_allocated 22563.02294921875 
[2025-03-26 13:41:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.03711211681365967 norm:0.0003904649638570845 max memory_allocated 22563.02294921875 
[2025-03-26 13:42:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.03584486246109009 norm:0.0003316909715067595 max memory_allocated 22563.02294921875 
[2025-03-26 13:42:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.03527820482850075 norm:0.00031102722277864814 max memory_allocated 22563.02294921875 
[2025-03-26 13:43:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.03499798849225044 norm:0.0002931949566118419 max memory_allocated 22563.02294921875 
[2025-03-26 13:43:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.034785110503435135 norm:0.00026039083604700863 max memory_allocated 22563.02294921875 
[2025-03-26 13:44:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.03467612713575363 norm:0.00021673516312148422 max memory_allocated 22563.02294921875 
[2025-03-26 13:45:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.034605130553245544 norm:0.00021331675816327333 max memory_allocated 22563.02294921875 
[2025-03-26 13:45:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.034558847546577454 norm:0.00022269797045737505 max memory_allocated 22563.02294921875 
[2025-03-26 13:46:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.034465763717889786 norm:0.00022046256344765425 max memory_allocated 22563.02294921875 
[2025-03-26 13:46:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.03440164774656296 norm:0.00021271489094942808 max memory_allocated 22563.02294921875 
[2025-03-26 13:47:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.03435707837343216 norm:0.00019770221842918545 max memory_allocated 22563.02294921875 
[2025-03-26 13:47:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.03436959162354469 norm:0.00020163989393040538 max memory_allocated 22563.02294921875 
[2025-03-26 13:48:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.034348879009485245 norm:0.00019403720216359943 max memory_allocated 22563.02294921875 
[2025-03-26 13:49:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.034328754991292953 norm:0.00019017988233827055 max memory_allocated 22563.02294921875 
[2025-03-26 13:49:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.03435123339295387 norm:0.00020152083016000688 max memory_allocated 22563.02294921875 
[2025-03-26 13:50:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.034415699541568756 norm:0.0002205506170867011 max memory_allocated 22563.02294921875 
[2025-03-26 13:50:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.034373119473457336 norm:0.0001963677059393376 max memory_allocated 22563.02294921875 
[2025-03-26 13:50:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-26 13:51:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.07045925408601761 norm:0.007013686932623386 max memory_allocated 22563.19482421875 
[2025-03-26 13:52:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.05261988192796707 norm:0.009597882628440857 max memory_allocated 22563.19482421875 
[2025-03-26 13:52:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.043166544288396835 norm:0.0005906333099119365 max memory_allocated 22563.19482421875 
[2025-03-26 13:53:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.04048185423016548 norm:0.000439195689978078 max memory_allocated 22563.19482421875 
[2025-03-26 13:53:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0392659567296505 norm:0.00038997881347313523 max memory_allocated 22563.19482421875 
[2025-03-26 13:54:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.03868018835783005 norm:0.00034333192161284387 max memory_allocated 22563.19482421875 
[2025-03-26 13:54:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.03838583827018738 norm:0.0003118959139101207 max memory_allocated 22563.19482421875 
[2025-03-26 13:55:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0381854847073555 norm:0.00031375495018437505 max memory_allocated 22563.19482421875 
[2025-03-26 13:55:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.03802208974957466 norm:0.0002612022217363119 max memory_allocated 22563.19482421875 
[2025-03-26 13:56:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.03782673180103302 norm:0.000254259241046384 max memory_allocated 22563.19482421875 
[2025-03-26 13:57:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.03776836767792702 norm:0.00024661188945174217 max memory_allocated 22563.19482421875 
[2025-03-26 13:57:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.037720486521720886 norm:0.00024946819758042693 max memory_allocated 22563.19482421875 
[2025-03-26 13:58:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.037697792053222656 norm:0.00024036243848968297 max memory_allocated 22563.19482421875 
[2025-03-26 13:58:45 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.03766174986958504 norm:0.00023491871252190322 max memory_allocated 22563.19482421875 
[2025-03-26 13:59:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.03761952742934227 norm:0.00021766795543953776 max memory_allocated 22563.19482421875 
[2025-03-26 13:59:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.037602171301841736 norm:0.00021635399025399238 max memory_allocated 22563.19482421875 
[2025-03-26 14:00:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.03761830925941467 norm:0.0002143769816029817 max memory_allocated 22563.19482421875 
[2025-03-26 14:01:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.03760440647602081 norm:0.00019439286552369595 max memory_allocated 22563.19482421875 
[2025-03-26 14:01:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.03753642737865448 norm:0.00020116663654334843 max memory_allocated 22563.19482421875 
[2025-03-26 14:02:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.03753496706485748 norm:0.00019103626254945993 max memory_allocated 22563.19482421875 
[2025-03-26 14:02:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-26 14:02:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.06477108597755432 norm:0.00171978201251477 max memory_allocated 22563.36669921875 
[2025-03-26 14:03:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.052739452570676804 norm:0.000628128123935312 max memory_allocated 22563.36669921875 
[2025-03-26 14:04:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.04542189836502075 norm:0.0003063380136154592 max memory_allocated 22563.36669921875 
[2025-03-26 14:04:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.043153248727321625 norm:0.0002502045826986432 max memory_allocated 22563.36669921875 
[2025-03-26 14:05:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.042146146297454834 norm:0.0002379593497607857 max memory_allocated 22563.36669921875 
[2025-03-26 14:05:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.04161334037780762 norm:0.00021639978513121605 max memory_allocated 22563.36669921875 
[2025-03-26 14:06:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.04132794216275215 norm:0.00021549446682911366 max memory_allocated 22563.36669921875 
[2025-03-26 14:06:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.041220732033252716 norm:0.00019344271277077496 max memory_allocated 22563.36669921875 
[2025-03-26 14:07:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.04116448760032654 norm:0.00017685219063423574 max memory_allocated 22563.36669921875 
[2025-03-26 14:07:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.04103073850274086 norm:0.00017341467901133 max memory_allocated 22563.36669921875 
[2025-03-26 14:08:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.04095606878399849 norm:0.00016785040497779846 max memory_allocated 22563.36669921875 
[2025-03-26 14:09:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.040944524109363556 norm:0.00018094800179824233 max memory_allocated 22563.36669921875 
[2025-03-26 14:09:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0410027876496315 norm:0.00019099575001746416 max memory_allocated 22563.36669921875 
[2025-03-26 14:10:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.04096445068717003 norm:0.0001760304585332051 max memory_allocated 22563.36669921875 
[2025-03-26 14:10:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.040953993797302246 norm:0.00017128374020103365 max memory_allocated 22563.36669921875 
[2025-03-26 14:11:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.04095229133963585 norm:0.00017750475672073662 max memory_allocated 22563.36669921875 
[2025-03-26 14:11:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.04094761982560158 norm:0.00018522243772167712 max memory_allocated 22563.36669921875 
[2025-03-26 14:12:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0409373864531517 norm:0.0001763016334734857 max memory_allocated 22563.36669921875 
[2025-03-26 14:13:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.04092860594391823 norm:0.00018616614397615194 max memory_allocated 22563.36669921875 
[2025-03-26 14:13:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.040909916162490845 norm:0.00018098311556968838 max memory_allocated 22563.36669921875 
[2025-03-26 14:13:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-26 14:14:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.07459145784378052 norm:0.006736154202371836 max memory_allocated 22563.53857421875 
[2025-03-26 14:14:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.05906759575009346 norm:0.0021923035383224487 max memory_allocated 22563.53857421875 
[2025-03-26 14:15:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.04928300902247429 norm:0.0006332074990496039 max memory_allocated 22563.53857421875 
[2025-03-26 14:16:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.04647791013121605 norm:0.0003823022125288844 max memory_allocated 22563.53857421875 
[2025-03-26 14:16:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.04527021199464798 norm:0.00031954486621543765 max memory_allocated 22563.53857421875 
[2025-03-26 14:17:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.044740766286849976 norm:0.0003088263620156795 max memory_allocated 22563.53857421875 
[2025-03-26 14:17:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.04442810267210007 norm:0.0002649982925504446 max memory_allocated 22563.53857421875 
[2025-03-26 14:18:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.04420857131481171 norm:0.000246871670242399 max memory_allocated 22563.53857421875 
[2025-03-26 14:18:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.04408527910709381 norm:0.00021218968322500587 max memory_allocated 22563.53857421875 
[2025-03-26 14:19:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.04401792585849762 norm:0.00021008719340898097 max memory_allocated 22563.53857421875 
[2025-03-26 14:19:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.043966908007860184 norm:0.00021486346668098122 max memory_allocated 22563.53857421875 
[2025-03-26 14:20:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.04387881979346275 norm:0.00019353844982106239 max memory_allocated 22563.53857421875 
[2025-03-26 14:21:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0438171923160553 norm:0.0001848461979534477 max memory_allocated 22563.53857421875 
[2025-03-26 14:21:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.04382096976041794 norm:0.00020376047177705914 max memory_allocated 22563.53857421875 
[2025-03-26 14:22:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0437500886619091 norm:0.00018299321527592838 max memory_allocated 22563.53857421875 
[2025-03-26 14:22:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.04372139275074005 norm:0.00017950353503692895 max memory_allocated 22563.53857421875 
[2025-03-26 14:23:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.04368837922811508 norm:0.00018178259779233485 max memory_allocated 22563.53857421875 
[2025-03-26 14:23:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.043631911277770996 norm:0.00017863026005215943 max memory_allocated 22563.53857421875 
[2025-03-26 14:24:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.043541744351387024 norm:0.0001704120950307697 max memory_allocated 22563.53857421875 
[2025-03-26 14:25:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.04353085160255432 norm:0.00016575887275394052 max memory_allocated 22563.53857421875 
[2025-03-26 14:25:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-26 14:25:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.07124479115009308 norm:0.002257296349853277 max memory_allocated 22563.71044921875 
[2025-03-26 14:26:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.059921976178884506 norm:0.0010039880871772766 max memory_allocated 22563.71044921875 
[2025-03-26 14:26:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.05145381763577461 norm:0.00045685749500989914 max memory_allocated 22563.71044921875 
[2025-03-26 14:27:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.04863483086228371 norm:0.00025709200417622924 max memory_allocated 22563.71044921875 
[2025-03-26 14:28:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.04758775979280472 norm:0.0002156323753297329 max memory_allocated 22563.71044921875 
[2025-03-26 14:28:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.04704548045992851 norm:0.00018928077770397067 max memory_allocated 22563.71044921875 
[2025-03-26 14:29:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.04679529741406441 norm:0.00018436484970152378 max memory_allocated 22563.71044921875 
[2025-03-26 14:29:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.04669208824634552 norm:0.00018354023632127792 max memory_allocated 22563.71044921875 
[2025-03-26 14:30:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.046569496393203735 norm:0.0001707947376416996 max memory_allocated 22563.71044921875 
[2025-03-26 14:30:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.046445582062006 norm:0.00017041564569808543 max memory_allocated 22563.71044921875 
[2025-03-26 14:31:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.04639880731701851 norm:0.00016692183271516114 max memory_allocated 22563.71044921875 
[2025-03-26 14:31:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.046351563185453415 norm:0.00016242725541815162 max memory_allocated 22563.71044921875 
[2025-03-26 14:32:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.04633376747369766 norm:0.00015642843209207058 max memory_allocated 22563.71044921875 
[2025-03-26 14:33:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.04629473388195038 norm:0.00015722325770184398 max memory_allocated 22563.71044921875 
[2025-03-26 14:33:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.046279698610305786 norm:0.00015618577890563756 max memory_allocated 22563.71044921875 
[2025-03-26 14:34:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0462665855884552 norm:0.00015002279542386532 max memory_allocated 22563.71044921875 
[2025-03-26 14:34:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.04622186720371246 norm:0.00014789463602937758 max memory_allocated 22563.71044921875 
[2025-03-26 14:35:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.04619557410478592 norm:0.00014860928058624268 max memory_allocated 22563.71044921875 
[2025-03-26 14:35:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.04617050662636757 norm:0.00014858547365292907 max memory_allocated 22563.71044921875 
[2025-03-26 14:36:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.04616301506757736 norm:0.00015022150182630867 max memory_allocated 22563.71044921875 
[2025-03-26 14:36:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-26 14:37:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0745987594127655 norm:0.002788822166621685 max memory_allocated 22563.88232421875 
[2025-03-26 14:37:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.06209329143166542 norm:0.0010831294348463416 max memory_allocated 22563.88232421875 
[2025-03-26 14:38:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.05357060208916664 norm:0.0004314680118113756 max memory_allocated 22563.88232421875 
[2025-03-26 14:38:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.05055847018957138 norm:0.00029059580992907286 max memory_allocated 22563.88232421875 
[2025-03-26 14:39:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.049435339868068695 norm:0.00023899547522887588 max memory_allocated 22563.88232421875 
[2025-03-26 14:40:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.04885159805417061 norm:0.00020873085304629058 max memory_allocated 22563.88232421875 
[2025-03-26 14:40:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.048554450273513794 norm:0.00019912942661903799 max memory_allocated 22563.88232421875 
[2025-03-26 14:41:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.048403795808553696 norm:0.0001826101797632873 max memory_allocated 22563.88232421875 
[2025-03-26 14:41:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.04834812879562378 norm:0.00018375100626144558 max memory_allocated 22563.88232421875 
[2025-03-26 14:42:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.04828996583819389 norm:0.0001797455915948376 max memory_allocated 22563.88232421875 
[2025-03-26 14:42:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.04823048412799835 norm:0.000177698559127748 max memory_allocated 22563.88232421875 
[2025-03-26 14:43:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.04817751795053482 norm:0.0001747715868987143 max memory_allocated 22563.88232421875 
[2025-03-26 14:43:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.04814925789833069 norm:0.00016176265489775687 max memory_allocated 22563.88232421875 
[2025-03-26 14:44:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.048141155391931534 norm:0.0001574311900185421 max memory_allocated 22563.88232421875 
[2025-03-26 14:45:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.04810449853539467 norm:0.00015613928553648293 max memory_allocated 22563.88232421875 
[2025-03-26 14:45:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.04809776693582535 norm:0.00015360026736743748 max memory_allocated 22563.88232421875 
[2025-03-26 14:46:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.04804537445306778 norm:0.00015030577196739614 max memory_allocated 22563.88232421875 
[2025-03-26 14:46:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.04802481830120087 norm:0.00014613836538046598 max memory_allocated 22563.88232421875 
[2025-03-26 14:47:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.048017166554927826 norm:0.0001476154720876366 max memory_allocated 22563.88232421875 
[2025-03-26 14:47:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.04798844829201698 norm:0.0001461236533941701 max memory_allocated 22563.88232421875 
[2025-03-26 14:48:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-26 14:48:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.07221285253763199 norm:0.002379950601607561 max memory_allocated 22564.05419921875 
[2025-03-26 14:49:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.061253830790519714 norm:0.0008728877874091268 max memory_allocated 22564.05419921875 
[2025-03-26 14:49:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.05419238656759262 norm:0.0003918005677405745 max memory_allocated 22564.05419921875 
[2025-03-26 14:50:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.05174778401851654 norm:0.0002629137015901506 max memory_allocated 22564.05419921875 
[2025-03-26 14:50:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.050694916397333145 norm:0.00020637636771425605 max memory_allocated 22564.05419921875 
[2025-03-26 14:51:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.05015282705426216 norm:0.0001736618287395686 max memory_allocated 22564.05419921875 
[2025-03-26 14:52:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.049880437552928925 norm:0.00020405995019245893 max memory_allocated 22564.05419921875 
[2025-03-26 14:52:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0496877059340477 norm:0.00017397472402080894 max memory_allocated 22564.05419921875 
[2025-03-26 14:53:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.04958100616931915 norm:0.0001643246941966936 max memory_allocated 22564.05419921875 
[2025-03-26 14:53:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.04947671294212341 norm:0.00016440509352833033 max memory_allocated 22564.05419921875 
[2025-03-26 14:54:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.049452923238277435 norm:0.00015544820053037256 max memory_allocated 22564.05419921875 
[2025-03-26 14:54:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.04943355545401573 norm:0.00014268145605456084 max memory_allocated 22564.05419921875 
[2025-03-26 14:55:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.04938017576932907 norm:0.00015020169666968286 max memory_allocated 22564.05419921875 
[2025-03-26 14:55:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.04930718243122101 norm:0.0001526846899650991 max memory_allocated 22564.05419921875 
[2025-03-26 14:56:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.04926590621471405 norm:0.0001393884449498728 max memory_allocated 22564.05419921875 
[2025-03-26 14:57:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0492560938000679 norm:0.0001310608204221353 max memory_allocated 22564.05419921875 
[2025-03-26 14:57:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0492086187005043 norm:0.00012915165279991925 max memory_allocated 22564.05419921875 
[2025-03-26 14:58:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.04919220879673958 norm:0.00013044773368164897 max memory_allocated 22564.05419921875 
[2025-03-26 14:58:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.04916137084364891 norm:0.00013099017087370157 max memory_allocated 22564.05419921875 
[2025-03-26 14:59:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.049159254878759384 norm:0.00013192732876632363 max memory_allocated 22564.05419921875 
[2025-03-26 14:59:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-26 15:00:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.07401872426271439 norm:0.0050328560173511505 max memory_allocated 22564.22607421875 
[2025-03-26 15:00:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0610627718269825 norm:0.0014523232821375132 max memory_allocated 22564.22607421875 
[2025-03-26 15:01:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.05361998826265335 norm:0.00046889245277270675 max memory_allocated 22564.22607421875 
[2025-03-26 15:01:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.05142314359545708 norm:0.00030977799906395376 max memory_allocated 22564.22607421875 
[2025-03-26 15:02:21 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.050359562039375305 norm:0.0002507881144993007 max memory_allocated 22564.22607421875 
[2025-03-26 15:02:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.049794599413871765 norm:0.00023088243324309587 max memory_allocated 22564.22607421875 
[2025-03-26 15:03:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.049447957426309586 norm:0.00021221091446932405 max memory_allocated 22564.22607421875 
[2025-03-26 15:04:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.049244243651628494 norm:0.00020401748770382255 max memory_allocated 22564.22607421875 
[2025-03-26 15:04:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.049123235046863556 norm:0.00021485761681105942 max memory_allocated 22564.22607421875 
[2025-03-26 15:05:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.04904572665691376 norm:0.00020866250270046294 max memory_allocated 22564.22607421875 
[2025-03-26 15:05:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.04895453527569771 norm:0.0001838849566411227 max memory_allocated 22564.22607421875 
[2025-03-26 15:06:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.048872362822294235 norm:0.00017412030138075352 max memory_allocated 22564.22607421875 
[2025-03-26 15:06:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.04878853261470795 norm:0.00015754438936710358 max memory_allocated 22564.22607421875 
[2025-03-26 15:07:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.048752542585134506 norm:0.00017193025269079953 max memory_allocated 22564.22607421875 
[2025-03-26 15:07:58 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.048735346645116806 norm:0.00016726306057535112 max memory_allocated 22564.22607421875 
[2025-03-26 15:08:32 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.048703424632549286 norm:0.0001442285574739799 max memory_allocated 22564.22607421875 
[2025-03-26 15:09:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.04871087521314621 norm:0.00013998911890666932 max memory_allocated 22564.22607421875 
[2025-03-26 15:09:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.048689015209674835 norm:0.00013411328836809844 max memory_allocated 22564.22607421875 
[2025-03-26 15:10:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.04868504777550697 norm:0.00014293215645011514 max memory_allocated 22564.22607421875 
[2025-03-26 15:10:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.04865173250436783 norm:0.00013732153456658125 max memory_allocated 22564.22607421875 
[2025-03-26 15:10:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-26 15:11:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.06909822672605515 norm:0.001967577263712883 max memory_allocated 22564.39794921875 
[2025-03-26 15:12:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.060043659061193466 norm:0.0007674377411603928 max memory_allocated 22564.39794921875 
[2025-03-26 15:12:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.05397440493106842 norm:0.0003378169203642756 max memory_allocated 22564.39794921875 
[2025-03-26 15:13:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0518953874707222 norm:0.00020133315410930663 max memory_allocated 22564.39794921875 
[2025-03-26 15:13:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.051072318106889725 norm:0.00018128965166397393 max memory_allocated 22564.39794921875 
[2025-03-26 15:14:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.05066078528761864 norm:0.00017736150766722858 max memory_allocated 22564.39794921875 
[2025-03-26 15:14:55 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.05043921619653702 norm:0.00017823335656430572 max memory_allocated 22564.39794921875 
[2025-03-26 15:15:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.050261639058589935 norm:0.00017681489407550544 max memory_allocated 22564.39794921875 
[2025-03-26 15:16:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.050107441842556 norm:0.00013971597945783287 max memory_allocated 22564.39794921875 
[2025-03-26 15:16:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.04998127371072769 norm:0.0001266886538360268 max memory_allocated 22564.39794921875 
[2025-03-26 15:17:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.049927473068237305 norm:0.00012872657680418342 max memory_allocated 22564.39794921875 
[2025-03-26 15:17:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.049861982464790344 norm:0.00012987511581741273 max memory_allocated 22564.39794921875 
[2025-03-26 15:18:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.04984859749674797 norm:0.00013087637489661574 max memory_allocated 22564.39794921875 
[2025-03-26 15:18:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.049798235297203064 norm:0.00011776744213420898 max memory_allocated 22564.39794921875 
[2025-03-26 15:19:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.04974311590194702 norm:0.00010936920443782583 max memory_allocated 22564.39794921875 
[2025-03-26 15:19:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.049720145761966705 norm:0.000107735933852382 max memory_allocated 22564.39794921875 
[2025-03-26 15:20:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0497165322303772 norm:0.00010234612273052335 max memory_allocated 22564.39794921875 
[2025-03-26 15:21:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.04971371963620186 norm:0.00010669536277418956 max memory_allocated 22564.39794921875 
[2025-03-26 15:21:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.04971034824848175 norm:0.00010409595415694639 max memory_allocated 22564.39794921875 
[2025-03-26 15:22:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.04969338700175285 norm:0.00010406534420326352 max memory_allocated 22564.39794921875 
[2025-03-26 15:22:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-26 15:22:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.07701548933982849 norm:0.0049048117361962795 max memory_allocated 22564.56982421875 
[2025-03-26 15:23:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.06433829665184021 norm:0.0016709249466657639 max memory_allocated 22564.56982421875 
[2025-03-26 15:24:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.05524756759405136 norm:0.0005696603329852223 max memory_allocated 22564.56982421875 
[2025-03-26 15:24:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.052778054028749466 norm:0.0003384190786164254 max memory_allocated 22564.56982421875 
[2025-03-26 15:25:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0516151487827301 norm:0.0002591970260255039 max memory_allocated 22564.56982421875 
[2025-03-26 15:25:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0510258823633194 norm:0.0002530368510633707 max memory_allocated 22564.56982421875 
[2025-03-26 15:26:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.050660714507102966 norm:0.00023650853836443275 max memory_allocated 22564.56982421875 
[2025-03-26 15:26:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.05043167248368263 norm:0.0002407948486506939 max memory_allocated 22564.56982421875 
[2025-03-26 15:27:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.05026239529252052 norm:0.00022536607866641134 max memory_allocated 22564.56982421875 
[2025-03-26 15:28:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.05012112855911255 norm:0.00018950847152154893 max memory_allocated 22564.56982421875 
[2025-03-26 15:28:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.05000336468219757 norm:0.00018195470329374075 max memory_allocated 22564.56982421875 
[2025-03-26 15:29:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.04990911856293678 norm:0.0001805006613722071 max memory_allocated 22564.56982421875 
[2025-03-26 15:29:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.04981109872460365 norm:0.00017050579481292516 max memory_allocated 22564.56982421875 
[2025-03-26 15:30:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0497361496090889 norm:0.0001603764103492722 max memory_allocated 22564.56982421875 
[2025-03-26 15:30:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.049694374203681946 norm:0.00016007659723982215 max memory_allocated 22564.56982421875 
[2025-03-26 15:31:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.049628496170043945 norm:0.00014825111429672688 max memory_allocated 22564.56982421875 
[2025-03-26 15:31:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.049583494663238525 norm:0.0001476995093980804 max memory_allocated 22564.56982421875 
[2025-03-26 15:32:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.04952738806605339 norm:0.00013775069965049624 max memory_allocated 22564.56982421875 
[2025-03-26 15:33:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.049508124589920044 norm:0.00013864391075912863 max memory_allocated 22564.56982421875 
[2025-03-26 15:33:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.049470897763967514 norm:0.00012715737102553248 max memory_allocated 22564.56982421875 
[2025-03-26 15:33:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-26 15:34:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.07632509618997574 norm:0.005130749195814133 max memory_allocated 22564.74169921875 
[2025-03-26 15:35:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.06477576494216919 norm:0.0018313019536435604 max memory_allocated 22564.74169921875 
[2025-03-26 15:35:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.05628550052642822 norm:0.0005842371610924602 max memory_allocated 22564.74169921875 
[2025-03-26 15:36:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.05385894328355789 norm:0.00036923735751770437 max memory_allocated 22564.74169921875 
[2025-03-26 15:36:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.052784692496061325 norm:0.0003255216288380325 max memory_allocated 22564.74169921875 
[2025-03-26 15:37:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.05218803882598877 norm:0.00028694869251921773 max memory_allocated 22564.74169921875 
[2025-03-26 15:37:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.05178992077708244 norm:0.00027080412837676704 max memory_allocated 22564.74169921875 
[2025-03-26 15:38:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.051614511758089066 norm:0.0002664511848706752 max memory_allocated 22564.74169921875 
[2025-03-26 15:38:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.0514419861137867 norm:0.00024300330551341176 max memory_allocated 22564.74169921875 
[2025-03-26 15:39:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.05131662264466286 norm:0.00022785141482017934 max memory_allocated 22564.74169921875 
[2025-03-26 15:40:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.051168426871299744 norm:0.0002047049638349563 max memory_allocated 22564.74169921875 
[2025-03-26 15:40:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.051057636737823486 norm:0.0001865029043983668 max memory_allocated 22564.74169921875 
[2025-03-26 15:41:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.05097020044922829 norm:0.00017591234063729644 max memory_allocated 22564.74169921875 
[2025-03-26 15:41:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.050883661955595016 norm:0.00016806356143206358 max memory_allocated 22564.74169921875 
[2025-03-26 15:42:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.05081115663051605 norm:0.000169130289577879 max memory_allocated 22564.74169921875 
[2025-03-26 15:42:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.05074647068977356 norm:0.00016334569954779 max memory_allocated 22564.74169921875 
[2025-03-26 15:43:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.05070876702666283 norm:0.00014580012066289783 max memory_allocated 22564.74169921875 
[2025-03-26 15:43:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.050663478672504425 norm:0.0001369014789815992 max memory_allocated 22564.74169921875 
[2025-03-26 15:44:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.05063892900943756 norm:0.00013877831224817783 max memory_allocated 22564.74169921875 
[2025-03-26 15:45:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.05063885822892189 norm:0.00014551276399288327 max memory_allocated 22564.74169921875 
[2025-03-26 15:45:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-26 15:45:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.07432261854410172 norm:0.0059584276750683784 max memory_allocated 22564.91357421875 
[2025-03-26 15:46:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.06461557000875473 norm:0.0024075156543403864 max memory_allocated 22564.91357421875 
[2025-03-26 15:47:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.05626901984214783 norm:0.0006070147501304746 max memory_allocated 22564.91357421875 
[2025-03-26 15:47:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.05431205779314041 norm:0.0003462547902017832 max memory_allocated 22564.91357421875 
[2025-03-26 15:48:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0533941313624382 norm:0.0003130478726234287 max memory_allocated 22564.91357421875 
[2025-03-26 15:48:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.05287422984838486 norm:0.00026444080867804587 max memory_allocated 22564.91357421875 
[2025-03-26 15:49:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.05252627655863762 norm:0.00024367347941733897 max memory_allocated 22564.91357421875 
[2025-03-26 15:49:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.052348315715789795 norm:0.00023070513270795345 max memory_allocated 22564.91357421875 
[2025-03-26 15:50:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0522453747689724 norm:0.0002280115440953523 max memory_allocated 22564.91357421875 
[2025-03-26 15:50:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.05209847167134285 norm:0.00018283278041053563 max memory_allocated 22564.91357421875 
[2025-03-26 15:51:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.05199912562966347 norm:0.00017719768220558763 max memory_allocated 22564.91357421875 
[2025-03-26 15:52:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.051973551511764526 norm:0.00016780452278908342 max memory_allocated 22564.91357421875 
[2025-03-26 15:52:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.05192435905337334 norm:0.0001489345304435119 max memory_allocated 22564.91357421875 
[2025-03-26 15:53:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.051887765526771545 norm:0.00014976385864429176 max memory_allocated 22564.91357421875 
[2025-03-26 15:53:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.05184606835246086 norm:0.00014079370885156095 max memory_allocated 22564.91357421875 
[2025-03-26 15:54:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.05180604010820389 norm:0.0001302071032114327 max memory_allocated 22564.91357421875 
[2025-03-26 15:54:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.051774363964796066 norm:0.0001253198424819857 max memory_allocated 22564.91357421875 
[2025-03-26 15:55:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.05176490917801857 norm:0.00012577077723108232 max memory_allocated 22564.91357421875 
[2025-03-26 15:56:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.05176125466823578 norm:0.0001193964053527452 max memory_allocated 22564.91357421875 
[2025-03-26 15:56:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.05174659192562103 norm:0.00011015060590580106 max memory_allocated 22564.91357421875 
[2025-03-26 15:56:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-26 15:57:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.08524533361196518 norm:0.007206411566585302 max memory_allocated 22565.08544921875 
[2025-03-26 15:57:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0717879980802536 norm:0.0025569957215338945 max memory_allocated 22565.08544921875 
[2025-03-26 15:58:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.06149294972419739 norm:0.0007919396739453077 max memory_allocated 22565.08544921875 
[2025-03-26 15:59:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.058732908219099045 norm:0.000398513104300946 max memory_allocated 22565.08544921875 
[2025-03-26 15:59:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.05756586790084839 norm:0.00031085225054994226 max memory_allocated 22565.08544921875 
[2025-03-26 16:00:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.05683894082903862 norm:0.00027241974021308124 max memory_allocated 22565.08544921875 
[2025-03-26 16:00:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.056456148624420166 norm:0.00027646240778267384 max memory_allocated 22565.08544921875 
[2025-03-26 16:01:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.056243713945150375 norm:0.00026413972955197096 max memory_allocated 22565.08544921875 
[2025-03-26 16:01:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.0560605563223362 norm:0.00023207312915474176 max memory_allocated 22565.08544921875 
[2025-03-26 16:02:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.05595333129167557 norm:0.00021134704002179205 max memory_allocated 22565.08544921875 
[2025-03-26 16:02:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.05589396506547928 norm:0.00021229956473689526 max memory_allocated 22565.08544921875 
[2025-03-26 16:03:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.05576309189200401 norm:0.00019733718363568187 max memory_allocated 22565.08544921875 
[2025-03-26 16:04:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.05566862225532532 norm:0.0001790598180377856 max memory_allocated 22565.08544921875 
[2025-03-26 16:04:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.055593542754650116 norm:0.00018176250159740448 max memory_allocated 22565.08544921875 
[2025-03-26 16:05:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.05553305149078369 norm:0.00017206762277055532 max memory_allocated 22565.08544921875 
[2025-03-26 16:05:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.05548400431871414 norm:0.00015334616182371974 max memory_allocated 22565.08544921875 
[2025-03-26 16:06:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.05545169487595558 norm:0.00014556804671883583 max memory_allocated 22565.08544921875 
[2025-03-26 16:06:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.0554145909845829 norm:0.0001456506724935025 max memory_allocated 22565.08544921875 
[2025-03-26 16:07:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0553746297955513 norm:0.00014846590056549758 max memory_allocated 22565.08544921875 
[2025-03-26 16:08:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.0553419403731823 norm:0.00013742962619289756 max memory_allocated 22565.08544921875 
[2025-03-26 16:08:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-26 16:08:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.08504076302051544 norm:0.00560329994186759 max memory_allocated 22565.25732421875 
[2025-03-26 16:09:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.07477867603302002 norm:0.0021246292162686586 max memory_allocated 22565.25732421875 
[2025-03-26 16:09:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.06515784561634064 norm:0.0006364559521898627 max memory_allocated 22565.25732421875 
[2025-03-26 16:10:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.06276505440473557 norm:0.0003375063242856413 max memory_allocated 22565.25732421875 
[2025-03-26 16:11:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.06164693459868431 norm:0.0002480294497217983 max memory_allocated 22565.25732421875 
[2025-03-26 16:11:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.06100216507911682 norm:0.00023601166321896017 max memory_allocated 22565.25732421875 
[2025-03-26 16:12:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.06069190427660942 norm:0.00022318778792396188 max memory_allocated 22565.25732421875 
[2025-03-26 16:12:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.060473550111055374 norm:0.00022020124015398324 max memory_allocated 22565.25732421875 
[2025-03-26 16:13:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.06032245606184006 norm:0.00019255487131886184 max memory_allocated 22565.25732421875 
[2025-03-26 16:13:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.060226038098335266 norm:0.00020280230091884732 max memory_allocated 22565.25732421875 
[2025-03-26 16:14:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.06011153757572174 norm:0.00018480668950360268 max memory_allocated 22565.25732421875 
[2025-03-26 16:14:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.060016240924596786 norm:0.00016001405310817063 max memory_allocated 22565.25732421875 
[2025-03-26 16:15:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.0599530003964901 norm:0.00014226269559003413 max memory_allocated 22565.25732421875 
[2025-03-26 16:16:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.059870727360248566 norm:0.00013303285231813788 max memory_allocated 22565.25732421875 
[2025-03-26 16:16:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.05977918207645416 norm:0.00012602713832166046 max memory_allocated 22565.25732421875 
[2025-03-26 16:17:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.05976211279630661 norm:0.00012534916459117085 max memory_allocated 22565.25732421875 
[2025-03-26 16:17:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.059726133942604065 norm:0.00012189283734187484 max memory_allocated 22565.25732421875 
[2025-03-26 16:18:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.059676483273506165 norm:0.000117054398288019 max memory_allocated 22565.25732421875 
[2025-03-26 16:18:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.05965229496359825 norm:0.00011259641905780882 max memory_allocated 22565.25732421875 
[2025-03-26 16:19:26 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.059636712074279785 norm:0.00010240695701213554 max memory_allocated 22565.25732421875 
[2025-03-26 16:19:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-26 16:20:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.09131127595901489 norm:0.005996445193886757 max memory_allocated 22565.42919921875 
[2025-03-26 16:20:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.08106783777475357 norm:0.002900693565607071 max memory_allocated 22565.42919921875 
[2025-03-26 16:21:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.07178834080696106 norm:0.0008164234459400177 max memory_allocated 22565.42919921875 
[2025-03-26 16:21:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.06945697963237762 norm:0.0004036616301164031 max memory_allocated 22565.42919921875 
[2025-03-26 16:22:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.06829661130905151 norm:0.00030988629441708326 max memory_allocated 22565.42919921875 
[2025-03-26 16:23:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.06757079809904099 norm:0.00028867481159977615 max memory_allocated 22565.42919921875 
[2025-03-26 16:23:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.06723237037658691 norm:0.00028023176128044724 max memory_allocated 22565.42919921875 
[2025-03-26 16:24:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.06701590865850449 norm:0.00027954508550465107 max memory_allocated 22565.42919921875 
[2025-03-26 16:24:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.06682935357093811 norm:0.00024957064306363463 max memory_allocated 22565.42919921875 
[2025-03-26 16:25:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.06668048352003098 norm:0.00023227828205563128 max memory_allocated 22565.42919921875 
[2025-03-26 16:25:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.06656809896230698 norm:0.00022807579080108553 max memory_allocated 22565.42919921875 
[2025-03-26 16:26:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.06647904962301254 norm:0.00021034927340224385 max memory_allocated 22565.42919921875 
[2025-03-26 16:26:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.06635435670614243 norm:0.00017693232803139836 max memory_allocated 22565.42919921875 
[2025-03-26 16:27:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.0662539154291153 norm:0.0001591720210853964 max memory_allocated 22565.42919921875 
[2025-03-26 16:28:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.06616781651973724 norm:0.00015755950880702585 max memory_allocated 22565.42919921875 
[2025-03-26 16:28:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.06611967831850052 norm:0.00015996798174455762 max memory_allocated 22565.42919921875 
[2025-03-26 16:29:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.06608545780181885 norm:0.00017049399320967495 max memory_allocated 22565.42919921875 
[2025-03-26 16:29:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.06603924185037613 norm:0.0001536167983431369 max memory_allocated 22565.42919921875 
[2025-03-26 16:30:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.06599323451519012 norm:0.00013453539577312768 max memory_allocated 22565.42919921875 
[2025-03-26 16:30:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.06595229357481003 norm:0.00012737307406496257 max memory_allocated 22565.42919921875 
[2025-03-26 16:31:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-26 16:31:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.09430339932441711 norm:0.004997209645807743 max memory_allocated 22565.60107421875 
[2025-03-26 16:32:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.08522280305624008 norm:0.001966365147382021 max memory_allocated 22565.60107421875 
[2025-03-26 16:32:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.07763509452342987 norm:0.0005608426872640848 max memory_allocated 22565.60107421875 
[2025-03-26 16:33:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.07568542659282684 norm:0.00028894038405269384 max memory_allocated 22565.60107421875 
[2025-03-26 16:33:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.07465562969446182 norm:0.00026577847893349826 max memory_allocated 22565.60107421875 
[2025-03-26 16:34:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.07405192404985428 norm:0.0002485540462657809 max memory_allocated 22565.60107421875 
[2025-03-26 16:35:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.07376157492399216 norm:0.0002330589632038027 max memory_allocated 22565.60107421875 
[2025-03-26 16:35:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.07357869297266006 norm:0.0002240677858935669 max memory_allocated 22565.60107421875 
[2025-03-26 16:36:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.07343017309904099 norm:0.00019307323964312673 max memory_allocated 22565.60107421875 
[2025-03-26 16:36:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.07332374155521393 norm:0.00018492696108296514 max memory_allocated 22565.60107421875 
[2025-03-26 16:37:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.07319826632738113 norm:0.00017098180251196027 max memory_allocated 22565.60107421875 
[2025-03-26 16:37:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.07312075048685074 norm:0.0001595989742781967 max memory_allocated 22565.60107421875 
[2025-03-26 16:38:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.07306306809186935 norm:0.000153587810928002 max memory_allocated 22565.60107421875 
[2025-03-26 16:38:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.0729958787560463 norm:0.00013644641148857772 max memory_allocated 22565.60107421875 
[2025-03-26 16:39:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.07297052443027496 norm:0.0001265069586224854 max memory_allocated 22565.60107421875 
[2025-03-26 16:40:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.07294013351202011 norm:0.0001306679187109694 max memory_allocated 22565.60107421875 
[2025-03-26 16:40:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.07288981974124908 norm:0.00012661721848417073 max memory_allocated 22565.60107421875 
[2025-03-26 16:41:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.07284810394048691 norm:0.00012061754387104884 max memory_allocated 22565.60107421875 
[2025-03-26 16:41:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.0728127509355545 norm:0.00011203575559193268 max memory_allocated 22565.60107421875 
[2025-03-26 16:42:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.07279088348150253 norm:0.00010599842062219977 max memory_allocated 22565.60107421875 
[2025-03-26 16:42:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-26 16:43:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.11048100143671036 norm:0.006398215889930725 max memory_allocated 22565.77294921875 
[2025-03-26 16:43:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.0978928729891777 norm:0.001986791379749775 max memory_allocated 22565.77294921875 
[2025-03-26 16:44:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.08872461318969727 norm:0.0006451705121435225 max memory_allocated 22565.77294921875 
[2025-03-26 16:44:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.0859665796160698 norm:0.0004135390045121312 max memory_allocated 22565.77294921875 
[2025-03-26 16:45:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.08482038974761963 norm:0.0003548849490471184 max memory_allocated 22565.77294921875 
[2025-03-26 16:45:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.0841631144285202 norm:0.0003386124153621495 max memory_allocated 22565.77294921875 
[2025-03-26 16:46:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.08383572846651077 norm:0.00029658598941750824 max memory_allocated 22565.77294921875 
[2025-03-26 16:47:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.08364985138177872 norm:0.0002615919802337885 max memory_allocated 22565.77294921875 
[2025-03-26 16:47:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.08349917083978653 norm:0.00023895535559859127 max memory_allocated 22565.77294921875 
[2025-03-26 16:48:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.08336908370256424 norm:0.00024948266218416393 max memory_allocated 22565.77294921875 
[2025-03-26 16:48:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.08322305232286453 norm:0.00024310620210599154 max memory_allocated 22565.77294921875 
[2025-03-26 16:49:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.08311665058135986 norm:0.0002383155660936609 max memory_allocated 22565.77294921875 
[2025-03-26 16:49:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.08303733170032501 norm:0.00023148441687226295 max memory_allocated 22565.77294921875 
[2025-03-26 16:50:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.08295028656721115 norm:0.0001857392635429278 max memory_allocated 22565.77294921875 
[2025-03-26 16:50:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.08287419378757477 norm:0.000188720048754476 max memory_allocated 22565.77294921875 
[2025-03-26 16:51:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.08282304555177689 norm:0.00019063343643210828 max memory_allocated 22565.77294921875 
[2025-03-26 16:52:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.08277256041765213 norm:0.0001767482899595052 max memory_allocated 22565.77294921875 
[2025-03-26 16:52:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.08275105804204941 norm:0.00016073374717961997 max memory_allocated 22565.77294921875 
[2025-03-26 16:53:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.0827346220612526 norm:0.0001658043183851987 max memory_allocated 22565.77294921875 
[2025-03-26 16:53:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.0826900526881218 norm:0.00015576159057673067 max memory_allocated 22565.77294921875 
[2025-03-26 16:53:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-26 16:54:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.11548805236816406 norm:0.003675051499158144 max memory_allocated 22565.94482421875 
[2025-03-26 16:55:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.10662200301885605 norm:0.0015047703636810184 max memory_allocated 22565.94482421875 
[2025-03-26 16:55:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.09832967817783356 norm:0.00046418586862273514 max memory_allocated 22565.94482421875 
[2025-03-26 16:56:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.09618701785802841 norm:0.0002838869404513389 max memory_allocated 22565.94482421875 
[2025-03-26 16:56:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.09505991637706757 norm:0.00030427140882238746 max memory_allocated 22565.94482421875 
[2025-03-26 16:57:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.09454983472824097 norm:0.00024027618928812444 max memory_allocated 22565.94482421875 
[2025-03-26 16:57:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.09432892501354218 norm:0.00022497127065435052 max memory_allocated 22565.94482421875 
[2025-03-26 16:58:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.09416720271110535 norm:0.00020835662144236267 max memory_allocated 22565.94482421875 
[2025-03-26 16:59:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.09405185282230377 norm:0.00019766984041780233 max memory_allocated 22565.94482421875 
[2025-03-26 16:59:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.09390689432621002 norm:0.00019616571080405265 max memory_allocated 22565.94482421875 
[2025-03-26 17:00:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.0938151627779007 norm:0.00017003470566123724 max memory_allocated 22565.94482421875 
[2025-03-26 17:00:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.09373144060373306 norm:0.00015492775128223002 max memory_allocated 22565.94482421875 
[2025-03-26 17:01:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.09364829957485199 norm:0.0001428077812306583 max memory_allocated 22565.94482421875 
[2025-03-26 17:01:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.09357747435569763 norm:0.00014402186207007617 max memory_allocated 22565.94482421875 
[2025-03-26 17:02:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.09351581335067749 norm:0.00013750072685070336 max memory_allocated 22565.94482421875 
[2025-03-26 17:02:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.09348207712173462 norm:0.0001233369839610532 max memory_allocated 22565.94482421875 
[2025-03-26 17:03:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.09344715625047684 norm:0.00011748084216378629 max memory_allocated 22565.94482421875 
[2025-03-26 17:04:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.09340641647577286 norm:0.00011367744446033612 max memory_allocated 22565.94482421875 
[2025-03-26 17:04:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.09337133169174194 norm:0.00011781424836954102 max memory_allocated 22565.94482421875 
[2025-03-26 17:05:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.09332723170518875 norm:0.0001119260341511108 max memory_allocated 22565.94482421875 
[2025-03-26 17:05:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-26 17:05:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.13017506897449493 norm:0.004594802390784025 max memory_allocated 22566.11669921875 
[2025-03-26 17:06:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.1192559227347374 norm:0.0014870271552354097 max memory_allocated 22566.11669921875 
[2025-03-26 17:07:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.1113068014383316 norm:0.0006957033183425665 max memory_allocated 22566.11669921875 
[2025-03-26 17:07:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.10849548876285553 norm:0.00036282557994127274 max memory_allocated 22566.11669921875 
[2025-03-26 17:08:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.10718584060668945 norm:0.00032064903643913567 max memory_allocated 22566.11669921875 
[2025-03-26 17:08:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.1066652461886406 norm:0.00029024732066318393 max memory_allocated 22566.11669921875 
[2025-03-26 17:09:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.10638057440519333 norm:0.00025552784791216254 max memory_allocated 22566.11669921875 
[2025-03-26 17:09:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.10622350126504898 norm:0.0002485564327798784 max memory_allocated 22566.11669921875 
[2025-03-26 17:10:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.10610469430685043 norm:0.00023715085990261286 max memory_allocated 22566.11669921875 
[2025-03-26 17:11:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.1059798002243042 norm:0.0002128321648342535 max memory_allocated 22566.11669921875 
[2025-03-26 17:11:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.1058414950966835 norm:0.0001950951700564474 max memory_allocated 22566.11669921875 
[2025-03-26 17:12:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.10571867972612381 norm:0.0001892463769763708 max memory_allocated 22566.11669921875 
[2025-03-26 17:12:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.10562703013420105 norm:0.00017816282343119383 max memory_allocated 22566.11669921875 
[2025-03-26 17:13:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.10558351874351501 norm:0.00016893708379939198 max memory_allocated 22566.11669921875 
[2025-03-26 17:13:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.10553552955389023 norm:0.00017241020395886153 max memory_allocated 22566.11669921875 
[2025-03-26 17:14:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.10548336803913116 norm:0.00015647304826416075 max memory_allocated 22566.11669921875 
[2025-03-26 17:14:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.10543425381183624 norm:0.00014171615475788713 max memory_allocated 22566.11669921875 
[2025-03-26 17:15:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.10539479553699493 norm:0.00015213947335723788 max memory_allocated 22566.11669921875 
[2025-03-26 17:16:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.1053464487195015 norm:0.000143925950396806 max memory_allocated 22566.11669921875 
[2025-03-26 17:16:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.1052904799580574 norm:0.00013539797510020435 max memory_allocated 22566.11669921875 
[2025-03-26 17:16:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-26 17:17:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.15154318511486053 norm:0.004096644930541515 max memory_allocated 22566.28857421875 
[2025-03-26 17:17:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.13993795216083527 norm:0.0020129752811044455 max memory_allocated 22566.28857421875 
[2025-03-26 17:18:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.12819939851760864 norm:0.0007715721148997545 max memory_allocated 22566.28857421875 
[2025-03-26 17:19:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.12463602423667908 norm:0.0004568005970213562 max memory_allocated 22566.28857421875 
[2025-03-26 17:19:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.12322872132062912 norm:0.00041049611172638834 max memory_allocated 22566.28857421875 
[2025-03-26 17:20:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.12266562879085541 norm:0.0004233003710396588 max memory_allocated 22566.28857421875 
[2025-03-26 17:20:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.12233670055866241 norm:0.0003786577726714313 max memory_allocated 22566.28857421875 
[2025-03-26 17:21:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.12209007889032364 norm:0.0003501179162412882 max memory_allocated 22566.28857421875 
[2025-03-26 17:21:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.12185115367174149 norm:0.00030319346114993095 max memory_allocated 22566.28857421875 
[2025-03-26 17:22:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.12161761522293091 norm:0.000268959382083267 max memory_allocated 22566.28857421875 
[2025-03-26 17:23:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.12140551954507828 norm:0.0002564145834185183 max memory_allocated 22566.28857421875 
[2025-03-26 17:23:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.12127702683210373 norm:0.0002515943197067827 max memory_allocated 22566.28857421875 
[2025-03-26 17:24:09 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.12119095772504807 norm:0.00022564196842722595 max memory_allocated 22566.28857421875 
[2025-03-26 17:24:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.12115369737148285 norm:0.0002160797012038529 max memory_allocated 22566.28857421875 
[2025-03-26 17:25:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.12103834003210068 norm:0.0001957228232640773 max memory_allocated 22566.28857421875 
[2025-03-26 17:25:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.12097492069005966 norm:0.00020909029990434647 max memory_allocated 22566.28857421875 
[2025-03-26 17:26:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.12085893005132675 norm:0.00019168529252056032 max memory_allocated 22566.28857421875 
[2025-03-26 17:26:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.12079180777072906 norm:0.0001789182424545288 max memory_allocated 22566.28857421875 
[2025-03-26 17:27:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.12073556333780289 norm:0.00017417244089301676 max memory_allocated 22566.28857421875 
[2025-03-26 17:28:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.12066998332738876 norm:0.00016616235370747745 max memory_allocated 22566.28857421875 
[2025-03-26 17:28:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-26 17:28:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.16311468183994293 norm:0.0038108602166175842 max memory_allocated 22566.46044921875 
[2025-03-26 17:29:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.152552992105484 norm:0.0015355239156633615 max memory_allocated 22566.46044921875 
[2025-03-26 17:29:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.14336156845092773 norm:0.0005193381221033633 max memory_allocated 22566.46044921875 
[2025-03-26 17:30:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.14035817980766296 norm:0.0004386232467368245 max memory_allocated 22566.46044921875 
[2025-03-26 17:31:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.13906551897525787 norm:0.00036122248275205493 max memory_allocated 22566.46044921875 
[2025-03-26 17:31:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.1385953277349472 norm:0.0003100486355833709 max memory_allocated 22566.46044921875 
[2025-03-26 17:32:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.13828806579113007 norm:0.00028075772570446134 max memory_allocated 22566.46044921875 
[2025-03-26 17:32:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.13800184428691864 norm:0.00026174256345257163 max memory_allocated 22566.46044921875 
[2025-03-26 17:33:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.13782861828804016 norm:0.00024774065241217613 max memory_allocated 22566.46044921875 
[2025-03-26 17:33:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.13767072558403015 norm:0.00023573063663206995 max memory_allocated 22566.46044921875 
[2025-03-26 17:34:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.13753369450569153 norm:0.0002087335888063535 max memory_allocated 22566.46044921875 
[2025-03-26 17:35:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.13740302622318268 norm:0.00019888559472747147 max memory_allocated 22566.46044921875 
[2025-03-26 17:35:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.13728922605514526 norm:0.00018594047287479043 max memory_allocated 22566.46044921875 
[2025-03-26 17:36:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.13720549643039703 norm:0.00017756142187863588 max memory_allocated 22566.46044921875 
[2025-03-26 17:36:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.137140154838562 norm:0.00016849693201947957 max memory_allocated 22566.46044921875 
[2025-03-26 17:37:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.13710591197013855 norm:0.00015984344645403326 max memory_allocated 22566.46044921875 
[2025-03-26 17:37:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.13701920211315155 norm:0.0001530415756860748 max memory_allocated 22566.46044921875 
[2025-03-26 17:38:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.1369476467370987 norm:0.00015860929852351546 max memory_allocated 22566.46044921875 
[2025-03-26 17:38:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.13691352307796478 norm:0.00014073486090637743 max memory_allocated 22566.46044921875 
[2025-03-26 17:39:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.13686642050743103 norm:0.00013880243932362646 max memory_allocated 22566.46044921875 
[2025-03-26 17:39:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-26 17:40:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.18824684619903564 norm:0.006764301098883152 max memory_allocated 22566.63232421875 
[2025-03-26 17:40:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.17401163280010223 norm:0.0024334487970918417 max memory_allocated 22566.63232421875 
[2025-03-26 17:41:26 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.1626029759645462 norm:0.0006146479863673449 max memory_allocated 22566.63232421875 
[2025-03-26 17:41:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.15943175554275513 norm:0.00036533083766698837 max memory_allocated 22566.63232421875 
[2025-03-26 17:42:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.1581837236881256 norm:0.0003524802450556308 max memory_allocated 22566.63232421875 
[2025-03-26 17:43:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.15777654945850372 norm:0.000336877244990319 max memory_allocated 22566.63232421875 
[2025-03-26 17:43:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.15753214061260223 norm:0.0003198194899596274 max memory_allocated 22566.63232421875 
[2025-03-26 17:44:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.15728431940078735 norm:0.0002862623368855566 max memory_allocated 22566.63232421875 
[2025-03-26 17:44:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.1570810228586197 norm:0.00026635974063538015 max memory_allocated 22566.63232421875 
[2025-03-26 17:45:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.15692201256752014 norm:0.0002381728554610163 max memory_allocated 22566.63232421875 
[2025-03-26 17:45:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.1568175107240677 norm:0.00022906280355527997 max memory_allocated 22566.63232421875 
[2025-03-26 17:46:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.15669015049934387 norm:0.00021221998031251132 max memory_allocated 22566.63232421875 
[2025-03-26 17:47:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.15658314526081085 norm:0.00019760295981541276 max memory_allocated 22566.63232421875 
[2025-03-26 17:47:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.1564970463514328 norm:0.0001924113603308797 max memory_allocated 22566.63232421875 
[2025-03-26 17:48:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.15642951428890228 norm:0.00019123253878206015 max memory_allocated 22566.63232421875 
[2025-03-26 17:48:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.15636222064495087 norm:0.0001786258799256757 max memory_allocated 22566.63232421875 
[2025-03-26 17:49:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.15629397332668304 norm:0.00017690373351797462 max memory_allocated 22566.63232421875 
[2025-03-26 17:49:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.15626002848148346 norm:0.0001693271624390036 max memory_allocated 22566.63232421875 
[2025-03-26 17:50:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.15622752904891968 norm:0.00016037252498790622 max memory_allocated 22566.63232421875 
[2025-03-26 17:50:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.15619033575057983 norm:0.00015995741705410182 max memory_allocated 22566.63232421875 
[2025-03-26 17:51:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-26 17:51:11 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 17:51:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.20990294218063354 norm:0.01610611006617546 max memory_allocated 22566.91943359375 
[2025-03-26 17:52:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.19703158736228943 norm:0.011421997100114822 max memory_allocated 22566.91943359375 
[2025-03-26 17:52:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.18673770129680634 norm:0.007100965827703476 max memory_allocated 22566.91943359375 
[2025-03-26 17:53:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.18324901163578033 norm:0.0056093004532158375 max memory_allocated 22566.91943359375 
[2025-03-26 17:54:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.18196739256381989 norm:0.004872908815741539 max memory_allocated 22566.91943359375 
[2025-03-26 17:54:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.18147629499435425 norm:0.004262902773916721 max memory_allocated 22566.91943359375 
[2025-03-26 17:55:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.18113845586776733 norm:0.0036956085823476315 max memory_allocated 22566.91943359375 
[2025-03-26 17:55:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.18080805242061615 norm:0.003174521727487445 max memory_allocated 22566.91943359375 
[2025-03-26 17:56:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.18052147328853607 norm:0.002741366857662797 max memory_allocated 22566.91943359375 
[2025-03-26 17:56:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.18042965233325958 norm:0.0026849042624235153 max memory_allocated 22566.91943359375 
[2025-03-26 17:57:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.18033991754055023 norm:0.002663680352270603 max memory_allocated 22566.91943359375 
[2025-03-26 17:57:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.18031907081604004 norm:0.0026818038895726204 max memory_allocated 22566.91943359375 
[2025-03-26 17:58:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.18013866245746613 norm:0.0024902678560465574 max memory_allocated 22566.91943359375 
[2025-03-26 17:59:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.18003305792808533 norm:0.0023200123105198145 max memory_allocated 22566.91943359375 
[2025-03-26 17:59:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.17993932962417603 norm:0.0022216984070837498 max memory_allocated 22566.91943359375 
[2025-03-26 18:00:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.17986661195755005 norm:0.0021453918889164925 max memory_allocated 22566.91943359375 
[2025-03-26 18:00:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.17980939149856567 norm:0.0020787923131138086 max memory_allocated 22566.91943359375 
[2025-03-26 18:01:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.17979374527931213 norm:0.002075652591884136 max memory_allocated 22566.91943359375 
[2025-03-26 18:01:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.1797923445701599 norm:0.002012710552662611 max memory_allocated 22566.91943359375 
[2025-03-26 18:02:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.17974919080734253 norm:0.0019999046344310045 max memory_allocated 22566.91943359375 
[2025-03-26 18:02:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-26 18:02:40 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 18:03:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.24182438850402832 norm:0.015586481429636478 max memory_allocated 22567.09130859375 
[2025-03-26 18:03:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.2263805866241455 norm:0.01071638148277998 max memory_allocated 22567.09130859375 
[2025-03-26 18:04:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.2142663598060608 norm:0.00669922586530447 max memory_allocated 22567.09130859375 
[2025-03-26 18:04:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.21056567132472992 norm:0.005610156338661909 max memory_allocated 22567.09130859375 
[2025-03-26 18:05:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.20912989974021912 norm:0.004805383272469044 max memory_allocated 22567.09130859375 
[2025-03-26 18:06:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.2085222601890564 norm:0.00416781147941947 max memory_allocated 22567.09130859375 
[2025-03-26 18:06:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.20799362659454346 norm:0.003580681513994932 max memory_allocated 22567.09130859375 
[2025-03-26 18:07:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.20762890577316284 norm:0.0030409847386181355 max memory_allocated 22567.09130859375 
[2025-03-26 18:07:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.20739005506038666 norm:0.0026965083088725805 max memory_allocated 22567.09130859375 
[2025-03-26 18:08:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.20720650255680084 norm:0.002577605191618204 max memory_allocated 22567.09130859375 
[2025-03-26 18:08:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.20713616907596588 norm:0.002710148924961686 max memory_allocated 22567.09130859375 
[2025-03-26 18:09:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.2071230709552765 norm:0.0026911853346973658 max memory_allocated 22567.09130859375 
[2025-03-26 18:09:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.20691758394241333 norm:0.002688917564228177 max memory_allocated 22567.09130859375 
[2025-03-26 18:10:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.20679716765880585 norm:0.0023492558393627405 max memory_allocated 22567.09130859375 
[2025-03-26 18:11:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.20669497549533844 norm:0.0023695307318121195 max memory_allocated 22567.09130859375 
[2025-03-26 18:11:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.2066013514995575 norm:0.0021636243909597397 max memory_allocated 22567.09130859375 
[2025-03-26 18:12:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.20652645826339722 norm:0.002172838430851698 max memory_allocated 22567.09130859375 
[2025-03-26 18:12:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.206455260515213 norm:0.002066792454570532 max memory_allocated 22567.09130859375 
[2025-03-26 18:13:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.2064184844493866 norm:0.0020249406807124615 max memory_allocated 22567.09130859375 
[2025-03-26 18:13:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.20642313361167908 norm:0.002041015774011612 max memory_allocated 22567.09130859375 
[2025-03-26 18:14:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-26 18:14:08 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 18:14:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.435255229473114 norm:0.049440719187259674 max memory_allocated 22567.26318359375 
[2025-03-26 18:15:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.3495176136493683 norm:0.03627220168709755 max memory_allocated 22567.26318359375 
[2025-03-26 18:15:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.3093820810317993 norm:0.027731455862522125 max memory_allocated 22567.26318359375 
[2025-03-26 18:16:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.29568564891815186 norm:0.0243776123970747 max memory_allocated 22567.26318359375 
[2025-03-26 18:16:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.29035717248916626 norm:0.02145029976963997 max memory_allocated 22567.26318359375 
[2025-03-26 18:17:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.2873310446739197 norm:0.01954505406320095 max memory_allocated 22567.26318359375 
[2025-03-26 18:18:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.28513166308403015 norm:0.019751178100705147 max memory_allocated 22567.26318359375 
[2025-03-26 18:18:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.28357818722724915 norm:0.01885373704135418 max memory_allocated 22567.26318359375 
[2025-03-26 18:19:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.28254303336143494 norm:0.018313078209757805 max memory_allocated 22567.26318359375 
[2025-03-26 18:19:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.2822502851486206 norm:0.017468338832259178 max memory_allocated 22567.26318359375 
[2025-03-26 18:20:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.2807994484901428 norm:0.016027089208364487 max memory_allocated 22567.26318359375 
[2025-03-26 18:20:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.28092676401138306 norm:0.015666289255023003 max memory_allocated 22567.26318359375 
[2025-03-26 18:21:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.2809511721134186 norm:0.014976716600358486 max memory_allocated 22567.26318359375 
[2025-03-26 18:22:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.28037208318710327 norm:0.015163777396082878 max memory_allocated 22567.26318359375 
[2025-03-26 18:22:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.28019604086875916 norm:0.014714883640408516 max memory_allocated 22567.26318359375 
[2025-03-26 18:23:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.28016406297683716 norm:0.014932923018932343 max memory_allocated 22567.26318359375 
[2025-03-26 18:23:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.2783072292804718 norm:0.013878529891371727 max memory_allocated 22567.26318359375 
[2025-03-26 18:24:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.2787039577960968 norm:0.013414948247373104 max memory_allocated 22567.26318359375 
[2025-03-26 18:24:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.28034377098083496 norm:0.01409061811864376 max memory_allocated 22567.26318359375 
[2025-03-26 18:25:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.2781997621059418 norm:0.012932594865560532 max memory_allocated 22567.26318359375 
[2025-03-26 18:25:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-26 18:25:38 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 18:26:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.6282126307487488 norm:0.0732639953494072 max memory_allocated 22567.43505859375 
[2025-03-26 18:26:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.56327885389328 norm:0.05291840434074402 max memory_allocated 22567.43505859375 
[2025-03-26 18:27:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.5152574777603149 norm:0.03735724836587906 max memory_allocated 22567.43505859375 
[2025-03-26 18:27:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.501285970211029 norm:0.03344206139445305 max memory_allocated 22567.43505859375 
[2025-03-26 18:28:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.4941732883453369 norm:0.02947065979242325 max memory_allocated 22567.43505859375 
[2025-03-26 18:29:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.48990723490715027 norm:0.02636175975203514 max memory_allocated 22567.43505859375 
[2025-03-26 18:29:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.48620346188545227 norm:0.024495892226696014 max memory_allocated 22567.43505859375 
[2025-03-26 18:30:08 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.4834284782409668 norm:0.023030871525406837 max memory_allocated 22567.43505859375 
[2025-03-26 18:30:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.4814048707485199 norm:0.022125663235783577 max memory_allocated 22567.43505859375 
[2025-03-26 18:31:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.4798356294631958 norm:0.02094535157084465 max memory_allocated 22567.43505859375 
[2025-03-26 18:31:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.4789156913757324 norm:0.020330611616373062 max memory_allocated 22567.43505859375 
[2025-03-26 18:32:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.4774840474128723 norm:0.019251253455877304 max memory_allocated 22567.43505859375 
[2025-03-26 18:32:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.47707653045654297 norm:0.01952163502573967 max memory_allocated 22567.43505859375 
[2025-03-26 18:33:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.47665876150131226 norm:0.019543495029211044 max memory_allocated 22567.43505859375 
[2025-03-26 18:34:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.4767070412635803 norm:0.020407041534781456 max memory_allocated 22567.43505859375 
[2025-03-26 18:34:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.475857675075531 norm:0.019137278199195862 max memory_allocated 22567.43505859375 
[2025-03-26 18:35:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.4752565026283264 norm:0.01739152893424034 max memory_allocated 22567.43505859375 
[2025-03-26 18:35:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.47590771317481995 norm:0.01878083497285843 max memory_allocated 22567.43505859375 
[2025-03-26 18:36:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.47632554173469543 norm:0.020502200350165367 max memory_allocated 22567.43505859375 
[2025-03-26 18:36:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.47611093521118164 norm:0.01968466304242611 max memory_allocated 22567.43505859375 
[2025-03-26 18:37:04 root] (main_calibration.py 365): INFO 21979.537672281265
[2025-03-26 18:37:10 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-26 18:38:21 root] (main_calibration.py 158): INFO wikitext2 : 5.728122711181641
[2025-03-26 18:38:21 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-26 18:40:12 root] (main_calibration.py 158): INFO c4 : 7.3133978843688965
[2025-03-26 20:13:11 root] (main_calibration.py 169): INFO {'wikitext2': 5.728122711181641, 'c4': 7.3133978843688965, 'results': {'winogrande': {'acc': 0.665351223362273, 'acc_stderr': 0.013261823629558371}, 'arc_challenge': {'acc': 0.3873720136518771, 'acc_stderr': 0.014235872487909872, 'acc_norm': 0.39590443686006827, 'acc_norm_stderr': 0.014291228393536588}, 'boolq': {'acc': 0.6896024464831805, 'acc_stderr': 0.008091910698229253}, 'hellaswag': {'acc': 0.551682931686915, 'acc_stderr': 0.004963053161193602, 'acc_norm': 0.7101175064728141, 'acc_norm_stderr': 0.004527804016253781}, 'arc_easy': {'acc': 0.6872895622895623, 'acc_stderr': 0.00951281949144374, 'acc_norm': 0.5248316498316499, 'acc_norm_stderr': 0.010247123122159293}, 'piqa': {'acc': 0.7747551686615887, 'acc_stderr': 0.009746643471032155, 'acc_norm': 0.7704026115342764, 'acc_norm_stderr': 0.009812682950815181}}, 'versions': {'winogrande': 0, 'arc_challenge': 0, 'boolq': 1, 'hellaswag': 0, 'arc_easy': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
