[2025-03-04 12:45:59 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-7b-hf-w4a6', save_dir='./log-calibration-compensation-lwc/quant/llama-7b-hf-w4a6', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-04 12:52:03 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-04 12:52:04 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-04 12:52:04 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-04 12:52:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-04 12:52:10 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 12:52:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.02341221086680889 norm:0.022283261641860008 max memory_allocated 22559.10693359375 
[2025-03-04 12:53:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.014347688294947147 norm:0.011764315888285637 max memory_allocated 22559.10693359375 
[2025-03-04 12:53:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.01059515867382288 norm:0.006734740920364857 max memory_allocated 22559.10693359375 
[2025-03-04 12:54:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.009567223489284515 norm:0.0056555140763521194 max memory_allocated 22559.10693359375 
[2025-03-04 12:54:58 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.009074799716472626 norm:0.004791738465428352 max memory_allocated 22559.10693359375 
[2025-03-04 12:55:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.008760891854763031 norm:0.004166030324995518 max memory_allocated 22559.10693359375 
[2025-03-04 12:56:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.008655314333736897 norm:0.003730740863829851 max memory_allocated 22559.10693359375 
[2025-03-04 12:56:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.008544820360839367 norm:0.003330918261781335 max memory_allocated 22559.10693359375 
[2025-03-04 12:57:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.008440970443189144 norm:0.0029604327864944935 max memory_allocated 22559.10693359375 
[2025-03-04 12:57:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.008205643855035305 norm:0.0024430707562714815 max memory_allocated 22559.10693359375 
[2025-03-04 12:58:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.008127577602863312 norm:0.0021705590188503265 max memory_allocated 22559.10693359375 
[2025-03-04 12:58:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.008076533675193787 norm:0.0019142101518809795 max memory_allocated 22559.10693359375 
[2025-03-04 12:59:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.008013706654310226 norm:0.0017694947309792042 max memory_allocated 22559.10693359375 
[2025-03-04 13:00:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.007994420826435089 norm:0.0015108402585610747 max memory_allocated 22559.10693359375 
[2025-03-04 13:00:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.007944278419017792 norm:0.0013441548217087984 max memory_allocated 22559.10693359375 
[2025-03-04 13:01:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.007979096844792366 norm:0.0013649521861225367 max memory_allocated 22559.10693359375 
[2025-03-04 13:01:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.007896941155195236 norm:0.0012587143573909998 max memory_allocated 22559.10693359375 
[2025-03-04 13:02:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00787430815398693 norm:0.001213669776916504 max memory_allocated 22559.10693359375 
[2025-03-04 13:02:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.007921087555587292 norm:0.0011982113355770707 max memory_allocated 22559.10693359375 
[2025-03-04 13:03:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.007966496981680393 norm:0.0011370036518201232 max memory_allocated 22559.10693359375 
[2025-03-04 13:03:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-04 13:03:36 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 13:04:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.04113568365573883 norm:0.0260616447776556 max memory_allocated 22559.27880859375 
[2025-03-04 13:04:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.02761990576982498 norm:0.018193108960986137 max memory_allocated 22559.27880859375 
[2025-03-04 13:05:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.02164669707417488 norm:0.009933291003108025 max memory_allocated 22559.27880859375 
[2025-03-04 13:05:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.019803646951913834 norm:0.00747754005715251 max memory_allocated 22559.27880859375 
[2025-03-04 13:06:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.01907876878976822 norm:0.006538094952702522 max memory_allocated 22559.27880859375 
[2025-03-04 13:06:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.018510716035962105 norm:0.00583992525935173 max memory_allocated 22559.27880859375 
[2025-03-04 13:07:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.018120478838682175 norm:0.0052622416988015175 max memory_allocated 22559.27880859375 
[2025-03-04 13:08:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0178668275475502 norm:0.004723598249256611 max memory_allocated 22559.27880859375 
[2025-03-04 13:08:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.01772543415427208 norm:0.004319584928452969 max memory_allocated 22559.27880859375 
[2025-03-04 13:09:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.017464090138673782 norm:0.003915945999324322 max memory_allocated 22559.27880859375 
[2025-03-04 13:09:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.017322963103652 norm:0.0036538438871502876 max memory_allocated 22559.27880859375 
[2025-03-04 13:10:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.01722487062215805 norm:0.0033216269221156836 max memory_allocated 22559.27880859375 
[2025-03-04 13:10:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.017135944217443466 norm:0.002987218089401722 max memory_allocated 22559.27880859375 
[2025-03-04 13:11:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.01706300675868988 norm:0.002726238453760743 max memory_allocated 22559.27880859375 
[2025-03-04 13:12:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.016974501311779022 norm:0.002414913149550557 max memory_allocated 22559.27880859375 
[2025-03-04 13:12:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.016919627785682678 norm:0.002233099890872836 max memory_allocated 22559.27880859375 
[2025-03-04 13:13:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.016875064000487328 norm:0.0019836947321891785 max memory_allocated 22559.27880859375 
[2025-03-04 13:13:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.016873953863978386 norm:0.002005873015150428 max memory_allocated 22559.27880859375 
[2025-03-04 13:14:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.016900695860385895 norm:0.0019377021817490458 max memory_allocated 22559.27880859375 
[2025-03-04 13:14:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.016910936683416367 norm:0.0018764123087748885 max memory_allocated 22559.27880859375 
[2025-03-04 13:15:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-04 13:15:03 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 13:15:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.047488048672676086 norm:0.013973206281661987 max memory_allocated 22559.45068359375 
[2025-03-04 13:16:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.04086547717452049 norm:0.008060893975198269 max memory_allocated 22559.45068359375 
[2025-03-04 13:16:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0362560898065567 norm:0.006761615164577961 max memory_allocated 22559.45068359375 
[2025-03-04 13:17:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.034416794776916504 norm:0.006877685431391001 max memory_allocated 22559.45068359375 
[2025-03-04 13:17:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.033530011773109436 norm:0.00656263530254364 max memory_allocated 22559.45068359375 
[2025-03-04 13:18:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.03303726017475128 norm:0.006144948303699493 max memory_allocated 22559.45068359375 
[2025-03-04 13:19:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.03264644742012024 norm:0.005856974981725216 max memory_allocated 22559.45068359375 
[2025-03-04 13:19:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.03261934965848923 norm:0.005729708820581436 max memory_allocated 22559.45068359375 
[2025-03-04 13:20:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.03218496963381767 norm:0.005697896704077721 max memory_allocated 22559.45068359375 
[2025-03-04 13:20:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.03211235627532005 norm:0.004997388459742069 max memory_allocated 22559.45068359375 
[2025-03-04 13:21:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.03208852931857109 norm:0.00485466280952096 max memory_allocated 22559.45068359375 
[2025-03-04 13:21:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0322757363319397 norm:0.0049105677753686905 max memory_allocated 22559.45068359375 
[2025-03-04 13:22:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.031953830271959305 norm:0.004684268496930599 max memory_allocated 22559.45068359375 
[2025-03-04 13:22:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.03218543156981468 norm:0.004965062253177166 max memory_allocated 22559.45068359375 
[2025-03-04 13:23:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.03237900137901306 norm:0.004843462258577347 max memory_allocated 22559.45068359375 
[2025-03-04 13:24:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.03213956207036972 norm:0.00465058209374547 max memory_allocated 22559.45068359375 
[2025-03-04 13:24:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.03209366276860237 norm:0.006376671604812145 max memory_allocated 22559.45068359375 
[2025-03-04 13:25:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.031949616968631744 norm:0.0053184423595666885 max memory_allocated 22559.45068359375 
[2025-03-04 13:25:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.03204265236854553 norm:0.005214956123381853 max memory_allocated 22559.45068359375 
[2025-03-04 13:26:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.03175295889377594 norm:0.004934070631861687 max memory_allocated 22559.45068359375 
[2025-03-04 13:26:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-04 13:27:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.06196045130491257 norm:0.013706419616937637 max memory_allocated 22559.50732421875 
[2025-03-04 13:27:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.045871879905462265 norm:0.003551728092133999 max memory_allocated 22559.50732421875 
[2025-03-04 13:28:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.03740761801600456 norm:0.001167639042250812 max memory_allocated 22559.50732421875 
[2025-03-04 13:28:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.03508497774600983 norm:0.0008028544252738357 max memory_allocated 22559.50732421875 
[2025-03-04 13:29:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.03393656760454178 norm:0.0006356040248647332 max memory_allocated 22559.50732421875 
[2025-03-04 13:29:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.03313426673412323 norm:0.000545412942301482 max memory_allocated 22559.50732421875 
[2025-03-04 13:30:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.032840073108673096 norm:0.0004930946743115783 max memory_allocated 22559.50732421875 
[2025-03-04 13:31:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.03260796517133713 norm:0.0004353793920017779 max memory_allocated 22559.50732421875 
[2025-03-04 13:31:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.03250811994075775 norm:0.00041868118569254875 max memory_allocated 22559.50732421875 
[2025-03-04 13:32:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.03239365294575691 norm:0.0003666839620564133 max memory_allocated 22559.50732421875 
[2025-03-04 13:32:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.03225893899798393 norm:0.0003307096485514194 max memory_allocated 22559.50732421875 
[2025-03-04 13:33:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.032174475491046906 norm:0.00032046553678810596 max memory_allocated 22559.50732421875 
[2025-03-04 13:33:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.03215105086565018 norm:0.0002865181886591017 max memory_allocated 22559.50732421875 
[2025-03-04 13:34:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.03212013840675354 norm:0.00026575918309390545 max memory_allocated 22559.50732421875 
[2025-03-04 13:34:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.032070454210042953 norm:0.0002536735264584422 max memory_allocated 22559.50732421875 
[2025-03-04 13:35:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0320926234126091 norm:0.0002452946500852704 max memory_allocated 22559.50732421875 
[2025-03-04 13:36:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.032139312475919724 norm:0.0002291898417752236 max memory_allocated 22559.50732421875 
[2025-03-04 13:36:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.03208506852388382 norm:0.00021535501582548022 max memory_allocated 22559.50732421875 
[2025-03-04 13:37:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.032072778791189194 norm:0.00020376249449327588 max memory_allocated 22559.50732421875 
[2025-03-04 13:37:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.032062653452157974 norm:0.00020519043027888983 max memory_allocated 22559.50732421875 
[2025-03-04 13:37:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-04 13:38:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.08012081682682037 norm:0.01489588525146246 max memory_allocated 22559.67919921875 
[2025-03-04 13:39:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.06044613569974899 norm:0.005029838532209396 max memory_allocated 22559.67919921875 
[2025-03-04 13:39:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.04572569206357002 norm:0.0013856670120730996 max memory_allocated 22559.67919921875 
[2025-03-04 13:40:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.042109325528144836 norm:0.0008300459594465792 max memory_allocated 22559.67919921875 
[2025-03-04 13:40:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.04060211032629013 norm:0.0006857234402559698 max memory_allocated 22559.67919921875 
[2025-03-04 13:41:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.039805442094802856 norm:0.0006316059152595699 max memory_allocated 22559.67919921875 
[2025-03-04 13:41:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.039479706436395645 norm:0.0005772144068032503 max memory_allocated 22559.67919921875 
[2025-03-04 13:42:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.039078548550605774 norm:0.0005028157611377537 max memory_allocated 22559.67919921875 
[2025-03-04 13:43:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.03896678239107132 norm:0.00046334165381267667 max memory_allocated 22559.67919921875 
[2025-03-04 13:43:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.038844432681798935 norm:0.0004279889981262386 max memory_allocated 22559.67919921875 
[2025-03-04 13:44:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.03876866027712822 norm:0.00040154028101824224 max memory_allocated 22559.67919921875 
[2025-03-04 13:44:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.03862776607275009 norm:0.00037766716559417546 max memory_allocated 22559.67919921875 
[2025-03-04 13:45:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.038598816841840744 norm:0.0003540025500115007 max memory_allocated 22559.67919921875 
[2025-03-04 13:45:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.038543540984392166 norm:0.0003388441982679069 max memory_allocated 22559.67919921875 
[2025-03-04 13:46:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.03843160718679428 norm:0.0003335795772727579 max memory_allocated 22559.67919921875 
[2025-03-04 13:46:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.038353368639945984 norm:0.00031210825545713305 max memory_allocated 22559.67919921875 
[2025-03-04 13:47:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.038404643535614014 norm:0.0002944299776572734 max memory_allocated 22559.67919921875 
[2025-03-04 13:48:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.038351595401763916 norm:0.00029877727502025664 max memory_allocated 22559.67919921875 
[2025-03-04 13:48:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.038409870117902756 norm:0.0002826220297720283 max memory_allocated 22559.67919921875 
[2025-03-04 13:49:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.03833623230457306 norm:0.0002829949662555009 max memory_allocated 22559.67919921875 
[2025-03-04 13:49:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-04 13:49:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0963410809636116 norm:0.022770242765545845 max memory_allocated 22559.85107421875 
[2025-03-04 13:50:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.06927937269210815 norm:0.00690773781388998 max memory_allocated 22559.85107421875 
[2025-03-04 13:51:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.05278933793306351 norm:0.0018760549137368798 max memory_allocated 22559.85107421875 
[2025-03-04 13:51:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.048076845705509186 norm:0.0010929115815088153 max memory_allocated 22559.85107421875 
[2025-03-04 13:52:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.046303462237119675 norm:0.0008584368042647839 max memory_allocated 22559.85107421875 
[2025-03-04 13:52:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.04518480598926544 norm:0.0007620444521307945 max memory_allocated 22559.85107421875 
[2025-03-04 13:53:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.044473353773355484 norm:0.0006793157663196325 max memory_allocated 22559.85107421875 
[2025-03-04 13:53:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.04406996816396713 norm:0.0005852736649103463 max memory_allocated 22559.85107421875 
[2025-03-04 13:54:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.04391062632203102 norm:0.0005678773741237819 max memory_allocated 22559.85107421875 
[2025-03-04 13:55:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.043838538229465485 norm:0.0005234978161752224 max memory_allocated 22559.85107421875 
[2025-03-04 13:55:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.043563202023506165 norm:0.00046164117520675063 max memory_allocated 22559.85107421875 
[2025-03-04 13:56:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.043534472584724426 norm:0.0004273447848390788 max memory_allocated 22559.85107421875 
[2025-03-04 13:56:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.04335577040910721 norm:0.00039387529250234365 max memory_allocated 22559.85107421875 
[2025-03-04 13:57:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.04321536049246788 norm:0.0003610621497500688 max memory_allocated 22559.85107421875 
[2025-03-04 13:57:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0431959368288517 norm:0.00035181024577468634 max memory_allocated 22559.85107421875 
[2025-03-04 13:58:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.043199047446250916 norm:0.0003326164442114532 max memory_allocated 22559.85107421875 
[2025-03-04 13:58:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.04330401122570038 norm:0.00034286477603018284 max memory_allocated 22559.85107421875 
[2025-03-04 13:59:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0432673878967762 norm:0.00031750398920848966 max memory_allocated 22559.85107421875 
[2025-03-04 14:00:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.04324522614479065 norm:0.0003157227474730462 max memory_allocated 22559.85107421875 
[2025-03-04 14:00:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0432310625910759 norm:0.0003045887278858572 max memory_allocated 22559.85107421875 
[2025-03-04 14:00:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-04 14:01:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.10147275775671005 norm:0.026967128738760948 max memory_allocated 22560.02294921875 
[2025-03-04 14:02:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.07796891033649445 norm:0.009808562695980072 max memory_allocated 22560.02294921875 
[2025-03-04 14:02:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.05965869501233101 norm:0.002664075465872884 max memory_allocated 22560.02294921875 
[2025-03-04 14:03:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.054078422486782074 norm:0.0013374635018408298 max memory_allocated 22560.02294921875 
[2025-03-04 14:03:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.051611173897981644 norm:0.0009471990051679313 max memory_allocated 22560.02294921875 
[2025-03-04 14:04:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.05015529692173004 norm:0.0006680744118057191 max memory_allocated 22560.02294921875 
[2025-03-04 14:04:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.04951849207282066 norm:0.0006206558318808675 max memory_allocated 22560.02294921875 
[2025-03-04 14:05:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.04921958968043327 norm:0.0005947956233285367 max memory_allocated 22560.02294921875 
[2025-03-04 14:05:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.04904145374894142 norm:0.0005645696073770523 max memory_allocated 22560.02294921875 
[2025-03-04 14:06:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.04889155924320221 norm:0.0005305704544298351 max memory_allocated 22560.02294921875 
[2025-03-04 14:07:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.04865645989775658 norm:0.0004706211038865149 max memory_allocated 22560.02294921875 
[2025-03-04 14:07:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.04901771992444992 norm:0.0005124246818013489 max memory_allocated 22560.02294921875 
[2025-03-04 14:08:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.04926806688308716 norm:0.0004950342699885368 max memory_allocated 22560.02294921875 
[2025-03-04 14:08:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.048837266862392426 norm:0.00039198395097628236 max memory_allocated 22560.02294921875 
[2025-03-04 14:09:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.048683252185583115 norm:0.0003847913758363575 max memory_allocated 22560.02294921875 
[2025-03-04 14:09:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.04867461323738098 norm:0.00037651543971151114 max memory_allocated 22560.02294921875 
[2025-03-04 14:10:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.04860387369990349 norm:0.00035156929516233504 max memory_allocated 22560.02294921875 
[2025-03-04 14:10:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.04850900173187256 norm:0.00034036277793347836 max memory_allocated 22560.02294921875 
[2025-03-04 14:11:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0483982115983963 norm:0.0003380774869583547 max memory_allocated 22560.02294921875 
[2025-03-04 14:12:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.04838850349187851 norm:0.00032218024716712534 max memory_allocated 22560.02294921875 
[2025-03-04 14:12:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-04 14:12:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.10172927379608154 norm:0.015943096950650215 max memory_allocated 22560.19482421875 
[2025-03-04 14:13:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.08027464151382446 norm:0.006044900976121426 max memory_allocated 22560.19482421875 
[2025-03-04 14:14:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.06286052614450455 norm:0.0017256694845855236 max memory_allocated 22560.19482421875 
[2025-03-04 14:14:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.05792944133281708 norm:0.0009099200833588839 max memory_allocated 22560.19482421875 
[2025-03-04 14:15:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.05610870569944382 norm:0.0006904692272655666 max memory_allocated 22560.19482421875 
[2025-03-04 14:15:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.05525518208742142 norm:0.0006517136353068054 max memory_allocated 22560.19482421875 
[2025-03-04 14:16:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.05468146502971649 norm:0.0005463879788294435 max memory_allocated 22560.19482421875 
[2025-03-04 14:16:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.054181020706892014 norm:0.00048395583871752024 max memory_allocated 22560.19482421875 
[2025-03-04 14:17:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.05399604141712189 norm:0.0004756041744258255 max memory_allocated 22560.19482421875 
[2025-03-04 14:17:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.054009608924388885 norm:0.0004524315590970218 max memory_allocated 22560.19482421875 
[2025-03-04 14:18:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.053998809307813644 norm:0.00043214287143200636 max memory_allocated 22560.19482421875 
[2025-03-04 14:19:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.05391484498977661 norm:0.0004104801919311285 max memory_allocated 22560.19482421875 
[2025-03-04 14:19:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.05371810868382454 norm:0.0003513630072120577 max memory_allocated 22560.19482421875 
[2025-03-04 14:20:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.05354796722531319 norm:0.00035915349144488573 max memory_allocated 22560.19482421875 
[2025-03-04 14:20:45 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.053553447127342224 norm:0.0003509435919113457 max memory_allocated 22560.19482421875 
[2025-03-04 14:21:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.05357642471790314 norm:0.00034755829256027937 max memory_allocated 22560.19482421875 
[2025-03-04 14:21:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.05360625311732292 norm:0.00033241533674299717 max memory_allocated 22560.19482421875 
[2025-03-04 14:22:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0536407046020031 norm:0.00033329721190966666 max memory_allocated 22560.19482421875 
[2025-03-04 14:23:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.053627777844667435 norm:0.00031057107844389975 max memory_allocated 22560.19482421875 
[2025-03-04 14:23:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.05360393598675728 norm:0.00030879699625074863 max memory_allocated 22560.19482421875 
[2025-03-04 14:23:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-04 14:24:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.10710231959819794 norm:0.011852862313389778 max memory_allocated 22560.36669921875 
[2025-03-04 14:24:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0828174278140068 norm:0.004204743541777134 max memory_allocated 22560.36669921875 
[2025-03-04 14:25:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06642182171344757 norm:0.0012178253382444382 max memory_allocated 22560.36669921875 
[2025-03-04 14:26:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.06199764832854271 norm:0.0006815880769863725 max memory_allocated 22560.36669921875 
[2025-03-04 14:26:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.060195788741111755 norm:0.0005604158504866064 max memory_allocated 22560.36669921875 
[2025-03-04 14:27:09 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.05910252034664154 norm:0.0004870643315371126 max memory_allocated 22560.36669921875 
[2025-03-04 14:27:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.058513641357421875 norm:0.00046017664135433733 max memory_allocated 22560.36669921875 
[2025-03-04 14:28:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.05830950289964676 norm:0.00039864153950475156 max memory_allocated 22560.36669921875 
[2025-03-04 14:28:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.05806650593876839 norm:0.00039489680784754455 max memory_allocated 22560.36669921875 
[2025-03-04 14:29:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.05802742764353752 norm:0.00039811668102629483 max memory_allocated 22560.36669921875 
[2025-03-04 14:29:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.05784276872873306 norm:0.0003632979351095855 max memory_allocated 22560.36669921875 
[2025-03-04 14:30:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0577983558177948 norm:0.0003431386430747807 max memory_allocated 22560.36669921875 
[2025-03-04 14:31:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.05792076885700226 norm:0.0003326429578009993 max memory_allocated 22560.36669921875 
[2025-03-04 14:31:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.05792994424700737 norm:0.0003280497621744871 max memory_allocated 22560.36669921875 
[2025-03-04 14:32:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.05803412199020386 norm:0.0003349289472680539 max memory_allocated 22560.36669921875 
[2025-03-04 14:32:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.058000415563583374 norm:0.0003130062541458756 max memory_allocated 22560.36669921875 
[2025-03-04 14:33:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.058007363229990005 norm:0.00030524228350259364 max memory_allocated 22560.36669921875 
[2025-03-04 14:33:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.05791310593485832 norm:0.0002915942168328911 max memory_allocated 22560.36669921875 
[2025-03-04 14:34:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.057830050587654114 norm:0.0002784843381959945 max memory_allocated 22560.36669921875 
[2025-03-04 14:35:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.05787084251642227 norm:0.00027368025621399283 max memory_allocated 22560.36669921875 
[2025-03-04 14:35:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-04 14:35:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.11347267776727676 norm:0.012057469226419926 max memory_allocated 22560.53857421875 
[2025-03-04 14:36:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.09049904346466064 norm:0.0047071087174117565 max memory_allocated 22560.53857421875 
[2025-03-04 14:36:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.07288813591003418 norm:0.0015188704710453749 max memory_allocated 22560.53857421875 
[2025-03-04 14:37:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.06729282438755035 norm:0.0007461460190825164 max memory_allocated 22560.53857421875 
[2025-03-04 14:38:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.06555063277482986 norm:0.0006551304250024259 max memory_allocated 22560.53857421875 
[2025-03-04 14:38:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.06480196863412857 norm:0.0006108778179623187 max memory_allocated 22560.53857421875 
[2025-03-04 14:39:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.06403018534183502 norm:0.0004946113913320005 max memory_allocated 22560.53857421875 
[2025-03-04 14:39:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.06336630880832672 norm:0.0004074914613738656 max memory_allocated 22560.53857421875 
[2025-03-04 14:40:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.06321894377470016 norm:0.00043044472113251686 max memory_allocated 22560.53857421875 
[2025-03-04 14:40:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.06338165700435638 norm:0.0004240788402967155 max memory_allocated 22560.53857421875 
[2025-03-04 14:41:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.06315268576145172 norm:0.00034999637864530087 max memory_allocated 22560.53857421875 
[2025-03-04 14:41:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.06293417513370514 norm:0.0003346470184624195 max memory_allocated 22560.53857421875 
[2025-03-04 14:42:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.06288094073534012 norm:0.00032363756326958537 max memory_allocated 22560.53857421875 
[2025-03-04 14:43:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.06280073523521423 norm:0.0002945683372672647 max memory_allocated 22560.53857421875 
[2025-03-04 14:43:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.06271356344223022 norm:0.0002851705939974636 max memory_allocated 22560.53857421875 
[2025-03-04 14:44:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.06277141720056534 norm:0.0002734375011641532 max memory_allocated 22560.53857421875 
[2025-03-04 14:44:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.06287486106157303 norm:0.0002920743136201054 max memory_allocated 22560.53857421875 
[2025-03-04 14:45:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06285719573497772 norm:0.00027523445896804333 max memory_allocated 22560.53857421875 
[2025-03-04 14:45:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.06293383240699768 norm:0.0002602668828330934 max memory_allocated 22560.53857421875 
[2025-03-04 14:46:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.06289702653884888 norm:0.0002471570624038577 max memory_allocated 22560.53857421875 
[2025-03-04 14:46:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-04 14:47:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.10033778101205826 norm:0.0070393080823123455 max memory_allocated 22560.71044921875 
[2025-03-04 14:47:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.08640486001968384 norm:0.0032200398854911327 max memory_allocated 22560.71044921875 
[2025-03-04 14:48:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.07368642836809158 norm:0.0009390374762006104 max memory_allocated 22560.71044921875 
[2025-03-04 14:48:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.06980723887681961 norm:0.0005132133373990655 max memory_allocated 22560.71044921875 
[2025-03-04 14:49:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0684247836470604 norm:0.0004372499533928931 max memory_allocated 22560.71044921875 
[2025-03-04 14:50:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.06755953282117844 norm:0.00033130618976429105 max memory_allocated 22560.71044921875 
[2025-03-04 14:50:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.06697279214859009 norm:0.00029492232715711 max memory_allocated 22560.71044921875 
[2025-03-04 14:51:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.06677176058292389 norm:0.0003066762874368578 max memory_allocated 22560.71044921875 
[2025-03-04 14:51:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.06672917306423187 norm:0.0002942374558188021 max memory_allocated 22560.71044921875 
[2025-03-04 14:52:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.066705621778965 norm:0.0002774592430796474 max memory_allocated 22560.71044921875 
[2025-03-04 14:52:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.06679786741733551 norm:0.0002830871962942183 max memory_allocated 22560.71044921875 
[2025-03-04 14:53:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.06678314507007599 norm:0.00024268064589705318 max memory_allocated 22560.71044921875 
[2025-03-04 14:53:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.06663337349891663 norm:0.00022670069301966578 max memory_allocated 22560.71044921875 
[2025-03-04 14:54:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.06654089689254761 norm:0.00021382025443017483 max memory_allocated 22560.71044921875 
[2025-03-04 14:55:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.06646957993507385 norm:0.00020601635333150625 max memory_allocated 22560.71044921875 
[2025-03-04 14:55:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.06657825410366058 norm:0.00021330616436898708 max memory_allocated 22560.71044921875 
[2025-03-04 14:56:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0667460560798645 norm:0.00023140385746955872 max memory_allocated 22560.71044921875 
[2025-03-04 14:56:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.06692849099636078 norm:0.00022601234377361834 max memory_allocated 22560.71044921875 
[2025-03-04 14:57:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.06695850193500519 norm:0.0002182856114814058 max memory_allocated 22560.71044921875 
[2025-03-04 14:57:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.06698733568191528 norm:0.00020741485059261322 max memory_allocated 22560.71044921875 
[2025-03-04 14:58:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-04 14:58:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.10508590191602707 norm:0.009206625632941723 max memory_allocated 22560.88232421875 
[2025-03-04 14:59:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.09072079509496689 norm:0.004216453060507774 max memory_allocated 22560.88232421875 
[2025-03-04 14:59:48 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.077140673995018 norm:0.001158128259703517 max memory_allocated 22560.88232421875 
[2025-03-04 15:00:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.07328949868679047 norm:0.0006145340157672763 max memory_allocated 22560.88232421875 
[2025-03-04 15:00:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.07183375209569931 norm:0.0005224416963756084 max memory_allocated 22560.88232421875 
[2025-03-04 15:01:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.07104538381099701 norm:0.0004976944765076041 max memory_allocated 22560.88232421875 
[2025-03-04 15:02:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.07059358805418015 norm:0.00041799581958912313 max memory_allocated 22560.88232421875 
[2025-03-04 15:02:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0702974870800972 norm:0.00038593471981585026 max memory_allocated 22560.88232421875 
[2025-03-04 15:03:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.07023275643587112 norm:0.00037026579957455397 max memory_allocated 22560.88232421875 
[2025-03-04 15:03:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.07015962898731232 norm:0.0003245174593757838 max memory_allocated 22560.88232421875 
[2025-03-04 15:04:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.06993363052606583 norm:0.0002774426538962871 max memory_allocated 22560.88232421875 
[2025-03-04 15:04:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.06989041715860367 norm:0.0002782770898193121 max memory_allocated 22560.88232421875 
[2025-03-04 15:05:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.06993941962718964 norm:0.00027541787130758166 max memory_allocated 22560.88232421875 
[2025-03-04 15:05:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.06995571404695511 norm:0.00025367538910359144 max memory_allocated 22560.88232421875 
[2025-03-04 15:06:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.06998156011104584 norm:0.00026220534346066415 max memory_allocated 22560.88232421875 
[2025-03-04 15:07:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.07002011686563492 norm:0.00023342715576291084 max memory_allocated 22560.88232421875 
[2025-03-04 15:07:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.06998679786920547 norm:0.00022283736325334758 max memory_allocated 22560.88232421875 
[2025-03-04 15:08:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.06996817886829376 norm:0.00021573751291725785 max memory_allocated 22560.88232421875 
[2025-03-04 15:08:48 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.06992516666650772 norm:0.00020825743558816612 max memory_allocated 22560.88232421875 
[2025-03-04 15:09:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.06987592577934265 norm:0.0001989813317777589 max memory_allocated 22560.88232421875 
[2025-03-04 15:09:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-04 15:10:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.10693641006946564 norm:0.006080604624003172 max memory_allocated 22561.05419921875 
[2025-03-04 15:10:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.09411390125751495 norm:0.0028643773403018713 max memory_allocated 22561.05419921875 
[2025-03-04 15:11:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.08218622952699661 norm:0.0011662370525300503 max memory_allocated 22561.05419921875 
[2025-03-04 15:11:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0774097666144371 norm:0.0005978410481475294 max memory_allocated 22561.05419921875 
[2025-03-04 15:12:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.07582085579633713 norm:0.0005162099841982126 max memory_allocated 22561.05419921875 
[2025-03-04 15:12:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.07495848834514618 norm:0.00044825224904343486 max memory_allocated 22561.05419921875 
[2025-03-04 15:13:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.07445713877677917 norm:0.00039027491584420204 max memory_allocated 22561.05419921875 
[2025-03-04 15:14:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.07413311302661896 norm:0.00034903077175840735 max memory_allocated 22561.05419921875 
[2025-03-04 15:14:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.07402756065130234 norm:0.00032466190168634057 max memory_allocated 22561.05419921875 
[2025-03-04 15:15:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.07384718209505081 norm:0.000286277529085055 max memory_allocated 22561.05419921875 
[2025-03-04 15:15:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.07374005764722824 norm:0.0002674985153134912 max memory_allocated 22561.05419921875 
[2025-03-04 15:16:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.07373294234275818 norm:0.0002665049978531897 max memory_allocated 22561.05419921875 
[2025-03-04 15:16:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.07370991259813309 norm:0.00023883982794359326 max memory_allocated 22561.05419921875 
[2025-03-04 15:17:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.07363218814134598 norm:0.00023118137323763222 max memory_allocated 22561.05419921875 
[2025-03-04 15:18:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.07358957082033157 norm:0.00021521961025428027 max memory_allocated 22561.05419921875 
[2025-03-04 15:18:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0735715925693512 norm:0.00020208809291943908 max memory_allocated 22561.05419921875 
[2025-03-04 15:19:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.07360033690929413 norm:0.00019910359696950763 max memory_allocated 22561.05419921875 
[2025-03-04 15:19:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.07357388734817505 norm:0.0001971879682969302 max memory_allocated 22561.05419921875 
[2025-03-04 15:20:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.07361340522766113 norm:0.0001927954435814172 max memory_allocated 22561.05419921875 
[2025-03-04 15:20:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.07367350906133652 norm:0.00018545647617429495 max memory_allocated 22561.05419921875 
[2025-03-04 15:20:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-04 15:21:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.11196911334991455 norm:0.007203421089798212 max memory_allocated 22561.22607421875 
[2025-03-04 15:22:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.09768932312726974 norm:0.0034115926828235388 max memory_allocated 22561.22607421875 
[2025-03-04 15:22:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.08525626361370087 norm:0.0012594496365636587 max memory_allocated 22561.22607421875 
[2025-03-04 15:23:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.08126050978899002 norm:0.0006402519647963345 max memory_allocated 22561.22607421875 
[2025-03-04 15:23:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.07947789132595062 norm:0.00044627272291108966 max memory_allocated 22561.22607421875 
[2025-03-04 15:24:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.07853446900844574 norm:0.00039846490835770965 max memory_allocated 22561.22607421875 
[2025-03-04 15:24:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.07810075581073761 norm:0.0003649626160040498 max memory_allocated 22561.22607421875 
[2025-03-04 15:25:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.07776292413473129 norm:0.00031754851806908846 max memory_allocated 22561.22607421875 
[2025-03-04 15:26:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.07742732763290405 norm:0.00026848638663068414 max memory_allocated 22561.22607421875 
[2025-03-04 15:26:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.07724615931510925 norm:0.00024250852584373206 max memory_allocated 22561.22607421875 
[2025-03-04 15:27:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.07718902826309204 norm:0.0002487683668732643 max memory_allocated 22561.22607421875 
[2025-03-04 15:27:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.07730375975370407 norm:0.0002611679374240339 max memory_allocated 22561.22607421875 
[2025-03-04 15:28:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.07731185108423233 norm:0.00024201399355661124 max memory_allocated 22561.22607421875 
[2025-03-04 15:28:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.07733815163373947 norm:0.00023582969151902944 max memory_allocated 22561.22607421875 
[2025-03-04 15:29:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.07716652005910873 norm:0.00021962275786790997 max memory_allocated 22561.22607421875 
[2025-03-04 15:30:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.07707149535417557 norm:0.00019148923456668854 max memory_allocated 22561.22607421875 
[2025-03-04 15:30:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.07709835469722748 norm:0.00018611295672599226 max memory_allocated 22561.22607421875 
[2025-03-04 15:31:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0770793929696083 norm:0.00017748020763974637 max memory_allocated 22561.22607421875 
[2025-03-04 15:31:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.07706499099731445 norm:0.0001713697420200333 max memory_allocated 22561.22607421875 
[2025-03-04 15:32:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.07703125476837158 norm:0.00016795433475635946 max memory_allocated 22561.22607421875 
[2025-03-04 15:32:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-04 15:33:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.12736551463603973 norm:0.011092152446508408 max memory_allocated 22561.39794921875 
[2025-03-04 15:33:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.11047817021608353 norm:0.005336528178304434 max memory_allocated 22561.39794921875 
[2025-03-04 15:34:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.09438876062631607 norm:0.0019302568398416042 max memory_allocated 22561.39794921875 
[2025-03-04 15:34:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0885741114616394 norm:0.0009544583735987544 max memory_allocated 22561.39794921875 
[2025-03-04 15:35:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.08639515191316605 norm:0.0007234278600662947 max memory_allocated 22561.39794921875 
[2025-03-04 15:35:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.08515729010105133 norm:0.000600347004365176 max memory_allocated 22561.39794921875 
[2025-03-04 15:36:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.08470681309700012 norm:0.0006025225156918168 max memory_allocated 22561.39794921875 
[2025-03-04 15:36:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.08446084707975388 norm:0.0005521883722394705 max memory_allocated 22561.39794921875 
[2025-03-04 15:37:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.08410410583019257 norm:0.000400593678932637 max memory_allocated 22561.39794921875 
[2025-03-04 15:38:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.08389943093061447 norm:0.0003645273973233998 max memory_allocated 22561.39794921875 
[2025-03-04 15:38:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.08373183012008667 norm:0.00035116664366796613 max memory_allocated 22561.39794921875 
[2025-03-04 15:39:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0836254134774208 norm:0.0003317884693387896 max memory_allocated 22561.39794921875 
[2025-03-04 15:39:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.08349266648292542 norm:0.00030326779233291745 max memory_allocated 22561.39794921875 
[2025-03-04 15:40:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0834425836801529 norm:0.0002993892412632704 max memory_allocated 22561.39794921875 
[2025-03-04 15:40:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.08332612365484238 norm:0.00027671956922858953 max memory_allocated 22561.39794921875 
[2025-03-04 15:41:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.08331578224897385 norm:0.000262468121945858 max memory_allocated 22561.39794921875 
[2025-03-04 15:42:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0833493173122406 norm:0.00026021242956630886 max memory_allocated 22561.39794921875 
[2025-03-04 15:42:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.08340852707624435 norm:0.0002690454130060971 max memory_allocated 22561.39794921875 
[2025-03-04 15:43:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.08337318897247314 norm:0.00026065538986586034 max memory_allocated 22561.39794921875 
[2025-03-04 15:43:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.08333022147417068 norm:0.00024147819203790277 max memory_allocated 22561.39794921875 
[2025-03-04 15:43:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-04 15:44:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.12113610655069351 norm:0.006315768696367741 max memory_allocated 22561.56982421875 
[2025-03-04 15:45:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.11012867838144302 norm:0.0033323285169899464 max memory_allocated 22561.56982421875 
[2025-03-04 15:45:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0979144498705864 norm:0.001260992605239153 max memory_allocated 22561.56982421875 
[2025-03-04 15:46:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.09344872087240219 norm:0.0006861971342004836 max memory_allocated 22561.56982421875 
[2025-03-04 15:46:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.09174005687236786 norm:0.0005136411637067795 max memory_allocated 22561.56982421875 
[2025-03-04 15:47:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.09079840034246445 norm:0.0004179137758910656 max memory_allocated 22561.56982421875 
[2025-03-04 15:47:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.09050340950489044 norm:0.0003894528781529516 max memory_allocated 22561.56982421875 
[2025-03-04 15:48:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.09036480635404587 norm:0.0003785813751164824 max memory_allocated 22561.56982421875 
[2025-03-04 15:48:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.09013499319553375 norm:0.0003063103067688644 max memory_allocated 22561.56982421875 
[2025-03-04 15:49:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.08995824307203293 norm:0.00026870565488934517 max memory_allocated 22561.56982421875 
[2025-03-04 15:50:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.08979249000549316 norm:0.00024075180408544838 max memory_allocated 22561.56982421875 
[2025-03-04 15:50:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.08973978459835052 norm:0.00024052674416452646 max memory_allocated 22561.56982421875 
[2025-03-04 15:51:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0897170752286911 norm:0.0002141895383829251 max memory_allocated 22561.56982421875 
[2025-03-04 15:51:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0896894559264183 norm:0.00021355951321311295 max memory_allocated 22561.56982421875 
[2025-03-04 15:52:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.08971741795539856 norm:0.00020958311506547034 max memory_allocated 22561.56982421875 
[2025-03-04 15:52:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.08969279378652573 norm:0.00020735383441206068 max memory_allocated 22561.56982421875 
[2025-03-04 15:53:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.08978810161352158 norm:0.0002138488198397681 max memory_allocated 22561.56982421875 
[2025-03-04 15:54:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.0896536186337471 norm:0.00019847671501338482 max memory_allocated 22561.56982421875 
[2025-03-04 15:54:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.08973857760429382 norm:0.00020178998238407075 max memory_allocated 22561.56982421875 
[2025-03-04 15:55:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.08978510648012161 norm:0.000205971286050044 max memory_allocated 22561.56982421875 
[2025-03-04 15:55:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-04 15:55:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.1467869132757187 norm:0.01257467083632946 max memory_allocated 22561.74169921875 
[2025-03-04 15:56:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.13003632426261902 norm:0.006562954746186733 max memory_allocated 22561.74169921875 
[2025-03-04 15:57:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.11217670142650604 norm:0.00261793308891356 max memory_allocated 22561.74169921875 
[2025-03-04 15:57:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.10471992194652557 norm:0.0011378985363990068 max memory_allocated 22561.74169921875 
[2025-03-04 15:58:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.10201374441385269 norm:0.0007716604741290212 max memory_allocated 22561.74169921875 
[2025-03-04 15:58:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.10074655711650848 norm:0.0006469275103881955 max memory_allocated 22561.74169921875 
[2025-03-04 15:59:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.09992506355047226 norm:0.0005283942446112633 max memory_allocated 22561.74169921875 
[2025-03-04 15:59:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.09960908442735672 norm:0.0005175234982743859 max memory_allocated 22561.74169921875 
[2025-03-04 16:00:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.0993553176522255 norm:0.0004682542057707906 max memory_allocated 22561.74169921875 
[2025-03-04 16:00:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.09908007830381393 norm:0.00043321825796738267 max memory_allocated 22561.74169921875 
[2025-03-04 16:01:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.09905566275119781 norm:0.0004202827112749219 max memory_allocated 22561.74169921875 
[2025-03-04 16:02:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.09896396100521088 norm:0.00038264848990365863 max memory_allocated 22561.74169921875 
[2025-03-04 16:02:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.09883474558591843 norm:0.0003744986897800118 max memory_allocated 22561.74169921875 
[2025-03-04 16:03:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.09884682297706604 norm:0.00034017005236819386 max memory_allocated 22561.74169921875 
[2025-03-04 16:03:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.09861385077238083 norm:0.0002975225797854364 max memory_allocated 22561.74169921875 
[2025-03-04 16:04:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.09860154241323471 norm:0.0002950905472971499 max memory_allocated 22561.74169921875 
[2025-03-04 16:04:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.09865045547485352 norm:0.00028983730589970946 max memory_allocated 22561.74169921875 
[2025-03-04 16:05:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.09859687089920044 norm:0.00027822054107673466 max memory_allocated 22561.74169921875 
[2025-03-04 16:06:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.09850376099348068 norm:0.00025940165505744517 max memory_allocated 22561.74169921875 
[2025-03-04 16:06:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.09850352257490158 norm:0.0002614272525534034 max memory_allocated 22561.74169921875 
[2025-03-04 16:06:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-04 16:07:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.15277045965194702 norm:0.010005072690546513 max memory_allocated 22561.91357421875 
[2025-03-04 16:07:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.13837043941020966 norm:0.005481255706399679 max memory_allocated 22561.91357421875 
[2025-03-04 16:08:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.12346737086772919 norm:0.0023359470069408417 max memory_allocated 22561.91357421875 
[2025-03-04 16:09:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.1166539341211319 norm:0.0009475825936533511 max memory_allocated 22561.91357421875 
[2025-03-04 16:09:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.11417853087186813 norm:0.0006487391656264663 max memory_allocated 22561.91357421875 
[2025-03-04 16:10:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.11306101083755493 norm:0.0005619662697426975 max memory_allocated 22561.91357421875 
[2025-03-04 16:10:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.1125752180814743 norm:0.0005168700008653104 max memory_allocated 22561.91357421875 
[2025-03-04 16:11:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.1122971922159195 norm:0.00046995520824566483 max memory_allocated 22561.91357421875 
[2025-03-04 16:11:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.11212273687124252 norm:0.0004130637680646032 max memory_allocated 22561.91357421875 
[2025-03-04 16:12:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.11177315562963486 norm:0.0003628614649642259 max memory_allocated 22561.91357421875 
[2025-03-04 16:12:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.11153492331504822 norm:0.0003256683121435344 max memory_allocated 22561.91357421875 
[2025-03-04 16:13:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.11138579994440079 norm:0.0002938852412626147 max memory_allocated 22561.91357421875 
[2025-03-04 16:14:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.1112721785902977 norm:0.0002849838638212532 max memory_allocated 22561.91357421875 
[2025-03-04 16:14:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.11126168817281723 norm:0.00028631239547394216 max memory_allocated 22561.91357421875 
[2025-03-04 16:15:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.11135821044445038 norm:0.0002766303368844092 max memory_allocated 22561.91357421875 
[2025-03-04 16:15:47 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.11137475818395615 norm:0.00027765773120336235 max memory_allocated 22561.91357421875 
[2025-03-04 16:16:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.1112893745303154 norm:0.0002629110240377486 max memory_allocated 22561.91357421875 
[2025-03-04 16:16:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.11118607223033905 norm:0.00024985274649225175 max memory_allocated 22561.91357421875 
[2025-03-04 16:17:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.11129479110240936 norm:0.0002482583513483405 max memory_allocated 22561.91357421875 
[2025-03-04 16:18:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.11122462153434753 norm:0.0002377882192377001 max memory_allocated 22561.91357421875 
[2025-03-04 16:18:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-04 16:18:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.17057715356349945 norm:0.010662661865353584 max memory_allocated 22562.08544921875 
[2025-03-04 16:19:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.15261521935462952 norm:0.0049545965157449245 max memory_allocated 22562.08544921875 
[2025-03-04 16:19:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.13721123337745667 norm:0.0018336049979552627 max memory_allocated 22562.08544921875 
[2025-03-04 16:20:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.1316598653793335 norm:0.0008688003290444613 max memory_allocated 22562.08544921875 
[2025-03-04 16:21:03 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.12952229380607605 norm:0.0006368470494635403 max memory_allocated 22562.08544921875 
[2025-03-04 16:21:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.12873223423957825 norm:0.0005433394107967615 max memory_allocated 22562.08544921875 
[2025-03-04 16:22:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.1283169537782669 norm:0.0004979661898687482 max memory_allocated 22562.08544921875 
[2025-03-04 16:22:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.1279878467321396 norm:0.00044436228927224874 max memory_allocated 22562.08544921875 
[2025-03-04 16:23:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.12782342731952667 norm:0.000424553087214008 max memory_allocated 22562.08544921875 
[2025-03-04 16:23:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.12762242555618286 norm:0.00038341726758517325 max memory_allocated 22562.08544921875 
[2025-03-04 16:24:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.1273709535598755 norm:0.00034197012428194284 max memory_allocated 22562.08544921875 
[2025-03-04 16:24:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.12729015946388245 norm:0.0003430452779866755 max memory_allocated 22562.08544921875 
[2025-03-04 16:25:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.12723281979560852 norm:0.00031934556318446994 max memory_allocated 22562.08544921875 
[2025-03-04 16:26:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.12711462378501892 norm:0.000296552840154618 max memory_allocated 22562.08544921875 
[2025-03-04 16:26:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.1270599365234375 norm:0.00028393304091878235 max memory_allocated 22562.08544921875 
[2025-03-04 16:27:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.12701360881328583 norm:0.00026912722387351096 max memory_allocated 22562.08544921875 
[2025-03-04 16:27:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.12694305181503296 norm:0.0002625844208523631 max memory_allocated 22562.08544921875 
[2025-03-04 16:28:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.1269083321094513 norm:0.0002629661757964641 max memory_allocated 22562.08544921875 
[2025-03-04 16:28:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.12694098055362701 norm:0.00025941093917936087 max memory_allocated 22562.08544921875 
[2025-03-04 16:29:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.1268843710422516 norm:0.00024666101671755314 max memory_allocated 22562.08544921875 
[2025-03-04 16:29:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-04 16:30:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.18587060272693634 norm:0.008986122906208038 max memory_allocated 22562.25732421875 
[2025-03-04 16:30:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.1718328446149826 norm:0.004781284369528294 max memory_allocated 22562.25732421875 
[2025-03-04 16:31:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.15685386955738068 norm:0.0019887315575033426 max memory_allocated 22562.25732421875 
[2025-03-04 16:31:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.15148931741714478 norm:0.0009628348634578288 max memory_allocated 22562.25732421875 
[2025-03-04 16:32:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.14960624277591705 norm:0.0007324905600398779 max memory_allocated 22562.25732421875 
[2025-03-04 16:33:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.14864932000637054 norm:0.0005682935588993132 max memory_allocated 22562.25732421875 
[2025-03-04 16:33:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.1480974555015564 norm:0.00046286347787827253 max memory_allocated 22562.25732421875 
[2025-03-04 16:34:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.14781759679317474 norm:0.00042550134821794927 max memory_allocated 22562.25732421875 
[2025-03-04 16:34:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.14769338071346283 norm:0.00040279983659274876 max memory_allocated 22562.25732421875 
[2025-03-04 16:35:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.14760585129261017 norm:0.00037578929914161563 max memory_allocated 22562.25732421875 
[2025-03-04 16:35:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.14748992025852203 norm:0.0003699381777551025 max memory_allocated 22562.25732421875 
[2025-03-04 16:36:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.14749562740325928 norm:0.00034048163797706366 max memory_allocated 22562.25732421875 
[2025-03-04 16:36:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.14724314212799072 norm:0.0003183119697496295 max memory_allocated 22562.25732421875 
[2025-03-04 16:37:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.14706213772296906 norm:0.00029017869383096695 max memory_allocated 22562.25732421875 
[2025-03-04 16:38:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.14704416692256927 norm:0.00029313721461221576 max memory_allocated 22562.25732421875 
[2025-03-04 16:38:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.14712797105312347 norm:0.00028587624547071755 max memory_allocated 22562.25732421875 
[2025-03-04 16:39:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.14703820645809174 norm:0.0002700521727092564 max memory_allocated 22562.25732421875 
[2025-03-04 16:39:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.14690928161144257 norm:0.00025577799533493817 max memory_allocated 22562.25732421875 
[2025-03-04 16:40:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.14690056443214417 norm:0.0002481987467035651 max memory_allocated 22562.25732421875 
[2025-03-04 16:40:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.14687833189964294 norm:0.0002381519298069179 max memory_allocated 22562.25732421875 
[2025-03-04 16:41:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-04 16:41:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.2264372855424881 norm:0.015181110240519047 max memory_allocated 22562.42919921875 
[2025-03-04 16:42:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.20863281190395355 norm:0.00833972916007042 max memory_allocated 22562.42919921875 
[2025-03-04 16:42:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.1883813589811325 norm:0.0032826378010213375 max memory_allocated 22562.42919921875 
[2025-03-04 16:43:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.18093222379684448 norm:0.0013695268426090479 max memory_allocated 22562.42919921875 
[2025-03-04 16:43:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.17842435836791992 norm:0.000948625965975225 max memory_allocated 22562.42919921875 
[2025-03-04 16:44:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.17733214795589447 norm:0.0008899690583348274 max memory_allocated 22562.42919921875 
[2025-03-04 16:45:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.17666678130626678 norm:0.0007287596818059683 max memory_allocated 22562.42919921875 
[2025-03-04 16:45:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.17615625262260437 norm:0.0006533098057843745 max memory_allocated 22562.42919921875 
[2025-03-04 16:46:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.17594020068645477 norm:0.0006508007645606995 max memory_allocated 22562.42919921875 
[2025-03-04 16:46:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.17570985853672028 norm:0.0005890119937248528 max memory_allocated 22562.42919921875 
[2025-03-04 16:47:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.17567087709903717 norm:0.0005919229006394744 max memory_allocated 22562.42919921875 
[2025-03-04 16:47:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.17545463144779205 norm:0.0005176175036467612 max memory_allocated 22562.42919921875 
[2025-03-04 16:48:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.17525359988212585 norm:0.0005331052234396338 max memory_allocated 22562.42919921875 
[2025-03-04 16:49:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.17515215277671814 norm:0.00046921646571718156 max memory_allocated 22562.42919921875 
[2025-03-04 16:49:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.1750391125679016 norm:0.00045975291868671775 max memory_allocated 22562.42919921875 
[2025-03-04 16:50:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.17491133511066437 norm:0.00041090024751611054 max memory_allocated 22562.42919921875 
[2025-03-04 16:50:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.17482402920722961 norm:0.00039396973443217576 max memory_allocated 22562.42919921875 
[2025-03-04 16:51:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.17463375627994537 norm:0.00036657723831012845 max memory_allocated 22562.42919921875 
[2025-03-04 16:51:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.17453083395957947 norm:0.000369697023415938 max memory_allocated 22562.42919921875 
[2025-03-04 16:52:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.1745014786720276 norm:0.00034713881905190647 max memory_allocated 22562.42919921875 
[2025-03-04 16:52:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-04 16:53:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.2570730447769165 norm:0.01178677473217249 max memory_allocated 22562.60107421875 
[2025-03-04 16:53:43 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.24069088697433472 norm:0.0063198646530508995 max memory_allocated 22562.60107421875 
[2025-03-04 16:54:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.21942588686943054 norm:0.0018227858236059546 max memory_allocated 22562.60107421875 
[2025-03-04 16:54:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.2126375436782837 norm:0.001129804295487702 max memory_allocated 22562.60107421875 
[2025-03-04 16:55:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.21049994230270386 norm:0.0008909933967515826 max memory_allocated 22562.60107421875 
[2025-03-04 16:55:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.20975692570209503 norm:0.0008812532178126276 max memory_allocated 22562.60107421875 
[2025-03-04 16:56:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.20926758646965027 norm:0.0008233435801230371 max memory_allocated 22562.60107421875 
[2025-03-04 16:57:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.20885278284549713 norm:0.0007675659726373851 max memory_allocated 22562.60107421875 
[2025-03-04 16:57:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.20861445367336273 norm:0.0007203810382634401 max memory_allocated 22562.60107421875 
[2025-03-04 16:58:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.20844456553459167 norm:0.0007010855479165912 max memory_allocated 22562.60107421875 
[2025-03-04 16:58:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.20830398797988892 norm:0.000729658524505794 max memory_allocated 22562.60107421875 
[2025-03-04 16:59:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.20842894911766052 norm:0.0007890512933954597 max memory_allocated 22562.60107421875 
[2025-03-04 16:59:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.2083844393491745 norm:0.0007996044005267322 max memory_allocated 22562.60107421875 
[2025-03-04 17:00:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.20834873616695404 norm:0.0006769135943613946 max memory_allocated 22562.60107421875 
[2025-03-04 17:01:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.20811934769153595 norm:0.0006216776673682034 max memory_allocated 22562.60107421875 
[2025-03-04 17:01:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.20806719362735748 norm:0.0005849581793881953 max memory_allocated 22562.60107421875 
[2025-03-04 17:02:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.2078864574432373 norm:0.0005463698180392385 max memory_allocated 22562.60107421875 
[2025-03-04 17:02:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.20772792398929596 norm:0.0005449058371596038 max memory_allocated 22562.60107421875 
[2025-03-04 17:03:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.20771607756614685 norm:0.0005156326224096119 max memory_allocated 22562.60107421875 
[2025-03-04 17:03:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.2076161652803421 norm:0.0005172988749109209 max memory_allocated 22562.60107421875 
[2025-03-04 17:03:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-04 17:04:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.2849068343639374 norm:0.008801023475825787 max memory_allocated 22562.77294921875 
[2025-03-04 17:05:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.269726425409317 norm:0.005448390729725361 max memory_allocated 22562.77294921875 
[2025-03-04 17:05:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.25194841623306274 norm:0.0019490642007440329 max memory_allocated 22562.77294921875 
[2025-03-04 17:06:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.24610763788223267 norm:0.0009231745498254895 max memory_allocated 22562.77294921875 
[2025-03-04 17:06:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.24432148039340973 norm:0.0007579774828627706 max memory_allocated 22562.77294921875 
[2025-03-04 17:07:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.2435777485370636 norm:0.0006598308100365102 max memory_allocated 22562.77294921875 
[2025-03-04 17:07:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.24305789172649384 norm:0.0005837467615492642 max memory_allocated 22562.77294921875 
[2025-03-04 17:08:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.24271854758262634 norm:0.0005447099101729691 max memory_allocated 22562.77294921875 
[2025-03-04 17:09:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.24240289628505707 norm:0.00048725979286246 max memory_allocated 22562.77294921875 
[2025-03-04 17:09:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.24208205938339233 norm:0.0004518642963375896 max memory_allocated 22562.77294921875 
[2025-03-04 17:10:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.24189266562461853 norm:0.00040878038271330297 max memory_allocated 22562.77294921875 
[2025-03-04 17:10:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.2418321818113327 norm:0.0003933168190997094 max memory_allocated 22562.77294921875 
[2025-03-04 17:11:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.24175886809825897 norm:0.000346505141351372 max memory_allocated 22562.77294921875 
[2025-03-04 17:11:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.2415086030960083 norm:0.0003338559763506055 max memory_allocated 22562.77294921875 
[2025-03-04 17:12:28 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.2414425164461136 norm:0.000322704145219177 max memory_allocated 22562.77294921875 
[2025-03-04 17:13:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.2414444237947464 norm:0.0003174300363752991 max memory_allocated 22562.77294921875 
[2025-03-04 17:13:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.24145609140396118 norm:0.0003037135466001928 max memory_allocated 22562.77294921875 
[2025-03-04 17:14:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.24155622720718384 norm:0.0003046803758479655 max memory_allocated 22562.77294921875 
[2025-03-04 17:14:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.24141672253608704 norm:0.0003005071193911135 max memory_allocated 22562.77294921875 
[2025-03-04 17:15:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.24129819869995117 norm:0.0002991226501762867 max memory_allocated 22562.77294921875 
[2025-03-04 17:15:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-04 17:16:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.3257097601890564 norm:0.006002604961395264 max memory_allocated 22562.94482421875 
[2025-03-04 17:16:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.31018367409706116 norm:0.0034313800279051065 max memory_allocated 22562.94482421875 
[2025-03-04 17:17:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.2932591140270233 norm:0.0016154635231941938 max memory_allocated 22562.94482421875 
[2025-03-04 17:17:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.2869410514831543 norm:0.0008078768150880933 max memory_allocated 22562.94482421875 
[2025-03-04 17:18:18 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.2849195897579193 norm:0.0006579175242222846 max memory_allocated 22562.94482421875 
[2025-03-04 17:18:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.28431645035743713 norm:0.0006153687718324363 max memory_allocated 22562.94482421875 
[2025-03-04 17:19:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.2839341163635254 norm:0.0005621236050501466 max memory_allocated 22562.94482421875 
[2025-03-04 17:19:59 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.2835971713066101 norm:0.0005115943495184183 max memory_allocated 22562.94482421875 
[2025-03-04 17:20:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.28328052163124084 norm:0.00046867720084264874 max memory_allocated 22562.94482421875 
[2025-03-04 17:21:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.28303098678588867 norm:0.00044018676271662116 max memory_allocated 22562.94482421875 
[2025-03-04 17:21:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.28281545639038086 norm:0.00041384948417544365 max memory_allocated 22562.94482421875 
[2025-03-04 17:22:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.2827122211456299 norm:0.00039292944711633027 max memory_allocated 22562.94482421875 
[2025-03-04 17:22:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.28256192803382874 norm:0.0003743689158000052 max memory_allocated 22562.94482421875 
[2025-03-04 17:23:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.2824411988258362 norm:0.0003617348556872457 max memory_allocated 22562.94482421875 
[2025-03-04 17:23:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.2823241055011749 norm:0.0003590292180888355 max memory_allocated 22562.94482421875 
[2025-03-04 17:24:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.2823554277420044 norm:0.0003549215034581721 max memory_allocated 22562.94482421875 
[2025-03-04 17:25:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.28234556317329407 norm:0.0003488248912617564 max memory_allocated 22562.94482421875 
[2025-03-04 17:25:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.2821967303752899 norm:0.00033515700488351285 max memory_allocated 22562.94482421875 
[2025-03-04 17:26:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.2822498679161072 norm:0.0003287537838332355 max memory_allocated 22562.94482421875 
[2025-03-04 17:26:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.2821793854236603 norm:0.00032583545544184744 max memory_allocated 22562.94482421875 
[2025-03-04 17:26:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-04 17:27:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.37015071511268616 norm:0.007082394324243069 max memory_allocated 22563.11669921875 
[2025-03-04 17:28:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.3545565903186798 norm:0.004555697552859783 max memory_allocated 22563.11669921875 
[2025-03-04 17:28:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.33644697070121765 norm:0.0025374898687005043 max memory_allocated 22563.11669921875 
[2025-03-04 17:29:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.3298753499984741 norm:0.0016467466484755278 max memory_allocated 22563.11669921875 
[2025-03-04 17:29:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.3277929425239563 norm:0.001326918601989746 max memory_allocated 22563.11669921875 
[2025-03-04 17:30:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.32685527205467224 norm:0.0011326743988320231 max memory_allocated 22563.11669921875 
[2025-03-04 17:30:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.3261016607284546 norm:0.000979333184659481 max memory_allocated 22563.11669921875 
[2025-03-04 17:31:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.32557258009910583 norm:0.0008607151685282588 max memory_allocated 22563.11669921875 
[2025-03-04 17:32:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.32523009181022644 norm:0.0007818307494744658 max memory_allocated 22563.11669921875 
[2025-03-04 17:32:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.3248620629310608 norm:0.0006869355565868318 max memory_allocated 22563.11669921875 
[2025-03-04 17:33:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.3244990408420563 norm:0.0006206037360243499 max memory_allocated 22563.11669921875 
[2025-03-04 17:33:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.32429754734039307 norm:0.0005666945944540203 max memory_allocated 22563.11669921875 
[2025-03-04 17:34:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.3241155743598938 norm:0.0005119048291817307 max memory_allocated 22563.11669921875 
[2025-03-04 17:34:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.32393932342529297 norm:0.00047501904191449285 max memory_allocated 22563.11669921875 
[2025-03-04 17:35:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.32385075092315674 norm:0.0004505717952270061 max memory_allocated 22563.11669921875 
[2025-03-04 17:35:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.32385683059692383 norm:0.00042291649151593447 max memory_allocated 22563.11669921875 
[2025-03-04 17:36:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.32376420497894287 norm:0.0004087012493982911 max memory_allocated 22563.11669921875 
[2025-03-04 17:37:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.3236996531486511 norm:0.00039040850242599845 max memory_allocated 22563.11669921875 
[2025-03-04 17:37:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.32355377078056335 norm:0.0003853640519082546 max memory_allocated 22563.11669921875 
[2025-03-04 17:38:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.32350030541419983 norm:0.00036935650859959424 max memory_allocated 22563.11669921875 
[2025-03-04 17:38:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-04 17:38:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.42002081871032715 norm:0.008405966684222221 max memory_allocated 22563.28857421875 
[2025-03-04 17:39:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.4040730893611908 norm:0.005061217118054628 max memory_allocated 22563.28857421875 
[2025-03-04 17:40:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.383312851190567 norm:0.0020650788210332394 max memory_allocated 22563.28857421875 
[2025-03-04 17:40:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.3757019340991974 norm:0.0008063503773882985 max memory_allocated 22563.28857421875 
[2025-03-04 17:41:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.3740658164024353 norm:0.0005981249269098043 max memory_allocated 22563.28857421875 
[2025-03-04 17:41:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.373247355222702 norm:0.0005128094926476479 max memory_allocated 22563.28857421875 
[2025-03-04 17:42:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.3725361227989197 norm:0.0004463749937713146 max memory_allocated 22563.28857421875 
[2025-03-04 17:42:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.3720068335533142 norm:0.0004177066730335355 max memory_allocated 22563.28857421875 
[2025-03-04 17:43:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.3717254102230072 norm:0.00041227389010600746 max memory_allocated 22563.28857421875 
[2025-03-04 17:44:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.3715144395828247 norm:0.0003811134083662182 max memory_allocated 22563.28857421875 
[2025-03-04 17:44:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.3714054226875305 norm:0.0003561257617548108 max memory_allocated 22563.28857421875 
[2025-03-04 17:45:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.3713070750236511 norm:0.000349621957866475 max memory_allocated 22563.28857421875 
[2025-03-04 17:45:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.3712650239467621 norm:0.000331346585880965 max memory_allocated 22563.28857421875 
[2025-03-04 17:46:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.3710993826389313 norm:0.00031041231704875827 max memory_allocated 22563.28857421875 
[2025-03-04 17:46:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.3710653483867645 norm:0.0002917645324487239 max memory_allocated 22563.28857421875 
[2025-03-04 17:47:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.370954692363739 norm:0.0002909820177592337 max memory_allocated 22563.28857421875 
[2025-03-04 17:47:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.370988667011261 norm:0.0002881770196836442 max memory_allocated 22563.28857421875 
[2025-03-04 17:48:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.3709438443183899 norm:0.0002829415607266128 max memory_allocated 22563.28857421875 
[2025-03-04 17:49:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.3708764612674713 norm:0.0002786926634144038 max memory_allocated 22563.28857421875 
[2025-03-04 17:49:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.3708750307559967 norm:0.0002862379769794643 max memory_allocated 22563.28857421875 
[2025-03-04 17:49:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-04 17:50:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.47450631856918335 norm:0.008578955195844173 max memory_allocated 22563.46044921875 
[2025-03-04 17:50:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.4568205773830414 norm:0.004987953696399927 max memory_allocated 22563.46044921875 
[2025-03-04 17:51:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.4355871081352234 norm:0.001890242681838572 max memory_allocated 22563.46044921875 
[2025-03-04 17:52:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.42785048484802246 norm:0.0008155835676006973 max memory_allocated 22563.46044921875 
[2025-03-04 17:52:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.4259209930896759 norm:0.000666958570946008 max memory_allocated 22563.46044921875 
[2025-03-04 17:53:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.42532169818878174 norm:0.0006038640276528895 max memory_allocated 22563.46044921875 
[2025-03-04 17:53:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.42480480670928955 norm:0.0005409900913946331 max memory_allocated 22563.46044921875 
[2025-03-04 17:54:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.4243699610233307 norm:0.00047968621947802603 max memory_allocated 22563.46044921875 
[2025-03-04 17:54:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.4239937365055084 norm:0.0004318194987718016 max memory_allocated 22563.46044921875 
[2025-03-04 17:55:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.4237533509731293 norm:0.0004132192407269031 max memory_allocated 22563.46044921875 
[2025-03-04 17:56:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.4235994517803192 norm:0.00039216605364345014 max memory_allocated 22563.46044921875 
[2025-03-04 17:56:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.42352211475372314 norm:0.0003717371728271246 max memory_allocated 22563.46044921875 
[2025-03-04 17:57:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.42342114448547363 norm:0.0003557163290679455 max memory_allocated 22563.46044921875 
[2025-03-04 17:57:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.42328494787216187 norm:0.0003520968894008547 max memory_allocated 22563.46044921875 
[2025-03-04 17:58:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.42314398288726807 norm:0.00033486634492874146 max memory_allocated 22563.46044921875 
[2025-03-04 17:58:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.42297282814979553 norm:0.0003225354594178498 max memory_allocated 22563.46044921875 
[2025-03-04 17:59:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.42294842004776 norm:0.0003091071266680956 max memory_allocated 22563.46044921875 
[2025-03-04 17:59:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.4228796064853668 norm:0.00030990096274763346 max memory_allocated 22563.46044921875 
[2025-03-04 18:00:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.4228137135505676 norm:0.0003002813318744302 max memory_allocated 22563.46044921875 
[2025-03-04 18:01:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.4228440523147583 norm:0.0002945767773780972 max memory_allocated 22563.46044921875 
[2025-03-04 18:01:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-04 18:01:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.5371180772781372 norm:0.010329953394830227 max memory_allocated 22563.63232421875 
[2025-03-04 18:02:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.5153400301933289 norm:0.005774018820375204 max memory_allocated 22563.63232421875 
[2025-03-04 18:02:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.49139806628227234 norm:0.0023362257052212954 max memory_allocated 22563.63232421875 
[2025-03-04 18:03:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.48221296072006226 norm:0.0008215864654630423 max memory_allocated 22563.63232421875 
[2025-03-04 18:04:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.4801797866821289 norm:0.000739223207347095 max memory_allocated 22563.63232421875 
[2025-03-04 18:04:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.4793384075164795 norm:0.000743484531994909 max memory_allocated 22563.63232421875 
[2025-03-04 18:05:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.47883668541908264 norm:0.0007026243256404996 max memory_allocated 22563.63232421875 
[2025-03-04 18:05:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.47844117879867554 norm:0.0006615107413381338 max memory_allocated 22563.63232421875 
[2025-03-04 18:06:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.4780845046043396 norm:0.0006096415454521775 max memory_allocated 22563.63232421875 
[2025-03-04 18:06:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.4777137339115143 norm:0.000573492783587426 max memory_allocated 22563.63232421875 
[2025-03-04 18:07:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.4775671660900116 norm:0.0005566648324020207 max memory_allocated 22563.63232421875 
[2025-03-04 18:08:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.4774255156517029 norm:0.0005034634377807379 max memory_allocated 22563.63232421875 
[2025-03-04 18:08:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.4773154556751251 norm:0.0004827911907341331 max memory_allocated 22563.63232421875 
[2025-03-04 18:09:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.4772000014781952 norm:0.00044379025348462164 max memory_allocated 22563.63232421875 
[2025-03-04 18:09:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.4771253764629364 norm:0.00042577655403874815 max memory_allocated 22563.63232421875 
[2025-03-04 18:10:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.4771934151649475 norm:0.0004488262929953635 max memory_allocated 22563.63232421875 
[2025-03-04 18:10:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.4770019054412842 norm:0.00042238630703650415 max memory_allocated 22563.63232421875 
[2025-03-04 18:11:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.4769873023033142 norm:0.000412701687309891 max memory_allocated 22563.63232421875 
[2025-03-04 18:11:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.4769884943962097 norm:0.00042074581142514944 max memory_allocated 22563.63232421875 
[2025-03-04 18:12:30 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.4769112765789032 norm:0.0004070148861501366 max memory_allocated 22563.63232421875 
[2025-03-04 18:12:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-04 18:12:43 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 18:13:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.6158406734466553 norm:0.026862334460020065 max memory_allocated 22563.91943359375 
[2025-03-04 18:13:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.5906121730804443 norm:0.021704765036702156 max memory_allocated 22563.91943359375 
[2025-03-04 18:14:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.5670759081840515 norm:0.013848412781953812 max memory_allocated 22563.91943359375 
[2025-03-04 18:14:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.5588427782058716 norm:0.01141185313463211 max memory_allocated 22563.91943359375 
[2025-03-04 18:15:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.5564701557159424 norm:0.010060220956802368 max memory_allocated 22563.91943359375 
[2025-03-04 18:16:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.5548893809318542 norm:0.00885736383497715 max memory_allocated 22563.91943359375 
[2025-03-04 18:16:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.5536192059516907 norm:0.007960984483361244 max memory_allocated 22563.91943359375 
[2025-03-04 18:17:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.5526933073997498 norm:0.007103737443685532 max memory_allocated 22563.91943359375 
[2025-03-04 18:17:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.5519625544548035 norm:0.006613488309085369 max memory_allocated 22563.91943359375 
[2025-03-04 18:18:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.5515597462654114 norm:0.006747560575604439 max memory_allocated 22563.91943359375 
[2025-03-04 18:18:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.5510134696960449 norm:0.006587838754057884 max memory_allocated 22563.91943359375 
[2025-03-04 18:19:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.5506971478462219 norm:0.006170055828988552 max memory_allocated 22563.91943359375 
[2025-03-04 18:20:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.5502086877822876 norm:0.006224780343472958 max memory_allocated 22563.91943359375 
[2025-03-04 18:20:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.549716591835022 norm:0.005684227216988802 max memory_allocated 22563.91943359375 
[2025-03-04 18:21:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.5493505001068115 norm:0.005577248055487871 max memory_allocated 22563.91943359375 
[2025-03-04 18:21:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.5490968227386475 norm:0.005195967853069305 max memory_allocated 22563.91943359375 
[2025-03-04 18:22:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.5489376783370972 norm:0.005299407988786697 max memory_allocated 22563.91943359375 
[2025-03-04 18:22:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.5489099025726318 norm:0.0053072767332196236 max memory_allocated 22563.91943359375 
[2025-03-04 18:23:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.5487719178199768 norm:0.0054120831191539764 max memory_allocated 22563.91943359375 
[2025-03-04 18:23:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.5486254096031189 norm:0.005180735606700182 max memory_allocated 22563.91943359375 
[2025-03-04 18:24:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-04 18:24:12 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 18:24:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.7049447298049927 norm:0.03246753290295601 max memory_allocated 22564.09130859375 
[2025-03-04 18:25:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.6780070662498474 norm:0.024042591452598572 max memory_allocated 22564.09130859375 
[2025-03-04 18:25:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.6502166986465454 norm:0.015964647755026817 max memory_allocated 22564.09130859375 
[2025-03-04 18:26:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.6402223110198975 norm:0.012356240302324295 max memory_allocated 22564.09130859375 
[2025-03-04 18:27:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.6374498605728149 norm:0.010596483945846558 max memory_allocated 22564.09130859375 
[2025-03-04 18:27:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.6360294222831726 norm:0.009141452610492706 max memory_allocated 22564.09130859375 
[2025-03-04 18:28:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.6351114511489868 norm:0.008019784465432167 max memory_allocated 22564.09130859375 
[2025-03-04 18:28:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.6343628168106079 norm:0.007195643149316311 max memory_allocated 22564.09130859375 
[2025-03-04 18:29:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.6338586807250977 norm:0.006494307424873114 max memory_allocated 22564.09130859375 
[2025-03-04 18:29:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.6334099769592285 norm:0.006108075380325317 max memory_allocated 22564.09130859375 
[2025-03-04 18:30:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.6332451701164246 norm:0.006106696091592312 max memory_allocated 22564.09130859375 
[2025-03-04 18:30:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.6329600214958191 norm:0.005854837596416473 max memory_allocated 22564.09130859375 
[2025-03-04 18:31:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.6328739523887634 norm:0.005792644806206226 max memory_allocated 22564.09130859375 
[2025-03-04 18:32:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.6325004696846008 norm:0.005631687585264444 max memory_allocated 22564.09130859375 
[2025-03-04 18:32:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.6322609782218933 norm:0.005241431761533022 max memory_allocated 22564.09130859375 
[2025-03-04 18:33:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.632202684879303 norm:0.00514675909653306 max memory_allocated 22564.09130859375 
[2025-03-04 18:33:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.6320829391479492 norm:0.005195041187107563 max memory_allocated 22564.09130859375 
[2025-03-04 18:34:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.6320102214813232 norm:0.005078388378024101 max memory_allocated 22564.09130859375 
[2025-03-04 18:34:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.6318486928939819 norm:0.004785774741321802 max memory_allocated 22564.09130859375 
[2025-03-04 18:35:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.6317558884620667 norm:0.004767191596329212 max memory_allocated 22564.09130859375 
[2025-03-04 18:35:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-04 18:35:41 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 18:36:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.9752591252326965 norm:0.03971373289823532 max memory_allocated 22564.26318359375 
[2025-03-04 18:36:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.8896628022193909 norm:0.021930372342467308 max memory_allocated 22564.26318359375 
[2025-03-04 18:37:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.8322765231132507 norm:0.023418143391609192 max memory_allocated 22564.26318359375 
[2025-03-04 18:37:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.8173495531082153 norm:0.024847518652677536 max memory_allocated 22564.26318359375 
[2025-03-04 18:38:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.812583327293396 norm:0.025066396221518517 max memory_allocated 22564.26318359375 
[2025-03-04 18:39:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.8085103631019592 norm:0.02645818702876568 max memory_allocated 22564.26318359375 
[2025-03-04 18:39:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.8055360317230225 norm:0.02397596836090088 max memory_allocated 22564.26318359375 
[2025-03-04 18:40:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.8054329752922058 norm:0.023955898359417915 max memory_allocated 22564.26318359375 
[2025-03-04 18:40:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.8023493885993958 norm:0.025315014645457268 max memory_allocated 22564.26318359375 
[2025-03-04 18:41:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.8007116317749023 norm:0.02437688782811165 max memory_allocated 22564.26318359375 
[2025-03-04 18:41:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.7993252277374268 norm:0.02426826022565365 max memory_allocated 22564.26318359375 
[2025-03-04 18:42:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.799115777015686 norm:0.024875173345208168 max memory_allocated 22564.26318359375 
[2025-03-04 18:43:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.7985534071922302 norm:0.02444477006793022 max memory_allocated 22564.26318359375 
[2025-03-04 18:43:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.7966950535774231 norm:0.024462755769491196 max memory_allocated 22564.26318359375 
[2025-03-04 18:44:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.7968069314956665 norm:0.023671820759773254 max memory_allocated 22564.26318359375 
[2025-03-04 18:44:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.7964239716529846 norm:0.023423034697771072 max memory_allocated 22564.26318359375 
[2025-03-04 18:45:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.7969621419906616 norm:0.023534249514341354 max memory_allocated 22564.26318359375 
[2025-03-04 18:45:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.7969769239425659 norm:0.024053743109107018 max memory_allocated 22564.26318359375 
[2025-03-04 18:46:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.7962819933891296 norm:0.02322399988770485 max memory_allocated 22564.26318359375 
[2025-03-04 18:46:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.7948023676872253 norm:0.022764738649129868 max memory_allocated 22564.26318359375 
[2025-03-04 18:47:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-04 18:47:10 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 18:47:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:1.6524629592895508 norm:0.1231764703989029 max memory_allocated 22564.43505859375 
[2025-03-04 18:48:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:1.5136387348175049 norm:0.09342401474714279 max memory_allocated 22564.43505859375 
[2025-03-04 18:48:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:1.4186303615570068 norm:0.07586407661437988 max memory_allocated 22564.43505859375 
[2025-03-04 18:49:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:1.3848142623901367 norm:0.07241148501634598 max memory_allocated 22564.43505859375 
[2025-03-04 18:49:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:1.3664686679840088 norm:0.07107655704021454 max memory_allocated 22564.43505859375 
[2025-03-04 18:50:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:1.3539830446243286 norm:0.06864166259765625 max memory_allocated 22564.43505859375 
[2025-03-04 18:51:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:1.342942237854004 norm:0.06263705343008041 max memory_allocated 22564.43505859375 
[2025-03-04 18:51:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:1.3372679948806763 norm:0.0633179247379303 max memory_allocated 22564.43505859375 
[2025-03-04 18:52:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:1.331751823425293 norm:0.059231627732515335 max memory_allocated 22564.43505859375 
[2025-03-04 18:52:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:1.3274405002593994 norm:0.05837186798453331 max memory_allocated 22564.43505859375 
[2025-03-04 18:53:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:1.3237416744232178 norm:0.062006425112485886 max memory_allocated 22564.43505859375 
[2025-03-04 18:53:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:1.3220006227493286 norm:0.061201006174087524 max memory_allocated 22564.43505859375 
[2025-03-04 18:54:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:1.3189313411712646 norm:0.06068792939186096 max memory_allocated 22564.43505859375 
[2025-03-04 18:55:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:1.3186298608779907 norm:0.05960688367486 max memory_allocated 22564.43505859375 
[2025-03-04 18:55:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:1.316971778869629 norm:0.05700978264212608 max memory_allocated 22564.43505859375 
[2025-03-04 18:56:10 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:1.3139963150024414 norm:0.054427072405815125 max memory_allocated 22564.43505859375 
[2025-03-04 18:56:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:1.3107982873916626 norm:0.05220333859324455 max memory_allocated 22564.43505859375 
[2025-03-04 18:57:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:1.310436487197876 norm:0.054220691323280334 max memory_allocated 22564.43505859375 
[2025-03-04 18:57:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:1.3098900318145752 norm:0.054533205926418304 max memory_allocated 22564.43505859375 
[2025-03-04 18:58:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:1.31158447265625 norm:0.05845768749713898 max memory_allocated 22564.43505859375 
[2025-03-04 18:58:35 root] (main_calibration.py 365): INFO 21991.387307167053
[2025-03-04 18:59:08 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-04 19:00:19 root] (main_calibration.py 158): INFO wikitext2 : 6.035562992095947
[2025-03-04 19:00:20 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-04 19:02:10 root] (main_calibration.py 158): INFO c4 : 7.587170600891113
[2025-03-04 20:35:18 root] (main_calibration.py 169): INFO {'wikitext2': 6.035562992095947, 'c4': 7.587170600891113, 'results': {'winogrande': {'acc': 0.6377269139700079, 'acc_stderr': 0.013508855476252512}, 'piqa': {'acc': 0.7698585418933623, 'acc_stderr': 0.009820832826839813, 'acc_norm': 0.7676822633297062, 'acc_norm_stderr': 0.009853201384168241}, 'arc_easy': {'acc': 0.6523569023569024, 'acc_stderr': 0.009771868846830909, 'acc_norm': 0.5105218855218855, 'acc_norm_stderr': 0.010257511546488225}, 'boolq': {'acc': 0.7064220183486238, 'acc_stderr': 0.007965011249420069}, 'arc_challenge': {'acc': 0.38054607508532423, 'acc_stderr': 0.014188277712349826, 'acc_norm': 0.40102389078498296, 'acc_norm_stderr': 0.014322255790719867}, 'hellaswag': {'acc': 0.54371639115714, 'acc_stderr': 0.0049706726515958526, 'acc_norm': 0.7047400916152161, 'acc_norm_stderr': 0.004552272447071699}}, 'versions': {'winogrande': 0, 'piqa': 0, 'arc_easy': 0, 'boolq': 1, 'arc_challenge': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
