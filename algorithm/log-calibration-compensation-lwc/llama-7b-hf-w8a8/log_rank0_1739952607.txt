[2025-02-19 08:10:07 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-7b-hf-w8a8', save_dir='./log-calibration-compensation-lwc/quant/llama-7b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-19 08:11:52 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-19 08:11:53 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-19 08:11:53 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-19 08:11:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-19 08:11:57 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:12:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0011997505789622664 norm:0.0020313006825745106 max memory_allocated 22559.10693359375 
[2025-02-19 08:12:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0007079349597916007 norm:0.0006860994035378098 max memory_allocated 22559.10693359375 
[2025-02-19 08:13:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0005871089524589479 norm:0.0008458266966044903 max memory_allocated 22559.10693359375 
[2025-02-19 08:14:00 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.000515988445840776 norm:0.0007409846875816584 max memory_allocated 22559.10693359375 
[2025-02-19 08:14:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.000472693849587813 norm:0.0006681098602712154 max memory_allocated 22559.10693359375 
[2025-02-19 08:15:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.00044728288776241243 norm:0.0006674613105133176 max memory_allocated 22559.10693359375 
[2025-02-19 08:15:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.00042765014222823083 norm:0.0006333822966553271 max memory_allocated 22559.10693359375 
[2025-02-19 08:16:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0004140684613958001 norm:0.0006140196928754449 max memory_allocated 22559.10693359375 
[2025-02-19 08:16:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0004036451573483646 norm:0.0005704505601897836 max memory_allocated 22559.10693359375 
[2025-02-19 08:17:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00039504977758042514 norm:0.0005462506669573486 max memory_allocated 22559.10693359375 
[2025-02-19 08:17:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0003896077396348119 norm:0.0005172521341592073 max memory_allocated 22559.10693359375 
[2025-02-19 08:18:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0003844655002467334 norm:0.00048263606731779873 max memory_allocated 22559.10693359375 
[2025-02-19 08:18:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0003816997050307691 norm:0.00046572243445552886 max memory_allocated 22559.10693359375 
[2025-02-19 08:19:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0003797464305534959 norm:0.00045517744729295373 max memory_allocated 22559.10693359375 
[2025-02-19 08:19:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.00037592873559333384 norm:0.00041768900700844824 max memory_allocated 22559.10693359375 
[2025-02-19 08:20:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0003769654722418636 norm:0.00041841197526082397 max memory_allocated 22559.10693359375 
[2025-02-19 08:20:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0003743368433788419 norm:0.000382655649445951 max memory_allocated 22559.10693359375 
[2025-02-19 08:21:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00037163530942052603 norm:0.00035812388523481786 max memory_allocated 22559.10693359375 
[2025-02-19 08:21:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0003695861669257283 norm:0.0003292390611022711 max memory_allocated 22559.10693359375 
[2025-02-19 08:22:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.00036864273715764284 norm:0.00032401809585280716 max memory_allocated 22559.10693359375 
[2025-02-19 08:22:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-19 08:22:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:22:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0015023290179669857 norm:0.0020305518992245197 max memory_allocated 22559.27880859375 
[2025-02-19 08:23:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0010189875029027462 norm:0.0005937905516475439 max memory_allocated 22559.27880859375 
[2025-02-19 08:24:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0008991481736302376 norm:0.000662396487314254 max memory_allocated 22559.27880859375 
[2025-02-19 08:24:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0008427149150520563 norm:0.0009063591132871807 max memory_allocated 22559.27880859375 
[2025-02-19 08:25:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0008027692092582583 norm:0.0008982869912870228 max memory_allocated 22559.27880859375 
[2025-02-19 08:25:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0007722382433712482 norm:0.0008687752415426075 max memory_allocated 22559.27880859375 
[2025-02-19 08:26:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0007550468435510993 norm:0.0008436075295321643 max memory_allocated 22559.27880859375 
[2025-02-19 08:26:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0007413293933495879 norm:0.0008078345563262701 max memory_allocated 22559.27880859375 
[2025-02-19 08:27:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0007223761640489101 norm:0.0007260691490955651 max memory_allocated 22559.27880859375 
[2025-02-19 08:27:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0007128602592274547 norm:0.000674425857141614 max memory_allocated 22559.27880859375 
[2025-02-19 08:28:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0007063395460136235 norm:0.0006484027253463864 max memory_allocated 22559.27880859375 
[2025-02-19 08:28:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0007064060773700476 norm:0.0006092602270655334 max memory_allocated 22559.27880859375 
[2025-02-19 08:29:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0007007151143625379 norm:0.0005891994806006551 max memory_allocated 22559.27880859375 
[2025-02-19 08:29:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0006952614057809114 norm:0.0005231975810602307 max memory_allocated 22559.27880859375 
[2025-02-19 08:30:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0006871936493553221 norm:0.00045820255763828754 max memory_allocated 22559.27880859375 
[2025-02-19 08:30:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0006850006175227463 norm:0.0004270529025234282 max memory_allocated 22559.27880859375 
[2025-02-19 08:31:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0006844615563750267 norm:0.00039018900133669376 max memory_allocated 22559.27880859375 
[2025-02-19 08:31:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0006850860663689673 norm:0.00036629638634622097 max memory_allocated 22559.27880859375 
[2025-02-19 08:32:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0006859131390228868 norm:0.00033518412965349853 max memory_allocated 22559.27880859375 
[2025-02-19 08:32:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0006842873408459127 norm:0.0003062669129576534 max memory_allocated 22559.27880859375 
[2025-02-19 08:32:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-19 08:32:53 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:33:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0018272335873916745 norm:0.0023915418423712254 max memory_allocated 22559.45068359375 
[2025-02-19 08:33:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0013367346255108714 norm:0.0012141827028244734 max memory_allocated 22559.45068359375 
[2025-02-19 08:34:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0011437548091635108 norm:0.0010372016113251448 max memory_allocated 22559.45068359375 
[2025-02-19 08:34:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0010553509928286076 norm:0.0009582536295056343 max memory_allocated 22559.45068359375 
[2025-02-19 08:35:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0009983476484194398 norm:0.0009757787338458002 max memory_allocated 22559.45068359375 
[2025-02-19 08:35:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0009584520594216883 norm:0.000884945213329047 max memory_allocated 22559.45068359375 
[2025-02-19 08:36:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0009406557073816657 norm:0.0009717422653920949 max memory_allocated 22559.45068359375 
[2025-02-19 08:36:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0009226796682924032 norm:0.0008708836394362152 max memory_allocated 22559.45068359375 
[2025-02-19 08:37:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0009064653422683477 norm:0.0007780262967571616 max memory_allocated 22559.45068359375 
[2025-02-19 08:38:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.000898156373295933 norm:0.0008426069980487227 max memory_allocated 22559.45068359375 
[2025-02-19 08:38:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0008969417540356517 norm:0.0007402598857879639 max memory_allocated 22559.45068359375 
[2025-02-19 08:39:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0008838932262733579 norm:0.0006841060821898282 max memory_allocated 22559.45068359375 
[2025-02-19 08:39:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0008788519771769643 norm:0.0006707906140945852 max memory_allocated 22559.45068359375 
[2025-02-19 08:40:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0008970042690634727 norm:0.0005598788266070187 max memory_allocated 22559.45068359375 
[2025-02-19 08:40:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0008902862900868058 norm:0.0006991964182816446 max memory_allocated 22559.45068359375 
[2025-02-19 08:41:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0008919168030843139 norm:0.0007725058239884675 max memory_allocated 22559.45068359375 
[2025-02-19 08:41:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0008677835576236248 norm:0.0006524451309815049 max memory_allocated 22559.45068359375 
[2025-02-19 08:42:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0008742006029933691 norm:0.0004606030124705285 max memory_allocated 22559.45068359375 
[2025-02-19 08:42:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0008858233923092484 norm:0.00041015195893123746 max memory_allocated 22559.45068359375 
[2025-02-19 08:43:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.000885936024133116 norm:0.0005435922648757696 max memory_allocated 22559.45068359375 
[2025-02-19 08:43:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-19 08:43:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0016665038419887424 norm:0.0005502198473550379 max memory_allocated 22559.50732421875 
[2025-02-19 08:44:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0012962330365553498 norm:0.00021643379295710474 max memory_allocated 22559.50732421875 
[2025-02-19 08:44:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0011640560114756227 norm:0.00013094815949443728 max memory_allocated 22559.50732421875 
[2025-02-19 08:45:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0010920511558651924 norm:0.00010952433513011783 max memory_allocated 22559.50732421875 
[2025-02-19 08:45:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.001046264311298728 norm:7.795271085342392e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:46:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0010171052999794483 norm:6.526501238113269e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:46:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0009987299563363194 norm:4.866100789513439e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:47:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0009868311462923884 norm:4.3670057493727654e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:47:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0009805745212361217 norm:3.402982838451862e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:48:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0009742010151967406 norm:3.0241910280892625e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:48:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0009656604379415512 norm:2.5072724383790046e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:49:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0009629817213863134 norm:3.066191857215017e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:49:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0009578119497746229 norm:2.3645621695322916e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:50:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0009526499779894948 norm:2.0221961676725186e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:50:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0009497194550931454 norm:1.9412853362155147e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:51:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0009488541400060058 norm:1.892583713924978e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:52:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0009473687969148159 norm:1.8563936464488506e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:52:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0009463878814131021 norm:2.1966492568026297e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:53:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0009464770555496216 norm:1.797987351892516e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:53:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0009456812986172736 norm:1.6594585758866742e-05 max memory_allocated 22559.50732421875 
[2025-02-19 08:53:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-19 08:54:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.001841560355387628 norm:0.0005699575413018465 max memory_allocated 22559.67919921875 
[2025-02-19 08:54:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0014726646477356553 norm:0.00019124610116705298 max memory_allocated 22559.67919921875 
[2025-02-19 08:55:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0013357275165617466 norm:0.00014329810801427811 max memory_allocated 22559.67919921875 
[2025-02-19 08:55:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0012644948437809944 norm:0.00011100962001364678 max memory_allocated 22559.67919921875 
[2025-02-19 08:56:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0012231399305164814 norm:8.580106077715755e-05 max memory_allocated 22559.67919921875 
[2025-02-19 08:56:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0012019367422908545 norm:7.378322334261611e-05 max memory_allocated 22559.67919921875 
[2025-02-19 08:57:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0011878173099830747 norm:6.461853627115488e-05 max memory_allocated 22559.67919921875 
[2025-02-19 08:57:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0011768655385822058 norm:5.3768180805491284e-05 max memory_allocated 22559.67919921875 
[2025-02-19 08:58:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.001169854775071144 norm:4.9127360398415476e-05 max memory_allocated 22559.67919921875 
[2025-02-19 08:58:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0011606122134253383 norm:4.220159098622389e-05 max memory_allocated 22559.67919921875 
[2025-02-19 08:59:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0011509476462379098 norm:3.651224687928334e-05 max memory_allocated 22559.67919921875 
[2025-02-19 08:59:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0011450037127360702 norm:3.4137552574975416e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:00:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.001142779365181923 norm:3.736341750482097e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:00:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0011396895861253142 norm:3.2893171010073274e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:01:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0011350124841555953 norm:3.2479023502673954e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:01:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0011335179442539811 norm:2.9129218091838993e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:02:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0011316394666209817 norm:2.7328514988766983e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:02:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0011284815846011043 norm:2.586501977930311e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:03:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0011272700503468513 norm:2.448508166708052e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:03:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0011271120747551322 norm:2.3448936190106906e-05 max memory_allocated 22559.67919921875 
[2025-02-19 09:04:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-19 09:04:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0018912733066827059 norm:0.00039796196506358683 max memory_allocated 22559.85107421875 
[2025-02-19 09:05:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0015603709034621716 norm:0.0001461492938688025 max memory_allocated 22559.85107421875 
[2025-02-19 09:05:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0014412032905966043 norm:0.00010406665387563407 max memory_allocated 22559.85107421875 
[2025-02-19 09:06:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0013785368064418435 norm:7.876538438722491e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:06:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0013400564203038812 norm:6.833214865764603e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:07:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0013176562497392297 norm:5.23530543432571e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:07:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.001302210963331163 norm:4.501464718487114e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:08:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0012927143834531307 norm:3.6590019590221345e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:08:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0012860361021012068 norm:3.4100616176147014e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:09:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0012809020699933171 norm:2.4048304112511687e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:09:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.00127546233125031 norm:2.7966583729721606e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:10:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0012696270132437348 norm:2.4565370040363632e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:10:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0012699814978986979 norm:2.5870556783047505e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:11:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0012673205928876996 norm:2.4099781512632035e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:11:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.00126365025062114 norm:2.1610372641589493e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:12:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0012617448810487986 norm:2.1044135792180896e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:12:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0012620831839740276 norm:2.3043974579195492e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:13:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0012595513835549355 norm:2.0674386178143322e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:13:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0012616883032023907 norm:2.2848751541459933e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:14:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0012600061018019915 norm:2.1057328922324814e-05 max memory_allocated 22559.85107421875 
[2025-02-19 09:14:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-19 09:14:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0022653702180832624 norm:0.0007620927644893527 max memory_allocated 22560.02294921875 
[2025-02-19 09:15:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0016767525812610984 norm:0.00019023758068215102 max memory_allocated 22560.02294921875 
[2025-02-19 09:16:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.0015709096333011985 norm:0.00012158362369518727 max memory_allocated 22560.02294921875 
[2025-02-19 09:16:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.001502191531471908 norm:0.00010137009667232633 max memory_allocated 22560.02294921875 
[2025-02-19 09:17:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0014652262907475233 norm:8.414480544161052e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:17:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0014399724313989282 norm:6.526816287077963e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:18:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0014246245846152306 norm:5.652189429383725e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:18:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.001413598656654358 norm:5.2268871513661e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:19:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0014055334031581879 norm:5.1657872973009944e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:19:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0013970582513138652 norm:3.60872654709965e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:20:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0013891879934817553 norm:3.6249861295800656e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:20:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0013843667693436146 norm:3.4767883335007355e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:21:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.0013795556733384728 norm:3.382946670171805e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:21:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0013762360904365778 norm:2.9198934498708695e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:22:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0013745285104960203 norm:2.706396480789408e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:22:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.001372583326883614 norm:2.7656333259074017e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:23:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0013699326664209366 norm:2.3585300368722528e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:23:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.001368713565170765 norm:2.2458711100625806e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:24:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0013668214669451118 norm:2.1111807654961012e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:24:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0013629199238494039 norm:2.1274925529723987e-05 max memory_allocated 22560.02294921875 
[2025-02-19 09:24:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-19 09:25:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.002210811013355851 norm:0.0005496835801750422 max memory_allocated 22560.19482421875 
[2025-02-19 09:25:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.001736293314024806 norm:0.00012123552733100951 max memory_allocated 22560.19482421875 
[2025-02-19 09:26:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.0016409022500738502 norm:8.945412264438346e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:26:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.001582192606292665 norm:7.185783761087805e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:27:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0015472500817850232 norm:5.722242713090964e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:27:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.001524924417026341 norm:5.299042095430195e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:28:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.001509938039816916 norm:4.830222314922139e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:28:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.001498873927630484 norm:4.8457604862051085e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:29:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.0014896729262545705 norm:3.928006117348559e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:29:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.001480209524743259 norm:3.651460065157153e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:30:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0014765921514481306 norm:3.5842567740473896e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:30:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.001474610180594027 norm:3.230503716622479e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:31:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0014711919939145446 norm:2.8521837521111593e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:31:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0014679275918751955 norm:2.874087294912897e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:32:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0014662055764347315 norm:2.7878415494342335e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:33:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0014652992831543088 norm:2.4842503989930265e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:33:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0014642226742580533 norm:1.916680957947392e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:34:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0014645634219050407 norm:1.9866643924615346e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:34:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0014637066051363945 norm:1.8347260265727527e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:35:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.001462357584387064 norm:1.7459344235248864e-05 max memory_allocated 22560.19482421875 
[2025-02-19 09:35:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-19 09:35:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.002078293589875102 norm:0.00040725289727561176 max memory_allocated 22560.36669921875 
[2025-02-19 09:36:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0017440200317651033 norm:7.508268026867881e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:36:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.0016871732659637928 norm:5.333545050234534e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:37:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0016401312313973904 norm:5.1391980377957225e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:37:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0016145624686032534 norm:4.5812550524715334e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:38:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0015986256767064333 norm:4.391450420371257e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:38:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0015860195271670818 norm:3.432023004279472e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:39:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0015775604406371713 norm:3.1916402804199606e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:39:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0015697061317041516 norm:3.3860465919133276e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:40:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0015662944642826915 norm:2.71169665211346e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:40:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0015625933883711696 norm:2.8109610866522416e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:41:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0015590447001159191 norm:2.22682865569368e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:41:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0015561982290819287 norm:2.4190134354284965e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:42:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0015539927408099174 norm:1.9293403966003098e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:42:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0015517867868766189 norm:2.0963781935279258e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:43:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0015509873628616333 norm:1.7484957425040193e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:43:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0015496636042371392 norm:1.5992502085282467e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:44:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.001550550339743495 norm:1.4663411093351897e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:44:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0015495026018470526 norm:1.6692674762452953e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:45:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.001549030072055757 norm:1.4562339856638573e-05 max memory_allocated 22560.36669921875 
[2025-02-19 09:45:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-19 09:46:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002351985080167651 norm:0.00046689886949025095 max memory_allocated 22560.53857421875 
[2025-02-19 09:46:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.00188408640678972 norm:7.798348815413192e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:47:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0018335768254473805 norm:5.737533501815051e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:47:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0018001775024458766 norm:4.922065636492334e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:48:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0017709460807964206 norm:4.54467699455563e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:48:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0017521900590509176 norm:3.8524736737599596e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:49:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0017404018435627222 norm:3.787238529184833e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:49:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.001730997348204255 norm:3.851262226817198e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:50:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0017229432705789804 norm:3.63240105798468e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:50:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0017155110836029053 norm:2.7850031983689405e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:51:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0017111992929130793 norm:3.0287441404652782e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:51:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0017061925027519464 norm:2.7838566893478855e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:52:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.001703666290268302 norm:2.38819993683137e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:52:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0017017279751598835 norm:2.333666998310946e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:53:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.001699688844382763 norm:2.180640512960963e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:53:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0016978837084025145 norm:2.2435358914663084e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:54:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0016976117622107267 norm:2.1595973521471024e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:54:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.001696133054792881 norm:2.0196075638523325e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:55:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0016952890437096357 norm:1.975103259610478e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:55:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0016953469021245837 norm:1.8367138181929477e-05 max memory_allocated 22560.53857421875 
[2025-02-19 09:55:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-19 09:56:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.002256522187963128 norm:0.00028748236945830286 max memory_allocated 22560.71044921875 
[2025-02-19 09:57:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.001964278519153595 norm:6.509873492177576e-05 max memory_allocated 22560.71044921875 
[2025-02-19 09:57:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0019159925868734717 norm:4.295537655707449e-05 max memory_allocated 22560.71044921875 
[2025-02-19 09:58:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0018775193020701408 norm:3.472304524620995e-05 max memory_allocated 22560.71044921875 
[2025-02-19 09:58:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0018542532343417406 norm:3.361704875715077e-05 max memory_allocated 22560.71044921875 
[2025-02-19 09:59:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0018385000294074416 norm:3.2337633456336334e-05 max memory_allocated 22560.71044921875 
[2025-02-19 09:59:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0018274507019668818 norm:2.8369722713250667e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:00:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.001820682780817151 norm:2.2732681827619672e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:00:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0018143780762329698 norm:2.2086192984716035e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:01:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.001809730427339673 norm:2.0702826077467762e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:01:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0018058386631309986 norm:2.1252266378724016e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:02:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0018022945150732994 norm:1.7068086890503764e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:02:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0018008179031312466 norm:1.5071740563143976e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:03:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0017994598019868135 norm:1.5540921594947577e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:03:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0017981892451643944 norm:1.767394132912159e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:04:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0017957795644178987 norm:1.4025290511199273e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:04:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0017941944533959031 norm:1.3532482626033016e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:05:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.001793967792764306 norm:1.1158131201227661e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:05:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0017940367106348276 norm:1.230094949278282e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:06:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.001793047646060586 norm:1.260613771592034e-05 max memory_allocated 22560.71044921875 
[2025-02-19 10:06:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-19 10:06:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0028557577170431614 norm:0.0006406353786587715 max memory_allocated 22560.88232421875 
[2025-02-19 10:07:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.002120593097060919 norm:0.00010876034502871335 max memory_allocated 22560.88232421875 
[2025-02-19 10:07:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0020379459019750357 norm:5.414768747868948e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:08:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0020016683265566826 norm:4.521334631135687e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:08:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.001971848076209426 norm:4.0395672840531915e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:09:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.0019523447845131159 norm:3.633693268056959e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:09:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0019339097198098898 norm:3.493235999485478e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:10:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0019246625015512109 norm:3.322124757687561e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:10:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0019166565034538507 norm:2.891911026381422e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:11:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0019111816072836518 norm:2.447906263114419e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:11:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0019062753999605775 norm:2.6158919354202226e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:12:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0019017868908122182 norm:2.3245664124260657e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:12:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0018982227193191648 norm:2.14820265682647e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:13:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0018963288748636842 norm:2.0304505596868694e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:14:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0018942758906632662 norm:2.0432245946722105e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:14:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0018923644674941897 norm:1.9433802663115785e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:15:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0018915042746812105 norm:1.675834209891036e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:15:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0018906567711383104 norm:1.7202335584443063e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:16:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0018902465235441923 norm:1.4754456060472876e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:16:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.0018886905163526535 norm:1.5144480130402371e-05 max memory_allocated 22560.88232421875 
[2025-02-19 10:16:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-19 10:17:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0025741145946085453 norm:0.0003036649723071605 max memory_allocated 22561.05419921875 
[2025-02-19 10:17:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.00221148575656116 norm:8.293713472085074e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:18:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002150719752535224 norm:5.092101127956994e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:18:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.002112861955538392 norm:4.108174834982492e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:19:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0020834545139223337 norm:3.546135121723637e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:19:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.0020652995444834232 norm:3.0258859624154866e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:20:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.002052714116871357 norm:2.8123282390879467e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:20:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0020420029759407043 norm:2.540289278840646e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:21:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0020359440241008997 norm:2.1423587895696983e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:21:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.002030141418799758 norm:2.0724994101328775e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:22:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0020256079733371735 norm:1.859239455370698e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:22:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0020228689536452293 norm:1.701021392364055e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:23:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.002019690815359354 norm:1.6947149561019614e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:23:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0020161608699709177 norm:1.5481979062315077e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:24:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002012585988268256 norm:1.4561365787812974e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:24:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.002011731965467334 norm:1.3564679647970479e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:25:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.002010574098676443 norm:1.2749511370202526e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:25:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.002009039046242833 norm:1.158990380645264e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:26:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.002007887000218034 norm:1.2044959476043005e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:26:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.002006935654208064 norm:1.2148175301263109e-05 max memory_allocated 22561.05419921875 
[2025-02-19 10:27:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-19 10:27:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.0026274402625858784 norm:0.00026149037876166403 max memory_allocated 22561.22607421875 
[2025-02-19 10:28:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0023403377272188663 norm:6.999907782301307e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:28:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.0022876085713505745 norm:4.532311868388206e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:29:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0022454073186963797 norm:3.856775219901465e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:29:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.002217640867456794 norm:3.434285827097483e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:30:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.002200207207351923 norm:3.1633349863113835e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:30:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.0021868476178497076 norm:2.6623076337273233e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:31:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0021764948032796383 norm:2.1878335246583447e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:31:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.0021694665774703026 norm:2.2972784790908918e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:32:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.0021638842299580574 norm:2.146238330169581e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:32:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0021600143518298864 norm:1.8001099306275137e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:33:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0021562317851930857 norm:1.8342982002650388e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:33:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.002152584493160248 norm:1.6036414308473468e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:34:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.002151050604879856 norm:1.447078011551639e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:34:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0021489057689905167 norm:1.4086734154261649e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:35:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.0021481222938746214 norm:1.3160120033717249e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:35:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0021472075022757053 norm:1.3166187272872776e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:36:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0021461863070726395 norm:1.2739824342133943e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:36:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.002145130420103669 norm:1.1328032087476458e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:37:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.002144373022019863 norm:1.111117671825923e-05 max memory_allocated 22561.22607421875 
[2025-02-19 10:37:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-19 10:38:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.002966886619105935 norm:0.00035702556488104165 max memory_allocated 22561.39794921875 
[2025-02-19 10:38:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0025840732268989086 norm:0.00010257305984850973 max memory_allocated 22561.39794921875 
[2025-02-19 10:39:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0025121013168245554 norm:6.173834117362276e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:39:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0024624462239444256 norm:5.1525639719329774e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:40:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.0024288338609039783 norm:4.43150638602674e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:40:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.0024046520702540874 norm:4.062405423610471e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:41:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.002387403743341565 norm:3.384809315321036e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:41:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.002375861397013068 norm:2.9982551495777443e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:42:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0023656676057726145 norm:2.6377407266409136e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:42:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0023569862823933363 norm:2.4147213480318896e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:43:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0023508325684815645 norm:2.2403390175895765e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:43:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0023462509270757437 norm:1.8995820937561803e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:44:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.0023422157391905785 norm:1.6326539480360225e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:44:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0023389256093651056 norm:1.5817218809388578e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:45:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.002335870638489723 norm:1.4971462405810598e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:45:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.002332960953935981 norm:1.4027795259607956e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:46:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.00233030179515481 norm:1.3805200069327839e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:46:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.002329228911548853 norm:1.2676377991738264e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:47:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0023271720856428146 norm:1.2988532944291364e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:47:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.002325948793441057 norm:1.2167482054792345e-05 max memory_allocated 22561.39794921875 
[2025-02-19 10:47:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-19 10:48:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.0029833505395799875 norm:0.00023907645663712174 max memory_allocated 22561.56982421875 
[2025-02-19 10:48:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.002733465749770403 norm:6.613336881855503e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:49:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0026788210961967707 norm:3.952693805331364e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:49:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0026384589727967978 norm:3.2757670851424336e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:50:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0026083397679030895 norm:2.9453420211211778e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:51:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.002588983392342925 norm:2.7755566406995058e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:51:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0025773141533136368 norm:2.4230019334936514e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:52:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0025693329516798258 norm:2.2424796043196693e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:52:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0025632644537836313 norm:1.9768089259741828e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:53:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0025578876957297325 norm:1.6644402421661653e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:53:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0025543058291077614 norm:1.5877018086030148e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:54:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.002550504868850112 norm:1.3685826161236037e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:54:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0025471432600170374 norm:1.3291008144733496e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:55:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0025440561585128307 norm:1.1849219845316838e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:55:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.002540815155953169 norm:1.1653982255666051e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:56:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0025393834803253412 norm:1.147358398156939e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:56:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.002536855870857835 norm:1.042056373989908e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:57:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.0025364449247717857 norm:1.0576926797511987e-05 max memory_allocated 22561.56982421875 
[2025-02-19 10:57:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0025361673906445503 norm:9.993018466047943e-06 max memory_allocated 22561.56982421875 
[2025-02-19 10:58:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.0025337794795632362 norm:9.645515092415735e-06 max memory_allocated 22561.56982421875 
[2025-02-19 10:58:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-19 10:58:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.004214804619550705 norm:0.0007897104369476438 max memory_allocated 22561.74169921875 
[2025-02-19 10:59:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.0032799330074340105 norm:0.0001699478307273239 max memory_allocated 22561.74169921875 
[2025-02-19 10:59:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0031383796595036983 norm:8.975745004136115e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:00:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.0030852032359689474 norm:7.100115180946887e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:00:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0030323306564241648 norm:5.8085377531824633e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:01:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.00299055315554142 norm:4.930197974317707e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:01:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.002961430698633194 norm:4.442574208951555e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:02:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0029405311215668917 norm:4.2693827708717436e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:02:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.002924976870417595 norm:3.9574082620674744e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:03:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.0029124715365469456 norm:3.598909097490832e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:03:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.0029009238351136446 norm:3.311543332529254e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:04:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.002892380114644766 norm:3.0974711989983916e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:04:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0028843374457210302 norm:2.8357779228826985e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:05:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.002878701314330101 norm:2.473084896337241e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:05:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.0028731042984873056 norm:2.3084703570930287e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:06:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.0028684462886303663 norm:2.20892525248928e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:07:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.0028646322898566723 norm:2.147266968677286e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:07:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.0028613589238375425 norm:2.109465276589617e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:08:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.00285730161704123 norm:1.7507283700979315e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:08:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.0028550659772008657 norm:1.8475364413461648e-05 max memory_allocated 22561.74169921875 
[2025-02-19 11:08:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-19 11:09:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.004714450798928738 norm:0.0008565057651139796 max memory_allocated 22561.91357421875 
[2025-02-19 11:09:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0037245547864586115 norm:0.0001845612860051915 max memory_allocated 22561.91357421875 
[2025-02-19 11:10:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.003577130613848567 norm:9.71840345300734e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:10:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.003520050086081028 norm:7.967129204189405e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:11:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0034797152038663626 norm:6.722923717461526e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:11:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.0034322065766900778 norm:5.774071178166196e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:12:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.003401048481464386 norm:5.339193012332544e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:12:47 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0033836006186902523 norm:4.8246965889120474e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:13:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0033651168923825026 norm:4.5951612264616415e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:13:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.003350270912051201 norm:4.2711941205197945e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:14:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.003339525545015931 norm:4.111734961043112e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:14:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.003330760868266225 norm:3.910695522790775e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:15:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.0033227514941245317 norm:3.596226088120602e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:15:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.003315105801448226 norm:3.068995647481643e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:16:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.0033081676810979843 norm:3.000734250235837e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:16:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.0033045951277017593 norm:3.0026330932741985e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:17:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.003298895200714469 norm:2.57322517427383e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:17:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0032951771281659603 norm:2.6340047043049708e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:18:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.0032909968867897987 norm:2.4044173187576234e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:18:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0032881831284612417 norm:2.2394380721380003e-05 max memory_allocated 22561.91357421875 
[2025-02-19 11:19:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-19 11:19:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.004877825267612934 norm:0.0005559538840316236 max memory_allocated 22562.08544921875 
[2025-02-19 11:20:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0042345281690359116 norm:0.00014839143841527402 max memory_allocated 22562.08544921875 
[2025-02-19 11:20:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.004115639254450798 norm:9.36046926653944e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:21:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.004042385146021843 norm:7.334119436563924e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:21:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.003977987449616194 norm:5.9645710280165076e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:22:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.0039346665143966675 norm:5.354118184186518e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:22:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.0039037566166371107 norm:4.8143654566956684e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:23:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.0038791317492723465 norm:4.41977936134208e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:23:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.0038606529124081135 norm:3.866688348352909e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:24:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.0038467543199658394 norm:3.3887346944538876e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:24:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.0038346198853105307 norm:3.0626011721324176e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:25:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.003822583705186844 norm:2.811415288306307e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:25:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.003814925439655781 norm:2.5907436793204397e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:26:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.0038081419188529253 norm:2.3057076759869233e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:26:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.003802054561674595 norm:2.0870902517344803e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:27:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.0037960519548505545 norm:1.8758099031401798e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:27:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.0037909322418272495 norm:1.7841655790107325e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:28:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.0037868344224989414 norm:1.684233393461909e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:28:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0037841664161533117 norm:1.6312074876623228e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:29:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.0037804837338626385 norm:1.6144455003086478e-05 max memory_allocated 22562.08544921875 
[2025-02-19 11:29:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-19 11:29:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.0062780678272247314 norm:0.0010891868732869625 max memory_allocated 22562.25732421875 
[2025-02-19 11:30:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.005012552719563246 norm:0.0002577764680609107 max memory_allocated 22562.25732421875 
[2025-02-19 11:31:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.0047777676954865456 norm:0.00012938972213305533 max memory_allocated 22562.25732421875 
[2025-02-19 11:31:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.004700714256614447 norm:0.00010442130587762222 max memory_allocated 22562.25732421875 
[2025-02-19 11:32:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.004645182751119137 norm:8.7006512330845e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:32:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.004602927248924971 norm:7.21447795513086e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:33:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.004560851491987705 norm:6.314836355159059e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:33:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.004531263839453459 norm:5.5334105127258226e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:34:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.004509642720222473 norm:5.2081726607866585e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:34:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.004494133871048689 norm:5.082936695544049e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:35:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.0044813393615186214 norm:4.4911339500686154e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:35:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.004469639156013727 norm:3.8682330341544e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:36:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.00445894617587328 norm:3.812191789620556e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:36:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.0044502755627036095 norm:3.696133353514597e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:37:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.00444386899471283 norm:3.3761509257601574e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:37:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.004437092691659927 norm:3.0384404453798197e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:38:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.004430895205587149 norm:2.764699820545502e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:38:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.004425993654876947 norm:2.4634426154079847e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:39:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.004421057645231485 norm:2.053678508673329e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:39:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.004417655989527702 norm:1.9109802451566793e-05 max memory_allocated 22562.25732421875 
[2025-02-19 11:39:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-19 11:40:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.009376075118780136 norm:0.0029584108851850033 max memory_allocated 22562.42919921875 
[2025-02-19 11:40:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.006399855017662048 norm:0.0006548208766616881 max memory_allocated 22562.42919921875 
[2025-02-19 11:41:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.005822576582431793 norm:0.00023925837012939155 max memory_allocated 22562.42919921875 
[2025-02-19 11:41:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.005680670961737633 norm:0.00016944507660809904 max memory_allocated 22562.42919921875 
[2025-02-19 11:42:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.005607060622423887 norm:0.00014273979468271136 max memory_allocated 22562.42919921875 
[2025-02-19 11:42:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0055531724356114864 norm:0.0001291769149247557 max memory_allocated 22562.42919921875 
[2025-02-19 11:43:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.005506368353962898 norm:0.00011359478230588138 max memory_allocated 22562.42919921875 
[2025-02-19 11:43:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.005461948458105326 norm:9.207032417180017e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:44:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.005422829184681177 norm:8.171682566171512e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:44:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.0053946091793477535 norm:7.490741700166836e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:45:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.005372434854507446 norm:7.213352364487946e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:45:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.005354324355721474 norm:6.83313119225204e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:46:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.005339611787348986 norm:6.326908624032512e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:46:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.0053278799168765545 norm:5.96188401686959e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:47:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.005317552946507931 norm:5.4356063628802076e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:48:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.005307258106768131 norm:5.152331868885085e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:48:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.005299585405737162 norm:4.967051791027188e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:49:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.005292399320751429 norm:4.874958176515065e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:49:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.005285727325826883 norm:4.41435186075978e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:50:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.005280039738863707 norm:4.242401337251067e-05 max memory_allocated 22562.42919921875 
[2025-02-19 11:50:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-19 11:50:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.008754570037126541 norm:0.0018329565646126866 max memory_allocated 22562.60107421875 
[2025-02-19 11:51:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.0069727967493236065 norm:0.00033838345552794635 max memory_allocated 22562.60107421875 
[2025-02-19 11:51:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.00668033491820097 norm:0.00016064208466559649 max memory_allocated 22562.60107421875 
[2025-02-19 11:52:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.0066012246534228325 norm:0.0001397747255396098 max memory_allocated 22562.60107421875 
[2025-02-19 11:52:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.0065513188019394875 norm:0.00013000323087908328 max memory_allocated 22562.60107421875 
[2025-02-19 11:53:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.006515320390462875 norm:0.00011887693108292297 max memory_allocated 22562.60107421875 
[2025-02-19 11:53:47 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.0064774928614497185 norm:0.00010192455374635756 max memory_allocated 22562.60107421875 
[2025-02-19 11:54:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.006440394092351198 norm:8.727944077691063e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:54:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.006411802023649216 norm:7.642187847523019e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:55:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.00639312295243144 norm:6.972548726480454e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:55:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.006376772653311491 norm:6.30389986326918e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:56:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.006364647764712572 norm:5.910304025746882e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:56:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.006350969895720482 norm:5.506752495421097e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:57:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.00634147971868515 norm:5.05883181176614e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:57:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.006334776058793068 norm:4.5746528485324234e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:58:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.006329115014523268 norm:4.3024654587497935e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:58:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.006321602966636419 norm:4.0810107748256996e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:59:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.006317071616649628 norm:3.8342208426911384e-05 max memory_allocated 22562.60107421875 
[2025-02-19 11:59:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.006310691125690937 norm:3.43744941346813e-05 max memory_allocated 22562.60107421875 
[2025-02-19 12:00:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.00630809273570776 norm:3.286872015451081e-05 max memory_allocated 22562.60107421875 
[2025-02-19 12:00:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-19 12:01:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.009528307244181633 norm:0.0013763359747827053 max memory_allocated 22562.77294921875 
[2025-02-19 12:01:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.008060110732913017 norm:0.0003201857616659254 max memory_allocated 22562.77294921875 
[2025-02-19 12:02:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.007767980918288231 norm:0.00015610114496666938 max memory_allocated 22562.77294921875 
[2025-02-19 12:02:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.007678881287574768 norm:0.00012661493383347988 max memory_allocated 22562.77294921875 
[2025-02-19 12:03:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.007624836638569832 norm:0.00010988111898768693 max memory_allocated 22562.77294921875 
[2025-02-19 12:03:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.007571869995445013 norm:0.00010031031706603244 max memory_allocated 22562.77294921875 
[2025-02-19 12:04:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.00752977654337883 norm:8.793507004156709e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:04:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.00749534647911787 norm:7.207216549431905e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:05:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.007465514820069075 norm:6.414185918401927e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:05:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.007445977069437504 norm:5.9874688304262236e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:06:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.007429212797433138 norm:5.7587280025472865e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:06:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.007414191495627165 norm:5.2411814976949245e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:07:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.007399400230497122 norm:4.9333546485286206e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:07:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.007387286983430386 norm:4.557723514153622e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:08:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.007376283872872591 norm:4.2449402826605365e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:08:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.007367108482867479 norm:3.7422381865326315e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:09:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.007360310293734074 norm:3.367995304870419e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:09:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.007353837601840496 norm:3.248322900617495e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:10:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.007348694838583469 norm:3.117246524197981e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:10:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.00734094250947237 norm:3.17265403282363e-05 max memory_allocated 22562.77294921875 
[2025-02-19 12:10:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-19 12:11:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.010219804011285305 norm:0.0006760168471373618 max memory_allocated 22562.94482421875 
[2025-02-19 12:12:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.009250904433429241 norm:0.00022228280431590974 max memory_allocated 22562.94482421875 
[2025-02-19 12:12:30 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.009010132402181625 norm:0.0001691248908173293 max memory_allocated 22562.94482421875 
[2025-02-19 12:13:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.008921517059206963 norm:0.00016611002502031624 max memory_allocated 22562.94482421875 
[2025-02-19 12:13:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.008846167474985123 norm:0.0001296736008953303 max memory_allocated 22562.94482421875 
[2025-02-19 12:14:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.0087716830894351 norm:9.126006625592709e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:14:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.008727732114493847 norm:7.333598477998748e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:15:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.008696531876921654 norm:6.222638330655172e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:15:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.008670717477798462 norm:5.242054976406507e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:16:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.008649125695228577 norm:4.8770452849566936e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:16:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.008632170036435127 norm:4.549224831862375e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:17:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.008623107336461544 norm:4.2738683987408876e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:17:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.008613098412752151 norm:4.38296192442067e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:18:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.00859888643026352 norm:3.9813428884372115e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:18:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.008591137826442719 norm:3.713769547175616e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:19:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.00858203787356615 norm:3.542841295711696e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:19:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.008574489504098892 norm:3.4031727409455925e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:20:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.00856900867074728 norm:2.9082561013638042e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:20:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.008564095944166183 norm:2.7730309739126824e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:21:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.008559845387935638 norm:2.594329089333769e-05 max memory_allocated 22562.94482421875 
[2025-02-19 12:21:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-19 12:21:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.011721232905983925 norm:0.0007616898510605097 max memory_allocated 22563.11669921875 
[2025-02-19 12:22:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.010760064236819744 norm:0.00029786370578221977 max memory_allocated 22563.11669921875 
[2025-02-19 12:22:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.010492016561329365 norm:0.0002141794393537566 max memory_allocated 22563.11669921875 
[2025-02-19 12:23:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.010359085164964199 norm:0.00018061071750707924 max memory_allocated 22563.11669921875 
[2025-02-19 12:23:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.010241619311273098 norm:0.0001436650491086766 max memory_allocated 22563.11669921875 
[2025-02-19 12:24:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.010158972814679146 norm:0.00011295123840682209 max memory_allocated 22563.11669921875 
[2025-02-19 12:24:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.010095823556184769 norm:9.400609269505367e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:25:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.010049362666904926 norm:7.892759458627552e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:25:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.010012278333306313 norm:6.865345494588837e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:26:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.009983866475522518 norm:6.273556937230751e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:26:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.009960561990737915 norm:5.475893340189941e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:27:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.009939558804035187 norm:4.898412953480147e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:27:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.009922402910888195 norm:4.4713244278682396e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:28:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.009909204207360744 norm:4.299162173992954e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:29:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.009896674193441868 norm:3.9955430111149326e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:29:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.009884454309940338 norm:3.6936533433618024e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:30:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.00987595971673727 norm:3.4059085010085255e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:30:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.009868551045656204 norm:3.076416760450229e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:31:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.009860215708613396 norm:2.8797181585105136e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:31:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.009851200506091118 norm:2.8292855859035626e-05 max memory_allocated 22563.11669921875 
[2025-02-19 12:31:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-19 12:32:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.012843126431107521 norm:0.0008479068637825549 max memory_allocated 22563.28857421875 
[2025-02-19 12:32:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.011898213997483253 norm:0.00022603050456382334 max memory_allocated 22563.28857421875 
[2025-02-19 12:33:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.011711819097399712 norm:0.0001672679209150374 max memory_allocated 22563.28857421875 
[2025-02-19 12:33:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.011630507186055183 norm:0.00015586566587444395 max memory_allocated 22563.28857421875 
[2025-02-19 12:34:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.011561145074665546 norm:0.00013228217721916735 max memory_allocated 22563.28857421875 
[2025-02-19 12:34:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.011501085013151169 norm:0.00010395122080808505 max memory_allocated 22563.28857421875 
[2025-02-19 12:35:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.011458732187747955 norm:8.939681720221415e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:35:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.011426102370023727 norm:7.47336889617145e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:36:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.011402026750147343 norm:6.1020462453598157e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:36:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.011384600773453712 norm:4.974503826815635e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:37:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.011367437429726124 norm:4.462785364012234e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:37:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.011353103443980217 norm:4.0294886275660247e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:38:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.01134288590401411 norm:3.735949212568812e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:38:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.011332928203046322 norm:3.6286877730162814e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:39:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.011326413601636887 norm:3.401352296350524e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:39:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.011322272941470146 norm:2.950150337710511e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:40:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.011315916664898396 norm:2.635320197441615e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:40:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.011310857720673084 norm:2.4743512767599896e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:41:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.011306364089250565 norm:2.336681427550502e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:41:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.0113023417070508 norm:2.2680784240947105e-05 max memory_allocated 22563.28857421875 
[2025-02-19 12:42:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-19 12:42:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.014573945663869381 norm:0.0007378013106063008 max memory_allocated 22563.46044921875 
[2025-02-19 12:43:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.01359520386904478 norm:0.0002535645035095513 max memory_allocated 22563.46044921875 
[2025-02-19 12:43:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.013372128829360008 norm:0.00019243833958171308 max memory_allocated 22563.46044921875 
[2025-02-19 12:44:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.013268331065773964 norm:0.00017055944772437215 max memory_allocated 22563.46044921875 
[2025-02-19 12:44:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.013187998905777931 norm:0.0001431069104000926 max memory_allocated 22563.46044921875 
[2025-02-19 12:45:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.013114042580127716 norm:0.00011231386451981962 max memory_allocated 22563.46044921875 
[2025-02-19 12:45:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.013065135106444359 norm:9.229788702214137e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:46:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.013027750886976719 norm:6.977206794545054e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:46:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.012999501079320908 norm:6.120337639003992e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:47:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.012981469742953777 norm:4.944958709529601e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:47:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.012967201881110668 norm:4.636200173990801e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:48:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.012950599193572998 norm:4.383354462333955e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:48:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.012935636565089226 norm:3.6986730265198275e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:49:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.012923642061650753 norm:3.354604632477276e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:49:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.012915089726448059 norm:2.655465686984826e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:50:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.01290835253894329 norm:2.541813228162937e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:50:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.012902661226689816 norm:2.3548618628410622e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:51:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.012897497043013573 norm:2.391721136518754e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:51:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.012892968021333218 norm:2.2570220608031377e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:52:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.012891756370663643 norm:2.2325861209537834e-05 max memory_allocated 22563.46044921875 
[2025-02-19 12:52:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-19 12:53:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.016817038878798485 norm:0.0011033969931304455 max memory_allocated 22563.63232421875 
[2025-02-19 12:53:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.015511428005993366 norm:0.00033970148069784045 max memory_allocated 22563.63232421875 
[2025-02-19 12:54:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.015233985148370266 norm:0.0002512971987016499 max memory_allocated 22563.63232421875 
[2025-02-19 12:54:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.015113424509763718 norm:0.00022317501134239137 max memory_allocated 22563.63232421875 
[2025-02-19 12:55:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.015000103041529655 norm:0.00018270997679792345 max memory_allocated 22563.63232421875 
[2025-02-19 12:55:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.014911259524524212 norm:0.00014513166388496757 max memory_allocated 22563.63232421875 
[2025-02-19 12:56:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.014843717217445374 norm:0.00012006815813947469 max memory_allocated 22563.63232421875 
[2025-02-19 12:56:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.014794593676924706 norm:9.389736078446731e-05 max memory_allocated 22563.63232421875 
[2025-02-19 12:57:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.014757481403648853 norm:7.905279926490039e-05 max memory_allocated 22563.63232421875 
[2025-02-19 12:57:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.014725888147950172 norm:6.964540807530284e-05 max memory_allocated 22563.63232421875 
[2025-02-19 12:58:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.014700309373438358 norm:6.206778198247775e-05 max memory_allocated 22563.63232421875 
[2025-02-19 12:58:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.014679577201604843 norm:5.8349167375126854e-05 max memory_allocated 22563.63232421875 
[2025-02-19 12:59:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.014660700224339962 norm:5.5524706112919375e-05 max memory_allocated 22563.63232421875 
[2025-02-19 12:59:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.014644082635641098 norm:5.1352988521102816e-05 max memory_allocated 22563.63232421875 
[2025-02-19 13:00:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.014629368670284748 norm:4.5608976506628096e-05 max memory_allocated 22563.63232421875 
[2025-02-19 13:00:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.014618631452322006 norm:3.9442100387532264e-05 max memory_allocated 22563.63232421875 
[2025-02-19 13:01:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.01461071614176035 norm:3.355218723299913e-05 max memory_allocated 22563.63232421875 
[2025-02-19 13:01:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.0146025400608778 norm:2.9270415325299837e-05 max memory_allocated 22563.63232421875 
[2025-02-19 13:02:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.014596053399145603 norm:2.8021175239700824e-05 max memory_allocated 22563.63232421875 
[2025-02-19 13:02:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.014591341838240623 norm:2.711263732635416e-05 max memory_allocated 22563.63232421875 
[2025-02-19 13:02:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-19 13:02:55 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:03:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.019090525805950165 norm:0.003148917108774185 max memory_allocated 22563.91943359375 
[2025-02-19 13:03:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.0181473046541214 norm:0.002111341804265976 max memory_allocated 22563.91943359375 
[2025-02-19 13:04:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.017810234799981117 norm:0.0016284700250253081 max memory_allocated 22563.91943359375 
[2025-02-19 13:04:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.01757432147860527 norm:0.0012832265347242355 max memory_allocated 22563.91943359375 
[2025-02-19 13:05:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.01739354245364666 norm:0.0010574592743068933 max memory_allocated 22563.91943359375 
[2025-02-19 13:05:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.01727340742945671 norm:0.0009080165764316916 max memory_allocated 22563.91943359375 
[2025-02-19 13:06:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.017182625830173492 norm:0.0007702333386987448 max memory_allocated 22563.91943359375 
[2025-02-19 13:07:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.017112303525209427 norm:0.000666929641738534 max memory_allocated 22563.91943359375 
[2025-02-19 13:07:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.017065415158867836 norm:0.0005593206733465195 max memory_allocated 22563.91943359375 
[2025-02-19 13:08:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.017021171748638153 norm:0.0005181015003472567 max memory_allocated 22563.91943359375 
[2025-02-19 13:08:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.01698191650211811 norm:0.0004676159587688744 max memory_allocated 22563.91943359375 
[2025-02-19 13:09:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.017042307183146477 norm:0.0007227810565382242 max memory_allocated 22563.91943359375 
[2025-02-19 13:09:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.017791099846363068 norm:0.002149218926206231 max memory_allocated 22563.91943359375 
[2025-02-19 13:10:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.017480289563536644 norm:0.0015856154495850205 max memory_allocated 22563.91943359375 
[2025-02-19 13:10:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.01771557703614235 norm:0.0019860677421092987 max memory_allocated 22563.91943359375 
[2025-02-19 13:11:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.016981955617666245 norm:0.0007552917813882232 max memory_allocated 22563.91943359375 
[2025-02-19 13:11:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.01692364364862442 norm:0.0005201477324590087 max memory_allocated 22563.91943359375 
[2025-02-19 13:12:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.01684761233627796 norm:0.0003990566765423864 max memory_allocated 22563.91943359375 
[2025-02-19 13:12:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.01682937704026699 norm:0.00047317807911895216 max memory_allocated 22563.91943359375 
[2025-02-19 13:13:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.01681392267346382 norm:0.000449667772045359 max memory_allocated 22563.91943359375 
[2025-02-19 13:13:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-19 13:13:20 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:13:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.021448900923132896 norm:0.003921855241060257 max memory_allocated 22564.09130859375 
[2025-02-19 13:14:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.02056926302611828 norm:0.0024027307517826557 max memory_allocated 22564.09130859375 
[2025-02-19 13:14:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.020282963290810585 norm:0.0018352337647229433 max memory_allocated 22564.09130859375 
[2025-02-19 13:15:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.02007393352687359 norm:0.0013921629870310426 max memory_allocated 22564.09130859375 
[2025-02-19 13:15:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.019916944205760956 norm:0.0010998196667060256 max memory_allocated 22564.09130859375 
[2025-02-19 13:16:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.01980593241751194 norm:0.0008040766697376966 max memory_allocated 22564.09130859375 
[2025-02-19 13:16:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.019717704504728317 norm:0.000792667968198657 max memory_allocated 22564.09130859375 
[2025-02-19 13:17:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.01964705064892769 norm:0.0006694801850244403 max memory_allocated 22564.09130859375 
[2025-02-19 13:17:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.019590802490711212 norm:0.0006431738147512078 max memory_allocated 22564.09130859375 
[2025-02-19 13:18:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.019546451047062874 norm:0.0005131436628289521 max memory_allocated 22564.09130859375 
[2025-02-19 13:18:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.019516615197062492 norm:0.0004859493637923151 max memory_allocated 22564.09130859375 
[2025-02-19 13:19:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.019500145688652992 norm:0.0004697517433669418 max memory_allocated 22564.09130859375 
[2025-02-19 13:20:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.019715242087841034 norm:0.0009718183428049088 max memory_allocated 22564.09130859375 
[2025-02-19 13:20:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.019497817382216454 norm:0.0006062646280042827 max memory_allocated 22564.09130859375 
[2025-02-19 13:21:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.019460270181298256 norm:0.0005018601077608764 max memory_allocated 22564.09130859375 
[2025-02-19 13:21:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.01941056363284588 norm:0.0003697130305226892 max memory_allocated 22564.09130859375 
[2025-02-19 13:22:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.019386064261198044 norm:0.0003394740342628211 max memory_allocated 22564.09130859375 
[2025-02-19 13:22:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.019373027607798576 norm:0.0003012580273207277 max memory_allocated 22564.09130859375 
[2025-02-19 13:23:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.019358748570084572 norm:0.00028412393294274807 max memory_allocated 22564.09130859375 
[2025-02-19 13:23:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.019349191337823868 norm:0.0002746054087765515 max memory_allocated 22564.09130859375 
[2025-02-19 13:23:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-19 13:23:46 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:24:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.030646108090877533 norm:0.006925354246050119 max memory_allocated 22564.26318359375 
[2025-02-19 13:24:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.02864449843764305 norm:0.004701302386820316 max memory_allocated 22564.26318359375 
[2025-02-19 13:25:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.027816422283649445 norm:0.004230544902384281 max memory_allocated 22564.26318359375 
[2025-02-19 13:25:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.027276786044239998 norm:0.004051669500768185 max memory_allocated 22564.26318359375 
[2025-02-19 13:26:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.026902571320533752 norm:0.003763272427022457 max memory_allocated 22564.26318359375 
[2025-02-19 13:26:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.026624172925949097 norm:0.0035522794350981712 max memory_allocated 22564.26318359375 
[2025-02-19 13:27:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.026419062167406082 norm:0.0033823070116341114 max memory_allocated 22564.26318359375 
[2025-02-19 13:27:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.026266714558005333 norm:0.0033155204728245735 max memory_allocated 22564.26318359375 
[2025-02-19 13:28:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.026138069108128548 norm:0.002938436344265938 max memory_allocated 22564.26318359375 
[2025-02-19 13:28:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.026044048368930817 norm:0.002715547103434801 max memory_allocated 22564.26318359375 
[2025-02-19 13:29:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.02596619352698326 norm:0.002501486800611019 max memory_allocated 22564.26318359375 
[2025-02-19 13:29:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.025917261838912964 norm:0.002398890443146229 max memory_allocated 22564.26318359375 
[2025-02-19 13:30:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.02588263526558876 norm:0.002252966398373246 max memory_allocated 22564.26318359375 
[2025-02-19 13:30:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.025827623903751373 norm:0.0019885478541254997 max memory_allocated 22564.26318359375 
[2025-02-19 13:31:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.025797072798013687 norm:0.0018099785083904862 max memory_allocated 22564.26318359375 
[2025-02-19 13:31:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.025792015716433525 norm:0.0018714930629357696 max memory_allocated 22564.26318359375 
[2025-02-19 13:32:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.02576461061835289 norm:0.0016770260408520699 max memory_allocated 22564.26318359375 
[2025-02-19 13:32:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.025773586705327034 norm:0.0018006147583946586 max memory_allocated 22564.26318359375 
[2025-02-19 13:33:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.025765500962734222 norm:0.0017254838021472096 max memory_allocated 22564.26318359375 
[2025-02-19 13:34:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.025767197832465172 norm:0.0016451542032882571 max memory_allocated 22564.26318359375 
[2025-02-19 13:34:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-19 13:34:11 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:34:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.05557767301797867 norm:0.00842913519591093 max memory_allocated 22564.43505859375 
[2025-02-19 13:35:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.05131109058856964 norm:0.00704082241281867 max memory_allocated 22564.43505859375 
[2025-02-19 13:35:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.04882096126675606 norm:0.0064911008812487125 max memory_allocated 22564.43505859375 
[2025-02-19 13:36:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.04699472337961197 norm:0.0061258296482264996 max memory_allocated 22564.43505859375 
[2025-02-19 13:36:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.0454605296254158 norm:0.00564690213650465 max memory_allocated 22564.43505859375 
[2025-02-19 13:37:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.04435182735323906 norm:0.005379095207899809 max memory_allocated 22564.43505859375 
[2025-02-19 13:37:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.04344495013356209 norm:0.004895972553640604 max memory_allocated 22564.43505859375 
[2025-02-19 13:38:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.042883168905973434 norm:0.004695311188697815 max memory_allocated 22564.43505859375 
[2025-02-19 13:38:48 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.04252517595887184 norm:0.004643734078854322 max memory_allocated 22564.43505859375 
[2025-02-19 13:39:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.04260215908288956 norm:0.005194530822336674 max memory_allocated 22564.43505859375 
[2025-02-19 13:39:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.0417497381567955 norm:0.0026328035164624453 max memory_allocated 22564.43505859375 
[2025-02-19 13:40:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.0412171334028244 norm:0.002922851825132966 max memory_allocated 22564.43505859375 
[2025-02-19 13:40:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.04098593443632126 norm:0.003700867062434554 max memory_allocated 22564.43505859375 
[2025-02-19 13:41:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.040823787450790405 norm:0.003965773154050112 max memory_allocated 22564.43505859375 
[2025-02-19 13:41:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.04066162928938866 norm:0.003940675873309374 max memory_allocated 22564.43505859375 
[2025-02-19 13:42:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.04052788019180298 norm:0.0038278752472251654 max memory_allocated 22564.43505859375 
[2025-02-19 13:42:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.0403529517352581 norm:0.0036710230633616447 max memory_allocated 22564.43505859375 
[2025-02-19 13:43:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.0402350127696991 norm:0.0035651521757245064 max memory_allocated 22564.43505859375 
[2025-02-19 13:43:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.04027962684631348 norm:0.003704874776303768 max memory_allocated 22564.43505859375 
[2025-02-19 13:44:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.04184181988239288 norm:0.006494197063148022 max memory_allocated 22564.43505859375 
[2025-02-19 13:44:34 root] (main_calibration.py 365): INFO 19961.691564559937
[2025-02-19 13:45:24 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-19 13:46:28 root] (main_calibration.py 158): INFO wikitext2 : 5.685418128967285
[2025-02-19 13:46:28 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-19 13:48:06 root] (main_calibration.py 158): INFO c4 : 7.097908020019531
[2025-02-19 15:26:14 root] (main_calibration.py 169): INFO {'wikitext2': 5.685418128967285, 'c4': 7.097908020019531, 'results': {'arc_challenge': {'acc': 0.38310580204778155, 'acc_stderr': 0.014206472661672883, 'acc_norm': 0.4206484641638225, 'acc_norm_stderr': 0.014426211252508403}, 'hellaswag': {'acc': 0.5636327424815774, 'acc_stderr': 0.004949207947265922, 'acc_norm': 0.7305317665803625, 'acc_norm_stderr': 0.004427767996301641}, 'boolq': {'acc': 0.7333333333333333, 'acc_stderr': 0.007734417634064953}, 'arc_easy': {'acc': 0.6708754208754208, 'acc_stderr': 0.009642048058060983, 'acc_norm': 0.5176767676767676, 'acc_norm_stderr': 0.010253369805698962}, 'winogrande': {'acc': 0.675611681136543, 'acc_stderr': 0.01315722572664163}, 'piqa': {'acc': 0.7818280739934712, 'acc_stderr': 0.009636081958374381, 'acc_norm': 0.7736670293797606, 'acc_norm_stderr': 0.009763294246879418}}, 'versions': {'arc_challenge': 0, 'hellaswag': 0, 'boolq': 1, 'arc_easy': 0, 'winogrande': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
