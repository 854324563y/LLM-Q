[2025-03-27 13:43:16 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/MMLU/Llama-2-7b-hf-w4a7', save_dir=None, resume='./log-calibration-compensation-lwc/Llama-2-7b-hf-w4a7/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-27 13:46:26 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-27 13:46:26 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-27 13:46:27 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-27 13:46:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-27 13:46:31 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:46:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-27 13:46:32 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:46:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-27 13:46:33 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:46:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-27 13:46:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-27 13:46:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-27 13:46:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-27 13:46:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-27 13:46:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-27 13:46:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-27 13:46:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-27 13:46:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-27 13:46:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-27 13:46:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-27 13:46:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-27 13:46:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-27 13:46:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-27 13:46:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-27 13:46:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-27 13:46:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-27 13:46:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-27 13:46:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-27 13:46:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-27 13:46:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-27 13:46:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-27 13:46:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-27 13:46:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-27 13:46:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-27 13:46:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-27 13:46:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:46:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-27 13:46:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:46:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-27 13:46:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:46:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-27 13:46:43 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:46:43 root] (main_calibration.py 365): INFO 17.00507950782776
[2025-03-27 15:11:49 root] (main_calibration.py 169): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.4049586776859504, 'acc_stderr': 0.044811377559424694, 'acc_norm': 0.5619834710743802, 'acc_norm_stderr': 0.045291468044357915}, 'hendrycksTest-high_school_biology': {'acc': 0.3193548387096774, 'acc_stderr': 0.026522709674667765, 'acc_norm': 0.3096774193548387, 'acc_norm_stderr': 0.026302774983517418}, 'hendrycksTest-college_physics': {'acc': 0.21568627450980393, 'acc_stderr': 0.04092563958237654, 'acc_norm': 0.23529411764705882, 'acc_norm_stderr': 0.042207736591714534}, 'hendrycksTest-college_biology': {'acc': 0.2847222222222222, 'acc_stderr': 0.03773809990686934, 'acc_norm': 0.22916666666666666, 'acc_norm_stderr': 0.035146974678623884}, 'hendrycksTest-electrical_engineering': {'acc': 0.31724137931034485, 'acc_stderr': 0.03878352372138622, 'acc_norm': 0.3448275862068966, 'acc_norm_stderr': 0.039609335494512087}, 'hendrycksTest-high_school_physics': {'acc': 0.2847682119205298, 'acc_stderr': 0.03684881521389024, 'acc_norm': 0.271523178807947, 'acc_norm_stderr': 0.03631329803969653}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2830687830687831, 'acc_stderr': 0.023201392938194978, 'acc_norm': 0.2962962962962963, 'acc_norm_stderr': 0.02351729433596328}, 'hendrycksTest-formal_logic': {'acc': 0.31746031746031744, 'acc_stderr': 0.04163453031302859, 'acc_norm': 0.2777777777777778, 'acc_norm_stderr': 0.04006168083848878}, 'hendrycksTest-human_aging': {'acc': 0.27802690582959644, 'acc_stderr': 0.030069584874494036, 'acc_norm': 0.22869955156950672, 'acc_norm_stderr': 0.028188240046929196}, 'hendrycksTest-jurisprudence': {'acc': 0.3333333333333333, 'acc_stderr': 0.04557239513497752, 'acc_norm': 0.39814814814814814, 'acc_norm_stderr': 0.047323326159788126}, 'hendrycksTest-moral_scenarios': {'acc': 0.2558659217877095, 'acc_stderr': 0.01459362092321073, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.3881578947368421, 'acc_stderr': 0.03965842097512744, 'acc_norm': 0.3684210526315789, 'acc_norm_stderr': 0.03925523381052932}, 'hendrycksTest-conceptual_physics': {'acc': 0.2680851063829787, 'acc_stderr': 0.028957342788342347, 'acc_norm': 0.225531914893617, 'acc_norm_stderr': 0.027321078417387536}, 'hendrycksTest-college_chemistry': {'acc': 0.25, 'acc_stderr': 0.04351941398892446, 'acc_norm': 0.33, 'acc_norm_stderr': 0.04725815626252605}, 'hendrycksTest-human_sexuality': {'acc': 0.42748091603053434, 'acc_stderr': 0.04338920305792401, 'acc_norm': 0.33587786259541985, 'acc_norm_stderr': 0.04142313771996665}, 'hendrycksTest-sociology': {'acc': 0.27860696517412936, 'acc_stderr': 0.03170056183497308, 'acc_norm': 0.3034825870646766, 'acc_norm_stderr': 0.03251006816458617}, 'hendrycksTest-prehistory': {'acc': 0.3395061728395062, 'acc_stderr': 0.026348564412011628, 'acc_norm': 0.25617283950617287, 'acc_norm_stderr': 0.024288533637726095}, 'hendrycksTest-professional_law': {'acc': 0.2627118644067797, 'acc_stderr': 0.011240545514995667, 'acc_norm': 0.29791395045632335, 'acc_norm_stderr': 0.011680717340400043}, 'hendrycksTest-virology': {'acc': 0.3433734939759036, 'acc_stderr': 0.036965843170106004, 'acc_norm': 0.27710843373493976, 'acc_norm_stderr': 0.034843315926805875}, 'hendrycksTest-machine_learning': {'acc': 0.32142857142857145, 'acc_stderr': 0.04432804055291519, 'acc_norm': 0.24107142857142858, 'acc_norm_stderr': 0.04059867246952686}, 'hendrycksTest-marketing': {'acc': 0.4444444444444444, 'acc_stderr': 0.03255326307272485, 'acc_norm': 0.39316239316239315, 'acc_norm_stderr': 0.03199957924651048}, 'hendrycksTest-us_foreign_policy': {'acc': 0.43, 'acc_stderr': 0.04975698519562428, 'acc_norm': 0.39, 'acc_norm_stderr': 0.04902071300001975}, 'hendrycksTest-high_school_european_history': {'acc': 0.3939393939393939, 'acc_stderr': 0.0381549430868893, 'acc_norm': 0.3393939393939394, 'acc_norm_stderr': 0.03697442205031596}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.31794871794871793, 'acc_stderr': 0.02361088430892786, 'acc_norm': 0.28205128205128205, 'acc_norm_stderr': 0.0228158130988966}, 'hendrycksTest-logical_fallacies': {'acc': 0.25766871165644173, 'acc_stderr': 0.03436150827846917, 'acc_norm': 0.294478527607362, 'acc_norm_stderr': 0.03581165790474082}, 'hendrycksTest-abstract_algebra': {'acc': 0.2, 'acc_stderr': 0.04020151261036845, 'acc_norm': 0.22, 'acc_norm_stderr': 0.0416333199893227}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.36787564766839376, 'acc_stderr': 0.03480175668466036, 'acc_norm': 0.30569948186528495, 'acc_norm_stderr': 0.03324837939758159}, 'hendrycksTest-moral_disputes': {'acc': 0.2745664739884393, 'acc_stderr': 0.02402774515526502, 'acc_norm': 0.33236994219653176, 'acc_norm_stderr': 0.025361168749688218}, 'hendrycksTest-high_school_world_history': {'acc': 0.3924050632911392, 'acc_stderr': 0.03178471874564729, 'acc_norm': 0.379746835443038, 'acc_norm_stderr': 0.031591887529658504}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.3025210084033613, 'acc_stderr': 0.029837962388291932, 'acc_norm': 0.38235294117647056, 'acc_norm_stderr': 0.03156663099215416}, 'hendrycksTest-college_medicine': {'acc': 0.28901734104046245, 'acc_stderr': 0.03456425745086999, 'acc_norm': 0.28901734104046245, 'acc_norm_stderr': 0.034564257450869995}, 'hendrycksTest-security_studies': {'acc': 0.43673469387755104, 'acc_stderr': 0.031751952375833226, 'acc_norm': 0.33877551020408164, 'acc_norm_stderr': 0.03029950656215418}, 'hendrycksTest-high_school_geography': {'acc': 0.2878787878787879, 'acc_stderr': 0.03225883512300992, 'acc_norm': 0.30303030303030304, 'acc_norm_stderr': 0.032742879140268674}, 'hendrycksTest-world_religions': {'acc': 0.3742690058479532, 'acc_stderr': 0.03711601185389481, 'acc_norm': 0.42105263157894735, 'acc_norm_stderr': 0.03786720706234214}, 'hendrycksTest-business_ethics': {'acc': 0.43, 'acc_stderr': 0.04975698519562428, 'acc_norm': 0.36, 'acc_norm_stderr': 0.048241815132442176}, 'hendrycksTest-professional_accounting': {'acc': 0.2624113475177305, 'acc_stderr': 0.026244920349843028, 'acc_norm': 0.24822695035460993, 'acc_norm_stderr': 0.0257700156442904}, 'hendrycksTest-global_facts': {'acc': 0.22, 'acc_stderr': 0.041633319989322674, 'acc_norm': 0.18, 'acc_norm_stderr': 0.038612291966536955}, 'hendrycksTest-high_school_us_history': {'acc': 0.35784313725490197, 'acc_stderr': 0.033644872860882996, 'acc_norm': 0.3235294117647059, 'acc_norm_stderr': 0.03283472056108567}, 'hendrycksTest-professional_psychology': {'acc': 0.2957516339869281, 'acc_stderr': 0.018463154132632817, 'acc_norm': 0.28104575163398693, 'acc_norm_stderr': 0.018185218954318082}, 'hendrycksTest-college_computer_science': {'acc': 0.3, 'acc_stderr': 0.04605661864718381, 'acc_norm': 0.29, 'acc_norm_stderr': 0.045604802157206845}, 'hendrycksTest-philosophy': {'acc': 0.33762057877813506, 'acc_stderr': 0.026858825879488544, 'acc_norm': 0.3183279742765273, 'acc_norm_stderr': 0.026457225067811025}, 'hendrycksTest-anatomy': {'acc': 0.32592592592592595, 'acc_stderr': 0.040491220417025055, 'acc_norm': 0.23703703703703705, 'acc_norm_stderr': 0.03673731683969506}, 'hendrycksTest-nutrition': {'acc': 0.33986928104575165, 'acc_stderr': 0.02712195607138885, 'acc_norm': 0.39869281045751637, 'acc_norm_stderr': 0.02803609227389176}, 'hendrycksTest-computer_security': {'acc': 0.32, 'acc_stderr': 0.046882617226215034, 'acc_norm': 0.33, 'acc_norm_stderr': 0.047258156262526045}, 'hendrycksTest-econometrics': {'acc': 0.2631578947368421, 'acc_stderr': 0.04142439719489362, 'acc_norm': 0.2631578947368421, 'acc_norm_stderr': 0.04142439719489362}, 'hendrycksTest-clinical_knowledge': {'acc': 0.29056603773584905, 'acc_stderr': 0.027943219989337142, 'acc_norm': 0.3622641509433962, 'acc_norm_stderr': 0.0295822451283843}, 'hendrycksTest-high_school_mathematics': {'acc': 0.2074074074074074, 'acc_stderr': 0.02472071319395217, 'acc_norm': 0.2740740740740741, 'acc_norm_stderr': 0.027195934804085622}, 'hendrycksTest-medical_genetics': {'acc': 0.26, 'acc_stderr': 0.04408440022768077, 'acc_norm': 0.34, 'acc_norm_stderr': 0.04760952285695236}, 'hendrycksTest-public_relations': {'acc': 0.3, 'acc_stderr': 0.04389311454644286, 'acc_norm': 0.2, 'acc_norm_stderr': 0.038313051408846034}, 'hendrycksTest-miscellaneous': {'acc': 0.3652618135376756, 'acc_stderr': 0.017218530028838643, 'acc_norm': 0.2962962962962963, 'acc_norm_stderr': 0.01632881442210205}, 'hendrycksTest-management': {'acc': 0.3786407766990291, 'acc_stderr': 0.04802694698258973, 'acc_norm': 0.3300970873786408, 'acc_norm_stderr': 0.04656147110012352}, 'hendrycksTest-professional_medicine': {'acc': 0.26838235294117646, 'acc_stderr': 0.026917481224377204, 'acc_norm': 0.2647058823529412, 'acc_norm_stderr': 0.026799562024887674}, 'hendrycksTest-high_school_computer_science': {'acc': 0.3, 'acc_stderr': 0.046056618647183814, 'acc_norm': 0.29, 'acc_norm_stderr': 0.04560480215720684}, 'hendrycksTest-high_school_psychology': {'acc': 0.3284403669724771, 'acc_stderr': 0.020135902797298405, 'acc_norm': 0.26788990825688075, 'acc_norm_stderr': 0.01898746225797865}, 'hendrycksTest-high_school_chemistry': {'acc': 0.2512315270935961, 'acc_stderr': 0.030516530732694436, 'acc_norm': 0.29064039408866993, 'acc_norm_stderr': 0.03194740072265541}, 'hendrycksTest-college_mathematics': {'acc': 0.23, 'acc_stderr': 0.04229525846816506, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-high_school_statistics': {'acc': 0.27314814814814814, 'acc_stderr': 0.030388051301678116, 'acc_norm': 0.27314814814814814, 'acc_norm_stderr': 0.030388051301678116}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 15:11:49 root] (main_calibration.py 196): INFO Average accuracy 0.2786 - STEM
[2025-03-27 15:11:49 root] (main_calibration.py 196): INFO Average accuracy 0.3309 - humanities
[2025-03-27 15:11:49 root] (main_calibration.py 196): INFO Average accuracy 0.3364 - social sciences
[2025-03-27 15:11:49 root] (main_calibration.py 196): INFO Average accuracy 0.3211 - other (business, health, misc.)
[2025-03-27 15:11:49 root] (main_calibration.py 198): INFO Average accuracy: 0.3131
