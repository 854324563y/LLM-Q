[2025-03-27 13:43:52 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/MMLU/Llama-2-13b-hf-w4a6', save_dir=None, resume='./log-calibration-compensation-lwc/Llama-2-13b-hf-w4a6/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-27 13:43:55 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-27 13:43:56 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-27 13:43:57 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-27 13:44:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-27 13:44:16 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:44:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-27 13:44:26 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:44:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-27 13:44:35 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:44:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-27 13:44:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-27 13:44:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-27 13:45:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-27 13:45:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-27 13:45:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-27 13:45:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-27 13:45:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-27 13:45:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-27 13:46:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-27 13:46:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-27 13:46:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-27 13:46:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-27 13:46:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-27 13:46:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-27 13:46:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-27 13:47:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-27 13:47:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-27 13:47:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-27 13:47:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-27 13:47:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-27 13:47:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-27 13:47:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-27 13:48:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-27 13:48:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-27 13:48:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-27 13:48:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-27 13:48:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-27 13:48:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-27 13:49:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-03-27 13:49:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-03-27 13:49:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-03-27 13:49:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-03-27 13:49:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-03-27 13:49:44 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:49:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-03-27 13:49:53 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:49:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-03-27 13:49:59 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:49:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-03-27 13:50:08 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:50:09 root] (main_calibration.py 365): INFO 373.548255443573
[2025-03-27 15:32:40 root] (main_calibration.py 169): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.45454545454545453, 'acc_stderr': 0.04545454545454545, 'acc_norm': 0.5702479338842975, 'acc_norm_stderr': 0.04519082021319773}, 'hendrycksTest-high_school_biology': {'acc': 0.3387096774193548, 'acc_stderr': 0.026923446059302844, 'acc_norm': 0.3161290322580645, 'acc_norm_stderr': 0.026450874489042767}, 'hendrycksTest-college_physics': {'acc': 0.2647058823529412, 'acc_stderr': 0.04389869956808777, 'acc_norm': 0.3235294117647059, 'acc_norm_stderr': 0.04655010411319619}, 'hendrycksTest-college_biology': {'acc': 0.3194444444444444, 'acc_stderr': 0.03899073687357336, 'acc_norm': 0.2638888888888889, 'acc_norm_stderr': 0.03685651095897532}, 'hendrycksTest-electrical_engineering': {'acc': 0.3793103448275862, 'acc_stderr': 0.04043461861916748, 'acc_norm': 0.31724137931034485, 'acc_norm_stderr': 0.03878352372138622}, 'hendrycksTest-high_school_physics': {'acc': 0.26490066225165565, 'acc_stderr': 0.03603038545360384, 'acc_norm': 0.26490066225165565, 'acc_norm_stderr': 0.03603038545360384}, 'hendrycksTest-elementary_mathematics': {'acc': 0.328042328042328, 'acc_stderr': 0.024180497164376893, 'acc_norm': 0.3306878306878307, 'acc_norm_stderr': 0.024229965298425093}, 'hendrycksTest-formal_logic': {'acc': 0.29365079365079366, 'acc_stderr': 0.04073524322147128, 'acc_norm': 0.2698412698412698, 'acc_norm_stderr': 0.03970158273235173}, 'hendrycksTest-human_aging': {'acc': 0.28699551569506726, 'acc_stderr': 0.030360379710291933, 'acc_norm': 0.23766816143497757, 'acc_norm_stderr': 0.02856807946471429}, 'hendrycksTest-jurisprudence': {'acc': 0.3611111111111111, 'acc_stderr': 0.04643454608906275, 'acc_norm': 0.4444444444444444, 'acc_norm_stderr': 0.04803752235190192}, 'hendrycksTest-moral_scenarios': {'acc': 0.28044692737430166, 'acc_stderr': 0.015024083883322903, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.40789473684210525, 'acc_stderr': 0.03999309712777471, 'acc_norm': 0.4407894736842105, 'acc_norm_stderr': 0.04040311062490436}, 'hendrycksTest-conceptual_physics': {'acc': 0.26382978723404255, 'acc_stderr': 0.028809989854102967, 'acc_norm': 0.20425531914893616, 'acc_norm_stderr': 0.02635515841334942}, 'hendrycksTest-college_chemistry': {'acc': 0.26, 'acc_stderr': 0.04408440022768079, 'acc_norm': 0.3, 'acc_norm_stderr': 0.04605661864718381}, 'hendrycksTest-human_sexuality': {'acc': 0.45038167938931295, 'acc_stderr': 0.04363643698524779, 'acc_norm': 0.3282442748091603, 'acc_norm_stderr': 0.04118438565806299}, 'hendrycksTest-sociology': {'acc': 0.35323383084577115, 'acc_stderr': 0.03379790611796777, 'acc_norm': 0.3781094527363184, 'acc_norm_stderr': 0.034288678487786564}, 'hendrycksTest-prehistory': {'acc': 0.3549382716049383, 'acc_stderr': 0.02662415247884585, 'acc_norm': 0.2777777777777778, 'acc_norm_stderr': 0.024922001168886335}, 'hendrycksTest-professional_law': {'acc': 0.24771838331160365, 'acc_stderr': 0.011025499291443742, 'acc_norm': 0.3057366362451108, 'acc_norm_stderr': 0.011766973847072915}, 'hendrycksTest-virology': {'acc': 0.3855421686746988, 'acc_stderr': 0.03789134424611548, 'acc_norm': 0.3373493975903614, 'acc_norm_stderr': 0.0368078369072758}, 'hendrycksTest-machine_learning': {'acc': 0.25892857142857145, 'acc_stderr': 0.04157751539865629, 'acc_norm': 0.30357142857142855, 'acc_norm_stderr': 0.043642261558410445}, 'hendrycksTest-marketing': {'acc': 0.47863247863247865, 'acc_stderr': 0.032726164476349545, 'acc_norm': 0.44017094017094016, 'acc_norm_stderr': 0.032520741720630506}, 'hendrycksTest-us_foreign_policy': {'acc': 0.53, 'acc_stderr': 0.050161355804659205, 'acc_norm': 0.44, 'acc_norm_stderr': 0.04988876515698589}, 'hendrycksTest-high_school_european_history': {'acc': 0.3939393939393939, 'acc_stderr': 0.0381549430868893, 'acc_norm': 0.3575757575757576, 'acc_norm_stderr': 0.037425970438065864}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.3717948717948718, 'acc_stderr': 0.024503472557110946, 'acc_norm': 0.30512820512820515, 'acc_norm_stderr': 0.023346335293325887}, 'hendrycksTest-logical_fallacies': {'acc': 0.32515337423312884, 'acc_stderr': 0.0368035037128646, 'acc_norm': 0.3374233128834356, 'acc_norm_stderr': 0.03714908409935574}, 'hendrycksTest-abstract_algebra': {'acc': 0.22, 'acc_stderr': 0.04163331998932269, 'acc_norm': 0.21, 'acc_norm_stderr': 0.040936018074033256}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.44041450777202074, 'acc_stderr': 0.03582724530036094, 'acc_norm': 0.3626943005181347, 'acc_norm_stderr': 0.03469713791704371}, 'hendrycksTest-moral_disputes': {'acc': 0.35260115606936415, 'acc_stderr': 0.025722802200895834, 'acc_norm': 0.3583815028901734, 'acc_norm_stderr': 0.0258167567915842}, 'hendrycksTest-high_school_world_history': {'acc': 0.4050632911392405, 'acc_stderr': 0.031955147413706725, 'acc_norm': 0.35443037974683544, 'acc_norm_stderr': 0.031137304297185812}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.3403361344537815, 'acc_stderr': 0.030778057422931677, 'acc_norm': 0.37815126050420167, 'acc_norm_stderr': 0.031499305777849054}, 'hendrycksTest-college_medicine': {'acc': 0.26011560693641617, 'acc_stderr': 0.033450369167889925, 'acc_norm': 0.27167630057803466, 'acc_norm_stderr': 0.033917503223216613}, 'hendrycksTest-security_studies': {'acc': 0.4530612244897959, 'acc_stderr': 0.03186785930004128, 'acc_norm': 0.33877551020408164, 'acc_norm_stderr': 0.030299506562154185}, 'hendrycksTest-high_school_geography': {'acc': 0.35858585858585856, 'acc_stderr': 0.03416903640391521, 'acc_norm': 0.3282828282828283, 'acc_norm_stderr': 0.03345678422756775}, 'hendrycksTest-world_religions': {'acc': 0.5263157894736842, 'acc_stderr': 0.038295098689947266, 'acc_norm': 0.4444444444444444, 'acc_norm_stderr': 0.0381107966983353}, 'hendrycksTest-business_ethics': {'acc': 0.48, 'acc_stderr': 0.050211673156867795, 'acc_norm': 0.4, 'acc_norm_stderr': 0.049236596391733084}, 'hendrycksTest-professional_accounting': {'acc': 0.2730496453900709, 'acc_stderr': 0.026577860943307854, 'acc_norm': 0.3049645390070922, 'acc_norm_stderr': 0.027464708442022128}, 'hendrycksTest-global_facts': {'acc': 0.24, 'acc_stderr': 0.04292346959909283, 'acc_norm': 0.2, 'acc_norm_stderr': 0.04020151261036843}, 'hendrycksTest-high_school_us_history': {'acc': 0.37745098039215685, 'acc_stderr': 0.03402272044340705, 'acc_norm': 0.3480392156862745, 'acc_norm_stderr': 0.03343311240488419}, 'hendrycksTest-professional_psychology': {'acc': 0.3349673202614379, 'acc_stderr': 0.019094228167000318, 'acc_norm': 0.29901960784313725, 'acc_norm_stderr': 0.018521756215423024}, 'hendrycksTest-college_computer_science': {'acc': 0.33, 'acc_stderr': 0.04725815626252605, 'acc_norm': 0.32, 'acc_norm_stderr': 0.046882617226215034}, 'hendrycksTest-philosophy': {'acc': 0.3504823151125402, 'acc_stderr': 0.027098652621301747, 'acc_norm': 0.33762057877813506, 'acc_norm_stderr': 0.026858825879488547}, 'hendrycksTest-anatomy': {'acc': 0.34074074074074073, 'acc_stderr': 0.04094376269996794, 'acc_norm': 0.22962962962962963, 'acc_norm_stderr': 0.03633384414073462}, 'hendrycksTest-nutrition': {'acc': 0.42810457516339867, 'acc_stderr': 0.028332397483664274, 'acc_norm': 0.42810457516339867, 'acc_norm_stderr': 0.028332397483664267}, 'hendrycksTest-computer_security': {'acc': 0.43, 'acc_stderr': 0.04975698519562428, 'acc_norm': 0.39, 'acc_norm_stderr': 0.04902071300001975}, 'hendrycksTest-econometrics': {'acc': 0.2719298245614035, 'acc_stderr': 0.041857744240220575, 'acc_norm': 0.22807017543859648, 'acc_norm_stderr': 0.03947152782669415}, 'hendrycksTest-clinical_knowledge': {'acc': 0.3169811320754717, 'acc_stderr': 0.028637235639800925, 'acc_norm': 0.35094339622641507, 'acc_norm_stderr': 0.029373646253234683}, 'hendrycksTest-high_school_mathematics': {'acc': 0.26296296296296295, 'acc_stderr': 0.026842057873833706, 'acc_norm': 0.2814814814814815, 'acc_norm_stderr': 0.027420019350945277}, 'hendrycksTest-medical_genetics': {'acc': 0.32, 'acc_stderr': 0.04688261722621504, 'acc_norm': 0.36, 'acc_norm_stderr': 0.04824181513244218}, 'hendrycksTest-public_relations': {'acc': 0.37272727272727274, 'acc_stderr': 0.04631381319425463, 'acc_norm': 0.21818181818181817, 'acc_norm_stderr': 0.03955932861795833}, 'hendrycksTest-miscellaneous': {'acc': 0.43039591315453385, 'acc_stderr': 0.017705868776292398, 'acc_norm': 0.3243933588761175, 'acc_norm_stderr': 0.01674092904716271}, 'hendrycksTest-management': {'acc': 0.4368932038834951, 'acc_stderr': 0.04911147107365777, 'acc_norm': 0.39805825242718446, 'acc_norm_stderr': 0.04846748253977237}, 'hendrycksTest-professional_medicine': {'acc': 0.375, 'acc_stderr': 0.029408372932278746, 'acc_norm': 0.30514705882352944, 'acc_norm_stderr': 0.027971541370170595}, 'hendrycksTest-high_school_computer_science': {'acc': 0.36, 'acc_stderr': 0.04824181513244218, 'acc_norm': 0.34, 'acc_norm_stderr': 0.047609522856952365}, 'hendrycksTest-high_school_psychology': {'acc': 0.3412844036697248, 'acc_stderr': 0.020328612816592435, 'acc_norm': 0.26972477064220185, 'acc_norm_stderr': 0.019028486711115445}, 'hendrycksTest-high_school_chemistry': {'acc': 0.2561576354679803, 'acc_stderr': 0.030712730070982592, 'acc_norm': 0.33497536945812806, 'acc_norm_stderr': 0.033208527423483104}, 'hendrycksTest-college_mathematics': {'acc': 0.24, 'acc_stderr': 0.04292346959909283, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117316}, 'hendrycksTest-high_school_statistics': {'acc': 0.3333333333333333, 'acc_stderr': 0.032149521478027486, 'acc_norm': 0.35185185185185186, 'acc_norm_stderr': 0.032568505702936464}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 15:32:40 root] (main_calibration.py 196): INFO Average accuracy 0.3066 - STEM
[2025-03-27 15:32:40 root] (main_calibration.py 196): INFO Average accuracy 0.3633 - humanities
[2025-03-27 15:32:40 root] (main_calibration.py 196): INFO Average accuracy 0.3849 - social sciences
[2025-03-27 15:32:40 root] (main_calibration.py 196): INFO Average accuracy 0.3609 - other (business, health, misc.)
[2025-03-27 15:32:40 root] (main_calibration.py 198): INFO Average accuracy: 0.3493
