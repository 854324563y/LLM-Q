[2025-03-27 13:44:36 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/MMLU/llama-7b-hf-w4a7', save_dir=None, resume='./log-calibration-compensation-lwc/llama-7b-hf-w4a7/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-27 13:47:54 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-27 13:47:55 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-27 13:47:56 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-27 13:48:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-27 13:48:01 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:48:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-27 13:48:03 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:48:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-27 13:48:04 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:48:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-27 13:48:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-27 13:48:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-27 13:48:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-27 13:48:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-27 13:48:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-27 13:48:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-27 13:48:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-27 13:48:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-27 13:48:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-27 13:48:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-27 13:48:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-27 13:48:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-27 13:48:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-27 13:48:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-27 13:48:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-27 13:48:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-27 13:48:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-27 13:48:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-27 13:48:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-27 13:48:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-27 13:48:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-27 13:48:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-27 13:48:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-27 13:48:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-27 13:48:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-27 13:48:20 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:48:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-27 13:48:21 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:48:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-27 13:48:21 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:48:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-27 13:48:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:48:22 root] (main_calibration.py 365): INFO 28.39343571662903
[2025-03-27 15:13:37 root] (main_calibration.py 169): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.30578512396694213, 'acc_stderr': 0.04205953933884124, 'acc_norm': 0.5371900826446281, 'acc_norm_stderr': 0.045517111961042175}, 'hendrycksTest-high_school_biology': {'acc': 0.2870967741935484, 'acc_stderr': 0.025736542745594535, 'acc_norm': 0.3064516129032258, 'acc_norm_stderr': 0.026226485652553883}, 'hendrycksTest-college_physics': {'acc': 0.24509803921568626, 'acc_stderr': 0.042801058373643966, 'acc_norm': 0.3627450980392157, 'acc_norm_stderr': 0.04784060704105653}, 'hendrycksTest-college_biology': {'acc': 0.25, 'acc_stderr': 0.03621034121889507, 'acc_norm': 0.2361111111111111, 'acc_norm_stderr': 0.03551446610810826}, 'hendrycksTest-electrical_engineering': {'acc': 0.36551724137931035, 'acc_stderr': 0.040131241954243856, 'acc_norm': 0.33793103448275863, 'acc_norm_stderr': 0.03941707632064889}, 'hendrycksTest-high_school_physics': {'acc': 0.24503311258278146, 'acc_stderr': 0.03511807571804723, 'acc_norm': 0.2582781456953642, 'acc_norm_stderr': 0.035737053147634576}, 'hendrycksTest-elementary_mathematics': {'acc': 0.30423280423280424, 'acc_stderr': 0.023695415009463087, 'acc_norm': 0.2777777777777778, 'acc_norm_stderr': 0.023068188848261107}, 'hendrycksTest-formal_logic': {'acc': 0.30158730158730157, 'acc_stderr': 0.04104947269903394, 'acc_norm': 0.31746031746031744, 'acc_norm_stderr': 0.04163453031302859}, 'hendrycksTest-human_aging': {'acc': 0.2825112107623318, 'acc_stderr': 0.03021683101150876, 'acc_norm': 0.21076233183856502, 'acc_norm_stderr': 0.027373095500540193}, 'hendrycksTest-jurisprudence': {'acc': 0.32407407407407407, 'acc_stderr': 0.04524596007030048, 'acc_norm': 0.42592592592592593, 'acc_norm_stderr': 0.0478034362693679}, 'hendrycksTest-moral_scenarios': {'acc': 0.23798882681564246, 'acc_stderr': 0.014242630070574915, 'acc_norm': 0.26927374301675977, 'acc_norm_stderr': 0.014835616582882584}, 'hendrycksTest-astronomy': {'acc': 0.42105263157894735, 'acc_stderr': 0.04017901275981749, 'acc_norm': 0.4276315789473684, 'acc_norm_stderr': 0.040260970832965585}, 'hendrycksTest-conceptual_physics': {'acc': 0.26382978723404255, 'acc_stderr': 0.028809989854102967, 'acc_norm': 0.20425531914893616, 'acc_norm_stderr': 0.02635515841334942}, 'hendrycksTest-college_chemistry': {'acc': 0.18, 'acc_stderr': 0.038612291966536955, 'acc_norm': 0.29, 'acc_norm_stderr': 0.04560480215720684}, 'hendrycksTest-human_sexuality': {'acc': 0.42748091603053434, 'acc_stderr': 0.04338920305792401, 'acc_norm': 0.31297709923664124, 'acc_norm_stderr': 0.04066962905677697}, 'hendrycksTest-sociology': {'acc': 0.30845771144278605, 'acc_stderr': 0.03265819588512699, 'acc_norm': 0.2537313432835821, 'acc_norm_stderr': 0.03076944496729602}, 'hendrycksTest-prehistory': {'acc': 0.30864197530864196, 'acc_stderr': 0.025702640260603767, 'acc_norm': 0.2716049382716049, 'acc_norm_stderr': 0.02474862449053738}, 'hendrycksTest-professional_law': {'acc': 0.2777053455019557, 'acc_stderr': 0.011438741422769556, 'acc_norm': 0.29465449804432853, 'acc_norm_stderr': 0.011643576764069546}, 'hendrycksTest-virology': {'acc': 0.35542168674698793, 'acc_stderr': 0.03726214354322415, 'acc_norm': 0.28313253012048195, 'acc_norm_stderr': 0.03507295431370518}, 'hendrycksTest-machine_learning': {'acc': 0.26785714285714285, 'acc_stderr': 0.04203277291467763, 'acc_norm': 0.25892857142857145, 'acc_norm_stderr': 0.041577515398656284}, 'hendrycksTest-marketing': {'acc': 0.4700854700854701, 'acc_stderr': 0.03269741106812443, 'acc_norm': 0.42735042735042733, 'acc_norm_stderr': 0.03240847393516326}, 'hendrycksTest-us_foreign_policy': {'acc': 0.46, 'acc_stderr': 0.05009082659620332, 'acc_norm': 0.41, 'acc_norm_stderr': 0.049431107042371025}, 'hendrycksTest-high_school_european_history': {'acc': 0.3515151515151515, 'acc_stderr': 0.037282069986826503, 'acc_norm': 0.3151515151515151, 'acc_norm_stderr': 0.0362773057502241}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.2923076923076923, 'acc_stderr': 0.023060438380857744, 'acc_norm': 0.2923076923076923, 'acc_norm_stderr': 0.023060438380857744}, 'hendrycksTest-logical_fallacies': {'acc': 0.26380368098159507, 'acc_stderr': 0.03462419931615622, 'acc_norm': 0.32515337423312884, 'acc_norm_stderr': 0.036803503712864616}, 'hendrycksTest-abstract_algebra': {'acc': 0.21, 'acc_stderr': 0.040936018074033256, 'acc_norm': 0.24, 'acc_norm_stderr': 0.04292346959909283}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.27979274611398963, 'acc_stderr': 0.03239637046735701, 'acc_norm': 0.29533678756476683, 'acc_norm_stderr': 0.03292296639155139}, 'hendrycksTest-moral_disputes': {'acc': 0.2630057803468208, 'acc_stderr': 0.023703099525258165, 'acc_norm': 0.30057803468208094, 'acc_norm_stderr': 0.024685316867257806}, 'hendrycksTest-high_school_world_history': {'acc': 0.28270042194092826, 'acc_stderr': 0.02931281415395592, 'acc_norm': 0.3206751054852321, 'acc_norm_stderr': 0.03038193194999041}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.31092436974789917, 'acc_stderr': 0.030066761582977934, 'acc_norm': 0.3697478991596639, 'acc_norm_stderr': 0.031357095996135904}, 'hendrycksTest-college_medicine': {'acc': 0.2947976878612717, 'acc_stderr': 0.034765996075164785, 'acc_norm': 0.30057803468208094, 'acc_norm_stderr': 0.03496101481191181}, 'hendrycksTest-security_studies': {'acc': 0.43673469387755104, 'acc_stderr': 0.031751952375833226, 'acc_norm': 0.32653061224489793, 'acc_norm_stderr': 0.030021056238440307}, 'hendrycksTest-high_school_geography': {'acc': 0.2777777777777778, 'acc_stderr': 0.03191178226713547, 'acc_norm': 0.31313131313131315, 'acc_norm_stderr': 0.033042050878136525}, 'hendrycksTest-world_religions': {'acc': 0.45614035087719296, 'acc_stderr': 0.038200425866029654, 'acc_norm': 0.4444444444444444, 'acc_norm_stderr': 0.038110796698335316}, 'hendrycksTest-business_ethics': {'acc': 0.34, 'acc_stderr': 0.04760952285695236, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-professional_accounting': {'acc': 0.24468085106382978, 'acc_stderr': 0.025645553622266726, 'acc_norm': 0.28368794326241137, 'acc_norm_stderr': 0.026891709428343957}, 'hendrycksTest-global_facts': {'acc': 0.27, 'acc_stderr': 0.0446196043338474, 'acc_norm': 0.24, 'acc_norm_stderr': 0.04292346959909282}, 'hendrycksTest-high_school_us_history': {'acc': 0.35294117647058826, 'acc_stderr': 0.0335409243759152, 'acc_norm': 0.3284313725490196, 'acc_norm_stderr': 0.032962451101722294}, 'hendrycksTest-professional_psychology': {'acc': 0.2679738562091503, 'acc_stderr': 0.017917974069594726, 'acc_norm': 0.2647058823529412, 'acc_norm_stderr': 0.017848089574913226}, 'hendrycksTest-college_computer_science': {'acc': 0.31, 'acc_stderr': 0.04648231987117316, 'acc_norm': 0.28, 'acc_norm_stderr': 0.04512608598542127}, 'hendrycksTest-philosophy': {'acc': 0.2861736334405145, 'acc_stderr': 0.02567025924218895, 'acc_norm': 0.2990353697749196, 'acc_norm_stderr': 0.02600330111788513}, 'hendrycksTest-anatomy': {'acc': 0.32592592592592595, 'acc_stderr': 0.040491220417025055, 'acc_norm': 0.22962962962962963, 'acc_norm_stderr': 0.03633384414073463}, 'hendrycksTest-nutrition': {'acc': 0.35294117647058826, 'acc_stderr': 0.02736359328468494, 'acc_norm': 0.4411764705882353, 'acc_norm_stderr': 0.028431095444176636}, 'hendrycksTest-computer_security': {'acc': 0.33, 'acc_stderr': 0.04725815626252604, 'acc_norm': 0.32, 'acc_norm_stderr': 0.046882617226215034}, 'hendrycksTest-econometrics': {'acc': 0.2807017543859649, 'acc_stderr': 0.042270544512322004, 'acc_norm': 0.21929824561403508, 'acc_norm_stderr': 0.03892431106518753}, 'hendrycksTest-clinical_knowledge': {'acc': 0.2981132075471698, 'acc_stderr': 0.028152837942493864, 'acc_norm': 0.35471698113207545, 'acc_norm_stderr': 0.029445175328199596}, 'hendrycksTest-high_school_mathematics': {'acc': 0.24814814814814815, 'acc_stderr': 0.0263357394040558, 'acc_norm': 0.28888888888888886, 'acc_norm_stderr': 0.027634907264178544}, 'hendrycksTest-medical_genetics': {'acc': 0.29, 'acc_stderr': 0.04560480215720683, 'acc_norm': 0.34, 'acc_norm_stderr': 0.04760952285695236}, 'hendrycksTest-public_relations': {'acc': 0.2727272727272727, 'acc_stderr': 0.04265792110940589, 'acc_norm': 0.14545454545454545, 'acc_norm_stderr': 0.033768983198330826}, 'hendrycksTest-miscellaneous': {'acc': 0.3486590038314176, 'acc_stderr': 0.017041243143490925, 'acc_norm': 0.2822477650063857, 'acc_norm_stderr': 0.01609530296987855}, 'hendrycksTest-management': {'acc': 0.2815533980582524, 'acc_stderr': 0.044532548363264673, 'acc_norm': 0.33980582524271846, 'acc_norm_stderr': 0.046897659372781335}, 'hendrycksTest-professional_medicine': {'acc': 0.2757352941176471, 'acc_stderr': 0.027146271936625162, 'acc_norm': 0.2867647058823529, 'acc_norm_stderr': 0.02747227447323382}, 'hendrycksTest-high_school_computer_science': {'acc': 0.35, 'acc_stderr': 0.0479372485441102, 'acc_norm': 0.32, 'acc_norm_stderr': 0.046882617226215034}, 'hendrycksTest-high_school_psychology': {'acc': 0.30825688073394497, 'acc_stderr': 0.019798366698367258, 'acc_norm': 0.25321100917431194, 'acc_norm_stderr': 0.018644073041375046}, 'hendrycksTest-high_school_chemistry': {'acc': 0.21674876847290642, 'acc_stderr': 0.02899033125251624, 'acc_norm': 0.270935960591133, 'acc_norm_stderr': 0.031270907132976984}, 'hendrycksTest-college_mathematics': {'acc': 0.17, 'acc_stderr': 0.0377525168068637, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117316}, 'hendrycksTest-high_school_statistics': {'acc': 0.3287037037037037, 'acc_stderr': 0.032036140846700596, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.03214952147802749}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 15:13:37 root] (main_calibration.py 196): INFO Average accuracy 0.2774 - STEM
[2025-03-27 15:13:37 root] (main_calibration.py 196): INFO Average accuracy 0.3086 - humanities
[2025-03-27 15:13:37 root] (main_calibration.py 196): INFO Average accuracy 0.3269 - social sciences
[2025-03-27 15:13:37 root] (main_calibration.py 196): INFO Average accuracy 0.3165 - other (business, health, misc.)
[2025-03-27 15:13:37 root] (main_calibration.py 198): INFO Average accuracy: 0.3045
