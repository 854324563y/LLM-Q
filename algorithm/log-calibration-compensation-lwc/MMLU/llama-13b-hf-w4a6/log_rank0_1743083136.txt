[2025-03-27 13:45:36 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/MMLU/llama-13b-hf-w4a6', save_dir=None, resume='./log-calibration-compensation-lwc/llama-13b-hf-w4a6/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-27 13:51:54 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-27 13:51:54 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-27 13:51:55 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-27 13:52:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-27 13:52:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:52:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-27 13:52:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:52:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-27 13:52:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:52:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-27 13:52:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-27 13:52:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-27 13:52:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-27 13:52:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-27 13:52:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-27 13:52:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-27 13:52:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-27 13:52:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-27 13:52:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-27 13:52:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-27 13:52:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-27 13:52:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-27 13:52:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-27 13:52:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-27 13:52:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-27 13:52:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-27 13:52:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-27 13:52:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-27 13:52:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-27 13:52:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-27 13:52:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-27 13:52:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-27 13:52:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-27 13:52:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-27 13:52:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-27 13:52:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-27 13:52:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-27 13:52:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-27 13:52:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-03-27 13:52:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-03-27 13:52:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-03-27 13:52:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-03-27 13:52:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-03-27 13:52:24 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:52:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-03-27 13:52:25 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:52:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-03-27 13:52:25 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:52:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-03-27 13:52:26 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-27 13:52:26 root] (main_calibration.py 365): INFO 32.641563177108765
[2025-03-27 15:35:53 root] (main_calibration.py 169): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.39669421487603307, 'acc_stderr': 0.044658697805310094, 'acc_norm': 0.5454545454545454, 'acc_norm_stderr': 0.04545454545454548}, 'hendrycksTest-high_school_biology': {'acc': 0.2870967741935484, 'acc_stderr': 0.025736542745594525, 'acc_norm': 0.2967741935483871, 'acc_norm_stderr': 0.025988500792411894}, 'hendrycksTest-college_physics': {'acc': 0.19607843137254902, 'acc_stderr': 0.039505818611799616, 'acc_norm': 0.27450980392156865, 'acc_norm_stderr': 0.044405219061793275}, 'hendrycksTest-college_biology': {'acc': 0.2777777777777778, 'acc_stderr': 0.03745554791462457, 'acc_norm': 0.2569444444444444, 'acc_norm_stderr': 0.03653946969442099}, 'hendrycksTest-electrical_engineering': {'acc': 0.33793103448275863, 'acc_stderr': 0.03941707632064891, 'acc_norm': 0.33793103448275863, 'acc_norm_stderr': 0.03941707632064889}, 'hendrycksTest-high_school_physics': {'acc': 0.304635761589404, 'acc_stderr': 0.03757949922943343, 'acc_norm': 0.2847682119205298, 'acc_norm_stderr': 0.03684881521389023}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2857142857142857, 'acc_stderr': 0.02326651221373057, 'acc_norm': 0.29894179894179895, 'acc_norm_stderr': 0.023577604791655802}, 'hendrycksTest-formal_logic': {'acc': 0.2777777777777778, 'acc_stderr': 0.04006168083848877, 'acc_norm': 0.2857142857142857, 'acc_norm_stderr': 0.04040610178208839}, 'hendrycksTest-human_aging': {'acc': 0.26905829596412556, 'acc_stderr': 0.029763779406874972, 'acc_norm': 0.22869955156950672, 'acc_norm_stderr': 0.028188240046929196}, 'hendrycksTest-jurisprudence': {'acc': 0.3148148148148148, 'acc_stderr': 0.04489931073591312, 'acc_norm': 0.4166666666666667, 'acc_norm_stderr': 0.04766075165356461}, 'hendrycksTest-moral_scenarios': {'acc': 0.24134078212290502, 'acc_stderr': 0.014310999547961459, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.35526315789473684, 'acc_stderr': 0.038947344870133176, 'acc_norm': 0.3815789473684211, 'acc_norm_stderr': 0.03953173377749194}, 'hendrycksTest-conceptual_physics': {'acc': 0.3276595744680851, 'acc_stderr': 0.030683020843231, 'acc_norm': 0.2297872340425532, 'acc_norm_stderr': 0.027501752944412417}, 'hendrycksTest-college_chemistry': {'acc': 0.24, 'acc_stderr': 0.042923469599092816, 'acc_norm': 0.33, 'acc_norm_stderr': 0.04725815626252604}, 'hendrycksTest-human_sexuality': {'acc': 0.4198473282442748, 'acc_stderr': 0.04328577215262972, 'acc_norm': 0.31297709923664124, 'acc_norm_stderr': 0.04066962905677697}, 'hendrycksTest-sociology': {'acc': 0.3034825870646766, 'acc_stderr': 0.032510068164586174, 'acc_norm': 0.2736318407960199, 'acc_norm_stderr': 0.03152439186555401}, 'hendrycksTest-prehistory': {'acc': 0.3148148148148148, 'acc_stderr': 0.025842248700902175, 'acc_norm': 0.25925925925925924, 'acc_norm_stderr': 0.024383665531035454}, 'hendrycksTest-professional_law': {'acc': 0.28683181225554105, 'acc_stderr': 0.011551504781176917, 'acc_norm': 0.303129074315515, 'acc_norm_stderr': 0.011738669951254298}, 'hendrycksTest-virology': {'acc': 0.3433734939759036, 'acc_stderr': 0.03696584317010601, 'acc_norm': 0.29518072289156627, 'acc_norm_stderr': 0.03550920185689631}, 'hendrycksTest-machine_learning': {'acc': 0.30357142857142855, 'acc_stderr': 0.04364226155841044, 'acc_norm': 0.26785714285714285, 'acc_norm_stderr': 0.04203277291467763}, 'hendrycksTest-marketing': {'acc': 0.44871794871794873, 'acc_stderr': 0.032583346493868806, 'acc_norm': 0.38461538461538464, 'acc_norm_stderr': 0.03187195347942466}, 'hendrycksTest-us_foreign_policy': {'acc': 0.45, 'acc_stderr': 0.05, 'acc_norm': 0.43, 'acc_norm_stderr': 0.049756985195624284}, 'hendrycksTest-high_school_european_history': {'acc': 0.3515151515151515, 'acc_stderr': 0.0372820699868265, 'acc_norm': 0.3212121212121212, 'acc_norm_stderr': 0.036462049632538115}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.31025641025641026, 'acc_stderr': 0.0234546748894043, 'acc_norm': 0.27692307692307694, 'acc_norm_stderr': 0.022688042352424994}, 'hendrycksTest-logical_fallacies': {'acc': 0.25766871165644173, 'acc_stderr': 0.03436150827846917, 'acc_norm': 0.34355828220858897, 'acc_norm_stderr': 0.03731133519673893}, 'hendrycksTest-abstract_algebra': {'acc': 0.21, 'acc_stderr': 0.040936018074033256, 'acc_norm': 0.23, 'acc_norm_stderr': 0.04229525846816505}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.35751295336787564, 'acc_stderr': 0.03458816042181005, 'acc_norm': 0.31088082901554404, 'acc_norm_stderr': 0.03340361906276587}, 'hendrycksTest-moral_disputes': {'acc': 0.30057803468208094, 'acc_stderr': 0.024685316867257803, 'acc_norm': 0.31213872832369943, 'acc_norm_stderr': 0.024946792225272314}, 'hendrycksTest-high_school_world_history': {'acc': 0.3628691983122363, 'acc_stderr': 0.03129920825530213, 'acc_norm': 0.32489451476793246, 'acc_norm_stderr': 0.030486039389105296}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.2815126050420168, 'acc_stderr': 0.02921354941437217, 'acc_norm': 0.3403361344537815, 'acc_norm_stderr': 0.03077805742293167}, 'hendrycksTest-college_medicine': {'acc': 0.27167630057803466, 'acc_stderr': 0.03391750322321658, 'acc_norm': 0.23121387283236994, 'acc_norm_stderr': 0.0321473730202947}, 'hendrycksTest-security_studies': {'acc': 0.42448979591836733, 'acc_stderr': 0.03164209487942942, 'acc_norm': 0.3469387755102041, 'acc_norm_stderr': 0.030472526026726496}, 'hendrycksTest-high_school_geography': {'acc': 0.31313131313131315, 'acc_stderr': 0.033042050878136525, 'acc_norm': 0.32323232323232326, 'acc_norm_stderr': 0.033322999210706444}, 'hendrycksTest-world_religions': {'acc': 0.4619883040935672, 'acc_stderr': 0.03823727092882307, 'acc_norm': 0.43859649122807015, 'acc_norm_stderr': 0.038057975055904594}, 'hendrycksTest-business_ethics': {'acc': 0.46, 'acc_stderr': 0.05009082659620333, 'acc_norm': 0.37, 'acc_norm_stderr': 0.048523658709391}, 'hendrycksTest-professional_accounting': {'acc': 0.24113475177304963, 'acc_stderr': 0.025518731049537766, 'acc_norm': 0.2553191489361702, 'acc_norm_stderr': 0.026011992930902006}, 'hendrycksTest-global_facts': {'acc': 0.2, 'acc_stderr': 0.04020151261036845, 'acc_norm': 0.16, 'acc_norm_stderr': 0.0368452949177471}, 'hendrycksTest-high_school_us_history': {'acc': 0.39215686274509803, 'acc_stderr': 0.03426712349247271, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.03308611113236435}, 'hendrycksTest-professional_psychology': {'acc': 0.29248366013071897, 'acc_stderr': 0.01840341571010979, 'acc_norm': 0.27450980392156865, 'acc_norm_stderr': 0.018054027458815194}, 'hendrycksTest-college_computer_science': {'acc': 0.38, 'acc_stderr': 0.04878317312145633, 'acc_norm': 0.28, 'acc_norm_stderr': 0.045126085985421276}, 'hendrycksTest-philosophy': {'acc': 0.2861736334405145, 'acc_stderr': 0.02567025924218894, 'acc_norm': 0.3183279742765273, 'acc_norm_stderr': 0.02645722506781103}, 'hendrycksTest-anatomy': {'acc': 0.34074074074074073, 'acc_stderr': 0.04094376269996794, 'acc_norm': 0.26666666666666666, 'acc_norm_stderr': 0.038201699145179055}, 'hendrycksTest-nutrition': {'acc': 0.3790849673202614, 'acc_stderr': 0.027780141207023323, 'acc_norm': 0.4117647058823529, 'acc_norm_stderr': 0.02818059632825929}, 'hendrycksTest-computer_security': {'acc': 0.27, 'acc_stderr': 0.044619604333847394, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-econometrics': {'acc': 0.2719298245614035, 'acc_stderr': 0.04185774424022056, 'acc_norm': 0.2543859649122807, 'acc_norm_stderr': 0.040969851398436716}, 'hendrycksTest-clinical_knowledge': {'acc': 0.2981132075471698, 'acc_stderr': 0.028152837942493875, 'acc_norm': 0.32075471698113206, 'acc_norm_stderr': 0.02872750295788026}, 'hendrycksTest-high_school_mathematics': {'acc': 0.24444444444444444, 'acc_stderr': 0.026202766534652148, 'acc_norm': 0.3148148148148148, 'acc_norm_stderr': 0.02831753349606647}, 'hendrycksTest-medical_genetics': {'acc': 0.33, 'acc_stderr': 0.047258156262526045, 'acc_norm': 0.35, 'acc_norm_stderr': 0.0479372485441102}, 'hendrycksTest-public_relations': {'acc': 0.2545454545454545, 'acc_stderr': 0.04172343038705383, 'acc_norm': 0.16363636363636364, 'acc_norm_stderr': 0.035434330542986774}, 'hendrycksTest-miscellaneous': {'acc': 0.38569604086845466, 'acc_stderr': 0.017406476619212907, 'acc_norm': 0.28735632183908044, 'acc_norm_stderr': 0.0161824107306827}, 'hendrycksTest-management': {'acc': 0.36893203883495146, 'acc_stderr': 0.04777615181156739, 'acc_norm': 0.3592233009708738, 'acc_norm_stderr': 0.04750458399041696}, 'hendrycksTest-professional_medicine': {'acc': 0.2757352941176471, 'acc_stderr': 0.02714627193662517, 'acc_norm': 0.26838235294117646, 'acc_norm_stderr': 0.02691748122437722}, 'hendrycksTest-high_school_computer_science': {'acc': 0.33, 'acc_stderr': 0.04725815626252604, 'acc_norm': 0.32, 'acc_norm_stderr': 0.04688261722621503}, 'hendrycksTest-high_school_psychology': {'acc': 0.29908256880733947, 'acc_stderr': 0.019630417285415168, 'acc_norm': 0.24954128440366974, 'acc_norm_stderr': 0.018553897629501617}, 'hendrycksTest-high_school_chemistry': {'acc': 0.26108374384236455, 'acc_stderr': 0.030903796952114482, 'acc_norm': 0.32019704433497537, 'acc_norm_stderr': 0.032826493853041504}, 'hendrycksTest-college_mathematics': {'acc': 0.23, 'acc_stderr': 0.04229525846816505, 'acc_norm': 0.32, 'acc_norm_stderr': 0.046882617226215034}, 'hendrycksTest-high_school_statistics': {'acc': 0.32407407407407407, 'acc_stderr': 0.03191923445686185, 'acc_norm': 0.3287037037037037, 'acc_norm_stderr': 0.032036140846700596}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-27 15:35:53 root] (main_calibration.py 196): INFO Average accuracy 0.2870 - STEM
[2025-03-27 15:35:53 root] (main_calibration.py 196): INFO Average accuracy 0.3266 - humanities
[2025-03-27 15:35:53 root] (main_calibration.py 196): INFO Average accuracy 0.3315 - social sciences
[2025-03-27 15:35:53 root] (main_calibration.py 196): INFO Average accuracy 0.3294 - other (business, health, misc.)
[2025-03-27 15:35:53 root] (main_calibration.py 198): INFO Average accuracy: 0.3158
