[2025-03-09 10:20:56 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-7b-hf-w4a7', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-09 10:23:03 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-09 10:23:03 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-09 10:23:04 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-09 10:23:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-09 10:23:08 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:23:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.015554429963231087 norm:0.01704760082066059 max memory_allocated 22559.10693359375 
[2025-03-09 10:24:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.008864453993737698 norm:0.00837983563542366 max memory_allocated 22559.10693359375 
[2025-03-09 10:24:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.006366616114974022 norm:0.005305313505232334 max memory_allocated 22559.10693359375 
[2025-03-09 10:25:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0055639129132032394 norm:0.004265854135155678 max memory_allocated 22559.10693359375 
[2025-03-09 10:25:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.005216033197939396 norm:0.0034911043476313353 max memory_allocated 22559.10693359375 
[2025-03-09 10:26:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0049819378182291985 norm:0.0029939974192529917 max memory_allocated 22559.10693359375 
[2025-03-09 10:26:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.004943458363413811 norm:0.0026913376059383154 max memory_allocated 22559.10693359375 
[2025-03-09 10:27:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.004829700104892254 norm:0.002352100098505616 max memory_allocated 22559.10693359375 
[2025-03-09 10:27:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.004727269522845745 norm:0.0019983339589089155 max memory_allocated 22559.10693359375 
[2025-03-09 10:28:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.004676996264606714 norm:0.0018211728893220425 max memory_allocated 22559.10693359375 
[2025-03-09 10:28:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.004593154415488243 norm:0.0015987425576895475 max memory_allocated 22559.10693359375 
[2025-03-09 10:29:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.004626208916306496 norm:0.0014864132972434163 max memory_allocated 22559.10693359375 
[2025-03-09 10:29:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.004531809128820896 norm:0.0013104223180562258 max memory_allocated 22559.10693359375 
[2025-03-09 10:30:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.004546285606920719 norm:0.0012331627076491714 max memory_allocated 22559.10693359375 
[2025-03-09 10:30:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.004481045994907618 norm:0.001033546170219779 max memory_allocated 22559.10693359375 
[2025-03-09 10:31:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0044957054778933525 norm:0.0010051869321614504 max memory_allocated 22559.10693359375 
[2025-03-09 10:31:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0044930484145879745 norm:0.0009705887641757727 max memory_allocated 22559.10693359375 
[2025-03-09 10:32:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0044564553536474705 norm:0.0009113455889746547 max memory_allocated 22559.10693359375 
[2025-03-09 10:32:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.004468549508601427 norm:0.0009224035311490297 max memory_allocated 22559.10693359375 
[2025-03-09 10:33:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.004369538743048906 norm:0.0007800614112056792 max memory_allocated 22559.10693359375 
[2025-03-09 10:33:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-09 10:33:34 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:34:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.026385128498077393 norm:0.020442571491003036 max memory_allocated 22559.27880859375 
[2025-03-09 10:34:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.016382375732064247 norm:0.0131176533177495 max memory_allocated 22559.27880859375 
[2025-03-09 10:35:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.012295003980398178 norm:0.00758882611989975 max memory_allocated 22559.27880859375 
[2025-03-09 10:35:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.011140422895550728 norm:0.00546093238517642 max memory_allocated 22559.27880859375 
[2025-03-09 10:36:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.010738424956798553 norm:0.004771484062075615 max memory_allocated 22559.27880859375 
[2025-03-09 10:36:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.010420267470180988 norm:0.004187481477856636 max memory_allocated 22559.27880859375 
[2025-03-09 10:37:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.010188097134232521 norm:0.0038101929239928722 max memory_allocated 22559.27880859375 
[2025-03-09 10:37:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.010005546733736992 norm:0.003545645158737898 max memory_allocated 22559.27880859375 
[2025-03-09 10:38:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.009842177852988243 norm:0.003239806741476059 max memory_allocated 22559.27880859375 
[2025-03-09 10:38:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.009747004136443138 norm:0.002932608360424638 max memory_allocated 22559.27880859375 
[2025-03-09 10:39:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.00964453537017107 norm:0.002676954260095954 max memory_allocated 22559.27880859375 
[2025-03-09 10:39:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.009573755785822868 norm:0.002466777805238962 max memory_allocated 22559.27880859375 
[2025-03-09 10:40:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.009516892023384571 norm:0.0022490688133984804 max memory_allocated 22559.27880859375 
[2025-03-09 10:40:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.009489296935498714 norm:0.0020540114492177963 max memory_allocated 22559.27880859375 
[2025-03-09 10:41:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.009449819102883339 norm:0.0018503890605643392 max memory_allocated 22559.27880859375 
[2025-03-09 10:41:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.00944209098815918 norm:0.0016730487113818526 max memory_allocated 22559.27880859375 
[2025-03-09 10:42:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.009425515308976173 norm:0.0014843018725514412 max memory_allocated 22559.27880859375 
[2025-03-09 10:42:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.009415755979716778 norm:0.0013707278994843364 max memory_allocated 22559.27880859375 
[2025-03-09 10:43:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0094196991994977 norm:0.001335800508968532 max memory_allocated 22559.27880859375 
[2025-03-09 10:43:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.009443869814276695 norm:0.0014217705465853214 max memory_allocated 22559.27880859375 
[2025-03-09 10:43:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-09 10:44:01 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:44:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.02829950861632824 norm:0.010212166234850883 max memory_allocated 22559.45068359375 
[2025-03-09 10:45:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.02472665347158909 norm:0.008452752605080605 max memory_allocated 22559.45068359375 
[2025-03-09 10:45:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.02062859758734703 norm:0.0066942088305950165 max memory_allocated 22559.45068359375 
[2025-03-09 10:46:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.019533773884177208 norm:0.006317149382084608 max memory_allocated 22559.45068359375 
[2025-03-09 10:46:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.01877669431269169 norm:0.005331569816917181 max memory_allocated 22559.45068359375 
[2025-03-09 10:47:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.01833435893058777 norm:0.004788764286786318 max memory_allocated 22559.45068359375 
[2025-03-09 10:47:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.018368273973464966 norm:0.004634183365851641 max memory_allocated 22559.45068359375 
[2025-03-09 10:48:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.01815277710556984 norm:0.004621228203177452 max memory_allocated 22559.45068359375 
[2025-03-09 10:48:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.017857644706964493 norm:0.004299997352063656 max memory_allocated 22559.45068359375 
[2025-03-09 10:49:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.017852533608675003 norm:0.004204686731100082 max memory_allocated 22559.45068359375 
[2025-03-09 10:49:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.017904333770275116 norm:0.004084113519638777 max memory_allocated 22559.45068359375 
[2025-03-09 10:50:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.017802048474550247 norm:0.003815260250121355 max memory_allocated 22559.45068359375 
[2025-03-09 10:50:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.017940040677785873 norm:0.004041067324578762 max memory_allocated 22559.45068359375 
[2025-03-09 10:51:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.01798630878329277 norm:0.003940763417631388 max memory_allocated 22559.45068359375 
[2025-03-09 10:51:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.017941914498806 norm:0.003578401170670986 max memory_allocated 22559.45068359375 
[2025-03-09 10:52:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.018382946029305458 norm:0.005568812135607004 max memory_allocated 22559.45068359375 
[2025-03-09 10:52:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.017971262335777283 norm:0.004378120414912701 max memory_allocated 22559.45068359375 
[2025-03-09 10:53:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.017865389585494995 norm:0.003948192577809095 max memory_allocated 22559.45068359375 
[2025-03-09 10:53:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.01790754124522209 norm:0.0038606575690209866 max memory_allocated 22559.45068359375 
[2025-03-09 10:54:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.018442606553435326 norm:0.004466290585696697 max memory_allocated 22559.45068359375 
[2025-03-09 10:54:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-09 10:54:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.034865330904722214 norm:0.005241917911916971 max memory_allocated 22559.50732421875 
[2025-03-09 10:55:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.025608528405427933 norm:0.0012416125973686576 max memory_allocated 22559.50732421875 
[2025-03-09 10:56:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.02180190198123455 norm:0.000651308975648135 max memory_allocated 22559.50732421875 
[2025-03-09 10:56:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.02042805217206478 norm:0.0004489199200179428 max memory_allocated 22559.50732421875 
[2025-03-09 10:57:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.01966390386223793 norm:0.0003491208772175014 max memory_allocated 22559.50732421875 
[2025-03-09 10:57:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.01911430060863495 norm:0.0002986751205753535 max memory_allocated 22559.50732421875 
[2025-03-09 10:58:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.01890414021909237 norm:0.00026855041505768895 max memory_allocated 22559.50732421875 
[2025-03-09 10:58:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.018819469958543777 norm:0.00023346557281911373 max memory_allocated 22559.50732421875 
[2025-03-09 10:59:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.018774423748254776 norm:0.00020709300588350743 max memory_allocated 22559.50732421875 
[2025-03-09 10:59:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.01871057227253914 norm:0.0001786567736417055 max memory_allocated 22559.50732421875 
[2025-03-09 11:00:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.01869140937924385 norm:0.00016379587759729475 max memory_allocated 22559.50732421875 
[2025-03-09 11:00:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.01867242157459259 norm:0.00014813439338468015 max memory_allocated 22559.50732421875 
[2025-03-09 11:01:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.01866421476006508 norm:0.00013672983914148062 max memory_allocated 22559.50732421875 
[2025-03-09 11:01:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.01866406388580799 norm:0.0001326438068645075 max memory_allocated 22559.50732421875 
[2025-03-09 11:02:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.01862964779138565 norm:0.00012834662629757077 max memory_allocated 22559.50732421875 
[2025-03-09 11:02:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.01864006742835045 norm:0.0001298158458666876 max memory_allocated 22559.50732421875 
[2025-03-09 11:03:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.018674537539482117 norm:0.00012578383029904217 max memory_allocated 22559.50732421875 
[2025-03-09 11:03:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.01864878460764885 norm:0.00011814334720838815 max memory_allocated 22559.50732421875 
[2025-03-09 11:04:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.01867358759045601 norm:0.00012639714987017214 max memory_allocated 22559.50732421875 
[2025-03-09 11:04:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0186636783182621 norm:0.00011655808339128271 max memory_allocated 22559.50732421875 
[2025-03-09 11:04:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-09 11:05:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.045264095067977905 norm:0.005275450646877289 max memory_allocated 22559.67919921875 
[2025-03-09 11:05:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.03325380012392998 norm:0.0013487895485013723 max memory_allocated 22559.67919921875 
[2025-03-09 11:06:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.027185019105672836 norm:0.0006740348180755973 max memory_allocated 22559.67919921875 
[2025-03-09 11:06:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.02514675073325634 norm:0.0004923857632093132 max memory_allocated 22559.67919921875 
[2025-03-09 11:07:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.024199850857257843 norm:0.00041636405512690544 max memory_allocated 22559.67919921875 
[2025-03-09 11:07:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.023665042594075203 norm:0.0003562146157491952 max memory_allocated 22559.67919921875 
[2025-03-09 11:08:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.023403344675898552 norm:0.0003130972618237138 max memory_allocated 22559.67919921875 
[2025-03-09 11:08:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.023280467838048935 norm:0.00031034869607537985 max memory_allocated 22559.67919921875 
[2025-03-09 11:09:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.023214709013700485 norm:0.00026155158411711454 max memory_allocated 22559.67919921875 
[2025-03-09 11:10:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.02313976176083088 norm:0.00024559019948355854 max memory_allocated 22559.67919921875 
[2025-03-09 11:10:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.023073658347129822 norm:0.0002321659412700683 max memory_allocated 22559.67919921875 
[2025-03-09 11:11:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.02304038219153881 norm:0.0002199791488237679 max memory_allocated 22559.67919921875 
[2025-03-09 11:11:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.023034855723381042 norm:0.00022772418742533773 max memory_allocated 22559.67919921875 
[2025-03-09 11:12:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.02297879569232464 norm:0.0002121433790307492 max memory_allocated 22559.67919921875 
[2025-03-09 11:12:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.022971095517277718 norm:0.00020717456936836243 max memory_allocated 22559.67919921875 
[2025-03-09 11:13:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.02301744744181633 norm:0.00021941073646303266 max memory_allocated 22559.67919921875 
[2025-03-09 11:13:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.02301720343530178 norm:0.0002165962796425447 max memory_allocated 22559.67919921875 
[2025-03-09 11:14:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.023086808621883392 norm:0.00021630195260513574 max memory_allocated 22559.67919921875 
[2025-03-09 11:14:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.023078154772520065 norm:0.0002149287611246109 max memory_allocated 22559.67919921875 
[2025-03-09 11:15:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.02301272749900818 norm:0.00021495320834219456 max memory_allocated 22559.67919921875 
[2025-03-09 11:15:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-09 11:15:50 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.053214751183986664 norm:0.007275338750332594 max memory_allocated 22559.85107421875 
[2025-03-09 11:16:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.03761124983429909 norm:0.0014692206168547273 max memory_allocated 22559.85107421875 
[2025-03-09 11:16:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.03077717311680317 norm:0.0006111774127930403 max memory_allocated 22559.85107421875 
[2025-03-09 11:17:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.028724297881126404 norm:0.0004882696084678173 max memory_allocated 22559.85107421875 
[2025-03-09 11:17:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.027653273195028305 norm:0.00043057091534137726 max memory_allocated 22559.85107421875 
[2025-03-09 11:18:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.027007371187210083 norm:0.00036569885560311377 max memory_allocated 22559.85107421875 
[2025-03-09 11:18:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.026681412011384964 norm:0.0003410216886550188 max memory_allocated 22559.85107421875 
[2025-03-09 11:19:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.026494916528463364 norm:0.00031904352363198996 max memory_allocated 22559.85107421875 
[2025-03-09 11:19:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.02637391723692417 norm:0.0002677809679880738 max memory_allocated 22559.85107421875 
[2025-03-09 11:20:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.02637982740998268 norm:0.00024920047144405544 max memory_allocated 22559.85107421875 
[2025-03-09 11:20:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.026267260313034058 norm:0.00022757418628316373 max memory_allocated 22559.85107421875 
[2025-03-09 11:21:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.026134442538022995 norm:0.00020940127433277667 max memory_allocated 22559.85107421875 
[2025-03-09 11:21:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.026088155806064606 norm:0.00019698665710166097 max memory_allocated 22559.85107421875 
[2025-03-09 11:22:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.026081888005137444 norm:0.00019018366583622992 max memory_allocated 22559.85107421875 
[2025-03-09 11:23:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.026088183745741844 norm:0.00018555654969532043 max memory_allocated 22559.85107421875 
[2025-03-09 11:23:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.026044733822345734 norm:0.0001901621581055224 max memory_allocated 22559.85107421875 
[2025-03-09 11:24:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.026013227179646492 norm:0.00018623295181896538 max memory_allocated 22559.85107421875 
[2025-03-09 11:24:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.026034533977508545 norm:0.00017748857499100268 max memory_allocated 22559.85107421875 
[2025-03-09 11:25:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.02604464441537857 norm:0.0001757298450684175 max memory_allocated 22559.85107421875 
[2025-03-09 11:25:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.026066895574331284 norm:0.00016407776274718344 max memory_allocated 22559.85107421875 
[2025-03-09 11:25:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-09 11:26:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.062301453202962875 norm:0.013336469419300556 max memory_allocated 22560.02294921875 
[2025-03-09 11:26:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.043173059821128845 norm:0.002845222130417824 max memory_allocated 22560.02294921875 
[2025-03-09 11:27:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.03461914137005806 norm:0.0006991573609411716 max memory_allocated 22560.02294921875 
[2025-03-09 11:27:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.032350122928619385 norm:0.0005153233068995178 max memory_allocated 22560.02294921875 
[2025-03-09 11:28:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.03119097650051117 norm:0.0004267003678251058 max memory_allocated 22560.02294921875 
[2025-03-09 11:28:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.030539100989699364 norm:0.00035479740472510457 max memory_allocated 22560.02294921875 
[2025-03-09 11:29:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.03018932230770588 norm:0.0003603118530008942 max memory_allocated 22560.02294921875 
[2025-03-09 11:29:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.029978282749652863 norm:0.00030271412106230855 max memory_allocated 22560.02294921875 
[2025-03-09 11:30:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0298064723610878 norm:0.0002780693175736815 max memory_allocated 22560.02294921875 
[2025-03-09 11:30:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.02975853718817234 norm:0.00027732111630029976 max memory_allocated 22560.02294921875 
[2025-03-09 11:31:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.029718399047851562 norm:0.00027855043299496174 max memory_allocated 22560.02294921875 
[2025-03-09 11:31:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.029692906886339188 norm:0.0002475200453773141 max memory_allocated 22560.02294921875 
[2025-03-09 11:32:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.029609065502882004 norm:0.00022259316756390035 max memory_allocated 22560.02294921875 
[2025-03-09 11:32:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0296182818710804 norm:0.0002152170054614544 max memory_allocated 22560.02294921875 
[2025-03-09 11:33:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.029573997482657433 norm:0.0002113257796736434 max memory_allocated 22560.02294921875 
[2025-03-09 11:33:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.029507772997021675 norm:0.00020995913655497134 max memory_allocated 22560.02294921875 
[2025-03-09 11:34:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.029467245563864708 norm:0.00020722243061754853 max memory_allocated 22560.02294921875 
[2025-03-09 11:34:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.02947305142879486 norm:0.00018969582743011415 max memory_allocated 22560.02294921875 
[2025-03-09 11:35:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.029524419456720352 norm:0.0001971628371393308 max memory_allocated 22560.02294921875 
[2025-03-09 11:35:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.029510224238038063 norm:0.0001895115419756621 max memory_allocated 22560.02294921875 
[2025-03-09 11:36:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-09 11:36:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.06526987254619598 norm:0.007834489457309246 max memory_allocated 22560.19482421875 
[2025-03-09 11:37:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.04770307615399361 norm:0.0018319857772439718 max memory_allocated 22560.19482421875 
[2025-03-09 11:37:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.038445957005023956 norm:0.0005724240327253938 max memory_allocated 22560.19482421875 
[2025-03-09 11:38:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.03583534061908722 norm:0.0003925224009435624 max memory_allocated 22560.19482421875 
[2025-03-09 11:38:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.034691259264945984 norm:0.00034028443042188883 max memory_allocated 22560.19482421875 
[2025-03-09 11:39:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.03408912941813469 norm:0.0002895272336900234 max memory_allocated 22560.19482421875 
[2025-03-09 11:39:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.03378710523247719 norm:0.00028121902141720057 max memory_allocated 22560.19482421875 
[2025-03-09 11:40:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.03359496593475342 norm:0.000274202466243878 max memory_allocated 22560.19482421875 
[2025-03-09 11:40:45 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.03348029777407646 norm:0.00024786789435893297 max memory_allocated 22560.19482421875 
[2025-03-09 11:41:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.03341823071241379 norm:0.00024243653751909733 max memory_allocated 22560.19482421875 
[2025-03-09 11:41:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.03337569907307625 norm:0.00022897175222169608 max memory_allocated 22560.19482421875 
[2025-03-09 11:42:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.033291857689619064 norm:0.00023555099323857576 max memory_allocated 22560.19482421875 
[2025-03-09 11:42:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.033277641981840134 norm:0.0002097324759233743 max memory_allocated 22560.19482421875 
[2025-03-09 11:43:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.03322197124361992 norm:0.00019693929061759263 max memory_allocated 22560.19482421875 
[2025-03-09 11:43:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.033197540789842606 norm:0.00018550657841842622 max memory_allocated 22560.19482421875 
[2025-03-09 11:44:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.03315335512161255 norm:0.0001794000854715705 max memory_allocated 22560.19482421875 
[2025-03-09 11:44:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.03316187858581543 norm:0.00017823412781581283 max memory_allocated 22560.19482421875 
[2025-03-09 11:45:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.03313199430704117 norm:0.00016802184109110385 max memory_allocated 22560.19482421875 
[2025-03-09 11:45:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.03311820700764656 norm:0.00017804164963308722 max memory_allocated 22560.19482421875 
[2025-03-09 11:46:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.033113181591033936 norm:0.000174914559465833 max memory_allocated 22560.19482421875 
[2025-03-09 11:46:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-09 11:47:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.07074439525604248 norm:0.006041021551936865 max memory_allocated 22560.36669921875 
[2025-03-09 11:47:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.051589369773864746 norm:0.001528621418401599 max memory_allocated 22560.36669921875 
[2025-03-09 11:48:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.04208614304661751 norm:0.0004952903254888952 max memory_allocated 22560.36669921875 
[2025-03-09 11:48:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.03931789472699165 norm:0.00035436885082162917 max memory_allocated 22560.36669921875 
[2025-03-09 11:49:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.03794746845960617 norm:0.0003070430539082736 max memory_allocated 22560.36669921875 
[2025-03-09 11:49:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.03731601685285568 norm:0.0002757296315394342 max memory_allocated 22560.36669921875 
[2025-03-09 11:50:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.036990005522966385 norm:0.000251617981120944 max memory_allocated 22560.36669921875 
[2025-03-09 11:50:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.03671355918049812 norm:0.000235823099501431 max memory_allocated 22560.36669921875 
[2025-03-09 11:51:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.036604758352041245 norm:0.00022234565403778106 max memory_allocated 22560.36669921875 
[2025-03-09 11:51:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.03647598996758461 norm:0.0002257026790175587 max memory_allocated 22560.36669921875 
[2025-03-09 11:52:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.03643939644098282 norm:0.00021430171909742057 max memory_allocated 22560.36669921875 
[2025-03-09 11:52:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.03631056100130081 norm:0.00020257080905139446 max memory_allocated 22560.36669921875 
[2025-03-09 11:53:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.03625495359301567 norm:0.0001929425197886303 max memory_allocated 22560.36669921875 
[2025-03-09 11:53:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.03625063598155975 norm:0.00019828909717034549 max memory_allocated 22560.36669921875 
[2025-03-09 11:54:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.03629698604345322 norm:0.00020334527653176337 max memory_allocated 22560.36669921875 
[2025-03-09 11:54:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.03628500923514366 norm:0.00019196965149603784 max memory_allocated 22560.36669921875 
[2025-03-09 11:55:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.036243945360183716 norm:0.00017900487000588328 max memory_allocated 22560.36669921875 
[2025-03-09 11:55:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.03620826452970505 norm:0.00017653433314990252 max memory_allocated 22560.36669921875 
[2025-03-09 11:56:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.03622893989086151 norm:0.0001832335110520944 max memory_allocated 22560.36669921875 
[2025-03-09 11:56:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.036197416484355927 norm:0.000182396819582209 max memory_allocated 22560.36669921875 
[2025-03-09 11:56:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-09 11:57:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.07608532905578613 norm:0.00645040487870574 max memory_allocated 22560.53857421875 
[2025-03-09 11:57:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.056247491389513016 norm:0.0017300089821219444 max memory_allocated 22560.53857421875 
[2025-03-09 11:58:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.04619516059756279 norm:0.0006000623106956482 max memory_allocated 22560.53857421875 
[2025-03-09 11:59:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.04312027618288994 norm:0.0004014459263999015 max memory_allocated 22560.53857421875 
[2025-03-09 11:59:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.04178599268198013 norm:0.00034998590126633644 max memory_allocated 22560.53857421875 
[2025-03-09 12:00:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.04106330871582031 norm:0.0003272460016887635 max memory_allocated 22560.53857421875 
[2025-03-09 12:00:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.040747158229351044 norm:0.00031249463791027665 max memory_allocated 22560.53857421875 
[2025-03-09 12:01:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.040431633591651917 norm:0.00025178163195960224 max memory_allocated 22560.53857421875 
[2025-03-09 12:01:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.04023043066263199 norm:0.0002253634884255007 max memory_allocated 22560.53857421875 
[2025-03-09 12:02:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.04007524624466896 norm:0.00021335032943170518 max memory_allocated 22560.53857421875 
[2025-03-09 12:02:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.04001079127192497 norm:0.00021679805649910122 max memory_allocated 22560.53857421875 
[2025-03-09 12:03:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.039998702704906464 norm:0.00020428228890523314 max memory_allocated 22560.53857421875 
[2025-03-09 12:03:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.03990129753947258 norm:0.00018119934247806668 max memory_allocated 22560.53857421875 
[2025-03-09 12:04:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.03991232067346573 norm:0.00017907549045048654 max memory_allocated 22560.53857421875 
[2025-03-09 12:04:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.03989285230636597 norm:0.00017747039964888245 max memory_allocated 22560.53857421875 
[2025-03-09 12:05:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.03976711630821228 norm:0.00016529409913346171 max memory_allocated 22560.53857421875 
[2025-03-09 12:05:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.03980397433042526 norm:0.00017817171465139836 max memory_allocated 22560.53857421875 
[2025-03-09 12:06:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.03976445645093918 norm:0.00016740938008297235 max memory_allocated 22560.53857421875 
[2025-03-09 12:06:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.039728615432977676 norm:0.0001652911741985008 max memory_allocated 22560.53857421875 
[2025-03-09 12:07:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.039731014519929886 norm:0.00015927475760690868 max memory_allocated 22560.53857421875 
[2025-03-09 12:07:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-09 12:07:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.06786427646875381 norm:0.0038644722662866116 max memory_allocated 22560.71044921875 
[2025-03-09 12:08:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.05603443458676338 norm:0.001353998901322484 max memory_allocated 22560.71044921875 
[2025-03-09 12:08:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.04784774035215378 norm:0.0004691902140621096 max memory_allocated 22560.71044921875 
[2025-03-09 12:09:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.04519754275679588 norm:0.00031612266320735216 max memory_allocated 22560.71044921875 
[2025-03-09 12:09:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.04403296485543251 norm:0.00025262360577471554 max memory_allocated 22560.71044921875 
[2025-03-09 12:10:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.04351327568292618 norm:0.0002389799919910729 max memory_allocated 22560.71044921875 
[2025-03-09 12:11:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.04325183108448982 norm:0.00020044219854753464 max memory_allocated 22560.71044921875 
[2025-03-09 12:11:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.04309713467955589 norm:0.00018808382446877658 max memory_allocated 22560.71044921875 
[2025-03-09 12:12:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.043004557490348816 norm:0.0001883654622361064 max memory_allocated 22560.71044921875 
[2025-03-09 12:12:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.042932264506816864 norm:0.00017058683442883193 max memory_allocated 22560.71044921875 
[2025-03-09 12:13:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.04289057478308678 norm:0.0001593099586898461 max memory_allocated 22560.71044921875 
[2025-03-09 12:13:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.04287440702319145 norm:0.00015157742018345743 max memory_allocated 22560.71044921875 
[2025-03-09 12:14:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.04278971999883652 norm:0.00013490454875864089 max memory_allocated 22560.71044921875 
[2025-03-09 12:14:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.04270936921238899 norm:0.00012735866766888648 max memory_allocated 22560.71044921875 
[2025-03-09 12:15:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.04273773729801178 norm:0.00013156997738406062 max memory_allocated 22560.71044921875 
[2025-03-09 12:15:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.04279182106256485 norm:0.00013476802268996835 max memory_allocated 22560.71044921875 
[2025-03-09 12:16:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.04284108802676201 norm:0.00013675805530510843 max memory_allocated 22560.71044921875 
[2025-03-09 12:16:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.04280387610197067 norm:0.00012914590479340404 max memory_allocated 22560.71044921875 
[2025-03-09 12:17:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.042797740548849106 norm:0.00012509324005804956 max memory_allocated 22560.71044921875 
[2025-03-09 12:17:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.04281868785619736 norm:0.0001259857672266662 max memory_allocated 22560.71044921875 
[2025-03-09 12:17:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-09 12:18:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.07224620878696442 norm:0.0057734232395887375 max memory_allocated 22560.88232421875 
[2025-03-09 12:18:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.058940332382917404 norm:0.001934103318490088 max memory_allocated 22560.88232421875 
[2025-03-09 12:19:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.049958281219005585 norm:0.0005464547430165112 max memory_allocated 22560.88232421875 
[2025-03-09 12:19:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.04756128042936325 norm:0.0003969618701376021 max memory_allocated 22560.88232421875 
[2025-03-09 12:20:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.046539124101400375 norm:0.00032824857044033706 max memory_allocated 22560.88232421875 
[2025-03-09 12:20:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.04597156122326851 norm:0.00026294298004359007 max memory_allocated 22560.88232421875 
[2025-03-09 12:21:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.04567926377058029 norm:0.0002451130421832204 max memory_allocated 22560.88232421875 
[2025-03-09 12:21:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.04548395052552223 norm:0.00022365724726114422 max memory_allocated 22560.88232421875 
[2025-03-09 12:22:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.04536070674657822 norm:0.00019403843907639384 max memory_allocated 22560.88232421875 
[2025-03-09 12:22:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.045207422226667404 norm:0.0001717897830531001 max memory_allocated 22560.88232421875 
[2025-03-09 12:23:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.04516003653407097 norm:0.00017601848230697215 max memory_allocated 22560.88232421875 
[2025-03-09 12:23:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0451691597700119 norm:0.00016638915985822678 max memory_allocated 22560.88232421875 
[2025-03-09 12:24:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.04515336453914642 norm:0.000150688283611089 max memory_allocated 22560.88232421875 
[2025-03-09 12:25:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.04511639103293419 norm:0.0001501065562479198 max memory_allocated 22560.88232421875 
[2025-03-09 12:25:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0450904481112957 norm:0.0001479045458836481 max memory_allocated 22560.88232421875 
[2025-03-09 12:26:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.045084744691848755 norm:0.00013868194946553558 max memory_allocated 22560.88232421875 
[2025-03-09 12:26:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.04504139721393585 norm:0.000132862085592933 max memory_allocated 22560.88232421875 
[2025-03-09 12:27:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.045080747455358505 norm:0.00013882659550290555 max memory_allocated 22560.88232421875 
[2025-03-09 12:27:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.04507548362016678 norm:0.00013252194912638515 max memory_allocated 22560.88232421875 
[2025-03-09 12:28:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.045072250068187714 norm:0.0001300366420764476 max memory_allocated 22560.88232421875 
[2025-03-09 12:28:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-09 12:28:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.07348993420600891 norm:0.0035073142498731613 max memory_allocated 22561.05419921875 
[2025-03-09 12:29:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.06147744134068489 norm:0.0012523215264081955 max memory_allocated 22561.05419921875 
[2025-03-09 12:29:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.05323513597249985 norm:0.0005110568599775434 max memory_allocated 22561.05419921875 
[2025-03-09 12:30:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.050493866205215454 norm:0.00035510805901139975 max memory_allocated 22561.05419921875 
[2025-03-09 12:30:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.04929118975996971 norm:0.0002721658383961767 max memory_allocated 22561.05419921875 
[2025-03-09 12:31:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.048632293939590454 norm:0.00024168213712982833 max memory_allocated 22561.05419921875 
[2025-03-09 12:31:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0482977032661438 norm:0.0002139199204975739 max memory_allocated 22561.05419921875 
[2025-03-09 12:32:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.04817158728837967 norm:0.00019777902343776077 max memory_allocated 22561.05419921875 
[2025-03-09 12:32:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.048027653247117996 norm:0.00017164273594971746 max memory_allocated 22561.05419921875 
[2025-03-09 12:33:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0478922463953495 norm:0.00015167694073170424 max memory_allocated 22561.05419921875 
[2025-03-09 12:33:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.04783576726913452 norm:0.0001475520693929866 max memory_allocated 22561.05419921875 
[2025-03-09 12:34:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.04779471457004547 norm:0.00014034024206921458 max memory_allocated 22561.05419921875 
[2025-03-09 12:34:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.04772626608610153 norm:0.0001365705393254757 max memory_allocated 22561.05419921875 
[2025-03-09 12:35:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.047666847705841064 norm:0.00012680482177529484 max memory_allocated 22561.05419921875 
[2025-03-09 12:35:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.047657012939453125 norm:0.0001219974146806635 max memory_allocated 22561.05419921875 
[2025-03-09 12:36:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.04762328043580055 norm:0.0001199909092974849 max memory_allocated 22561.05419921875 
[2025-03-09 12:36:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0476483479142189 norm:0.0001219809491885826 max memory_allocated 22561.05419921875 
[2025-03-09 12:37:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.04762044548988342 norm:0.00011977148096775636 max memory_allocated 22561.05419921875 
[2025-03-09 12:38:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.047605276107788086 norm:0.00011866507702507079 max memory_allocated 22561.05419921875 
[2025-03-09 12:38:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.04760296270251274 norm:0.00012043994502164423 max memory_allocated 22561.05419921875 
[2025-03-09 12:38:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-09 12:39:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.07605370879173279 norm:0.00396449351683259 max memory_allocated 22561.22607421875 
[2025-03-09 12:39:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.06346449255943298 norm:0.001355857471935451 max memory_allocated 22561.22607421875 
[2025-03-09 12:40:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.05525395646691322 norm:0.0004897923208773136 max memory_allocated 22561.22607421875 
[2025-03-09 12:40:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.05279059335589409 norm:0.00031449919333681464 max memory_allocated 22561.22607421875 
[2025-03-09 12:41:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.05162061005830765 norm:0.00025233981432393193 max memory_allocated 22561.22607421875 
[2025-03-09 12:41:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.05095917731523514 norm:0.00021621002815663815 max memory_allocated 22561.22607421875 
[2025-03-09 12:42:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.05061524733901024 norm:0.00021025846945121884 max memory_allocated 22561.22607421875 
[2025-03-09 12:42:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.05044794827699661 norm:0.00019957503536716104 max memory_allocated 22561.22607421875 
[2025-03-09 12:43:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.05033186450600624 norm:0.000168115075211972 max memory_allocated 22561.22607421875 
[2025-03-09 12:43:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.050201281905174255 norm:0.00015419382543768734 max memory_allocated 22561.22607421875 
[2025-03-09 12:44:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0500931516289711 norm:0.00013050493726041168 max memory_allocated 22561.22607421875 
[2025-03-09 12:44:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.050080642104148865 norm:0.00013154525368008763 max memory_allocated 22561.22607421875 
[2025-03-09 12:45:21 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.050008077174425125 norm:0.00011901924881385639 max memory_allocated 22561.22607421875 
[2025-03-09 12:45:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.049960557371377945 norm:0.00011281939077889547 max memory_allocated 22561.22607421875 
[2025-03-09 12:46:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.04995613545179367 norm:0.00011274813732597977 max memory_allocated 22561.22607421875 
[2025-03-09 12:46:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.049935899674892426 norm:0.00011398805509088561 max memory_allocated 22561.22607421875 
[2025-03-09 12:47:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.04991519823670387 norm:0.00010999220830854028 max memory_allocated 22561.22607421875 
[2025-03-09 12:47:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.049932800233364105 norm:0.00010985491826431826 max memory_allocated 22561.22607421875 
[2025-03-09 12:48:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.049904223531484604 norm:0.00010991317685693502 max memory_allocated 22561.22607421875 
[2025-03-09 12:48:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.04991593211889267 norm:0.00011113767686765641 max memory_allocated 22561.22607421875 
[2025-03-09 12:49:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-09 12:49:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.0856175571680069 norm:0.005526601802557707 max memory_allocated 22561.39794921875 
[2025-03-09 12:50:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.06966304779052734 norm:0.0016637323424220085 max memory_allocated 22561.39794921875 
[2025-03-09 12:50:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.060237567871809006 norm:0.0006424949388019741 max memory_allocated 22561.39794921875 
[2025-03-09 12:51:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.057156436145305634 norm:0.00040543556679040194 max memory_allocated 22561.39794921875 
[2025-03-09 12:51:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.055847134441137314 norm:0.00035585204022936523 max memory_allocated 22561.39794921875 
[2025-03-09 12:52:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.05520416796207428 norm:0.00031484858482144773 max memory_allocated 22561.39794921875 
[2025-03-09 12:52:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0548882819712162 norm:0.0002853480400517583 max memory_allocated 22561.39794921875 
[2025-03-09 12:53:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.054660942405462265 norm:0.0002620251616463065 max memory_allocated 22561.39794921875 
[2025-03-09 12:53:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.05449804291129112 norm:0.00023229426005855203 max memory_allocated 22561.39794921875 
[2025-03-09 12:54:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.05432238429784775 norm:0.00020562710415106267 max memory_allocated 22561.39794921875 
[2025-03-09 12:54:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.05425553396344185 norm:0.0002053494390565902 max memory_allocated 22561.39794921875 
[2025-03-09 12:55:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.054168228060007095 norm:0.0001828614913392812 max memory_allocated 22561.39794921875 
[2025-03-09 12:55:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.05405476316809654 norm:0.0001661848946241662 max memory_allocated 22561.39794921875 
[2025-03-09 12:56:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.05399288982152939 norm:0.00015730092127341777 max memory_allocated 22561.39794921875 
[2025-03-09 12:56:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.05395758897066116 norm:0.00014356481551658362 max memory_allocated 22561.39794921875 
[2025-03-09 12:57:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.053946033120155334 norm:0.0001385401701554656 max memory_allocated 22561.39794921875 
[2025-03-09 12:57:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.053922951221466064 norm:0.00013516034232452512 max memory_allocated 22561.39794921875 
[2025-03-09 12:58:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.05386997386813164 norm:0.00013141974341124296 max memory_allocated 22561.39794921875 
[2025-03-09 12:58:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.05384215712547302 norm:0.00012451980728656054 max memory_allocated 22561.39794921875 
[2025-03-09 12:59:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.053793683648109436 norm:0.00012311345199123025 max memory_allocated 22561.39794921875 
[2025-03-09 12:59:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-09 13:00:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.08216102421283722 norm:0.00328713096678257 max memory_allocated 22561.56982421875 
[2025-03-09 13:00:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.07118597626686096 norm:0.0012742995750159025 max memory_allocated 22561.56982421875 
[2025-03-09 13:01:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.06293678283691406 norm:0.00042125460458919406 max memory_allocated 22561.56982421875 
[2025-03-09 13:01:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.060285553336143494 norm:0.00027308313292451203 max memory_allocated 22561.56982421875 
[2025-03-09 13:02:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.05921361222863197 norm:0.0002397682110313326 max memory_allocated 22561.56982421875 
[2025-03-09 13:02:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.05875552073121071 norm:0.0002266891679028049 max memory_allocated 22561.56982421875 
[2025-03-09 13:03:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.05854540318250656 norm:0.00020587985636666417 max memory_allocated 22561.56982421875 
[2025-03-09 13:03:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.058372996747493744 norm:0.00016993560711853206 max memory_allocated 22561.56982421875 
[2025-03-09 13:04:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.05825064331293106 norm:0.00016700205742381513 max memory_allocated 22561.56982421875 
[2025-03-09 13:04:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.05819776654243469 norm:0.00014895477215759456 max memory_allocated 22561.56982421875 
[2025-03-09 13:05:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.05813842639327049 norm:0.00014466571155935526 max memory_allocated 22561.56982421875 
[2025-03-09 13:05:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0580432154238224 norm:0.00014147556794341654 max memory_allocated 22561.56982421875 
[2025-03-09 13:06:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.05796702951192856 norm:0.00013018415484111756 max memory_allocated 22561.56982421875 
[2025-03-09 13:06:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.05788952112197876 norm:0.0001279061398236081 max memory_allocated 22561.56982421875 
[2025-03-09 13:07:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.05791536346077919 norm:0.00013204629067331553 max memory_allocated 22561.56982421875 
[2025-03-09 13:07:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.05792447552084923 norm:0.00012288123252801597 max memory_allocated 22561.56982421875 
[2025-03-09 13:08:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.057979099452495575 norm:0.00013683567522093654 max memory_allocated 22561.56982421875 
[2025-03-09 13:08:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.05792689323425293 norm:0.0001302050513913855 max memory_allocated 22561.56982421875 
[2025-03-09 13:09:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.05790592357516289 norm:0.00012611059355549514 max memory_allocated 22561.56982421875 
[2025-03-09 13:09:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.05788801982998848 norm:0.00012140343460487202 max memory_allocated 22561.56982421875 
[2025-03-09 13:09:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-09 13:10:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.09950049221515656 norm:0.007555154152214527 max memory_allocated 22561.74169921875 
[2025-03-09 13:10:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.08218993246555328 norm:0.0025894888676702976 max memory_allocated 22561.74169921875 
[2025-03-09 13:11:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.07061094790697098 norm:0.0008415922056883574 max memory_allocated 22561.74169921875 
[2025-03-09 13:11:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.06697605550289154 norm:0.0004325160407461226 max memory_allocated 22561.74169921875 
[2025-03-09 13:12:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.06545573472976685 norm:0.0003423461166676134 max memory_allocated 22561.74169921875 
[2025-03-09 13:12:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.06474833190441132 norm:0.0003239932411815971 max memory_allocated 22561.74169921875 
[2025-03-09 13:13:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.06433798372745514 norm:0.00028145330725237727 max memory_allocated 22561.74169921875 
[2025-03-09 13:14:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.06409496814012527 norm:0.00026408385019749403 max memory_allocated 22561.74169921875 
[2025-03-09 13:14:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.06400550156831741 norm:0.0002719911572057754 max memory_allocated 22561.74169921875 
[2025-03-09 13:15:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.06386883556842804 norm:0.00022806334891356528 max memory_allocated 22561.74169921875 
[2025-03-09 13:15:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.06370095908641815 norm:0.00021150503016542643 max memory_allocated 22561.74169921875 
[2025-03-09 13:16:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.06358569860458374 norm:0.00019017281010746956 max memory_allocated 22561.74169921875 
[2025-03-09 13:16:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0634610652923584 norm:0.00018288764113094658 max memory_allocated 22561.74169921875 
[2025-03-09 13:17:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0633920356631279 norm:0.00017130615015048534 max memory_allocated 22561.74169921875 
[2025-03-09 13:17:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.06336279213428497 norm:0.0001658786495681852 max memory_allocated 22561.74169921875 
[2025-03-09 13:18:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.06332561373710632 norm:0.0001562835241202265 max memory_allocated 22561.74169921875 
[2025-03-09 13:18:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.0632738396525383 norm:0.0001536308991489932 max memory_allocated 22561.74169921875 
[2025-03-09 13:19:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.06328636407852173 norm:0.00014619003923144192 max memory_allocated 22561.74169921875 
[2025-03-09 13:19:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.06322478502988815 norm:0.0001387378288200125 max memory_allocated 22561.74169921875 
[2025-03-09 13:20:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.06320033967494965 norm:0.00013910929556004703 max memory_allocated 22561.74169921875 
[2025-03-09 13:20:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-09 13:20:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.10544957965612411 norm:0.006954090669751167 max memory_allocated 22561.91357421875 
[2025-03-09 13:21:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.09019947797060013 norm:0.0026434012688696384 max memory_allocated 22561.91357421875 
[2025-03-09 13:21:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.07848081737756729 norm:0.0008629606454633176 max memory_allocated 22561.91357421875 
[2025-03-09 13:22:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.07477716356515884 norm:0.0004589087038766593 max memory_allocated 22561.91357421875 
[2025-03-09 13:22:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.07331383973360062 norm:0.000372985698049888 max memory_allocated 22561.91357421875 
[2025-03-09 13:23:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.07263516634702682 norm:0.0003191236755810678 max memory_allocated 22561.91357421875 
[2025-03-09 13:23:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.07227575778961182 norm:0.0002977267431560904 max memory_allocated 22561.91357421875 
[2025-03-09 13:24:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0720723494887352 norm:0.00027249750564806163 max memory_allocated 22561.91357421875 
[2025-03-09 13:24:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.07188409566879272 norm:0.0002334953169338405 max memory_allocated 22561.91357421875 
[2025-03-09 13:25:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.07170289754867554 norm:0.00019462103955447674 max memory_allocated 22561.91357421875 
[2025-03-09 13:25:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.07160674780607224 norm:0.00018808987806551158 max memory_allocated 22561.91357421875 
[2025-03-09 13:26:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.07148601859807968 norm:0.00017553375801071525 max memory_allocated 22561.91357421875 
[2025-03-09 13:26:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.07140381634235382 norm:0.00017502394621260464 max memory_allocated 22561.91357421875 
[2025-03-09 13:27:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.07134965807199478 norm:0.0001662323047639802 max memory_allocated 22561.91357421875 
[2025-03-09 13:28:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.07132507860660553 norm:0.00016104077803902328 max memory_allocated 22561.91357421875 
[2025-03-09 13:28:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.07131367176771164 norm:0.0001608781167306006 max memory_allocated 22561.91357421875 
[2025-03-09 13:29:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.07125405967235565 norm:0.00014810568245593458 max memory_allocated 22561.91357421875 
[2025-03-09 13:29:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.07125429809093475 norm:0.00014579558046534657 max memory_allocated 22561.91357421875 
[2025-03-09 13:30:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.07121455669403076 norm:0.00013762086746282876 max memory_allocated 22561.91357421875 
[2025-03-09 13:30:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.07114624977111816 norm:0.00013690262858290225 max memory_allocated 22561.91357421875 
[2025-03-09 13:30:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-09 13:31:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.11383447796106339 norm:0.0058451490476727486 max memory_allocated 22562.08544921875 
[2025-03-09 13:31:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.09810402989387512 norm:0.001906207180581987 max memory_allocated 22562.08544921875 
[2025-03-09 13:32:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.08757787942886353 norm:0.0006528401863761246 max memory_allocated 22562.08544921875 
[2025-03-09 13:32:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.08441900461912155 norm:0.0004086414410267025 max memory_allocated 22562.08544921875 
[2025-03-09 13:33:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.08307406306266785 norm:0.0003406517789699137 max memory_allocated 22562.08544921875 
[2025-03-09 13:33:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.0825364887714386 norm:0.0003188692789990455 max memory_allocated 22562.08544921875 
[2025-03-09 13:34:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.08227294683456421 norm:0.00027929540374316275 max memory_allocated 22562.08544921875 
[2025-03-09 13:34:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.08201994001865387 norm:0.0002584338071756065 max memory_allocated 22562.08544921875 
[2025-03-09 13:35:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.08186124265193939 norm:0.000229343626415357 max memory_allocated 22562.08544921875 
[2025-03-09 13:35:51 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.08170312643051147 norm:0.00021413953800220042 max memory_allocated 22562.08544921875 
[2025-03-09 13:36:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.08158722519874573 norm:0.00019745378813240677 max memory_allocated 22562.08544921875 
[2025-03-09 13:36:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.08149975538253784 norm:0.00018565106438472867 max memory_allocated 22562.08544921875 
[2025-03-09 13:37:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.08139051496982574 norm:0.00016783589671831578 max memory_allocated 22562.08544921875 
[2025-03-09 13:37:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.08129274100065231 norm:0.00015983528282959014 max memory_allocated 22562.08544921875 
[2025-03-09 13:38:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.08120697736740112 norm:0.00015040098514873534 max memory_allocated 22562.08544921875 
[2025-03-09 13:38:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.08117002993822098 norm:0.00014281248149927706 max memory_allocated 22562.08544921875 
[2025-03-09 13:39:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.08114606142044067 norm:0.00013920404308009893 max memory_allocated 22562.08544921875 
[2025-03-09 13:39:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.08123958110809326 norm:0.00013700184354092926 max memory_allocated 22562.08544921875 
[2025-03-09 13:40:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.08122839033603668 norm:0.00013151885650586337 max memory_allocated 22562.08544921875 
[2025-03-09 13:40:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.08117800205945969 norm:0.0001280195137951523 max memory_allocated 22562.08544921875 
[2025-03-09 13:41:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-09 13:41:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.12635159492492676 norm:0.005799937527626753 max memory_allocated 22562.25732421875 
[2025-03-09 13:42:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.11235226690769196 norm:0.0023128590546548367 max memory_allocated 22562.25732421875 
[2025-03-09 13:42:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.1005411297082901 norm:0.0006855244864709675 max memory_allocated 22562.25732421875 
[2025-03-09 13:43:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.09721548855304718 norm:0.00038152685738168657 max memory_allocated 22562.25732421875 
[2025-03-09 13:43:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.09594769775867462 norm:0.000347430061083287 max memory_allocated 22562.25732421875 
[2025-03-09 13:44:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.09544439613819122 norm:0.000317034893669188 max memory_allocated 22562.25732421875 
[2025-03-09 13:44:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.09512940794229507 norm:0.00027925890753977 max memory_allocated 22562.25732421875 
[2025-03-09 13:45:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.09491363912820816 norm:0.0002560611756052822 max memory_allocated 22562.25732421875 
[2025-03-09 13:45:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.09479247778654099 norm:0.0002443380653858185 max memory_allocated 22562.25732421875 
[2025-03-09 13:46:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.0946168377995491 norm:0.00021095982810948044 max memory_allocated 22562.25732421875 
[2025-03-09 13:46:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.09448330104351044 norm:0.00019582318782340735 max memory_allocated 22562.25732421875 
[2025-03-09 13:47:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.09438201785087585 norm:0.00018274728790856898 max memory_allocated 22562.25732421875 
[2025-03-09 13:47:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.09425871819257736 norm:0.00017998262774199247 max memory_allocated 22562.25732421875 
[2025-03-09 13:48:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.09428847581148148 norm:0.00018170304247178137 max memory_allocated 22562.25732421875 
[2025-03-09 13:48:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.09422066807746887 norm:0.00016681323177181184 max memory_allocated 22562.25732421875 
[2025-03-09 13:49:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.09415601193904877 norm:0.00016366789350286126 max memory_allocated 22562.25732421875 
[2025-03-09 13:49:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.09412592649459839 norm:0.00016901102208066732 max memory_allocated 22562.25732421875 
[2025-03-09 13:50:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.09412318468093872 norm:0.00016080090426839888 max memory_allocated 22562.25732421875 
[2025-03-09 13:50:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.09409817308187485 norm:0.00015601437189616263 max memory_allocated 22562.25732421875 
[2025-03-09 13:51:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.09407684206962585 norm:0.00015026544861029834 max memory_allocated 22562.25732421875 
[2025-03-09 13:51:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-09 13:52:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.15662360191345215 norm:0.011662149801850319 max memory_allocated 22562.42919921875 
[2025-03-09 13:52:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.13739457726478577 norm:0.004816514439880848 max memory_allocated 22562.42919921875 
[2025-03-09 13:53:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.12094106525182724 norm:0.0013133363099768758 max memory_allocated 22562.42919921875 
[2025-03-09 13:53:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.11650761216878891 norm:0.0006512337713502347 max memory_allocated 22562.42919921875 
[2025-03-09 13:54:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.11485190689563751 norm:0.0005586030893027782 max memory_allocated 22562.42919921875 
[2025-03-09 13:54:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.11410006880760193 norm:0.0005145221948623657 max memory_allocated 22562.42919921875 
[2025-03-09 13:55:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.11370760947465897 norm:0.0005154947284609079 max memory_allocated 22562.42919921875 
[2025-03-09 13:55:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.11342862248420715 norm:0.00046722107799723744 max memory_allocated 22562.42919921875 
[2025-03-09 13:56:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.11320482939481735 norm:0.0004224381991662085 max memory_allocated 22562.42919921875 
[2025-03-09 13:56:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.11297201365232468 norm:0.00036258524050936103 max memory_allocated 22562.42919921875 
[2025-03-09 13:57:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.11276973783969879 norm:0.00033469792106188834 max memory_allocated 22562.42919921875 
[2025-03-09 13:57:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.11256863176822662 norm:0.0003179574850946665 max memory_allocated 22562.42919921875 
[2025-03-09 13:58:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.11242374032735825 norm:0.0002868887677323073 max memory_allocated 22562.42919921875 
[2025-03-09 13:58:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.11234335601329803 norm:0.00027841608971357346 max memory_allocated 22562.42919921875 
[2025-03-09 13:59:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.11228682100772858 norm:0.00025608495343476534 max memory_allocated 22562.42919921875 
[2025-03-09 13:59:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.11218634247779846 norm:0.0002402403624728322 max memory_allocated 22562.42919921875 
[2025-03-09 14:00:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.11212655901908875 norm:0.000229532815865241 max memory_allocated 22562.42919921875 
[2025-03-09 14:00:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.11200936883687973 norm:0.00022329828061629087 max memory_allocated 22562.42919921875 
[2025-03-09 14:01:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.11197024583816528 norm:0.00020897467038594186 max memory_allocated 22562.42919921875 
[2025-03-09 14:01:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.11198684573173523 norm:0.00020716121071018279 max memory_allocated 22562.42919921875 
[2025-03-09 14:01:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-09 14:02:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.17438580095767975 norm:0.007262979634106159 max memory_allocated 22562.60107421875 
[2025-03-09 14:03:00 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.15680460631847382 norm:0.002755992114543915 max memory_allocated 22562.60107421875 
[2025-03-09 14:03:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.14161014556884766 norm:0.0006928628426976502 max memory_allocated 22562.60107421875 
[2025-03-09 14:04:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.13765428960323334 norm:0.0006706773419864476 max memory_allocated 22562.60107421875 
[2025-03-09 14:04:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.13619446754455566 norm:0.0006284780683927238 max memory_allocated 22562.60107421875 
[2025-03-09 14:05:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.1354147493839264 norm:0.0005479548708535731 max memory_allocated 22562.60107421875 
[2025-03-09 14:05:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.13492752611637115 norm:0.0004936625482514501 max memory_allocated 22562.60107421875 
[2025-03-09 14:06:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.1346512883901596 norm:0.0004314327670726925 max memory_allocated 22562.60107421875 
[2025-03-09 14:06:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.13457229733467102 norm:0.00048099173000082374 max memory_allocated 22562.60107421875 
[2025-03-09 14:07:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.1343371570110321 norm:0.0003765985311474651 max memory_allocated 22562.60107421875 
[2025-03-09 14:07:36 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.13420765101909637 norm:0.000385743856895715 max memory_allocated 22562.60107421875 
[2025-03-09 14:08:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.13409793376922607 norm:0.00038512286846525967 max memory_allocated 22562.60107421875 
[2025-03-09 14:08:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.13396921753883362 norm:0.0003536190197337419 max memory_allocated 22562.60107421875 
[2025-03-09 14:09:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.1338285654783249 norm:0.0003380567068234086 max memory_allocated 22562.60107421875 
[2025-03-09 14:09:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.13372677564620972 norm:0.00031093513825908303 max memory_allocated 22562.60107421875 
[2025-03-09 14:10:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.13361957669258118 norm:0.00032496702624484897 max memory_allocated 22562.60107421875 
[2025-03-09 14:10:40 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.13352639973163605 norm:0.0003195466415490955 max memory_allocated 22562.60107421875 
[2025-03-09 14:11:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.13344471156597137 norm:0.00033005920704454184 max memory_allocated 22562.60107421875 
[2025-03-09 14:11:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.13331469893455505 norm:0.0003432697558309883 max memory_allocated 22562.60107421875 
[2025-03-09 14:12:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.133323073387146 norm:0.0003104759962297976 max memory_allocated 22562.60107421875 
[2025-03-09 14:12:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-09 14:13:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.19479669630527496 norm:0.006778585258871317 max memory_allocated 22562.77294921875 
[2025-03-09 14:13:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.1775699108839035 norm:0.0029020351357758045 max memory_allocated 22562.77294921875 
[2025-03-09 14:14:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.16313764452934265 norm:0.0008213400142267346 max memory_allocated 22562.77294921875 
[2025-03-09 14:14:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.1589544415473938 norm:0.0004773396358359605 max memory_allocated 22562.77294921875 
[2025-03-09 14:15:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.1576448231935501 norm:0.0004178021044936031 max memory_allocated 22562.77294921875 
[2025-03-09 14:15:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.15711164474487305 norm:0.0003996953018940985 max memory_allocated 22562.77294921875 
[2025-03-09 14:16:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.1566743701696396 norm:0.00032296826248057187 max memory_allocated 22562.77294921875 
[2025-03-09 14:16:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.15638700127601624 norm:0.0003068938967771828 max memory_allocated 22562.77294921875 
[2025-03-09 14:17:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.15613219141960144 norm:0.00029138749232515693 max memory_allocated 22562.77294921875 
[2025-03-09 14:17:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.15589341521263123 norm:0.0002587151248008013 max memory_allocated 22562.77294921875 
[2025-03-09 14:18:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.15574373304843903 norm:0.0002463725977577269 max memory_allocated 22562.77294921875 
[2025-03-09 14:18:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.15559235215187073 norm:0.0002223819465143606 max memory_allocated 22562.77294921875 
[2025-03-09 14:19:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.15545065701007843 norm:0.00021591353288386017 max memory_allocated 22562.77294921875 
[2025-03-09 14:19:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.15535077452659607 norm:0.00020076180226169527 max memory_allocated 22562.77294921875 
[2025-03-09 14:20:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.15529941022396088 norm:0.00020202278392389417 max memory_allocated 22562.77294921875 
[2025-03-09 14:20:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.15521007776260376 norm:0.00020187858899589628 max memory_allocated 22562.77294921875 
[2025-03-09 14:21:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.1551441252231598 norm:0.00019972393056377769 max memory_allocated 22562.77294921875 
[2025-03-09 14:21:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.15506820380687714 norm:0.00019614212214946747 max memory_allocated 22562.77294921875 
[2025-03-09 14:22:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.15506760776042938 norm:0.00020808067347388715 max memory_allocated 22562.77294921875 
[2025-03-09 14:22:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.15502116084098816 norm:0.0002016732469201088 max memory_allocated 22562.77294921875 
[2025-03-09 14:22:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-09 14:23:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.21987494826316833 norm:0.003791155992075801 max memory_allocated 22562.94482421875 
[2025-03-09 14:23:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.20351135730743408 norm:0.0014736498706042767 max memory_allocated 22562.94482421875 
[2025-03-09 14:24:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.1902785450220108 norm:0.0006427306216210127 max memory_allocated 22562.94482421875 
[2025-03-09 14:24:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.1861707866191864 norm:0.0004915722529403865 max memory_allocated 22562.94482421875 
[2025-03-09 14:25:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.18479859828948975 norm:0.00043253108742646873 max memory_allocated 22562.94482421875 
[2025-03-09 14:26:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.18430815637111664 norm:0.00038486975245177746 max memory_allocated 22562.94482421875 
[2025-03-09 14:26:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.1838570088148117 norm:0.00035524877603165805 max memory_allocated 22562.94482421875 
[2025-03-09 14:27:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.18354907631874084 norm:0.00032348884269595146 max memory_allocated 22562.94482421875 
[2025-03-09 14:27:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.18325959146022797 norm:0.00028824061155319214 max memory_allocated 22562.94482421875 
[2025-03-09 14:28:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.18308916687965393 norm:0.0002802615927066654 max memory_allocated 22562.94482421875 
[2025-03-09 14:28:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.18292871117591858 norm:0.00026531011098995805 max memory_allocated 22562.94482421875 
[2025-03-09 14:29:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.1827852576971054 norm:0.00024889371707104146 max memory_allocated 22562.94482421875 
[2025-03-09 14:29:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.18268465995788574 norm:0.00024039193522185087 max memory_allocated 22562.94482421875 
[2025-03-09 14:30:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.18256601691246033 norm:0.00023217132547870278 max memory_allocated 22562.94482421875 
[2025-03-09 14:30:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.1825183928012848 norm:0.00023074708587955683 max memory_allocated 22562.94482421875 
[2025-03-09 14:31:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.18240392208099365 norm:0.00022750135394744575 max memory_allocated 22562.94482421875 
[2025-03-09 14:31:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.18233853578567505 norm:0.00023171762586571276 max memory_allocated 22562.94482421875 
[2025-03-09 14:32:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.18225225806236267 norm:0.00023245664488058537 max memory_allocated 22562.94482421875 
[2025-03-09 14:32:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.18224549293518066 norm:0.0002228883095085621 max memory_allocated 22562.94482421875 
[2025-03-09 14:33:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.1822260171175003 norm:0.00022386095952242613 max memory_allocated 22562.94482421875 
[2025-03-09 14:33:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-09 14:33:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.24906235933303833 norm:0.004535372368991375 max memory_allocated 22563.11669921875 
[2025-03-09 14:34:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.23284289240837097 norm:0.002140772994607687 max memory_allocated 22563.11669921875 
[2025-03-09 14:34:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.2177698165178299 norm:0.0010024658404290676 max memory_allocated 22563.11669921875 
[2025-03-09 14:35:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.21353211998939514 norm:0.0006920128362253308 max memory_allocated 22563.11669921875 
[2025-03-09 14:35:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.21229615807533264 norm:0.000592837284784764 max memory_allocated 22563.11669921875 
[2025-03-09 14:36:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.21171844005584717 norm:0.0005182690802030265 max memory_allocated 22563.11669921875 
[2025-03-09 14:36:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.2113698124885559 norm:0.00046210968866944313 max memory_allocated 22563.11669921875 
[2025-03-09 14:37:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.2111331969499588 norm:0.00041900810902006924 max memory_allocated 22563.11669921875 
[2025-03-09 14:37:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.2108466625213623 norm:0.0003727771691046655 max memory_allocated 22563.11669921875 
[2025-03-09 14:38:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.21059741079807281 norm:0.00034523571957834065 max memory_allocated 22563.11669921875 
[2025-03-09 14:38:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.210385262966156 norm:0.00031347860931418836 max memory_allocated 22563.11669921875 
[2025-03-09 14:39:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.21028581261634827 norm:0.0003036469570361078 max memory_allocated 22563.11669921875 
[2025-03-09 14:39:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.2102075219154358 norm:0.0003001137520186603 max memory_allocated 22563.11669921875 
[2025-03-09 14:40:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.21008160710334778 norm:0.0002862437395378947 max memory_allocated 22563.11669921875 
[2025-03-09 14:41:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.2099476009607315 norm:0.0002743838995229453 max memory_allocated 22563.11669921875 
[2025-03-09 14:41:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.2099216729402542 norm:0.00026466624694876373 max memory_allocated 22563.11669921875 
[2025-03-09 14:42:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.20983465015888214 norm:0.0002602384192869067 max memory_allocated 22563.11669921875 
[2025-03-09 14:42:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.20971523225307465 norm:0.00024886406026780605 max memory_allocated 22563.11669921875 
[2025-03-09 14:43:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.2096647173166275 norm:0.00024296699848491699 max memory_allocated 22563.11669921875 
[2025-03-09 14:43:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.20962509512901306 norm:0.00023990590125322342 max memory_allocated 22563.11669921875 
[2025-03-09 14:43:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-09 14:44:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.2859076261520386 norm:0.0058060577139258385 max memory_allocated 22563.28857421875 
[2025-03-09 14:44:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.26695194840431213 norm:0.002247000113129616 max memory_allocated 22563.28857421875 
[2025-03-09 14:45:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.24975812435150146 norm:0.000733585620764643 max memory_allocated 22563.28857421875 
[2025-03-09 14:45:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.24514140188694 norm:0.00040702024125494063 max memory_allocated 22563.28857421875 
[2025-03-09 14:46:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.2440437227487564 norm:0.000403833226300776 max memory_allocated 22563.28857421875 
[2025-03-09 14:46:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.24352484941482544 norm:0.0003450933436397463 max memory_allocated 22563.28857421875 
[2025-03-09 14:47:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.24320104718208313 norm:0.00031147035770118237 max memory_allocated 22563.28857421875 
[2025-03-09 14:47:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.24296584725379944 norm:0.00029005203396081924 max memory_allocated 22563.28857421875 
[2025-03-09 14:48:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.24263498187065125 norm:0.00026552737108431756 max memory_allocated 22563.28857421875 
[2025-03-09 14:48:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.24239063262939453 norm:0.0002587236522231251 max memory_allocated 22563.28857421875 
[2025-03-09 14:49:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.24228215217590332 norm:0.00023854148457758129 max memory_allocated 22563.28857421875 
[2025-03-09 14:49:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.24215762317180634 norm:0.00022536602045875043 max memory_allocated 22563.28857421875 
[2025-03-09 14:50:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.24203048646450043 norm:0.00022106990218162537 max memory_allocated 22563.28857421875 
[2025-03-09 14:50:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.2418462038040161 norm:0.0002087093744194135 max memory_allocated 22563.28857421875 
[2025-03-09 14:51:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.2417241930961609 norm:0.00020813761511817575 max memory_allocated 22563.28857421875 
[2025-03-09 14:51:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.24171993136405945 norm:0.00019699364202097058 max memory_allocated 22563.28857421875 
[2025-03-09 14:52:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.24161504209041595 norm:0.0001965331903193146 max memory_allocated 22563.28857421875 
[2025-03-09 14:52:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.24155022203922272 norm:0.00020126241724938154 max memory_allocated 22563.28857421875 
[2025-03-09 14:53:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.2414931207895279 norm:0.00020075426436960697 max memory_allocated 22563.28857421875 
[2025-03-09 14:53:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.24146506190299988 norm:0.0001936192566063255 max memory_allocated 22563.28857421875 
[2025-03-09 14:54:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-09 14:54:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.3233087956905365 norm:0.005999891087412834 max memory_allocated 22563.46044921875 
[2025-03-09 14:55:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.30377933382987976 norm:0.002499879337847233 max memory_allocated 22563.46044921875 
[2025-03-09 14:55:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.28565236926078796 norm:0.00076573237311095 max memory_allocated 22563.46044921875 
[2025-03-09 14:56:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.2805354595184326 norm:0.0004767748177982867 max memory_allocated 22563.46044921875 
[2025-03-09 14:56:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.2794030010700226 norm:0.00044381461339071393 max memory_allocated 22563.46044921875 
[2025-03-09 14:57:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.27885550260543823 norm:0.0004085670516360551 max memory_allocated 22563.46044921875 
[2025-03-09 14:57:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.278443843126297 norm:0.00037507302477024496 max memory_allocated 22563.46044921875 
[2025-03-09 14:58:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.2781183123588562 norm:0.0003396058746147901 max memory_allocated 22563.46044921875 
[2025-03-09 14:58:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.27784860134124756 norm:0.0003187423280905932 max memory_allocated 22563.46044921875 
[2025-03-09 14:59:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.2777005434036255 norm:0.00028517981991171837 max memory_allocated 22563.46044921875 
[2025-03-09 14:59:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.27758073806762695 norm:0.00027097429847344756 max memory_allocated 22563.46044921875 
[2025-03-09 15:00:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.277327299118042 norm:0.0002691252448130399 max memory_allocated 22563.46044921875 
[2025-03-09 15:00:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.2772107720375061 norm:0.00024972588289529085 max memory_allocated 22563.46044921875 
[2025-03-09 15:01:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.27708399295806885 norm:0.0002485462464392185 max memory_allocated 22563.46044921875 
[2025-03-09 15:01:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.27693358063697815 norm:0.0002499195106793195 max memory_allocated 22563.46044921875 
[2025-03-09 15:02:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.2768413722515106 norm:0.00023696651624049991 max memory_allocated 22563.46044921875 
[2025-03-09 15:02:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.2767734229564667 norm:0.00022527635155711323 max memory_allocated 22563.46044921875 
[2025-03-09 15:03:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.27671536803245544 norm:0.00022275005176197737 max memory_allocated 22563.46044921875 
[2025-03-09 15:03:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.27661794424057007 norm:0.00021758196817245334 max memory_allocated 22563.46044921875 
[2025-03-09 15:04:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.2765541076660156 norm:0.0002164846082450822 max memory_allocated 22563.46044921875 
[2025-03-09 15:04:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-09 15:05:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.36104339361190796 norm:0.005473352503031492 max memory_allocated 22563.63232421875 
[2025-03-09 15:05:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.338302880525589 norm:0.0018580070463940501 max memory_allocated 22563.63232421875 
[2025-03-09 15:06:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.3213234543800354 norm:0.0007851956761442125 max memory_allocated 22563.63232421875 
[2025-03-09 15:06:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.31645259261131287 norm:0.0005371766164898872 max memory_allocated 22563.63232421875 
[2025-03-09 15:07:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.3153785169124603 norm:0.0005022929981350899 max memory_allocated 22563.63232421875 
[2025-03-09 15:07:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.31477808952331543 norm:0.00046416628174483776 max memory_allocated 22563.63232421875 
[2025-03-09 15:08:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.31433606147766113 norm:0.0004263298469595611 max memory_allocated 22563.63232421875 
[2025-03-09 15:08:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.313994437456131 norm:0.0004011104174423963 max memory_allocated 22563.63232421875 
[2025-03-09 15:09:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.31382882595062256 norm:0.00037599456845782697 max memory_allocated 22563.63232421875 
[2025-03-09 15:09:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.31360334157943726 norm:0.0003523663035593927 max memory_allocated 22563.63232421875 
[2025-03-09 15:10:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.31339341402053833 norm:0.00032298744190484285 max memory_allocated 22563.63232421875 
[2025-03-09 15:10:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.3132532835006714 norm:0.0002815324114635587 max memory_allocated 22563.63232421875 
[2025-03-09 15:11:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.31309637427330017 norm:0.00026833315496332943 max memory_allocated 22563.63232421875 
[2025-03-09 15:11:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.3129667043685913 norm:0.00025994516909122467 max memory_allocated 22563.63232421875 
[2025-03-09 15:12:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.31281980872154236 norm:0.0002529884804971516 max memory_allocated 22563.63232421875 
[2025-03-09 15:12:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.31272047758102417 norm:0.0002483910939190537 max memory_allocated 22563.63232421875 
[2025-03-09 15:13:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.31262826919555664 norm:0.00024161278270184994 max memory_allocated 22563.63232421875 
[2025-03-09 15:13:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.3125685751438141 norm:0.00022983341477811337 max memory_allocated 22563.63232421875 
[2025-03-09 15:14:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.31247571110725403 norm:0.0002279868785990402 max memory_allocated 22563.63232421875 
[2025-03-09 15:14:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.3124085068702698 norm:0.00023972864437382668 max memory_allocated 22563.63232421875 
[2025-03-09 15:14:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-09 15:14:59 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 15:15:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.41667240858078003 norm:0.02399393543601036 max memory_allocated 22563.91943359375 
[2025-03-09 15:16:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.39228248596191406 norm:0.018271440640091896 max memory_allocated 22563.91943359375 
[2025-03-09 15:16:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.37252604961395264 norm:0.01108314748853445 max memory_allocated 22563.91943359375 
[2025-03-09 15:17:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.3669874370098114 norm:0.00962492823600769 max memory_allocated 22563.91943359375 
[2025-03-09 15:17:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.3654419183731079 norm:0.008390766568481922 max memory_allocated 22563.91943359375 
[2025-03-09 15:18:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.3644769787788391 norm:0.007315796799957752 max memory_allocated 22563.91943359375 
[2025-03-09 15:18:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.3637315034866333 norm:0.0064979312010109425 max memory_allocated 22563.91943359375 
[2025-03-09 15:19:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.36312976479530334 norm:0.005624950397759676 max memory_allocated 22563.91943359375 
[2025-03-09 15:19:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.36267539858818054 norm:0.005135158076882362 max memory_allocated 22563.91943359375 
[2025-03-09 15:20:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.36252790689468384 norm:0.005243786610662937 max memory_allocated 22563.91943359375 
[2025-03-09 15:20:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.3623899519443512 norm:0.005460320971906185 max memory_allocated 22563.91943359375 
[2025-03-09 15:21:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.3621848523616791 norm:0.005039792973548174 max memory_allocated 22563.91943359375 
[2025-03-09 15:21:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.361815869808197 norm:0.00473439134657383 max memory_allocated 22563.91943359375 
[2025-03-09 15:22:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.36157435178756714 norm:0.004372036550194025 max memory_allocated 22563.91943359375 
[2025-03-09 15:22:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.3614780902862549 norm:0.004326097201555967 max memory_allocated 22563.91943359375 
[2025-03-09 15:23:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.3613860607147217 norm:0.004327769391238689 max memory_allocated 22563.91943359375 
[2025-03-09 15:23:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.36128583550453186 norm:0.004389817826449871 max memory_allocated 22563.91943359375 
[2025-03-09 15:24:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.3611672520637512 norm:0.004207161720842123 max memory_allocated 22563.91943359375 
[2025-03-09 15:24:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.3610677421092987 norm:0.004153334069997072 max memory_allocated 22563.91943359375 
[2025-03-09 15:25:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.3609648644924164 norm:0.003999016713351011 max memory_allocated 22563.91943359375 
[2025-03-09 15:25:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-09 15:25:26 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 15:25:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.47793757915496826 norm:0.028222359716892242 max memory_allocated 22564.09130859375 
[2025-03-09 15:26:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.4514539837837219 norm:0.019434012472629547 max memory_allocated 22564.09130859375 
[2025-03-09 15:26:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.4295949339866638 norm:0.012740417383611202 max memory_allocated 22564.09130859375 
[2025-03-09 15:27:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.4240664541721344 norm:0.01025204174220562 max memory_allocated 22564.09130859375 
[2025-03-09 15:28:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.4225946068763733 norm:0.008972534909844398 max memory_allocated 22564.09130859375 
[2025-03-09 15:28:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.42164233326911926 norm:0.007718471344560385 max memory_allocated 22564.09130859375 
[2025-03-09 15:29:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.4209390878677368 norm:0.006670676171779633 max memory_allocated 22564.09130859375 
[2025-03-09 15:29:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.4203057885169983 norm:0.005826710723340511 max memory_allocated 22564.09130859375 
[2025-03-09 15:30:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.4199120104312897 norm:0.005533371586352587 max memory_allocated 22564.09130859375 
[2025-03-09 15:30:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.4197797477245331 norm:0.005451376549899578 max memory_allocated 22564.09130859375 
[2025-03-09 15:31:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.4194371700286865 norm:0.0052472073584795 max memory_allocated 22564.09130859375 
[2025-03-09 15:31:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.4192832112312317 norm:0.004961922764778137 max memory_allocated 22564.09130859375 
[2025-03-09 15:32:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.41899001598358154 norm:0.004693758208304644 max memory_allocated 22564.09130859375 
[2025-03-09 15:32:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.4188568890094757 norm:0.004486025311052799 max memory_allocated 22564.09130859375 
[2025-03-09 15:33:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.41884368658065796 norm:0.004460736643522978 max memory_allocated 22564.09130859375 
[2025-03-09 15:33:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.4186950922012329 norm:0.004265195690095425 max memory_allocated 22564.09130859375 
[2025-03-09 15:34:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.4186277985572815 norm:0.0042610084637999535 max memory_allocated 22564.09130859375 
[2025-03-09 15:34:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.4185508191585541 norm:0.004115536343306303 max memory_allocated 22564.09130859375 
[2025-03-09 15:35:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.41850531101226807 norm:0.0040579261258244514 max memory_allocated 22564.09130859375 
[2025-03-09 15:35:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.41855931282043457 norm:0.003979836590588093 max memory_allocated 22564.09130859375 
[2025-03-09 15:35:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-09 15:35:53 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 15:36:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.6428635716438293 norm:0.03955239802598953 max memory_allocated 22564.26318359375 
[2025-03-09 15:36:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.5906715393066406 norm:0.02712172642350197 max memory_allocated 22564.26318359375 
[2025-03-09 15:37:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.5524874925613403 norm:0.013753549195826054 max memory_allocated 22564.26318359375 
[2025-03-09 15:37:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.5438669919967651 norm:0.013773482292890549 max memory_allocated 22564.26318359375 
[2025-03-09 15:38:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.5406128764152527 norm:0.014235550537705421 max memory_allocated 22564.26318359375 
[2025-03-09 15:38:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.5385833978652954 norm:0.016208060085773468 max memory_allocated 22564.26318359375 
[2025-03-09 15:39:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.5372875928878784 norm:0.01659460738301277 max memory_allocated 22564.26318359375 
[2025-03-09 15:40:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.5364051461219788 norm:0.017596036195755005 max memory_allocated 22564.26318359375 
[2025-03-09 15:40:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.5354722738265991 norm:0.0175493024289608 max memory_allocated 22564.26318359375 
[2025-03-09 15:41:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.5346166491508484 norm:0.018312528729438782 max memory_allocated 22564.26318359375 
[2025-03-09 15:41:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.535098671913147 norm:0.01923983171582222 max memory_allocated 22564.26318359375 
[2025-03-09 15:42:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.5336310863494873 norm:0.018875887617468834 max memory_allocated 22564.26318359375 
[2025-03-09 15:42:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.5333316326141357 norm:0.018015041947364807 max memory_allocated 22564.26318359375 
[2025-03-09 15:43:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.5328715443611145 norm:0.018088040873408318 max memory_allocated 22564.26318359375 
[2025-03-09 15:43:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.5327129364013672 norm:0.018354201689362526 max memory_allocated 22564.26318359375 
[2025-03-09 15:44:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.5346103310585022 norm:0.01952880620956421 max memory_allocated 22564.26318359375 
[2025-03-09 15:44:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.5342991352081299 norm:0.020395174622535706 max memory_allocated 22564.26318359375 
[2025-03-09 15:45:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.5325018167495728 norm:0.019385792315006256 max memory_allocated 22564.26318359375 
[2025-03-09 15:45:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.532107412815094 norm:0.01945047825574875 max memory_allocated 22564.26318359375 
[2025-03-09 15:46:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.5325748920440674 norm:0.018968679010868073 max memory_allocated 22564.26318359375 
[2025-03-09 15:46:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-09 15:46:20 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 15:46:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:1.102247953414917 norm:0.09336645156145096 max memory_allocated 22564.43505859375 
[2025-03-09 15:47:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.9967360496520996 norm:0.06828650832176208 max memory_allocated 22564.43505859375 
[2025-03-09 15:47:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.9210793972015381 norm:0.04858170822262764 max memory_allocated 22564.43505859375 
[2025-03-09 15:48:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.8999981880187988 norm:0.04429201781749725 max memory_allocated 22564.43505859375 
[2025-03-09 15:48:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.8905519843101501 norm:0.042303577065467834 max memory_allocated 22564.43505859375 
[2025-03-09 15:49:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.885189414024353 norm:0.040737271308898926 max memory_allocated 22564.43505859375 
[2025-03-09 15:49:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.8809962272644043 norm:0.038949329406023026 max memory_allocated 22564.43505859375 
[2025-03-09 15:50:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.8762984275817871 norm:0.0374872162938118 max memory_allocated 22564.43505859375 
[2025-03-09 15:50:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.8729597330093384 norm:0.03436322510242462 max memory_allocated 22564.43505859375 
[2025-03-09 15:51:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.8703647255897522 norm:0.03343546390533447 max memory_allocated 22564.43505859375 
[2025-03-09 15:51:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.8699142336845398 norm:0.03243526816368103 max memory_allocated 22564.43505859375 
[2025-03-09 15:52:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.8686856627464294 norm:0.03451458737254143 max memory_allocated 22564.43505859375 
[2025-03-09 15:53:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.8679569959640503 norm:0.03302823007106781 max memory_allocated 22564.43505859375 
[2025-03-09 15:53:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.8664910197257996 norm:0.03237153962254524 max memory_allocated 22564.43505859375 
[2025-03-09 15:54:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.8659364581108093 norm:0.03206460922956467 max memory_allocated 22564.43505859375 
[2025-03-09 15:54:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.8656555414199829 norm:0.030831942334771156 max memory_allocated 22564.43505859375 
[2025-03-09 15:55:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.8647142648696899 norm:0.02930711768567562 max memory_allocated 22564.43505859375 
[2025-03-09 15:55:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.8633632659912109 norm:0.027598584070801735 max memory_allocated 22564.43505859375 
[2025-03-09 15:56:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.864246129989624 norm:0.029509900137782097 max memory_allocated 22564.43505859375 
[2025-03-09 15:56:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.8649899363517761 norm:0.031031033024191856 max memory_allocated 22564.43505859375 
[2025-03-09 15:56:45 root] (main_calibration.py 365): INFO 20022.022653341293
[2025-03-09 15:56:49 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-09 15:57:53 root] (main_calibration.py 158): INFO wikitext2 : 5.89616584777832
[2025-03-09 15:57:53 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-09 15:59:30 root] (main_calibration.py 158): INFO c4 : 7.384713172912598
[2025-03-09 17:35:18 root] (main_calibration.py 169): INFO {'wikitext2': 5.89616584777832, 'c4': 7.384713172912598, 'results': {'piqa': {'acc': 0.7774755168661589, 'acc_stderr': 0.009704600975718243, 'acc_norm': 0.7780195865070729, 'acc_norm_stderr': 0.009696120744662029}, 'arc_challenge': {'acc': 0.37372013651877134, 'acc_stderr': 0.014137708601759095, 'acc_norm': 0.3916382252559727, 'acc_norm_stderr': 0.014264122124938213}, 'winogrande': {'acc': 0.6606156274664562, 'acc_stderr': 0.013307714928941752}, 'hellaswag': {'acc': 0.5522804222266481, 'acc_stderr': 0.004962429881904026, 'acc_norm': 0.7129057956582354, 'acc_norm_stderr': 0.00451481336322115}, 'boolq': {'acc': 0.7192660550458716, 'acc_stderr': 0.007859316642849496}, 'arc_easy': {'acc': 0.6502525252525253, 'acc_stderr': 0.009785578618940728, 'acc_norm': 0.5155723905723906, 'acc_norm_stderr': 0.0102548063319619}}, 'versions': {'piqa': 0, 'arc_challenge': 0, 'winogrande': 0, 'hellaswag': 0, 'boolq': 1, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
