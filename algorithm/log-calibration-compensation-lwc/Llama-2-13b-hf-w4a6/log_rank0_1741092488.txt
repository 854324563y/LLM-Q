[2025-03-04 12:48:08 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-13b-hf-w4a6', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-13b-hf', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-04 12:48:19 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-04 12:48:19 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-04 12:48:19 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-04 12:48:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-04 12:48:50 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 12:49:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.020125821232795715 norm:0.021167097613215446 max memory_allocated 29271.02001953125 
[2025-03-04 12:50:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.012384078465402126 norm:0.011658924631774426 max memory_allocated 29271.02001953125 
[2025-03-04 12:51:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.009675453417003155 norm:0.008585282601416111 max memory_allocated 29271.02001953125 
[2025-03-04 12:52:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.008639657869935036 norm:0.007191338576376438 max memory_allocated 29271.02001953125 
[2025-03-04 12:53:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.008272741921246052 norm:0.006552733015269041 max memory_allocated 29271.02001953125 
[2025-03-04 12:53:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.007966091856360435 norm:0.005337318405508995 max memory_allocated 29271.02001953125 
[2025-03-04 12:54:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.007790875155478716 norm:0.004954826552420855 max memory_allocated 29271.02001953125 
[2025-03-04 12:55:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.007571675349026918 norm:0.004117884673178196 max memory_allocated 29271.02001953125 
[2025-03-04 12:56:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.007475436199456453 norm:0.0035122099798172712 max memory_allocated 29271.02001953125 
[2025-03-04 12:57:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.007440474350005388 norm:0.0031836102716624737 max memory_allocated 29271.02001953125 
[2025-03-04 12:58:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.007438243366777897 norm:0.00303816981613636 max memory_allocated 29271.02001953125 
[2025-03-04 12:58:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.007310028653591871 norm:0.0027976389974355698 max memory_allocated 29271.02001953125 
[2025-03-04 12:59:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.007291808724403381 norm:0.0025078076869249344 max memory_allocated 29271.02001953125 
[2025-03-04 13:00:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.007263864390552044 norm:0.0026520746760070324 max memory_allocated 29271.02001953125 
[2025-03-04 13:01:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.007312475703656673 norm:0.0022387674544006586 max memory_allocated 29271.02001953125 
[2025-03-04 13:02:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0072523984126746655 norm:0.00224075885489583 max memory_allocated 29271.02001953125 
[2025-03-04 13:03:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00727001391351223 norm:0.0020458176732063293 max memory_allocated 29271.02001953125 
[2025-03-04 13:03:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.007237100973725319 norm:0.0020006983540952206 max memory_allocated 29271.02001953125 
[2025-03-04 13:04:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.007294138427823782 norm:0.0019543981179594994 max memory_allocated 29271.02001953125 
[2025-03-04 13:05:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.007452015765011311 norm:0.0019395810086280107 max memory_allocated 29271.02001953125 
[2025-03-04 13:05:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-04 13:06:05 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 13:06:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.03872297331690788 norm:0.015640312805771828 max memory_allocated 29271.02001953125 
[2025-03-04 13:07:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.029203854501247406 norm:0.011054888367652893 max memory_allocated 29271.02001953125 
[2025-03-04 13:08:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.023975612595677376 norm:0.00758542213588953 max memory_allocated 29271.02001953125 
[2025-03-04 13:09:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.022183623164892197 norm:0.005715410690754652 max memory_allocated 29271.02001953125 
[2025-03-04 13:10:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.021408747881650925 norm:0.0048140306025743484 max memory_allocated 29271.02001953125 
[2025-03-04 13:11:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.02097376435995102 norm:0.004109515808522701 max memory_allocated 29271.02001953125 
[2025-03-04 13:11:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.02066599577665329 norm:0.0035710004158318043 max memory_allocated 29271.02001953125 
[2025-03-04 13:12:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.020455490797758102 norm:0.003061783267185092 max memory_allocated 29271.02001953125 
[2025-03-04 13:13:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.020245004445314407 norm:0.0026232420932501554 max memory_allocated 29271.02001953125 
[2025-03-04 13:14:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.02007708139717579 norm:0.0023128094617277384 max memory_allocated 29271.02001953125 
[2025-03-04 13:15:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.020006196573376656 norm:0.0022060303017497063 max memory_allocated 29271.02001953125 
[2025-03-04 13:16:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.019941164180636406 norm:0.002215670421719551 max memory_allocated 29271.02001953125 
[2025-03-04 13:16:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.01996004208922386 norm:0.001988394185900688 max memory_allocated 29271.02001953125 
[2025-03-04 13:17:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.01982186548411846 norm:0.0019396694842725992 max memory_allocated 29271.02001953125 
[2025-03-04 13:18:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.019851164892315865 norm:0.0018538706935942173 max memory_allocated 29271.02001953125 
[2025-03-04 13:19:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.019750729203224182 norm:0.001820483710616827 max memory_allocated 29271.02001953125 
[2025-03-04 13:20:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.019779298454523087 norm:0.0017142933793365955 max memory_allocated 29271.02001953125 
[2025-03-04 13:21:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.019618846476078033 norm:0.0016627293080091476 max memory_allocated 29271.02001953125 
[2025-03-04 13:21:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.019674044102430344 norm:0.001609692582860589 max memory_allocated 29271.02001953125 
[2025-03-04 13:22:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.019604437053203583 norm:0.001575057627633214 max memory_allocated 29271.02001953125 
[2025-03-04 13:23:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-04 13:23:15 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 13:24:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.044330619275569916 norm:0.013597967103123665 max memory_allocated 29271.39501953125 
[2025-03-04 13:24:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.03576674684882164 norm:0.009767884388566017 max memory_allocated 29271.39501953125 
[2025-03-04 13:25:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.030656276270747185 norm:0.007007870357483625 max memory_allocated 29271.39501953125 
[2025-03-04 13:26:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.028763415291905403 norm:0.005464403424412012 max memory_allocated 29271.39501953125 
[2025-03-04 13:27:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.027934102341532707 norm:0.004465318284928799 max memory_allocated 29271.39501953125 
[2025-03-04 13:28:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.02741367556154728 norm:0.003656059503555298 max memory_allocated 29271.39501953125 
[2025-03-04 13:29:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.027005672454833984 norm:0.003031267086043954 max memory_allocated 29271.39501953125 
[2025-03-04 13:29:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.026737142354249954 norm:0.0027150025125592947 max memory_allocated 29271.39501953125 
[2025-03-04 13:30:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.026536207646131516 norm:0.0025334013625979424 max memory_allocated 29271.39501953125 
[2025-03-04 13:31:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.02639620006084442 norm:0.002394259674474597 max memory_allocated 29271.39501953125 
[2025-03-04 13:32:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.026374708861112595 norm:0.0022546297404915094 max memory_allocated 29271.39501953125 
[2025-03-04 13:33:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.026310255751013756 norm:0.0022138915956020355 max memory_allocated 29271.39501953125 
[2025-03-04 13:34:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.02634207159280777 norm:0.002147495048120618 max memory_allocated 29271.39501953125 
[2025-03-04 13:34:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.02637035772204399 norm:0.0021007563918828964 max memory_allocated 29271.39501953125 
[2025-03-04 13:35:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.026319587603211403 norm:0.0020080897957086563 max memory_allocated 29271.39501953125 
[2025-03-04 13:36:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.02630850113928318 norm:0.001956871710717678 max memory_allocated 29271.39501953125 
[2025-03-04 13:37:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.026321789249777794 norm:0.0019121695077046752 max memory_allocated 29271.39501953125 
[2025-03-04 13:38:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.026367487385869026 norm:0.001895720837637782 max memory_allocated 29271.39501953125 
[2025-03-04 13:39:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.026380503550171852 norm:0.0018378295935690403 max memory_allocated 29271.39501953125 
[2025-03-04 13:40:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.02639414370059967 norm:0.00182178127579391 max memory_allocated 29271.39501953125 
[2025-03-04 13:40:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-04 13:41:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.08281663805246353 norm:0.011445283889770508 max memory_allocated 29271.39501953125 
[2025-03-04 13:42:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.06042570620775223 norm:0.0044331117533147335 max memory_allocated 29271.39501953125 
[2025-03-04 13:43:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.05164424702525139 norm:0.0036395182833075523 max memory_allocated 29271.39501953125 
[2025-03-04 13:43:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.04862477257847786 norm:0.0036597829312086105 max memory_allocated 29271.39501953125 
[2025-03-04 13:44:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.04650557041168213 norm:0.002673223614692688 max memory_allocated 29271.39501953125 
[2025-03-04 13:45:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.04580363258719444 norm:0.002956648822873831 max memory_allocated 29271.39501953125 
[2025-03-04 13:46:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.044000450521707535 norm:0.0030066552571952343 max memory_allocated 29271.39501953125 
[2025-03-04 13:47:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.04306735843420029 norm:0.0027435915544629097 max memory_allocated 29271.39501953125 
[2025-03-04 13:48:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.04308687523007393 norm:0.0028102262876927853 max memory_allocated 29271.39501953125 
[2025-03-04 13:48:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.04188191145658493 norm:0.0025776878464967012 max memory_allocated 29271.39501953125 
[2025-03-04 13:49:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.042011454701423645 norm:0.0025004390627145767 max memory_allocated 29271.39501953125 
[2025-03-04 13:50:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.040813323110342026 norm:0.002250595949590206 max memory_allocated 29271.39501953125 
[2025-03-04 13:51:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.04028020054101944 norm:0.0017740704352036119 max memory_allocated 29271.39501953125 
[2025-03-04 13:52:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.04049462825059891 norm:0.0022252427879720926 max memory_allocated 29271.39501953125 
[2025-03-04 13:53:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0413501113653183 norm:0.0022175966296344995 max memory_allocated 29271.39501953125 
[2025-03-04 13:53:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.04001335799694061 norm:0.0017639902653172612 max memory_allocated 29271.39501953125 
[2025-03-04 13:54:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.04030534625053406 norm:0.001963662914931774 max memory_allocated 29271.39501953125 
[2025-03-04 13:55:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.040856167674064636 norm:0.0019380728481337428 max memory_allocated 29271.39501953125 
[2025-03-04 13:56:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.04059438407421112 norm:0.0017115287482738495 max memory_allocated 29271.39501953125 
[2025-03-04 13:57:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.03979428857564926 norm:0.001636673230677843 max memory_allocated 29271.39501953125 
[2025-03-04 13:57:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-04 13:58:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.05445823073387146 norm:0.0009140116162598133 max memory_allocated 29271.39501953125 
[2025-03-04 13:59:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.04743537679314613 norm:0.00041281545418314636 max memory_allocated 29271.39501953125 
[2025-03-04 14:00:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.04288763180375099 norm:0.00026657196576707065 max memory_allocated 29271.39501953125 
[2025-03-04 14:01:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.04134189337491989 norm:0.00020886099082417786 max memory_allocated 29271.39501953125 
[2025-03-04 14:02:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.04054016247391701 norm:0.00018508585344534367 max memory_allocated 29271.39501953125 
[2025-03-04 14:02:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.040016621351242065 norm:0.00017932304763235152 max memory_allocated 29271.39501953125 
[2025-03-04 14:03:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.03966177999973297 norm:0.0001702436711639166 max memory_allocated 29271.39501953125 
[2025-03-04 14:04:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.03946444019675255 norm:0.00017339421901851892 max memory_allocated 29271.39501953125 
[2025-03-04 14:05:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.03937268629670143 norm:0.00017560568812768906 max memory_allocated 29271.39501953125 
[2025-03-04 14:06:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.03931809216737747 norm:0.0001653667277423665 max memory_allocated 29271.39501953125 
[2025-03-04 14:07:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.039312779903411865 norm:0.00017193549138028175 max memory_allocated 29271.39501953125 
[2025-03-04 14:07:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.03928034007549286 norm:0.0001688457268755883 max memory_allocated 29271.39501953125 
[2025-03-04 14:08:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.039249908179044724 norm:0.00016942825459409505 max memory_allocated 29271.39501953125 
[2025-03-04 14:09:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.03924486041069031 norm:0.00016213870549108833 max memory_allocated 29271.39501953125 
[2025-03-04 14:10:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.039261624217033386 norm:0.0001688425982138142 max memory_allocated 29271.39501953125 
[2025-03-04 14:11:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.03924832493066788 norm:0.00017050604219548404 max memory_allocated 29271.39501953125 
[2025-03-04 14:12:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.039238087832927704 norm:0.00017779192421585321 max memory_allocated 29271.39501953125 
[2025-03-04 14:12:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.039231158792972565 norm:0.00017259996093343943 max memory_allocated 29271.39501953125 
[2025-03-04 14:13:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.03920654207468033 norm:0.000172962318174541 max memory_allocated 29271.39501953125 
[2025-03-04 14:14:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.039196375757455826 norm:0.00017527720774523914 max memory_allocated 29271.39501953125 
[2025-03-04 14:14:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-04 14:15:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.06164724752306938 norm:0.0014325310476124287 max memory_allocated 29271.81298828125 
[2025-03-04 14:16:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.05283694714307785 norm:0.0005714803701266646 max memory_allocated 29271.81298828125 
[2025-03-04 14:17:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.047770675271749496 norm:0.00033237008028663695 max memory_allocated 29271.81298828125 
[2025-03-04 14:18:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.046008579432964325 norm:0.00022414783597923815 max memory_allocated 29271.81298828125 
[2025-03-04 14:19:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.04504761844873428 norm:0.00018420135893393308 max memory_allocated 29271.81298828125 
[2025-03-04 14:20:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.04442322626709938 norm:0.0001668241311563179 max memory_allocated 29271.81298828125 
[2025-03-04 14:20:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.04404938220977783 norm:0.0001616089284652844 max memory_allocated 29271.81298828125 
[2025-03-04 14:21:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.04383805766701698 norm:0.0001559845550218597 max memory_allocated 29271.81298828125 
[2025-03-04 14:22:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.043698277324438095 norm:0.00015546927170362324 max memory_allocated 29271.81298828125 
[2025-03-04 14:23:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.04361788555979729 norm:0.0001508190034655854 max memory_allocated 29271.81298828125 
[2025-03-04 14:24:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.04356466978788376 norm:0.00015355387586168945 max memory_allocated 29271.81298828125 
[2025-03-04 14:25:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0435192845761776 norm:0.00015270005678758025 max memory_allocated 29271.81298828125 
[2025-03-04 14:25:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.043490175157785416 norm:0.0001528096036054194 max memory_allocated 29271.81298828125 
[2025-03-04 14:26:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.04348679631948471 norm:0.00015127744700293988 max memory_allocated 29271.81298828125 
[2025-03-04 14:27:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.043470658361911774 norm:0.0001494566531619057 max memory_allocated 29271.81298828125 
[2025-03-04 14:28:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.04346273094415665 norm:0.0001464648375986144 max memory_allocated 29271.81298828125 
[2025-03-04 14:29:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.04346093162894249 norm:0.00015049707144498825 max memory_allocated 29271.81298828125 
[2025-03-04 14:30:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.043453603982925415 norm:0.00014655105769634247 max memory_allocated 29271.81298828125 
[2025-03-04 14:30:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.043451808393001556 norm:0.00014722715422976762 max memory_allocated 29271.81298828125 
[2025-03-04 14:31:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.04344959557056427 norm:0.00014985399320721626 max memory_allocated 29271.81298828125 
[2025-03-04 14:32:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-04 14:33:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.06885497272014618 norm:0.0026463111862540245 max memory_allocated 29271.81298828125 
[2025-03-04 14:34:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.058459240943193436 norm:0.0012357656378298998 max memory_allocated 29271.81298828125 
[2025-03-04 14:34:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.05237998813390732 norm:0.0006937092985026538 max memory_allocated 29271.81298828125 
[2025-03-04 14:35:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.050327204167842865 norm:0.000436773436376825 max memory_allocated 29271.81298828125 
[2025-03-04 14:36:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.049179814755916595 norm:0.0002500627306289971 max memory_allocated 29271.81298828125 
[2025-03-04 14:37:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.048536233603954315 norm:0.00021463853772729635 max memory_allocated 29271.81298828125 
[2025-03-04 14:38:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.04814471676945686 norm:0.00019600072118919343 max memory_allocated 29271.81298828125 
[2025-03-04 14:39:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.04793182387948036 norm:0.0001906439574668184 max memory_allocated 29271.81298828125 
[2025-03-04 14:39:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0477876253426075 norm:0.00018773251213133335 max memory_allocated 29271.81298828125 
[2025-03-04 14:40:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.04771459847688675 norm:0.0001822453923523426 max memory_allocated 29271.81298828125 
[2025-03-04 14:41:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.047640085220336914 norm:0.00017849729920271784 max memory_allocated 29271.81298828125 
[2025-03-04 14:42:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.04758812114596367 norm:0.00017442442185711116 max memory_allocated 29271.81298828125 
[2025-03-04 14:43:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.04755151644349098 norm:0.00017487560398876667 max memory_allocated 29271.81298828125 
[2025-03-04 14:44:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.047528065741062164 norm:0.00017048552399501204 max memory_allocated 29271.81298828125 
[2025-03-04 14:44:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.04751571640372276 norm:0.0001741950400173664 max memory_allocated 29271.81298828125 
[2025-03-04 14:45:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.04747376590967178 norm:0.0001699770800769329 max memory_allocated 29271.81298828125 
[2025-03-04 14:46:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.04745277389883995 norm:0.00016652053454890847 max memory_allocated 29271.81298828125 
[2025-03-04 14:47:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.04743564873933792 norm:0.00016520393546670675 max memory_allocated 29271.81298828125 
[2025-03-04 14:48:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.04744172468781471 norm:0.00016790245717857033 max memory_allocated 29271.81298828125 
[2025-03-04 14:49:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.04743645340204239 norm:0.0001658405235502869 max memory_allocated 29271.81298828125 
[2025-03-04 14:49:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-04 14:50:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.08071170747280121 norm:0.001970031065866351 max memory_allocated 29271.81298828125 
[2025-03-04 14:51:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.06605017185211182 norm:0.0007007545209489763 max memory_allocated 29271.81298828125 
[2025-03-04 14:52:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.05847547575831413 norm:0.0003975949948653579 max memory_allocated 29271.81298828125 
[2025-03-04 14:52:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.05633708834648132 norm:0.0003022869350388646 max memory_allocated 29271.81298828125 
[2025-03-04 14:53:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.055137500166893005 norm:0.0002582340675871819 max memory_allocated 29271.81298828125 
[2025-03-04 14:54:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.05441596359014511 norm:0.00023916602367535233 max memory_allocated 29271.81298828125 
[2025-03-04 14:55:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.05394963175058365 norm:0.00022430947865359485 max memory_allocated 29271.81298828125 
[2025-03-04 14:56:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.053704507648944855 norm:0.00022365304175764322 max memory_allocated 29271.81298828125 
[2025-03-04 14:57:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.05351793020963669 norm:0.00022596878989133984 max memory_allocated 29271.81298828125 
[2025-03-04 14:57:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.05339201167225838 norm:0.00021876920072827488 max memory_allocated 29271.81298828125 
[2025-03-04 14:58:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.05332458019256592 norm:0.00021818134700879455 max memory_allocated 29271.81298828125 
[2025-03-04 14:59:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.053286511451005936 norm:0.00021274515893310308 max memory_allocated 29271.81298828125 
[2025-03-04 15:00:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.05324965715408325 norm:0.0002091908681904897 max memory_allocated 29271.81298828125 
[2025-03-04 15:01:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.05320364981889725 norm:0.00020932887855451554 max memory_allocated 29271.81298828125 
[2025-03-04 15:02:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.053166184574365616 norm:0.000202613155124709 max memory_allocated 29271.81298828125 
[2025-03-04 15:02:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.05313322693109512 norm:0.00019979040371254086 max memory_allocated 29271.81298828125 
[2025-03-04 15:03:45 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.053120099008083344 norm:0.00019856987637467682 max memory_allocated 29271.81298828125 
[2025-03-04 15:04:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.053064655512571335 norm:0.00020059218513779342 max memory_allocated 29271.81298828125 
[2025-03-04 15:05:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.05303490534424782 norm:0.0001986920542549342 max memory_allocated 29271.81298828125 
[2025-03-04 15:06:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.053054459393024445 norm:0.00020248675718903542 max memory_allocated 29271.81298828125 
[2025-03-04 15:06:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-04 15:07:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.08563042432069778 norm:0.004322560969740152 max memory_allocated 29272.37548828125 
[2025-03-04 15:08:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.07162615656852722 norm:0.0022094659507274628 max memory_allocated 29272.37548828125 
[2025-03-04 15:09:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06341573596000671 norm:0.0012734245974570513 max memory_allocated 29272.37548828125 
[2025-03-04 15:10:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.06072016432881355 norm:0.0008397847414016724 max memory_allocated 29272.37548828125 
[2025-03-04 15:11:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.05927512049674988 norm:0.000606362649705261 max memory_allocated 29272.37548828125 
[2025-03-04 15:11:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.058385275304317474 norm:0.00048335856990888715 max memory_allocated 29272.37548828125 
[2025-03-04 15:12:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.05786324664950371 norm:0.0004001555498689413 max memory_allocated 29272.37548828125 
[2025-03-04 15:13:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.05755365639925003 norm:0.0003475719131529331 max memory_allocated 29272.37548828125 
[2025-03-04 15:14:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.05733655393123627 norm:0.0003111026016995311 max memory_allocated 29272.37548828125 
[2025-03-04 15:15:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.057210952043533325 norm:0.0002758461923804134 max memory_allocated 29272.37548828125 
[2025-03-04 15:16:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.05713149905204773 norm:0.00024875043891370296 max memory_allocated 29272.37548828125 
[2025-03-04 15:16:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.05704483389854431 norm:0.00022995770268607885 max memory_allocated 29272.37548828125 
[2025-03-04 15:17:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.05699462443590164 norm:0.00021774503693450242 max memory_allocated 29272.37548828125 
[2025-03-04 15:18:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.05691776052117348 norm:0.00019908291869796813 max memory_allocated 29272.37548828125 
[2025-03-04 15:19:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.05688245967030525 norm:0.00018981058383360505 max memory_allocated 29272.37548828125 
[2025-03-04 15:20:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.056888651102781296 norm:0.0001862948847701773 max memory_allocated 29272.37548828125 
[2025-03-04 15:21:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.056872978806495667 norm:0.00018068894860334694 max memory_allocated 29272.37548828125 
[2025-03-04 15:21:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.05684699863195419 norm:0.0001752987882355228 max memory_allocated 29272.37548828125 
[2025-03-04 15:22:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.05683652311563492 norm:0.00017140727140940726 max memory_allocated 29272.37548828125 
[2025-03-04 15:23:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.05683789774775505 norm:0.00017301943444181234 max memory_allocated 29272.37548828125 
[2025-03-04 15:23:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-04 15:25:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.11300830543041229 norm:0.004453741945326328 max memory_allocated 29272.56298828125 
[2025-03-04 15:25:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.08496617525815964 norm:0.0016154029872268438 max memory_allocated 29272.56298828125 
[2025-03-04 15:26:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.07140906900167465 norm:0.0007477960316464305 max memory_allocated 29272.56298828125 
[2025-03-04 15:27:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.06750515103340149 norm:0.00048663667985238135 max memory_allocated 29272.56298828125 
[2025-03-04 15:28:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.06566165387630463 norm:0.00039947303594090044 max memory_allocated 29272.56298828125 
[2025-03-04 15:29:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.06453420221805573 norm:0.0003563139180187136 max memory_allocated 29272.56298828125 
[2025-03-04 15:30:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0638391301035881 norm:0.0003155739395879209 max memory_allocated 29272.56298828125 
[2025-03-04 15:30:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.06332229822874069 norm:0.00028901611221954226 max memory_allocated 29272.56298828125 
[2025-03-04 15:31:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.06310898065567017 norm:0.00028734945226460695 max memory_allocated 29272.56298828125 
[2025-03-04 15:32:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.06295781582593918 norm:0.0002798879286274314 max memory_allocated 29272.56298828125 
[2025-03-04 15:33:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.06275297701358795 norm:0.0002533927618060261 max memory_allocated 29272.56298828125 
[2025-03-04 15:34:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.06256064027547836 norm:0.0002383978571742773 max memory_allocated 29272.56298828125 
[2025-03-04 15:35:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.062388524413108826 norm:0.00022937393805477768 max memory_allocated 29272.56298828125 
[2025-03-04 15:35:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.06230664998292923 norm:0.00022546174295712262 max memory_allocated 29272.56298828125 
[2025-03-04 15:36:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0622207373380661 norm:0.00021247926633805037 max memory_allocated 29272.56298828125 
[2025-03-04 15:37:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.062238890677690506 norm:0.0002182657626690343 max memory_allocated 29272.56298828125 
[2025-03-04 15:38:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.06219958886504173 norm:0.0002150077052647248 max memory_allocated 29272.56298828125 
[2025-03-04 15:39:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06213924288749695 norm:0.00020021104137413204 max memory_allocated 29272.56298828125 
[2025-03-04 15:40:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.06210741400718689 norm:0.00020021060481667519 max memory_allocated 29272.56298828125 
[2025-03-04 15:40:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.062062039971351624 norm:0.00019591741147451103 max memory_allocated 29272.56298828125 
[2025-03-04 15:41:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-04 15:42:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.09951470792293549 norm:0.0032329712994396687 max memory_allocated 29272.75048828125 
[2025-03-04 15:43:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0828508585691452 norm:0.0011884134728461504 max memory_allocated 29272.75048828125 
[2025-03-04 15:43:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.07337412238121033 norm:0.000576724938582629 max memory_allocated 29272.75048828125 
[2025-03-04 15:44:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.07016859948635101 norm:0.0003822734870482236 max memory_allocated 29272.75048828125 
[2025-03-04 15:45:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.06862218677997589 norm:0.00029764111968688667 max memory_allocated 29272.75048828125 
[2025-03-04 15:46:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.06773819774389267 norm:0.00026098533999174833 max memory_allocated 29272.75048828125 
[2025-03-04 15:47:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.06725029647350311 norm:0.0002380533260293305 max memory_allocated 29272.75048828125 
[2025-03-04 15:48:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.06695016473531723 norm:0.00022026392980478704 max memory_allocated 29272.75048828125 
[2025-03-04 15:48:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.06682164967060089 norm:0.00021357170771807432 max memory_allocated 29272.75048828125 
[2025-03-04 15:49:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0666794404387474 norm:0.00020363563089631498 max memory_allocated 29272.75048828125 
[2025-03-04 15:50:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.06657207012176514 norm:0.00019800524751190096 max memory_allocated 29272.75048828125 
[2025-03-04 15:51:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0664941594004631 norm:0.0001881858188426122 max memory_allocated 29272.75048828125 
[2025-03-04 15:52:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.06642358750104904 norm:0.00018497022392693907 max memory_allocated 29272.75048828125 
[2025-03-04 15:53:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.06638806313276291 norm:0.00018011793144978583 max memory_allocated 29272.75048828125 
[2025-03-04 15:53:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.06632612645626068 norm:0.00017813866725191474 max memory_allocated 29272.75048828125 
[2025-03-04 15:54:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.06630271673202515 norm:0.0001721296866890043 max memory_allocated 29272.75048828125 
[2025-03-04 15:55:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.06625895202159882 norm:0.00016723327280487865 max memory_allocated 29272.75048828125 
[2025-03-04 15:56:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.06624196469783783 norm:0.0001648249162826687 max memory_allocated 29272.75048828125 
[2025-03-04 15:57:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.06622873991727829 norm:0.00016728625632822514 max memory_allocated 29272.75048828125 
[2025-03-04 15:58:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.06618648767471313 norm:0.00016409165982622653 max memory_allocated 29272.75048828125 
[2025-03-04 15:58:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-04 15:59:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.09958940744400024 norm:0.002812382997944951 max memory_allocated 29272.93798828125 
[2025-03-04 16:00:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.08493748307228088 norm:0.0010573279578238726 max memory_allocated 29272.93798828125 
[2025-03-04 16:01:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.07612201571464539 norm:0.000522307469509542 max memory_allocated 29272.93798828125 
[2025-03-04 16:02:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.07326551526784897 norm:0.0003106136282440275 max memory_allocated 29272.93798828125 
[2025-03-04 16:02:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.07200850546360016 norm:0.0002336838806513697 max memory_allocated 29272.93798828125 
[2025-03-04 16:03:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.07128480821847916 norm:0.00020711550314445049 max memory_allocated 29272.93798828125 
[2025-03-04 16:04:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.07084207981824875 norm:0.0001891445426736027 max memory_allocated 29272.93798828125 
[2025-03-04 16:05:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.07060567289590836 norm:0.00017871122690849006 max memory_allocated 29272.93798828125 
[2025-03-04 16:06:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.07045836746692657 norm:0.0001733552635414526 max memory_allocated 29272.93798828125 
[2025-03-04 16:07:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.07035793364048004 norm:0.0001717608392937109 max memory_allocated 29272.93798828125 
[2025-03-04 16:07:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.07027100026607513 norm:0.00016566063277423382 max memory_allocated 29272.93798828125 
[2025-03-04 16:08:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.07020503282546997 norm:0.00016037921886891127 max memory_allocated 29272.93798828125 
[2025-03-04 16:09:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.07018740475177765 norm:0.0001590882457094267 max memory_allocated 29272.93798828125 
[2025-03-04 16:10:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.07015583664178848 norm:0.00015604107466060668 max memory_allocated 29272.93798828125 
[2025-03-04 16:11:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.07012040913105011 norm:0.0001532068708911538 max memory_allocated 29272.93798828125 
[2025-03-04 16:12:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.07008025050163269 norm:0.00015553535195067525 max memory_allocated 29272.93798828125 
[2025-03-04 16:12:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.07003931701183319 norm:0.0001499812351539731 max memory_allocated 29272.93798828125 
[2025-03-04 16:13:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0700269341468811 norm:0.00015004845045041293 max memory_allocated 29272.93798828125 
[2025-03-04 16:14:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.07000931352376938 norm:0.00014832147280685604 max memory_allocated 29272.93798828125 
[2025-03-04 16:15:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.07001099735498428 norm:0.00014915355131961405 max memory_allocated 29272.93798828125 
[2025-03-04 16:15:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-04 16:16:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.1080092191696167 norm:0.0027050364296883345 max memory_allocated 29273.12548828125 
[2025-03-04 16:17:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.08947688341140747 norm:0.0010039324406534433 max memory_allocated 29273.12548828125 
[2025-03-04 16:18:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.07956486195325851 norm:0.0004650663468055427 max memory_allocated 29273.12548828125 
[2025-03-04 16:19:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.07644105702638626 norm:0.00030452467035502195 max memory_allocated 29273.12548828125 
[2025-03-04 16:20:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.07504244893789291 norm:0.00024825878790579736 max memory_allocated 29273.12548828125 
[2025-03-04 16:20:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.07425184547901154 norm:0.00021608604583889246 max memory_allocated 29273.12548828125 
[2025-03-04 16:21:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.07375480979681015 norm:0.00019475791486911476 max memory_allocated 29273.12548828125 
[2025-03-04 16:22:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.07346278429031372 norm:0.00018140760948881507 max memory_allocated 29273.12548828125 
[2025-03-04 16:23:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0732901394367218 norm:0.0001780186576070264 max memory_allocated 29273.12548828125 
[2025-03-04 16:24:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.07317763566970825 norm:0.00016849354142323136 max memory_allocated 29273.12548828125 
[2025-03-04 16:25:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.07305161654949188 norm:0.00015865956083871424 max memory_allocated 29273.12548828125 
[2025-03-04 16:25:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.07296191155910492 norm:0.00015231038560159504 max memory_allocated 29273.12548828125 
[2025-03-04 16:26:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.07292255759239197 norm:0.0001479123457102105 max memory_allocated 29273.12548828125 
[2025-03-04 16:27:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.07291325181722641 norm:0.00014772731810808182 max memory_allocated 29273.12548828125 
[2025-03-04 16:28:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.07291430234909058 norm:0.000149331332067959 max memory_allocated 29273.12548828125 
[2025-03-04 16:29:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.07285425066947937 norm:0.00014895883214194328 max memory_allocated 29273.12548828125 
[2025-03-04 16:30:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0728055015206337 norm:0.00014639575965702534 max memory_allocated 29273.12548828125 
[2025-03-04 16:30:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0727844387292862 norm:0.0001449335104553029 max memory_allocated 29273.12548828125 
[2025-03-04 16:31:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0727417916059494 norm:0.00014129737974144518 max memory_allocated 29273.12548828125 
[2025-03-04 16:32:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.07270021736621857 norm:0.00013953341112937778 max memory_allocated 29273.12548828125 
[2025-03-04 16:32:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-04 16:34:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.10760753601789474 norm:0.0033084300812333822 max memory_allocated 29273.31298828125 
[2025-03-04 16:34:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.09201183915138245 norm:0.0012805314036086202 max memory_allocated 29273.31298828125 
[2025-03-04 16:35:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.08285966515541077 norm:0.0006275827763602138 max memory_allocated 29273.31298828125 
[2025-03-04 16:36:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0797996073961258 norm:0.00043585285311564803 max memory_allocated 29273.31298828125 
[2025-03-04 16:37:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.07834073156118393 norm:0.0003441841690801084 max memory_allocated 29273.31298828125 
[2025-03-04 16:38:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.07744517922401428 norm:0.0002873299235943705 max memory_allocated 29273.31298828125 
[2025-03-04 16:39:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.07690062373876572 norm:0.0002482067502569407 max memory_allocated 29273.31298828125 
[2025-03-04 16:39:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0765807256102562 norm:0.00022747120237909257 max memory_allocated 29273.31298828125 
[2025-03-04 16:40:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.07637248933315277 norm:0.00021183346689213067 max memory_allocated 29273.31298828125 
[2025-03-04 16:41:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.07621205598115921 norm:0.0002010389434872195 max memory_allocated 29273.31298828125 
[2025-03-04 16:42:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.07608658075332642 norm:0.00018870200437959284 max memory_allocated 29273.31298828125 
[2025-03-04 16:43:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0759924128651619 norm:0.0001787229412002489 max memory_allocated 29273.31298828125 
[2025-03-04 16:44:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0759538859128952 norm:0.00017506867880001664 max memory_allocated 29273.31298828125 
[2025-03-04 16:44:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.07592685520648956 norm:0.0001672699290793389 max memory_allocated 29273.31298828125 
[2025-03-04 16:45:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.07585001736879349 norm:0.00016363475879188627 max memory_allocated 29273.31298828125 
[2025-03-04 16:46:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.07582559436559677 norm:0.0001591785257915035 max memory_allocated 29273.31298828125 
[2025-03-04 16:47:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.07580073177814484 norm:0.00015277029888238758 max memory_allocated 29273.31298828125 
[2025-03-04 16:48:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.07573942095041275 norm:0.00014850155275780708 max memory_allocated 29273.31298828125 
[2025-03-04 16:49:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.07571491599082947 norm:0.00014673997065983713 max memory_allocated 29273.31298828125 
[2025-03-04 16:49:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.07568106055259705 norm:0.00014584953896701336 max memory_allocated 29273.31298828125 
[2025-03-04 16:50:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-04 16:51:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.11143673956394196 norm:0.00303198816254735 max memory_allocated 29273.50048828125 
[2025-03-04 16:52:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.09650819003582001 norm:0.0013603493571281433 max memory_allocated 29273.50048828125 
[2025-03-04 16:53:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.08650534600019455 norm:0.0006786200683563948 max memory_allocated 29273.50048828125 
[2025-03-04 16:53:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.08320435881614685 norm:0.0004406103689689189 max memory_allocated 29273.50048828125 
[2025-03-04 16:54:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.08186986297369003 norm:0.00034437241265550256 max memory_allocated 29273.50048828125 
[2025-03-04 16:55:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.08109938353300095 norm:0.0002955971285700798 max memory_allocated 29273.50048828125 
[2025-03-04 16:56:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.08054427802562714 norm:0.0002428757434245199 max memory_allocated 29273.50048828125 
[2025-03-04 16:57:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.08021698147058487 norm:0.00022358441492542624 max memory_allocated 29273.50048828125 
[2025-03-04 16:58:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.08004103600978851 norm:0.00021144149650353938 max memory_allocated 29273.50048828125 
[2025-03-04 16:58:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.07988255470991135 norm:0.00019461351621430367 max memory_allocated 29273.50048828125 
[2025-03-04 16:59:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.07975079864263535 norm:0.00017953416681848466 max memory_allocated 29273.50048828125 
[2025-03-04 17:00:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.07965131849050522 norm:0.00017165087047033012 max memory_allocated 29273.50048828125 
[2025-03-04 17:01:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.07960225641727448 norm:0.0001674430095590651 max memory_allocated 29273.50048828125 
[2025-03-04 17:02:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.07953652739524841 norm:0.000160259660333395 max memory_allocated 29273.50048828125 
[2025-03-04 17:03:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.07948873192071915 norm:0.0001553285983391106 max memory_allocated 29273.50048828125 
[2025-03-04 17:03:55 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.07945065200328827 norm:0.0001535232295282185 max memory_allocated 29273.50048828125 
[2025-03-04 17:04:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.07939216494560242 norm:0.00014595009270124137 max memory_allocated 29273.50048828125 
[2025-03-04 17:05:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.0793532133102417 norm:0.00014207222557161003 max memory_allocated 29273.50048828125 
[2025-03-04 17:06:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.079311303794384 norm:0.0001402979833073914 max memory_allocated 29273.50048828125 
[2025-03-04 17:07:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.07930900156497955 norm:0.00013971612497698516 max memory_allocated 29273.50048828125 
[2025-03-04 17:07:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-04 17:08:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.109253890812397 norm:0.0032919305376708508 max memory_allocated 29273.68798828125 
[2025-03-04 17:09:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.09635445475578308 norm:0.0013213952770456672 max memory_allocated 29273.68798828125 
[2025-03-04 17:10:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0871906727552414 norm:0.0005729973781853914 max memory_allocated 29273.68798828125 
[2025-03-04 17:11:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.08421961218118668 norm:0.0003551609115675092 max memory_allocated 29273.68798828125 
[2025-03-04 17:11:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.08291314542293549 norm:0.0002654893323779106 max memory_allocated 29273.68798828125 
[2025-03-04 17:12:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.08218085765838623 norm:0.00024263041268568486 max memory_allocated 29273.68798828125 
[2025-03-04 17:13:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.08170172572135925 norm:0.00021068478235974908 max memory_allocated 29273.68798828125 
[2025-03-04 17:14:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0814039409160614 norm:0.00020112628408242017 max memory_allocated 29273.68798828125 
[2025-03-04 17:15:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.08121150732040405 norm:0.00019203624106012285 max memory_allocated 29273.68798828125 
[2025-03-04 17:16:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.08108008652925491 norm:0.00018452030781190842 max memory_allocated 29273.68798828125 
[2025-03-04 17:16:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.08096460998058319 norm:0.00017350580310449004 max memory_allocated 29273.68798828125 
[2025-03-04 17:17:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.08088800311088562 norm:0.00016659573884680867 max memory_allocated 29273.68798828125 
[2025-03-04 17:18:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.08084680140018463 norm:0.00016480725025758147 max memory_allocated 29273.68798828125 
[2025-03-04 17:19:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.08081643283367157 norm:0.00015776942018419504 max memory_allocated 29273.68798828125 
[2025-03-04 17:20:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.08078185468912125 norm:0.00015252773300744593 max memory_allocated 29273.68798828125 
[2025-03-04 17:21:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.08073446154594421 norm:0.00014985275629442185 max memory_allocated 29273.68798828125 
[2025-03-04 17:21:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.08070897310972214 norm:0.000144803270814009 max memory_allocated 29273.68798828125 
[2025-03-04 17:22:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.08068839460611343 norm:0.00014105174341239035 max memory_allocated 29273.68798828125 
[2025-03-04 17:23:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0806783065199852 norm:0.000138837902341038 max memory_allocated 29273.68798828125 
[2025-03-04 17:24:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.08066701143980026 norm:0.00013809739903081208 max memory_allocated 29273.68798828125 
[2025-03-04 17:24:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-04 17:25:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.12333227694034576 norm:0.00746869295835495 max memory_allocated 29273.87548828125 
[2025-03-04 17:26:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.10441748797893524 norm:0.0026153528597205877 max memory_allocated 29273.87548828125 
[2025-03-04 17:27:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.09253185242414474 norm:0.0010316201951354742 max memory_allocated 29273.87548828125 
[2025-03-04 17:28:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.08865249156951904 norm:0.0005833128816448152 max memory_allocated 29273.87548828125 
[2025-03-04 17:28:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.08702124655246735 norm:0.0004610852338373661 max memory_allocated 29273.87548828125 
[2025-03-04 17:29:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.08605189621448517 norm:0.00039153493707999587 max memory_allocated 29273.87548828125 
[2025-03-04 17:30:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.08541091531515121 norm:0.0003529060631990433 max memory_allocated 29273.87548828125 
[2025-03-04 17:31:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.08496270328760147 norm:0.00029982850537635386 max memory_allocated 29273.87548828125 
[2025-03-04 17:32:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.08472396433353424 norm:0.00030191929545253515 max memory_allocated 29273.87548828125 
[2025-03-04 17:33:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.08453498035669327 norm:0.0002838397631421685 max memory_allocated 29273.87548828125 
[2025-03-04 17:33:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.08436843752861023 norm:0.000265883922111243 max memory_allocated 29273.87548828125 
[2025-03-04 17:34:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.08423685282468796 norm:0.00025697724777273834 max memory_allocated 29273.87548828125 
[2025-03-04 17:35:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.08412375301122665 norm:0.00024330044107045978 max memory_allocated 29273.87548828125 
[2025-03-04 17:36:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.08406279981136322 norm:0.00024398294044658542 max memory_allocated 29273.87548828125 
[2025-03-04 17:37:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.08397959172725677 norm:0.00022921492927707732 max memory_allocated 29273.87548828125 
[2025-03-04 17:38:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.08391419053077698 norm:0.00022474625438917428 max memory_allocated 29273.87548828125 
[2025-03-04 17:39:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.08385373651981354 norm:0.00021266132534947246 max memory_allocated 29273.87548828125 
[2025-03-04 17:39:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.08378978818655014 norm:0.00020075906650163233 max memory_allocated 29273.87548828125 
[2025-03-04 17:40:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.08372620493173599 norm:0.0001893460430437699 max memory_allocated 29273.87548828125 
[2025-03-04 17:41:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.08368631452322006 norm:0.00018377833475824445 max memory_allocated 29273.87548828125 
[2025-03-04 17:41:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-04 17:42:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.11579935252666473 norm:0.006512722000479698 max memory_allocated 29274.06298828125 
[2025-03-04 17:43:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.10164868831634521 norm:0.0022871303372085094 max memory_allocated 29274.06298828125 
[2025-03-04 17:44:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.09221172332763672 norm:0.0007356500136666 max memory_allocated 29274.06298828125 
[2025-03-04 17:45:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.08939123153686523 norm:0.0004102847306057811 max memory_allocated 29274.06298828125 
[2025-03-04 17:46:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0880017951130867 norm:0.0003201569488737732 max memory_allocated 29274.06298828125 
[2025-03-04 17:46:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.08713889867067337 norm:0.0002986490144394338 max memory_allocated 29274.06298828125 
[2025-03-04 17:47:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.08660846203565598 norm:0.000280113861663267 max memory_allocated 29274.06298828125 
[2025-03-04 17:48:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.08628927916288376 norm:0.0002702418714761734 max memory_allocated 29274.06298828125 
[2025-03-04 17:49:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.08611302822828293 norm:0.00025701409322209656 max memory_allocated 29274.06298828125 
[2025-03-04 17:50:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.08597031235694885 norm:0.0002435836649965495 max memory_allocated 29274.06298828125 
[2025-03-04 17:51:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.0858457162976265 norm:0.00023022133973427117 max memory_allocated 29274.06298828125 
[2025-03-04 17:51:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.08575797080993652 norm:0.00022301792341750115 max memory_allocated 29274.06298828125 
[2025-03-04 17:52:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.08569762855768204 norm:0.0002128459746018052 max memory_allocated 29274.06298828125 
[2025-03-04 17:53:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.08561235666275024 norm:0.00019846443319693208 max memory_allocated 29274.06298828125 
[2025-03-04 17:54:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.08555210381746292 norm:0.0001943339011631906 max memory_allocated 29274.06298828125 
[2025-03-04 17:55:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.08553694188594818 norm:0.00019681843696162105 max memory_allocated 29274.06298828125 
[2025-03-04 17:56:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.08549990504980087 norm:0.0001872974680736661 max memory_allocated 29274.06298828125 
[2025-03-04 17:56:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0854397565126419 norm:0.00017208191275130957 max memory_allocated 29274.06298828125 
[2025-03-04 17:57:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.08542105555534363 norm:0.00017203138850163668 max memory_allocated 29274.06298828125 
[2025-03-04 17:58:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.08540654927492142 norm:0.00017007876886054873 max memory_allocated 29274.06298828125 
[2025-03-04 17:58:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-04 17:59:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.11698709428310394 norm:0.007189623080193996 max memory_allocated 29274.25048828125 
[2025-03-04 18:00:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.10255873203277588 norm:0.002192889340221882 max memory_allocated 29274.25048828125 
[2025-03-04 18:01:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.09425381571054459 norm:0.0007548414287157357 max memory_allocated 29274.25048828125 
[2025-03-04 18:02:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.09186013042926788 norm:0.00043135203304700553 max memory_allocated 29274.25048828125 
[2025-03-04 18:03:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.09057699143886566 norm:0.00034734950168058276 max memory_allocated 29274.25048828125 
[2025-03-04 18:03:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.08974772691726685 norm:0.00032895506592467427 max memory_allocated 29274.25048828125 
[2025-03-04 18:04:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.08925719559192657 norm:0.00030793697806075215 max memory_allocated 29274.25048828125 
[2025-03-04 18:05:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.08891043812036514 norm:0.0002716604503802955 max memory_allocated 29274.25048828125 
[2025-03-04 18:06:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.08867069333791733 norm:0.00024412064522039145 max memory_allocated 29274.25048828125 
[2025-03-04 18:07:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.08850043267011642 norm:0.0002283200155943632 max memory_allocated 29274.25048828125 
[2025-03-04 18:08:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.08839371055364609 norm:0.00022128812270238996 max memory_allocated 29274.25048828125 
[2025-03-04 18:08:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.08831238746643066 norm:0.00021105541964061558 max memory_allocated 29274.25048828125 
[2025-03-04 18:09:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.08824241906404495 norm:0.00020537199452519417 max memory_allocated 29274.25048828125 
[2025-03-04 18:10:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.08818463981151581 norm:0.00019203376723453403 max memory_allocated 29274.25048828125 
[2025-03-04 18:11:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.0881303995847702 norm:0.00018814855138771236 max memory_allocated 29274.25048828125 
[2025-03-04 18:12:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.0880785584449768 norm:0.00017311527335550636 max memory_allocated 29274.25048828125 
[2025-03-04 18:13:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.08804604411125183 norm:0.00017313503485638648 max memory_allocated 29274.25048828125 
[2025-03-04 18:13:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.08803579956293106 norm:0.00017153119551949203 max memory_allocated 29274.25048828125 
[2025-03-04 18:14:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.08800540864467621 norm:0.00016582194075454026 max memory_allocated 29274.25048828125 
[2025-03-04 18:15:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.08797188103199005 norm:0.00016108345880638808 max memory_allocated 29274.25048828125 
[2025-03-04 18:15:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-04 18:16:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.1194937601685524 norm:0.006024572532624006 max memory_allocated 29274.43798828125 
[2025-03-04 18:17:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.10720978677272797 norm:0.0019640016835182905 max memory_allocated 29274.43798828125 
[2025-03-04 18:18:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.09905675798654556 norm:0.0007339011644944549 max memory_allocated 29274.43798828125 
[2025-03-04 18:19:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.09661208093166351 norm:0.0004259442794136703 max memory_allocated 29274.43798828125 
[2025-03-04 18:20:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.09534986317157745 norm:0.00033941378933377564 max memory_allocated 29274.43798828125 
[2025-03-04 18:20:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.09457452595233917 norm:0.00033486197935417295 max memory_allocated 29274.43798828125 
[2025-03-04 18:21:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.09400033950805664 norm:0.00027833558851853013 max memory_allocated 29274.43798828125 
[2025-03-04 18:22:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.09365960955619812 norm:0.00026886534760706127 max memory_allocated 29274.43798828125 
[2025-03-04 18:23:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.09344908595085144 norm:0.00024971814127638936 max memory_allocated 29274.43798828125 
[2025-03-04 18:24:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.09327301383018494 norm:0.0002279043401358649 max memory_allocated 29274.43798828125 
[2025-03-04 18:25:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.09316171705722809 norm:0.0002159371506422758 max memory_allocated 29274.43798828125 
[2025-03-04 18:25:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.09310577809810638 norm:0.0002145509497495368 max memory_allocated 29274.43798828125 
[2025-03-04 18:26:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.0930289775133133 norm:0.0002020968240685761 max memory_allocated 29274.43798828125 
[2025-03-04 18:27:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.09293867647647858 norm:0.000185350829269737 max memory_allocated 29274.43798828125 
[2025-03-04 18:28:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.09287842363119125 norm:0.00017580184794496745 max memory_allocated 29274.43798828125 
[2025-03-04 18:29:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.09281790256500244 norm:0.00016925287491176277 max memory_allocated 29274.43798828125 
[2025-03-04 18:30:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.0927843302488327 norm:0.00016559641517233104 max memory_allocated 29274.43798828125 
[2025-03-04 18:30:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.0927487164735794 norm:0.00016045292431954294 max memory_allocated 29274.43798828125 
[2025-03-04 18:31:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.09272265434265137 norm:0.00015841783897485584 max memory_allocated 29274.43798828125 
[2025-03-04 18:32:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0926920548081398 norm:0.00014905742136761546 max memory_allocated 29274.43798828125 
[2025-03-04 18:32:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-04 18:33:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.1219894289970398 norm:0.005085664801299572 max memory_allocated 29274.62548828125 
[2025-03-04 18:34:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.11098013818264008 norm:0.001545398379676044 max memory_allocated 29274.62548828125 
[2025-03-04 18:35:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.10368068516254425 norm:0.0006365565932355821 max memory_allocated 29274.62548828125 
[2025-03-04 18:36:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.10145759582519531 norm:0.0004123118123970926 max memory_allocated 29274.62548828125 
[2025-03-04 18:37:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.10027482360601425 norm:0.0003358206886332482 max memory_allocated 29274.62548828125 
[2025-03-04 18:38:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0994667261838913 norm:0.00030008505564182997 max memory_allocated 29274.62548828125 
[2025-03-04 18:38:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.09890905767679214 norm:0.0002566437469795346 max memory_allocated 29274.62548828125 
[2025-03-04 18:39:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.09858956187963486 norm:0.0002327675756532699 max memory_allocated 29274.62548828125 
[2025-03-04 18:40:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.09839285165071487 norm:0.00022267457097768784 max memory_allocated 29274.62548828125 
[2025-03-04 18:41:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.09828342497348785 norm:0.00021310349984560162 max memory_allocated 29274.62548828125 
[2025-03-04 18:42:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.09815164655447006 norm:0.00019244325812906027 max memory_allocated 29274.62548828125 
[2025-03-04 18:43:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.09804539382457733 norm:0.00018313477630726993 max memory_allocated 29274.62548828125 
[2025-03-04 18:43:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.09799633920192719 norm:0.00017965235747396946 max memory_allocated 29274.62548828125 
[2025-03-04 18:44:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.09793306887149811 norm:0.00017542012210469693 max memory_allocated 29274.62548828125 
[2025-03-04 18:45:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.09785788506269455 norm:0.0001651169586693868 max memory_allocated 29274.62548828125 
[2025-03-04 18:46:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.09781941771507263 norm:0.00016108072304632515 max memory_allocated 29274.62548828125 
[2025-03-04 18:47:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.09779731929302216 norm:0.0001566942810313776 max memory_allocated 29274.62548828125 
[2025-03-04 18:48:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.09779021143913269 norm:0.00015148022794164717 max memory_allocated 29274.62548828125 
[2025-03-04 18:48:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.09776763617992401 norm:0.00014855664630886167 max memory_allocated 29274.62548828125 
[2025-03-04 18:49:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.09775646775960922 norm:0.0001449723495170474 max memory_allocated 29274.62548828125 
[2025-03-04 18:49:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-04 18:50:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.1327306628227234 norm:0.0043221451342105865 max memory_allocated 29274.81298828125 
[2025-03-04 18:51:47 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.121567502617836 norm:0.001673034392297268 max memory_allocated 29274.81298828125 
[2025-03-04 18:52:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.11333906650543213 norm:0.0007559807854704559 max memory_allocated 29274.81298828125 
[2025-03-04 18:53:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.11067351698875427 norm:0.0004341338644735515 max memory_allocated 29274.81298828125 
[2025-03-04 18:54:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.10943365842103958 norm:0.0003219727077521384 max memory_allocated 29274.81298828125 
[2025-03-04 18:55:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.10857650637626648 norm:0.0002966324973385781 max memory_allocated 29274.81298828125 
[2025-03-04 18:55:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.10802975296974182 norm:0.00027969462098553777 max memory_allocated 29274.81298828125 
[2025-03-04 18:56:47 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.1077042669057846 norm:0.0002575113030616194 max memory_allocated 29274.81298828125 
[2025-03-04 18:57:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.10749451071023941 norm:0.00024255072639789432 max memory_allocated 29274.81298828125 
[2025-03-04 18:58:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.10732053965330124 norm:0.00023152807261794806 max memory_allocated 29274.81298828125 
[2025-03-04 18:59:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.10721783339977264 norm:0.00022197638463694602 max memory_allocated 29274.81298828125 
[2025-03-04 19:00:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.10712546110153198 norm:0.0002122738369507715 max memory_allocated 29274.81298828125 
[2025-03-04 19:00:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.10704445093870163 norm:0.00020461741951294243 max memory_allocated 29274.81298828125 
[2025-03-04 19:01:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.10699668526649475 norm:0.0002010307798627764 max memory_allocated 29274.81298828125 
[2025-03-04 19:02:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.10693933069705963 norm:0.00019179769151378423 max memory_allocated 29274.81298828125 
[2025-03-04 19:03:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.10686849802732468 norm:0.00018291767628397793 max memory_allocated 29274.81298828125 
[2025-03-04 19:04:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.10681093484163284 norm:0.0001771043025655672 max memory_allocated 29274.81298828125 
[2025-03-04 19:05:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.10678227990865707 norm:0.00017406542610842735 max memory_allocated 29274.81298828125 
[2025-03-04 19:05:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.10674286633729935 norm:0.00016755267279222608 max memory_allocated 29274.81298828125 
[2025-03-04 19:06:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.10670200735330582 norm:0.00016520865028724074 max memory_allocated 29274.81298828125 
[2025-03-04 19:07:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-04 19:08:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.1375986635684967 norm:0.004878251813352108 max memory_allocated 29275.00048828125 
[2025-03-04 19:08:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.12790901958942413 norm:0.0012548635713756084 max memory_allocated 29275.00048828125 
[2025-03-04 19:09:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.12100754678249359 norm:0.0006276628701016307 max memory_allocated 29275.00048828125 
[2025-03-04 19:10:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.1189064234495163 norm:0.0003549677785485983 max memory_allocated 29275.00048828125 
[2025-03-04 19:11:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.11775558441877365 norm:0.000270781631115824 max memory_allocated 29275.00048828125 
[2025-03-04 19:12:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.11690070480108261 norm:0.00024062921875156462 max memory_allocated 29275.00048828125 
[2025-03-04 19:13:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.1164030134677887 norm:0.00022518352488987148 max memory_allocated 29275.00048828125 
[2025-03-04 19:13:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.1161259114742279 norm:0.00021105990163050592 max memory_allocated 29275.00048828125 
[2025-03-04 19:14:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.11596017330884933 norm:0.00019486271776258945 max memory_allocated 29275.00048828125 
[2025-03-04 19:15:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.11582941561937332 norm:0.0001827981323003769 max memory_allocated 29275.00048828125 
[2025-03-04 19:16:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.11572647839784622 norm:0.00017519512039143592 max memory_allocated 29275.00048828125 
[2025-03-04 19:17:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.11561648547649384 norm:0.0001677854306763038 max memory_allocated 29275.00048828125 
[2025-03-04 19:18:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.11554504930973053 norm:0.00016064741066657007 max memory_allocated 29275.00048828125 
[2025-03-04 19:18:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.11549778282642365 norm:0.00015707204875070602 max memory_allocated 29275.00048828125 
[2025-03-04 19:19:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.11544477194547653 norm:0.0001516273623565212 max memory_allocated 29275.00048828125 
[2025-03-04 19:20:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.11540527641773224 norm:0.0001478532503824681 max memory_allocated 29275.00048828125 
[2025-03-04 19:21:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.11537118256092072 norm:0.00014205365732777864 max memory_allocated 29275.00048828125 
[2025-03-04 19:22:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.1153247281908989 norm:0.00013775911065749824 max memory_allocated 29275.00048828125 
[2025-03-04 19:23:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.11526936292648315 norm:0.0001325226912740618 max memory_allocated 29275.00048828125 
[2025-03-04 19:23:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.11524173617362976 norm:0.0001304797042394057 max memory_allocated 29275.00048828125 
[2025-03-04 19:24:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-04 19:25:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.1472499668598175 norm:0.0033933441154658794 max memory_allocated 29275.18798828125 
[2025-03-04 19:26:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.13867008686065674 norm:0.0015551430406048894 max memory_allocated 29275.18798828125 
[2025-03-04 19:26:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.1319488137960434 norm:0.0008756998577155173 max memory_allocated 29275.18798828125 
[2025-03-04 19:27:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.12982094287872314 norm:0.0005077330861240625 max memory_allocated 29275.18798828125 
[2025-03-04 19:28:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.12857037782669067 norm:0.00026362587232142687 max memory_allocated 29275.18798828125 
[2025-03-04 19:29:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.12774428725242615 norm:0.00023297680309042335 max memory_allocated 29275.18798828125 
[2025-03-04 19:30:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.12726902961730957 norm:0.00021967578504700214 max memory_allocated 29275.18798828125 
[2025-03-04 19:31:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.1270051896572113 norm:0.0002099974371958524 max memory_allocated 29275.18798828125 
[2025-03-04 19:31:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.12684887647628784 norm:0.00019608998263720423 max memory_allocated 29275.18798828125 
[2025-03-04 19:32:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.12673670053482056 norm:0.0004496718174777925 max memory_allocated 29275.18798828125 
[2025-03-04 19:33:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.12663544714450836 norm:0.00018097554857376963 max memory_allocated 29275.18798828125 
[2025-03-04 19:34:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.12655580043792725 norm:0.00017104329890571535 max memory_allocated 29275.18798828125 
[2025-03-04 19:35:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.1264737844467163 norm:0.00016550940927118063 max memory_allocated 29275.18798828125 
[2025-03-04 19:36:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.12640027701854706 norm:0.00015908677596598864 max memory_allocated 29275.18798828125 
[2025-03-04 19:36:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.12633901834487915 norm:0.0001534185721538961 max memory_allocated 29275.18798828125 
[2025-03-04 19:37:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.12629839777946472 norm:0.00015102821635082364 max memory_allocated 29275.18798828125 
[2025-03-04 19:38:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.1262541562318802 norm:0.00014354666927829385 max memory_allocated 29275.18798828125 
[2025-03-04 19:39:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.12621352076530457 norm:0.00013945449609309435 max memory_allocated 29275.18798828125 
[2025-03-04 19:40:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.1261948049068451 norm:0.00013815617421641946 max memory_allocated 29275.18798828125 
[2025-03-04 19:41:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.12615032494068146 norm:0.00013068047701381147 max memory_allocated 29275.18798828125 
[2025-03-04 19:41:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-04 19:42:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.15882377326488495 norm:0.00517225731164217 max memory_allocated 29275.37548828125 
[2025-03-04 19:43:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.14947624504566193 norm:0.001755907665938139 max memory_allocated 29275.37548828125 
[2025-03-04 19:44:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.14264380931854248 norm:0.0007587920408695936 max memory_allocated 29275.37548828125 
[2025-03-04 19:44:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.14085209369659424 norm:0.0004400222678668797 max memory_allocated 29275.37548828125 
[2025-03-04 19:45:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.13974085450172424 norm:0.00031968907569535077 max memory_allocated 29275.37548828125 
[2025-03-04 19:46:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.13891470432281494 norm:0.0002838430809788406 max memory_allocated 29275.37548828125 
[2025-03-04 19:47:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.13843557238578796 norm:0.0002585070033092052 max memory_allocated 29275.37548828125 
[2025-03-04 19:48:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.13820867240428925 norm:0.0002461311814840883 max memory_allocated 29275.37548828125 
[2025-03-04 19:49:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.13807415962219238 norm:0.0002259380416944623 max memory_allocated 29275.37548828125 
[2025-03-04 19:49:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.13798236846923828 norm:0.00022488531249109656 max memory_allocated 29275.37548828125 
[2025-03-04 19:50:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.13789865374565125 norm:0.0002066721353912726 max memory_allocated 29275.37548828125 
[2025-03-04 19:51:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.13783328235149384 norm:0.00020122851128689945 max memory_allocated 29275.37548828125 
[2025-03-04 19:52:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.13776040077209473 norm:0.00018531738896854222 max memory_allocated 29275.37548828125 
[2025-03-04 19:53:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.13769353926181793 norm:0.00017242730245925486 max memory_allocated 29275.37548828125 
[2025-03-04 19:54:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.13764406740665436 norm:0.00016490336565766484 max memory_allocated 29275.37548828125 
[2025-03-04 19:54:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.13760699331760406 norm:0.00016720208805054426 max memory_allocated 29275.37548828125 
[2025-03-04 19:55:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.13757528364658356 norm:0.000164654033142142 max memory_allocated 29275.37548828125 
[2025-03-04 19:56:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.13755255937576294 norm:0.00016195795615203679 max memory_allocated 29275.37548828125 
[2025-03-04 19:57:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.1375204622745514 norm:0.0001574529887875542 max memory_allocated 29275.37548828125 
[2025-03-04 19:58:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.13749061524868011 norm:0.00015605649969074875 max memory_allocated 29275.37548828125 
[2025-03-04 19:58:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-04 19:59:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.17034180462360382 norm:0.0031454572454094887 max memory_allocated 29275.56298828125 
[2025-03-04 20:00:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.16252663731575012 norm:0.001370414742268622 max memory_allocated 29275.56298828125 
[2025-03-04 20:01:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.15560299158096313 norm:0.0006908044451847672 max memory_allocated 29275.56298828125 
[2025-03-04 20:02:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.15366166830062866 norm:0.00035776259028352797 max memory_allocated 29275.56298828125 
[2025-03-04 20:02:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.15257062017917633 norm:0.00028595596086233854 max memory_allocated 29275.56298828125 
[2025-03-04 20:03:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.15176086127758026 norm:0.00023796629102434963 max memory_allocated 29275.56298828125 
[2025-03-04 20:04:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.151321142911911 norm:0.0002118564152624458 max memory_allocated 29275.56298828125 
[2025-03-04 20:05:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.15112006664276123 norm:0.00020035592024214566 max memory_allocated 29275.56298828125 
[2025-03-04 20:06:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.1510002315044403 norm:0.00018664231174625456 max memory_allocated 29275.56298828125 
[2025-03-04 20:07:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.1509142816066742 norm:0.00017987580213230103 max memory_allocated 29275.56298828125 
[2025-03-04 20:07:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.1508382260799408 norm:0.00016668494208715856 max memory_allocated 29275.56298828125 
[2025-03-04 20:08:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.15075024962425232 norm:0.0001529001456219703 max memory_allocated 29275.56298828125 
[2025-03-04 20:09:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.15069802105426788 norm:0.000145383964991197 max memory_allocated 29275.56298828125 
[2025-03-04 20:10:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.15066580474376678 norm:0.00014263016055338085 max memory_allocated 29275.56298828125 
[2025-03-04 20:11:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.15061941742897034 norm:0.00013787638454232365 max memory_allocated 29275.56298828125 
[2025-03-04 20:12:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.15058718621730804 norm:0.00013213024067226797 max memory_allocated 29275.56298828125 
[2025-03-04 20:12:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.1505618691444397 norm:0.00012834180961363018 max memory_allocated 29275.56298828125 
[2025-03-04 20:13:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.15053604543209076 norm:0.00012376571248751134 max memory_allocated 29275.56298828125 
[2025-03-04 20:14:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.15050974488258362 norm:0.00011991804058197886 max memory_allocated 29275.56298828125 
[2025-03-04 20:15:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.15048709511756897 norm:0.0001167261871160008 max memory_allocated 29275.56298828125 
[2025-03-04 20:15:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-04 20:16:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.18666358292102814 norm:0.002043770160526037 max memory_allocated 29275.75048828125 
[2025-03-04 20:17:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.17871960997581482 norm:0.0010432410053908825 max memory_allocated 29275.75048828125 
[2025-03-04 20:18:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.1716158539056778 norm:0.0006180116324685514 max memory_allocated 29275.75048828125 
[2025-03-04 20:19:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.16956308484077454 norm:0.00041791415424086154 max memory_allocated 29275.75048828125 
[2025-03-04 20:20:01 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.1682194471359253 norm:0.00032532415934838355 max memory_allocated 29275.75048828125 
[2025-03-04 20:20:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.1673433482646942 norm:0.0002843691036105156 max memory_allocated 29275.75048828125 
[2025-03-04 20:21:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.1669171154499054 norm:0.0002561714791227132 max memory_allocated 29275.75048828125 
[2025-03-04 20:22:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.16665805876255035 norm:0.00022291200002655387 max memory_allocated 29275.75048828125 
[2025-03-04 20:23:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.16649380326271057 norm:0.00019827143114525825 max memory_allocated 29275.75048828125 
[2025-03-04 20:24:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.16638372838497162 norm:0.00017470144666731358 max memory_allocated 29275.75048828125 
[2025-03-04 20:25:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.1663135290145874 norm:0.00016283142031170428 max memory_allocated 29275.75048828125 
[2025-03-04 20:25:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.16619040071964264 norm:0.0001545479753986001 max memory_allocated 29275.75048828125 
[2025-03-04 20:26:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.16615961492061615 norm:0.00015511181845795363 max memory_allocated 29275.75048828125 
[2025-03-04 20:27:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.16608893871307373 norm:0.00014984533481765538 max memory_allocated 29275.75048828125 
[2025-03-04 20:28:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.16602979600429535 norm:0.00014870698214508593 max memory_allocated 29275.75048828125 
[2025-03-04 20:29:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.16601262986660004 norm:0.00014712964184582233 max memory_allocated 29275.75048828125 
[2025-03-04 20:30:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.1659558117389679 norm:0.00014254760753829032 max memory_allocated 29275.75048828125 
[2025-03-04 20:30:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.16591835021972656 norm:0.00014377411571331322 max memory_allocated 29275.75048828125 
[2025-03-04 20:31:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.16586561501026154 norm:0.00014098078827373683 max memory_allocated 29275.75048828125 
[2025-03-04 20:32:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.16581743955612183 norm:0.00013912192662246525 max memory_allocated 29275.75048828125 
[2025-03-04 20:32:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-04 20:33:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.1979680210351944 norm:0.0006432370282709599 max memory_allocated 29275.93798828125 
[2025-03-04 20:34:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.19154328107833862 norm:0.0003247241547796875 max memory_allocated 29275.93798828125 
[2025-03-04 20:35:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.18520618975162506 norm:0.00019647413864731789 max memory_allocated 29275.93798828125 
[2025-03-04 20:36:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.18368640542030334 norm:0.00015693331079091877 max memory_allocated 29275.93798828125 
[2025-03-04 20:37:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.18255965411663055 norm:0.00013872100680600852 max memory_allocated 29275.93798828125 
[2025-03-04 20:38:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.18171750009059906 norm:0.0001276954571949318 max memory_allocated 29275.93798828125 
[2025-03-04 20:38:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.18134774267673492 norm:0.00012223076191730797 max memory_allocated 29275.93798828125 
[2025-03-04 20:39:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.18116983771324158 norm:0.00011750242265406996 max memory_allocated 29275.93798828125 
[2025-03-04 20:40:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.18106788396835327 norm:0.00011492758494568989 max memory_allocated 29275.93798828125 
[2025-03-04 20:41:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.18100592494010925 norm:0.00011025177082046866 max memory_allocated 29275.93798828125 
[2025-03-04 20:42:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.18094411492347717 norm:0.00010880981426453218 max memory_allocated 29275.93798828125 
[2025-03-04 20:43:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.18089377880096436 norm:0.00010825831850524992 max memory_allocated 29275.93798828125 
[2025-03-04 20:43:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.18085670471191406 norm:0.0001064238022081554 max memory_allocated 29275.93798828125 
[2025-03-04 20:44:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.18081651628017426 norm:0.00010447513341205195 max memory_allocated 29275.93798828125 
[2025-03-04 20:45:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.18077969551086426 norm:0.00010311822552466765 max memory_allocated 29275.93798828125 
[2025-03-04 20:46:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.1807466298341751 norm:0.00010281210415996611 max memory_allocated 29275.93798828125 
[2025-03-04 20:47:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.18071934580802917 norm:0.0001029385020956397 max memory_allocated 29275.93798828125 
[2025-03-04 20:48:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.1806815266609192 norm:0.0001020361014525406 max memory_allocated 29275.93798828125 
[2025-03-04 20:48:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.1806584894657135 norm:0.00010111196024809033 max memory_allocated 29275.93798828125 
[2025-03-04 20:49:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.18064872920513153 norm:0.00010116228804690763 max memory_allocated 29275.93798828125 
[2025-03-04 20:50:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-04 20:50:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.221476748585701 norm:0.0029565980657935143 max memory_allocated 29276.12548828125 
[2025-03-04 20:51:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.21234826743602753 norm:0.0013511102879419923 max memory_allocated 29276.12548828125 
[2025-03-04 20:52:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.20492130517959595 norm:0.0008657517028041184 max memory_allocated 29276.12548828125 
[2025-03-04 20:53:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.20277772843837738 norm:0.0006227968260645866 max memory_allocated 29276.12548828125 
[2025-03-04 20:54:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.20132339000701904 norm:0.0005081576528027654 max memory_allocated 29276.12548828125 
[2025-03-04 20:55:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.20038209855556488 norm:0.000423305929871276 max memory_allocated 29276.12548828125 
[2025-03-04 20:56:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.19990262389183044 norm:0.0003326852456666529 max memory_allocated 29276.12548828125 
[2025-03-04 20:56:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.19962215423583984 norm:0.00027459979173727334 max memory_allocated 29276.12548828125 
[2025-03-04 20:57:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.19945508241653442 norm:0.00024170585675165057 max memory_allocated 29276.12548828125 
[2025-03-04 20:58:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.19928352534770966 norm:0.00021749918232671916 max memory_allocated 29276.12548828125 
[2025-03-04 20:59:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.19915062189102173 norm:0.00019477713794913143 max memory_allocated 29276.12548828125 
[2025-03-04 21:00:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.1990388184785843 norm:0.00018581669428385794 max memory_allocated 29276.12548828125 
[2025-03-04 21:01:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.19897951185703278 norm:0.0001823117199819535 max memory_allocated 29276.12548828125 
[2025-03-04 21:01:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.19893726706504822 norm:0.00020218349527567625 max memory_allocated 29276.12548828125 
[2025-03-04 21:02:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.19887039065361023 norm:0.00018314106273464859 max memory_allocated 29276.12548828125 
[2025-03-04 21:03:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.198832705616951 norm:0.0001772404066286981 max memory_allocated 29276.12548828125 
[2025-03-04 21:04:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.19879579544067383 norm:0.0001778317237040028 max memory_allocated 29276.12548828125 
[2025-03-04 21:05:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.19872649013996124 norm:0.00017343518265988678 max memory_allocated 29276.12548828125 
[2025-03-04 21:06:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.19872726500034332 norm:0.00017816566105466336 max memory_allocated 29276.12548828125 
[2025-03-04 21:06:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.19870395958423615 norm:0.000171769002918154 max memory_allocated 29276.12548828125 
[2025-03-04 21:07:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-04 21:08:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.23683609068393707 norm:0.0010342319728806615 max memory_allocated 29276.31298828125 
[2025-03-04 21:08:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.2295408397912979 norm:0.0005056264926679432 max memory_allocated 29276.31298828125 
[2025-03-04 21:09:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.22253252565860748 norm:0.00027479123673401773 max memory_allocated 29276.31298828125 
[2025-03-04 21:10:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.2207317352294922 norm:0.00018788097077049315 max memory_allocated 29276.31298828125 
[2025-03-04 21:11:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.21945717930793762 norm:0.00015391572378575802 max memory_allocated 29276.31298828125 
[2025-03-04 21:12:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.21867546439170837 norm:0.00013659041724167764 max memory_allocated 29276.31298828125 
[2025-03-04 21:13:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.2183663249015808 norm:0.0001288668718189001 max memory_allocated 29276.31298828125 
[2025-03-04 21:13:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.21820572018623352 norm:0.000122768702567555 max memory_allocated 29276.31298828125 
[2025-03-04 21:14:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.21811312437057495 norm:0.000115245595225133 max memory_allocated 29276.31298828125 
[2025-03-04 21:15:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.21804162859916687 norm:0.0001119417865993455 max memory_allocated 29276.31298828125 
[2025-03-04 21:16:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.2179681658744812 norm:0.00011017340875696391 max memory_allocated 29276.31298828125 
[2025-03-04 21:17:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.21790698170661926 norm:0.00010617928637657315 max memory_allocated 29276.31298828125 
[2025-03-04 21:18:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.2178511768579483 norm:0.0001056161054293625 max memory_allocated 29276.31298828125 
[2025-03-04 21:19:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.21779830753803253 norm:0.00010281689901603386 max memory_allocated 29276.31298828125 
[2025-03-04 21:19:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.2177642434835434 norm:0.00010268978076055646 max memory_allocated 29276.31298828125 
[2025-03-04 21:20:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.21771807968616486 norm:0.0001022041033138521 max memory_allocated 29276.31298828125 
[2025-03-04 21:21:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.21767759323120117 norm:0.00010083247616421431 max memory_allocated 29276.31298828125 
[2025-03-04 21:22:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.2176610380411148 norm:0.00010058983752969652 max memory_allocated 29276.31298828125 
[2025-03-04 21:23:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.21763308346271515 norm:9.844846499618143e-05 max memory_allocated 29276.31298828125 
[2025-03-04 21:24:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.21762242913246155 norm:9.827484609559178e-05 max memory_allocated 29276.31298828125 
[2025-03-04 21:24:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-04 21:25:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.2625315189361572 norm:0.00189007178414613 max memory_allocated 29276.50048828125 
[2025-03-04 21:26:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.2532432973384857 norm:0.0009220561478286982 max memory_allocated 29276.50048828125 
[2025-03-04 21:26:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.24457620084285736 norm:0.0005107474862597883 max memory_allocated 29276.50048828125 
[2025-03-04 21:27:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.24224060773849487 norm:0.00031787226907908916 max memory_allocated 29276.50048828125 
[2025-03-04 21:28:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.2407190501689911 norm:0.0002538725093472749 max memory_allocated 29276.50048828125 
[2025-03-04 21:29:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.23993650078773499 norm:0.00022528118279296905 max memory_allocated 29276.50048828125 
[2025-03-04 21:30:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.23962503671646118 norm:0.0002054318756563589 max memory_allocated 29276.50048828125 
[2025-03-04 21:31:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.23943012952804565 norm:0.00018912937957793474 max memory_allocated 29276.50048828125 
[2025-03-04 21:31:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.23927825689315796 norm:0.0001749430230120197 max memory_allocated 29276.50048828125 
[2025-03-04 21:32:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.23914943635463715 norm:0.00016415506252087653 max memory_allocated 29276.50048828125 
[2025-03-04 21:33:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.23903848230838776 norm:0.00015250573051162064 max memory_allocated 29276.50048828125 
[2025-03-04 21:34:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.23895645141601562 norm:0.00014454683696385473 max memory_allocated 29276.50048828125 
[2025-03-04 21:35:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.23888708651065826 norm:0.00013961490185465664 max memory_allocated 29276.50048828125 
[2025-03-04 21:36:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.2388264238834381 norm:0.00013457395834848285 max memory_allocated 29276.50048828125 
[2025-03-04 21:36:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.2387702465057373 norm:0.0001300958829233423 max memory_allocated 29276.50048828125 
[2025-03-04 21:37:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.23871269822120667 norm:0.00012641461216844618 max memory_allocated 29276.50048828125 
[2025-03-04 21:38:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.2386499047279358 norm:0.00012169279216323048 max memory_allocated 29276.50048828125 
[2025-03-04 21:39:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.2386116087436676 norm:0.00011797949991887435 max memory_allocated 29276.50048828125 
[2025-03-04 21:40:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.2385757714509964 norm:0.00011505562724778429 max memory_allocated 29276.50048828125 
[2025-03-04 21:41:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.23852311074733734 norm:0.00011322677892167121 max memory_allocated 29276.50048828125 
[2025-03-04 21:41:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-04 21:42:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.28494855761528015 norm:0.001814754563383758 max memory_allocated 29276.68798828125 
[2025-03-04 21:43:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.27616506814956665 norm:0.0010056020691990852 max memory_allocated 29276.68798828125 
[2025-03-04 21:44:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.26715436577796936 norm:0.0005030896281823516 max memory_allocated 29276.68798828125 
[2025-03-04 21:44:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.2646885812282562 norm:0.0002757128677330911 max memory_allocated 29276.68798828125 
[2025-03-04 21:45:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.26320335268974304 norm:0.0002379222132731229 max memory_allocated 29276.68798828125 
[2025-03-04 21:46:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.2624815106391907 norm:0.00022138265194371343 max memory_allocated 29276.68798828125 
[2025-03-04 21:47:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.26220762729644775 norm:0.00020996823150198907 max memory_allocated 29276.68798828125 
[2025-03-04 21:48:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.262046217918396 norm:0.00019963571685366333 max memory_allocated 29276.68798828125 
[2025-03-04 21:49:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.26192033290863037 norm:0.00019161087402608246 max memory_allocated 29276.68798828125 
[2025-03-04 21:49:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.26179736852645874 norm:0.0001816022995626554 max memory_allocated 29276.68798828125 
[2025-03-04 21:50:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.26169899106025696 norm:0.00017630775982979685 max memory_allocated 29276.68798828125 
[2025-03-04 21:51:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.26160308718681335 norm:0.0001706931652734056 max memory_allocated 29276.68798828125 
[2025-03-04 21:52:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.26151785254478455 norm:0.00016469109687022865 max memory_allocated 29276.68798828125 
[2025-03-04 21:53:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.2614513039588928 norm:0.00015920800797175616 max memory_allocated 29276.68798828125 
[2025-03-04 21:54:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.26137471199035645 norm:0.00015361103578470647 max memory_allocated 29276.68798828125 
[2025-03-04 21:54:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.2613227963447571 norm:0.00015519000589847565 max memory_allocated 29276.68798828125 
[2025-03-04 21:55:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.26127392053604126 norm:0.00015177369641605765 max memory_allocated 29276.68798828125 
[2025-03-04 21:56:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.2612340450286865 norm:0.0001499215723015368 max memory_allocated 29276.68798828125 
[2025-03-04 21:57:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.2611842155456543 norm:0.0001443845103494823 max memory_allocated 29276.68798828125 
[2025-03-04 21:58:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.261143296957016 norm:0.00014357201871462166 max memory_allocated 29276.68798828125 
[2025-03-04 21:58:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-03-04 21:59:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.31178733706474304 norm:0.0018860392738133669 max memory_allocated 29276.87548828125 
[2025-03-04 22:00:19 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.30214759707450867 norm:0.0010384003398939967 max memory_allocated 29276.87548828125 
[2025-03-04 22:01:09 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.29296162724494934 norm:0.0006430197972804308 max memory_allocated 29276.87548828125 
[2025-03-04 22:01:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.29024121165275574 norm:0.0004488670965656638 max memory_allocated 29276.87548828125 
[2025-03-04 22:02:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.28859809041023254 norm:0.0003464181500021368 max memory_allocated 29276.87548828125 
[2025-03-04 22:03:40 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.28787586092948914 norm:0.0002254151477245614 max memory_allocated 29276.87548828125 
[2025-03-04 22:04:30 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.28757572174072266 norm:0.00020936263899784535 max memory_allocated 29276.87548828125 
[2025-03-04 22:05:20 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.28739041090011597 norm:0.00019718860858120024 max memory_allocated 29276.87548828125 
[2025-03-04 22:06:11 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.28724953532218933 norm:0.0001882885699160397 max memory_allocated 29276.87548828125 
[2025-03-04 22:07:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.28712475299835205 norm:0.00017897909856401384 max memory_allocated 29276.87548828125 
[2025-03-04 22:07:51 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.28701937198638916 norm:0.0001732868840917945 max memory_allocated 29276.87548828125 
[2025-03-04 22:08:42 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.2869415581226349 norm:0.0001668454788159579 max memory_allocated 29276.87548828125 
[2025-03-04 22:09:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.2868679165840149 norm:0.00016247443272732198 max memory_allocated 29276.87548828125 
[2025-03-04 22:10:22 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.2867814302444458 norm:0.0001583476987434551 max memory_allocated 29276.87548828125 
[2025-03-04 22:11:13 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.2867394685745239 norm:0.00015574907592963427 max memory_allocated 29276.87548828125 
[2025-03-04 22:12:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.28668421506881714 norm:0.0001505483378423378 max memory_allocated 29276.87548828125 
[2025-03-04 22:12:53 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.2866329550743103 norm:0.00014587579062208533 max memory_allocated 29276.87548828125 
[2025-03-04 22:13:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.28658854961395264 norm:0.00014131603529676795 max memory_allocated 29276.87548828125 
[2025-03-04 22:14:34 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.28655049204826355 norm:0.00013832088734488934 max memory_allocated 29276.87548828125 
[2025-03-04 22:15:24 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.28651130199432373 norm:0.00013452942948788404 max memory_allocated 29276.87548828125 
[2025-03-04 22:15:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-03-04 22:16:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.3432438373565674 norm:0.002963824663311243 max memory_allocated 29277.06298828125 
[2025-03-04 22:17:26 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.3313342332839966 norm:0.001362126087769866 max memory_allocated 29277.06298828125 
[2025-03-04 22:18:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.32065874338150024 norm:0.0007618910167366266 max memory_allocated 29277.06298828125 
[2025-03-04 22:19:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.3175252377986908 norm:0.0004938517231494188 max memory_allocated 29277.06298828125 
[2025-03-04 22:19:57 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.3157394528388977 norm:0.0004034932644572109 max memory_allocated 29277.06298828125 
[2025-03-04 22:20:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.3150084912776947 norm:0.0003461830201558769 max memory_allocated 29277.06298828125 
[2025-03-04 22:21:38 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.3146498203277588 norm:0.0003028936043847352 max memory_allocated 29277.06298828125 
[2025-03-04 22:22:28 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.3144320845603943 norm:0.0002734634908847511 max memory_allocated 29277.06298828125 
[2025-03-04 22:23:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.31427133083343506 norm:0.00024951115483418107 max memory_allocated 29277.06298828125 
[2025-03-04 22:24:09 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.3141480088233948 norm:0.0002165515470551327 max memory_allocated 29277.06298828125 
[2025-03-04 22:24:59 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.31401127576828003 norm:0.00021105156338308007 max memory_allocated 29277.06298828125 
[2025-03-04 22:25:49 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.31387755274772644 norm:0.00019827480718959123 max memory_allocated 29277.06298828125 
[2025-03-04 22:26:40 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.31382325291633606 norm:0.00018901011208072305 max memory_allocated 29277.06298828125 
[2025-03-04 22:27:30 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.31378284096717834 norm:0.00018530033412389457 max memory_allocated 29277.06298828125 
[2025-03-04 22:28:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.31368717551231384 norm:0.00017636033589951694 max memory_allocated 29277.06298828125 
[2025-03-04 22:29:10 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.31366807222366333 norm:0.00016462727217003703 max memory_allocated 29277.06298828125 
[2025-03-04 22:30:01 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.313707172870636 norm:0.0001624550495762378 max memory_allocated 29277.06298828125 
[2025-03-04 22:30:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.313605934381485 norm:0.0001584733254276216 max memory_allocated 29277.06298828125 
[2025-03-04 22:31:42 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.31357890367507935 norm:0.00015478624845854938 max memory_allocated 29277.06298828125 
[2025-03-04 22:32:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.3135238587856293 norm:0.00014666734205093235 max memory_allocated 29277.06298828125 
[2025-03-04 22:32:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-03-04 22:33:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.37465307116508484 norm:0.0015122788026928902 max memory_allocated 29277.25048828125 
[2025-03-04 22:34:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.36354872584342957 norm:0.0008471196051687002 max memory_allocated 29277.25048828125 
[2025-03-04 22:35:28 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.35364261269569397 norm:0.0005319399642758071 max memory_allocated 29277.25048828125 
[2025-03-04 22:36:19 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.35051429271698 norm:0.0004091692971996963 max memory_allocated 29277.25048828125 
[2025-03-04 22:37:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.34878548979759216 norm:0.00034573563607409596 max memory_allocated 29277.25048828125 
[2025-03-04 22:37:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.3481569290161133 norm:0.0002936996170319617 max memory_allocated 29277.25048828125 
[2025-03-04 22:38:50 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.3478524386882782 norm:0.00025416549760848284 max memory_allocated 29277.25048828125 
[2025-03-04 22:39:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.34761589765548706 norm:0.00022446733782999218 max memory_allocated 29277.25048828125 
[2025-03-04 22:40:30 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.3474212884902954 norm:0.0002028628223342821 max memory_allocated 29277.25048828125 
[2025-03-04 22:41:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.34727513790130615 norm:0.00018283326062373817 max memory_allocated 29277.25048828125 
[2025-03-04 22:42:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.34714019298553467 norm:0.00016731141658965498 max memory_allocated 29277.25048828125 
[2025-03-04 22:43:01 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.34704339504241943 norm:0.0001624721335247159 max memory_allocated 29277.25048828125 
[2025-03-04 22:43:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.34696847200393677 norm:0.00015739331138320267 max memory_allocated 29277.25048828125 
[2025-03-04 22:44:41 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.3468799591064453 norm:0.00015111804532352835 max memory_allocated 29277.25048828125 
[2025-03-04 22:45:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.3467913866043091 norm:0.00014710568939335644 max memory_allocated 29277.25048828125 
[2025-03-04 22:46:22 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.34671321511268616 norm:0.00014259116142056882 max memory_allocated 29277.25048828125 
[2025-03-04 22:47:12 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.3466503322124481 norm:0.00013748784840572625 max memory_allocated 29277.25048828125 
[2025-03-04 22:48:02 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.3465999960899353 norm:0.00013224611757323146 max memory_allocated 29277.25048828125 
[2025-03-04 22:48:52 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.3465507924556732 norm:0.00012979326129425317 max memory_allocated 29277.25048828125 
[2025-03-04 22:49:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.34651416540145874 norm:0.00012830813648179173 max memory_allocated 29277.25048828125 
[2025-03-04 22:49:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-03-04 22:50:57 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.41066399216651917 norm:0.0011305824154987931 max memory_allocated 29277.43798828125 
[2025-03-04 22:51:47 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.3987880349159241 norm:0.0005304106744006276 max memory_allocated 29277.43798828125 
[2025-03-04 22:52:37 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.38835203647613525 norm:0.0003169492701999843 max memory_allocated 29277.43798828125 
[2025-03-04 22:53:28 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.38521403074264526 norm:0.0002499626425560564 max memory_allocated 29277.43798828125 
[2025-03-04 22:54:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.3835172951221466 norm:0.00020754584693349898 max memory_allocated 29277.43798828125 
[2025-03-04 22:55:08 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.3829335868358612 norm:0.00018730797455646098 max memory_allocated 29277.43798828125 
[2025-03-04 22:55:59 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.38261914253234863 norm:0.00017061833932530135 max memory_allocated 29277.43798828125 
[2025-03-04 22:56:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.38243022561073303 norm:0.00016366527415812016 max memory_allocated 29277.43798828125 
[2025-03-04 22:57:40 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.3822704255580902 norm:0.00015284727851394564 max memory_allocated 29277.43798828125 
[2025-03-04 22:58:30 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.38212352991104126 norm:0.00014866594574414194 max memory_allocated 29277.43798828125 
[2025-03-04 22:59:21 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.3820139467716217 norm:0.00014313052815850824 max memory_allocated 29277.43798828125 
[2025-03-04 23:00:11 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.3819214105606079 norm:0.0001392031117575243 max memory_allocated 29277.43798828125 
[2025-03-04 23:01:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.38184332847595215 norm:0.00013840681640431285 max memory_allocated 29277.43798828125 
[2025-03-04 23:01:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.38176289200782776 norm:0.0001352181425318122 max memory_allocated 29277.43798828125 
[2025-03-04 23:02:42 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.38167804479599 norm:0.0001329689985141158 max memory_allocated 29277.43798828125 
[2025-03-04 23:03:32 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.3816181719303131 norm:0.00013117038179188967 max memory_allocated 29277.43798828125 
[2025-03-04 23:04:23 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.3815768361091614 norm:0.00013106732512824237 max memory_allocated 29277.43798828125 
[2025-03-04 23:05:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.3815136253833771 norm:0.00013120716903358698 max memory_allocated 29277.43798828125 
[2025-03-04 23:06:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.3814636766910553 norm:0.0001299042341997847 max memory_allocated 29277.43798828125 
[2025-03-04 23:06:54 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.38143956661224365 norm:0.00012925163900945336 max memory_allocated 29277.43798828125 
[2025-03-04 23:07:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-03-04 23:07:17 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 23:08:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.47062546014785767 norm:0.014557353220880032 max memory_allocated 29277.77001953125 
[2025-03-04 23:08:58 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.45213109254837036 norm:0.010651866905391216 max memory_allocated 29277.77001953125 
[2025-03-04 23:09:48 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.43679219484329224 norm:0.007531153038144112 max memory_allocated 29277.77001953125 
[2025-03-04 23:10:39 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.4312039315700531 norm:0.006137892138212919 max memory_allocated 29277.77001953125 
[2025-03-04 23:11:29 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.42867302894592285 norm:0.005142685491591692 max memory_allocated 29277.77001953125 
[2025-03-04 23:12:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.4273269772529602 norm:0.004355796612799168 max memory_allocated 29277.77001953125 
[2025-03-04 23:13:10 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.4265280067920685 norm:0.003752118907868862 max memory_allocated 29277.77001953125 
[2025-03-04 23:14:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.4260176420211792 norm:0.0033408512827008963 max memory_allocated 29277.77001953125 
[2025-03-04 23:14:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.42576873302459717 norm:0.0033112040255218744 max memory_allocated 29277.77001953125 
[2025-03-04 23:15:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.4254332184791565 norm:0.0032356923911720514 max memory_allocated 29277.77001953125 
[2025-03-04 23:16:32 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.4251170754432678 norm:0.003134125377982855 max memory_allocated 29277.77001953125 
[2025-03-04 23:17:23 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.42510777711868286 norm:0.003158309729769826 max memory_allocated 29277.77001953125 
[2025-03-04 23:18:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.4248201251029968 norm:0.0030242297798395157 max memory_allocated 29277.77001953125 
[2025-03-04 23:19:04 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.4246540069580078 norm:0.0027898168191313744 max memory_allocated 29277.77001953125 
[2025-03-04 23:19:55 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.42452341318130493 norm:0.002752291737124324 max memory_allocated 29277.77001953125 
[2025-03-04 23:20:45 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.42457813024520874 norm:0.0027960925363004208 max memory_allocated 29277.77001953125 
[2025-03-04 23:21:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.42437317967414856 norm:0.002740481635555625 max memory_allocated 29277.77001953125 
[2025-03-04 23:22:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.4242995083332062 norm:0.0026075351051986217 max memory_allocated 29277.77001953125 
[2025-03-04 23:23:17 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.4241957366466522 norm:0.002546256175264716 max memory_allocated 29277.77001953125 
[2025-03-04 23:24:08 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.4242454767227173 norm:0.0025884509086608887 max memory_allocated 29277.77001953125 
[2025-03-04 23:24:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-03-04 23:24:31 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 23:25:21 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.5224902033805847 norm:0.021917041391134262 max memory_allocated 29277.95751953125 
[2025-03-04 23:26:12 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.5011277794837952 norm:0.014011943712830544 max memory_allocated 29277.95751953125 
[2025-03-04 23:27:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.4844532310962677 norm:0.009429746307432652 max memory_allocated 29277.95751953125 
[2025-03-04 23:27:52 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.47890186309814453 norm:0.008048856630921364 max memory_allocated 29277.95751953125 
[2025-03-04 23:28:43 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.4763038754463196 norm:0.006881693843752146 max memory_allocated 29277.95751953125 
[2025-03-04 23:29:33 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.4750964939594269 norm:0.0060273767448961735 max memory_allocated 29277.95751953125 
[2025-03-04 23:30:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.4744059443473816 norm:0.005457933526486158 max memory_allocated 29277.95751953125 
[2025-03-04 23:31:15 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.47386911511421204 norm:0.004829621408134699 max memory_allocated 29277.95751953125 
[2025-03-04 23:32:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.47340869903564453 norm:0.00426256051287055 max memory_allocated 29277.95751953125 
[2025-03-04 23:32:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.4730352461338043 norm:0.003902016207575798 max memory_allocated 29277.95751953125 
[2025-03-04 23:33:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.47274503111839294 norm:0.003555484814569354 max memory_allocated 29277.95751953125 
[2025-03-04 23:34:37 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.4725136160850525 norm:0.0033882868010550737 max memory_allocated 29277.95751953125 
[2025-03-04 23:35:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.4723667800426483 norm:0.00331940408796072 max memory_allocated 29277.95751953125 
[2025-03-04 23:36:18 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.4722785949707031 norm:0.003342297626659274 max memory_allocated 29277.95751953125 
[2025-03-04 23:37:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.47217676043510437 norm:0.0032603407744318247 max memory_allocated 29277.95751953125 
[2025-03-04 23:37:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.47214475274086 norm:0.0033548015635460615 max memory_allocated 29277.95751953125 
[2025-03-04 23:38:50 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.47217583656311035 norm:0.003371406579390168 max memory_allocated 29277.95751953125 
[2025-03-04 23:39:40 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.4721035361289978 norm:0.003296362701803446 max memory_allocated 29277.95751953125 
[2025-03-04 23:40:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.4719778895378113 norm:0.003188624745234847 max memory_allocated 29277.95751953125 
[2025-03-04 23:41:21 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.47187724709510803 norm:0.0031218668445944786 max memory_allocated 29277.95751953125 
[2025-03-04 23:41:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-03-04 23:41:47 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 23:42:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.6247410774230957 norm:0.02180105820298195 max memory_allocated 29278.14501953125 
[2025-03-04 23:43:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.5973802804946899 norm:0.007229846902191639 max memory_allocated 29278.14501953125 
[2025-03-04 23:44:18 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.5739679336547852 norm:0.00701911561191082 max memory_allocated 29278.14501953125 
[2025-03-04 23:45:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.5648562908172607 norm:0.007485714741051197 max memory_allocated 29278.14501953125 
[2025-03-04 23:45:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.5606873035430908 norm:0.007249480113387108 max memory_allocated 29278.14501953125 
[2025-03-04 23:46:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.5589269399642944 norm:0.007053445093333721 max memory_allocated 29278.14501953125 
[2025-03-04 23:47:40 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.5573907494544983 norm:0.006864152383059263 max memory_allocated 29278.14501953125 
[2025-03-04 23:48:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.556065559387207 norm:0.006883453577756882 max memory_allocated 29278.14501953125 
[2025-03-04 23:49:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.5550004243850708 norm:0.006643990054726601 max memory_allocated 29278.14501953125 
[2025-03-04 23:50:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.5542840361595154 norm:0.006572714075446129 max memory_allocated 29278.14501953125 
[2025-03-04 23:51:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.5537035465240479 norm:0.006551999598741531 max memory_allocated 29278.14501953125 
[2025-03-04 23:51:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.553418755531311 norm:0.0066552660427987576 max memory_allocated 29278.14501953125 
[2025-03-04 23:52:44 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.5530208945274353 norm:0.006633405573666096 max memory_allocated 29278.14501953125 
[2025-03-04 23:53:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.5528664588928223 norm:0.006693594623357058 max memory_allocated 29278.14501953125 
[2025-03-04 23:54:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.5526615977287292 norm:0.0067835901863873005 max memory_allocated 29278.14501953125 
[2025-03-04 23:55:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.5524433851242065 norm:0.006796489469707012 max memory_allocated 29278.14501953125 
[2025-03-04 23:56:06 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.5522584319114685 norm:0.006642872467637062 max memory_allocated 29278.14501953125 
[2025-03-04 23:56:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.552151083946228 norm:0.006524898111820221 max memory_allocated 29278.14501953125 
[2025-03-04 23:57:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.5519924163818359 norm:0.006414117757230997 max memory_allocated 29278.14501953125 
[2025-03-04 23:58:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.5518172383308411 norm:0.0063142622821033 max memory_allocated 29278.14501953125 
[2025-03-04 23:58:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-03-04 23:59:03 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 23:59:53 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.8714687824249268 norm:0.07884731143712997 max memory_allocated 29278.33251953125 
[2025-03-05 00:00:43 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.7960014939308167 norm:0.0492711141705513 max memory_allocated 29278.33251953125 
[2025-03-05 00:01:33 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.7492833137512207 norm:0.03267904743552208 max memory_allocated 29278.33251953125 
[2025-03-05 00:02:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.731204092502594 norm:0.02587750367820263 max memory_allocated 29278.33251953125 
[2025-03-05 00:03:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.7228505611419678 norm:0.022821879014372826 max memory_allocated 29278.33251953125 
[2025-03-05 00:04:05 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.7170824408531189 norm:0.019924934953451157 max memory_allocated 29278.33251953125 
[2025-03-05 00:04:56 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.713026762008667 norm:0.017408471554517746 max memory_allocated 29278.33251953125 
[2025-03-05 00:05:46 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.7100006341934204 norm:0.01700890250504017 max memory_allocated 29278.33251953125 
[2025-03-05 00:06:36 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.708057701587677 norm:0.016688302159309387 max memory_allocated 29278.33251953125 
[2025-03-05 00:07:27 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.7065480947494507 norm:0.015321940183639526 max memory_allocated 29278.33251953125 
[2025-03-05 00:08:17 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.7050895690917969 norm:0.01474976260215044 max memory_allocated 29278.33251953125 
[2025-03-05 00:09:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.7034819722175598 norm:0.013658077456057072 max memory_allocated 29278.33251953125 
[2025-03-05 00:09:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.7022479772567749 norm:0.013050546869635582 max memory_allocated 29278.33251953125 
[2025-03-05 00:10:48 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.7016919851303101 norm:0.012937028892338276 max memory_allocated 29278.33251953125 
[2025-03-05 00:11:39 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.7016390562057495 norm:0.013947144150733948 max memory_allocated 29278.33251953125 
[2025-03-05 00:12:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.7021137475967407 norm:0.013805439695715904 max memory_allocated 29278.33251953125 
[2025-03-05 00:13:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.7014851570129395 norm:0.013499151915311813 max memory_allocated 29278.33251953125 
[2025-03-05 00:14:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.700575590133667 norm:0.012416993267834187 max memory_allocated 29278.33251953125 
[2025-03-05 00:15:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.6992461085319519 norm:0.012460354715585709 max memory_allocated 29278.33251953125 
[2025-03-05 00:15:53 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.698531985282898 norm:0.012172713875770569 max memory_allocated 29278.33251953125 
[2025-03-05 00:16:07 root] (main_calibration.py 365): INFO 41268.65600705147
[2025-03-05 00:17:12 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-05 00:19:11 root] (main_calibration.py 158): INFO wikitext2 : 5.173344135284424
[2025-03-05 00:19:11 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-05 00:22:15 root] (main_calibration.py 158): INFO c4 : 6.875760555267334
[2025-03-05 02:19:50 root] (main_calibration.py 169): INFO {'wikitext2': 5.173344135284424, 'c4': 6.875760555267334, 'results': {'arc_easy': {'acc': 0.7108585858585859, 'acc_stderr': 0.009302827114597425, 'acc_norm': 0.5656565656565656, 'acc_norm_stderr': 0.010170943451269421}, 'winogrande': {'acc': 0.648776637726914, 'acc_stderr': 0.013415981370545131}, 'arc_challenge': {'acc': 0.4249146757679181, 'acc_stderr': 0.014445698968520765, 'acc_norm': 0.4308873720136519, 'acc_norm_stderr': 0.01447113339264247}, 'piqa': {'acc': 0.7752992383025027, 'acc_stderr': 0.009738282586548394, 'acc_norm': 0.7731229597388466, 'acc_norm_stderr': 0.00977158425921518}, 'hellaswag': {'acc': 0.5797649870543716, 'acc_stderr': 0.004925877705771197, 'acc_norm': 0.741983668591914, 'acc_norm_stderr': 0.004366488167386392}, 'boolq': {'acc': 0.6651376146788991, 'acc_stderr': 0.008254323342627927}}, 'versions': {'arc_easy': 0, 'winogrande': 0, 'arc_challenge': 0, 'piqa': 0, 'hellaswag': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
