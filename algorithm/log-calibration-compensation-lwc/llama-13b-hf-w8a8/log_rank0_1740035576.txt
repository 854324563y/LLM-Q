[2025-02-20 07:12:56 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-13b-hf-w8a8', save_dir='./log-calibration-compensation-lwc/quant/llama-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-20 07:13:36 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-20 07:13:37 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-20 07:13:37 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-20 07:13:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-20 07:13:43 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:14:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0017808845732361078 norm:0.002111205365508795 max memory_allocated 29268.02001953125 
[2025-02-20 07:15:19 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0011080350959673524 norm:0.001486068358644843 max memory_allocated 29268.02001953125 
[2025-02-20 07:16:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0009280950180254877 norm:0.00121159921400249 max memory_allocated 29268.02001953125 
[2025-02-20 07:16:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0008220189483836293 norm:0.001057333080098033 max memory_allocated 29268.02001953125 
[2025-02-20 07:17:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0007579452358186245 norm:0.0009558679303154349 max memory_allocated 29268.02001953125 
[2025-02-20 07:18:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0007098030182532966 norm:0.0008641427848488092 max memory_allocated 29268.02001953125 
[2025-02-20 07:19:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0006780771072953939 norm:0.0007966418634168804 max memory_allocated 29268.02001953125 
[2025-02-20 07:20:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0006565182120539248 norm:0.0007276436081156135 max memory_allocated 29268.02001953125 
[2025-02-20 07:21:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0006406023167073727 norm:0.0006815922679379582 max memory_allocated 29268.02001953125 
[2025-02-20 07:21:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0006288823205977678 norm:0.0006337354425340891 max memory_allocated 29268.02001953125 
[2025-02-20 07:22:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0006149885011836886 norm:0.000571387936361134 max memory_allocated 29268.02001953125 
[2025-02-20 07:23:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0006060139276087284 norm:0.0005264640785753727 max memory_allocated 29268.02001953125 
[2025-02-20 07:24:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0006001273286528885 norm:0.0004932287847623229 max memory_allocated 29268.02001953125 
[2025-02-20 07:25:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0005981504800729454 norm:0.00047663107397966087 max memory_allocated 29268.02001953125 
[2025-02-20 07:25:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0005929383332841098 norm:0.00042761306394822896 max memory_allocated 29268.02001953125 
[2025-02-20 07:26:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0005867453874088824 norm:0.0003938529989682138 max memory_allocated 29268.02001953125 
[2025-02-20 07:27:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0005861337413080037 norm:0.00037062453338876367 max memory_allocated 29268.02001953125 
[2025-02-20 07:28:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0005833792383782566 norm:0.00034220554516650736 max memory_allocated 29268.02001953125 
[2025-02-20 07:29:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0005800211220048368 norm:0.0003212470328435302 max memory_allocated 29268.02001953125 
[2025-02-20 07:30:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0005777305923402309 norm:0.00030053273076191545 max memory_allocated 29268.02001953125 
[2025-02-20 07:30:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-20 07:30:20 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:31:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0016445228829979897 norm:0.002094021998345852 max memory_allocated 29268.02001953125 
[2025-02-20 07:31:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0011082910932600498 norm:0.0005470893811434507 max memory_allocated 29268.02001953125 
[2025-02-20 07:32:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0009851340437307954 norm:0.000607045425567776 max memory_allocated 29268.02001953125 
[2025-02-20 07:33:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0009097917936742306 norm:0.0007077245390973985 max memory_allocated 29268.02001953125 
[2025-02-20 07:34:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0008652008837088943 norm:0.0007014828734099865 max memory_allocated 29268.02001953125 
[2025-02-20 07:35:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0008339872001670301 norm:0.0006600100896321237 max memory_allocated 29268.02001953125 
[2025-02-20 07:36:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0008154046372510493 norm:0.0006388789042830467 max memory_allocated 29268.02001953125 
[2025-02-20 07:36:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0008013384649530053 norm:0.0006260222871787846 max memory_allocated 29268.02001953125 
[2025-02-20 07:37:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0007895317394286394 norm:0.000606911547947675 max memory_allocated 29268.02001953125 
[2025-02-20 07:38:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0007802161853760481 norm:0.0005702223279513419 max memory_allocated 29268.02001953125 
[2025-02-20 07:39:20 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0007708102930337191 norm:0.000547284260392189 max memory_allocated 29268.02001953125 
[2025-02-20 07:40:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0007648440077900887 norm:0.0005139260902069509 max memory_allocated 29268.02001953125 
[2025-02-20 07:40:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0007594118360430002 norm:0.0004861153429374099 max memory_allocated 29268.02001953125 
[2025-02-20 07:41:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0007559825899079442 norm:0.0004686500469688326 max memory_allocated 29268.02001953125 
[2025-02-20 07:42:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0007521425141021609 norm:0.0004417879390530288 max memory_allocated 29268.02001953125 
[2025-02-20 07:43:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0007502980879507959 norm:0.0004277364641893655 max memory_allocated 29268.02001953125 
[2025-02-20 07:44:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0007473527221009135 norm:0.00039531695074401796 max memory_allocated 29268.02001953125 
[2025-02-20 07:45:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0007467402028851211 norm:0.0003798502730205655 max memory_allocated 29268.02001953125 
[2025-02-20 07:45:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0007457663305103779 norm:0.00035736116115003824 max memory_allocated 29268.02001953125 
[2025-02-20 07:46:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0007454482838511467 norm:0.0003430652723181993 max memory_allocated 29268.02001953125 
[2025-02-20 07:46:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-20 07:47:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:47:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.001859205076470971 norm:0.001983583439141512 max memory_allocated 29268.02001953125 
[2025-02-20 07:48:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.001477717887610197 norm:0.001483792089857161 max memory_allocated 29268.02001953125 
[2025-02-20 07:49:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0013276413083076477 norm:0.001167672104202211 max memory_allocated 29268.02001953125 
[2025-02-20 07:50:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0012321901740506291 norm:0.001131191966123879 max memory_allocated 29268.02001953125 
[2025-02-20 07:51:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0011609148932620883 norm:0.0009533717529848218 max memory_allocated 29268.02001953125 
[2025-02-20 07:51:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0011377953924238682 norm:0.0008899163804017007 max memory_allocated 29268.02001953125 
[2025-02-20 07:52:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0010930470889434218 norm:0.0007168238516896963 max memory_allocated 29268.02001953125 
[2025-02-20 07:53:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0010643573477864265 norm:0.000756491266656667 max memory_allocated 29268.02001953125 
[2025-02-20 07:54:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0010566888377070427 norm:0.0007535096956416965 max memory_allocated 29268.02001953125 
[2025-02-20 07:55:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0010401476174592972 norm:0.0006342200795188546 max memory_allocated 29268.02001953125 
[2025-02-20 07:56:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0010348750511184335 norm:0.0006487116916105151 max memory_allocated 29268.02001953125 
[2025-02-20 07:56:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0010109414579346776 norm:0.0005967261968180537 max memory_allocated 29268.02001953125 
[2025-02-20 07:57:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0010028744582086802 norm:0.0005731859127990901 max memory_allocated 29268.02001953125 
[2025-02-20 07:58:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0009992690756917 norm:0.0005005682469345629 max memory_allocated 29268.02001953125 
[2025-02-20 07:59:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0010100409854203463 norm:0.000542026071343571 max memory_allocated 29268.02001953125 
[2025-02-20 08:00:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0009969811653718352 norm:0.00047585301217623055 max memory_allocated 29268.02001953125 
[2025-02-20 08:00:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0009987856028601527 norm:0.0004321547457948327 max memory_allocated 29268.02001953125 
[2025-02-20 08:01:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0009964870987460017 norm:0.0003813124494627118 max memory_allocated 29268.02001953125 
[2025-02-20 08:02:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.000996573711745441 norm:0.0004356469726189971 max memory_allocated 29268.02001953125 
[2025-02-20 08:03:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0010023615323007107 norm:0.0004462915239855647 max memory_allocated 29268.02001953125 
[2025-02-20 08:03:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-20 08:04:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0020806349348276854 norm:0.0008369594579562545 max memory_allocated 29268.43798828125 
[2025-02-20 08:05:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0015370916808024049 norm:0.00028357416158542037 max memory_allocated 29268.43798828125 
[2025-02-20 08:06:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0013824129709973931 norm:0.00018703944806475192 max memory_allocated 29268.43798828125 
[2025-02-20 08:06:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.001295184250921011 norm:0.00015845206507947296 max memory_allocated 29268.43798828125 
[2025-02-20 08:07:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.001240083947777748 norm:0.00013049572589807212 max memory_allocated 29268.43798828125 
[2025-02-20 08:08:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0012082885950803757 norm:0.00010817529255291447 max memory_allocated 29268.43798828125 
[2025-02-20 08:09:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0011905382853001356 norm:9.399135888088495e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:10:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0011803610250353813 norm:8.639640145702288e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:11:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0011734840227290988 norm:7.228773029055446e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:11:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0011661089956760406 norm:6.787783786421642e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:12:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0011597385164350271 norm:5.858500662725419e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:13:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0011531509226188064 norm:5.2152390708215535e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:14:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0011492216726765037 norm:4.190967956674285e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:15:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0011467300355434418 norm:4.728707790491171e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:15:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0011439311783760786 norm:3.8317928556352854e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:16:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0011423781979829073 norm:3.54380754288286e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:17:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0011411488521844149 norm:2.9956932849017903e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:18:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0011394594330340624 norm:2.604956171126105e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:19:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0011400289367884398 norm:2.7522097298060544e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:20:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0011384481331333518 norm:2.2319563868222758e-05 max memory_allocated 29268.43798828125 
[2025-02-20 08:20:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-20 08:21:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0038834568113088608 norm:0.003496395656839013 max memory_allocated 29268.62548828125 
[2025-02-20 08:21:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0019922510255128145 norm:0.0006957092555239797 max memory_allocated 29268.62548828125 
[2025-02-20 08:22:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0017489660531282425 norm:0.00036589291994459927 max memory_allocated 29268.62548828125 
[2025-02-20 08:23:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0016349069774150848 norm:0.00027527703787200153 max memory_allocated 29268.62548828125 
[2025-02-20 08:24:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.001540619763545692 norm:0.00020714735728688538 max memory_allocated 29268.62548828125 
[2025-02-20 08:25:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0014841214288026094 norm:0.00016958126798272133 max memory_allocated 29268.62548828125 
[2025-02-20 08:26:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0014492679620161653 norm:0.00014629773795604706 max memory_allocated 29268.62548828125 
[2025-02-20 08:26:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0014261443866416812 norm:0.00013507460243999958 max memory_allocated 29268.62548828125 
[2025-02-20 08:27:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0014119322877377272 norm:0.00011153699597343802 max memory_allocated 29268.62548828125 
[2025-02-20 08:28:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.00140279287006706 norm:0.00011437907232902944 max memory_allocated 29268.62548828125 
[2025-02-20 08:29:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.001391921890899539 norm:0.00010879411274800077 max memory_allocated 29268.62548828125 
[2025-02-20 08:30:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0013826731592416763 norm:9.739419328980148e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:30:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0013770272489637136 norm:9.416321699973196e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:31:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0013748728670179844 norm:7.905396341811866e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:32:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0013724996242672205 norm:7.457237370545045e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:33:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0013678116956725717 norm:7.172923506004736e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:34:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0013646810548380017 norm:6.578227475984022e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:34:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0013574312906712294 norm:5.7144821766996756e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:35:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0013563460670411587 norm:5.338783375918865e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:36:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0013525157701224089 norm:5.058337046648376e-05 max memory_allocated 29268.62548828125 
[2025-02-20 08:36:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-20 08:37:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.003184682922437787 norm:0.0019276464590802789 max memory_allocated 29268.81298828125 
[2025-02-20 08:38:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.001952462480403483 norm:0.0003957150620408356 max memory_allocated 29268.81298828125 
[2025-02-20 08:39:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0018044523894786835 norm:0.0002620599989313632 max memory_allocated 29268.81298828125 
[2025-02-20 08:40:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0017094340873882174 norm:0.00019274397345725447 max memory_allocated 29268.81298828125 
[2025-02-20 08:40:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0016461043851450086 norm:0.0001508924033259973 max memory_allocated 29268.81298828125 
[2025-02-20 08:41:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0016052131541073322 norm:0.00012554688146337867 max memory_allocated 29268.81298828125 
[2025-02-20 08:42:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0015770146856084466 norm:0.00010670543269952759 max memory_allocated 29268.81298828125 
[2025-02-20 08:43:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0015596705488860607 norm:9.488606883678585e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:44:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0015433859080076218 norm:8.808654092717916e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:45:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0015315713826566935 norm:8.007294673006982e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:45:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0015220630448311567 norm:7.418708264594898e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:46:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0015156653244048357 norm:6.14906894043088e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:47:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0015086719067767262 norm:6.036547711119056e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:48:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0014992994256317616 norm:5.685368523700163e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:49:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0014951000921428204 norm:4.848686512559652e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:49:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0014914064668118954 norm:4.844514114665799e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:50:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0014883456751704216 norm:4.1262723243562505e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:51:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.001485636574216187 norm:3.772487980313599e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:52:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0014818338677287102 norm:3.4536868042778224e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:53:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.001479477621614933 norm:2.9591865313705057e-05 max memory_allocated 29268.81298828125 
[2025-02-20 08:53:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-20 08:54:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.002782464027404785 norm:0.0005677406443282962 max memory_allocated 29269.00048828125 
[2025-02-20 08:55:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.00214227638207376 norm:0.00021735290647484362 max memory_allocated 29269.00048828125 
[2025-02-20 08:55:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.002015255857259035 norm:0.00013625173596665263 max memory_allocated 29269.00048828125 
[2025-02-20 08:56:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0019432392437011003 norm:0.00012987585796508938 max memory_allocated 29269.00048828125 
[2025-02-20 08:57:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0018571699038147926 norm:8.872384205460548e-05 max memory_allocated 29269.00048828125 
[2025-02-20 08:58:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0018546690698713064 norm:9.759077511262149e-05 max memory_allocated 29269.00048828125 
[2025-02-20 08:59:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0018526760395616293 norm:9.34187337406911e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:00:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0018419360276311636 norm:7.659206312382594e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:00:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0018392548663541675 norm:7.355270645348355e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:01:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0018210179405286908 norm:8.747685205889866e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:02:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0017988847102969885 norm:6.135337753221393e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:03:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0017968115862458944 norm:5.411624806583859e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:04:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.001797364791855216 norm:5.407145363278687e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:04:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0017948683816939592 norm:4.977383650839329e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:05:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0017940760590136051 norm:4.937908306601457e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:06:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0017876038327813148 norm:4.467694088816643e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:07:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0017815363826230168 norm:4.284813621779904e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:08:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.001780461985617876 norm:4.2585612391121686e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:09:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0017751872073858976 norm:4.595464633894153e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:09:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0017733575077727437 norm:5.2617637265939265e-05 max memory_allocated 29269.00048828125 
[2025-02-20 09:10:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-20 09:10:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.003104845993220806 norm:0.0006169299595057964 max memory_allocated 29269.18798828125 
[2025-02-20 09:11:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0021672945003956556 norm:0.0001415107981301844 max memory_allocated 29269.18798828125 
[2025-02-20 09:12:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.002027770271524787 norm:7.511142030125484e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:13:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.001968521624803543 norm:6.175146700115874e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:14:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0019256507512181997 norm:4.9725906137609854e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:15:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.0018957408610731363 norm:3.992300480604172e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:15:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.001877956441603601 norm:3.634493623394519e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:16:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0018647313117980957 norm:3.33274292643182e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:17:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.001855722744949162 norm:3.1270250474335626e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:18:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0018471089424565434 norm:2.8262322302907705e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:19:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0018420739797875285 norm:2.7844840587931685e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:19:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0018376808147877455 norm:2.4832648705341853e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:20:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0018345753196626902 norm:2.3890459488029592e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:21:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0018319787923246622 norm:2.1907011614530347e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:22:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0018301942618563771 norm:2.2837579308543354e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:23:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0018281994853168726 norm:2.0057721485500224e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:24:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0018270793370902538 norm:2.0937093722750433e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:24:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0018255419563502073 norm:2.107593354594428e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:25:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0018217229517176747 norm:1.883023651316762e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:26:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0018225453095510602 norm:1.9029075701837428e-05 max memory_allocated 29269.18798828125 
[2025-02-20 09:26:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-20 09:27:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0027123058680444956 norm:0.0002832819300238043 max memory_allocated 29269.37548828125 
[2025-02-20 09:28:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0022171013988554478 norm:8.064614667091519e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:29:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.002123525133356452 norm:5.4720076150260866e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:30:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0020723200868815184 norm:4.2723539081634954e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:30:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.002038901438936591 norm:3.144213405903429e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:31:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.002019197214394808 norm:2.868924093490932e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:32:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.002003829926252365 norm:2.7183374186279252e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:33:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0019910153932869434 norm:2.411773311905563e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:34:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0019834355916827917 norm:2.2093327061156742e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:34:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.001977949868887663 norm:2.1220075723249465e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:35:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.001973335165530443 norm:2.069846414087806e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:36:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.001969240140169859 norm:1.9293618606752716e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:37:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.001966563519090414 norm:2.07904649869306e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:38:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0019636619836091995 norm:2.050602597591933e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:39:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.001961048925295472 norm:1.859295298345387e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:39:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0019598749931901693 norm:1.7644493709667586e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:40:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0019588752184063196 norm:1.702175723039545e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:41:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.001956772990524769 norm:1.7284506611758843e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:42:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0019560065120458603 norm:1.7388800188200548e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:43:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.001955238403752446 norm:1.5693351087975316e-05 max memory_allocated 29269.37548828125 
[2025-02-20 09:43:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-20 09:44:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.0027104984037578106 norm:0.00021548764198087156 max memory_allocated 29269.56298828125 
[2025-02-20 09:45:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0022923843935132027 norm:5.911566404392943e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:45:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.002216492546722293 norm:3.547743835952133e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:46:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.002176515990868211 norm:2.8340789867797866e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:47:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0021494380198419094 norm:2.3218439309857786e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:48:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0021335177589207888 norm:2.1109284716658294e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:49:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.002123023383319378 norm:1.8759517843136564e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:49:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0021143490448594093 norm:1.825873914640397e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:50:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.002108785090968013 norm:1.7433558241464198e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:51:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.002103532664477825 norm:1.6355224943254143e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:52:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.002099704695865512 norm:1.5522560715908185e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:53:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0020967109594494104 norm:1.512242124590557e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:54:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0020937472581863403 norm:1.4516552255372517e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:54:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.002091459697112441 norm:1.320245792157948e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:55:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0020907935686409473 norm:1.3324541214387864e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:56:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.002089862944558263 norm:1.253492973773973e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:57:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0020887444261461496 norm:1.2532480468507856e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:58:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0020873185712844133 norm:1.2551653526315931e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:58:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.00208640331402421 norm:1.1825959518318996e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:59:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0020857430063188076 norm:1.169317965832306e-05 max memory_allocated 29269.56298828125 
[2025-02-20 09:59:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-20 10:00:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.003404969349503517 norm:0.0004617542726919055 max memory_allocated 29269.75048828125 
[2025-02-20 10:01:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0025584266986697912 norm:0.00011628240463323891 max memory_allocated 29269.75048828125 
[2025-02-20 10:02:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0024194950237870216 norm:5.803187741548754e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:03:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.002365414286032319 norm:4.428461397765204e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:04:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0023330370895564556 norm:3.741721957339905e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:04:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0023077810183167458 norm:3.100883259321563e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:05:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0022913161665201187 norm:2.6152456484851427e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:06:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.0022799884900450706 norm:2.311897333129309e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:07:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0022688747849315405 norm:2.0056450011907145e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:08:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0022621985990554094 norm:1.8488563000573777e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:09:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0022558332420885563 norm:1.904333839775063e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:09:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0022510981652885675 norm:1.9143339159199968e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:10:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.00224756239913404 norm:1.7323985957773402e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:11:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0022449465468525887 norm:1.683246409811545e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:12:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0022429635282605886 norm:1.602712472958956e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:13:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0022403148468583822 norm:1.5776244254084304e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:13:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.002238646149635315 norm:1.4823289347987156e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:14:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0022383681498467922 norm:1.528313077869825e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:15:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.00223695021122694 norm:1.4428059330384713e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:16:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.002235767897218466 norm:1.3784354450763203e-05 max memory_allocated 29269.75048828125 
[2025-02-20 10:16:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-20 10:17:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0030563357286155224 norm:0.0001981080131372437 max memory_allocated 29269.93798828125 
[2025-02-20 10:18:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0026493477635085583 norm:5.944157237536274e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:19:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0025615827180445194 norm:3.640742943389341e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:19:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0025182596873492002 norm:2.9036957130301744e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:20:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.002489331644028425 norm:2.3520180548075587e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:21:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002470326842740178 norm:1.9412980691413395e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:22:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0024583046324551105 norm:1.7738053429638967e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:23:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0024502170272171497 norm:1.6774971300037578e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:24:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0024427385069429874 norm:1.545202212582808e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:24:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0024377296213060617 norm:1.4568634469469544e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:25:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.002433456713333726 norm:1.4272454791353084e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:26:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0024298876523971558 norm:1.4142192412691657e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:27:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0024267390836030245 norm:1.3341034900804516e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:28:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0024254450108855963 norm:1.2550653991638683e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:28:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.002423497149720788 norm:1.182987580250483e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:29:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0024214782752096653 norm:1.0362014108977746e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:30:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.002420200500637293 norm:1.1029866072931327e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:31:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.002419381169602275 norm:1.0407603440398816e-05 max memory_allocated 29269.93798828125 
[2025-02-20 10:32:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0024184859357774258 norm:9.79090418695705e-06 max memory_allocated 29269.93798828125 
[2025-02-20 10:33:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.002417126437649131 norm:9.189810953103006e-06 max memory_allocated 29269.93798828125 
[2025-02-20 10:33:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-20 10:34:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.003167924005538225 norm:0.00020386384858284146 max memory_allocated 29270.12548828125 
[2025-02-20 10:34:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0027913451194763184 norm:5.991799844196066e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:35:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002714464208111167 norm:3.44260515703354e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:36:34 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0026776432059705257 norm:2.8928998290211894e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:37:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0026529114693403244 norm:2.419212250970304e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:38:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.00263429107144475 norm:2.0029097868246026e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:39:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.002621315885335207 norm:1.813966991903726e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:39:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.002612372161820531 norm:1.6198287994484417e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:40:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0026054694317281246 norm:1.5350289686466567e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:41:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0025995818432420492 norm:1.3817601939081214e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:42:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0025956202298402786 norm:1.3125149052939378e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:43:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0025917361490428448 norm:1.2729069567285478e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:43:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.002589145675301552 norm:1.2007772056676913e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:44:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0025860259775072336 norm:1.1396272384445183e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:45:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.0025846860371530056 norm:1.071280348696746e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:46:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0025831039529293776 norm:1.0516910151636694e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:47:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0025817605201154947 norm:1.0245391422358807e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:48:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0025804992765188217 norm:1.0811269021360204e-05 max memory_allocated 29270.12548828125 
[2025-02-20 10:48:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0025791204534471035 norm:9.687943020253442e-06 max memory_allocated 29270.12548828125 
[2025-02-20 10:49:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0025782110169529915 norm:9.656676411395893e-06 max memory_allocated 29270.12548828125 
[2025-02-20 10:49:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-20 10:50:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.0032841016072779894 norm:0.0001275517715839669 max memory_allocated 29270.31298828125 
[2025-02-20 10:51:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.003025130368769169 norm:4.381173130241223e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:52:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.002962761791422963 norm:2.7880661946255714e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:53:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0029301480390131474 norm:2.2589196305489168e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:54:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.0029058882500976324 norm:1.9052891730098054e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:54:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0028886101208627224 norm:1.6369158402085304e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:55:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.002876994200050831 norm:1.5292916941689327e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:56:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0028684462886303663 norm:1.4262526747188531e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:57:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.0028609258588403463 norm:1.357096425635973e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:58:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.002855445258319378 norm:1.2621150744962506e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:58:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0028508631512522697 norm:1.1863412510138005e-05 max memory_allocated 29270.31298828125 
[2025-02-20 10:59:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0028481839690357447 norm:1.1679634553729557e-05 max memory_allocated 29270.31298828125 
[2025-02-20 11:00:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0028453462291508913 norm:1.1476888175820932e-05 max memory_allocated 29270.31298828125 
[2025-02-20 11:01:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.002842316636815667 norm:1.1254574019403663e-05 max memory_allocated 29270.31298828125 
[2025-02-20 11:02:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0028396667912602425 norm:1.0446909072925337e-05 max memory_allocated 29270.31298828125 
[2025-02-20 11:03:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.002837692853063345 norm:9.91912111203419e-06 max memory_allocated 29270.31298828125 
[2025-02-20 11:03:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0028367661871016026 norm:9.780817890714388e-06 max memory_allocated 29270.31298828125 
[2025-02-20 11:04:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.002835656516253948 norm:9.965791832655668e-06 max memory_allocated 29270.31298828125 
[2025-02-20 11:05:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.002834585029631853 norm:9.375337867822964e-06 max memory_allocated 29270.31298828125 
[2025-02-20 11:06:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0028338914271444082 norm:9.336093171441462e-06 max memory_allocated 29270.31298828125 
[2025-02-20 11:06:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-20 11:07:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.003945725038647652 norm:0.0002767238474916667 max memory_allocated 29270.50048828125 
[2025-02-20 11:08:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0033915594685822725 norm:8.996157703222707e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:09:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0032721906900405884 norm:4.663367144530639e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:09:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0032208310440182686 norm:3.645103788585402e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:10:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.003187417984008789 norm:3.1351391953649e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:11:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.003158353269100189 norm:2.6835328753804788e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:12:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0031383398454636335 norm:2.3945443899719976e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:13:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.003123227506875992 norm:2.157959170290269e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:13:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0031118718907237053 norm:1.984795198950451e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:14:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0031023877672851086 norm:1.819395401980728e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:15:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0030960619915276766 norm:1.672724465606734e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:16:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0030899636913090944 norm:1.5829991752980277e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:17:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.003085170639678836 norm:1.5190347767202184e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:17:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0030811221804469824 norm:1.3936893992649857e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:18:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.00307805510237813 norm:1.3493248843587935e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:19:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.0030749915167689323 norm:1.271581459150184e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:20:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0030733193270862103 norm:1.2746124411933124e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:21:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.003071741433814168 norm:1.256566793017555e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:22:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0030693309381604195 norm:1.2189847439003643e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:22:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.0030670850537717342 norm:1.1072833331127185e-05 max memory_allocated 29270.50048828125 
[2025-02-20 11:23:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-20 11:23:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.004862711764872074 norm:0.0005983546143397689 max memory_allocated 29270.68798828125 
[2025-02-20 11:24:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.003857651026919484 norm:0.00015070750669110566 max memory_allocated 29270.68798828125 
[2025-02-20 11:25:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.003690616460517049 norm:8.307653479278088e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:26:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0036283708177506924 norm:6.641024083364755e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:27:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0035866389516741037 norm:5.5578457249794155e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:28:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.003547198837623 norm:4.492204243433662e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:28:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.003520316444337368 norm:3.85718813049607e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:29:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.003502574283629656 norm:3.485162233118899e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:30:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.00348659441806376 norm:3.1499104807153344e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:31:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0034744839649647474 norm:2.8864109481219202e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:32:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.003464752109721303 norm:2.6115429136552848e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:32:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.003457577433437109 norm:2.453555316606071e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:33:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0034515459556132555 norm:2.2832276954432018e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:34:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.003445208538323641 norm:2.1105621271999553e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:35:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.0034398771822452545 norm:1.9644776330096647e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:36:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.003436880651861429 norm:1.986069037229754e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:37:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.003433042671531439 norm:1.8603950593387708e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:37:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.0034308331087231636 norm:1.7873411707114428e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:38:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.003427906194701791 norm:1.6806190615170635e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:39:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.0034250104799866676 norm:1.6333880921592936e-05 max memory_allocated 29270.68798828125 
[2025-02-20 11:39:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-20 11:40:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.0045851957984268665 norm:0.00029616765095852315 max memory_allocated 29270.87548828125 
[2025-02-20 11:41:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.004055777098983526 norm:8.898120722733438e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:42:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.003948161844164133 norm:5.186066482565366e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:43:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.003900005482137203 norm:4.158927913522348e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:43:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0038627157919108868 norm:3.3830521715572104e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:44:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0038338368758559227 norm:2.803486495395191e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:45:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.003813509363681078 norm:2.4705295800231397e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:46:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.00379851832985878 norm:2.1632664356729947e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:47:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.003787003690376878 norm:1.948978751897812e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:47:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.003778284415602684 norm:1.790690112102311e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:48:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.00377119448967278 norm:1.6476256860187277e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:49:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.0037657904904335737 norm:1.5372817870229483e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:50:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0037603480741381645 norm:1.4227067367755808e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:51:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0037570632994174957 norm:1.3481583664542995e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:52:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.003754204837605357 norm:1.2544624041765928e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:52:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.0037507705856114626 norm:1.2065761438861955e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:53:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.003748477203771472 norm:1.1412033018132206e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:54:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.003746527247130871 norm:1.1537031241459772e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:55:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0037442301400005817 norm:1.1153502782690339e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:56:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.003742196597158909 norm:1.0915085113083478e-05 max memory_allocated 29270.87548828125 
[2025-02-20 11:56:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-20 11:57:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.004919521510601044 norm:0.00027213364955969155 max memory_allocated 29271.06298828125 
[2025-02-20 11:58:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.004407464060932398 norm:8.61962980707176e-05 max memory_allocated 29271.06298828125 
[2025-02-20 11:58:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.004293499980121851 norm:4.869584881816991e-05 max memory_allocated 29271.06298828125 
[2025-02-20 11:59:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.004241278860718012 norm:3.9009712054394186e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:00:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.004203339107334614 norm:3.273188121966086e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:01:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.004173635505139828 norm:2.74753929261351e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:02:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.0041528670117259026 norm:2.366722583246883e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:02:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.004137896001338959 norm:2.0952818886144087e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:03:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.004127116873860359 norm:1.9053570213145576e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:04:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.004117433913052082 norm:1.7616739569348283e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:05:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.004110801964998245 norm:1.6244768630713224e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:06:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.004104656167328358 norm:1.6002552001737058e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:07:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.004099109210073948 norm:1.479402544646291e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:07:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.004094634670764208 norm:1.4875415217829868e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:08:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.004090854898095131 norm:1.4136267054709606e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:09:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.004086539149284363 norm:1.3599637895822525e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:10:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.004083558917045593 norm:1.272322242584778e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:11:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0040809595957398415 norm:1.219574551214464e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:11:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.004078493919223547 norm:1.1688530321407598e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:12:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0040766592137515545 norm:1.1635477676463779e-05 max memory_allocated 29271.06298828125 
[2025-02-20 12:12:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-20 12:13:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.0057845343835651875 norm:0.0004234157386235893 max memory_allocated 29271.25048828125 
[2025-02-20 12:14:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.005044439807534218 norm:0.00014556656242348254 max memory_allocated 29271.25048828125 
[2025-02-20 12:15:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.004882967099547386 norm:7.884384831413627e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:16:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.00481108482927084 norm:5.990476347506046e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:17:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.004760029725730419 norm:4.98825975228101e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:17:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.004721521399915218 norm:4.281298606656492e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:18:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.004689805209636688 norm:3.753496639546938e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:19:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.00466918246820569 norm:3.3661650377325714e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:20:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.0046534184366464615 norm:2.999052230734378e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:21:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.004636481869965792 norm:2.7781930839410052e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:22:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.004623706918209791 norm:2.535529711167328e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:22:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.004614340607076883 norm:2.3979617253644392e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:23:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.004606932867318392 norm:2.0880172087345272e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:24:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.004600482527166605 norm:1.9819806766463444e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:25:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.004593325313180685 norm:1.9452081687632017e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:26:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.004586831200867891 norm:1.8033420928986743e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:26:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.004580735228955746 norm:1.6667549061821774e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:27:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.004576481878757477 norm:1.538829928904306e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:28:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0045736427418887615 norm:1.552169123897329e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:29:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.004569726996123791 norm:1.4765419109608047e-05 max memory_allocated 29271.25048828125 
[2025-02-20 12:29:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-20 12:30:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.00626419810578227 norm:0.0003762581036426127 max memory_allocated 29271.43798828125 
[2025-02-20 12:31:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.005573449656367302 norm:0.00011944789730478078 max memory_allocated 29271.43798828125 
[2025-02-20 12:32:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.005420065484941006 norm:6.748158921254799e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:32:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.005356163252145052 norm:5.2502829930745065e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:33:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.005310902837663889 norm:4.375442222226411e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:34:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.005272755865007639 norm:3.6428340536076576e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:35:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0052439747378230095 norm:3.175862366333604e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:36:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0052221049554646015 norm:2.7892958314623684e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:37:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.005204804241657257 norm:2.5137464035651647e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:37:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.005191797390580177 norm:2.2555097530130297e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:38:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.005181301850825548 norm:2.1209090846241452e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:39:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.005173343233764172 norm:1.9755549146793783e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:40:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.005166569724678993 norm:1.8644597730599344e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:41:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.005160449538379908 norm:1.7892642063088715e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:41:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.005155802704393864 norm:1.7127455066656694e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:42:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.005151018500328064 norm:1.6085079550975934e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:43:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.005147288553416729 norm:1.5629164408892393e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:44:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.0051438515074551105 norm:1.5065415027493145e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:45:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.005141003988683224 norm:1.4063510207051877e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:46:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.005138978827744722 norm:1.3986559679324273e-05 max memory_allocated 29271.43798828125 
[2025-02-20 12:46:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-20 12:47:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.0073004295118153095 norm:0.0005602488527074456 max memory_allocated 29271.62548828125 
[2025-02-20 12:47:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.006332886405289173 norm:0.0001768604852259159 max memory_allocated 29271.62548828125 
[2025-02-20 12:48:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.006105478852987289 norm:9.39561941777356e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:49:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.00601798202842474 norm:7.153301703510806e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:50:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.0059565044939517975 norm:5.865540879312903e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:51:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.005908021703362465 norm:4.987501961295493e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:52:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.005871941801160574 norm:4.204730430501513e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:52:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.005843971855938435 norm:3.681113230413757e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:53:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.005820382386445999 norm:3.2352556445403025e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:54:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.0058030495420098305 norm:2.933256655524019e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:55:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.0057899234816432 norm:2.72056113317376e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:56:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.005779235158115625 norm:2.5058976461878046e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:56:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.005769138224422932 norm:2.288830728502944e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:57:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.005760879721492529 norm:2.1582496628980152e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:58:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.005752980709075928 norm:1.9876184524036944e-05 max memory_allocated 29271.62548828125 
[2025-02-20 12:59:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.005747159011662006 norm:1.8610002371133305e-05 max memory_allocated 29271.62548828125 
[2025-02-20 13:00:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.005741236265748739 norm:1.7901094906846993e-05 max memory_allocated 29271.62548828125 
[2025-02-20 13:01:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.005736896302551031 norm:1.6904879885260016e-05 max memory_allocated 29271.62548828125 
[2025-02-20 13:01:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.005733152851462364 norm:1.6083396985777654e-05 max memory_allocated 29271.62548828125 
[2025-02-20 13:02:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.005729585886001587 norm:1.5264235116774216e-05 max memory_allocated 29271.62548828125 
[2025-02-20 13:02:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-20 13:03:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.008203219622373581 norm:0.0008368031121790409 max memory_allocated 29271.81298828125 
[2025-02-20 13:04:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.007107285782694817 norm:0.0002195845590904355 max memory_allocated 29271.81298828125 
[2025-02-20 13:05:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.006896907463669777 norm:0.00011327968240948394 max memory_allocated 29271.81298828125 
[2025-02-20 13:06:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.006806825287640095 norm:8.422208338743076e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:07:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.006749995052814484 norm:6.971126276766881e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:07:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.006701335310935974 norm:5.7898505474440753e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:08:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.00666210800409317 norm:4.993293987354264e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:09:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.006633084267377853 norm:4.3701016693376005e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:10:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.006611568853259087 norm:3.957143417210318e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:11:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.006593469530344009 norm:3.6367833672557026e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:11:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.006578060332685709 norm:3.380538691999391e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:12:43 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.006565720774233341 norm:3.0428380341618322e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:13:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.006556316744536161 norm:2.8601152735063806e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:14:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.006547367200255394 norm:2.655588832567446e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:15:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.006538848392665386 norm:2.4641398340463638e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:15:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.006531746592372656 norm:2.2819456717115827e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:16:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.006526365410536528 norm:2.169834988308139e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:17:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.006520581431686878 norm:2.056757512036711e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:18:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.006516617722809315 norm:1.9933773728553206e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:19:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.006512988358736038 norm:1.9167377104167826e-05 max memory_allocated 29271.81298828125 
[2025-02-20 13:19:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-20 13:20:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.008661908097565174 norm:0.0004228749021422118 max memory_allocated 29272.00048828125 
[2025-02-20 13:21:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.00795449037104845 norm:0.00014202235615812242 max memory_allocated 29272.00048828125 
[2025-02-20 13:22:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.007778283208608627 norm:8.062723645707592e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:22:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.007695665117353201 norm:6.230289000086486e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:23:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.007636432070285082 norm:5.145187606103718e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:24:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.007590826600790024 norm:4.3166757677681744e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:25:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.0075547946617007256 norm:3.767941234400496e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:26:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.007526432164013386 norm:3.3273670851485804e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:26:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.007505586836487055 norm:3.0126193450996652e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:27:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.00748884491622448 norm:2.8073085559299216e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:28:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.007476126775145531 norm:2.647596556926146e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:29:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.007465951144695282 norm:2.492310704838019e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:30:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.007456820923835039 norm:2.2767993868910708e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:30:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.007447372656315565 norm:2.1316778656910174e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:31:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.007441306486725807 norm:1.9953564333263785e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:32:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.007435969542711973 norm:1.933523890329525e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:33:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.007429489400237799 norm:1.885852361738216e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:34:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.007426210213452578 norm:1.8398046449874528e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:35:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.0074236392974853516 norm:1.7762815332389437e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:35:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.007418835535645485 norm:1.7377049516653642e-05 max memory_allocated 29272.00048828125 
[2025-02-20 13:36:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-20 13:37:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.009982884861528873 norm:0.0005385252879932523 max memory_allocated 29272.18798828125 
[2025-02-20 13:37:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.009191097691655159 norm:0.00018559308955445886 max memory_allocated 29272.18798828125 
[2025-02-20 13:38:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.00900399498641491 norm:0.00011710768012562767 max memory_allocated 29272.18798828125 
[2025-02-20 13:39:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.008906571194529533 norm:9.13710828172043e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:40:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.008829205296933651 norm:7.343917241087183e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:41:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.008771104738116264 norm:6.109831156209111e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:41:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.008726518601179123 norm:5.2614210289902985e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:42:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.00869169645011425 norm:4.5598844735650346e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:43:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.008663089945912361 norm:4.057236947119236e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:44:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.008640402927994728 norm:3.695750638144091e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:45:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.008620554581284523 norm:3.3199903555214405e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:45:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.008605513721704483 norm:3.0071441869949922e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:46:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.008593554608523846 norm:2.802389644784853e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:47:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.0085828872397542 norm:2.688307358766906e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:48:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.008571349084377289 norm:2.5532446670695208e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:49:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.008561930619180202 norm:2.463396049279254e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:50:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.008554325439035892 norm:2.323185071873013e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:50:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.00854691956192255 norm:2.266402771056164e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:51:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.008541528135538101 norm:2.1414987713797018e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:52:30 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.008536460809409618 norm:2.003307054110337e-05 max memory_allocated 29272.18798828125 
[2025-02-20 13:52:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-20 13:53:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.010742241516709328 norm:0.0004373153788037598 max memory_allocated 29272.37548828125 
[2025-02-20 13:54:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.01009390689432621 norm:0.00017120024131145328 max memory_allocated 29272.37548828125 
[2025-02-20 13:55:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.00993549358099699 norm:0.00010972960444632918 max memory_allocated 29272.37548828125 
[2025-02-20 13:56:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.009861038997769356 norm:7.866558735258877e-05 max memory_allocated 29272.37548828125 
[2025-02-20 13:56:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.00980265624821186 norm:6.171596032800153e-05 max memory_allocated 29272.37548828125 
[2025-02-20 13:57:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.009755849838256836 norm:5.114655868965201e-05 max memory_allocated 29272.37548828125 
[2025-02-20 13:58:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.00972348265349865 norm:4.4219777919352055e-05 max memory_allocated 29272.37548828125 
[2025-02-20 13:59:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.009698066860437393 norm:3.653938620118424e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:00:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.009678700938820839 norm:3.449600626481697e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:00:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.00966392271220684 norm:3.230660513509065e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:01:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.009650942869484425 norm:3.10407776851207e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:02:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.009644467383623123 norm:2.8559126803884283e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:03:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.009635886177420616 norm:2.6652152882888913e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:04:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.009626081213355064 norm:2.5176179406116717e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:05:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.009619985707104206 norm:2.361305450904183e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:05:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.009614461101591587 norm:2.2502592400996946e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:06:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.009610375389456749 norm:2.1888297851546668e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:07:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.00960796233266592 norm:2.117115946020931e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:08:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.00960473157465458 norm:1.9826025891234167e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:09:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.0096023790538311 norm:1.9500379494274966e-05 max memory_allocated 29272.37548828125 
[2025-02-20 14:09:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-20 14:10:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.012090404517948627 norm:0.0005779271014034748 max memory_allocated 29272.56298828125 
[2025-02-20 14:11:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.011326143518090248 norm:0.00018343405099585652 max memory_allocated 29272.56298828125 
[2025-02-20 14:11:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.011171646416187286 norm:0.00011128088226541877 max memory_allocated 29272.56298828125 
[2025-02-20 14:12:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.011094787158071995 norm:8.860177331371233e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:13:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.01102716475725174 norm:7.211623596958816e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:14:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.01097466703504324 norm:5.916752706980333e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:15:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.010935304686427116 norm:5.027231964049861e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:15:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.010907132178544998 norm:4.3494106648722664e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:16:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.010884465649724007 norm:3.813824150711298e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:17:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.01086629182100296 norm:3.6017368984175846e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:18:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.010855531319975853 norm:3.3234489819733426e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:19:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.010841810144484043 norm:3.0214087018975988e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:20:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.01082982961088419 norm:2.7776544811786152e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:20:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.010820520110428333 norm:2.6834903110284358e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:21:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.010813799686729908 norm:2.503514042473398e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:22:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.010809432715177536 norm:2.1815805666847154e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:23:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.01080325711518526 norm:2.0440067601157352e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:24:09 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.010797823779284954 norm:1.9424847778282128e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:24:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.010791968554258347 norm:1.8328899386688136e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:25:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.010789517313241959 norm:1.707878618617542e-05 max memory_allocated 29272.56298828125 
[2025-02-20 14:26:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-20 14:26:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.012688611634075642 norm:0.0002366480475757271 max memory_allocated 29272.75048828125 
[2025-02-20 14:27:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.01239762268960476 norm:0.00010851414845092222 max memory_allocated 29272.75048828125 
[2025-02-20 14:28:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.012285677716135979 norm:7.302694575628266e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:29:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.012213665060698986 norm:5.5284221161855385e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:30:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.012161962687969208 norm:4.548913784674369e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:30:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.012126324698328972 norm:3.824614395853132e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:31:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.012098885141313076 norm:3.353304055053741e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:32:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.0120769701898098 norm:2.8659862437052652e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:33:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.012059810571372509 norm:2.5849265512079e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:34:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.012046466581523418 norm:2.3347769456449896e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:35:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.012035561725497246 norm:2.115613824571483e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:35:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.012025939300656319 norm:1.9235125364502892e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:36:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.012017285451292992 norm:1.8031652871286497e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:37:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.012010818347334862 norm:1.7459986338508315e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:38:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.01200532540678978 norm:1.6520792996743694e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:39:09 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.012000392191112041 norm:1.5784677088959143e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:39:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.011995608918368816 norm:1.4947514500818215e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:40:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.011991440318524837 norm:1.4556588212144561e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:41:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.011989237740635872 norm:1.4155495591694489e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:42:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.011986564844846725 norm:1.3692285392608028e-05 max memory_allocated 29272.75048828125 
[2025-02-20 14:42:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-20 14:43:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.014153729192912579 norm:0.0002529805642552674 max memory_allocated 29272.93798828125 
[2025-02-20 14:44:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.01385965570807457 norm:0.00011801203072536737 max memory_allocated 29272.93798828125 
[2025-02-20 14:45:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.013740193098783493 norm:8.332782454090193e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:45:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.013659387826919556 norm:6.395200762199238e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:46:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.013605756685137749 norm:5.1574625103967264e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:47:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.013568989001214504 norm:4.4503638491733e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:48:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.01354280672967434 norm:4.142565012443811e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:49:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.013520325534045696 norm:3.796080272877589e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:50:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.013504069298505783 norm:3.5548833693610504e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:50:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.013491148129105568 norm:3.3072836231440306e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:51:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.013479020446538925 norm:3.242997627239674e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:52:30 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.01346647460013628 norm:3.0603758204961196e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:53:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.013457732275128365 norm:2.9737537261098623e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:54:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.013448107987642288 norm:2.8777678380720317e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:54:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.013438715599477291 norm:2.6015850380645134e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:55:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.01343550905585289 norm:2.76586324616801e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:56:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.013424912467598915 norm:2.4807666704873554e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:57:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.013419454917311668 norm:2.3652177333133295e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:58:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.013414417393505573 norm:2.243858943984378e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:59:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.013407237827777863 norm:2.1452869987115264e-05 max memory_allocated 29272.93798828125 
[2025-02-20 14:59:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-20 15:00:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.016774170100688934 norm:0.0013365530176088214 max memory_allocated 29273.12548828125 
[2025-02-20 15:00:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.015449898317456245 norm:0.0003650254220701754 max memory_allocated 29273.12548828125 
[2025-02-20 15:01:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.015257105231285095 norm:0.00020267567015253007 max memory_allocated 29273.12548828125 
[2025-02-20 15:02:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.01517985388636589 norm:0.00016769833746366203 max memory_allocated 29273.12548828125 
[2025-02-20 15:03:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.015121781267225742 norm:0.00014866111450828612 max memory_allocated 29273.12548828125 
[2025-02-20 15:04:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.015071308240294456 norm:0.0001229773333761841 max memory_allocated 29273.12548828125 
[2025-02-20 15:05:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.015031719580292702 norm:0.00010713616211432964 max memory_allocated 29273.12548828125 
[2025-02-20 15:05:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.014999068342149258 norm:9.381971904076636e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:06:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.014971751719713211 norm:8.436395728494972e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:07:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.014951212331652641 norm:7.896368333604187e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:08:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.014934523962438107 norm:7.692640065215528e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:09:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.014922212809324265 norm:6.95640774210915e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:09:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.014911693520843983 norm:7.263380393851548e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:10:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.014900178648531437 norm:6.626106187468395e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:11:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.014891053549945354 norm:6.561840564245358e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:12:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.01488229725509882 norm:6.111084076110274e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:13:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.014874467626214027 norm:6.27474655630067e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:14:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.014867697842419147 norm:5.642871474265121e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:14:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.014861563220620155 norm:5.6110002333298326e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:15:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.014857950620353222 norm:5.181424057809636e-05 max memory_allocated 29273.12548828125 
[2025-02-20 15:15:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-20 15:16:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.016989244148135185 norm:0.00030683737713843584 max memory_allocated 29273.31298828125 
[2025-02-20 15:17:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.016676902770996094 norm:0.00013035957817919552 max memory_allocated 29273.31298828125 
[2025-02-20 15:18:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.016578353941440582 norm:8.541814895579591e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:19:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.016509495675563812 norm:6.454256799770519e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:20:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.016459431499242783 norm:5.3531341109192e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:20:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.016422269865870476 norm:4.544176408671774e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:21:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.01639556512236595 norm:3.96227951569017e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:22:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.016373679041862488 norm:3.4492670238250867e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:23:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.016356879845261574 norm:2.9319769964786246e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:24:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.01634340174496174 norm:2.6123569114133716e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:24:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.01633109711110592 norm:2.436813883832656e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:25:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.01632075384259224 norm:2.2339823772199452e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:26:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.01631287857890129 norm:2.0932395273121074e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:27:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.01630590669810772 norm:1.9274099031463265e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:28:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.016300570219755173 norm:1.7734280845616013e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:29:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.016296125948429108 norm:1.672232428973075e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:29:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.016291441395878792 norm:1.535975025035441e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:30:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.016288692131638527 norm:1.4986912901804317e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:31:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.016285646706819534 norm:1.503562270954717e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:32:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.01628190651535988 norm:1.4664241462014616e-05 max memory_allocated 29273.31298828125 
[2025-02-20 15:32:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-20 15:33:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.018638212233781815 norm:0.00023813851294107735 max memory_allocated 29273.50048828125 
[2025-02-20 15:34:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.018384655937552452 norm:0.00012694706674665213 max memory_allocated 29273.50048828125 
[2025-02-20 15:35:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.018268194049596786 norm:8.90214359969832e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:35:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.01819123513996601 norm:6.893502722959965e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:36:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.018140308558940887 norm:5.6237633543787524e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:37:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.018107682466506958 norm:5.3025534725748e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:38:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.01808270439505577 norm:4.64496151835192e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:39:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.018058311194181442 norm:4.272440855856985e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:39:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.01803809404373169 norm:3.8998456147965044e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:40:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.018022267147898674 norm:3.677424319903366e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:41:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.018005874007940292 norm:3.4012238756986335e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:42:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.017993930727243423 norm:3.3204345527337864e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:43:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.01798723265528679 norm:3.8384750951081514e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:44:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.017971832305192947 norm:4.4300912122707814e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:44:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.01795787364244461 norm:3.442387969698757e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:45:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.01795337162911892 norm:3.6279721825849265e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:46:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.01794598437845707 norm:3.2864023523870856e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:47:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.017940586432814598 norm:3.131073390250094e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:48:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.017936166375875473 norm:2.983239210152533e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:48:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.017931431531906128 norm:2.7251127903582528e-05 max memory_allocated 29273.50048828125 
[2025-02-20 15:49:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-20 15:50:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.020220456644892693 norm:0.0002349945716559887 max memory_allocated 29273.68798828125 
[2025-02-20 15:50:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.01998973824083805 norm:0.00011562003055587411 max memory_allocated 29273.68798828125 
[2025-02-20 15:51:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.01988912560045719 norm:7.922798249637708e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:52:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.019820550456643105 norm:6.221106741577387e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:53:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.019776292145252228 norm:5.216041245148517e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:54:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.01974416896700859 norm:4.521344089880586e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:54:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.019719185307621956 norm:3.7709214666392654e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:55:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.019698983058333397 norm:3.2720672606956214e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:56:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.019684679806232452 norm:3.0194705686881207e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:57:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.019668705761432648 norm:2.7549922378966585e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:58:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.019658081233501434 norm:2.5216617359546944e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:59:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.019648706540465355 norm:2.3339618564932607e-05 max memory_allocated 29273.68798828125 
[2025-02-20 15:59:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.01964283362030983 norm:2.234161365777254e-05 max memory_allocated 29273.68798828125 
[2025-02-20 16:00:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.019636210054159164 norm:2.231339931313414e-05 max memory_allocated 29273.68798828125 
[2025-02-20 16:01:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.01962927170097828 norm:2.141675940947607e-05 max memory_allocated 29273.68798828125 
[2025-02-20 16:02:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.019625691697001457 norm:2.0909572413074784e-05 max memory_allocated 29273.68798828125 
[2025-02-20 16:03:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.019621221348643303 norm:1.859387157310266e-05 max memory_allocated 29273.68798828125 
[2025-02-20 16:03:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.019616708159446716 norm:1.8419235857436433e-05 max memory_allocated 29273.68798828125 
[2025-02-20 16:04:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.019613921642303467 norm:1.74690576386638e-05 max memory_allocated 29273.68798828125 
[2025-02-20 16:05:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.019610973075032234 norm:1.8365053620073013e-05 max memory_allocated 29273.68798828125 
[2025-02-20 16:05:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-20 16:06:40 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.0220082625746727 norm:0.00025078855105675757 max memory_allocated 29273.87548828125 
[2025-02-20 16:07:29 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.021769434213638306 norm:0.00012570318358484656 max memory_allocated 29273.87548828125 
[2025-02-20 16:08:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.021652840077877045 norm:8.839975635055453e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:09:07 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.021581176668405533 norm:6.905109330546111e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:09:56 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.02153003215789795 norm:5.623977995128371e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:10:44 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.021491168066859245 norm:4.569584416458383e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:11:34 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.021460039541125298 norm:3.7703382986364886e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:12:23 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.021434558555483818 norm:3.363488212926313e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:13:12 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.021414335817098618 norm:2.90041025436949e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:14:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.02139837108552456 norm:2.689857683435548e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:14:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.021386273205280304 norm:2.378707540628966e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:15:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.021375427022576332 norm:2.184678305638954e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:16:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.021366003900766373 norm:2.0183830201858655e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:17:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.021359317004680634 norm:1.841173434513621e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:18:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.021356139332056046 norm:1.7430218576919287e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:18:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.021351147443056107 norm:1.626145422051195e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:19:44 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.021345239132642746 norm:1.587485894560814e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:20:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.021341390907764435 norm:1.5819659893168136e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:21:22 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.02133932150900364 norm:1.6209542081924155e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:22:11 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.021336445584893227 norm:1.5490237274207175e-05 max memory_allocated 29273.87548828125 
[2025-02-20 16:22:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-20 16:23:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.02409457415342331 norm:0.00023174102534539998 max memory_allocated 29274.06298828125 
[2025-02-20 16:24:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.023831600323319435 norm:0.00012695786426775157 max memory_allocated 29274.06298828125 
[2025-02-20 16:24:56 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.023700349032878876 norm:8.68232746142894e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:25:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.02362186834216118 norm:6.734984344802797e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:26:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.02356894128024578 norm:5.670296377502382e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:27:23 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.02352326735854149 norm:4.789068043464795e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:28:12 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.02349250577390194 norm:4.306453047320247e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:29:01 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.023463841527700424 norm:3.761262632906437e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:29:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.02344757318496704 norm:3.3331234590150416e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:30:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.023433994501829147 norm:2.8992168154218234e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:31:27 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.023415906354784966 norm:2.656818469404243e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:32:17 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.023400921374559402 norm:2.571658478700556e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:33:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.02338431216776371 norm:2.4184193534892984e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:33:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.02337656170129776 norm:2.390001100138761e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:34:43 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.023366378620266914 norm:2.12994527828414e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:35:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.02335895225405693 norm:2.033551936619915e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:36:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.02335291914641857 norm:2.0205510736559518e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:37:10 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.023345407098531723 norm:1.8252361769555137e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:37:59 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.023340269923210144 norm:1.8218288460047916e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:38:48 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.02333526872098446 norm:1.8334751075599343e-05 max memory_allocated 29274.06298828125 
[2025-02-20 16:39:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-20 16:39:55 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.026693196967244148 norm:0.000263435416854918 max memory_allocated 29274.25048828125 
[2025-02-20 16:40:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.026325266808271408 norm:0.00015394145157188177 max memory_allocated 29274.25048828125 
[2025-02-20 16:41:33 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.026133636012673378 norm:0.00010549259604886174 max memory_allocated 29274.25048828125 
[2025-02-20 16:42:22 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.026014557108283043 norm:8.32331643323414e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:43:11 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.025930551812052727 norm:6.794263026677072e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:44:00 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.025868428871035576 norm:5.614867404801771e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:44:49 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.025820616632699966 norm:4.75445413030684e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:45:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.025786664336919785 norm:4.083480598637834e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:46:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.025755994021892548 norm:3.7550213164649904e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:47:16 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.02572796493768692 norm:3.410279896343127e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:48:05 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.025707077234983444 norm:3.1714145734440535e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:48:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.025687813758850098 norm:2.7740636141970754e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:49:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.02567315474152565 norm:2.5244144126190804e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:50:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.025661254301667213 norm:2.4900386051740497e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:51:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.025651728734374046 norm:2.2814248950453475e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:52:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.025642862543463707 norm:2.1699612261727452e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:52:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.025632651522755623 norm:2.0356983441160992e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:53:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.025624040514230728 norm:2.0041092284373008e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:54:36 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.025618553161621094 norm:1.898827395052649e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:55:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.025612452998757362 norm:1.890892963274382e-05 max memory_allocated 29274.25048828125 
[2025-02-20 16:55:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-20 16:56:32 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.029457062482833862 norm:0.00027426923043094575 max memory_allocated 29274.43798828125 
[2025-02-20 16:57:20 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.02897253818809986 norm:0.00016274082008749247 max memory_allocated 29274.43798828125 
[2025-02-20 16:58:09 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.028725601732730865 norm:0.00011842525418614969 max memory_allocated 29274.43798828125 
[2025-02-20 16:58:58 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.02857029065489769 norm:9.369983308715746e-05 max memory_allocated 29274.43798828125 
[2025-02-20 16:59:47 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.028461528941988945 norm:7.67736419220455e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:00:37 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.02837861329317093 norm:6.456576375057921e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:01:25 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.028311561793088913 norm:5.574545502895489e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:02:14 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.028259966522455215 norm:5.0989190640393645e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:03:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.028220120817422867 norm:4.609377356246114e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:03:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.028187625110149384 norm:4.1451647120993584e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:04:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.028157178312540054 norm:3.706659481395036e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:05:30 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.028133317828178406 norm:3.397675754968077e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:06:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.02811361849308014 norm:3.113025377388112e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:07:08 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.02809496596455574 norm:2.8874930649180897e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:07:57 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.028083954006433487 norm:2.7922054869122803e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:08:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.02807423286139965 norm:2.553066769905854e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:09:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.02806306816637516 norm:2.6016870833700523e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:10:25 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.028052929788827896 norm:2.467016201990191e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:11:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.02804129384458065 norm:2.482365380274132e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:12:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.0280318446457386 norm:2.4256125470856205e-05 max memory_allocated 29274.43798828125 
[2025-02-20 17:12:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-20 17:12:20 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:13:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.03232836723327637 norm:0.001788724446669221 max memory_allocated 29274.77001953125 
[2025-02-20 17:13:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.03182019665837288 norm:0.0012578038731589913 max memory_allocated 29274.77001953125 
[2025-02-20 17:14:48 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.03154728561639786 norm:0.001003708690404892 max memory_allocated 29274.77001953125 
[2025-02-20 17:15:37 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.031377971172332764 norm:0.000855302787385881 max memory_allocated 29274.77001953125 
[2025-02-20 17:16:26 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.031254641711711884 norm:0.0007492263684980571 max memory_allocated 29274.77001953125 
[2025-02-20 17:17:15 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.031160030514001846 norm:0.0006791742634959519 max memory_allocated 29274.77001953125 
[2025-02-20 17:18:04 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.031090103089809418 norm:0.0005996804684400558 max memory_allocated 29274.77001953125 
[2025-02-20 17:18:53 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.031037211418151855 norm:0.000548330950550735 max memory_allocated 29274.77001953125 
[2025-02-20 17:19:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.030990442261099815 norm:0.00046526119695045054 max memory_allocated 29274.77001953125 
[2025-02-20 17:20:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.03100433759391308 norm:0.0006632615113630891 max memory_allocated 29274.77001953125 
[2025-02-20 17:21:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.031046414747834206 norm:0.0008253000560216606 max memory_allocated 29274.77001953125 
[2025-02-20 17:22:10 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.03120771422982216 norm:0.0011749230325222015 max memory_allocated 29274.77001953125 
[2025-02-20 17:22:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.031083647161722183 norm:0.001013473840430379 max memory_allocated 29274.77001953125 
[2025-02-20 17:23:48 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.03089948371052742 norm:0.0005358532071113586 max memory_allocated 29274.77001953125 
[2025-02-20 17:24:37 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.03083389438688755 norm:0.0003753590863198042 max memory_allocated 29274.77001953125 
[2025-02-20 17:25:26 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.030810758471488953 norm:0.0003524997446220368 max memory_allocated 29274.77001953125 
[2025-02-20 17:26:16 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.030796170234680176 norm:0.00032844184897840023 max memory_allocated 29274.77001953125 
[2025-02-20 17:27:05 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.030782539397478104 norm:0.0003095766296610236 max memory_allocated 29274.77001953125 
[2025-02-20 17:27:54 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.030770592391490936 norm:0.00029407619149424136 max memory_allocated 29274.77001953125 
[2025-02-20 17:28:43 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.030759582296013832 norm:0.0002813885221257806 max memory_allocated 29274.77001953125 
[2025-02-20 17:28:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-20 17:29:01 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:29:50 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.03764913231134415 norm:0.002665281994268298 max memory_allocated 29274.95751953125 
[2025-02-20 17:30:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.03663291409611702 norm:0.0018359976820647717 max memory_allocated 29274.95751953125 
[2025-02-20 17:31:28 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.036075688898563385 norm:0.001337786903604865 max memory_allocated 29274.95751953125 
[2025-02-20 17:32:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.03570014238357544 norm:0.0012163391802459955 max memory_allocated 29274.95751953125 
[2025-02-20 17:33:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.035410329699516296 norm:0.001167446025647223 max memory_allocated 29274.95751953125 
[2025-02-20 17:33:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.03522632271051407 norm:0.0012381051201373339 max memory_allocated 29274.95751953125 
[2025-02-20 17:34:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.035087332129478455 norm:0.0012244260869920254 max memory_allocated 29274.95751953125 
[2025-02-20 17:35:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.034968651831150055 norm:0.0010755328694358468 max memory_allocated 29274.95751953125 
[2025-02-20 17:36:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.03487478941679001 norm:0.0010034003062173724 max memory_allocated 29274.95751953125 
[2025-02-20 17:37:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.03479379415512085 norm:0.0009500441374257207 max memory_allocated 29274.95751953125 
[2025-02-20 17:38:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.03472822904586792 norm:0.0008787037804722786 max memory_allocated 29274.95751953125 
[2025-02-20 17:38:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.03467998653650284 norm:0.0008116770186461508 max memory_allocated 29274.95751953125 
[2025-02-20 17:39:40 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.03465111553668976 norm:0.000735459616407752 max memory_allocated 29274.95751953125 
[2025-02-20 17:40:29 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.034629955887794495 norm:0.0007554460316896439 max memory_allocated 29274.95751953125 
[2025-02-20 17:41:18 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.03460876643657684 norm:0.0007509045535698533 max memory_allocated 29274.95751953125 
[2025-02-20 17:42:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.034583043307065964 norm:0.000652391230687499 max memory_allocated 29274.95751953125 
[2025-02-20 17:42:57 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.03456926718354225 norm:0.0006711096502840519 max memory_allocated 29274.95751953125 
[2025-02-20 17:43:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.03457111865282059 norm:0.0006727498257532716 max memory_allocated 29274.95751953125 
[2025-02-20 17:44:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.034552402794361115 norm:0.0006499747978523374 max memory_allocated 29274.95751953125 
[2025-02-20 17:45:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.034546658396720886 norm:0.0006166405510157347 max memory_allocated 29274.95751953125 
[2025-02-20 17:45:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-20 17:45:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:46:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.06858459115028381 norm:0.046194855123758316 max memory_allocated 29275.14501953125 
[2025-02-20 17:47:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.05716985836625099 norm:0.033148109912872314 max memory_allocated 29275.14501953125 
[2025-02-20 17:48:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.05322468653321266 norm:0.02441379427909851 max memory_allocated 29275.14501953125 
[2025-02-20 17:48:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.051259078085422516 norm:0.017497874796390533 max memory_allocated 29275.14501953125 
[2025-02-20 17:49:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.04956844449043274 norm:0.012102536857128143 max memory_allocated 29275.14501953125 
[2025-02-20 17:50:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.04848089441657066 norm:0.008876948617398739 max memory_allocated 29275.14501953125 
[2025-02-20 17:51:26 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.047670453786849976 norm:0.006785467732697725 max memory_allocated 29275.14501953125 
[2025-02-20 17:52:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.04711417108774185 norm:0.005983112379908562 max memory_allocated 29275.14501953125 
[2025-02-20 17:53:04 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.04668073728680611 norm:0.005048975348472595 max memory_allocated 29275.14501953125 
[2025-02-20 17:53:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.04629575088620186 norm:0.004515026696026325 max memory_allocated 29275.14501953125 
[2025-02-20 17:54:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.045934222638607025 norm:0.004274420440196991 max memory_allocated 29275.14501953125 
[2025-02-20 17:55:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.045777175575494766 norm:0.004306293558329344 max memory_allocated 29275.14501953125 
[2025-02-20 17:56:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.04551374912261963 norm:0.00407575024291873 max memory_allocated 29275.14501953125 
[2025-02-20 17:57:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.045380622148513794 norm:0.004007619805634022 max memory_allocated 29275.14501953125 
[2025-02-20 17:57:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.04521828144788742 norm:0.0038571078330278397 max memory_allocated 29275.14501953125 
[2025-02-20 17:58:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.0450688973069191 norm:0.003618344198912382 max memory_allocated 29275.14501953125 
[2025-02-20 17:59:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.04499468207359314 norm:0.0035288450308144093 max memory_allocated 29275.14501953125 
[2025-02-20 18:00:26 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.04477829113602638 norm:0.0031139284837991 max memory_allocated 29275.14501953125 
[2025-02-20 18:01:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.044758401811122894 norm:0.0031595344189554453 max memory_allocated 29275.14501953125 
[2025-02-20 18:02:04 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.0446738637983799 norm:0.003052131738513708 max memory_allocated 29275.14501953125 
[2025-02-20 18:02:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-20 18:02:23 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 18:03:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.09174621850252151 norm:0.010791628621518612 max memory_allocated 29275.33251953125 
[2025-02-20 18:04:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.08590111881494522 norm:0.006461401004344225 max memory_allocated 29275.33251953125 
[2025-02-20 18:04:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.08170954138040543 norm:0.004669753834605217 max memory_allocated 29275.33251953125 
[2025-02-20 18:05:39 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.07884328812360764 norm:0.004869144409894943 max memory_allocated 29275.33251953125 
[2025-02-20 18:06:28 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.0767069160938263 norm:0.004835742525756359 max memory_allocated 29275.33251953125 
[2025-02-20 18:07:17 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.07500045001506805 norm:0.004695119336247444 max memory_allocated 29275.33251953125 
[2025-02-20 18:08:06 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.07358190417289734 norm:0.004650183953344822 max memory_allocated 29275.33251953125 
[2025-02-20 18:08:56 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.07257577776908875 norm:0.00454721599817276 max memory_allocated 29275.33251953125 
[2025-02-20 18:09:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.07153597474098206 norm:0.003889963496476412 max memory_allocated 29275.33251953125 
[2025-02-20 18:10:34 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.0709284320473671 norm:0.004366745240986347 max memory_allocated 29275.33251953125 
[2025-02-20 18:11:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.08508270233869553 norm:0.019393181428313255 max memory_allocated 29275.33251953125 
[2025-02-20 18:12:13 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.07943063229322433 norm:0.014688920229673386 max memory_allocated 29275.33251953125 
[2025-02-20 18:13:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.06998392939567566 norm:0.003975213970988989 max memory_allocated 29275.33251953125 
[2025-02-20 18:13:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.06981305778026581 norm:0.0036935482639819384 max memory_allocated 29275.33251953125 
[2025-02-20 18:14:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.06969750672578812 norm:0.0036271081771701574 max memory_allocated 29275.33251953125 
[2025-02-20 18:15:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.06959843635559082 norm:0.0035792894195765257 max memory_allocated 29275.33251953125 
[2025-02-20 18:16:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.06946815550327301 norm:0.003481128253042698 max memory_allocated 29275.33251953125 
[2025-02-20 18:17:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.06929966062307358 norm:0.003535131923854351 max memory_allocated 29275.33251953125 
[2025-02-20 18:17:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.06914450228214264 norm:0.003439790103584528 max memory_allocated 29275.33251953125 
[2025-02-20 18:18:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.06900855153799057 norm:0.003273590235039592 max memory_allocated 29275.33251953125 
[2025-02-20 18:19:01 root] (main_calibration.py 365): INFO 39924.70862150192
[2025-02-20 18:20:03 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-20 18:21:57 root] (main_calibration.py 158): INFO wikitext2 : 5.099164962768555
[2025-02-20 18:21:58 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-20 18:24:56 root] (main_calibration.py 158): INFO c4 : 6.623444080352783
[2025-02-20 20:26:04 root] (main_calibration.py 169): INFO {'wikitext2': 5.099164962768555, 'c4': 6.623444080352783, 'results': {'winogrande': {'acc': 0.696921862667719, 'acc_stderr': 0.01291672746263446}, 'arc_easy': {'acc': 0.7415824915824916, 'acc_stderr': 0.008982741341291296, 'acc_norm': 0.5904882154882155, 'acc_norm_stderr': 0.010090368160990062}, 'hellaswag': {'acc': 0.5899223262298346, 'acc_stderr': 0.004908423147162024, 'acc_norm': 0.7605058753236407, 'acc_norm_stderr': 0.004259025448541509}, 'boolq': {'acc': 0.6825688073394496, 'acc_stderr': 0.00814124002260939}, 'piqa': {'acc': 0.7894450489662677, 'acc_stderr': 0.009512378081238736, 'acc_norm': 0.7850924918389554, 'acc_norm_stderr': 0.009583665082653313}, 'arc_challenge': {'acc': 0.43856655290102387, 'acc_stderr': 0.014500682618212864, 'acc_norm': 0.44368600682593856, 'acc_norm_stderr': 0.014518421825670452}}, 'versions': {'winogrande': 0, 'arc_easy': 0, 'hellaswag': 0, 'boolq': 1, 'piqa': 0, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
