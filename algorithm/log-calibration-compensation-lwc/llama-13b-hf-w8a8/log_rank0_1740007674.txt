[2025-02-19 23:27:54 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-13b-hf-w8a8', save_dir='./log-calibration-compensation-lwc/quant/llama-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-19 23:32:54 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-19 23:32:54 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-19 23:32:55 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-19 23:32:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-19 23:33:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 23:33:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0017808845732361078 norm:0.002111205365508795 max memory_allocated 29268.02001953125 
[2025-02-19 23:34:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0011080350959673524 norm:0.001486068358644843 max memory_allocated 29268.02001953125 
[2025-02-19 23:35:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0009280950180254877 norm:0.00121159921400249 max memory_allocated 29268.02001953125 
[2025-02-19 23:36:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0008220189483836293 norm:0.001057333080098033 max memory_allocated 29268.02001953125 
[2025-02-19 23:37:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0007579452358186245 norm:0.0009558679303154349 max memory_allocated 29268.02001953125 
[2025-02-19 23:37:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0007098030182532966 norm:0.0008641427848488092 max memory_allocated 29268.02001953125 
[2025-02-19 23:38:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0006780771072953939 norm:0.0007966418634168804 max memory_allocated 29268.02001953125 
[2025-02-19 23:39:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0006565182120539248 norm:0.0007276436081156135 max memory_allocated 29268.02001953125 
[2025-02-19 23:40:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0006406023167073727 norm:0.0006815922679379582 max memory_allocated 29268.02001953125 
[2025-02-19 23:41:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0006288823205977678 norm:0.0006337354425340891 max memory_allocated 29268.02001953125 
[2025-02-19 23:41:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0006149885011836886 norm:0.000571387936361134 max memory_allocated 29268.02001953125 
[2025-02-19 23:42:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0006060139276087284 norm:0.0005264640785753727 max memory_allocated 29268.02001953125 
[2025-02-19 23:43:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0006001273286528885 norm:0.0004932287847623229 max memory_allocated 29268.02001953125 
[2025-02-19 23:44:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0005981504800729454 norm:0.00047663107397966087 max memory_allocated 29268.02001953125 
[2025-02-19 23:45:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0005929383332841098 norm:0.00042761306394822896 max memory_allocated 29268.02001953125 
[2025-02-19 23:46:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0005867453874088824 norm:0.0003938529989682138 max memory_allocated 29268.02001953125 
[2025-02-19 23:46:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0005861337413080037 norm:0.00037062453338876367 max memory_allocated 29268.02001953125 
[2025-02-19 23:47:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0005833792383782566 norm:0.00034220554516650736 max memory_allocated 29268.02001953125 
[2025-02-19 23:48:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0005800211220048368 norm:0.0003212470328435302 max memory_allocated 29268.02001953125 
[2025-02-19 23:49:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0005777305923402309 norm:0.00030053273076191545 max memory_allocated 29268.02001953125 
[2025-02-19 23:49:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-19 23:49:38 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 23:50:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0016445228829979897 norm:0.002094021998345852 max memory_allocated 29268.02001953125 
[2025-02-19 23:51:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0011082910932600498 norm:0.0005470893811434507 max memory_allocated 29268.02001953125 
[2025-02-19 23:52:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0009851340437307954 norm:0.000607045425567776 max memory_allocated 29268.02001953125 
[2025-02-19 23:52:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0009097917936742306 norm:0.0007077245390973985 max memory_allocated 29268.02001953125 
[2025-02-19 23:53:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0008652008837088943 norm:0.0007014828734099865 max memory_allocated 29268.02001953125 
[2025-02-19 23:54:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0008339872001670301 norm:0.0006600100896321237 max memory_allocated 29268.02001953125 
[2025-02-19 23:55:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0008154046372510493 norm:0.0006388789042830467 max memory_allocated 29268.02001953125 
[2025-02-19 23:56:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0008013384649530053 norm:0.0006260222871787846 max memory_allocated 29268.02001953125 
[2025-02-19 23:57:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0007895317394286394 norm:0.000606911547947675 max memory_allocated 29268.02001953125 
[2025-02-19 23:57:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0007802161853760481 norm:0.0005702223279513419 max memory_allocated 29268.02001953125 
[2025-02-19 23:58:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0007708102930337191 norm:0.000547284260392189 max memory_allocated 29268.02001953125 
[2025-02-19 23:59:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0007648440077900887 norm:0.0005139260902069509 max memory_allocated 29268.02001953125 
[2025-02-20 00:00:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0007594118360430002 norm:0.0004861153429374099 max memory_allocated 29268.02001953125 
[2025-02-20 00:01:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0007559825899079442 norm:0.0004686500469688326 max memory_allocated 29268.02001953125 
[2025-02-20 00:01:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0007521425141021609 norm:0.0004417879390530288 max memory_allocated 29268.02001953125 
[2025-02-20 00:02:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0007502980879507959 norm:0.0004277364641893655 max memory_allocated 29268.02001953125 
[2025-02-20 00:03:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0007473527221009135 norm:0.00039531695074401796 max memory_allocated 29268.02001953125 
[2025-02-20 00:04:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0007467402028851211 norm:0.0003798502730205655 max memory_allocated 29268.02001953125 
[2025-02-20 00:05:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0007457663305103779 norm:0.00035736116115003824 max memory_allocated 29268.02001953125 
[2025-02-20 00:06:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0007454482838511467 norm:0.0003430652723181993 max memory_allocated 29268.02001953125 
[2025-02-20 00:06:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-20 00:06:19 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 00:07:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.001859205076470971 norm:0.001983583439141512 max memory_allocated 29268.02001953125 
[2025-02-20 00:07:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.001477717887610197 norm:0.001483792089857161 max memory_allocated 29268.02001953125 
[2025-02-20 00:08:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0013276413083076477 norm:0.001167672104202211 max memory_allocated 29268.02001953125 
[2025-02-20 00:09:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0012321901740506291 norm:0.001131191966123879 max memory_allocated 29268.02001953125 
[2025-02-20 00:10:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0011609148932620883 norm:0.0009533717529848218 max memory_allocated 29268.02001953125 
[2025-02-20 00:11:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0011377953924238682 norm:0.0008899163804017007 max memory_allocated 29268.02001953125 
[2025-02-20 00:12:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0010930470889434218 norm:0.0007168238516896963 max memory_allocated 29268.02001953125 
[2025-02-20 00:12:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0010643573477864265 norm:0.000756491266656667 max memory_allocated 29268.02001953125 
[2025-02-20 00:13:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0010566888377070427 norm:0.0007535096956416965 max memory_allocated 29268.02001953125 
[2025-02-20 00:14:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0010401476174592972 norm:0.0006342200795188546 max memory_allocated 29268.02001953125 
[2025-02-20 00:15:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0010348750511184335 norm:0.0006487116916105151 max memory_allocated 29268.02001953125 
[2025-02-20 00:16:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0010109414579346776 norm:0.0005967261968180537 max memory_allocated 29268.02001953125 
[2025-02-20 00:16:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0010028744582086802 norm:0.0005731859127990901 max memory_allocated 29268.02001953125 
[2025-02-20 00:17:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0009992690756917 norm:0.0005005682469345629 max memory_allocated 29268.02001953125 
[2025-02-20 00:18:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0010100409854203463 norm:0.000542026071343571 max memory_allocated 29268.02001953125 
[2025-02-20 00:19:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0009969811653718352 norm:0.00047585301217623055 max memory_allocated 29268.02001953125 
[2025-02-20 00:20:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0009987856028601527 norm:0.0004321547457948327 max memory_allocated 29268.02001953125 
[2025-02-20 00:21:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0009964870987460017 norm:0.0003813124494627118 max memory_allocated 29268.02001953125 
[2025-02-20 00:21:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.000996573711745441 norm:0.0004356469726189971 max memory_allocated 29268.02001953125 
[2025-02-20 00:22:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0010023615323007107 norm:0.0004462915239855647 max memory_allocated 29268.02001953125 
[2025-02-20 00:22:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-20 00:23:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0020806349348276854 norm:0.0008369594579562545 max memory_allocated 29268.43798828125 
[2025-02-20 00:24:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0015370916808024049 norm:0.00028357416158542037 max memory_allocated 29268.43798828125 
[2025-02-20 00:25:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0013824129709973931 norm:0.00018703944806475192 max memory_allocated 29268.43798828125 
[2025-02-20 00:26:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.001295184250921011 norm:0.00015845206507947296 max memory_allocated 29268.43798828125 
[2025-02-20 00:27:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.001240083947777748 norm:0.00013049572589807212 max memory_allocated 29268.43798828125 
[2025-02-20 00:27:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0012082885950803757 norm:0.00010817529255291447 max memory_allocated 29268.43798828125 
[2025-02-20 00:28:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0011905382853001356 norm:9.399135888088495e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:29:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0011803610250353813 norm:8.639640145702288e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:30:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0011734840227290988 norm:7.228773029055446e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:31:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0011661089956760406 norm:6.787783786421642e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:31:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0011597385164350271 norm:5.858500662725419e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:32:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0011531509226188064 norm:5.2152390708215535e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:33:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0011492216726765037 norm:4.190967956674285e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:34:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0011467300355434418 norm:4.728707790491171e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:35:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0011439311783760786 norm:3.8317928556352854e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:36:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0011423781979829073 norm:3.54380754288286e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:36:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0011411488521844149 norm:2.9956932849017903e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:37:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0011394594330340624 norm:2.604956171126105e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:38:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0011400289367884398 norm:2.7522097298060544e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:39:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0011384481331333518 norm:2.2319563868222758e-05 max memory_allocated 29268.43798828125 
[2025-02-20 00:39:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-20 00:40:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0038834568113088608 norm:0.003496395656839013 max memory_allocated 29268.62548828125 
[2025-02-20 00:41:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0019922510255128145 norm:0.0006957092555239797 max memory_allocated 29268.62548828125 
[2025-02-20 00:42:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0017489660531282425 norm:0.00036589291994459927 max memory_allocated 29268.62548828125 
[2025-02-20 00:42:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0016349069774150848 norm:0.00027527703787200153 max memory_allocated 29268.62548828125 
[2025-02-20 00:43:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.001540619763545692 norm:0.00020714735728688538 max memory_allocated 29268.62548828125 
[2025-02-20 00:44:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0014841214288026094 norm:0.00016958126798272133 max memory_allocated 29268.62548828125 
[2025-02-20 00:45:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0014492679620161653 norm:0.00014629773795604706 max memory_allocated 29268.62548828125 
[2025-02-20 00:46:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0014261443866416812 norm:0.00013507460243999958 max memory_allocated 29268.62548828125 
[2025-02-20 00:46:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0014119322877377272 norm:0.00011153699597343802 max memory_allocated 29268.62548828125 
[2025-02-20 00:47:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.00140279287006706 norm:0.00011437907232902944 max memory_allocated 29268.62548828125 
[2025-02-20 00:48:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.001391921890899539 norm:0.00010879411274800077 max memory_allocated 29268.62548828125 
[2025-02-20 00:49:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0013826731592416763 norm:9.739419328980148e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:50:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0013770272489637136 norm:9.416321699973196e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:50:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0013748728670179844 norm:7.905396341811866e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:51:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0013724996242672205 norm:7.457237370545045e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:52:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0013678116956725717 norm:7.172923506004736e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:53:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0013646810548380017 norm:6.578227475984022e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:54:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0013574312906712294 norm:5.7144821766996756e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:55:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0013563460670411587 norm:5.338783375918865e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:55:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0013525157701224089 norm:5.058337046648376e-05 max memory_allocated 29268.62548828125 
[2025-02-20 00:56:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-20 00:57:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.003184682922437787 norm:0.0019276464590802789 max memory_allocated 29268.81298828125 
[2025-02-20 00:57:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.001952462480403483 norm:0.0003957150620408356 max memory_allocated 29268.81298828125 
[2025-02-20 00:58:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0018044523894786835 norm:0.0002620599989313632 max memory_allocated 29268.81298828125 
[2025-02-20 00:59:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0017094340873882174 norm:0.00019274397345725447 max memory_allocated 29268.81298828125 
[2025-02-20 01:00:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0016461043851450086 norm:0.0001508924033259973 max memory_allocated 29268.81298828125 
[2025-02-20 01:01:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0016052131541073322 norm:0.00012554688146337867 max memory_allocated 29268.81298828125 
[2025-02-20 01:01:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0015770146856084466 norm:0.00010670543269952759 max memory_allocated 29268.81298828125 
[2025-02-20 01:02:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0015596705488860607 norm:9.488606883678585e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:03:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0015433859080076218 norm:8.808654092717916e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:04:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0015315713826566935 norm:8.007294673006982e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:05:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0015220630448311567 norm:7.418708264594898e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:05:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0015156653244048357 norm:6.14906894043088e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:06:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0015086719067767262 norm:6.036547711119056e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:07:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0014992994256317616 norm:5.685368523700163e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:08:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0014951000921428204 norm:4.848686512559652e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:09:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0014914064668118954 norm:4.844514114665799e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:10:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0014883456751704216 norm:4.1262723243562505e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:10:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.001485636574216187 norm:3.772487980313599e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:11:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0014818338677287102 norm:3.4536868042778224e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:12:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.001479477621614933 norm:2.9591865313705057e-05 max memory_allocated 29268.81298828125 
[2025-02-20 01:12:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-20 01:13:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.002782464027404785 norm:0.0005677406443282962 max memory_allocated 29269.00048828125 
[2025-02-20 01:14:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.00214227638207376 norm:0.00021735290647484362 max memory_allocated 29269.00048828125 
[2025-02-20 01:15:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.002015255857259035 norm:0.00013625173596665263 max memory_allocated 29269.00048828125 
[2025-02-20 01:16:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0019432392437011003 norm:0.00012987585796508938 max memory_allocated 29269.00048828125 
[2025-02-20 01:16:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0018571699038147926 norm:8.872384205460548e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:17:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0018546690698713064 norm:9.759077511262149e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:18:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0018526760395616293 norm:9.34187337406911e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:19:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0018419360276311636 norm:7.659206312382594e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:20:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0018392548663541675 norm:7.355270645348355e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:20:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0018210179405286908 norm:8.747685205889866e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:21:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0017988847102969885 norm:6.135337753221393e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:22:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0017968115862458944 norm:5.411624806583859e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:23:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.001797364791855216 norm:5.407145363278687e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:24:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0017948683816939592 norm:4.977383650839329e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:25:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0017940760590136051 norm:4.937908306601457e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:25:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0017876038327813148 norm:4.467694088816643e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:26:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0017815363826230168 norm:4.284813621779904e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:27:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.001780461985617876 norm:4.2585612391121686e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:28:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0017751872073858976 norm:4.595464633894153e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:29:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0017733575077727437 norm:5.2617637265939265e-05 max memory_allocated 29269.00048828125 
[2025-02-20 01:29:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-20 01:30:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.003104845993220806 norm:0.0006169299595057964 max memory_allocated 29269.18798828125 
[2025-02-20 01:31:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0021672945003956556 norm:0.0001415107981301844 max memory_allocated 29269.18798828125 
[2025-02-20 01:31:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.002027770271524787 norm:7.511142030125484e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:32:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.001968521624803543 norm:6.175146700115874e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:33:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0019256507512181997 norm:4.9725906137609854e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:34:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.0018957408610731363 norm:3.992300480604172e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:35:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.001877956441603601 norm:3.634493623394519e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:35:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0018647313117980957 norm:3.33274292643182e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:36:45 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.001855722744949162 norm:3.1270250474335626e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:37:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0018471089424565434 norm:2.8262322302907705e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:38:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0018420739797875285 norm:2.7844840587931685e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:39:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0018376808147877455 norm:2.4832648705341853e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:40:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0018345753196626902 norm:2.3890459488029592e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:40:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0018319787923246622 norm:2.1907011614530347e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:41:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0018301942618563771 norm:2.2837579308543354e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:42:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0018281994853168726 norm:2.0057721485500224e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:43:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0018270793370902538 norm:2.0937093722750433e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:44:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0018255419563502073 norm:2.107593354594428e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:44:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0018217229517176747 norm:1.883023651316762e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:45:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0018225453095510602 norm:1.9029075701837428e-05 max memory_allocated 29269.18798828125 
[2025-02-20 01:45:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-20 01:46:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0027123058680444956 norm:0.0002832819300238043 max memory_allocated 29269.37548828125 
[2025-02-20 01:47:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0022171013988554478 norm:8.064614667091519e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:48:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.002123525133356452 norm:5.4720076150260866e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:49:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0020723200868815184 norm:4.2723539081634954e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:50:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.002038901438936591 norm:3.144213405903429e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:50:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.002019197214394808 norm:2.868924093490932e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:51:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.002003829926252365 norm:2.7183374186279252e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:52:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0019910153932869434 norm:2.411773311905563e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:53:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0019834355916827917 norm:2.2093327061156742e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:54:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.001977949868887663 norm:2.1220075723249465e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:55:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.001973335165530443 norm:2.069846414087806e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:55:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.001969240140169859 norm:1.9293618606752716e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:56:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.001966563519090414 norm:2.07904649869306e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:57:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0019636619836091995 norm:2.050602597591933e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:58:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.001961048925295472 norm:1.859295298345387e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:59:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0019598749931901693 norm:1.7644493709667586e-05 max memory_allocated 29269.37548828125 
[2025-02-20 01:59:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0019588752184063196 norm:1.702175723039545e-05 max memory_allocated 29269.37548828125 
[2025-02-20 02:00:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.001956772990524769 norm:1.7284506611758843e-05 max memory_allocated 29269.37548828125 
[2025-02-20 02:01:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0019560065120458603 norm:1.7388800188200548e-05 max memory_allocated 29269.37548828125 
[2025-02-20 02:02:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.001955238403752446 norm:1.5693351087975316e-05 max memory_allocated 29269.37548828125 
[2025-02-20 02:02:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-20 02:03:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.0027104984037578106 norm:0.00021548764198087156 max memory_allocated 29269.56298828125 
[2025-02-20 02:04:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0022923843935132027 norm:5.911566404392943e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:05:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.002216492546722293 norm:3.547743835952133e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:05:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.002176515990868211 norm:2.8340789867797866e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:06:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0021494380198419094 norm:2.3218439309857786e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:07:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0021335177589207888 norm:2.1109284716658294e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:08:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.002123023383319378 norm:1.8759517843136564e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:09:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0021143490448594093 norm:1.825873914640397e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:09:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.002108785090968013 norm:1.7433558241464198e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:10:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.002103532664477825 norm:1.6355224943254143e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:11:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.002099704695865512 norm:1.5522560715908185e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:12:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0020967109594494104 norm:1.512242124590557e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:13:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0020937472581863403 norm:1.4516552255372517e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:14:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.002091459697112441 norm:1.320245792157948e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:14:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0020907935686409473 norm:1.3324541214387864e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:15:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.002089862944558263 norm:1.253492973773973e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:16:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0020887444261461496 norm:1.2532480468507856e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:17:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0020873185712844133 norm:1.2551653526315931e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:18:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.00208640331402421 norm:1.1825959518318996e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:18:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0020857430063188076 norm:1.169317965832306e-05 max memory_allocated 29269.56298828125 
[2025-02-20 02:19:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-20 02:20:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.003404969349503517 norm:0.0004617542726919055 max memory_allocated 29269.75048828125 
[2025-02-20 02:20:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0025584266986697912 norm:0.00011628240463323891 max memory_allocated 29269.75048828125 
[2025-02-20 02:21:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0024194950237870216 norm:5.803187741548754e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:22:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.002365414286032319 norm:4.428461397765204e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:23:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0023330370895564556 norm:3.741721957339905e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:24:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0023077810183167458 norm:3.100883259321563e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:24:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0022913161665201187 norm:2.6152456484851427e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:25:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.0022799884900450706 norm:2.311897333129309e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:26:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0022688747849315405 norm:2.0056450011907145e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:27:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0022621985990554094 norm:1.8488563000573777e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:28:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0022558332420885563 norm:1.904333839775063e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:29:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0022510981652885675 norm:1.9143339159199968e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:29:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.00224756239913404 norm:1.7323985957773402e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:30:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0022449465468525887 norm:1.683246409811545e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:31:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0022429635282605886 norm:1.602712472958956e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:32:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0022403148468583822 norm:1.5776244254084304e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:33:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.002238646149635315 norm:1.4823289347987156e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:33:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0022383681498467922 norm:1.528313077869825e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:34:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.00223695021122694 norm:1.4428059330384713e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:35:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.002235767897218466 norm:1.3784354450763203e-05 max memory_allocated 29269.75048828125 
[2025-02-20 02:35:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-20 02:36:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0030563357286155224 norm:0.0001981080131372437 max memory_allocated 29269.93798828125 
[2025-02-20 02:37:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0026493477635085583 norm:5.944157237536274e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:38:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0025615827180445194 norm:3.640742943389341e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:39:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0025182596873492002 norm:2.9036957130301744e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:39:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.002489331644028425 norm:2.3520180548075587e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:40:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002470326842740178 norm:1.9412980691413395e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:41:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0024583046324551105 norm:1.7738053429638967e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:42:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0024502170272171497 norm:1.6774971300037578e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:43:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0024427385069429874 norm:1.545202212582808e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:44:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0024377296213060617 norm:1.4568634469469544e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:44:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.002433456713333726 norm:1.4272454791353084e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:45:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0024298876523971558 norm:1.4142192412691657e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:46:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0024267390836030245 norm:1.3341034900804516e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:47:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0024254450108855963 norm:1.2550653991638683e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:48:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.002423497149720788 norm:1.182987580250483e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:48:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0024214782752096653 norm:1.0362014108977746e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:49:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.002420200500637293 norm:1.1029866072931327e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:50:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.002419381169602275 norm:1.0407603440398816e-05 max memory_allocated 29269.93798828125 
[2025-02-20 02:51:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0024184859357774258 norm:9.79090418695705e-06 max memory_allocated 29269.93798828125 
[2025-02-20 02:52:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.002417126437649131 norm:9.189810953103006e-06 max memory_allocated 29269.93798828125 
[2025-02-20 02:52:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-20 02:53:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.003167924005538225 norm:0.00020386384858284146 max memory_allocated 29270.12548828125 
[2025-02-20 02:54:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0027913451194763184 norm:5.991799844196066e-05 max memory_allocated 29270.12548828125 
[2025-02-20 02:54:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002714464208111167 norm:3.44260515703354e-05 max memory_allocated 29270.12548828125 
[2025-02-20 02:55:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0026776432059705257 norm:2.8928998290211894e-05 max memory_allocated 29270.12548828125 
[2025-02-20 02:56:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0026529114693403244 norm:2.419212250970304e-05 max memory_allocated 29270.12548828125 
[2025-02-20 02:57:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.00263429107144475 norm:2.0029097868246026e-05 max memory_allocated 29270.12548828125 
[2025-02-20 02:58:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.002621315885335207 norm:1.813966991903726e-05 max memory_allocated 29270.12548828125 
[2025-02-20 02:59:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.002612372161820531 norm:1.6198287994484417e-05 max memory_allocated 29270.12548828125 
[2025-02-20 02:59:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0026054694317281246 norm:1.5350289686466567e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:00:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0025995818432420492 norm:1.3817601939081214e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:01:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0025956202298402786 norm:1.3125149052939378e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:02:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0025917361490428448 norm:1.2729069567285478e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:03:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.002589145675301552 norm:1.2007772056676913e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:03:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0025860259775072336 norm:1.1396272384445183e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:04:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.0025846860371530056 norm:1.071280348696746e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:05:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0025831039529293776 norm:1.0516910151636694e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:06:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0025817605201154947 norm:1.0245391422358807e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:07:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0025804992765188217 norm:1.0811269021360204e-05 max memory_allocated 29270.12548828125 
[2025-02-20 03:07:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0025791204534471035 norm:9.687943020253442e-06 max memory_allocated 29270.12548828125 
[2025-02-20 03:08:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0025782110169529915 norm:9.656676411395893e-06 max memory_allocated 29270.12548828125 
[2025-02-20 03:09:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-20 03:09:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.0032841016072779894 norm:0.0001275517715839669 max memory_allocated 29270.31298828125 
[2025-02-20 03:10:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.003025130368769169 norm:4.381173130241223e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:11:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.002962761791422963 norm:2.7880661946255714e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:12:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0029301480390131474 norm:2.2589196305489168e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:13:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.0029058882500976324 norm:1.9052891730098054e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:14:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0028886101208627224 norm:1.6369158402085304e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:14:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.002876994200050831 norm:1.5292916941689327e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:15:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0028684462886303663 norm:1.4262526747188531e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:16:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.0028609258588403463 norm:1.357096425635973e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:17:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.002855445258319378 norm:1.2621150744962506e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:18:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0028508631512522697 norm:1.1863412510138005e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:18:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0028481839690357447 norm:1.1679634553729557e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:19:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0028453462291508913 norm:1.1476888175820932e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:20:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.002842316636815667 norm:1.1254574019403663e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:21:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0028396667912602425 norm:1.0446909072925337e-05 max memory_allocated 29270.31298828125 
[2025-02-20 03:22:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.002837692853063345 norm:9.91912111203419e-06 max memory_allocated 29270.31298828125 
[2025-02-20 03:22:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0028367661871016026 norm:9.780817890714388e-06 max memory_allocated 29270.31298828125 
[2025-02-20 03:23:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.002835656516253948 norm:9.965791832655668e-06 max memory_allocated 29270.31298828125 
[2025-02-20 03:24:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.002834585029631853 norm:9.375337867822964e-06 max memory_allocated 29270.31298828125 
[2025-02-20 03:25:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0028338914271444082 norm:9.336093171441462e-06 max memory_allocated 29270.31298828125 
[2025-02-20 03:25:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-20 03:26:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.003945725038647652 norm:0.0002767238474916667 max memory_allocated 29270.50048828125 
[2025-02-20 03:27:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0033915594685822725 norm:8.996157703222707e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:28:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0032721906900405884 norm:4.663367144530639e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:28:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0032208310440182686 norm:3.645103788585402e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:29:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.003187417984008789 norm:3.1351391953649e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:30:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.003158353269100189 norm:2.6835328753804788e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:31:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0031383398454636335 norm:2.3945443899719976e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:32:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.003123227506875992 norm:2.157959170290269e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:33:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0031118718907237053 norm:1.984795198950451e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:33:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0031023877672851086 norm:1.819395401980728e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:34:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0030960619915276766 norm:1.672724465606734e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:35:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0030899636913090944 norm:1.5829991752980277e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:36:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.003085170639678836 norm:1.5190347767202184e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:37:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0030811221804469824 norm:1.3936893992649857e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:37:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.00307805510237813 norm:1.3493248843587935e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:38:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.0030749915167689323 norm:1.271581459150184e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:39:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0030733193270862103 norm:1.2746124411933124e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:40:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.003071741433814168 norm:1.256566793017555e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:41:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0030693309381604195 norm:1.2189847439003643e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:42:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.0030670850537717342 norm:1.1072833331127185e-05 max memory_allocated 29270.50048828125 
[2025-02-20 03:42:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-20 03:43:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.004862711764872074 norm:0.0005983546143397689 max memory_allocated 29270.68798828125 
[2025-02-20 03:43:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.003857651026919484 norm:0.00015070750669110566 max memory_allocated 29270.68798828125 
[2025-02-20 03:44:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.003690616460517049 norm:8.307653479278088e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:45:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0036283708177506924 norm:6.641024083364755e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:46:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0035866389516741037 norm:5.5578457249794155e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:47:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.003547198837623 norm:4.492204243433662e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:48:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.003520316444337368 norm:3.85718813049607e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:48:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.003502574283629656 norm:3.485162233118899e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:49:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.00348659441806376 norm:3.1499104807153344e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:50:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0034744839649647474 norm:2.8864109481219202e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:51:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.003464752109721303 norm:2.6115429136552848e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:52:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.003457577433437109 norm:2.453555316606071e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:52:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0034515459556132555 norm:2.2832276954432018e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:53:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.003445208538323641 norm:2.1105621271999553e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:54:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.0034398771822452545 norm:1.9644776330096647e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:55:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.003436880651861429 norm:1.986069037229754e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:56:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.003433042671531439 norm:1.8603950593387708e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:57:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.0034308331087231636 norm:1.7873411707114428e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:57:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.003427906194701791 norm:1.6806190615170635e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:58:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.0034250104799866676 norm:1.6333880921592936e-05 max memory_allocated 29270.68798828125 
[2025-02-20 03:58:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-20 03:59:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.0045851957984268665 norm:0.00029616765095852315 max memory_allocated 29270.87548828125 
[2025-02-20 04:00:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.004055777098983526 norm:8.898120722733438e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:01:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.003948161844164133 norm:5.186066482565366e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:02:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.003900005482137203 norm:4.158927913522348e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:03:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0038627157919108868 norm:3.3830521715572104e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:03:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0038338368758559227 norm:2.803486495395191e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:04:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.003813509363681078 norm:2.4705295800231397e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:05:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.00379851832985878 norm:2.1632664356729947e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:06:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.003787003690376878 norm:1.948978751897812e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:07:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.003778284415602684 norm:1.790690112102311e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:07:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.00377119448967278 norm:1.6476256860187277e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:08:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.0037657904904335737 norm:1.5372817870229483e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:09:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0037603480741381645 norm:1.4227067367755808e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:10:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0037570632994174957 norm:1.3481583664542995e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:11:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.003754204837605357 norm:1.2544624041765928e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:12:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.0037507705856114626 norm:1.2065761438861955e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:12:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.003748477203771472 norm:1.1412033018132206e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:13:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.003746527247130871 norm:1.1537031241459772e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:14:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0037442301400005817 norm:1.1153502782690339e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:15:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.003742196597158909 norm:1.0915085113083478e-05 max memory_allocated 29270.87548828125 
[2025-02-20 04:15:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-20 04:16:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.004919521510601044 norm:0.00027213364955969155 max memory_allocated 29271.06298828125 
[2025-02-20 04:17:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.004407464060932398 norm:8.61962980707176e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:18:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.004293499980121851 norm:4.869584881816991e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:18:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.004241278860718012 norm:3.9009712054394186e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:19:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.004203339107334614 norm:3.273188121966086e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:20:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.004173635505139828 norm:2.74753929261351e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:21:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.0041528670117259026 norm:2.366722583246883e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:22:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.004137896001338959 norm:2.0952818886144087e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:22:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.004127116873860359 norm:1.9053570213145576e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:23:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.004117433913052082 norm:1.7616739569348283e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:24:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.004110801964998245 norm:1.6244768630713224e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:25:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.004104656167328358 norm:1.6002552001737058e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:26:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.004099109210073948 norm:1.479402544646291e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:26:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.004094634670764208 norm:1.4875415217829868e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:27:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.004090854898095131 norm:1.4136267054709606e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:28:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.004086539149284363 norm:1.3599637895822525e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:29:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.004083558917045593 norm:1.272322242584778e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:30:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0040809595957398415 norm:1.219574551214464e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:31:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.004078493919223547 norm:1.1688530321407598e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:31:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0040766592137515545 norm:1.1635477676463779e-05 max memory_allocated 29271.06298828125 
[2025-02-20 04:32:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-20 04:33:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.0057845343835651875 norm:0.0004234157386235893 max memory_allocated 29271.25048828125 
[2025-02-20 04:33:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.005044439807534218 norm:0.00014556656242348254 max memory_allocated 29271.25048828125 
[2025-02-20 04:34:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.004882967099547386 norm:7.884384831413627e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:35:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.00481108482927084 norm:5.990476347506046e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:36:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.004760029725730419 norm:4.98825975228101e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:37:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.004721521399915218 norm:4.281298606656492e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:37:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.004689805209636688 norm:3.753496639546938e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:38:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.00466918246820569 norm:3.3661650377325714e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:39:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.0046534184366464615 norm:2.999052230734378e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:40:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.004636481869965792 norm:2.7781930839410052e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:41:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.004623706918209791 norm:2.535529711167328e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:41:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.004614340607076883 norm:2.3979617253644392e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:42:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.004606932867318392 norm:2.0880172087345272e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:43:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.004600482527166605 norm:1.9819806766463444e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:44:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.004593325313180685 norm:1.9452081687632017e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:45:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.004586831200867891 norm:1.8033420928986743e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:46:03 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.004580735228955746 norm:1.6667549061821774e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:46:51 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.004576481878757477 norm:1.538829928904306e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:47:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0045736427418887615 norm:1.552169123897329e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:48:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.004569726996123791 norm:1.4765419109608047e-05 max memory_allocated 29271.25048828125 
[2025-02-20 04:48:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-20 04:49:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.00626419810578227 norm:0.0003762581036426127 max memory_allocated 29271.43798828125 
[2025-02-20 04:50:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.005573449656367302 norm:0.00011944789730478078 max memory_allocated 29271.43798828125 
[2025-02-20 04:51:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.005420065484941006 norm:6.748158921254799e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:52:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.005356163252145052 norm:5.2502829930745065e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:52:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.005310902837663889 norm:4.375442222226411e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:53:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.005272755865007639 norm:3.6428340536076576e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:54:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0052439747378230095 norm:3.175862366333604e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:55:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0052221049554646015 norm:2.7892958314623684e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:56:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.005204804241657257 norm:2.5137464035651647e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:56:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.005191797390580177 norm:2.2555097530130297e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:57:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.005181301850825548 norm:2.1209090846241452e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:58:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.005173343233764172 norm:1.9755549146793783e-05 max memory_allocated 29271.43798828125 
[2025-02-20 04:59:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.005166569724678993 norm:1.8644597730599344e-05 max memory_allocated 29271.43798828125 
[2025-02-20 05:00:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.005160449538379908 norm:1.7892642063088715e-05 max memory_allocated 29271.43798828125 
[2025-02-20 05:01:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.005155802704393864 norm:1.7127455066656694e-05 max memory_allocated 29271.43798828125 
[2025-02-20 05:01:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.005151018500328064 norm:1.6085079550975934e-05 max memory_allocated 29271.43798828125 
[2025-02-20 05:02:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.005147288553416729 norm:1.5629164408892393e-05 max memory_allocated 29271.43798828125 
[2025-02-20 05:03:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.0051438515074551105 norm:1.5065415027493145e-05 max memory_allocated 29271.43798828125 
[2025-02-20 05:04:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.005141003988683224 norm:1.4063510207051877e-05 max memory_allocated 29271.43798828125 
[2025-02-20 05:05:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.005138978827744722 norm:1.3986559679324273e-05 max memory_allocated 29271.43798828125 
[2025-02-20 05:05:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-20 05:06:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.0073004295118153095 norm:0.0005602488527074456 max memory_allocated 29271.62548828125 
[2025-02-20 05:07:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.006332886405289173 norm:0.0001768604852259159 max memory_allocated 29271.62548828125 
[2025-02-20 05:07:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.006105478852987289 norm:9.39561941777356e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:08:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.00601798202842474 norm:7.153301703510806e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:09:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.0059565044939517975 norm:5.865540879312903e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:10:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.005908021703362465 norm:4.987501961295493e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:11:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.005871941801160574 norm:4.204730430501513e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:11:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.005843971855938435 norm:3.681113230413757e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:12:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.005820382386445999 norm:3.2352556445403025e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:13:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.0058030495420098305 norm:2.933256655524019e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:14:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.0057899234816432 norm:2.72056113317376e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:15:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.005779235158115625 norm:2.5058976461878046e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:16:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.005769138224422932 norm:2.288830728502944e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:16:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.005760879721492529 norm:2.1582496628980152e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:17:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.005752980709075928 norm:1.9876184524036944e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:18:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.005747159011662006 norm:1.8610002371133305e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:19:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.005741236265748739 norm:1.7901094906846993e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:20:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.005736896302551031 norm:1.6904879885260016e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:20:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.005733152851462364 norm:1.6083396985777654e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:21:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.005729585886001587 norm:1.5264235116774216e-05 max memory_allocated 29271.62548828125 
[2025-02-20 05:21:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-20 05:22:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.008203219622373581 norm:0.0008368031121790409 max memory_allocated 29271.81298828125 
[2025-02-20 05:23:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.007107285782694817 norm:0.0002195845590904355 max memory_allocated 29271.81298828125 
