[2025-03-26 12:32:19 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-13b-hf-w4a6', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-26 12:32:40 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-26 12:32:40 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-26 12:32:40 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-26 12:32:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-26 12:32:47 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 12:33:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.02717066928744316 norm:0.020310627296566963 max memory_allocated 29268.02001953125 
[2025-03-26 12:34:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.017647070810198784 norm:0.011233149096369743 max memory_allocated 29268.02001953125 
[2025-03-26 12:35:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.013231180608272552 norm:0.007065847981721163 max memory_allocated 29268.02001953125 
[2025-03-26 12:36:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.01193287130445242 norm:0.005943232215940952 max memory_allocated 29268.02001953125 
[2025-03-26 12:36:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.01139471773058176 norm:0.005207504145801067 max memory_allocated 29268.02001953125 
[2025-03-26 12:37:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.011085709556937218 norm:0.0045709507539868355 max memory_allocated 29268.02001953125 
[2025-03-26 12:38:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.010765856131911278 norm:0.003995129838585854 max memory_allocated 29268.02001953125 
[2025-03-26 12:39:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.010752853937447071 norm:0.003679483663290739 max memory_allocated 29268.02001953125 
[2025-03-26 12:40:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.010579882189631462 norm:0.0031678462401032448 max memory_allocated 29268.02001953125 
[2025-03-26 12:41:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.01054454781115055 norm:0.002879711566492915 max memory_allocated 29268.02001953125 
[2025-03-26 12:41:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.010367866605520248 norm:0.0025227710139006376 max memory_allocated 29268.02001953125 
[2025-03-26 12:42:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.010338467545807362 norm:0.0022273040376603603 max memory_allocated 29268.02001953125 
[2025-03-26 12:43:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.010264108888804913 norm:0.0020189641509205103 max memory_allocated 29268.02001953125 
[2025-03-26 12:44:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.010327765718102455 norm:0.0018949134973809123 max memory_allocated 29268.02001953125 
[2025-03-26 12:45:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.010170654393732548 norm:0.0016261334531009197 max memory_allocated 29268.02001953125 
[2025-03-26 12:46:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.010203409008681774 norm:0.001672232523560524 max memory_allocated 29268.02001953125 
[2025-03-26 12:46:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.01009672787040472 norm:0.0015479676658287644 max memory_allocated 29268.02001953125 
[2025-03-26 12:47:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.010096434503793716 norm:0.0015548696974292397 max memory_allocated 29268.02001953125 
[2025-03-26 12:48:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.010136330500245094 norm:0.0014657404972240329 max memory_allocated 29268.02001953125 
[2025-03-26 12:49:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.010037329979240894 norm:0.001410877681337297 max memory_allocated 29268.02001953125 
[2025-03-26 12:49:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-26 12:49:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 12:50:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.04361420124769211 norm:0.020213192328810692 max memory_allocated 29268.02001953125 
[2025-03-26 12:51:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.028990602120757103 norm:0.012062489055097103 max memory_allocated 29268.02001953125 
[2025-03-26 12:52:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.022728823125362396 norm:0.007264270447194576 max memory_allocated 29268.02001953125 
[2025-03-26 12:53:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.020814474672079086 norm:0.006022078450769186 max memory_allocated 29268.02001953125 
[2025-03-26 12:53:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.020011182874441147 norm:0.005332925356924534 max memory_allocated 29268.02001953125 
[2025-03-26 12:54:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.01949821598827839 norm:0.004788877908140421 max memory_allocated 29268.02001953125 
[2025-03-26 12:55:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.019150465726852417 norm:0.004308963660150766 max memory_allocated 29268.02001953125 
[2025-03-26 12:56:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.018863432109355927 norm:0.003945940174162388 max memory_allocated 29268.02001953125 
[2025-03-26 12:57:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.01865510083734989 norm:0.003614713903516531 max memory_allocated 29268.02001953125 
[2025-03-26 12:58:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.018446985632181168 norm:0.0033081748988479376 max memory_allocated 29268.02001953125 
[2025-03-26 12:58:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.018251536414027214 norm:0.003072710009291768 max memory_allocated 29268.02001953125 
[2025-03-26 12:59:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.018067993223667145 norm:0.002797712804749608 max memory_allocated 29268.02001953125 
[2025-03-26 13:00:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.01791244000196457 norm:0.0025270101614296436 max memory_allocated 29268.02001953125 
[2025-03-26 13:01:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.01781737059354782 norm:0.0022995262406766415 max memory_allocated 29268.02001953125 
[2025-03-26 13:02:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.017716074362397194 norm:0.002085413783788681 max memory_allocated 29268.02001953125 
[2025-03-26 13:03:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.017646733671426773 norm:0.001895574270747602 max memory_allocated 29268.02001953125 
[2025-03-26 13:03:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.017574500292539597 norm:0.0017307178350165486 max memory_allocated 29268.02001953125 
[2025-03-26 13:04:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.01755032315850258 norm:0.0017240192973986268 max memory_allocated 29268.02001953125 
[2025-03-26 13:05:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.017627540975809097 norm:0.001719017163850367 max memory_allocated 29268.02001953125 
[2025-03-26 13:06:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.01757255755364895 norm:0.0016060054767876863 max memory_allocated 29268.02001953125 
[2025-03-26 13:06:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-26 13:06:44 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 13:07:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.04606279358267784 norm:0.010439390316605568 max memory_allocated 29268.39501953125 
[2025-03-26 13:08:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.038999058306217194 norm:0.00935313105583191 max memory_allocated 29268.39501953125 
[2025-03-26 13:09:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.03442557156085968 norm:0.006567282602190971 max memory_allocated 29268.39501953125 
[2025-03-26 13:10:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.03292173892259598 norm:0.00607310002669692 max memory_allocated 29268.39501953125 
[2025-03-26 13:10:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.03226320818066597 norm:0.0057640899904072285 max memory_allocated 29268.39501953125 
[2025-03-26 13:11:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.031579937785863876 norm:0.005554886534810066 max memory_allocated 29268.39501953125 
[2025-03-26 13:12:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.031324803829193115 norm:0.0056050908751785755 max memory_allocated 29268.39501953125 
[2025-03-26 13:13:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.030705656856298447 norm:0.005320149939507246 max memory_allocated 29268.39501953125 
[2025-03-26 13:14:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.030542699620127678 norm:0.005191712640225887 max memory_allocated 29268.39501953125 
[2025-03-26 13:15:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.03028983436524868 norm:0.004873114172369242 max memory_allocated 29268.39501953125 
[2025-03-26 13:15:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.030201172456145287 norm:0.0047170245088636875 max memory_allocated 29268.39501953125 
[2025-03-26 13:16:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.03024306893348694 norm:0.004623619373887777 max memory_allocated 29268.39501953125 
[2025-03-26 13:17:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.029920050874352455 norm:0.004657916724681854 max memory_allocated 29268.39501953125 
[2025-03-26 13:18:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.03032863698899746 norm:0.005035987589508295 max memory_allocated 29268.39501953125 
[2025-03-26 13:19:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.03031214512884617 norm:0.0048768422566354275 max memory_allocated 29268.39501953125 
[2025-03-26 13:20:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.030204376205801964 norm:0.0048348805867135525 max memory_allocated 29268.39501953125 
[2025-03-26 13:20:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.029713548719882965 norm:0.004378356970846653 max memory_allocated 29268.39501953125 
[2025-03-26 13:21:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.030205052345991135 norm:0.00441835867241025 max memory_allocated 29268.39501953125 
[2025-03-26 13:22:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.029834484681487083 norm:0.004574389196932316 max memory_allocated 29268.39501953125 
[2025-03-26 13:23:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.030156413093209267 norm:0.0043533057905733585 max memory_allocated 29268.39501953125 
[2025-03-26 13:23:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-26 13:24:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.06773686408996582 norm:0.014197653159499168 max memory_allocated 29268.43798828125 
[2025-03-26 13:25:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.05037768930196762 norm:0.004268617369234562 max memory_allocated 29268.43798828125 
[2025-03-26 13:26:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.03975694254040718 norm:0.001415291684679687 max memory_allocated 29268.43798828125 
[2025-03-26 13:27:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.03697263449430466 norm:0.0009150184923782945 max memory_allocated 29268.43798828125 
[2025-03-26 13:27:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.03584299609065056 norm:0.0007835743599571288 max memory_allocated 29268.43798828125 
[2025-03-26 13:28:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.03508954495191574 norm:0.000695300754159689 max memory_allocated 29268.43798828125 
[2025-03-26 13:29:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.03459891676902771 norm:0.0006639185594394803 max memory_allocated 29268.43798828125 
[2025-03-26 13:30:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.034284330904483795 norm:0.0005640971357934177 max memory_allocated 29268.43798828125 
[2025-03-26 13:31:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0340590700507164 norm:0.00048551568761467934 max memory_allocated 29268.43798828125 
[2025-03-26 13:32:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0339379608631134 norm:0.00042380436207167804 max memory_allocated 29268.43798828125 
[2025-03-26 13:32:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.03386884927749634 norm:0.0004141692479606718 max memory_allocated 29268.43798828125 
[2025-03-26 13:33:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.03382926061749458 norm:0.0003915138659067452 max memory_allocated 29268.43798828125 
[2025-03-26 13:34:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.03375363349914551 norm:0.0003549243847373873 max memory_allocated 29268.43798828125 
[2025-03-26 13:35:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.03375888615846634 norm:0.00033322785748168826 max memory_allocated 29268.43798828125 
[2025-03-26 13:36:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.03371881693601608 norm:0.00030833075288683176 max memory_allocated 29268.43798828125 
[2025-03-26 13:37:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.033689700067043304 norm:0.00030679337214678526 max memory_allocated 29268.43798828125 
[2025-03-26 13:37:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.033667661249637604 norm:0.0002636788412928581 max memory_allocated 29268.43798828125 
[2025-03-26 13:38:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.03363754600286484 norm:0.0002790880389511585 max memory_allocated 29268.43798828125 
[2025-03-26 13:39:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.03367641195654869 norm:0.0002762095828074962 max memory_allocated 29268.43798828125 
[2025-03-26 13:40:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.03373456001281738 norm:0.00028359738644212484 max memory_allocated 29268.43798828125 
[2025-03-26 13:40:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-26 13:41:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.07573562860488892 norm:0.018829837441444397 max memory_allocated 29268.43798828125 
[2025-03-26 13:42:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.05809108167886734 norm:0.005787537433207035 max memory_allocated 29268.43798828125 
[2025-03-26 13:43:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.047269996255636215 norm:0.002550066914409399 max memory_allocated 29268.43798828125 
[2025-03-26 13:44:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.04422054439783096 norm:0.0014536416856572032 max memory_allocated 29268.43798828125 
[2025-03-26 13:44:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.04262540489435196 norm:0.0010302491718903184 max memory_allocated 29268.43798828125 
[2025-03-26 13:45:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.04182824864983559 norm:0.000903434120118618 max memory_allocated 29268.43798828125 
[2025-03-26 13:46:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.04144442081451416 norm:0.0008452184265479445 max memory_allocated 29268.43798828125 
[2025-03-26 13:47:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.041089944541454315 norm:0.0007223293650895357 max memory_allocated 29268.43798828125 
[2025-03-26 13:48:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.040894486010074615 norm:0.0006568911485373974 max memory_allocated 29268.43798828125 
[2025-03-26 13:49:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.040800027549266815 norm:0.0006094802520237863 max memory_allocated 29268.43798828125 
[2025-03-26 13:49:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0407322496175766 norm:0.0005541419377550483 max memory_allocated 29268.43798828125 
[2025-03-26 13:50:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0407845601439476 norm:0.0004978800425305963 max memory_allocated 29268.43798828125 
[2025-03-26 13:51:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.040727630257606506 norm:0.00044929751311428845 max memory_allocated 29268.43798828125 
[2025-03-26 13:52:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0406588613986969 norm:0.0004128846921958029 max memory_allocated 29268.43798828125 
[2025-03-26 13:53:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.04065229743719101 norm:0.00040176406037062407 max memory_allocated 29268.43798828125 
[2025-03-26 13:54:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.04058219492435455 norm:0.0003724545822478831 max memory_allocated 29268.43798828125 
[2025-03-26 13:54:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0405590794980526 norm:0.0003632979351095855 max memory_allocated 29268.43798828125 
[2025-03-26 13:55:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.04052793234586716 norm:0.00032988633029162884 max memory_allocated 29268.43798828125 
[2025-03-26 13:56:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.04052403196692467 norm:0.00030102921300567687 max memory_allocated 29268.43798828125 
[2025-03-26 13:57:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.04051174968481064 norm:0.00028968945844098926 max memory_allocated 29268.43798828125 
[2025-03-26 13:57:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-26 13:58:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.09882556647062302 norm:0.02964629977941513 max memory_allocated 29268.81298828125 
[2025-03-26 13:59:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.07322023063898087 norm:0.01042198110371828 max memory_allocated 29268.81298828125 
[2025-03-26 14:00:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.05444863438606262 norm:0.0029075448401272297 max memory_allocated 29268.81298828125 
[2025-03-26 14:01:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.049963656812906265 norm:0.0015171419363468885 max memory_allocated 29268.81298828125 
[2025-03-26 14:01:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.04831033572554588 norm:0.001226387219503522 max memory_allocated 29268.81298828125 
[2025-03-26 14:02:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.04749009758234024 norm:0.0010985053377225995 max memory_allocated 29268.81298828125 
[2025-03-26 14:03:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.046783555299043655 norm:0.000899036880582571 max memory_allocated 29268.81298828125 
[2025-03-26 14:04:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.04638595134019852 norm:0.0008003002149052918 max memory_allocated 29268.81298828125 
[2025-03-26 14:05:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.04599854350090027 norm:0.0007081141811795533 max memory_allocated 29268.81298828125 
[2025-03-26 14:06:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.04577720910310745 norm:0.0006730692693963647 max memory_allocated 29268.81298828125 
[2025-03-26 14:06:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.045632123947143555 norm:0.0006103909108787775 max memory_allocated 29268.81298828125 
[2025-03-26 14:07:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.045538805425167084 norm:0.000589907169342041 max memory_allocated 29268.81298828125 
[2025-03-26 14:08:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0454757884144783 norm:0.0005432264297269285 max memory_allocated 29268.81298828125 
[2025-03-26 14:09:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.045392509549856186 norm:0.0005678971065208316 max memory_allocated 29268.81298828125 
[2025-03-26 14:10:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.04533996433019638 norm:0.0004979528021067381 max memory_allocated 29268.81298828125 
[2025-03-26 14:11:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.045329414308071136 norm:0.00047553027980029583 max memory_allocated 29268.81298828125 
[2025-03-26 14:11:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.04532025381922722 norm:0.0004326911875978112 max memory_allocated 29268.81298828125 
[2025-03-26 14:12:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.04515473544597626 norm:0.00040915413410402834 max memory_allocated 29268.81298828125 
[2025-03-26 14:13:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.045094408094882965 norm:0.00037558170151896775 max memory_allocated 29268.81298828125 
[2025-03-26 14:14:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.04510955139994621 norm:0.00037867913488298655 max memory_allocated 29268.81298828125 
[2025-03-26 14:14:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-26 14:15:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.08993423730134964 norm:0.005872506648302078 max memory_allocated 29269.00048828125 
[2025-03-26 14:16:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.07335920631885529 norm:0.0025536715984344482 max memory_allocated 29269.00048828125 
[2025-03-26 14:17:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.06073710694909096 norm:0.0009805192239582539 max memory_allocated 29269.00048828125 
[2025-03-26 14:18:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0573321208357811 norm:0.0007662153802812099 max memory_allocated 29269.00048828125 
[2025-03-26 14:18:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.055810846388339996 norm:0.0007474403828382492 max memory_allocated 29269.00048828125 
[2025-03-26 14:19:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.05512846261262894 norm:0.0006348444730974734 max memory_allocated 29269.00048828125 
[2025-03-26 14:20:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0547330416738987 norm:0.0005764506058767438 max memory_allocated 29269.00048828125 
[2025-03-26 14:21:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.05423348397016525 norm:0.0005247630761004984 max memory_allocated 29269.00048828125 
[2025-03-26 14:22:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.05403871089220047 norm:0.0005348031409084797 max memory_allocated 29269.00048828125 
[2025-03-26 14:23:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.05386386811733246 norm:0.0004935328033752739 max memory_allocated 29269.00048828125 
[2025-03-26 14:23:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.05375310778617859 norm:0.00045899164979346097 max memory_allocated 29269.00048828125 
[2025-03-26 14:24:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.05364935100078583 norm:0.0004437820171006024 max memory_allocated 29269.00048828125 
[2025-03-26 14:25:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.053594931960105896 norm:0.00046355751692317426 max memory_allocated 29269.00048828125 
[2025-03-26 14:26:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.053598418831825256 norm:0.000440611649537459 max memory_allocated 29269.00048828125 
[2025-03-26 14:27:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.053538884967565536 norm:0.0004392245609778911 max memory_allocated 29269.00048828125 
[2025-03-26 14:28:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.05349063128232956 norm:0.0004963121027685702 max memory_allocated 29269.00048828125 
[2025-03-26 14:28:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.053508609533309937 norm:0.0004696659161709249 max memory_allocated 29269.00048828125 
[2025-03-26 14:29:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.05349971726536751 norm:0.0007261923747137189 max memory_allocated 29269.00048828125 
[2025-03-26 14:30:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.05330611765384674 norm:0.0004446996026672423 max memory_allocated 29269.00048828125 
[2025-03-26 14:31:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.05335782468318939 norm:0.0004348706279415637 max memory_allocated 29269.00048828125 
[2025-03-26 14:31:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-26 14:32:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.09157754480838776 norm:0.005470158997923136 max memory_allocated 29269.18798828125 
[2025-03-26 14:33:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.07738347351551056 norm:0.002842755988240242 max memory_allocated 29269.18798828125 
[2025-03-26 14:34:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.06475792080163956 norm:0.0010158304357901216 max memory_allocated 29269.18798828125 
[2025-03-26 14:35:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.060772091150283813 norm:0.0005057715461589396 max memory_allocated 29269.18798828125 
[2025-03-26 14:35:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.05908927321434021 norm:0.00038872691220603883 max memory_allocated 29269.18798828125 
[2025-03-26 14:36:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.058299094438552856 norm:0.00037845768383704126 max memory_allocated 29269.18798828125 
[2025-03-26 14:37:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.05790559947490692 norm:0.0003716813225764781 max memory_allocated 29269.18798828125 
[2025-03-26 14:38:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.05773890018463135 norm:0.000377621385268867 max memory_allocated 29269.18798828125 
[2025-03-26 14:39:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.057626329362392426 norm:0.00035086029674857855 max memory_allocated 29269.18798828125 
[2025-03-26 14:40:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.05743494629859924 norm:0.0003070133680012077 max memory_allocated 29269.18798828125 
[2025-03-26 14:40:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.05731185898184776 norm:0.0002974196686409414 max memory_allocated 29269.18798828125 
[2025-03-26 14:41:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.05723964422941208 norm:0.0002798670611809939 max memory_allocated 29269.18798828125 
[2025-03-26 14:42:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.057206183671951294 norm:0.0002777298213914037 max memory_allocated 29269.18798828125 
[2025-03-26 14:43:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.05716664344072342 norm:0.0002660943428054452 max memory_allocated 29269.18798828125 
[2025-03-26 14:44:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.05717587471008301 norm:0.0002736138994805515 max memory_allocated 29269.18798828125 
[2025-03-26 14:45:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.057205114513635635 norm:0.0002667021472007036 max memory_allocated 29269.18798828125 
[2025-03-26 14:46:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.05712450295686722 norm:0.0002449550083838403 max memory_allocated 29269.18798828125 
[2025-03-26 14:46:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.057094722986221313 norm:0.00024088259669952095 max memory_allocated 29269.18798828125 
[2025-03-26 14:47:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.057046785950660706 norm:0.00023779566981829703 max memory_allocated 29269.18798828125 
[2025-03-26 14:48:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0570833794772625 norm:0.00023576754028908908 max memory_allocated 29269.18798828125 
[2025-03-26 14:48:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-26 14:49:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.09839874505996704 norm:0.005071990191936493 max memory_allocated 29269.37548828125 
[2025-03-26 14:50:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.08204010128974915 norm:0.002357293851673603 max memory_allocated 29269.37548828125 
[2025-03-26 14:51:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06998181343078613 norm:0.0008975434466265142 max memory_allocated 29269.37548828125 
[2025-03-26 14:52:09 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.06616511195898056 norm:0.000545657763723284 max memory_allocated 29269.37548828125 
[2025-03-26 14:52:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.06434550881385803 norm:0.00040075278957374394 max memory_allocated 29269.37548828125 
[2025-03-26 14:53:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.06336914002895355 norm:0.00037154543679207563 max memory_allocated 29269.37548828125 
[2025-03-26 14:54:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0628574788570404 norm:0.000354351126588881 max memory_allocated 29269.37548828125 
[2025-03-26 14:55:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.062438614666461945 norm:0.0003214823082089424 max memory_allocated 29269.37548828125 
[2025-03-26 14:56:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.06230270117521286 norm:0.00031065903021954 max memory_allocated 29269.37548828125 
[2025-03-26 14:57:09 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.06211010366678238 norm:0.00029255906702019274 max memory_allocated 29269.37548828125 
[2025-03-26 14:57:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.062053948640823364 norm:0.0002758070477284491 max memory_allocated 29269.37548828125 
[2025-03-26 14:58:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.06207309663295746 norm:0.0002758415648713708 max memory_allocated 29269.37548828125 
[2025-03-26 14:59:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.06202821433544159 norm:0.00026402395451441407 max memory_allocated 29269.37548828125 
[2025-03-26 15:00:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.06202482432126999 norm:0.00026190862990915775 max memory_allocated 29269.37548828125 
[2025-03-26 15:01:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.062068045139312744 norm:0.0002582187589723617 max memory_allocated 29269.37548828125 
[2025-03-26 15:02:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.06199944391846657 norm:0.000245263654505834 max memory_allocated 29269.37548828125 
[2025-03-26 15:03:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.062021654099226 norm:0.00025086826644837856 max memory_allocated 29269.37548828125 
[2025-03-26 15:03:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.062078095972537994 norm:0.00024789076996967196 max memory_allocated 29269.37548828125 
[2025-03-26 15:04:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.06213895231485367 norm:0.0002523966250009835 max memory_allocated 29269.37548828125 
[2025-03-26 15:05:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.06218099594116211 norm:0.00025093459407798946 max memory_allocated 29269.37548828125 
[2025-03-26 15:05:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-26 15:06:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.10363024473190308 norm:0.005020216107368469 max memory_allocated 29269.56298828125 
[2025-03-26 15:07:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.08828970789909363 norm:0.0024359161034226418 max memory_allocated 29269.56298828125 
[2025-03-26 15:08:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.07395004481077194 norm:0.0007564632687717676 max memory_allocated 29269.56298828125 
[2025-03-26 15:09:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.07013179361820221 norm:0.00038596868398599327 max memory_allocated 29269.56298828125 
[2025-03-26 15:09:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.06863163411617279 norm:0.0003395059029571712 max memory_allocated 29269.56298828125 
[2025-03-26 15:10:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.06778541207313538 norm:0.00031102538923732936 max memory_allocated 29269.56298828125 
[2025-03-26 15:11:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.06726907193660736 norm:0.00029847753467038274 max memory_allocated 29269.56298828125 
[2025-03-26 15:12:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0669814795255661 norm:0.00028044587816111743 max memory_allocated 29269.56298828125 
[2025-03-26 15:13:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.06677651405334473 norm:0.0002675727300811559 max memory_allocated 29269.56298828125 
[2025-03-26 15:14:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.06670709699392319 norm:0.00026735253049992025 max memory_allocated 29269.56298828125 
[2025-03-26 15:15:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.06671285629272461 norm:0.00027311270241625607 max memory_allocated 29269.56298828125 
[2025-03-26 15:15:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.06671362370252609 norm:0.0002622086904011667 max memory_allocated 29269.56298828125 
[2025-03-26 15:16:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.06669962406158447 norm:0.00025457527954131365 max memory_allocated 29269.56298828125 
[2025-03-26 15:17:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.06664182245731354 norm:0.00024521054001525044 max memory_allocated 29269.56298828125 
[2025-03-26 15:18:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.06664395332336426 norm:0.00023498786322306842 max memory_allocated 29269.56298828125 
[2025-03-26 15:19:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.06660154461860657 norm:0.00022731770877726376 max memory_allocated 29269.56298828125 
[2025-03-26 15:20:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.06660721451044083 norm:0.00022229310707189143 max memory_allocated 29269.56298828125 
[2025-03-26 15:20:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06661462783813477 norm:0.00022199748491402715 max memory_allocated 29269.56298828125 
[2025-03-26 15:21:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.06659918278455734 norm:0.00021670004935003817 max memory_allocated 29269.56298828125 
[2025-03-26 15:22:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.06661181151866913 norm:0.00021851174824405462 max memory_allocated 29269.56298828125 
[2025-03-26 15:22:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-26 15:23:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.10530261695384979 norm:0.0059191640466451645 max memory_allocated 29269.75048828125 
[2025-03-26 15:24:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0905025526881218 norm:0.0026714568957686424 max memory_allocated 29269.75048828125 
[2025-03-26 15:25:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0786476731300354 norm:0.0009680010261945426 max memory_allocated 29269.75048828125 
[2025-03-26 15:26:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.07489128410816193 norm:0.0005417157080955803 max memory_allocated 29269.75048828125 
[2025-03-26 15:27:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.07342322915792465 norm:0.0004298041167203337 max memory_allocated 29269.75048828125 
[2025-03-26 15:27:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.07266703993082047 norm:0.00038865377428010106 max memory_allocated 29269.75048828125 
[2025-03-26 15:28:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.07217735797166824 norm:0.00036116709816269577 max memory_allocated 29269.75048828125 
[2025-03-26 15:29:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.07200506329536438 norm:0.00035234910319559276 max memory_allocated 29269.75048828125 
[2025-03-26 15:30:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.07188332080841064 norm:0.000330527953337878 max memory_allocated 29269.75048828125 
[2025-03-26 15:31:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.07177913188934326 norm:0.0003139665932394564 max memory_allocated 29269.75048828125 
[2025-03-26 15:32:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.07174677401781082 norm:0.0002924799046013504 max memory_allocated 29269.75048828125 
[2025-03-26 15:32:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.07171209901571274 norm:0.00028068918618373573 max memory_allocated 29269.75048828125 
[2025-03-26 15:33:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.07157588005065918 norm:0.00025669182650744915 max memory_allocated 29269.75048828125 
[2025-03-26 15:34:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0716819316148758 norm:0.00027222413336858153 max memory_allocated 29269.75048828125 
[2025-03-26 15:35:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.07179178297519684 norm:0.0002640889724716544 max memory_allocated 29269.75048828125 
[2025-03-26 15:36:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.07177131623029709 norm:0.0002570853685028851 max memory_allocated 29269.75048828125 
[2025-03-26 15:37:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.07190828025341034 norm:0.0002624277549330145 max memory_allocated 29269.75048828125 
[2025-03-26 15:37:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.07192566990852356 norm:0.000252505240496248 max memory_allocated 29269.75048828125 
[2025-03-26 15:38:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.07187873870134354 norm:0.00024178993771784008 max memory_allocated 29269.75048828125 
[2025-03-26 15:39:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.07174498587846756 norm:0.00022829665977042168 max memory_allocated 29269.75048828125 
[2025-03-26 15:39:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-26 15:40:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.11018817126750946 norm:0.0039016525261104107 max memory_allocated 29269.93798828125 
[2025-03-26 15:41:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.09662981331348419 norm:0.0019073807634413242 max memory_allocated 29269.93798828125 
[2025-03-26 15:42:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.08421941846609116 norm:0.0006868956843391061 max memory_allocated 29269.93798828125 
[2025-03-26 15:43:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.08066132664680481 norm:0.0003721165703609586 max memory_allocated 29269.93798828125 
[2025-03-26 15:44:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0793980285525322 norm:0.00032971525797620416 max memory_allocated 29269.93798828125 
[2025-03-26 15:44:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.07862595468759537 norm:0.0002941835846286267 max memory_allocated 29269.93798828125 
[2025-03-26 15:45:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.07816775888204575 norm:0.0002669176901690662 max memory_allocated 29269.93798828125 
[2025-03-26 15:46:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.07791933417320251 norm:0.0002513788058422506 max memory_allocated 29269.93798828125 
[2025-03-26 15:47:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0777900218963623 norm:0.00024539264268241823 max memory_allocated 29269.93798828125 
[2025-03-26 15:48:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.07776222378015518 norm:0.00024297868367284536 max memory_allocated 29269.93798828125 
[2025-03-26 15:49:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.07776745408773422 norm:0.00023924437118694186 max memory_allocated 29269.93798828125 
[2025-03-26 15:49:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.07780162990093231 norm:0.0002388215798418969 max memory_allocated 29269.93798828125 
[2025-03-26 15:50:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.07772749662399292 norm:0.000227269614697434 max memory_allocated 29269.93798828125 
[2025-03-26 15:51:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.07759515196084976 norm:0.0002154507819795981 max memory_allocated 29269.93798828125 
[2025-03-26 15:52:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.07753331959247589 norm:0.00020617512927856296 max memory_allocated 29269.93798828125 
[2025-03-26 15:53:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.07745633274316788 norm:0.0001976846542675048 max memory_allocated 29269.93798828125 
[2025-03-26 15:54:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0774274617433548 norm:0.00019219973182771355 max memory_allocated 29269.93798828125 
[2025-03-26 15:54:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.07745461910963058 norm:0.00019538638298399746 max memory_allocated 29269.93798828125 
[2025-03-26 15:55:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.07742999494075775 norm:0.00018803763668984175 max memory_allocated 29269.93798828125 
[2025-03-26 15:56:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.07744676619768143 norm:0.00018828909378498793 max memory_allocated 29269.93798828125 
[2025-03-26 15:56:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-26 15:57:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.1125374287366867 norm:0.0035242652520537376 max memory_allocated 29270.12548828125 
[2025-03-26 15:58:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.09914398193359375 norm:0.001481015351600945 max memory_allocated 29270.12548828125 
[2025-03-26 15:59:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0890476331114769 norm:0.000656063319183886 max memory_allocated 29270.12548828125 
[2025-03-26 16:00:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.08588874340057373 norm:0.0004192452470306307 max memory_allocated 29270.12548828125 
[2025-03-26 16:01:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.08457756787538528 norm:0.0003264378756284714 max memory_allocated 29270.12548828125 
[2025-03-26 16:01:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.08392364531755447 norm:0.000299744337098673 max memory_allocated 29270.12548828125 
[2025-03-26 16:02:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.08354459702968597 norm:0.00026517146034166217 max memory_allocated 29270.12548828125 
[2025-03-26 16:03:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.08325639367103577 norm:0.00023884832626208663 max memory_allocated 29270.12548828125 
[2025-03-26 16:04:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.08304785192012787 norm:0.00022128243290353566 max memory_allocated 29270.12548828125 
[2025-03-26 16:05:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0829453319311142 norm:0.00021515379194170237 max memory_allocated 29270.12548828125 
[2025-03-26 16:06:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.08288773149251938 norm:0.00021246298274490982 max memory_allocated 29270.12548828125 
[2025-03-26 16:06:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.08294658362865448 norm:0.00021876057144254446 max memory_allocated 29270.12548828125 
[2025-03-26 16:07:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.08295590430498123 norm:0.00020845337712671608 max memory_allocated 29270.12548828125 
[2025-03-26 16:08:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0829710140824318 norm:0.00020703596237581223 max memory_allocated 29270.12548828125 
[2025-03-26 16:09:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.0829189121723175 norm:0.00019980450451839715 max memory_allocated 29270.12548828125 
[2025-03-26 16:10:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.08286953717470169 norm:0.00019678936223499477 max memory_allocated 29270.12548828125 
[2025-03-26 16:11:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.08289402723312378 norm:0.00019575576880015433 max memory_allocated 29270.12548828125 
[2025-03-26 16:11:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.08287893980741501 norm:0.00018885913596022874 max memory_allocated 29270.12548828125 
[2025-03-26 16:12:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0828503668308258 norm:0.0001893372682388872 max memory_allocated 29270.12548828125 
[2025-03-26 16:13:34 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.08282776176929474 norm:0.00018491367518436164 max memory_allocated 29270.12548828125 
[2025-03-26 16:13:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-26 16:14:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.11901798844337463 norm:0.0031963312067091465 max memory_allocated 29270.31298828125 
[2025-03-26 16:15:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.10644469410181046 norm:0.0015565854264423251 max memory_allocated 29270.31298828125 
[2025-03-26 16:16:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.09597545117139816 norm:0.0006580672925338149 max memory_allocated 29270.31298828125 
[2025-03-26 16:17:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.09243354201316833 norm:0.00040940800681710243 max memory_allocated 29270.31298828125 
[2025-03-26 16:18:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.09089577943086624 norm:0.00033399980748072267 max memory_allocated 29270.31298828125 
[2025-03-26 16:18:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.08998659998178482 norm:0.00028251169715076685 max memory_allocated 29270.31298828125 
[2025-03-26 16:19:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.08949940651655197 norm:0.00027467665495350957 max memory_allocated 29270.31298828125 
[2025-03-26 16:20:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.08918416500091553 norm:0.00024942285381257534 max memory_allocated 29270.31298828125 
[2025-03-26 16:21:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.08901571482419968 norm:0.00023978902027010918 max memory_allocated 29270.31298828125 
[2025-03-26 16:22:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.08894123882055283 norm:0.00023527091252617538 max memory_allocated 29270.31298828125 
[2025-03-26 16:23:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.08890151977539062 norm:0.00023792305728420615 max memory_allocated 29270.31298828125 
[2025-03-26 16:23:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.08885759115219116 norm:0.00022818961588200182 max memory_allocated 29270.31298828125 
[2025-03-26 16:24:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.08879189938306808 norm:0.00021775337518192828 max memory_allocated 29270.31298828125 
[2025-03-26 16:25:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.08875882625579834 norm:0.00021460420975927263 max memory_allocated 29270.31298828125 
[2025-03-26 16:26:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.08872926235198975 norm:0.00021080896840430796 max memory_allocated 29270.31298828125 
[2025-03-26 16:27:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.08871262520551682 norm:0.0002026779402513057 max memory_allocated 29270.31298828125 
[2025-03-26 16:28:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.08868936449289322 norm:0.00020064010459464043 max memory_allocated 29270.31298828125 
[2025-03-26 16:28:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0886857807636261 norm:0.000201055096113123 max memory_allocated 29270.31298828125 
[2025-03-26 16:29:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.08875270187854767 norm:0.00020317720191087574 max memory_allocated 29270.31298828125 
[2025-03-26 16:30:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.08877018094062805 norm:0.00020170441712252796 max memory_allocated 29270.31298828125 
[2025-03-26 16:30:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-26 16:31:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.1339172124862671 norm:0.0054751671850681305 max memory_allocated 29270.50048828125 
[2025-03-26 16:32:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.11898094415664673 norm:0.0028198636136949062 max memory_allocated 29270.50048828125 
[2025-03-26 16:33:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.10431159287691116 norm:0.00117275130469352 max memory_allocated 29270.50048828125 
[2025-03-26 16:34:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.09980176389217377 norm:0.0006944859633222222 max memory_allocated 29270.50048828125 
[2025-03-26 16:35:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.09782497584819794 norm:0.0005604164325632155 max memory_allocated 29270.50048828125 
[2025-03-26 16:35:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.09672607481479645 norm:0.00048116815742105246 max memory_allocated 29270.50048828125 
[2025-03-26 16:36:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.09597361087799072 norm:0.0003976586158387363 max memory_allocated 29270.50048828125 
[2025-03-26 16:37:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.09553319215774536 norm:0.00033794197952374816 max memory_allocated 29270.50048828125 
[2025-03-26 16:38:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.09530338644981384 norm:0.00030206734663806856 max memory_allocated 29270.50048828125 
[2025-03-26 16:39:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.09517727792263031 norm:0.0002878289087675512 max memory_allocated 29270.50048828125 
[2025-03-26 16:40:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.09510867297649384 norm:0.00028067853418178856 max memory_allocated 29270.50048828125 
[2025-03-26 16:40:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.09505587816238403 norm:0.00027400910039432347 max memory_allocated 29270.50048828125 
[2025-03-26 16:41:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.09504101425409317 norm:0.0002709749969653785 max memory_allocated 29270.50048828125 
[2025-03-26 16:42:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.09499040246009827 norm:0.00025415437994524837 max memory_allocated 29270.50048828125 
[2025-03-26 16:43:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.09485506266355515 norm:0.00023210383369587362 max memory_allocated 29270.50048828125 
[2025-03-26 16:44:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.09485422819852829 norm:0.0002347484405618161 max memory_allocated 29270.50048828125 
[2025-03-26 16:45:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.09480796009302139 norm:0.00022977434855420142 max memory_allocated 29270.50048828125 
[2025-03-26 16:45:55 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.09472870081663132 norm:0.0002156794653274119 max memory_allocated 29270.50048828125 
[2025-03-26 16:46:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.09471587091684341 norm:0.00021674259915016592 max memory_allocated 29270.50048828125 
[2025-03-26 16:47:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.09470085054636002 norm:0.0002151275984942913 max memory_allocated 29270.50048828125 
[2025-03-26 16:47:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-26 16:48:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.15284942090511322 norm:0.015483839437365532 max memory_allocated 29270.68798828125 
[2025-03-26 16:49:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.13313670456409454 norm:0.006868958938866854 max memory_allocated 29270.68798828125 
[2025-03-26 16:50:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.11684642732143402 norm:0.0028661361429840326 max memory_allocated 29270.68798828125 
[2025-03-26 16:51:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.1101105809211731 norm:0.001076013082638383 max memory_allocated 29270.68798828125 
[2025-03-26 16:52:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.10762330144643784 norm:0.0008083785069175065 max memory_allocated 29270.68798828125 
[2025-03-26 16:52:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.10627230256795883 norm:0.0007114379550330341 max memory_allocated 29270.68798828125 
[2025-03-26 16:53:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.10543784499168396 norm:0.0006283879047259688 max memory_allocated 29270.68798828125 
[2025-03-26 16:54:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.10490308701992035 norm:0.0005570128560066223 max memory_allocated 29270.68798828125 
[2025-03-26 16:55:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.10452508926391602 norm:0.0005160197615623474 max memory_allocated 29270.68798828125 
[2025-03-26 16:56:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.10439024120569229 norm:0.0005292263231240213 max memory_allocated 29270.68798828125 
[2025-03-26 16:57:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.10437542200088501 norm:0.0005088289617560804 max memory_allocated 29270.68798828125 
[2025-03-26 16:57:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.10408858954906464 norm:0.0003947727382183075 max memory_allocated 29270.68798828125 
[2025-03-26 16:58:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.1039317324757576 norm:0.00038066564593464136 max memory_allocated 29270.68798828125 
[2025-03-26 16:59:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.10396343469619751 norm:0.0003853423404507339 max memory_allocated 29270.68798828125 
[2025-03-26 17:00:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.1038474440574646 norm:0.0003518211015034467 max memory_allocated 29270.68798828125 
[2025-03-26 17:01:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.10384760051965714 norm:0.00036717246985062957 max memory_allocated 29270.68798828125 
[2025-03-26 17:02:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.10376668721437454 norm:0.00033960436121560633 max memory_allocated 29270.68798828125 
[2025-03-26 17:02:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.10367435216903687 norm:0.00031902632326819 max memory_allocated 29270.68798828125 
[2025-03-26 17:03:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.10361376404762268 norm:0.0002988850465044379 max memory_allocated 29270.68798828125 
[2025-03-26 17:04:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.10359157621860504 norm:0.00029978694510646164 max memory_allocated 29270.68798828125 
[2025-03-26 17:04:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-26 17:05:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.1441149115562439 norm:0.005829259287565947 max memory_allocated 29270.87548828125 
[2025-03-26 17:06:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.1316712200641632 norm:0.0032664728350937366 max memory_allocated 29270.87548828125 
[2025-03-26 17:07:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.11917074769735336 norm:0.0011636579874902964 max memory_allocated 29270.87548828125 
[2025-03-26 17:08:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.1152135357260704 norm:0.0005260645993985236 max memory_allocated 29270.87548828125 
[2025-03-26 17:09:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.11347531527280807 norm:0.00040232494939118624 max memory_allocated 29270.87548828125 
[2025-03-26 17:09:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.11252975463867188 norm:0.00035131449112668633 max memory_allocated 29270.87548828125 
[2025-03-26 17:10:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.111972376704216 norm:0.0003098305023740977 max memory_allocated 29270.87548828125 
[2025-03-26 17:11:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.1115620955824852 norm:0.0002677521842997521 max memory_allocated 29270.87548828125 
[2025-03-26 17:12:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.11145781725645065 norm:0.00027730665169656277 max memory_allocated 29270.87548828125 
[2025-03-26 17:13:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.11143386363983154 norm:0.0002931212948169559 max memory_allocated 29270.87548828125 
[2025-03-26 17:14:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.11131416261196136 norm:0.0002549914352130145 max memory_allocated 29270.87548828125 
[2025-03-26 17:14:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.11130028963088989 norm:0.00026128889294341207 max memory_allocated 29270.87548828125 
[2025-03-26 17:15:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.11129257082939148 norm:0.00026011193403974175 max memory_allocated 29270.87548828125 
[2025-03-26 17:16:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.11117283254861832 norm:0.00024162601039279252 max memory_allocated 29270.87548828125 
[2025-03-26 17:17:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.11107733100652695 norm:0.00022812833776697516 max memory_allocated 29270.87548828125 
[2025-03-26 17:18:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.11107476055622101 norm:0.0002281148626934737 max memory_allocated 29270.87548828125 
[2025-03-26 17:19:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.11110535264015198 norm:0.00022809994698036462 max memory_allocated 29270.87548828125 
[2025-03-26 17:19:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.11106488108634949 norm:0.00022016749426256865 max memory_allocated 29270.87548828125 
[2025-03-26 17:20:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.11099112033843994 norm:0.0002145000617019832 max memory_allocated 29270.87548828125 
[2025-03-26 17:21:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.1109427958726883 norm:0.00020876443886663765 max memory_allocated 29270.87548828125 
[2025-03-26 17:21:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-26 17:22:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.15137521922588348 norm:0.004163796082139015 max memory_allocated 29271.06298828125 
[2025-03-26 17:23:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.14036740362644196 norm:0.002434393623843789 max memory_allocated 29271.06298828125 
[2025-03-26 17:24:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.12841486930847168 norm:0.0008996159886009991 max memory_allocated 29271.06298828125 
[2025-03-26 17:25:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.1246066465973854 norm:0.00047051176079548895 max memory_allocated 29271.06298828125 
[2025-03-26 17:26:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.12306766957044601 norm:0.0004097526252735406 max memory_allocated 29271.06298828125 
[2025-03-26 17:26:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.12219774723052979 norm:0.00037211162270978093 max memory_allocated 29271.06298828125 
[2025-03-26 17:27:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.12163569778203964 norm:0.0003098052111454308 max memory_allocated 29271.06298828125 
[2025-03-26 17:28:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.12140213698148727 norm:0.0002996370894834399 max memory_allocated 29271.06298828125 
[2025-03-26 17:29:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.12131048738956451 norm:0.00029487465508282185 max memory_allocated 29271.06298828125 
[2025-03-26 17:30:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.12118639796972275 norm:0.00028233390185050666 max memory_allocated 29271.06298828125 
[2025-03-26 17:31:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.12104640901088715 norm:0.00026011219597421587 max memory_allocated 29271.06298828125 
[2025-03-26 17:31:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.12086869776248932 norm:0.00023523849085904658 max memory_allocated 29271.06298828125 
[2025-03-26 17:32:47 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.12078122794628143 norm:0.00022750059724785388 max memory_allocated 29271.06298828125 
[2025-03-26 17:33:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.12075500190258026 norm:0.00022242713021114469 max memory_allocated 29271.06298828125 
[2025-03-26 17:34:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.12076607346534729 norm:0.0002227331860922277 max memory_allocated 29271.06298828125 
[2025-03-26 17:35:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.12077488005161285 norm:0.00022052746498957276 max memory_allocated 29271.06298828125 
[2025-03-26 17:36:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.12069594860076904 norm:0.00021142938931006938 max memory_allocated 29271.06298828125 
[2025-03-26 17:36:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.12063819169998169 norm:0.00020698529260698706 max memory_allocated 29271.06298828125 
[2025-03-26 17:37:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.12057788670063019 norm:0.00020118473912589252 max memory_allocated 29271.06298828125 
[2025-03-26 17:38:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.12053868174552917 norm:0.00019987211271654814 max memory_allocated 29271.06298828125 
[2025-03-26 17:38:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-26 17:39:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.1776534616947174 norm:0.009371207095682621 max memory_allocated 29271.25048828125 
[2025-03-26 17:40:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.16111509501934052 norm:0.004706396721303463 max memory_allocated 29271.25048828125 
[2025-03-26 17:41:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.14501261711120605 norm:0.0018236904870718718 max memory_allocated 29271.25048828125 
[2025-03-26 17:42:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.13993693888187408 norm:0.000998924719169736 max memory_allocated 29271.25048828125 
[2025-03-26 17:43:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.13783833384513855 norm:0.0008247699588537216 max memory_allocated 29271.25048828125 
[2025-03-26 17:43:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.13670912384986877 norm:0.0007045823731459677 max memory_allocated 29271.25048828125 
[2025-03-26 17:44:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.13606205582618713 norm:0.0006098666926845908 max memory_allocated 29271.25048828125 
[2025-03-26 17:45:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.13562023639678955 norm:0.0005048484308645129 max memory_allocated 29271.25048828125 
[2025-03-26 17:46:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.13529305160045624 norm:0.0004090667935088277 max memory_allocated 29271.25048828125 
[2025-03-26 17:47:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.13505807518959045 norm:0.00033883863943628967 max memory_allocated 29271.25048828125 
[2025-03-26 17:48:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.1349101960659027 norm:0.000326737470459193 max memory_allocated 29271.25048828125 
[2025-03-26 17:48:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.1348249614238739 norm:0.0003244312247261405 max memory_allocated 29271.25048828125 
[2025-03-26 17:49:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.1347832977771759 norm:0.0003133337595500052 max memory_allocated 29271.25048828125 
[2025-03-26 17:50:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.13469824194908142 norm:0.00029417520272545516 max memory_allocated 29271.25048828125 
[2025-03-26 17:51:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.13461297750473022 norm:0.00028691740590147674 max memory_allocated 29271.25048828125 
[2025-03-26 17:52:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.13457676768302917 norm:0.00028253733762539923 max memory_allocated 29271.25048828125 
[2025-03-26 17:53:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.13453249633312225 norm:0.0002752433065325022 max memory_allocated 29271.25048828125 
[2025-03-26 17:53:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.13445840775966644 norm:0.00026060669915750623 max memory_allocated 29271.25048828125 
[2025-03-26 17:54:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.13440856337547302 norm:0.0002559247659519315 max memory_allocated 29271.25048828125 
[2025-03-26 17:55:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.13435040414333344 norm:0.0002521737478673458 max memory_allocated 29271.25048828125 
[2025-03-26 17:55:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-26 17:56:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.1832941174507141 norm:0.005067423451691866 max memory_allocated 29271.43798828125 
[2025-03-26 17:57:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.17090485990047455 norm:0.0027473680675029755 max memory_allocated 29271.43798828125 
[2025-03-26 17:58:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.15877549350261688 norm:0.0011011278256773949 max memory_allocated 29271.43798828125 
[2025-03-26 17:59:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.1545853465795517 norm:0.0005455851787701249 max memory_allocated 29271.43798828125 
[2025-03-26 18:00:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.1526576727628708 norm:0.00039827899308875203 max memory_allocated 29271.43798828125 
[2025-03-26 18:00:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.15185685455799103 norm:0.00038121090619824827 max memory_allocated 29271.43798828125 
[2025-03-26 18:01:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.15152007341384888 norm:0.0003710763994604349 max memory_allocated 29271.43798828125 
[2025-03-26 18:02:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.15129391849040985 norm:0.00034601721563376486 max memory_allocated 29271.43798828125 
[2025-03-26 18:03:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.15102671086788177 norm:0.0003098959568887949 max memory_allocated 29271.43798828125 
[2025-03-26 18:04:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.15089645981788635 norm:0.00030224118381738663 max memory_allocated 29271.43798828125 
[2025-03-26 18:05:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.1507970094680786 norm:0.00029019638895988464 max memory_allocated 29271.43798828125 
[2025-03-26 18:05:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.1507360190153122 norm:0.0002775992907118052 max memory_allocated 29271.43798828125 
[2025-03-26 18:06:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.15064147114753723 norm:0.00027567544020712376 max memory_allocated 29271.43798828125 
[2025-03-26 18:07:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.1505611538887024 norm:0.00025748752523213625 max memory_allocated 29271.43798828125 
[2025-03-26 18:08:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.1504746675491333 norm:0.0002524434821680188 max memory_allocated 29271.43798828125 
[2025-03-26 18:09:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.15041083097457886 norm:0.00023630543728359044 max memory_allocated 29271.43798828125 
[2025-03-26 18:10:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.15035270154476166 norm:0.00023498974042013288 max memory_allocated 29271.43798828125 
[2025-03-26 18:10:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.1503603756427765 norm:0.00023674324620515108 max memory_allocated 29271.43798828125 
[2025-03-26 18:11:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.15030573308467865 norm:0.00022916353191249073 max memory_allocated 29271.43798828125 
[2025-03-26 18:12:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.15028738975524902 norm:0.00023113616043701768 max memory_allocated 29271.43798828125 
[2025-03-26 18:12:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-26 18:13:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.20741575956344604 norm:0.0071775964461266994 max memory_allocated 29271.62548828125 
[2025-03-26 18:14:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.1915881335735321 norm:0.003909643739461899 max memory_allocated 29271.62548828125 
[2025-03-26 18:15:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.17708386480808258 norm:0.0018107980722561479 max memory_allocated 29271.62548828125 
[2025-03-26 18:16:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.1720484346151352 norm:0.0009875097312033176 max memory_allocated 29271.62548828125 
[2025-03-26 18:17:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.16999906301498413 norm:0.0007645564619451761 max memory_allocated 29271.62548828125 
[2025-03-26 18:17:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.1687801629304886 norm:0.0005397432250902057 max memory_allocated 29271.62548828125 
[2025-03-26 18:18:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.16822218894958496 norm:0.0004070524300914258 max memory_allocated 29271.62548828125 
[2025-03-26 18:19:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.16795483231544495 norm:0.0003887042694259435 max memory_allocated 29271.62548828125 
[2025-03-26 18:20:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.1677366942167282 norm:0.000363275408744812 max memory_allocated 29271.62548828125 
[2025-03-26 18:21:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.16753503680229187 norm:0.00033622977207414806 max memory_allocated 29271.62548828125 
[2025-03-26 18:22:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.16738095879554749 norm:0.00031704443972557783 max memory_allocated 29271.62548828125 
[2025-03-26 18:22:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.16730734705924988 norm:0.00031156890327110887 max memory_allocated 29271.62548828125 
[2025-03-26 18:23:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.1672232747077942 norm:0.00030150022939778864 max memory_allocated 29271.62548828125 
[2025-03-26 18:24:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.16711361706256866 norm:0.000286812981357798 max memory_allocated 29271.62548828125 
[2025-03-26 18:25:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.1670355498790741 norm:0.0002813345054164529 max memory_allocated 29271.62548828125 
[2025-03-26 18:26:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.16699057817459106 norm:0.0002760728821158409 max memory_allocated 29271.62548828125 
[2025-03-26 18:27:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.16688138246536255 norm:0.00025883459602482617 max memory_allocated 29271.62548828125 
[2025-03-26 18:28:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.1668165773153305 norm:0.00024937576381489635 max memory_allocated 29271.62548828125 
[2025-03-26 18:28:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.1667512208223343 norm:0.00024263311934191734 max memory_allocated 29271.62548828125 
[2025-03-26 18:29:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.16673880815505981 norm:0.0002424243721179664 max memory_allocated 29271.62548828125 
[2025-03-26 18:29:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-26 18:30:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.22595159709453583 norm:0.005884752608835697 max memory_allocated 29271.81298828125 
[2025-03-26 18:31:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.2116372287273407 norm:0.003254072042182088 max memory_allocated 29271.81298828125 
[2025-03-26 18:32:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.19800888001918793 norm:0.0013693240471184254 max memory_allocated 29271.81298828125 
[2025-03-26 18:33:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.19348038733005524 norm:0.0006690735463052988 max memory_allocated 29271.81298828125 
[2025-03-26 18:34:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.19170048832893372 norm:0.0005834283656440675 max memory_allocated 29271.81298828125 
[2025-03-26 18:34:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.1909150332212448 norm:0.0005168355419300497 max memory_allocated 29271.81298828125 
[2025-03-26 18:35:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.19055460393428802 norm:0.0004779283481184393 max memory_allocated 29271.81298828125 
[2025-03-26 18:36:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.19023877382278442 norm:0.0004268308402970433 max memory_allocated 29271.81298828125 
[2025-03-26 18:37:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.19003257155418396 norm:0.00040876385173760355 max memory_allocated 29271.81298828125 
[2025-03-26 18:38:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.18986772000789642 norm:0.00036872929194942117 max memory_allocated 29271.81298828125 
[2025-03-26 18:39:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.1897720992565155 norm:0.0003609064151532948 max memory_allocated 29271.81298828125 
[2025-03-26 18:40:00 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.1895470917224884 norm:0.00031291425693780184 max memory_allocated 29271.81298828125 
[2025-03-26 18:40:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.18945792317390442 norm:0.0003066123172175139 max memory_allocated 29271.81298828125 
[2025-03-26 18:41:40 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.18943262100219727 norm:0.00030191687983460724 max memory_allocated 29271.81298828125 
[2025-03-26 18:42:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.18932853639125824 norm:0.0002748807892203331 max memory_allocated 29271.81298828125 
[2025-03-26 18:43:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.18921034038066864 norm:0.000264100031927228 max memory_allocated 29271.81298828125 
[2025-03-26 18:44:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.18915915489196777 norm:0.00026219768915325403 max memory_allocated 29271.81298828125 
[2025-03-26 18:45:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.18918175995349884 norm:0.0002618820290081203 max memory_allocated 29271.81298828125 
[2025-03-26 18:45:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.18914617598056793 norm:0.00025065019144676626 max memory_allocated 29271.81298828125 
[2025-03-26 18:46:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.189121276140213 norm:0.00024444732116535306 max memory_allocated 29271.81298828125 
[2025-03-26 18:46:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-26 18:47:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.25401657819747925 norm:0.0046705035492777824 max memory_allocated 29272.00048828125 
[2025-03-26 18:48:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.23871193826198578 norm:0.0024679875932633877 max memory_allocated 29272.00048828125 
[2025-03-26 18:49:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.2245478332042694 norm:0.001077893772162497 max memory_allocated 29272.00048828125 
[2025-03-26 18:50:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.22022201120853424 norm:0.0005606784834526479 max memory_allocated 29272.00048828125 
[2025-03-26 18:51:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.2183384746313095 norm:0.00046625721734017134 max memory_allocated 29272.00048828125 
[2025-03-26 18:52:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.2175736427307129 norm:0.0004378778103273362 max memory_allocated 29272.00048828125 
[2025-03-26 18:52:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.2172079086303711 norm:0.0004111412854399532 max memory_allocated 29272.00048828125 
[2025-03-26 18:53:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.21691305935382843 norm:0.00037989328848198056 max memory_allocated 29272.00048828125 
[2025-03-26 18:54:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.21665985882282257 norm:0.00035186996683478355 max memory_allocated 29272.00048828125 
[2025-03-26 18:55:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.2165062129497528 norm:0.00034428443177603185 max memory_allocated 29272.00048828125 
[2025-03-26 18:56:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.21641722321510315 norm:0.0003329628671053797 max memory_allocated 29272.00048828125 
[2025-03-26 18:57:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.21626904606819153 norm:0.0003166019741911441 max memory_allocated 29272.00048828125 
[2025-03-26 18:57:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.21616046130657196 norm:0.00030139301088638604 max memory_allocated 29272.00048828125 
[2025-03-26 18:58:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.21610596776008606 norm:0.00029766507213935256 max memory_allocated 29272.00048828125 
[2025-03-26 18:59:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.21599115431308746 norm:0.00028241699328646064 max memory_allocated 29272.00048828125 
[2025-03-26 19:00:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.21588274836540222 norm:0.0002762064686976373 max memory_allocated 29272.00048828125 
[2025-03-26 19:01:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.21585872769355774 norm:0.00027614436112344265 max memory_allocated 29272.00048828125 
[2025-03-26 19:02:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.2158324122428894 norm:0.0002701242046896368 max memory_allocated 29272.00048828125 
[2025-03-26 19:02:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.21577520668506622 norm:0.00026323169004172087 max memory_allocated 29272.00048828125 
[2025-03-26 19:03:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.21568259596824646 norm:0.00025490790721960366 max memory_allocated 29272.00048828125 
[2025-03-26 19:03:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-26 19:04:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.28780287504196167 norm:0.004927324131131172 max memory_allocated 29272.18798828125 
[2025-03-26 19:05:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.2723289728164673 norm:0.0028147578705102205 max memory_allocated 29272.18798828125 
[2025-03-26 19:06:30 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.2578456401824951 norm:0.0014181934529915452 max memory_allocated 29272.18798828125 
[2025-03-26 19:07:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.2531169056892395 norm:0.0007098733331076801 max memory_allocated 29272.18798828125 
[2025-03-26 19:08:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.2511364221572876 norm:0.0005732437130063772 max memory_allocated 29272.18798828125 
[2025-03-26 19:09:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.2503422796726227 norm:0.0005270656547509134 max memory_allocated 29272.18798828125 
[2025-03-26 19:09:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.24993552267551422 norm:0.0004987470456399024 max memory_allocated 29272.18798828125 
[2025-03-26 19:10:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.24959123134613037 norm:0.00045282745850272477 max memory_allocated 29272.18798828125 
[2025-03-26 19:11:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.249271959066391 norm:0.0004144287668168545 max memory_allocated 29272.18798828125 
[2025-03-26 19:12:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.24909460544586182 norm:0.00040538574103266 max memory_allocated 29272.18798828125 
[2025-03-26 19:13:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.24892590939998627 norm:0.0003858883574139327 max memory_allocated 29272.18798828125 
[2025-03-26 19:14:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.24872413277626038 norm:0.00036168465157970786 max memory_allocated 29272.18798828125 
[2025-03-26 19:14:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.2486049383878708 norm:0.00034646823769435287 max memory_allocated 29272.18798828125 
[2025-03-26 19:15:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.24850697815418243 norm:0.00033478395198471844 max memory_allocated 29272.18798828125 
[2025-03-26 19:16:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.24843865633010864 norm:0.0003208141424693167 max memory_allocated 29272.18798828125 
[2025-03-26 19:17:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.2483682632446289 norm:0.00030910546774975955 max memory_allocated 29272.18798828125 
[2025-03-26 19:18:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.2482980489730835 norm:0.00030333586619235575 max memory_allocated 29272.18798828125 
[2025-03-26 19:19:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.24821262061595917 norm:0.0002964856685139239 max memory_allocated 29272.18798828125 
[2025-03-26 19:19:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.24818603694438934 norm:0.0002874854835681617 max memory_allocated 29272.18798828125 
[2025-03-26 19:20:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.24808475375175476 norm:0.00028176826890558004 max memory_allocated 29272.18798828125 
[2025-03-26 19:20:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-26 19:21:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.3134453296661377 norm:0.0034323842264711857 max memory_allocated 29272.37548828125 
[2025-03-26 19:22:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.30004996061325073 norm:0.001994671067222953 max memory_allocated 29272.37548828125 
[2025-03-26 19:23:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.28753843903541565 norm:0.0011162698501721025 max memory_allocated 29272.37548828125 
[2025-03-26 19:24:21 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.28321823477745056 norm:0.0006586736417375505 max memory_allocated 29272.37548828125 
[2025-03-26 19:25:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.2814122438430786 norm:0.0005543380975723267 max memory_allocated 29272.37548828125 
[2025-03-26 19:26:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.2807469666004181 norm:0.00045559590216726065 max memory_allocated 29272.37548828125 
[2025-03-26 19:26:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.2803129255771637 norm:0.000398555479478091 max memory_allocated 29272.37548828125 
[2025-03-26 19:27:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.2799740433692932 norm:0.00035800671321339905 max memory_allocated 29272.37548828125 
[2025-03-26 19:28:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.2796934247016907 norm:0.00030885220621712506 max memory_allocated 29272.37548828125 
[2025-03-26 19:29:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.27956342697143555 norm:0.0002959545236080885 max memory_allocated 29272.37548828125 
[2025-03-26 19:30:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.2795267701148987 norm:0.0002925352018792182 max memory_allocated 29272.37548828125 
[2025-03-26 19:31:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.2794247567653656 norm:0.00027979176957160234 max memory_allocated 29272.37548828125 
[2025-03-26 19:31:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.27931031584739685 norm:0.0002748075348790735 max memory_allocated 29272.37548828125 
[2025-03-26 19:32:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.2792457044124603 norm:0.0002679045719560236 max memory_allocated 29272.37548828125 
[2025-03-26 19:33:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.27925989031791687 norm:0.00026627740589901805 max memory_allocated 29272.37548828125 
[2025-03-26 19:34:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.27912792563438416 norm:0.0002552512160036713 max memory_allocated 29272.37548828125 
[2025-03-26 19:35:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.27903082966804504 norm:0.00024801515974104404 max memory_allocated 29272.37548828125 
[2025-03-26 19:36:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.27899253368377686 norm:0.00024246569955721498 max memory_allocated 29272.37548828125 
[2025-03-26 19:36:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.2789418697357178 norm:0.00023911308380775154 max memory_allocated 29272.37548828125 
[2025-03-26 19:37:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.2789120674133301 norm:0.00023769823019392788 max memory_allocated 29272.37548828125 
[2025-03-26 19:37:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-26 19:38:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.35833626985549927 norm:0.006303497590124607 max memory_allocated 29272.56298828125 
[2025-03-26 19:39:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.34116506576538086 norm:0.003456642385572195 max memory_allocated 29272.56298828125 
[2025-03-26 19:40:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.3245461881160736 norm:0.0016289702616631985 max memory_allocated 29272.56298828125 
[2025-03-26 19:41:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.31938180327415466 norm:0.0008742787758819759 max memory_allocated 29272.56298828125 
[2025-03-26 19:42:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.317084938287735 norm:0.0004697625699918717 max memory_allocated 29272.56298828125 
[2025-03-26 19:43:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.31633490324020386 norm:0.0004657043609768152 max memory_allocated 29272.56298828125 
[2025-03-26 19:43:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.31595709919929504 norm:0.00044948450522497296 max memory_allocated 29272.56298828125 
[2025-03-26 19:44:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.31572505831718445 norm:0.0004205138538964093 max memory_allocated 29272.56298828125 
[2025-03-26 19:45:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.31552648544311523 norm:0.00039395250496454537 max memory_allocated 29272.56298828125 
[2025-03-26 19:46:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.3153267204761505 norm:0.00034734781365841627 max memory_allocated 29272.56298828125 
[2025-03-26 19:47:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.3151710629463196 norm:0.00034668506123125553 max memory_allocated 29272.56298828125 
[2025-03-26 19:48:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.31504306197166443 norm:0.0003437254927121103 max memory_allocated 29272.56298828125 
[2025-03-26 19:48:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.31493255496025085 norm:0.0003323331184219569 max memory_allocated 29272.56298828125 
[2025-03-26 19:49:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.31483733654022217 norm:0.00031360003049485385 max memory_allocated 29272.56298828125 
[2025-03-26 19:50:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.3147924542427063 norm:0.0003209617570973933 max memory_allocated 29272.56298828125 
[2025-03-26 19:51:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.3146952986717224 norm:0.0003019314899574965 max memory_allocated 29272.56298828125 
[2025-03-26 19:52:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.3146240711212158 norm:0.00029695985722355545 max memory_allocated 29272.56298828125 
[2025-03-26 19:53:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.3145884871482849 norm:0.00028911413392052054 max memory_allocated 29272.56298828125 
[2025-03-26 19:53:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.3145652711391449 norm:0.00028729013865813613 max memory_allocated 29272.56298828125 
[2025-03-26 19:54:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.31451618671417236 norm:0.0002814617473632097 max memory_allocated 29272.56298828125 
[2025-03-26 19:54:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-26 19:55:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.38836222887039185 norm:0.0031802388839423656 max memory_allocated 29272.75048828125 
[2025-03-26 19:56:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.37341970205307007 norm:0.0018076514825224876 max memory_allocated 29272.75048828125 
[2025-03-26 19:57:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.35842183232307434 norm:0.000871696975082159 max memory_allocated 29272.75048828125 
[2025-03-26 19:58:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.35395753383636475 norm:0.0005164812901057303 max memory_allocated 29272.75048828125 
[2025-03-26 19:59:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.35236871242523193 norm:0.0004297509731259197 max memory_allocated 29272.75048828125 
[2025-03-26 20:00:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.3518535792827606 norm:0.0003888100618496537 max memory_allocated 29272.75048828125 
[2025-03-26 20:00:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.3515058755874634 norm:0.0003542849444784224 max memory_allocated 29272.75048828125 
[2025-03-26 20:01:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.35123157501220703 norm:0.0003263890102971345 max memory_allocated 29272.75048828125 
[2025-03-26 20:02:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.35103803873062134 norm:0.0003057930734939873 max memory_allocated 29272.75048828125 
[2025-03-26 20:03:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.3508518934249878 norm:0.0002872873446904123 max memory_allocated 29272.75048828125 
[2025-03-26 20:04:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.350722074508667 norm:0.0002763482043519616 max memory_allocated 29272.75048828125 
[2025-03-26 20:05:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.35060563683509827 norm:0.00026728800730779767 max memory_allocated 29272.75048828125 
[2025-03-26 20:05:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.35049745440483093 norm:0.00025665771681815386 max memory_allocated 29272.75048828125 
[2025-03-26 20:06:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.350401908159256 norm:0.00024889345513656735 max memory_allocated 29272.75048828125 
[2025-03-26 20:07:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.3503303527832031 norm:0.00024454452795907855 max memory_allocated 29272.75048828125 
[2025-03-26 20:08:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.3502640128135681 norm:0.0002390687441220507 max memory_allocated 29272.75048828125 
[2025-03-26 20:09:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.3502129912376404 norm:0.00023271939426194876 max memory_allocated 29272.75048828125 
[2025-03-26 20:10:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.35016492009162903 norm:0.00022865168284624815 max memory_allocated 29272.75048828125 
[2025-03-26 20:10:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.35009831190109253 norm:0.0002252861304441467 max memory_allocated 29272.75048828125 
[2025-03-26 20:11:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.35003677010536194 norm:0.00022294168593361974 max memory_allocated 29272.75048828125 
[2025-03-26 20:11:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-26 20:12:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.43254512548446655 norm:0.00225795223377645 max memory_allocated 29272.93798828125 
[2025-03-26 20:13:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.4170980155467987 norm:0.0014962940476834774 max memory_allocated 29272.93798828125 
[2025-03-26 20:14:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.4010773003101349 norm:0.0008280885522253811 max memory_allocated 29272.93798828125 
[2025-03-26 20:15:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.39625927805900574 norm:0.00047668194747529924 max memory_allocated 29272.93798828125 
[2025-03-26 20:16:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.3944578766822815 norm:0.0004054008168168366 max memory_allocated 29272.93798828125 
[2025-03-26 20:17:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.3938322961330414 norm:0.0003609111299738288 max memory_allocated 29272.93798828125 
[2025-03-26 20:17:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.39349472522735596 norm:0.0003405101306270808 max memory_allocated 29272.93798828125 
[2025-03-26 20:18:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.393237829208374 norm:0.00032476053456775844 max memory_allocated 29272.93798828125 
[2025-03-26 20:19:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.3930356502532959 norm:0.0003062596661038697 max memory_allocated 29272.93798828125 
[2025-03-26 20:20:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.39285755157470703 norm:0.00029659466235898435 max memory_allocated 29272.93798828125 
[2025-03-26 20:21:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.3927236497402191 norm:0.0002844051050487906 max memory_allocated 29272.93798828125 
[2025-03-26 20:22:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.3925844132900238 norm:0.00027455613599158823 max memory_allocated 29272.93798828125 
[2025-03-26 20:22:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.392482191324234 norm:0.00026731533580459654 max memory_allocated 29272.93798828125 
[2025-03-26 20:23:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.39237308502197266 norm:0.0002588456845842302 max memory_allocated 29272.93798828125 
[2025-03-26 20:24:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.39227670431137085 norm:0.00025286158779636025 max memory_allocated 29272.93798828125 
[2025-03-26 20:25:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.39220374822616577 norm:0.00024555035633966327 max memory_allocated 29272.93798828125 
[2025-03-26 20:26:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.39211803674697876 norm:0.00024233708973042667 max memory_allocated 29272.93798828125 
[2025-03-26 20:27:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.3920450210571289 norm:0.00023976438387762755 max memory_allocated 29272.93798828125 
[2025-03-26 20:27:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.39196711778640747 norm:0.0002365725376876071 max memory_allocated 29272.93798828125 
[2025-03-26 20:28:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.39193862676620483 norm:0.00023639941355213523 max memory_allocated 29272.93798828125 
[2025-03-26 20:29:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-26 20:29:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.4845949411392212 norm:0.007611233275383711 max memory_allocated 29273.12548828125 
[2025-03-26 20:30:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.46402856707572937 norm:0.00400138646364212 max memory_allocated 29273.12548828125 
[2025-03-26 20:31:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.44565215706825256 norm:0.002184485550969839 max memory_allocated 29273.12548828125 
[2025-03-26 20:32:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.4401327073574066 norm:0.0012893684906885028 max memory_allocated 29273.12548828125 
[2025-03-26 20:33:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.4381961226463318 norm:0.0009351255721412599 max memory_allocated 29273.12548828125 
[2025-03-26 20:34:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.4374566972255707 norm:0.0008397914352826774 max memory_allocated 29273.12548828125 
[2025-03-26 20:34:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.43693792819976807 norm:0.0006969103123992682 max memory_allocated 29273.12548828125 
[2025-03-26 20:35:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.43649110198020935 norm:0.0005661023315042257 max memory_allocated 29273.12548828125 
[2025-03-26 20:36:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.4361812472343445 norm:0.0005171624361537397 max memory_allocated 29273.12548828125 
[2025-03-26 20:37:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.4359781742095947 norm:0.0004897292237728834 max memory_allocated 29273.12548828125 
[2025-03-26 20:38:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.43583837151527405 norm:0.000497329281643033 max memory_allocated 29273.12548828125 
[2025-03-26 20:39:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.43576234579086304 norm:0.000489081721752882 max memory_allocated 29273.12548828125 
[2025-03-26 20:39:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.4356347918510437 norm:0.00047447564429603517 max memory_allocated 29273.12548828125 
[2025-03-26 20:40:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.4355148673057556 norm:0.0004279774730093777 max memory_allocated 29273.12548828125 
[2025-03-26 20:41:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.4353888928890228 norm:0.000410514505347237 max memory_allocated 29273.12548828125 
[2025-03-26 20:42:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.4352867305278778 norm:0.00040598184568807483 max memory_allocated 29273.12548828125 
[2025-03-26 20:43:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.43517857789993286 norm:0.0003902043681591749 max memory_allocated 29273.12548828125 
[2025-03-26 20:44:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.435059517621994 norm:0.00038985986611805856 max memory_allocated 29273.12548828125 
[2025-03-26 20:44:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.4350258409976959 norm:0.0003737275255843997 max memory_allocated 29273.12548828125 
[2025-03-26 20:45:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.4350256323814392 norm:0.00037917698500677943 max memory_allocated 29273.12548828125 
[2025-03-26 20:46:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-26 20:46:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.5196154713630676 norm:0.0032037943601608276 max memory_allocated 29273.31298828125 
[2025-03-26 20:47:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.5033323764801025 norm:0.002028956077992916 max memory_allocated 29273.31298828125 
[2025-03-26 20:48:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.4858582615852356 norm:0.0010236017405986786 max memory_allocated 29273.31298828125 
[2025-03-26 20:49:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.4806171655654907 norm:0.0005499667022377253 max memory_allocated 29273.31298828125 
[2025-03-26 20:50:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.4788091778755188 norm:0.00037895815330557525 max memory_allocated 29273.31298828125 
[2025-03-26 20:51:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.4782455265522003 norm:0.0003676055639516562 max memory_allocated 29273.31298828125 
[2025-03-26 20:51:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.4779418706893921 norm:0.0003495478304103017 max memory_allocated 29273.31298828125 
[2025-03-26 20:52:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.4777522683143616 norm:0.0003296373470220715 max memory_allocated 29273.31298828125 
[2025-03-26 20:53:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.47757378220558167 norm:0.00030762175447307527 max memory_allocated 29273.31298828125 
[2025-03-26 20:54:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.4774208664894104 norm:0.0002919183752965182 max memory_allocated 29273.31298828125 
[2025-03-26 20:55:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.47727876901626587 norm:0.00027269311249256134 max memory_allocated 29273.31298828125 
[2025-03-26 20:56:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.47716671228408813 norm:0.00026546220760792494 max memory_allocated 29273.31298828125 
[2025-03-26 20:56:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.4770682156085968 norm:0.00025315710809081793 max memory_allocated 29273.31298828125 
[2025-03-26 20:57:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.476948082447052 norm:0.0002448006998747587 max memory_allocated 29273.31298828125 
[2025-03-26 20:58:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.47687479853630066 norm:0.000241466099396348 max memory_allocated 29273.31298828125 
[2025-03-26 20:59:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.47681835293769836 norm:0.00023951503681018949 max memory_allocated 29273.31298828125 
[2025-03-26 21:00:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.4767574369907379 norm:0.00023617735132575035 max memory_allocated 29273.31298828125 
[2025-03-26 21:01:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.47669023275375366 norm:0.00023059155500959605 max memory_allocated 29273.31298828125 
[2025-03-26 21:01:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.47663047909736633 norm:0.00022688355238642544 max memory_allocated 29273.31298828125 
[2025-03-26 21:02:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.47658154368400574 norm:0.00022207017173059285 max memory_allocated 29273.31298828125 
[2025-03-26 21:03:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-26 21:03:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.570316731929779 norm:0.003134142141789198 max memory_allocated 29273.50048828125 
[2025-03-26 21:04:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.5533400177955627 norm:0.0019962002988904715 max memory_allocated 29273.50048828125 
[2025-03-26 21:05:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.5358932018280029 norm:0.0012319710804149508 max memory_allocated 29273.50048828125 
[2025-03-26 21:06:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.530502438545227 norm:0.0009068117942661047 max memory_allocated 29273.50048828125 
[2025-03-26 21:07:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.5287574529647827 norm:0.000744809745810926 max memory_allocated 29273.50048828125 
[2025-03-26 21:08:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.5281586647033691 norm:0.0006474305992014706 max memory_allocated 29273.50048828125 
[2025-03-26 21:08:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.5276973843574524 norm:0.0005688461824320257 max memory_allocated 29273.50048828125 
[2025-03-26 21:09:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.5273396968841553 norm:0.0005093145882710814 max memory_allocated 29273.50048828125 
[2025-03-26 21:10:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.5270259380340576 norm:0.00045561877777799964 max memory_allocated 29273.50048828125 
[2025-03-26 21:11:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.5267850756645203 norm:0.0004175748326815665 max memory_allocated 29273.50048828125 
[2025-03-26 21:12:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.5266401767730713 norm:0.00038142441189847887 max memory_allocated 29273.50048828125 
[2025-03-26 21:13:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.5264956951141357 norm:0.00037402717862278223 max memory_allocated 29273.50048828125 
[2025-03-26 21:13:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.5263770222663879 norm:0.00035534737980924547 max memory_allocated 29273.50048828125 
[2025-03-26 21:14:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.5262451171875 norm:0.0003415499231778085 max memory_allocated 29273.50048828125 
[2025-03-26 21:15:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.526149570941925 norm:0.0003269809822086245 max memory_allocated 29273.50048828125 
[2025-03-26 21:16:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.5260229706764221 norm:0.0003181629872415215 max memory_allocated 29273.50048828125 
[2025-03-26 21:17:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.5258257389068604 norm:0.00028709007892757654 max memory_allocated 29273.50048828125 
[2025-03-26 21:18:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.525729238986969 norm:0.00028855548589490354 max memory_allocated 29273.50048828125 
[2025-03-26 21:18:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.5256856083869934 norm:0.0002971254289150238 max memory_allocated 29273.50048828125 
[2025-03-26 21:19:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.5256615877151489 norm:0.00029102806001901627 max memory_allocated 29273.50048828125 
[2025-03-26 21:20:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-26 21:20:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.6213679909706116 norm:0.0028107340913265944 max memory_allocated 29273.68798828125 
[2025-03-26 21:21:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.6039285063743591 norm:0.0017837182385846972 max memory_allocated 29273.68798828125 
[2025-03-26 21:22:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.5854406356811523 norm:0.0009292477043345571 max memory_allocated 29273.68798828125 
[2025-03-26 21:23:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.5798863172531128 norm:0.0005972645594738424 max memory_allocated 29273.68798828125 
[2025-03-26 21:24:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.5781360864639282 norm:0.0004610865144059062 max memory_allocated 29273.68798828125 
[2025-03-26 21:25:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.5775048732757568 norm:0.0003964836068917066 max memory_allocated 29273.68798828125 
[2025-03-26 21:25:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.5771132111549377 norm:0.0003487059148028493 max memory_allocated 29273.68798828125 
[2025-03-26 21:26:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.576855480670929 norm:0.00033206932130269706 max memory_allocated 29273.68798828125 
[2025-03-26 21:27:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.57660973072052 norm:0.0003086841315962374 max memory_allocated 29273.68798828125 
[2025-03-26 21:28:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.5764574408531189 norm:0.000303259352222085 max memory_allocated 29273.68798828125 
[2025-03-26 21:29:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.5763115882873535 norm:0.00028970217681489885 max memory_allocated 29273.68798828125 
[2025-03-26 21:30:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.5761823654174805 norm:0.0002770056016743183 max memory_allocated 29273.68798828125 
[2025-03-26 21:30:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.576050877571106 norm:0.0002669577661436051 max memory_allocated 29273.68798828125 
[2025-03-26 21:31:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.5759410858154297 norm:0.00026261640596203506 max memory_allocated 29273.68798828125 
[2025-03-26 21:32:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.5758727788925171 norm:0.0002577790291979909 max memory_allocated 29273.68798828125 
[2025-03-26 21:33:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.575820803642273 norm:0.00025515074958093464 max memory_allocated 29273.68798828125 
[2025-03-26 21:34:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.5757643580436707 norm:0.00025563480448909104 max memory_allocated 29273.68798828125 
[2025-03-26 21:35:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.5757323503494263 norm:0.0002510986232664436 max memory_allocated 29273.68798828125 
[2025-03-26 21:35:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.5756780505180359 norm:0.00024863312137313187 max memory_allocated 29273.68798828125 
[2025-03-26 21:36:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.5756222009658813 norm:0.0002411246532574296 max memory_allocated 29273.68798828125 
[2025-03-26 21:37:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-03-26 21:37:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.6854246258735657 norm:0.009051519446074963 max memory_allocated 29273.87548828125 
[2025-03-26 21:38:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.6628721952438354 norm:0.004971644841134548 max memory_allocated 29273.87548828125 
[2025-03-26 21:39:35 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.6397151350975037 norm:0.0023415700998157263 max memory_allocated 29273.87548828125 
[2025-03-26 21:40:25 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.632104218006134 norm:0.0011216229759156704 max memory_allocated 29273.87548828125 
[2025-03-26 21:41:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.6299418807029724 norm:0.0007442234782502055 max memory_allocated 29273.87548828125 
[2025-03-26 21:42:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.6291245818138123 norm:0.0005525965243577957 max memory_allocated 29273.87548828125 
[2025-03-26 21:42:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.6285974979400635 norm:0.00044840408372692764 max memory_allocated 29273.87548828125 
[2025-03-26 21:43:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.628233015537262 norm:0.000420184776885435 max memory_allocated 29273.87548828125 
[2025-03-26 21:44:35 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.6279613375663757 norm:0.0003924439661204815 max memory_allocated 29273.87548828125 
[2025-03-26 21:45:25 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.6276928782463074 norm:0.00036533738602884114 max memory_allocated 29273.87548828125 
[2025-03-26 21:46:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.6275001168251038 norm:0.0003580657939892262 max memory_allocated 29273.87548828125 
[2025-03-26 21:47:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.6273132562637329 norm:0.00033604016061872244 max memory_allocated 29273.87548828125 
[2025-03-26 21:47:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.6271697282791138 norm:0.000322965148370713 max memory_allocated 29273.87548828125 
[2025-03-26 21:48:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.6270648837089539 norm:0.00032281881431117654 max memory_allocated 29273.87548828125 
[2025-03-26 21:49:35 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.6269497871398926 norm:0.0003045199264306575 max memory_allocated 29273.87548828125 
[2025-03-26 21:50:25 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.626873254776001 norm:0.000297494581900537 max memory_allocated 29273.87548828125 
[2025-03-26 21:51:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.626758873462677 norm:0.00028088121325708926 max memory_allocated 29273.87548828125 
[2025-03-26 21:52:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.6266357898712158 norm:0.0002771849976852536 max memory_allocated 29273.87548828125 
[2025-03-26 21:52:56 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.6265653371810913 norm:0.00026736233849078417 max memory_allocated 29273.87548828125 
[2025-03-26 21:53:46 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.6265005469322205 norm:0.0002666437067091465 max memory_allocated 29273.87548828125 
[2025-03-26 21:54:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-03-26 21:54:55 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.7362619042396545 norm:0.0031618971843272448 max memory_allocated 29274.06298828125 
[2025-03-26 21:55:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.7169525623321533 norm:0.0019636540673673153 max memory_allocated 29274.06298828125 
[2025-03-26 21:56:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.6973910927772522 norm:0.0011370828142389655 max memory_allocated 29274.06298828125 
[2025-03-26 21:57:25 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.6916038990020752 norm:0.0007681656279601157 max memory_allocated 29274.06298828125 
[2025-03-26 21:58:15 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.6899927258491516 norm:0.000574262929148972 max memory_allocated 29274.06298828125 
[2025-03-26 21:59:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.6891880035400391 norm:0.0004861578345298767 max memory_allocated 29274.06298828125 
[2025-03-26 21:59:55 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.6887255907058716 norm:0.00044276105472818017 max memory_allocated 29274.06298828125 
[2025-03-26 22:00:44 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.6883543133735657 norm:0.00040588172851130366 max memory_allocated 29274.06298828125 
[2025-03-26 22:01:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.6880676746368408 norm:0.0003793273644987494 max memory_allocated 29274.06298828125 
[2025-03-26 22:02:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.687842607498169 norm:0.00036149725201539695 max memory_allocated 29274.06298828125 
[2025-03-26 22:03:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.687639594078064 norm:0.00034484569914638996 max memory_allocated 29274.06298828125 
[2025-03-26 22:04:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.6874899864196777 norm:0.00033564132172614336 max memory_allocated 29274.06298828125 
[2025-03-26 22:04:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.6873382925987244 norm:0.00032885203836485744 max memory_allocated 29274.06298828125 
[2025-03-26 22:05:44 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.6872268319129944 norm:0.0003190671559423208 max memory_allocated 29274.06298828125 
[2025-03-26 22:06:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.6871411800384521 norm:0.00031491703703068197 max memory_allocated 29274.06298828125 
[2025-03-26 22:07:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.687006413936615 norm:0.0003081562463194132 max memory_allocated 29274.06298828125 
[2025-03-26 22:08:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.6868754625320435 norm:0.00030459192930720747 max memory_allocated 29274.06298828125 
[2025-03-26 22:09:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.6868041753768921 norm:0.00030359087395481765 max memory_allocated 29274.06298828125 
[2025-03-26 22:09:55 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.6867300271987915 norm:0.00030481882276944816 max memory_allocated 29274.06298828125 
[2025-03-26 22:10:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.6866547465324402 norm:0.0002993708476424217 max memory_allocated 29274.06298828125 
[2025-03-26 22:10:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-03-26 22:11:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.8240694999694824 norm:0.005999166518449783 max memory_allocated 29274.25048828125 
[2025-03-26 22:12:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.7974793314933777 norm:0.003603348508477211 max memory_allocated 29274.25048828125 
[2025-03-26 22:13:33 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.771902322769165 norm:0.0020593288354575634 max memory_allocated 29274.25048828125 
[2025-03-26 22:14:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.7637859582901001 norm:0.0011564899468794465 max memory_allocated 29274.25048828125 
[2025-03-26 22:15:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.7614660263061523 norm:0.0007753712125122547 max memory_allocated 29274.25048828125 
[2025-03-26 22:16:03 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.760388195514679 norm:0.0006759442621842027 max memory_allocated 29274.25048828125 
[2025-03-26 22:16:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.7595857977867126 norm:0.000598568469285965 max memory_allocated 29274.25048828125 
[2025-03-26 22:17:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.7590218782424927 norm:0.0005604746984317899 max memory_allocated 29274.25048828125 
[2025-03-26 22:18:33 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.7586328387260437 norm:0.0005299178301356733 max memory_allocated 29274.25048828125 
[2025-03-26 22:19:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.7582700848579407 norm:0.0004971508751623333 max memory_allocated 29274.25048828125 
[2025-03-26 22:20:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.7579765915870667 norm:0.0004649665206670761 max memory_allocated 29274.25048828125 
[2025-03-26 22:21:03 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.7577860355377197 norm:0.0004437683383002877 max memory_allocated 29274.25048828125 
[2025-03-26 22:21:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.757586658000946 norm:0.0004311552329454571 max memory_allocated 29274.25048828125 
[2025-03-26 22:22:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.7573425769805908 norm:0.0004284447350073606 max memory_allocated 29274.25048828125 
[2025-03-26 22:23:33 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.7570785284042358 norm:0.00042158158612437546 max memory_allocated 29274.25048828125 
[2025-03-26 22:24:24 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.7569230198860168 norm:0.0004147311265114695 max memory_allocated 29274.25048828125 
[2025-03-26 22:25:14 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.7568237781524658 norm:0.00040448157233186066 max memory_allocated 29274.25048828125 
[2025-03-26 22:26:04 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.7567092776298523 norm:0.0003865028265863657 max memory_allocated 29274.25048828125 
[2025-03-26 22:26:55 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.7566207051277161 norm:0.00037575847818516195 max memory_allocated 29274.25048828125 
[2025-03-26 22:27:45 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.7565515637397766 norm:0.0003769249015022069 max memory_allocated 29274.25048828125 
[2025-03-26 22:28:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-03-26 22:28:54 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.8999837040901184 norm:0.004837376996874809 max memory_allocated 29274.43798828125 
[2025-03-26 22:29:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.8733018636703491 norm:0.0030855026561766863 max memory_allocated 29274.43798828125 
[2025-03-26 22:30:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.8467959761619568 norm:0.0018715490587055683 max memory_allocated 29274.43798828125 
[2025-03-26 22:31:25 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.8383615612983704 norm:0.0011185763869434595 max memory_allocated 29274.43798828125 
[2025-03-26 22:32:15 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.8357502222061157 norm:0.0008657437865622342 max memory_allocated 29274.43798828125 
[2025-03-26 22:33:05 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.8344265818595886 norm:0.0007635673973709345 max memory_allocated 29274.43798828125 
[2025-03-26 22:33:55 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.8336330652236938 norm:0.0007009113323874772 max memory_allocated 29274.43798828125 
[2025-03-26 22:34:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.8329257965087891 norm:0.0006463307072408497 max memory_allocated 29274.43798828125 
[2025-03-26 22:35:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.8323996067047119 norm:0.0006187980761751533 max memory_allocated 29274.43798828125 
[2025-03-26 22:36:25 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.8318802118301392 norm:0.0005912526394240558 max memory_allocated 29274.43798828125 
[2025-03-26 22:37:15 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.8315539360046387 norm:0.0005718526663258672 max memory_allocated 29274.43798828125 
[2025-03-26 22:38:05 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.8311609625816345 norm:0.0005484668654389679 max memory_allocated 29274.43798828125 
[2025-03-26 22:38:56 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.830791175365448 norm:0.0005321493372321129 max memory_allocated 29274.43798828125 
[2025-03-26 22:39:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.8305143713951111 norm:0.0005185757181607187 max memory_allocated 29274.43798828125 
[2025-03-26 22:40:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.8303681015968323 norm:0.0005060781259089708 max memory_allocated 29274.43798828125 
[2025-03-26 22:41:26 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.8301927447319031 norm:0.0004883454530499876 max memory_allocated 29274.43798828125 
[2025-03-26 22:42:16 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.8300033807754517 norm:0.0004883281653746963 max memory_allocated 29274.43798828125 
[2025-03-26 22:43:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.8298289775848389 norm:0.00048340263310819864 max memory_allocated 29274.43798828125 
[2025-03-26 22:43:57 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.8296396136283875 norm:0.0004767563659697771 max memory_allocated 29274.43798828125 
[2025-03-26 22:44:47 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.8294973373413086 norm:0.00046156314783729613 max memory_allocated 29274.43798828125 
[2025-03-26 22:45:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-03-26 22:45:06 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 22:45:57 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.9861366748809814 norm:0.018931392580270767 max memory_allocated 29274.77001953125 
[2025-03-26 22:46:48 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.9561944007873535 norm:0.014813611283898354 max memory_allocated 29274.77001953125 
[2025-03-26 22:47:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.9273460507392883 norm:0.011201728135347366 max memory_allocated 29274.77001953125 
[2025-03-26 22:48:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.9194921255111694 norm:0.009513972327113152 max memory_allocated 29274.77001953125 
[2025-03-26 22:49:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.9166188836097717 norm:0.008105837740004063 max memory_allocated 29274.77001953125 
[2025-03-26 22:50:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.9149195551872253 norm:0.006970936432480812 max memory_allocated 29274.77001953125 
[2025-03-26 22:51:00 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.9139034152030945 norm:0.006136516109108925 max memory_allocated 29274.77001953125 
[2025-03-26 22:51:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.9131181836128235 norm:0.00562111334875226 max memory_allocated 29274.77001953125 
[2025-03-26 22:52:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.9124188423156738 norm:0.005295603536069393 max memory_allocated 29274.77001953125 
[2025-03-26 22:53:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.9120306968688965 norm:0.005126602482050657 max memory_allocated 29274.77001953125 
[2025-03-26 22:54:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.9116488695144653 norm:0.005011575296521187 max memory_allocated 29274.77001953125 
[2025-03-26 22:55:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.9112295508384705 norm:0.004801652859896421 max memory_allocated 29274.77001953125 
[2025-03-26 22:56:03 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.9108631014823914 norm:0.004813002422451973 max memory_allocated 29274.77001953125 
[2025-03-26 22:56:54 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.9104863405227661 norm:0.004703634884208441 max memory_allocated 29274.77001953125 
[2025-03-26 22:57:44 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.9103995561599731 norm:0.0048392415046691895 max memory_allocated 29274.77001953125 
[2025-03-26 22:58:35 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.9101361036300659 norm:0.004683374427258968 max memory_allocated 29274.77001953125 
[2025-03-26 22:59:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.9100549221038818 norm:0.004660104401409626 max memory_allocated 29274.77001953125 
[2025-03-26 23:00:16 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.9098376035690308 norm:0.0046214452013373375 max memory_allocated 29274.77001953125 
[2025-03-26 23:01:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.9097238779067993 norm:0.004570079036056995 max memory_allocated 29274.77001953125 
[2025-03-26 23:01:57 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.9095594882965088 norm:0.004475595895200968 max memory_allocated 29274.77001953125 
[2025-03-26 23:02:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-03-26 23:02:15 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 23:03:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:1.1361393928527832 norm:0.038572072982788086 max memory_allocated 29274.95751953125 
[2025-03-26 23:03:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:1.0888532400131226 norm:0.026285812258720398 max memory_allocated 29274.95751953125 
[2025-03-26 23:04:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:1.0473064184188843 norm:0.01730228215456009 max memory_allocated 29274.95751953125 
[2025-03-26 23:05:36 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:1.0357571840286255 norm:0.013480620458722115 max memory_allocated 29274.95751953125 
[2025-03-26 23:06:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:1.0313823223114014 norm:0.011030726134777069 max memory_allocated 29274.95751953125 
[2025-03-26 23:07:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:1.0287837982177734 norm:0.010948576033115387 max memory_allocated 29274.95751953125 
[2025-03-26 23:08:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:1.027111530303955 norm:0.01211412064731121 max memory_allocated 29274.95751953125 
[2025-03-26 23:08:58 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:1.0254366397857666 norm:0.009943059645593166 max memory_allocated 29274.95751953125 
[2025-03-26 23:09:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:1.0242969989776611 norm:0.009131187573075294 max memory_allocated 29274.95751953125 
[2025-03-26 23:10:38 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:1.0233981609344482 norm:0.008404386229813099 max memory_allocated 29274.95751953125 
[2025-03-26 23:11:29 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:1.0226503610610962 norm:0.00798963475972414 max memory_allocated 29274.95751953125 
[2025-03-26 23:12:20 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:1.0220952033996582 norm:0.007760827895253897 max memory_allocated 29274.95751953125 
[2025-03-26 23:13:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:1.021675944328308 norm:0.007570264395326376 max memory_allocated 29274.95751953125 
[2025-03-26 23:14:00 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:1.0213395357131958 norm:0.007367092650383711 max memory_allocated 29274.95751953125 
[2025-03-26 23:14:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:1.0207045078277588 norm:0.006974466145038605 max memory_allocated 29274.95751953125 
[2025-03-26 23:15:41 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:1.0205535888671875 norm:0.0067161559127271175 max memory_allocated 29274.95751953125 
[2025-03-26 23:16:32 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:1.020806908607483 norm:0.00671052560210228 max memory_allocated 29274.95751953125 
[2025-03-26 23:17:22 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:1.0203773975372314 norm:0.006620377767831087 max memory_allocated 29274.95751953125 
[2025-03-26 23:18:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:1.0200291872024536 norm:0.005975979380309582 max memory_allocated 29274.95751953125 
[2025-03-26 23:19:03 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:1.020308256149292 norm:0.006123539060354233 max memory_allocated 29274.95751953125 
[2025-03-26 23:19:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-03-26 23:19:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 23:20:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:1.5430772304534912 norm:0.2633705735206604 max memory_allocated 29275.14501953125 
[2025-03-26 23:21:03 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:1.373983383178711 norm:0.1988992542028427 max memory_allocated 29275.14501953125 
[2025-03-26 23:21:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:1.3109229803085327 norm:0.15177609026432037 max memory_allocated 29275.14501953125 
[2025-03-26 23:22:44 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:1.282705545425415 norm:0.12268530577421188 max memory_allocated 29275.14501953125 
[2025-03-26 23:23:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:1.2658872604370117 norm:0.09742127358913422 max memory_allocated 29275.14501953125 
[2025-03-26 23:24:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:1.256430983543396 norm:0.07476089894771576 max memory_allocated 29275.14501953125 
[2025-03-26 23:25:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:1.25075101852417 norm:0.0579342320561409 max memory_allocated 29275.14501953125 
[2025-03-26 23:26:06 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:1.2454288005828857 norm:0.0456978976726532 max memory_allocated 29275.14501953125 
[2025-03-26 23:26:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:1.242362380027771 norm:0.03785461187362671 max memory_allocated 29275.14501953125 
[2025-03-26 23:27:46 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:1.2393025159835815 norm:0.03303072229027748 max memory_allocated 29275.14501953125 
[2025-03-26 23:28:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:1.237196922302246 norm:0.0318947471678257 max memory_allocated 29275.14501953125 
[2025-03-26 23:29:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:1.2354556322097778 norm:0.030224530026316643 max memory_allocated 29275.14501953125 
[2025-03-26 23:30:17 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:1.2338522672653198 norm:0.02837185561656952 max memory_allocated 29275.14501953125 
[2025-03-26 23:31:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:1.2322908639907837 norm:0.02783813700079918 max memory_allocated 29275.14501953125 
[2025-03-26 23:31:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:1.2311159372329712 norm:0.027051445096731186 max memory_allocated 29275.14501953125 
[2025-03-26 23:32:49 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:1.2299244403839111 norm:0.026258133351802826 max memory_allocated 29275.14501953125 
[2025-03-26 23:33:39 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:1.229569435119629 norm:0.026086289435625076 max memory_allocated 29275.14501953125 
[2025-03-26 23:34:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:1.2287737131118774 norm:0.024363819509744644 max memory_allocated 29275.14501953125 
[2025-03-26 23:35:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:1.2279292345046997 norm:0.023876318708062172 max memory_allocated 29275.14501953125 
[2025-03-26 23:36:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:1.2272052764892578 norm:0.022889284417033195 max memory_allocated 29275.14501953125 
[2025-03-26 23:36:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-03-26 23:36:29 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-26 23:37:20 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:2.2814218997955322 norm:0.14127177000045776 max memory_allocated 29275.33251953125 
[2025-03-26 23:38:10 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:2.1391773223876953 norm:0.12067775428295135 max memory_allocated 29275.33251953125 
[2025-03-26 23:39:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:2.027836322784424 norm:0.10027388483285904 max memory_allocated 29275.33251953125 
[2025-03-26 23:39:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:1.9915697574615479 norm:0.09733748435974121 max memory_allocated 29275.33251953125 
[2025-03-26 23:40:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:1.9722237586975098 norm:0.10634510219097137 max memory_allocated 29275.33251953125 
[2025-03-26 23:41:31 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:1.9573105573654175 norm:0.10178345441818237 max memory_allocated 29275.33251953125 
[2025-03-26 23:42:22 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:1.9460163116455078 norm:0.09648638218641281 max memory_allocated 29275.33251953125 
[2025-03-26 23:43:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:1.9370251893997192 norm:0.0933627337217331 max memory_allocated 29275.33251953125 
[2025-03-26 23:44:03 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:1.9299979209899902 norm:0.0911654680967331 max memory_allocated 29275.33251953125 
[2025-03-26 23:44:53 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:1.9215490818023682 norm:0.08932996541261673 max memory_allocated 29275.33251953125 
[2025-03-26 23:45:43 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:1.9168556928634644 norm:0.08474946022033691 max memory_allocated 29275.33251953125 
[2025-03-26 23:46:33 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:1.91204035282135 norm:0.08611959964036942 max memory_allocated 29275.33251953125 
[2025-03-26 23:47:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:1.9108970165252686 norm:0.08512541651725769 max memory_allocated 29275.33251953125 
[2025-03-26 23:48:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:1.906568169593811 norm:0.08139833807945251 max memory_allocated 29275.33251953125 
[2025-03-26 23:49:05 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:1.9039238691329956 norm:0.07754798233509064 max memory_allocated 29275.33251953125 
[2025-03-26 23:49:56 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:1.9000146389007568 norm:0.07622840255498886 max memory_allocated 29275.33251953125 
[2025-03-26 23:50:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:1.8990272283554077 norm:0.0726100504398346 max memory_allocated 29275.33251953125 
[2025-03-26 23:51:38 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:1.89870023727417 norm:0.07294485718011856 max memory_allocated 29275.33251953125 
[2025-03-26 23:52:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:1.8959077596664429 norm:0.07125058770179749 max memory_allocated 29275.33251953125 
[2025-03-26 23:53:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:1.8928935527801514 norm:0.0691041648387909 max memory_allocated 29275.33251953125 
[2025-03-26 23:53:34 root] (main_calibration.py 365): INFO 40854.24332737923
[2025-03-26 23:53:43 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-26 23:55:43 root] (main_calibration.py 158): INFO wikitext2 : 5.385950088500977
[2025-03-26 23:55:43 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-26 23:58:47 root] (main_calibration.py 158): INFO c4 : 6.9780473709106445
[2025-03-27 01:54:24 root] (main_calibration.py 169): INFO {'wikitext2': 5.385950088500977, 'c4': 6.9780473709106445, 'results': {'winogrande': {'acc': 0.6724546172059984, 'acc_stderr': 0.013190169546797017}, 'arc_easy': {'acc': 0.7276936026936027, 'acc_stderr': 0.00913421844765268, 'acc_norm': 0.5782828282828283, 'acc_norm_stderr': 0.010133255284012314}, 'arc_challenge': {'acc': 0.41723549488054607, 'acc_stderr': 0.01440982551840308, 'acc_norm': 0.41638225255972694, 'acc_norm_stderr': 0.01440561827943618}, 'piqa': {'acc': 0.780739934711643, 'acc_stderr': 0.0096533574636053, 'acc_norm': 0.779651795429815, 'acc_norm_stderr': 0.009670535456853131}, 'boolq': {'acc': 0.6529051987767585, 'acc_stderr': 0.008326100668151903}, 'hellaswag': {'acc': 0.570902210714997, 'acc_stderr': 0.004939358145561318, 'acc_norm': 0.7412865962955587, 'acc_norm_stderr': 0.004370328224831784}}, 'versions': {'winogrande': 0, 'arc_easy': 0, 'arc_challenge': 0, 'piqa': 0, 'boolq': 1, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
