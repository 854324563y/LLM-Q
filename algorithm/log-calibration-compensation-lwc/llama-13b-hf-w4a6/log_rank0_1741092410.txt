[2025-03-04 12:46:50 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-13b-hf-w4a6', save_dir='./log-calibration-compensation-lwc/quant/llama-13b-hf-w4a6', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-04 12:58:47 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-04 12:58:47 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-04 12:58:47 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-04 12:58:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-04 12:58:55 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 12:59:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.02717066928744316 norm:0.020310627296566963 max memory_allocated 29268.02001953125 
[2025-03-04 13:00:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.017647070810198784 norm:0.011233149096369743 max memory_allocated 29268.02001953125 
[2025-03-04 13:01:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.013231180608272552 norm:0.007065847981721163 max memory_allocated 29268.02001953125 
[2025-03-04 13:02:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.01193287130445242 norm:0.005943232215940952 max memory_allocated 29268.02001953125 
[2025-03-04 13:03:00 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.01139471773058176 norm:0.005207504145801067 max memory_allocated 29268.02001953125 
[2025-03-04 13:03:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.011085709556937218 norm:0.0045709507539868355 max memory_allocated 29268.02001953125 
[2025-03-04 13:04:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.010765856131911278 norm:0.003995129838585854 max memory_allocated 29268.02001953125 
[2025-03-04 13:05:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.010752853937447071 norm:0.003679483663290739 max memory_allocated 29268.02001953125 
[2025-03-04 13:06:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.010579882189631462 norm:0.0031678462401032448 max memory_allocated 29268.02001953125 
[2025-03-04 13:07:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.01054454781115055 norm:0.002879711566492915 max memory_allocated 29268.02001953125 
[2025-03-04 13:07:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.010367866605520248 norm:0.0025227710139006376 max memory_allocated 29268.02001953125 
[2025-03-04 13:08:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.010338467545807362 norm:0.0022273040376603603 max memory_allocated 29268.02001953125 
[2025-03-04 13:09:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.010264108888804913 norm:0.0020189641509205103 max memory_allocated 29268.02001953125 
[2025-03-04 13:10:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.010327765718102455 norm:0.0018949134973809123 max memory_allocated 29268.02001953125 
[2025-03-04 13:11:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.010170654393732548 norm:0.0016261334531009197 max memory_allocated 29268.02001953125 
[2025-03-04 13:12:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.010203409008681774 norm:0.001672232523560524 max memory_allocated 29268.02001953125 
[2025-03-04 13:12:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.01009672787040472 norm:0.0015479676658287644 max memory_allocated 29268.02001953125 
[2025-03-04 13:13:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.010096434503793716 norm:0.0015548696974292397 max memory_allocated 29268.02001953125 
[2025-03-04 13:14:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.010136330500245094 norm:0.0014657404972240329 max memory_allocated 29268.02001953125 
[2025-03-04 13:15:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.010037329979240894 norm:0.001410877681337297 max memory_allocated 29268.02001953125 
[2025-03-04 13:15:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-04 13:15:39 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 13:16:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.04361420124769211 norm:0.020213192328810692 max memory_allocated 29268.02001953125 
[2025-03-04 13:17:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.028990602120757103 norm:0.012062489055097103 max memory_allocated 29268.02001953125 
[2025-03-04 13:18:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.022728823125362396 norm:0.007264270447194576 max memory_allocated 29268.02001953125 
[2025-03-04 13:18:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.020814474672079086 norm:0.006022078450769186 max memory_allocated 29268.02001953125 
[2025-03-04 13:19:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.020011182874441147 norm:0.005332925356924534 max memory_allocated 29268.02001953125 
[2025-03-04 13:20:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.01949821598827839 norm:0.004788877908140421 max memory_allocated 29268.02001953125 
[2025-03-04 13:21:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.019150465726852417 norm:0.004308963660150766 max memory_allocated 29268.02001953125 
[2025-03-04 13:22:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.018863432109355927 norm:0.003945940174162388 max memory_allocated 29268.02001953125 
[2025-03-04 13:23:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.01865510083734989 norm:0.003614713903516531 max memory_allocated 29268.02001953125 
[2025-03-04 13:23:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.018446985632181168 norm:0.0033081748988479376 max memory_allocated 29268.02001953125 
[2025-03-04 13:24:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.018251536414027214 norm:0.003072710009291768 max memory_allocated 29268.02001953125 
[2025-03-04 13:25:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.018067993223667145 norm:0.002797712804749608 max memory_allocated 29268.02001953125 
[2025-03-04 13:26:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.01791244000196457 norm:0.0025270101614296436 max memory_allocated 29268.02001953125 
[2025-03-04 13:27:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.01781737059354782 norm:0.0022995262406766415 max memory_allocated 29268.02001953125 
[2025-03-04 13:28:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.017716074362397194 norm:0.002085413783788681 max memory_allocated 29268.02001953125 
[2025-03-04 13:28:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.017646733671426773 norm:0.001895574270747602 max memory_allocated 29268.02001953125 
[2025-03-04 13:29:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.017574500292539597 norm:0.0017307178350165486 max memory_allocated 29268.02001953125 
[2025-03-04 13:30:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.01755032315850258 norm:0.0017240192973986268 max memory_allocated 29268.02001953125 
[2025-03-04 13:31:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.017627540975809097 norm:0.001719017163850367 max memory_allocated 29268.02001953125 
[2025-03-04 13:32:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.01757255755364895 norm:0.0016060054767876863 max memory_allocated 29268.02001953125 
[2025-03-04 13:32:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-04 13:32:27 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 13:33:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.04606279358267784 norm:0.010439390316605568 max memory_allocated 29268.39501953125 
[2025-03-04 13:34:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.038999058306217194 norm:0.00935313105583191 max memory_allocated 29268.39501953125 
[2025-03-04 13:34:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.03442557156085968 norm:0.006567282602190971 max memory_allocated 29268.39501953125 
[2025-03-04 13:35:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.03292173892259598 norm:0.00607310002669692 max memory_allocated 29268.39501953125 
[2025-03-04 13:36:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.03226320818066597 norm:0.0057640899904072285 max memory_allocated 29268.39501953125 
[2025-03-04 13:37:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.031579937785863876 norm:0.005554886534810066 max memory_allocated 29268.39501953125 
[2025-03-04 13:38:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.031324803829193115 norm:0.0056050908751785755 max memory_allocated 29268.39501953125 
[2025-03-04 13:39:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.030705656856298447 norm:0.005320149939507246 max memory_allocated 29268.39501953125 
[2025-03-04 13:39:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.030542699620127678 norm:0.005191712640225887 max memory_allocated 29268.39501953125 
[2025-03-04 13:40:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.03028983436524868 norm:0.004873114172369242 max memory_allocated 29268.39501953125 
[2025-03-04 13:41:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.030201172456145287 norm:0.0047170245088636875 max memory_allocated 29268.39501953125 
[2025-03-04 13:42:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.03024306893348694 norm:0.004623619373887777 max memory_allocated 29268.39501953125 
[2025-03-04 13:43:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.029920050874352455 norm:0.004657916724681854 max memory_allocated 29268.39501953125 
[2025-03-04 13:44:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.03032863698899746 norm:0.005035987589508295 max memory_allocated 29268.39501953125 
[2025-03-04 13:44:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.03031214512884617 norm:0.0048768422566354275 max memory_allocated 29268.39501953125 
[2025-03-04 13:45:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.030204376205801964 norm:0.0048348805867135525 max memory_allocated 29268.39501953125 
[2025-03-04 13:46:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.029713548719882965 norm:0.004378356970846653 max memory_allocated 29268.39501953125 
[2025-03-04 13:47:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.030205052345991135 norm:0.00441835867241025 max memory_allocated 29268.39501953125 
[2025-03-04 13:48:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.029834484681487083 norm:0.004574389196932316 max memory_allocated 29268.39501953125 
[2025-03-04 13:48:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.030156413093209267 norm:0.0043533057905733585 max memory_allocated 29268.39501953125 
[2025-03-04 13:49:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-04 13:50:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.06773686408996582 norm:0.014197653159499168 max memory_allocated 29268.43798828125 
[2025-03-04 13:50:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.05037768930196762 norm:0.004268617369234562 max memory_allocated 29268.43798828125 
[2025-03-04 13:51:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.03975694254040718 norm:0.001415291684679687 max memory_allocated 29268.43798828125 
[2025-03-04 13:52:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.03697263449430466 norm:0.0009150184923782945 max memory_allocated 29268.43798828125 
[2025-03-04 13:53:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.03584299609065056 norm:0.0007835743599571288 max memory_allocated 29268.43798828125 
[2025-03-04 13:54:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.03508954495191574 norm:0.000695300754159689 max memory_allocated 29268.43798828125 
[2025-03-04 13:55:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.03459891676902771 norm:0.0006639185594394803 max memory_allocated 29268.43798828125 
[2025-03-04 13:55:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.034284330904483795 norm:0.0005640971357934177 max memory_allocated 29268.43798828125 
[2025-03-04 13:56:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0340590700507164 norm:0.00048551568761467934 max memory_allocated 29268.43798828125 
[2025-03-04 13:57:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0339379608631134 norm:0.00042380436207167804 max memory_allocated 29268.43798828125 
[2025-03-04 13:58:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.03386884927749634 norm:0.0004141692479606718 max memory_allocated 29268.43798828125 
[2025-03-04 13:59:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.03382926061749458 norm:0.0003915138659067452 max memory_allocated 29268.43798828125 
[2025-03-04 13:59:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.03375363349914551 norm:0.0003549243847373873 max memory_allocated 29268.43798828125 
[2025-03-04 14:00:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.03375888615846634 norm:0.00033322785748168826 max memory_allocated 29268.43798828125 
[2025-03-04 14:01:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.03371881693601608 norm:0.00030833075288683176 max memory_allocated 29268.43798828125 
[2025-03-04 14:02:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.033689700067043304 norm:0.00030679337214678526 max memory_allocated 29268.43798828125 
[2025-03-04 14:03:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.033667661249637604 norm:0.0002636788412928581 max memory_allocated 29268.43798828125 
[2025-03-04 14:04:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.03363754600286484 norm:0.0002790880389511585 max memory_allocated 29268.43798828125 
[2025-03-04 14:04:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.03367641195654869 norm:0.0002762095828074962 max memory_allocated 29268.43798828125 
[2025-03-04 14:05:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.03373456001281738 norm:0.00028359738644212484 max memory_allocated 29268.43798828125 
[2025-03-04 14:05:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-04 14:06:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.07573562860488892 norm:0.018829837441444397 max memory_allocated 29268.62548828125 
[2025-03-04 14:07:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.05809108167886734 norm:0.005787537433207035 max memory_allocated 29268.62548828125 
[2025-03-04 14:08:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.047269996255636215 norm:0.002550066914409399 max memory_allocated 29268.62548828125 
[2025-03-04 14:09:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.04422054439783096 norm:0.0014536416856572032 max memory_allocated 29268.62548828125 
[2025-03-04 14:10:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.04262540489435196 norm:0.0010302491718903184 max memory_allocated 29268.62548828125 
[2025-03-04 14:10:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.04182824864983559 norm:0.000903434120118618 max memory_allocated 29268.62548828125 
[2025-03-04 14:11:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.04144442081451416 norm:0.0008452184265479445 max memory_allocated 29268.62548828125 
[2025-03-04 14:12:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.041089944541454315 norm:0.0007223293650895357 max memory_allocated 29268.62548828125 
[2025-03-04 14:13:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.040894486010074615 norm:0.0006568911485373974 max memory_allocated 29268.62548828125 
[2025-03-04 14:14:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.040800027549266815 norm:0.0006094802520237863 max memory_allocated 29268.62548828125 
[2025-03-04 14:15:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0407322496175766 norm:0.0005541419377550483 max memory_allocated 29268.62548828125 
[2025-03-04 14:15:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0407845601439476 norm:0.0004978800425305963 max memory_allocated 29268.62548828125 
[2025-03-04 14:16:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.040727630257606506 norm:0.00044929751311428845 max memory_allocated 29268.62548828125 
[2025-03-04 14:17:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0406588613986969 norm:0.0004128846921958029 max memory_allocated 29268.62548828125 
[2025-03-04 14:18:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.04065229743719101 norm:0.00040176406037062407 max memory_allocated 29268.62548828125 
[2025-03-04 14:19:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.04058219492435455 norm:0.0003724545822478831 max memory_allocated 29268.62548828125 
[2025-03-04 14:20:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0405590794980526 norm:0.0003632979351095855 max memory_allocated 29268.62548828125 
[2025-03-04 14:20:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.04052793234586716 norm:0.00032988633029162884 max memory_allocated 29268.62548828125 
[2025-03-04 14:21:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.04052403196692467 norm:0.00030102921300567687 max memory_allocated 29268.62548828125 
[2025-03-04 14:22:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.04051174968481064 norm:0.00028968945844098926 max memory_allocated 29268.62548828125 
[2025-03-04 14:22:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-04 14:23:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.09882556647062302 norm:0.02964629977941513 max memory_allocated 29268.81298828125 
[2025-03-04 14:24:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.07322023063898087 norm:0.01042198110371828 max memory_allocated 29268.81298828125 
[2025-03-04 14:25:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.05444863438606262 norm:0.0029075448401272297 max memory_allocated 29268.81298828125 
[2025-03-04 14:26:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.049963656812906265 norm:0.0015171419363468885 max memory_allocated 29268.81298828125 
[2025-03-04 14:26:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.04831033572554588 norm:0.001226387219503522 max memory_allocated 29268.81298828125 
[2025-03-04 14:27:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.04749009758234024 norm:0.0010985053377225995 max memory_allocated 29268.81298828125 
[2025-03-04 14:28:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.046783555299043655 norm:0.000899036880582571 max memory_allocated 29268.81298828125 
[2025-03-04 14:29:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.04638595134019852 norm:0.0008003002149052918 max memory_allocated 29268.81298828125 
[2025-03-04 14:30:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.04599854350090027 norm:0.0007081141811795533 max memory_allocated 29268.81298828125 
[2025-03-04 14:31:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.04577720910310745 norm:0.0006730692693963647 max memory_allocated 29268.81298828125 
[2025-03-04 14:31:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.045632123947143555 norm:0.0006103909108787775 max memory_allocated 29268.81298828125 
[2025-03-04 14:32:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.045538805425167084 norm:0.000589907169342041 max memory_allocated 29268.81298828125 
[2025-03-04 14:33:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0454757884144783 norm:0.0005432264297269285 max memory_allocated 29268.81298828125 
[2025-03-04 14:34:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.045392509549856186 norm:0.0005678971065208316 max memory_allocated 29268.81298828125 
[2025-03-04 14:35:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.04533996433019638 norm:0.0004979528021067381 max memory_allocated 29268.81298828125 
[2025-03-04 14:36:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.045329414308071136 norm:0.00047553027980029583 max memory_allocated 29268.81298828125 
[2025-03-04 14:36:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.04532025381922722 norm:0.0004326911875978112 max memory_allocated 29268.81298828125 
[2025-03-04 14:37:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.04515473544597626 norm:0.00040915413410402834 max memory_allocated 29268.81298828125 
[2025-03-04 14:38:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.045094408094882965 norm:0.00037558170151896775 max memory_allocated 29268.81298828125 
[2025-03-04 14:39:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.04510955139994621 norm:0.00037867913488298655 max memory_allocated 29268.81298828125 
[2025-03-04 14:39:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-04 14:40:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.08993423730134964 norm:0.005872506648302078 max memory_allocated 29269.00048828125 
[2025-03-04 14:41:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.07335920631885529 norm:0.0025536715984344482 max memory_allocated 29269.00048828125 
[2025-03-04 14:42:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.06073710694909096 norm:0.0009805192239582539 max memory_allocated 29269.00048828125 
[2025-03-04 14:42:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0573321208357811 norm:0.0007662153802812099 max memory_allocated 29269.00048828125 
[2025-03-04 14:43:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.055810846388339996 norm:0.0007474403828382492 max memory_allocated 29269.00048828125 
[2025-03-04 14:44:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.05512846261262894 norm:0.0006348444730974734 max memory_allocated 29269.00048828125 
[2025-03-04 14:45:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0547330416738987 norm:0.0005764506058767438 max memory_allocated 29269.00048828125 
[2025-03-04 14:46:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.05423348397016525 norm:0.0005247630761004984 max memory_allocated 29269.00048828125 
[2025-03-04 14:47:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.05403871089220047 norm:0.0005348031409084797 max memory_allocated 29269.00048828125 
[2025-03-04 14:47:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.05386386811733246 norm:0.0004935328033752739 max memory_allocated 29269.00048828125 
[2025-03-04 14:48:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.05375310778617859 norm:0.00045899164979346097 max memory_allocated 29269.00048828125 
[2025-03-04 14:49:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.05364935100078583 norm:0.0004437820171006024 max memory_allocated 29269.00048828125 
[2025-03-04 14:50:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.053594931960105896 norm:0.00046355751692317426 max memory_allocated 29269.00048828125 
[2025-03-04 14:51:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.053598418831825256 norm:0.000440611649537459 max memory_allocated 29269.00048828125 
[2025-03-04 14:51:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.053538884967565536 norm:0.0004392245609778911 max memory_allocated 29269.00048828125 
[2025-03-04 14:52:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.05349063128232956 norm:0.0004963121027685702 max memory_allocated 29269.00048828125 
[2025-03-04 14:53:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.053508609533309937 norm:0.0004696659161709249 max memory_allocated 29269.00048828125 
[2025-03-04 14:54:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.05349971726536751 norm:0.0007261923747137189 max memory_allocated 29269.00048828125 
[2025-03-04 14:55:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.05330611765384674 norm:0.0004446996026672423 max memory_allocated 29269.00048828125 
[2025-03-04 14:56:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.05335782468318939 norm:0.0004348706279415637 max memory_allocated 29269.00048828125 
[2025-03-04 14:56:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-04 14:57:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.09157754480838776 norm:0.005470158997923136 max memory_allocated 29269.18798828125 
[2025-03-04 14:58:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.07738347351551056 norm:0.002842755988240242 max memory_allocated 29269.18798828125 
[2025-03-04 14:58:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.06475792080163956 norm:0.0010158304357901216 max memory_allocated 29269.18798828125 
[2025-03-04 14:59:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.060772091150283813 norm:0.0005057715461589396 max memory_allocated 29269.18798828125 
[2025-03-04 15:00:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.05908927321434021 norm:0.00038872691220603883 max memory_allocated 29269.18798828125 
[2025-03-04 15:01:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.058299094438552856 norm:0.00037845768383704126 max memory_allocated 29269.18798828125 
[2025-03-04 15:02:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.05790559947490692 norm:0.0003716813225764781 max memory_allocated 29269.18798828125 
[2025-03-04 15:02:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.05773890018463135 norm:0.000377621385268867 max memory_allocated 29269.18798828125 
[2025-03-04 15:03:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.057626329362392426 norm:0.00035086029674857855 max memory_allocated 29269.18798828125 
[2025-03-04 15:04:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.05743494629859924 norm:0.0003070133680012077 max memory_allocated 29269.18798828125 
[2025-03-04 15:05:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.05731185898184776 norm:0.0002974196686409414 max memory_allocated 29269.18798828125 
[2025-03-04 15:06:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.05723964422941208 norm:0.0002798670611809939 max memory_allocated 29269.18798828125 
[2025-03-04 15:07:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.057206183671951294 norm:0.0002777298213914037 max memory_allocated 29269.18798828125 
[2025-03-04 15:07:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.05716664344072342 norm:0.0002660943428054452 max memory_allocated 29269.18798828125 
[2025-03-04 15:08:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.05717587471008301 norm:0.0002736138994805515 max memory_allocated 29269.18798828125 
[2025-03-04 15:09:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.057205114513635635 norm:0.0002667021472007036 max memory_allocated 29269.18798828125 
[2025-03-04 15:10:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.05712450295686722 norm:0.0002449550083838403 max memory_allocated 29269.18798828125 
[2025-03-04 15:11:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.057094722986221313 norm:0.00024088259669952095 max memory_allocated 29269.18798828125 
[2025-03-04 15:12:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.057046785950660706 norm:0.00023779566981829703 max memory_allocated 29269.18798828125 
[2025-03-04 15:12:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0570833794772625 norm:0.00023576754028908908 max memory_allocated 29269.18798828125 
[2025-03-04 15:13:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-04 15:13:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.09839874505996704 norm:0.005071990191936493 max memory_allocated 29269.37548828125 
[2025-03-04 15:14:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.08204010128974915 norm:0.002357293851673603 max memory_allocated 29269.37548828125 
[2025-03-04 15:15:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06998181343078613 norm:0.0008975434466265142 max memory_allocated 29269.37548828125 
[2025-03-04 15:16:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.06616511195898056 norm:0.000545657763723284 max memory_allocated 29269.37548828125 
[2025-03-04 15:17:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.06434550881385803 norm:0.00040075278957374394 max memory_allocated 29269.37548828125 
[2025-03-04 15:18:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.06336914002895355 norm:0.00037154543679207563 max memory_allocated 29269.37548828125 
[2025-03-04 15:18:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0628574788570404 norm:0.000354351126588881 max memory_allocated 29269.37548828125 
[2025-03-04 15:19:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.062438614666461945 norm:0.0003214823082089424 max memory_allocated 29269.37548828125 
[2025-03-04 15:20:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.06230270117521286 norm:0.00031065903021954 max memory_allocated 29269.37548828125 
[2025-03-04 15:21:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.06211010366678238 norm:0.00029255906702019274 max memory_allocated 29269.37548828125 
[2025-03-04 15:22:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.062053948640823364 norm:0.0002758070477284491 max memory_allocated 29269.37548828125 
[2025-03-04 15:23:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.06207309663295746 norm:0.0002758415648713708 max memory_allocated 29269.37548828125 
[2025-03-04 15:23:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.06202821433544159 norm:0.00026402395451441407 max memory_allocated 29269.37548828125 
[2025-03-04 15:24:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.06202482432126999 norm:0.00026190862990915775 max memory_allocated 29269.37548828125 
[2025-03-04 15:25:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.062068045139312744 norm:0.0002582187589723617 max memory_allocated 29269.37548828125 
[2025-03-04 15:26:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.06199944391846657 norm:0.000245263654505834 max memory_allocated 29269.37548828125 
[2025-03-04 15:27:09 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.062021654099226 norm:0.00025086826644837856 max memory_allocated 29269.37548828125 
[2025-03-04 15:27:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.062078095972537994 norm:0.00024789076996967196 max memory_allocated 29269.37548828125 
[2025-03-04 15:28:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.06213895231485367 norm:0.0002523966250009835 max memory_allocated 29269.37548828125 
[2025-03-04 15:29:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.06218099594116211 norm:0.00025093459407798946 max memory_allocated 29269.37548828125 
[2025-03-04 15:29:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-04 15:30:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.10363024473190308 norm:0.005020216107368469 max memory_allocated 29269.56298828125 
[2025-03-04 15:31:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.08828970789909363 norm:0.0024359161034226418 max memory_allocated 29269.56298828125 
[2025-03-04 15:32:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.07395004481077194 norm:0.0007564632687717676 max memory_allocated 29269.56298828125 
[2025-03-04 15:33:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.07013179361820221 norm:0.00038596868398599327 max memory_allocated 29269.56298828125 
[2025-03-04 15:34:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.06863163411617279 norm:0.0003395059029571712 max memory_allocated 29269.56298828125 
[2025-03-04 15:34:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.06778541207313538 norm:0.00031102538923732936 max memory_allocated 29269.56298828125 
[2025-03-04 15:35:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.06726907193660736 norm:0.00029847753467038274 max memory_allocated 29269.56298828125 
[2025-03-04 15:36:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0669814795255661 norm:0.00028044587816111743 max memory_allocated 29269.56298828125 
[2025-03-04 15:37:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.06677651405334473 norm:0.0002675727300811559 max memory_allocated 29269.56298828125 
[2025-03-04 15:38:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.06670709699392319 norm:0.00026735253049992025 max memory_allocated 29269.56298828125 
[2025-03-04 15:38:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.06671285629272461 norm:0.00027311270241625607 max memory_allocated 29269.56298828125 
[2025-03-04 15:39:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.06671362370252609 norm:0.0002622086904011667 max memory_allocated 29269.56298828125 
[2025-03-04 15:40:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.06669962406158447 norm:0.00025457527954131365 max memory_allocated 29269.56298828125 
[2025-03-04 15:41:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.06664182245731354 norm:0.00024521054001525044 max memory_allocated 29269.56298828125 
[2025-03-04 15:42:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.06664395332336426 norm:0.00023498786322306842 max memory_allocated 29269.56298828125 
[2025-03-04 15:43:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.06660154461860657 norm:0.00022731770877726376 max memory_allocated 29269.56298828125 
[2025-03-04 15:43:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.06660721451044083 norm:0.00022229310707189143 max memory_allocated 29269.56298828125 
[2025-03-04 15:44:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06661462783813477 norm:0.00022199748491402715 max memory_allocated 29269.56298828125 
[2025-03-04 15:45:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.06659918278455734 norm:0.00021670004935003817 max memory_allocated 29269.56298828125 
[2025-03-04 15:46:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.06661181151866913 norm:0.00021851174824405462 max memory_allocated 29269.56298828125 
[2025-03-04 15:46:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-04 15:47:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.10530261695384979 norm:0.0059191640466451645 max memory_allocated 29269.75048828125 
[2025-03-04 15:48:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0905025526881218 norm:0.0026714568957686424 max memory_allocated 29269.75048828125 
[2025-03-04 15:49:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0786476731300354 norm:0.0009680010261945426 max memory_allocated 29269.75048828125 
[2025-03-04 15:50:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.07489128410816193 norm:0.0005417157080955803 max memory_allocated 29269.75048828125 
[2025-03-04 15:50:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.07342322915792465 norm:0.0004298041167203337 max memory_allocated 29269.75048828125 
[2025-03-04 15:51:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.07266703993082047 norm:0.00038865377428010106 max memory_allocated 29269.75048828125 
[2025-03-04 15:52:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.07217735797166824 norm:0.00036116709816269577 max memory_allocated 29269.75048828125 
[2025-03-04 15:53:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.07200506329536438 norm:0.00035234910319559276 max memory_allocated 29269.75048828125 
[2025-03-04 15:54:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.07188332080841064 norm:0.000330527953337878 max memory_allocated 29269.75048828125 
[2025-03-04 15:54:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.07177913188934326 norm:0.0003139665932394564 max memory_allocated 29269.75048828125 
[2025-03-04 15:55:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.07174677401781082 norm:0.0002924799046013504 max memory_allocated 29269.75048828125 
[2025-03-04 15:56:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.07171209901571274 norm:0.00028068918618373573 max memory_allocated 29269.75048828125 
[2025-03-04 15:57:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.07157588005065918 norm:0.00025669182650744915 max memory_allocated 29269.75048828125 
[2025-03-04 15:58:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0716819316148758 norm:0.00027222413336858153 max memory_allocated 29269.75048828125 
[2025-03-04 15:59:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.07179178297519684 norm:0.0002640889724716544 max memory_allocated 29269.75048828125 
[2025-03-04 15:59:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.07177131623029709 norm:0.0002570853685028851 max memory_allocated 29269.75048828125 
[2025-03-04 16:00:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.07190828025341034 norm:0.0002624277549330145 max memory_allocated 29269.75048828125 
[2025-03-04 16:01:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.07192566990852356 norm:0.000252505240496248 max memory_allocated 29269.75048828125 
[2025-03-04 16:02:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.07187873870134354 norm:0.00024178993771784008 max memory_allocated 29269.75048828125 
[2025-03-04 16:03:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.07174498587846756 norm:0.00022829665977042168 max memory_allocated 29269.75048828125 
[2025-03-04 16:03:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-04 16:04:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.11018817126750946 norm:0.0039016525261104107 max memory_allocated 29269.93798828125 
[2025-03-04 16:05:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.09662981331348419 norm:0.0019073807634413242 max memory_allocated 29269.93798828125 
[2025-03-04 16:05:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.08421941846609116 norm:0.0006868956843391061 max memory_allocated 29269.93798828125 
[2025-03-04 16:06:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.08066132664680481 norm:0.0003721165703609586 max memory_allocated 29269.93798828125 
[2025-03-04 16:07:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0793980285525322 norm:0.00032971525797620416 max memory_allocated 29269.93798828125 
[2025-03-04 16:08:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.07862595468759537 norm:0.0002941835846286267 max memory_allocated 29269.93798828125 
[2025-03-04 16:09:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.07816775888204575 norm:0.0002669176901690662 max memory_allocated 29269.93798828125 
[2025-03-04 16:10:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.07791933417320251 norm:0.0002513788058422506 max memory_allocated 29269.93798828125 
[2025-03-04 16:10:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0777900218963623 norm:0.00024539264268241823 max memory_allocated 29269.93798828125 
[2025-03-04 16:11:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.07776222378015518 norm:0.00024297868367284536 max memory_allocated 29269.93798828125 
[2025-03-04 16:12:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.07776745408773422 norm:0.00023924437118694186 max memory_allocated 29269.93798828125 
[2025-03-04 16:13:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.07780162990093231 norm:0.0002388215798418969 max memory_allocated 29269.93798828125 
[2025-03-04 16:14:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.07772749662399292 norm:0.000227269614697434 max memory_allocated 29269.93798828125 
[2025-03-04 16:15:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.07759515196084976 norm:0.0002154507819795981 max memory_allocated 29269.93798828125 
[2025-03-04 16:15:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.07753331959247589 norm:0.00020617512927856296 max memory_allocated 29269.93798828125 
[2025-03-04 16:16:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.07745633274316788 norm:0.0001976846542675048 max memory_allocated 29269.93798828125 
[2025-03-04 16:17:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0774274617433548 norm:0.00019219973182771355 max memory_allocated 29269.93798828125 
[2025-03-04 16:18:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.07745461910963058 norm:0.00019538638298399746 max memory_allocated 29269.93798828125 
[2025-03-04 16:19:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.07742999494075775 norm:0.00018803763668984175 max memory_allocated 29269.93798828125 
[2025-03-04 16:19:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.07744676619768143 norm:0.00018828909378498793 max memory_allocated 29269.93798828125 
[2025-03-04 16:20:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-04 16:21:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.1125374287366867 norm:0.0035242652520537376 max memory_allocated 29270.12548828125 
[2025-03-04 16:21:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.09914398193359375 norm:0.001481015351600945 max memory_allocated 29270.12548828125 
[2025-03-04 16:22:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0890476331114769 norm:0.000656063319183886 max memory_allocated 29270.12548828125 
[2025-03-04 16:23:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.08588874340057373 norm:0.0004192452470306307 max memory_allocated 29270.12548828125 
[2025-03-04 16:24:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.08457756787538528 norm:0.0003264378756284714 max memory_allocated 29270.12548828125 
[2025-03-04 16:25:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.08392364531755447 norm:0.000299744337098673 max memory_allocated 29270.12548828125 
[2025-03-04 16:26:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.08354459702968597 norm:0.00026517146034166217 max memory_allocated 29270.12548828125 
[2025-03-04 16:26:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.08325639367103577 norm:0.00023884832626208663 max memory_allocated 29270.12548828125 
[2025-03-04 16:27:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.08304785192012787 norm:0.00022128243290353566 max memory_allocated 29270.12548828125 
[2025-03-04 16:28:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0829453319311142 norm:0.00021515379194170237 max memory_allocated 29270.12548828125 
[2025-03-04 16:29:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.08288773149251938 norm:0.00021246298274490982 max memory_allocated 29270.12548828125 
[2025-03-04 16:30:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.08294658362865448 norm:0.00021876057144254446 max memory_allocated 29270.12548828125 
[2025-03-04 16:30:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.08295590430498123 norm:0.00020845337712671608 max memory_allocated 29270.12548828125 
[2025-03-04 16:31:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0829710140824318 norm:0.00020703596237581223 max memory_allocated 29270.12548828125 
[2025-03-04 16:32:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.0829189121723175 norm:0.00019980450451839715 max memory_allocated 29270.12548828125 
[2025-03-04 16:33:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.08286953717470169 norm:0.00019678936223499477 max memory_allocated 29270.12548828125 
[2025-03-04 16:34:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.08289402723312378 norm:0.00019575576880015433 max memory_allocated 29270.12548828125 
[2025-03-04 16:35:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.08287893980741501 norm:0.00018885913596022874 max memory_allocated 29270.12548828125 
[2025-03-04 16:35:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0828503668308258 norm:0.0001893372682388872 max memory_allocated 29270.12548828125 
[2025-03-04 16:36:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.08282776176929474 norm:0.00018491367518436164 max memory_allocated 29270.12548828125 
[2025-03-04 16:36:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-04 16:37:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.11901798844337463 norm:0.0031963312067091465 max memory_allocated 29270.31298828125 
[2025-03-04 16:38:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.10644469410181046 norm:0.0015565854264423251 max memory_allocated 29270.31298828125 
[2025-03-04 16:39:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.09597545117139816 norm:0.0006580672925338149 max memory_allocated 29270.31298828125 
[2025-03-04 16:40:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.09243354201316833 norm:0.00040940800681710243 max memory_allocated 29270.31298828125 
[2025-03-04 16:41:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.09089577943086624 norm:0.00033399980748072267 max memory_allocated 29270.31298828125 
[2025-03-04 16:41:58 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.08998659998178482 norm:0.00028251169715076685 max memory_allocated 29270.31298828125 
[2025-03-04 16:42:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.08949940651655197 norm:0.00027467665495350957 max memory_allocated 29270.31298828125 
[2025-03-04 16:43:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.08918416500091553 norm:0.00024942285381257534 max memory_allocated 29270.31298828125 
[2025-03-04 16:44:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.08901571482419968 norm:0.00023978902027010918 max memory_allocated 29270.31298828125 
[2025-03-04 16:45:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.08894123882055283 norm:0.00023527091252617538 max memory_allocated 29270.31298828125 
[2025-03-04 16:46:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.08890151977539062 norm:0.00023792305728420615 max memory_allocated 29270.31298828125 
[2025-03-04 16:46:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.08885759115219116 norm:0.00022818961588200182 max memory_allocated 29270.31298828125 
[2025-03-04 16:47:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.08879189938306808 norm:0.00021775337518192828 max memory_allocated 29270.31298828125 
[2025-03-04 16:48:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.08875882625579834 norm:0.00021460420975927263 max memory_allocated 29270.31298828125 
[2025-03-04 16:49:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.08872926235198975 norm:0.00021080896840430796 max memory_allocated 29270.31298828125 
[2025-03-04 16:50:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.08871262520551682 norm:0.0002026779402513057 max memory_allocated 29270.31298828125 
[2025-03-04 16:51:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.08868936449289322 norm:0.00020064010459464043 max memory_allocated 29270.31298828125 
[2025-03-04 16:51:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0886857807636261 norm:0.000201055096113123 max memory_allocated 29270.31298828125 
[2025-03-04 16:52:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.08875270187854767 norm:0.00020317720191087574 max memory_allocated 29270.31298828125 
[2025-03-04 16:53:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.08877018094062805 norm:0.00020170441712252796 max memory_allocated 29270.31298828125 
[2025-03-04 16:53:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-04 16:54:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.1339172124862671 norm:0.0054751671850681305 max memory_allocated 29270.50048828125 
[2025-03-04 16:55:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.11898094415664673 norm:0.0028198636136949062 max memory_allocated 29270.50048828125 
[2025-03-04 16:56:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.10431159287691116 norm:0.00117275130469352 max memory_allocated 29270.50048828125 
[2025-03-04 16:57:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.09980176389217377 norm:0.0006944859633222222 max memory_allocated 29270.50048828125 
[2025-03-04 16:57:56 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.09782497584819794 norm:0.0005604164325632155 max memory_allocated 29270.50048828125 
[2025-03-04 16:58:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.09672607481479645 norm:0.00048116815742105246 max memory_allocated 29270.50048828125 
[2025-03-04 16:59:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.09597361087799072 norm:0.0003976586158387363 max memory_allocated 29270.50048828125 
[2025-03-04 17:00:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.09553319215774536 norm:0.00033794197952374816 max memory_allocated 29270.50048828125 
[2025-03-04 17:01:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.09530338644981384 norm:0.00030206734663806856 max memory_allocated 29270.50048828125 
[2025-03-04 17:02:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.09517727792263031 norm:0.0002878289087675512 max memory_allocated 29270.50048828125 
[2025-03-04 17:02:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.09510867297649384 norm:0.00028067853418178856 max memory_allocated 29270.50048828125 
[2025-03-04 17:03:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.09505587816238403 norm:0.00027400910039432347 max memory_allocated 29270.50048828125 
[2025-03-04 17:04:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.09504101425409317 norm:0.0002709749969653785 max memory_allocated 29270.50048828125 
[2025-03-04 17:05:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.09499040246009827 norm:0.00025415437994524837 max memory_allocated 29270.50048828125 
[2025-03-04 17:06:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.09485506266355515 norm:0.00023210383369587362 max memory_allocated 29270.50048828125 
[2025-03-04 17:06:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.09485422819852829 norm:0.0002347484405618161 max memory_allocated 29270.50048828125 
[2025-03-04 17:07:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.09480796009302139 norm:0.00022977434855420142 max memory_allocated 29270.50048828125 
[2025-03-04 17:08:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.09472870081663132 norm:0.0002156794653274119 max memory_allocated 29270.50048828125 
[2025-03-04 17:09:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.09471587091684341 norm:0.00021674259915016592 max memory_allocated 29270.50048828125 
[2025-03-04 17:10:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.09470085054636002 norm:0.0002151275984942913 max memory_allocated 29270.50048828125 
[2025-03-04 17:10:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-04 17:11:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.15284942090511322 norm:0.015483839437365532 max memory_allocated 29270.68798828125 
[2025-03-04 17:12:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.13313670456409454 norm:0.006868958938866854 max memory_allocated 29270.68798828125 
[2025-03-04 17:13:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.11684642732143402 norm:0.0028661361429840326 max memory_allocated 29270.68798828125 
[2025-03-04 17:13:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.1101105809211731 norm:0.001076013082638383 max memory_allocated 29270.68798828125 
[2025-03-04 17:14:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.10762330144643784 norm:0.0008083785069175065 max memory_allocated 29270.68798828125 
[2025-03-04 17:15:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.10627230256795883 norm:0.0007114379550330341 max memory_allocated 29270.68798828125 
[2025-03-04 17:16:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.10543784499168396 norm:0.0006283879047259688 max memory_allocated 29270.68798828125 
[2025-03-04 17:17:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.10490308701992035 norm:0.0005570128560066223 max memory_allocated 29270.68798828125 
[2025-03-04 17:18:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.10452508926391602 norm:0.0005160197615623474 max memory_allocated 29270.68798828125 
[2025-03-04 17:18:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.10439024120569229 norm:0.0005292263231240213 max memory_allocated 29270.68798828125 
[2025-03-04 17:19:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.10437542200088501 norm:0.0005088289617560804 max memory_allocated 29270.68798828125 
[2025-03-04 17:20:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.10408858954906464 norm:0.0003947727382183075 max memory_allocated 29270.68798828125 
[2025-03-04 17:21:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.1039317324757576 norm:0.00038066564593464136 max memory_allocated 29270.68798828125 
[2025-03-04 17:22:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.10396343469619751 norm:0.0003853423404507339 max memory_allocated 29270.68798828125 
[2025-03-04 17:22:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.1038474440574646 norm:0.0003518211015034467 max memory_allocated 29270.68798828125 
[2025-03-04 17:23:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.10384760051965714 norm:0.00036717246985062957 max memory_allocated 29270.68798828125 
[2025-03-04 17:24:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.10376668721437454 norm:0.00033960436121560633 max memory_allocated 29270.68798828125 
[2025-03-04 17:25:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.10367435216903687 norm:0.00031902632326819 max memory_allocated 29270.68798828125 
[2025-03-04 17:26:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.10361376404762268 norm:0.0002988850465044379 max memory_allocated 29270.68798828125 
[2025-03-04 17:27:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.10359157621860504 norm:0.00029978694510646164 max memory_allocated 29270.68798828125 
[2025-03-04 17:27:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-04 17:28:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.1441149115562439 norm:0.005829259287565947 max memory_allocated 29270.87548828125 
[2025-03-04 17:29:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.1316712200641632 norm:0.0032664728350937366 max memory_allocated 29270.87548828125 
[2025-03-04 17:29:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.11917074769735336 norm:0.0011636579874902964 max memory_allocated 29270.87548828125 
[2025-03-04 17:30:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.1152135357260704 norm:0.0005260645993985236 max memory_allocated 29270.87548828125 
[2025-03-04 17:31:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.11347531527280807 norm:0.00040232494939118624 max memory_allocated 29270.87548828125 
[2025-03-04 17:32:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.11252975463867188 norm:0.00035131449112668633 max memory_allocated 29270.87548828125 
[2025-03-04 17:33:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.111972376704216 norm:0.0003098305023740977 max memory_allocated 29270.87548828125 
[2025-03-04 17:33:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.1115620955824852 norm:0.0002677521842997521 max memory_allocated 29270.87548828125 
[2025-03-04 17:34:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.11145781725645065 norm:0.00027730665169656277 max memory_allocated 29270.87548828125 
[2025-03-04 17:35:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.11143386363983154 norm:0.0002931212948169559 max memory_allocated 29270.87548828125 
[2025-03-04 17:36:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.11131416261196136 norm:0.0002549914352130145 max memory_allocated 29270.87548828125 
[2025-03-04 17:37:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.11130028963088989 norm:0.00026128889294341207 max memory_allocated 29270.87548828125 
[2025-03-04 17:38:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.11129257082939148 norm:0.00026011193403974175 max memory_allocated 29270.87548828125 
[2025-03-04 17:38:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.11117283254861832 norm:0.00024162601039279252 max memory_allocated 29270.87548828125 
[2025-03-04 17:39:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.11107733100652695 norm:0.00022812833776697516 max memory_allocated 29270.87548828125 
[2025-03-04 17:40:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.11107476055622101 norm:0.0002281148626934737 max memory_allocated 29270.87548828125 
[2025-03-04 17:41:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.11110535264015198 norm:0.00022809994698036462 max memory_allocated 29270.87548828125 
[2025-03-04 17:42:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.11106488108634949 norm:0.00022016749426256865 max memory_allocated 29270.87548828125 
[2025-03-04 17:43:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.11099112033843994 norm:0.0002145000617019832 max memory_allocated 29270.87548828125 
[2025-03-04 17:43:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.1109427958726883 norm:0.00020876443886663765 max memory_allocated 29270.87548828125 
[2025-03-04 17:44:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-04 17:44:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.15137521922588348 norm:0.004163796082139015 max memory_allocated 29271.06298828125 
[2025-03-04 17:45:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.14036740362644196 norm:0.002434393623843789 max memory_allocated 29271.06298828125 
[2025-03-04 17:46:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.12841486930847168 norm:0.0008996159886009991 max memory_allocated 29271.06298828125 
[2025-03-04 17:47:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.1246066465973854 norm:0.00047051176079548895 max memory_allocated 29271.06298828125 
[2025-03-04 17:48:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.12306766957044601 norm:0.0004097526252735406 max memory_allocated 29271.06298828125 
[2025-03-04 17:49:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.12219774723052979 norm:0.00037211162270978093 max memory_allocated 29271.06298828125 
[2025-03-04 17:49:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.12163569778203964 norm:0.0003098052111454308 max memory_allocated 29271.06298828125 
[2025-03-04 17:50:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.12140213698148727 norm:0.0002996370894834399 max memory_allocated 29271.06298828125 
[2025-03-04 17:51:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.12131048738956451 norm:0.00029487465508282185 max memory_allocated 29271.06298828125 
[2025-03-04 17:52:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.12118639796972275 norm:0.00028233390185050666 max memory_allocated 29271.06298828125 
[2025-03-04 17:53:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.12104640901088715 norm:0.00026011219597421587 max memory_allocated 29271.06298828125 
[2025-03-04 17:54:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.12086869776248932 norm:0.00023523849085904658 max memory_allocated 29271.06298828125 
[2025-03-04 17:54:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.12078122794628143 norm:0.00022750059724785388 max memory_allocated 29271.06298828125 
[2025-03-04 17:55:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.12075500190258026 norm:0.00022242713021114469 max memory_allocated 29271.06298828125 
[2025-03-04 17:56:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.12076607346534729 norm:0.0002227331860922277 max memory_allocated 29271.06298828125 
[2025-03-04 17:57:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.12077488005161285 norm:0.00022052746498957276 max memory_allocated 29271.06298828125 
[2025-03-04 17:58:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.12069594860076904 norm:0.00021142938931006938 max memory_allocated 29271.06298828125 
[2025-03-04 17:58:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.12063819169998169 norm:0.00020698529260698706 max memory_allocated 29271.06298828125 
[2025-03-04 17:59:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.12057788670063019 norm:0.00020118473912589252 max memory_allocated 29271.06298828125 
[2025-03-04 18:00:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.12053868174552917 norm:0.00019987211271654814 max memory_allocated 29271.06298828125 
[2025-03-04 18:00:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-04 18:01:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.1776534616947174 norm:0.009371207095682621 max memory_allocated 29271.25048828125 
[2025-03-04 18:02:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.16111509501934052 norm:0.004706396721303463 max memory_allocated 29271.25048828125 
[2025-03-04 18:03:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.14501261711120605 norm:0.0018236904870718718 max memory_allocated 29271.25048828125 
[2025-03-04 18:04:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.13993693888187408 norm:0.000998924719169736 max memory_allocated 29271.25048828125 
[2025-03-04 18:05:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.13783833384513855 norm:0.0008247699588537216 max memory_allocated 29271.25048828125 
[2025-03-04 18:05:51 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.13670912384986877 norm:0.0007045823731459677 max memory_allocated 29271.25048828125 
[2025-03-04 18:06:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.13606205582618713 norm:0.0006098666926845908 max memory_allocated 29271.25048828125 
[2025-03-04 18:07:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.13562023639678955 norm:0.0005048484308645129 max memory_allocated 29271.25048828125 
[2025-03-04 18:08:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.13529305160045624 norm:0.0004090667935088277 max memory_allocated 29271.25048828125 
[2025-03-04 18:09:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.13505807518959045 norm:0.00033883863943628967 max memory_allocated 29271.25048828125 
[2025-03-04 18:09:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.1349101960659027 norm:0.000326737470459193 max memory_allocated 29271.25048828125 
[2025-03-04 18:10:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.1348249614238739 norm:0.0003244312247261405 max memory_allocated 29271.25048828125 
[2025-03-04 18:11:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.1347832977771759 norm:0.0003133337595500052 max memory_allocated 29271.25048828125 
[2025-03-04 18:12:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.13469824194908142 norm:0.00029417520272545516 max memory_allocated 29271.25048828125 
[2025-03-04 18:13:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.13461297750473022 norm:0.00028691740590147674 max memory_allocated 29271.25048828125 
[2025-03-04 18:14:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.13457676768302917 norm:0.00028253733762539923 max memory_allocated 29271.25048828125 
[2025-03-04 18:14:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.13453249633312225 norm:0.0002752433065325022 max memory_allocated 29271.25048828125 
[2025-03-04 18:15:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.13445840775966644 norm:0.00026060669915750623 max memory_allocated 29271.25048828125 
[2025-03-04 18:16:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.13440856337547302 norm:0.0002559247659519315 max memory_allocated 29271.25048828125 
[2025-03-04 18:17:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.13435040414333344 norm:0.0002521737478673458 max memory_allocated 29271.25048828125 
[2025-03-04 18:17:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-04 18:18:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.1832941174507141 norm:0.005067423451691866 max memory_allocated 29271.43798828125 
[2025-03-04 18:19:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.17090485990047455 norm:0.0027473680675029755 max memory_allocated 29271.43798828125 
[2025-03-04 18:20:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.15877549350261688 norm:0.0011011278256773949 max memory_allocated 29271.43798828125 
[2025-03-04 18:20:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.1545853465795517 norm:0.0005455851787701249 max memory_allocated 29271.43798828125 
[2025-03-04 18:21:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.1526576727628708 norm:0.00039827899308875203 max memory_allocated 29271.43798828125 
[2025-03-04 18:22:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.15185685455799103 norm:0.00038121090619824827 max memory_allocated 29271.43798828125 
[2025-03-04 18:23:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.15152007341384888 norm:0.0003710763994604349 max memory_allocated 29271.43798828125 
[2025-03-04 18:24:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.15129391849040985 norm:0.00034601721563376486 max memory_allocated 29271.43798828125 
[2025-03-04 18:25:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.15102671086788177 norm:0.0003098959568887949 max memory_allocated 29271.43798828125 
[2025-03-04 18:25:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.15089645981788635 norm:0.00030224118381738663 max memory_allocated 29271.43798828125 
[2025-03-04 18:26:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.1507970094680786 norm:0.00029019638895988464 max memory_allocated 29271.43798828125 
[2025-03-04 18:27:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.1507360190153122 norm:0.0002775992907118052 max memory_allocated 29271.43798828125 
[2025-03-04 18:28:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.15064147114753723 norm:0.00027567544020712376 max memory_allocated 29271.43798828125 
[2025-03-04 18:29:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.1505611538887024 norm:0.00025748752523213625 max memory_allocated 29271.43798828125 
[2025-03-04 18:30:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.1504746675491333 norm:0.0002524434821680188 max memory_allocated 29271.43798828125 
[2025-03-04 18:30:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.15041083097457886 norm:0.00023630543728359044 max memory_allocated 29271.43798828125 
[2025-03-04 18:31:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.15035270154476166 norm:0.00023498974042013288 max memory_allocated 29271.43798828125 
[2025-03-04 18:32:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.1503603756427765 norm:0.00023674324620515108 max memory_allocated 29271.43798828125 
[2025-03-04 18:33:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.15030573308467865 norm:0.00022916353191249073 max memory_allocated 29271.43798828125 
[2025-03-04 18:34:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.15028738975524902 norm:0.00023113616043701768 max memory_allocated 29271.43798828125 
[2025-03-04 18:34:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-04 18:35:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.20741575956344604 norm:0.0071775964461266994 max memory_allocated 29271.62548828125 
[2025-03-04 18:36:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.1915881335735321 norm:0.003909643739461899 max memory_allocated 29271.62548828125 
[2025-03-04 18:36:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.17708386480808258 norm:0.0018107980722561479 max memory_allocated 29271.62548828125 
[2025-03-04 18:37:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.1720484346151352 norm:0.0009875097312033176 max memory_allocated 29271.62548828125 
[2025-03-04 18:38:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.16999906301498413 norm:0.0007645564619451761 max memory_allocated 29271.62548828125 
[2025-03-04 18:39:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.1687801629304886 norm:0.0005397432250902057 max memory_allocated 29271.62548828125 
[2025-03-04 18:40:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.16822218894958496 norm:0.0004070524300914258 max memory_allocated 29271.62548828125 
[2025-03-04 18:41:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.16795483231544495 norm:0.0003887042694259435 max memory_allocated 29271.62548828125 
[2025-03-04 18:41:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.1677366942167282 norm:0.000363275408744812 max memory_allocated 29271.62548828125 
[2025-03-04 18:42:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.16753503680229187 norm:0.00033622977207414806 max memory_allocated 29271.62548828125 
[2025-03-04 18:43:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.16738095879554749 norm:0.00031704443972557783 max memory_allocated 29271.62548828125 
[2025-03-04 18:44:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.16730734705924988 norm:0.00031156890327110887 max memory_allocated 29271.62548828125 
[2025-03-04 18:45:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.1672232747077942 norm:0.00030150022939778864 max memory_allocated 29271.62548828125 
[2025-03-04 18:45:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.16711361706256866 norm:0.000286812981357798 max memory_allocated 29271.62548828125 
[2025-03-04 18:46:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.1670355498790741 norm:0.0002813345054164529 max memory_allocated 29271.62548828125 
[2025-03-04 18:47:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.16699057817459106 norm:0.0002760728821158409 max memory_allocated 29271.62548828125 
[2025-03-04 18:48:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.16688138246536255 norm:0.00025883459602482617 max memory_allocated 29271.62548828125 
[2025-03-04 18:49:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.1668165773153305 norm:0.00024937576381489635 max memory_allocated 29271.62548828125 
[2025-03-04 18:50:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.1667512208223343 norm:0.00024263311934191734 max memory_allocated 29271.62548828125 
[2025-03-04 18:50:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.16673880815505981 norm:0.0002424243721179664 max memory_allocated 29271.62548828125 
[2025-03-04 18:51:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-04 18:52:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.22595159709453583 norm:0.005884752608835697 max memory_allocated 29271.81298828125 
[2025-03-04 18:52:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.2116372287273407 norm:0.003254072042182088 max memory_allocated 29271.81298828125 
[2025-03-04 18:53:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.19800888001918793 norm:0.0013693240471184254 max memory_allocated 29271.81298828125 
[2025-03-04 18:54:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.19348038733005524 norm:0.0006690735463052988 max memory_allocated 29271.81298828125 
[2025-03-04 18:55:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.19170048832893372 norm:0.0005834283656440675 max memory_allocated 29271.81298828125 
[2025-03-04 18:56:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.1909150332212448 norm:0.0005168355419300497 max memory_allocated 29271.81298828125 
[2025-03-04 18:56:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.19055460393428802 norm:0.0004779283481184393 max memory_allocated 29271.81298828125 
[2025-03-04 18:57:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.19023877382278442 norm:0.0004268308402970433 max memory_allocated 29271.81298828125 
[2025-03-04 18:58:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.19003257155418396 norm:0.00040876385173760355 max memory_allocated 29271.81298828125 
[2025-03-04 18:59:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.18986772000789642 norm:0.00036872929194942117 max memory_allocated 29271.81298828125 
[2025-03-04 19:00:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.1897720992565155 norm:0.0003609064151532948 max memory_allocated 29271.81298828125 
[2025-03-04 19:01:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.1895470917224884 norm:0.00031291425693780184 max memory_allocated 29271.81298828125 
[2025-03-04 19:01:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.18945792317390442 norm:0.0003066123172175139 max memory_allocated 29271.81298828125 
[2025-03-04 19:02:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.18943262100219727 norm:0.00030191687983460724 max memory_allocated 29271.81298828125 
[2025-03-04 19:03:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.18932853639125824 norm:0.0002748807892203331 max memory_allocated 29271.81298828125 
[2025-03-04 19:04:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.18921034038066864 norm:0.000264100031927228 max memory_allocated 29271.81298828125 
[2025-03-04 19:05:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.18915915489196777 norm:0.00026219768915325403 max memory_allocated 29271.81298828125 
[2025-03-04 19:06:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.18918175995349884 norm:0.0002618820290081203 max memory_allocated 29271.81298828125 
[2025-03-04 19:06:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.18914617598056793 norm:0.00025065019144676626 max memory_allocated 29271.81298828125 
[2025-03-04 19:07:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.189121276140213 norm:0.00024444732116535306 max memory_allocated 29271.81298828125 
[2025-03-04 19:07:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-04 19:08:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.25401657819747925 norm:0.0046705035492777824 max memory_allocated 29272.00048828125 
[2025-03-04 19:09:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.23871193826198578 norm:0.0024679875932633877 max memory_allocated 29272.00048828125 
[2025-03-04 19:10:28 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.2245478332042694 norm:0.001077893772162497 max memory_allocated 29272.00048828125 
[2025-03-04 19:11:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.22022201120853424 norm:0.0005606784834526479 max memory_allocated 29272.00048828125 
[2025-03-04 19:12:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.2183384746313095 norm:0.00046625721734017134 max memory_allocated 29272.00048828125 
[2025-03-04 19:12:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.2175736427307129 norm:0.0004378778103273362 max memory_allocated 29272.00048828125 
[2025-03-04 19:13:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.2172079086303711 norm:0.0004111412854399532 max memory_allocated 29272.00048828125 
[2025-03-04 19:14:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.21691305935382843 norm:0.00037989328848198056 max memory_allocated 29272.00048828125 
[2025-03-04 19:15:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.21665985882282257 norm:0.00035186996683478355 max memory_allocated 29272.00048828125 
[2025-03-04 19:16:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.2165062129497528 norm:0.00034428443177603185 max memory_allocated 29272.00048828125 
[2025-03-04 19:17:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.21641722321510315 norm:0.0003329628671053797 max memory_allocated 29272.00048828125 
[2025-03-04 19:17:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.21626904606819153 norm:0.0003166019741911441 max memory_allocated 29272.00048828125 
[2025-03-04 19:18:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.21616046130657196 norm:0.00030139301088638604 max memory_allocated 29272.00048828125 
[2025-03-04 19:19:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.21610596776008606 norm:0.00029766507213935256 max memory_allocated 29272.00048828125 
[2025-03-04 19:20:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.21599115431308746 norm:0.00028241699328646064 max memory_allocated 29272.00048828125 
[2025-03-04 19:21:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.21588274836540222 norm:0.0002762064686976373 max memory_allocated 29272.00048828125 
[2025-03-04 19:22:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.21585872769355774 norm:0.00027614436112344265 max memory_allocated 29272.00048828125 
[2025-03-04 19:22:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.2158324122428894 norm:0.0002701242046896368 max memory_allocated 29272.00048828125 
[2025-03-04 19:23:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.21577520668506622 norm:0.00026323169004172087 max memory_allocated 29272.00048828125 
[2025-03-04 19:24:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.21568259596824646 norm:0.00025490790721960366 max memory_allocated 29272.00048828125 
[2025-03-04 19:24:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-04 19:25:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.28780287504196167 norm:0.004927324131131172 max memory_allocated 29272.18798828125 
[2025-03-04 19:26:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.2723289728164673 norm:0.0028147578705102205 max memory_allocated 29272.18798828125 
[2025-03-04 19:27:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.2578456401824951 norm:0.0014181934529915452 max memory_allocated 29272.18798828125 
[2025-03-04 19:28:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.2531169056892395 norm:0.0007098733331076801 max memory_allocated 29272.18798828125 
[2025-03-04 19:28:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.2511364221572876 norm:0.0005732437130063772 max memory_allocated 29272.18798828125 
[2025-03-04 19:29:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.2503422796726227 norm:0.0005270656547509134 max memory_allocated 29272.18798828125 
[2025-03-04 19:30:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.24993552267551422 norm:0.0004987470456399024 max memory_allocated 29272.18798828125 
[2025-03-04 19:31:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.24959123134613037 norm:0.00045282745850272477 max memory_allocated 29272.18798828125 
[2025-03-04 19:32:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.249271959066391 norm:0.0004144287668168545 max memory_allocated 29272.18798828125 
[2025-03-04 19:33:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.24909460544586182 norm:0.00040538574103266 max memory_allocated 29272.18798828125 
[2025-03-04 19:33:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.24892590939998627 norm:0.0003858883574139327 max memory_allocated 29272.18798828125 
[2025-03-04 19:34:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.24872413277626038 norm:0.00036168465157970786 max memory_allocated 29272.18798828125 
[2025-03-04 19:35:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.2486049383878708 norm:0.00034646823769435287 max memory_allocated 29272.18798828125 
[2025-03-04 19:36:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.24850697815418243 norm:0.00033478395198471844 max memory_allocated 29272.18798828125 
[2025-03-04 19:37:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.24843865633010864 norm:0.0003208141424693167 max memory_allocated 29272.18798828125 
[2025-03-04 19:38:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.2483682632446289 norm:0.00030910546774975955 max memory_allocated 29272.18798828125 
[2025-03-04 19:38:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.2482980489730835 norm:0.00030333586619235575 max memory_allocated 29272.18798828125 
[2025-03-04 19:39:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.24821262061595917 norm:0.0002964856685139239 max memory_allocated 29272.18798828125 
[2025-03-04 19:40:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.24818603694438934 norm:0.0002874854835681617 max memory_allocated 29272.18798828125 
[2025-03-04 19:41:18 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.24808475375175476 norm:0.00028176826890558004 max memory_allocated 29272.18798828125 
[2025-03-04 19:41:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-04 19:42:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.3134453296661377 norm:0.0034323842264711857 max memory_allocated 29272.37548828125 
[2025-03-04 19:43:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.30004996061325073 norm:0.001994671067222953 max memory_allocated 29272.37548828125 
[2025-03-04 19:44:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.28753843903541565 norm:0.0011162698501721025 max memory_allocated 29272.37548828125 
[2025-03-04 19:44:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.28321823477745056 norm:0.0006586736417375505 max memory_allocated 29272.37548828125 
[2025-03-04 19:45:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.2814122438430786 norm:0.0005543380975723267 max memory_allocated 29272.37548828125 
[2025-03-04 19:46:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.2807469666004181 norm:0.00045559590216726065 max memory_allocated 29272.37548828125 
[2025-03-04 19:47:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.2803129255771637 norm:0.000398555479478091 max memory_allocated 29272.37548828125 
[2025-03-04 19:48:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.2799740433692932 norm:0.00035800671321339905 max memory_allocated 29272.37548828125 
[2025-03-04 19:49:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.2796934247016907 norm:0.00030885220621712506 max memory_allocated 29272.37548828125 
[2025-03-04 19:49:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.27956342697143555 norm:0.0002959545236080885 max memory_allocated 29272.37548828125 
[2025-03-04 19:50:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.2795267701148987 norm:0.0002925352018792182 max memory_allocated 29272.37548828125 
[2025-03-04 19:51:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.2794247567653656 norm:0.00027979176957160234 max memory_allocated 29272.37548828125 
[2025-03-04 19:52:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.27931031584739685 norm:0.0002748075348790735 max memory_allocated 29272.37548828125 
[2025-03-04 19:53:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.2792457044124603 norm:0.0002679045719560236 max memory_allocated 29272.37548828125 
[2025-03-04 19:53:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.27925989031791687 norm:0.00026627740589901805 max memory_allocated 29272.37548828125 
[2025-03-04 19:54:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.27912792563438416 norm:0.0002552512160036713 max memory_allocated 29272.37548828125 
[2025-03-04 19:55:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.27903082966804504 norm:0.00024801515974104404 max memory_allocated 29272.37548828125 
[2025-03-04 19:56:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.27899253368377686 norm:0.00024246569955721498 max memory_allocated 29272.37548828125 
[2025-03-04 19:57:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.2789418697357178 norm:0.00023911308380775154 max memory_allocated 29272.37548828125 
[2025-03-04 19:58:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.2789120674133301 norm:0.00023769823019392788 max memory_allocated 29272.37548828125 
[2025-03-04 19:58:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-04 19:59:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.35833626985549927 norm:0.006303497590124607 max memory_allocated 29272.56298828125 
[2025-03-04 20:00:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.34116506576538086 norm:0.003456642385572195 max memory_allocated 29272.56298828125 
[2025-03-04 20:00:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.3245461881160736 norm:0.0016289702616631985 max memory_allocated 29272.56298828125 
[2025-03-04 20:01:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.31938180327415466 norm:0.0008742787758819759 max memory_allocated 29272.56298828125 
[2025-03-04 20:02:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.317084938287735 norm:0.0004697625699918717 max memory_allocated 29272.56298828125 
[2025-03-04 20:03:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.31633490324020386 norm:0.0004657043609768152 max memory_allocated 29272.56298828125 
[2025-03-04 20:04:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.31595709919929504 norm:0.00044948450522497296 max memory_allocated 29272.56298828125 
[2025-03-04 20:05:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.31572505831718445 norm:0.0004205138538964093 max memory_allocated 29272.56298828125 
[2025-03-04 20:05:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.31552648544311523 norm:0.00039395250496454537 max memory_allocated 29272.56298828125 
[2025-03-04 20:06:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.3153267204761505 norm:0.00034734781365841627 max memory_allocated 29272.56298828125 
[2025-03-04 20:07:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.3151710629463196 norm:0.00034668506123125553 max memory_allocated 29272.56298828125 
[2025-03-04 20:08:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.31504306197166443 norm:0.0003437254927121103 max memory_allocated 29272.56298828125 
[2025-03-04 20:09:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.31493255496025085 norm:0.0003323331184219569 max memory_allocated 29272.56298828125 
[2025-03-04 20:09:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.31483733654022217 norm:0.00031360003049485385 max memory_allocated 29272.56298828125 
[2025-03-04 20:10:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.3147924542427063 norm:0.0003209617570973933 max memory_allocated 29272.56298828125 
[2025-03-04 20:11:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.3146952986717224 norm:0.0003019314899574965 max memory_allocated 29272.56298828125 
[2025-03-04 20:12:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.3146240711212158 norm:0.00029695985722355545 max memory_allocated 29272.56298828125 
[2025-03-04 20:13:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.3145884871482849 norm:0.00028911413392052054 max memory_allocated 29272.56298828125 
[2025-03-04 20:14:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.3145652711391449 norm:0.00028729013865813613 max memory_allocated 29272.56298828125 
[2025-03-04 20:14:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.31451618671417236 norm:0.0002814617473632097 max memory_allocated 29272.56298828125 
[2025-03-04 20:15:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-04 20:16:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.38836222887039185 norm:0.0031802388839423656 max memory_allocated 29272.75048828125 
[2025-03-04 20:16:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.37341970205307007 norm:0.0018076514825224876 max memory_allocated 29272.75048828125 
[2025-03-04 20:17:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.35842183232307434 norm:0.000871696975082159 max memory_allocated 29272.75048828125 
[2025-03-04 20:18:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.35395753383636475 norm:0.0005164812901057303 max memory_allocated 29272.75048828125 
[2025-03-04 20:19:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.35236871242523193 norm:0.0004297509731259197 max memory_allocated 29272.75048828125 
[2025-03-04 20:20:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.3518535792827606 norm:0.0003888100618496537 max memory_allocated 29272.75048828125 
[2025-03-04 20:20:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.3515058755874634 norm:0.0003542849444784224 max memory_allocated 29272.75048828125 
[2025-03-04 20:21:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.35123157501220703 norm:0.0003263890102971345 max memory_allocated 29272.75048828125 
[2025-03-04 20:22:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.35103803873062134 norm:0.0003057930734939873 max memory_allocated 29272.75048828125 
[2025-03-04 20:23:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.3508518934249878 norm:0.0002872873446904123 max memory_allocated 29272.75048828125 
[2025-03-04 20:24:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.350722074508667 norm:0.0002763482043519616 max memory_allocated 29272.75048828125 
[2025-03-04 20:25:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.35060563683509827 norm:0.00026728800730779767 max memory_allocated 29272.75048828125 
[2025-03-04 20:25:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.35049745440483093 norm:0.00025665771681815386 max memory_allocated 29272.75048828125 
[2025-03-04 20:26:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.350401908159256 norm:0.00024889345513656735 max memory_allocated 29272.75048828125 
[2025-03-04 20:27:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.3503303527832031 norm:0.00024454452795907855 max memory_allocated 29272.75048828125 
[2025-03-04 20:28:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.3502640128135681 norm:0.0002390687441220507 max memory_allocated 29272.75048828125 
[2025-03-04 20:29:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.3502129912376404 norm:0.00023271939426194876 max memory_allocated 29272.75048828125 
[2025-03-04 20:30:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.35016492009162903 norm:0.00022865168284624815 max memory_allocated 29272.75048828125 
[2025-03-04 20:30:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.35009831190109253 norm:0.0002252861304441467 max memory_allocated 29272.75048828125 
[2025-03-04 20:31:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.35003677010536194 norm:0.00022294168593361974 max memory_allocated 29272.75048828125 
[2025-03-04 20:31:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-04 20:32:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.43254512548446655 norm:0.00225795223377645 max memory_allocated 29272.93798828125 
[2025-03-04 20:33:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.4170980155467987 norm:0.0014962940476834774 max memory_allocated 29272.93798828125 
[2025-03-04 20:34:30 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.4010773003101349 norm:0.0008280885522253811 max memory_allocated 29272.93798828125 
[2025-03-04 20:35:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.39625927805900574 norm:0.00047668194747529924 max memory_allocated 29272.93798828125 
[2025-03-04 20:36:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.3944578766822815 norm:0.0004054008168168366 max memory_allocated 29272.93798828125 
[2025-03-04 20:36:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.3938322961330414 norm:0.0003609111299738288 max memory_allocated 29272.93798828125 
[2025-03-04 20:37:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.39349472522735596 norm:0.0003405101306270808 max memory_allocated 29272.93798828125 
[2025-03-04 20:38:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.393237829208374 norm:0.00032476053456775844 max memory_allocated 29272.93798828125 
[2025-03-04 20:39:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.3930356502532959 norm:0.0003062596661038697 max memory_allocated 29272.93798828125 
[2025-03-04 20:40:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.39285755157470703 norm:0.00029659466235898435 max memory_allocated 29272.93798828125 
[2025-03-04 20:41:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.3927236497402191 norm:0.0002844051050487906 max memory_allocated 29272.93798828125 
[2025-03-04 20:41:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.3925844132900238 norm:0.00027455613599158823 max memory_allocated 29272.93798828125 
[2025-03-04 20:42:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.392482191324234 norm:0.00026731533580459654 max memory_allocated 29272.93798828125 
[2025-03-04 20:43:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.39237308502197266 norm:0.0002588456845842302 max memory_allocated 29272.93798828125 
[2025-03-04 20:44:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.39227670431137085 norm:0.00025286158779636025 max memory_allocated 29272.93798828125 
[2025-03-04 20:45:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.39220374822616577 norm:0.00024555035633966327 max memory_allocated 29272.93798828125 
[2025-03-04 20:46:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.39211803674697876 norm:0.00024233708973042667 max memory_allocated 29272.93798828125 
[2025-03-04 20:46:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.3920450210571289 norm:0.00023976438387762755 max memory_allocated 29272.93798828125 
[2025-03-04 20:47:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.39196711778640747 norm:0.0002365725376876071 max memory_allocated 29272.93798828125 
[2025-03-04 20:48:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.39193862676620483 norm:0.00023639941355213523 max memory_allocated 29272.93798828125 
[2025-03-04 20:48:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-04 20:49:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.4845949411392212 norm:0.007611233275383711 max memory_allocated 29273.12548828125 
[2025-03-04 20:50:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.46402856707572937 norm:0.00400138646364212 max memory_allocated 29273.12548828125 
[2025-03-04 20:51:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.44565215706825256 norm:0.002184485550969839 max memory_allocated 29273.12548828125 
[2025-03-04 20:52:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.4401327073574066 norm:0.0012893684906885028 max memory_allocated 29273.12548828125 
[2025-03-04 20:52:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.4381961226463318 norm:0.0009351255721412599 max memory_allocated 29273.12548828125 
[2025-03-04 20:53:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.4374566972255707 norm:0.0008397914352826774 max memory_allocated 29273.12548828125 
[2025-03-04 20:54:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.43693792819976807 norm:0.0006969103123992682 max memory_allocated 29273.12548828125 
[2025-03-04 20:55:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.43649110198020935 norm:0.0005661023315042257 max memory_allocated 29273.12548828125 
[2025-03-04 20:56:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.4361812472343445 norm:0.0005171624361537397 max memory_allocated 29273.12548828125 
[2025-03-04 20:57:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.4359781742095947 norm:0.0004897292237728834 max memory_allocated 29273.12548828125 
[2025-03-04 20:57:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.43583837151527405 norm:0.000497329281643033 max memory_allocated 29273.12548828125 
[2025-03-04 20:58:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.43576234579086304 norm:0.000489081721752882 max memory_allocated 29273.12548828125 
[2025-03-04 20:59:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.4356347918510437 norm:0.00047447564429603517 max memory_allocated 29273.12548828125 
[2025-03-04 21:00:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.4355148673057556 norm:0.0004279774730093777 max memory_allocated 29273.12548828125 
[2025-03-04 21:01:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.4353888928890228 norm:0.000410514505347237 max memory_allocated 29273.12548828125 
[2025-03-04 21:02:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.4352867305278778 norm:0.00040598184568807483 max memory_allocated 29273.12548828125 
[2025-03-04 21:02:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.43517857789993286 norm:0.0003902043681591749 max memory_allocated 29273.12548828125 
[2025-03-04 21:03:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.435059517621994 norm:0.00038985986611805856 max memory_allocated 29273.12548828125 
[2025-03-04 21:04:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.4350258409976959 norm:0.0003737275255843997 max memory_allocated 29273.12548828125 
[2025-03-04 21:05:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.4350256323814392 norm:0.00037917698500677943 max memory_allocated 29273.12548828125 
[2025-03-04 21:05:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-04 21:06:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.5196154713630676 norm:0.0032037943601608276 max memory_allocated 29273.31298828125 
[2025-03-04 21:07:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.5033323764801025 norm:0.002028956077992916 max memory_allocated 29273.31298828125 
[2025-03-04 21:08:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.4858582615852356 norm:0.0010236017405986786 max memory_allocated 29273.31298828125 
[2025-03-04 21:08:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.4806171655654907 norm:0.0005499667022377253 max memory_allocated 29273.31298828125 
[2025-03-04 21:09:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.4788091778755188 norm:0.00037895815330557525 max memory_allocated 29273.31298828125 
[2025-03-04 21:10:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.4782455265522003 norm:0.0003676055639516562 max memory_allocated 29273.31298828125 
[2025-03-04 21:11:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.4779418706893921 norm:0.0003495478304103017 max memory_allocated 29273.31298828125 
[2025-03-04 21:12:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.4777522683143616 norm:0.0003296373470220715 max memory_allocated 29273.31298828125 
[2025-03-04 21:13:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.47757378220558167 norm:0.00030762175447307527 max memory_allocated 29273.31298828125 
[2025-03-04 21:13:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.4774208664894104 norm:0.0002919183752965182 max memory_allocated 29273.31298828125 
[2025-03-04 21:14:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.47727876901626587 norm:0.00027269311249256134 max memory_allocated 29273.31298828125 
[2025-03-04 21:15:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.47716671228408813 norm:0.00026546220760792494 max memory_allocated 29273.31298828125 
[2025-03-04 21:16:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.4770682156085968 norm:0.00025315710809081793 max memory_allocated 29273.31298828125 
[2025-03-04 21:17:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.476948082447052 norm:0.0002448006998747587 max memory_allocated 29273.31298828125 
[2025-03-04 21:18:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.47687479853630066 norm:0.000241466099396348 max memory_allocated 29273.31298828125 
[2025-03-04 21:18:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.47681835293769836 norm:0.00023951503681018949 max memory_allocated 29273.31298828125 
[2025-03-04 21:19:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.4767574369907379 norm:0.00023617735132575035 max memory_allocated 29273.31298828125 
[2025-03-04 21:20:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.47669023275375366 norm:0.00023059155500959605 max memory_allocated 29273.31298828125 
[2025-03-04 21:21:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.47663047909736633 norm:0.00022688355238642544 max memory_allocated 29273.31298828125 
[2025-03-04 21:22:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.47658154368400574 norm:0.00022207017173059285 max memory_allocated 29273.31298828125 
[2025-03-04 21:22:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-04 21:23:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.570316731929779 norm:0.003134142141789198 max memory_allocated 29273.50048828125 
[2025-03-04 21:24:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.5533400177955627 norm:0.0019962002988904715 max memory_allocated 29273.50048828125 
[2025-03-04 21:24:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.5358932018280029 norm:0.0012319710804149508 max memory_allocated 29273.50048828125 
[2025-03-04 21:25:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.530502438545227 norm:0.0009068117942661047 max memory_allocated 29273.50048828125 
[2025-03-04 21:26:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.5287574529647827 norm:0.000744809745810926 max memory_allocated 29273.50048828125 
[2025-03-04 21:27:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.5281586647033691 norm:0.0006474305992014706 max memory_allocated 29273.50048828125 
[2025-03-04 21:28:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.5276973843574524 norm:0.0005688461824320257 max memory_allocated 29273.50048828125 
[2025-03-04 21:29:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.5273396968841553 norm:0.0005093145882710814 max memory_allocated 29273.50048828125 
[2025-03-04 21:29:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.5270259380340576 norm:0.00045561877777799964 max memory_allocated 29273.50048828125 
[2025-03-04 21:30:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.5267850756645203 norm:0.0004175748326815665 max memory_allocated 29273.50048828125 
[2025-03-04 21:31:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.5266401767730713 norm:0.00038142441189847887 max memory_allocated 29273.50048828125 
[2025-03-04 21:32:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.5264956951141357 norm:0.00037402717862278223 max memory_allocated 29273.50048828125 
[2025-03-04 21:33:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.5263770222663879 norm:0.00035534737980924547 max memory_allocated 29273.50048828125 
[2025-03-04 21:34:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.5262451171875 norm:0.0003415499231778085 max memory_allocated 29273.50048828125 
[2025-03-04 21:34:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.526149570941925 norm:0.0003269809822086245 max memory_allocated 29273.50048828125 
[2025-03-04 21:35:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.5260229706764221 norm:0.0003181629872415215 max memory_allocated 29273.50048828125 
[2025-03-04 21:36:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.5258257389068604 norm:0.00028709007892757654 max memory_allocated 29273.50048828125 
[2025-03-04 21:37:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.525729238986969 norm:0.00028855548589490354 max memory_allocated 29273.50048828125 
[2025-03-04 21:38:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.5256856083869934 norm:0.0002971254289150238 max memory_allocated 29273.50048828125 
[2025-03-04 21:38:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.5256615877151489 norm:0.00029102806001901627 max memory_allocated 29273.50048828125 
[2025-03-04 21:39:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-04 21:40:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.6213679909706116 norm:0.0028107340913265944 max memory_allocated 29273.68798828125 
[2025-03-04 21:40:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.6039285063743591 norm:0.0017837182385846972 max memory_allocated 29273.68798828125 
[2025-03-04 21:41:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.5854406356811523 norm:0.0009292477043345571 max memory_allocated 29273.68798828125 
[2025-03-04 21:42:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.5798863172531128 norm:0.0005972645594738424 max memory_allocated 29273.68798828125 
[2025-03-04 21:43:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.5781360864639282 norm:0.0004610865144059062 max memory_allocated 29273.68798828125 
[2025-03-04 21:44:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.5775048732757568 norm:0.0003964836068917066 max memory_allocated 29273.68798828125 
[2025-03-04 21:45:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.5771132111549377 norm:0.0003487059148028493 max memory_allocated 29273.68798828125 
[2025-03-04 21:45:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.576855480670929 norm:0.00033206932130269706 max memory_allocated 29273.68798828125 
[2025-03-04 21:46:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.57660973072052 norm:0.0003086841315962374 max memory_allocated 29273.68798828125 
[2025-03-04 21:47:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.5764574408531189 norm:0.000303259352222085 max memory_allocated 29273.68798828125 
[2025-03-04 21:48:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.5763115882873535 norm:0.00028970217681489885 max memory_allocated 29273.68798828125 
[2025-03-04 21:49:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.5761823654174805 norm:0.0002770056016743183 max memory_allocated 29273.68798828125 
[2025-03-04 21:49:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.576050877571106 norm:0.0002669577661436051 max memory_allocated 29273.68798828125 
[2025-03-04 21:50:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.5759410858154297 norm:0.00026261640596203506 max memory_allocated 29273.68798828125 
[2025-03-04 21:51:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.5758727788925171 norm:0.0002577790291979909 max memory_allocated 29273.68798828125 
[2025-03-04 21:52:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.575820803642273 norm:0.00025515074958093464 max memory_allocated 29273.68798828125 
[2025-03-04 21:53:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.5757643580436707 norm:0.00025563480448909104 max memory_allocated 29273.68798828125 
[2025-03-04 21:54:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.5757323503494263 norm:0.0002510986232664436 max memory_allocated 29273.68798828125 
[2025-03-04 21:54:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.5756780505180359 norm:0.00024863312137313187 max memory_allocated 29273.68798828125 
[2025-03-04 21:55:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.5756222009658813 norm:0.0002411246532574296 max memory_allocated 29273.68798828125 
[2025-03-04 21:55:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-03-04 21:56:52 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.6854246258735657 norm:0.009051519446074963 max memory_allocated 29273.87548828125 
[2025-03-04 21:57:42 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.6628721952438354 norm:0.004971644841134548 max memory_allocated 29273.87548828125 
[2025-03-04 21:58:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.6397151350975037 norm:0.0023415700998157263 max memory_allocated 29273.87548828125 
[2025-03-04 21:59:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.632104218006134 norm:0.0011216229759156704 max memory_allocated 29273.87548828125 
[2025-03-04 22:00:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.6299418807029724 norm:0.0007442234782502055 max memory_allocated 29273.87548828125 
[2025-03-04 22:01:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.6291245818138123 norm:0.0005525965243577957 max memory_allocated 29273.87548828125 
[2025-03-04 22:01:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.6285974979400635 norm:0.00044840408372692764 max memory_allocated 29273.87548828125 
[2025-03-04 22:02:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.628233015537262 norm:0.000420184776885435 max memory_allocated 29273.87548828125 
[2025-03-04 22:03:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.6279613375663757 norm:0.0003924439661204815 max memory_allocated 29273.87548828125 
[2025-03-04 22:04:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.6276928782463074 norm:0.00036533738602884114 max memory_allocated 29273.87548828125 
[2025-03-04 22:05:07 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.6275001168251038 norm:0.0003580657939892262 max memory_allocated 29273.87548828125 
[2025-03-04 22:05:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.6273132562637329 norm:0.00033604016061872244 max memory_allocated 29273.87548828125 
[2025-03-04 22:06:46 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.6271697282791138 norm:0.000322965148370713 max memory_allocated 29273.87548828125 
[2025-03-04 22:07:36 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.6270648837089539 norm:0.00032281881431117654 max memory_allocated 29273.87548828125 
[2025-03-04 22:08:25 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.6269497871398926 norm:0.0003045199264306575 max memory_allocated 29273.87548828125 
[2025-03-04 22:09:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.626873254776001 norm:0.000297494581900537 max memory_allocated 29273.87548828125 
[2025-03-04 22:10:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.626758873462677 norm:0.00028088121325708926 max memory_allocated 29273.87548828125 
[2025-03-04 22:10:54 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.6266357898712158 norm:0.0002771849976852536 max memory_allocated 29273.87548828125 
[2025-03-04 22:11:44 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.6265653371810913 norm:0.00026736233849078417 max memory_allocated 29273.87548828125 
[2025-03-04 22:12:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.6265005469322205 norm:0.0002666437067091465 max memory_allocated 29273.87548828125 
[2025-03-04 22:12:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-03-04 22:13:41 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.7362619042396545 norm:0.0031618971843272448 max memory_allocated 29274.06298828125 
[2025-03-04 22:14:30 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.7169525623321533 norm:0.0019636540673673153 max memory_allocated 29274.06298828125 
[2025-03-04 22:15:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.6973910927772522 norm:0.0011370828142389655 max memory_allocated 29274.06298828125 
[2025-03-04 22:16:09 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.6916038990020752 norm:0.0007681656279601157 max memory_allocated 29274.06298828125 
[2025-03-04 22:16:59 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.6899927258491516 norm:0.000574262929148972 max memory_allocated 29274.06298828125 
[2025-03-04 22:17:48 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.6891880035400391 norm:0.0004861578345298767 max memory_allocated 29274.06298828125 
[2025-03-04 22:18:37 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.6887255907058716 norm:0.00044276105472818017 max memory_allocated 29274.06298828125 
[2025-03-04 22:19:27 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.6883543133735657 norm:0.00040588172851130366 max memory_allocated 29274.06298828125 
[2025-03-04 22:20:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.6880676746368408 norm:0.0003793273644987494 max memory_allocated 29274.06298828125 
[2025-03-04 22:21:06 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.687842607498169 norm:0.00036149725201539695 max memory_allocated 29274.06298828125 
[2025-03-04 22:21:55 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.687639594078064 norm:0.00034484569914638996 max memory_allocated 29274.06298828125 
[2025-03-04 22:22:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.6874899864196777 norm:0.00033564132172614336 max memory_allocated 29274.06298828125 
[2025-03-04 22:23:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.6873382925987244 norm:0.00032885203836485744 max memory_allocated 29274.06298828125 
[2025-03-04 22:24:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.6872268319129944 norm:0.0003190671559423208 max memory_allocated 29274.06298828125 
[2025-03-04 22:25:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.6871411800384521 norm:0.00031491703703068197 max memory_allocated 29274.06298828125 
[2025-03-04 22:26:03 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.687006413936615 norm:0.0003081562463194132 max memory_allocated 29274.06298828125 
[2025-03-04 22:26:53 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.6868754625320435 norm:0.00030459192930720747 max memory_allocated 29274.06298828125 
[2025-03-04 22:27:42 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.6868041753768921 norm:0.00030359087395481765 max memory_allocated 29274.06298828125 
[2025-03-04 22:28:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.6867300271987915 norm:0.00030481882276944816 max memory_allocated 29274.06298828125 
[2025-03-04 22:29:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.6866547465324402 norm:0.0002993708476424217 max memory_allocated 29274.06298828125 
[2025-03-04 22:29:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-03-04 22:30:29 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.8240694999694824 norm:0.005999166518449783 max memory_allocated 29274.25048828125 
[2025-03-04 22:31:18 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.7974793314933777 norm:0.003603348508477211 max memory_allocated 29274.25048828125 
[2025-03-04 22:32:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.771902322769165 norm:0.0020593288354575634 max memory_allocated 29274.25048828125 
[2025-03-04 22:32:57 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.7637859582901001 norm:0.0011564899468794465 max memory_allocated 29274.25048828125 
[2025-03-04 22:33:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.7614660263061523 norm:0.0007753712125122547 max memory_allocated 29274.25048828125 
[2025-03-04 22:34:36 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.760388195514679 norm:0.0006759442621842027 max memory_allocated 29274.25048828125 
[2025-03-04 22:35:26 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.7595857977867126 norm:0.000598568469285965 max memory_allocated 29274.25048828125 
[2025-03-04 22:36:16 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.7590218782424927 norm:0.0005604746984317899 max memory_allocated 29274.25048828125 
[2025-03-04 22:37:05 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.7586328387260437 norm:0.0005299178301356733 max memory_allocated 29274.25048828125 
[2025-03-04 22:37:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.7582700848579407 norm:0.0004971508751623333 max memory_allocated 29274.25048828125 
[2025-03-04 22:38:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.7579765915870667 norm:0.0004649665206670761 max memory_allocated 29274.25048828125 
[2025-03-04 22:39:34 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.7577860355377197 norm:0.0004437683383002877 max memory_allocated 29274.25048828125 
[2025-03-04 22:40:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.757586658000946 norm:0.0004311552329454571 max memory_allocated 29274.25048828125 
[2025-03-04 22:41:12 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.7573425769805908 norm:0.0004284447350073606 max memory_allocated 29274.25048828125 
[2025-03-04 22:42:02 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.7570785284042358 norm:0.00042158158612437546 max memory_allocated 29274.25048828125 
[2025-03-04 22:42:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.7569230198860168 norm:0.0004147311265114695 max memory_allocated 29274.25048828125 
[2025-03-04 22:43:41 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.7568237781524658 norm:0.00040448157233186066 max memory_allocated 29274.25048828125 
[2025-03-04 22:44:30 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.7567092776298523 norm:0.0003865028265863657 max memory_allocated 29274.25048828125 
[2025-03-04 22:45:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.7566207051277161 norm:0.00037575847818516195 max memory_allocated 29274.25048828125 
[2025-03-04 22:46:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.7565515637397766 norm:0.0003769249015022069 max memory_allocated 29274.25048828125 
[2025-03-04 22:46:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-03-04 22:47:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.8999837040901184 norm:0.004837376996874809 max memory_allocated 29274.43798828125 
[2025-03-04 22:48:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.8733018636703491 norm:0.0030855026561766863 max memory_allocated 29274.43798828125 
[2025-03-04 22:48:56 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.8467959761619568 norm:0.0018715490587055683 max memory_allocated 29274.43798828125 
[2025-03-04 22:49:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.8383615612983704 norm:0.0011185763869434595 max memory_allocated 29274.43798828125 
[2025-03-04 22:50:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.8357502222061157 norm:0.0008657437865622342 max memory_allocated 29274.43798828125 
[2025-03-04 22:51:24 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.8344265818595886 norm:0.0007635673973709345 max memory_allocated 29274.43798828125 
[2025-03-04 22:52:14 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.8336330652236938 norm:0.0007009113323874772 max memory_allocated 29274.43798828125 
[2025-03-04 22:53:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.8329257965087891 norm:0.0006463307072408497 max memory_allocated 29274.43798828125 
[2025-03-04 22:53:53 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.8323996067047119 norm:0.0006187980761751533 max memory_allocated 29274.43798828125 
[2025-03-04 22:54:42 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.8318802118301392 norm:0.0005912526394240558 max memory_allocated 29274.43798828125 
[2025-03-04 22:55:32 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.8315539360046387 norm:0.0005718526663258672 max memory_allocated 29274.43798828125 
[2025-03-04 22:56:21 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.8311609625816345 norm:0.0005484668654389679 max memory_allocated 29274.43798828125 
[2025-03-04 22:57:11 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.830791175365448 norm:0.0005321493372321129 max memory_allocated 29274.43798828125 
[2025-03-04 22:58:01 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.8305143713951111 norm:0.0005185757181607187 max memory_allocated 29274.43798828125 
[2025-03-04 22:58:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.8303681015968323 norm:0.0005060781259089708 max memory_allocated 29274.43798828125 
[2025-03-04 22:59:40 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.8301927447319031 norm:0.0004883454530499876 max memory_allocated 29274.43798828125 
[2025-03-04 23:00:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.8300033807754517 norm:0.0004883281653746963 max memory_allocated 29274.43798828125 
[2025-03-04 23:01:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.8298289775848389 norm:0.00048340263310819864 max memory_allocated 29274.43798828125 
[2025-03-04 23:02:08 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.8296396136283875 norm:0.0004767563659697771 max memory_allocated 29274.43798828125 
[2025-03-04 23:02:58 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.8294973373413086 norm:0.00046156314783729613 max memory_allocated 29274.43798828125 
[2025-03-04 23:03:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-03-04 23:03:16 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 23:04:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.9861366748809814 norm:0.018931392580270767 max memory_allocated 29274.77001953125 
[2025-03-04 23:04:55 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.9561944007873535 norm:0.014813611283898354 max memory_allocated 29274.77001953125 
[2025-03-04 23:05:45 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.9273460507392883 norm:0.011201728135347366 max memory_allocated 29274.77001953125 
[2025-03-04 23:06:35 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.9194921255111694 norm:0.009513972327113152 max memory_allocated 29274.77001953125 
[2025-03-04 23:07:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.9166188836097717 norm:0.008105837740004063 max memory_allocated 29274.77001953125 
[2025-03-04 23:08:14 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.9149195551872253 norm:0.006970936432480812 max memory_allocated 29274.77001953125 
[2025-03-04 23:09:04 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.9139034152030945 norm:0.006136516109108925 max memory_allocated 29274.77001953125 
[2025-03-04 23:09:53 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.9131181836128235 norm:0.00562111334875226 max memory_allocated 29274.77001953125 
[2025-03-04 23:10:43 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.9124188423156738 norm:0.005295603536069393 max memory_allocated 29274.77001953125 
[2025-03-04 23:11:33 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.9120306968688965 norm:0.005126602482050657 max memory_allocated 29274.77001953125 
[2025-03-04 23:12:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.9116488695144653 norm:0.005011575296521187 max memory_allocated 29274.77001953125 
[2025-03-04 23:13:12 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.9112295508384705 norm:0.004801652859896421 max memory_allocated 29274.77001953125 
[2025-03-04 23:14:02 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.9108631014823914 norm:0.004813002422451973 max memory_allocated 29274.77001953125 
[2025-03-04 23:14:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.9104863405227661 norm:0.004703634884208441 max memory_allocated 29274.77001953125 
[2025-03-04 23:15:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.9103995561599731 norm:0.0048392415046691895 max memory_allocated 29274.77001953125 
[2025-03-04 23:16:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.9101361036300659 norm:0.004683374427258968 max memory_allocated 29274.77001953125 
[2025-03-04 23:17:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.9100549221038818 norm:0.004660104401409626 max memory_allocated 29274.77001953125 
[2025-03-04 23:18:10 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.9098376035690308 norm:0.0046214452013373375 max memory_allocated 29274.77001953125 
[2025-03-04 23:19:00 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.9097238779067993 norm:0.004570079036056995 max memory_allocated 29274.77001953125 
[2025-03-04 23:19:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.9095594882965088 norm:0.004475595895200968 max memory_allocated 29274.77001953125 
[2025-03-04 23:20:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-03-04 23:20:08 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 23:20:58 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:1.1361393928527832 norm:0.038572072982788086 max memory_allocated 29274.95751953125 
[2025-03-04 23:21:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:1.0888532400131226 norm:0.026285812258720398 max memory_allocated 29274.95751953125 
[2025-03-04 23:22:37 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:1.0473064184188843 norm:0.01730228215456009 max memory_allocated 29274.95751953125 
[2025-03-04 23:23:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:1.0357571840286255 norm:0.013480620458722115 max memory_allocated 29274.95751953125 
[2025-03-04 23:24:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:1.0313823223114014 norm:0.011030726134777069 max memory_allocated 29274.95751953125 
[2025-03-04 23:25:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:1.0287837982177734 norm:0.010948576033115387 max memory_allocated 29274.95751953125 
[2025-03-04 23:25:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:1.027111530303955 norm:0.01211412064731121 max memory_allocated 29274.95751953125 
[2025-03-04 23:26:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:1.0254366397857666 norm:0.009943059645593166 max memory_allocated 29274.95751953125 
[2025-03-04 23:27:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:1.0242969989776611 norm:0.009131187573075294 max memory_allocated 29274.95751953125 
[2025-03-04 23:28:25 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:1.0233981609344482 norm:0.008404386229813099 max memory_allocated 29274.95751953125 
[2025-03-04 23:29:15 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:1.0226503610610962 norm:0.00798963475972414 max memory_allocated 29274.95751953125 
[2025-03-04 23:30:04 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:1.0220952033996582 norm:0.007760827895253897 max memory_allocated 29274.95751953125 
[2025-03-04 23:30:54 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:1.021675944328308 norm:0.007570264395326376 max memory_allocated 29274.95751953125 
[2025-03-04 23:31:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:1.0213395357131958 norm:0.007367092650383711 max memory_allocated 29274.95751953125 
[2025-03-04 23:32:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:1.0207045078277588 norm:0.006974466145038605 max memory_allocated 29274.95751953125 
[2025-03-04 23:33:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:1.0205535888671875 norm:0.0067161559127271175 max memory_allocated 29274.95751953125 
[2025-03-04 23:34:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:1.020806908607483 norm:0.00671052560210228 max memory_allocated 29274.95751953125 
[2025-03-04 23:35:03 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:1.0203773975372314 norm:0.006620377767831087 max memory_allocated 29274.95751953125 
[2025-03-04 23:35:52 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:1.0200291872024536 norm:0.005975979380309582 max memory_allocated 29274.95751953125 
[2025-03-04 23:36:42 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:1.020308256149292 norm:0.006123539060354233 max memory_allocated 29274.95751953125 
[2025-03-04 23:36:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-03-04 23:37:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 23:37:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:1.5430772304534912 norm:0.2633705735206604 max memory_allocated 29275.14501953125 
[2025-03-04 23:38:40 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:1.373983383178711 norm:0.1988992542028427 max memory_allocated 29275.14501953125 
[2025-03-04 23:39:29 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:1.3109229803085327 norm:0.15177609026432037 max memory_allocated 29275.14501953125 
[2025-03-04 23:40:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:1.282705545425415 norm:0.12268530577421188 max memory_allocated 29275.14501953125 
[2025-03-04 23:41:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:1.2658872604370117 norm:0.09742127358913422 max memory_allocated 29275.14501953125 
[2025-03-04 23:41:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:1.256430983543396 norm:0.07476089894771576 max memory_allocated 29275.14501953125 
[2025-03-04 23:42:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:1.25075101852417 norm:0.0579342320561409 max memory_allocated 29275.14501953125 
[2025-03-04 23:43:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:1.2454288005828857 norm:0.0456978976726532 max memory_allocated 29275.14501953125 
[2025-03-04 23:44:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:1.242362380027771 norm:0.03785461187362671 max memory_allocated 29275.14501953125 
[2025-03-04 23:45:17 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:1.2393025159835815 norm:0.03303072229027748 max memory_allocated 29275.14501953125 
[2025-03-04 23:46:07 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:1.237196922302246 norm:0.0318947471678257 max memory_allocated 29275.14501953125 
[2025-03-04 23:46:57 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:1.2354556322097778 norm:0.030224530026316643 max memory_allocated 29275.14501953125 
[2025-03-04 23:47:46 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:1.2338522672653198 norm:0.02837185561656952 max memory_allocated 29275.14501953125 
[2025-03-04 23:48:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:1.2322908639907837 norm:0.02783813700079918 max memory_allocated 29275.14501953125 
[2025-03-04 23:49:26 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:1.2311159372329712 norm:0.027051445096731186 max memory_allocated 29275.14501953125 
[2025-03-04 23:50:16 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:1.2299244403839111 norm:0.026258133351802826 max memory_allocated 29275.14501953125 
[2025-03-04 23:51:05 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:1.229569435119629 norm:0.026086289435625076 max memory_allocated 29275.14501953125 
[2025-03-04 23:51:55 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:1.2287737131118774 norm:0.024363819509744644 max memory_allocated 29275.14501953125 
[2025-03-04 23:52:45 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:1.2279292345046997 norm:0.023876318708062172 max memory_allocated 29275.14501953125 
[2025-03-04 23:53:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:1.2272052764892578 norm:0.022889284417033195 max memory_allocated 29275.14501953125 
[2025-03-04 23:53:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-03-04 23:53:53 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 23:54:42 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:2.2814218997955322 norm:0.14127177000045776 max memory_allocated 29275.33251953125 
[2025-03-04 23:55:32 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:2.1391773223876953 norm:0.12067775428295135 max memory_allocated 29275.33251953125 
[2025-03-04 23:56:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:2.027836322784424 norm:0.10027388483285904 max memory_allocated 29275.33251953125 
[2025-03-04 23:57:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:1.9915697574615479 norm:0.09733748435974121 max memory_allocated 29275.33251953125 
[2025-03-04 23:58:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:1.9722237586975098 norm:0.10634510219097137 max memory_allocated 29275.33251953125 
[2025-03-04 23:58:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:1.9573105573654175 norm:0.10178345441818237 max memory_allocated 29275.33251953125 
[2025-03-04 23:59:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:1.9460163116455078 norm:0.09648638218641281 max memory_allocated 29275.33251953125 
[2025-03-05 00:00:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:1.9370251893997192 norm:0.0933627337217331 max memory_allocated 29275.33251953125 
[2025-03-05 00:01:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:1.9299979209899902 norm:0.0911654680967331 max memory_allocated 29275.33251953125 
[2025-03-05 00:02:09 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:1.9215490818023682 norm:0.08932996541261673 max memory_allocated 29275.33251953125 
[2025-03-05 00:02:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:1.9168556928634644 norm:0.08474946022033691 max memory_allocated 29275.33251953125 
[2025-03-05 00:03:48 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:1.91204035282135 norm:0.08611959964036942 max memory_allocated 29275.33251953125 
[2025-03-05 00:04:38 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:1.9108970165252686 norm:0.08512541651725769 max memory_allocated 29275.33251953125 
[2025-03-05 00:05:28 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:1.906568169593811 norm:0.08139833807945251 max memory_allocated 29275.33251953125 
[2025-03-05 00:06:17 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:1.9039238691329956 norm:0.07754798233509064 max memory_allocated 29275.33251953125 
[2025-03-05 00:07:07 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:1.9000146389007568 norm:0.07622840255498886 max memory_allocated 29275.33251953125 
[2025-03-05 00:07:56 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:1.8990272283554077 norm:0.0726100504398346 max memory_allocated 29275.33251953125 
[2025-03-05 00:08:46 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:1.89870023727417 norm:0.07294485718011856 max memory_allocated 29275.33251953125 
[2025-03-05 00:09:35 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:1.8959077596664429 norm:0.07125058770179749 max memory_allocated 29275.33251953125 
[2025-03-05 00:10:25 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:1.8928935527801514 norm:0.0691041648387909 max memory_allocated 29275.33251953125 
[2025-03-05 00:10:39 root] (main_calibration.py 365): INFO 40312.40663266182
[2025-03-05 00:11:41 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-05 00:13:37 root] (main_calibration.py 158): INFO wikitext2 : 5.385950088500977
[2025-03-05 00:13:37 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-05 00:16:37 root] (main_calibration.py 158): INFO c4 : 6.9780473709106445
[2025-03-05 02:11:59 root] (main_calibration.py 169): INFO {'wikitext2': 5.385950088500977, 'c4': 6.9780473709106445, 'results': {'hellaswag': {'acc': 0.570902210714997, 'acc_stderr': 0.004939358145561318, 'acc_norm': 0.7412865962955587, 'acc_norm_stderr': 0.004370328224831784}, 'piqa': {'acc': 0.780739934711643, 'acc_stderr': 0.0096533574636053, 'acc_norm': 0.779651795429815, 'acc_norm_stderr': 0.009670535456853131}, 'arc_easy': {'acc': 0.7276936026936027, 'acc_stderr': 0.00913421844765268, 'acc_norm': 0.5782828282828283, 'acc_norm_stderr': 0.010133255284012314}, 'winogrande': {'acc': 0.6724546172059984, 'acc_stderr': 0.013190169546797017}, 'boolq': {'acc': 0.6529051987767585, 'acc_stderr': 0.008326100668151903}, 'arc_challenge': {'acc': 0.41723549488054607, 'acc_stderr': 0.01440982551840308, 'acc_norm': 0.41638225255972694, 'acc_norm_stderr': 0.01440561827943618}}, 'versions': {'hellaswag': 0, 'piqa': 0, 'arc_easy': 0, 'winogrande': 0, 'boolq': 1, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
