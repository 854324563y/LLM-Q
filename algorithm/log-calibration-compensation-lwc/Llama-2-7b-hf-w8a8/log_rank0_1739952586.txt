[2025-02-19 08:09:46 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-7b-hf-w8a8', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-7b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-19 08:11:05 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-19 08:11:05 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-19 08:11:06 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-19 08:11:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-19 08:11:10 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:11:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0011683744378387928 norm:0.0035215341486036777 max memory_allocated 22562.10693359375 
[2025-02-19 08:12:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0005213635158725083 norm:0.0007448424585163593 max memory_allocated 22562.10693359375 
[2025-02-19 08:12:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0004415447765495628 norm:0.001009830622933805 max memory_allocated 22562.10693359375 
[2025-02-19 08:13:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0003886211197823286 norm:0.001059019356034696 max memory_allocated 22562.10693359375 
[2025-02-19 08:13:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0003526890941429883 norm:0.000985733582638204 max memory_allocated 22562.10693359375 
[2025-02-19 08:14:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0003266834537498653 norm:0.0009528941009193659 max memory_allocated 22562.10693359375 
[2025-02-19 08:14:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.00030584470368921757 norm:0.000929676927626133 max memory_allocated 22562.10693359375 
[2025-02-19 08:15:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0002885157009586692 norm:0.0008506239973939955 max memory_allocated 22562.10693359375 
[2025-02-19 08:15:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0002800257643684745 norm:0.0008564565214328468 max memory_allocated 22562.10693359375 
[2025-02-19 08:16:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00026893141330219805 norm:0.0008130047935992479 max memory_allocated 22562.10693359375 
[2025-02-19 08:16:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0002611334202811122 norm:0.0007909620180726051 max memory_allocated 22562.10693359375 
[2025-02-19 08:17:19 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.00025888136588037014 norm:0.0007946145487949252 max memory_allocated 22562.10693359375 
[2025-02-19 08:17:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0002545391907915473 norm:0.0007821015315130353 max memory_allocated 22562.10693359375 
[2025-02-19 08:18:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0002455304202158004 norm:0.0007182531990110874 max memory_allocated 22562.10693359375 
[2025-02-19 08:18:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.00024589375243522227 norm:0.0007151745958253741 max memory_allocated 22562.10693359375 
[2025-02-19 08:19:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.00024229870177805424 norm:0.0007015133160166442 max memory_allocated 22562.10693359375 
[2025-02-19 08:19:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00023897120263427496 norm:0.0006545680807903409 max memory_allocated 22562.10693359375 
[2025-02-19 08:20:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00023735046852380037 norm:0.0006425409228540957 max memory_allocated 22562.10693359375 
[2025-02-19 08:20:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00023581039567943662 norm:0.0006135329022072256 max memory_allocated 22562.10693359375 
[2025-02-19 08:21:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.00023711730318609625 norm:0.0006256807246245444 max memory_allocated 22562.10693359375 
[2025-02-19 08:21:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-19 08:21:36 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:22:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0027683808002620935 norm:0.003818978788331151 max memory_allocated 22562.27880859375 
[2025-02-19 08:22:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0017194526735693216 norm:0.0015362557023763657 max memory_allocated 22562.27880859375 
[2025-02-19 08:23:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0015161981573328376 norm:0.0015368956374004483 max memory_allocated 22562.27880859375 
[2025-02-19 08:23:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.001376486849039793 norm:0.0016003350028768182 max memory_allocated 22562.27880859375 
[2025-02-19 08:24:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.001315924571827054 norm:0.001656652893871069 max memory_allocated 22562.27880859375 
[2025-02-19 08:24:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0012348545715212822 norm:0.001463799155317247 max memory_allocated 22562.27880859375 
[2025-02-19 08:25:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0012238358613103628 norm:0.0016509948763996363 max memory_allocated 22562.27880859375 
[2025-02-19 08:25:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0011752613354474306 norm:0.0016179498052224517 max memory_allocated 22562.27880859375 
[2025-02-19 08:26:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0011371398577466607 norm:0.0014736586017534137 max memory_allocated 22562.27880859375 
[2025-02-19 08:26:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.001112183090299368 norm:0.0014904059935361147 max memory_allocated 22562.27880859375 
[2025-02-19 08:27:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0011215267004445195 norm:0.001556349452584982 max memory_allocated 22562.27880859375 
[2025-02-19 08:27:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0010935687460005283 norm:0.0015008000191301107 max memory_allocated 22562.27880859375 
[2025-02-19 08:28:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0010837105801329017 norm:0.0014377960469573736 max memory_allocated 22562.27880859375 
[2025-02-19 08:28:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0010668151080608368 norm:0.001282631536014378 max memory_allocated 22562.27880859375 
[2025-02-19 08:29:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0010581323876976967 norm:0.001292365719564259 max memory_allocated 22562.27880859375 
[2025-02-19 08:29:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0010781163582578301 norm:0.0013069359119981527 max memory_allocated 22562.27880859375 
[2025-02-19 08:30:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0010541820665821433 norm:0.001253666589036584 max memory_allocated 22562.27880859375 
[2025-02-19 08:30:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.001041090814396739 norm:0.0011180364526808262 max memory_allocated 22562.27880859375 
[2025-02-19 08:31:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0010647722519934177 norm:0.0012357369996607304 max memory_allocated 22562.27880859375 
[2025-02-19 08:31:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0010384992929175496 norm:0.0011556674726307392 max memory_allocated 22562.27880859375 
[2025-02-19 08:32:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-19 08:32:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:32:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0022783123422414064 norm:0.00413218978792429 max memory_allocated 22562.45068359375 
[2025-02-19 08:33:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0015428386395797133 norm:0.002781886840239167 max memory_allocated 22562.45068359375 
[2025-02-19 08:33:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0013272971846163273 norm:0.0019102657679468393 max memory_allocated 22562.45068359375 
[2025-02-19 08:34:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0012195566669106483 norm:0.0014899647794663906 max memory_allocated 22562.45068359375 
[2025-02-19 08:34:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.001148464740253985 norm:0.001201496459543705 max memory_allocated 22562.45068359375 
[2025-02-19 08:35:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.001100502209737897 norm:0.0010112510062754154 max memory_allocated 22562.45068359375 
[2025-02-19 08:35:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0010753227397799492 norm:0.0008963425643742085 max memory_allocated 22562.45068359375 
[2025-02-19 08:36:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0010527744889259338 norm:0.0008005022536963224 max memory_allocated 22562.45068359375 
[2025-02-19 08:36:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.00103608681820333 norm:0.0007397233275696635 max memory_allocated 22562.45068359375 
[2025-02-19 08:37:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.001024052151478827 norm:0.0006740457029081881 max memory_allocated 22562.45068359375 
[2025-02-19 08:37:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0010169096058234572 norm:0.0006211764994077384 max memory_allocated 22562.45068359375 
[2025-02-19 08:38:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.001010249019600451 norm:0.0005888537270948291 max memory_allocated 22562.45068359375 
[2025-02-19 08:38:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0010053063742816448 norm:0.0005427116993814707 max memory_allocated 22562.45068359375 
[2025-02-19 08:39:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0010004573268815875 norm:0.0005102937575429678 max memory_allocated 22562.45068359375 
[2025-02-19 08:39:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0009964117780327797 norm:0.0004859452601522207 max memory_allocated 22562.45068359375 
[2025-02-19 08:40:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0009919077856466174 norm:0.0004616625956259668 max memory_allocated 22562.45068359375 
[2025-02-19 08:40:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0009903921745717525 norm:0.00043858762364834547 max memory_allocated 22562.45068359375 
[2025-02-19 08:41:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0009868878405541182 norm:0.00041317619616165757 max memory_allocated 22562.45068359375 
[2025-02-19 08:41:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0009843666339293122 norm:0.0003873854293487966 max memory_allocated 22562.45068359375 
[2025-02-19 08:42:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0009809851180762053 norm:0.00036607845686376095 max memory_allocated 22562.45068359375 
[2025-02-19 08:42:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-19 08:43:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0021659082267433405 norm:0.0007865829975344241 max memory_allocated 22562.50732421875 
[2025-02-19 08:43:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.001661150366999209 norm:0.00033316807821393013 max memory_allocated 22562.50732421875 
[2025-02-19 08:44:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0014826422557234764 norm:0.0002156993723474443 max memory_allocated 22562.50732421875 
[2025-02-19 08:44:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0013761022128164768 norm:0.00014940195251256227 max memory_allocated 22562.50732421875 
[2025-02-19 08:45:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.001320424722507596 norm:0.0001189026515930891 max memory_allocated 22562.50732421875 
[2025-02-19 08:45:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0012808667961508036 norm:8.533651271136478e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:46:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0012586860684677958 norm:7.680746784899384e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:46:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.001240278361365199 norm:5.823142782901414e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:47:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0012342117261141539 norm:5.353564120014198e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:47:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0012290624435991049 norm:4.842804992222227e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:48:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0012194718001410365 norm:4.176111178821884e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:48:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0012132113333791494 norm:3.460761945461854e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:49:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0012032717932015657 norm:3.265762279625051e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:49:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.001201388775371015 norm:3.0055931347305886e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:50:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.001199800055474043 norm:2.9447093766066246e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:50:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0012022979790344834 norm:2.8732949431287125e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:51:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.001193889300338924 norm:2.5450859538977966e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:51:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0011932675261050463 norm:2.770085666270461e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:52:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0011896074283868074 norm:2.733310975600034e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:52:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0011890511959791183 norm:2.6291969334124587e-05 max memory_allocated 22562.50732421875 
[2025-02-19 08:52:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-19 08:53:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.002101575955748558 norm:0.0004362060280982405 max memory_allocated 22562.67919921875 
[2025-02-19 08:53:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0016923598013818264 norm:0.00020244228653609753 max memory_allocated 22562.67919921875 
[2025-02-19 08:54:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0015448799822479486 norm:0.00012566521763801575 max memory_allocated 22562.67919921875 
[2025-02-19 08:54:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0014702626504004002 norm:9.788289753487334e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:55:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0014222137397155166 norm:7.26290891179815e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:56:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0013931433204561472 norm:5.646179124596529e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:56:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0013756430707871914 norm:5.113043516757898e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:57:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.001361358561553061 norm:4.057621117681265e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:57:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0013542954111471772 norm:3.2890842703636736e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:58:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0013479491462931037 norm:3.0719991627847776e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:58:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.001341126742772758 norm:3.0713450541952625e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:59:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0013394479174166918 norm:3.099489549640566e-05 max memory_allocated 22562.67919921875 
[2025-02-19 08:59:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.00133711623493582 norm:2.748321094259154e-05 max memory_allocated 22562.67919921875 
[2025-02-19 09:00:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.001333188614808023 norm:3.510998431011103e-05 max memory_allocated 22562.67919921875 
[2025-02-19 09:00:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.001330133993178606 norm:2.3419157514581457e-05 max memory_allocated 22562.67919921875 
[2025-02-19 09:01:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0013271233765408397 norm:2.278441024827771e-05 max memory_allocated 22562.67919921875 
[2025-02-19 09:01:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0013261380372568965 norm:2.1408768589026295e-05 max memory_allocated 22562.67919921875 
[2025-02-19 09:02:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0013264010194689035 norm:2.3302596673602238e-05 max memory_allocated 22562.67919921875 
[2025-02-19 09:02:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.001325610326603055 norm:2.08507917704992e-05 max memory_allocated 22562.67919921875 
[2025-02-19 09:03:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0013234724756330252 norm:2.0458304788917303e-05 max memory_allocated 22562.67919921875 
[2025-02-19 09:03:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-19 09:03:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0019188023870810866 norm:0.0002802120870910585 max memory_allocated 22562.85107421875 
[2025-02-19 09:04:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0016377619467675686 norm:0.00012968803639523685 max memory_allocated 22562.85107421875 
[2025-02-19 09:04:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0015369834145531058 norm:8.791488653514534e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:05:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.001481630140915513 norm:6.86024795868434e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:05:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0014473970513790846 norm:5.192071694182232e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:06:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0014279771130532026 norm:4.262397487764247e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:06:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0014162142761051655 norm:3.629955244832672e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:07:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0014089684700593352 norm:2.6187139155808836e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:07:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0014022558461874723 norm:2.7735100957215764e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:08:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0013976853806525469 norm:2.9337124942685477e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:08:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0013940018834546208 norm:2.2811067537986673e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:09:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0013916795141994953 norm:2.265534749312792e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:09:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0013909987173974514 norm:2.3530772523372434e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:10:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.001389101380482316 norm:2.2066402380005457e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:11:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0013871864648535848 norm:2.2501151761389337e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:11:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0013843881897628307 norm:2.0594992747646756e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:12:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0013823341578245163 norm:1.9486096789478324e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:12:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0013815815327689052 norm:2.0420859073055908e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:13:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0013810655800625682 norm:2.242001392005477e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:13:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0013806517235934734 norm:2.033571399806533e-05 max memory_allocated 22562.85107421875 
[2025-02-19 09:13:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-19 09:14:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0020209539216011763 norm:0.00024235356249846518 max memory_allocated 22563.02294921875 
[2025-02-19 09:14:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0017560648266226053 norm:9.872690861811861e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:15:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.0016582173993811011 norm:7.457657193299383e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:15:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0016060143243521452 norm:5.556565884035081e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:16:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0015750264283269644 norm:4.443951911525801e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:16:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0015553465345874429 norm:3.482604006421752e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:17:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0015436600660905242 norm:3.371136335772462e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:17:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0015335124917328358 norm:3.1049366953084245e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:18:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0015273654134944081 norm:2.3534368665423244e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:18:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0015212390571832657 norm:2.2189758965396322e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:19:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0015162695199251175 norm:2.1952162569505163e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:19:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0015127516817301512 norm:2.0533965653157793e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:20:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.0015103283803910017 norm:2.0431831217138097e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:20:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0015082706231623888 norm:1.975495797523763e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:21:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0015064275357872248 norm:1.7982123608817346e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:21:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0015068654902279377 norm:1.844504186010454e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:22:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.001506064785644412 norm:1.820697434595786e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:22:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0015038697747513652 norm:1.6346006304956973e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:23:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.001502003986388445 norm:1.70064959092997e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:23:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0015024221502244473 norm:1.7689186279312707e-05 max memory_allocated 22563.02294921875 
[2025-02-19 09:24:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-19 09:24:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.002174990950152278 norm:0.0002767383703030646 max memory_allocated 22563.19482421875 
[2025-02-19 09:25:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0018633637810125947 norm:0.0001138139414251782 max memory_allocated 22563.19482421875 
[2025-02-19 09:25:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.0017688428051769733 norm:7.726255716988817e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:26:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0017142692813649774 norm:5.9622521803248674e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:26:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0016789424698799849 norm:4.293177698855288e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:27:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.001655404339544475 norm:3.8372054405044764e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:27:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0016403237823396921 norm:3.549216489773244e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:28:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0016303586307913065 norm:3.161140193697065e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:28:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.0016225522849708796 norm:2.9039847504463978e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:29:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0016173023032024503 norm:2.883226625272073e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:29:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.001611924497410655 norm:2.3649945433135144e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:30:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0016076972242444754 norm:2.0651825252571143e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:30:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0016044528456404805 norm:1.9431972759775817e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:31:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0016026637749746442 norm:1.768821311998181e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:31:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0016002131160348654 norm:1.9639794118120335e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:32:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0015988118248060346 norm:1.9576382328523323e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:32:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0015984625788405538 norm:1.8835184164345264e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:33:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.001595273963175714 norm:1.6547674022149295e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:33:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.001595839741639793 norm:1.7692738765617833e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:34:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0015959092415869236 norm:1.7133987057604827e-05 max memory_allocated 22563.19482421875 
[2025-02-19 09:34:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-19 09:35:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0019779016729444265 norm:8.523309952579439e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:35:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0018296854104846716 norm:4.514797910815105e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:36:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.0017672941321507096 norm:3.488452421152033e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:36:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0017360940109938383 norm:2.5925401132553816e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:37:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0017167889745905995 norm:2.58977943303762e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:37:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.001705315662547946 norm:1.9798373614321463e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:38:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0016988154966384172 norm:1.87461992027238e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:38:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0016940664499998093 norm:1.878277907962911e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:39:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0016881581395864487 norm:1.8732884200289845e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:39:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0016867058584466577 norm:1.6800207959022373e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:40:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0016832457622513175 norm:1.7704589481581934e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:40:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0016821782337501645 norm:1.6659538232488558e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:41:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0016785009065642953 norm:1.6465142834931612e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:41:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0016769571229815483 norm:1.6217611118918285e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:42:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0016766905318945646 norm:1.5352294212789275e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:42:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.001675618113949895 norm:1.5785428331582807e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:43:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0016749210190027952 norm:1.4974236364651006e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:43:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0016759943682700396 norm:1.6111669538076967e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:44:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0016737486002966762 norm:1.565130332892295e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:44:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.001672853366471827 norm:1.5405414160341024e-05 max memory_allocated 22563.36669921875 
[2025-02-19 09:44:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-19 09:45:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002314719371497631 norm:0.000293190183583647 max memory_allocated 22563.53857421875 
[2025-02-19 09:45:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.00198630359955132 norm:8.968933980213478e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:46:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0019158836221322417 norm:7.004771032370627e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:46:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0018665216630324721 norm:4.6138997277012095e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:47:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0018429717747494578 norm:4.0199971408583224e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:48:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.001827227184548974 norm:3.5086235584458336e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:48:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0018134221900254488 norm:2.7893234801013023e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:49:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0018047287594527006 norm:2.9613271181005985e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:49:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0017969072796404362 norm:2.5208782972185872e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:50:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.00179117894731462 norm:2.463142118358519e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:50:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0017863800749182701 norm:2.3688444343861192e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:51:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0017834503669291735 norm:2.158415736630559e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:51:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0017786435782909393 norm:2.139863499905914e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:52:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0017752859275788069 norm:1.875199995993171e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:52:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0017737442394718528 norm:1.773611620592419e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:53:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.001772164017893374 norm:1.8054161046165973e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:53:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.001771420007571578 norm:1.7895756172947586e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:54:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0017683892510831356 norm:1.5432249711011536e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:54:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0017672850517556071 norm:1.5905050531728193e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:55:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0017663396429270506 norm:1.5482295566471294e-05 max memory_allocated 22563.53857421875 
[2025-02-19 09:55:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-19 09:55:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0021580106113106012 norm:0.00015302763495128602 max memory_allocated 22563.71044921875 
[2025-02-19 09:56:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.001994461053982377 norm:6.444822793127969e-05 max memory_allocated 22563.71044921875 
[2025-02-19 09:56:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.001932453247718513 norm:4.232614446664229e-05 max memory_allocated 22563.71044921875 
[2025-02-19 09:57:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0018977465806528926 norm:3.331576590426266e-05 max memory_allocated 22563.71044921875 
[2025-02-19 09:57:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0018771138275042176 norm:2.7702546503860503e-05 max memory_allocated 22563.71044921875 
[2025-02-19 09:58:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0018621239578351378 norm:2.4521270461264066e-05 max memory_allocated 22563.71044921875 
[2025-02-19 09:58:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0018515496049076319 norm:2.1180498151807114e-05 max memory_allocated 22563.71044921875 
[2025-02-19 09:59:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.001844953978434205 norm:1.904306009237189e-05 max memory_allocated 22563.71044921875 
[2025-02-19 09:59:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0018383844289928675 norm:1.8590160834719427e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:00:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0018330446910113096 norm:1.599536153662484e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:00:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0018294922774657607 norm:1.4855819244985469e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:01:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0018265938851982355 norm:1.382681330142077e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:01:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0018253575544804335 norm:1.3370589840633329e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:02:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0018233367009088397 norm:1.1954815818171483e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:03:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0018228522967547178 norm:1.1349397027515806e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:03:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0018225832609459758 norm:1.2824125406041276e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:04:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0018214015290141106 norm:1.21252915050718e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:04:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0018207714892923832 norm:1.1734096915461123e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:05:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0018188959220424294 norm:1.1078829629695974e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:05:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0018185126828029752 norm:1.1395943147363141e-05 max memory_allocated 22563.71044921875 
[2025-02-19 10:05:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-19 10:06:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0021735429763793945 norm:8.480103861074895e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:06:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0020400898065418005 norm:4.392836490296759e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:07:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0019818665459752083 norm:3.275708149885759e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:07:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0019509962294250727 norm:2.4855584342731163e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:08:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0019326595356687903 norm:2.3180497009889223e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:08:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.0019207217264920473 norm:1.9502056602505036e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:09:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0019141525262966752 norm:1.8291861124453135e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:09:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0019091879948973656 norm:1.6236273950198665e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:10:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0019038389436900616 norm:1.7151443898910657e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:10:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0019013496348634362 norm:1.4113055840425659e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:11:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.001897664973512292 norm:1.2826010788558051e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:11:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0018957196734845638 norm:1.2064623660990037e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:12:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.001892944797873497 norm:1.2714759577647783e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:12:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0018895624671131372 norm:1.1818295206467155e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:13:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0018895636312663555 norm:1.1613141396082938e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:13:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0018880480201914907 norm:1.1802740118582733e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:14:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0018891512881964445 norm:1.1295859621895943e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:14:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0018885302124544978 norm:1.1227347386011388e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:15:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0018872665241360664 norm:9.657874215918127e-06 max memory_allocated 22563.88232421875 
[2025-02-19 10:15:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.0018873821245506406 norm:1.1495083526824601e-05 max memory_allocated 22563.88232421875 
[2025-02-19 10:16:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-19 10:16:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.002190113766118884 norm:7.397135777864605e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:17:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0020885406993329525 norm:3.590035339584574e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:17:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002043097512796521 norm:2.6785504815052263e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:18:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0020185732282698154 norm:2.3763188437442295e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:18:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.002002355642616749 norm:1.9019424144062214e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:19:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.0019911127164959908 norm:1.408823754900368e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:19:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.001983179245144129 norm:1.55067682499066e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:20:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0019778795540332794 norm:1.3622830920212436e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:20:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0019731500651687384 norm:1.2405095731082838e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:21:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0019699574913829565 norm:1.1793770681833848e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:21:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0019675451330840588 norm:1.1110021659987979e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:22:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.001966390758752823 norm:1.0341627785237506e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:22:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0019639497622847557 norm:1.05922717921203e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:23:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0019623334519565105 norm:1.0035208106273785e-05 max memory_allocated 22564.05419921875 
[2025-02-19 10:23:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.0019613001495599747 norm:9.72825819189893e-06 max memory_allocated 22564.05419921875 
[2025-02-19 10:24:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0019610070157796144 norm:9.012639566208236e-06 max memory_allocated 22564.05419921875 
[2025-02-19 10:24:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.001960363471880555 norm:8.974396223493386e-06 max memory_allocated 22564.05419921875 
[2025-02-19 10:25:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0019604526460170746 norm:9.813320502871647e-06 max memory_allocated 22564.05419921875 
[2025-02-19 10:25:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0019597189966589212 norm:9.50724097492639e-06 max memory_allocated 22564.05419921875 
[2025-02-19 10:26:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.001959415152668953 norm:9.64870287134545e-06 max memory_allocated 22564.05419921875 
[2025-02-19 10:26:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-19 10:27:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.0023523536510765553 norm:0.00014652672689408064 max memory_allocated 22564.22607421875 
[2025-02-19 10:27:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0021753571927547455 norm:6.121749902376905e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:28:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.0021119338925927877 norm:4.24897916673217e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:28:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.002075646771118045 norm:3.3599877497181296e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:29:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.00204782048240304 norm:2.7826659788843244e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:29:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.002030660631135106 norm:2.1810792532050982e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:30:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.002020043320953846 norm:2.0053827029187232e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:30:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0020107009913772345 norm:1.6818754374980927e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:31:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.002004507929086685 norm:1.6713000150048174e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:31:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.0019996780902147293 norm:1.535095725557767e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:32:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.001996136736124754 norm:1.4073344573262148e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:32:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.001991801429539919 norm:1.3037869393883739e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:33:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0019892347045242786 norm:1.1718017049133778e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:33:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.001986603019759059 norm:1.1865751730510965e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:34:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0019850279204547405 norm:1.1714534593920689e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:34:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.0019826493225991726 norm:1.0568753168627154e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:35:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0019805708434432745 norm:9.846736247709487e-06 max memory_allocated 22564.22607421875 
[2025-02-19 10:35:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.001979669788852334 norm:1.0012576240114868e-05 max memory_allocated 22564.22607421875 
[2025-02-19 10:36:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0019793473184108734 norm:9.092571417568251e-06 max memory_allocated 22564.22607421875 
[2025-02-19 10:36:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.001978759653866291 norm:9.59253611654276e-06 max memory_allocated 22564.22607421875 
[2025-02-19 10:36:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-19 10:37:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.0023042752873152494 norm:7.253717194544151e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:37:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0022031678818166256 norm:3.1660583772463724e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:38:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0021615398582071066 norm:2.341545587114524e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:38:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.002134170150384307 norm:2.051634874078445e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:39:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.0021155360154807568 norm:1.5878833437454887e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:40:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.0021024823654443026 norm:1.4271010513766669e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:40:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0020931337494403124 norm:1.1570103197300341e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:41:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0020860861986875534 norm:1.1924382306460757e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:41:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.002080736216157675 norm:1.071325550583424e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:42:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0020766977686434984 norm:1.0113281859958079e-05 max memory_allocated 22564.39794921875 
[2025-02-19 10:42:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.002073111478239298 norm:9.571612281433772e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:43:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.002069920301437378 norm:8.261432412837166e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:43:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.0020684166811406612 norm:8.560176866012625e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:44:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.002067138906568289 norm:8.141853868437465e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:44:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0020655146799981594 norm:7.972182174853515e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:45:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.002063916763290763 norm:7.3726678238017485e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:45:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.002062616404145956 norm:7.16043450665893e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:46:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.0020629980135709047 norm:7.4209274316672236e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:46:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0020620166324079037 norm:8.216431524488144e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:47:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.002061299979686737 norm:7.48126331018284e-06 max memory_allocated 22564.39794921875 
[2025-02-19 10:47:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-19 10:47:57 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.002768945647403598 norm:0.00031655936618335545 max memory_allocated 22564.56982421875 
[2025-02-19 10:48:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.002350832102820277 norm:7.058510527713224e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:48:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.002288831863552332 norm:4.669673216994852e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:49:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0022473924327641726 norm:3.838112024823204e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:49:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.002213177504017949 norm:3.109449608018622e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:50:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.002190572442486882 norm:2.7051692086388357e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:51:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0021760291419923306 norm:2.3897608116385527e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:51:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.002164888195693493 norm:2.1904070308664814e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:52:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.002157205482944846 norm:1.8778278899844736e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:52:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.002148737432435155 norm:1.6772351955296472e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:53:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.002142013516277075 norm:1.5081056517374236e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:53:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0021361433900892735 norm:1.5549221643595956e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:54:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0021316069178283215 norm:1.4215120245353319e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:54:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0021283181849867105 norm:1.3673818102688529e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:55:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.0021255514584481716 norm:1.2510264241427649e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:55:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0021237689070403576 norm:1.2019679161312524e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:56:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.0021224450320005417 norm:1.1580989848880563e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:56:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.002119774464517832 norm:1.0121066225110553e-05 max memory_allocated 22564.56982421875 
[2025-02-19 10:57:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0021179101895540953 norm:9.268916983273812e-06 max memory_allocated 22564.56982421875 
[2025-02-19 10:57:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.002115404000505805 norm:9.528292139293626e-06 max memory_allocated 22564.56982421875 
[2025-02-19 10:57:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-19 10:58:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.0030517568811774254 norm:0.00046453054528683424 max memory_allocated 22564.74169921875 
[2025-02-19 10:58:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.0025233884807676077 norm:0.00010280940477969125 max memory_allocated 22564.74169921875 
[2025-02-19 10:59:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0024381764233112335 norm:6.662713713012636e-05 max memory_allocated 22564.74169921875 
[2025-02-19 10:59:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.0023837373591959476 norm:5.236674041952938e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:00:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0023449964355677366 norm:4.201782576274127e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:00:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0023168656043708324 norm:3.655527325463481e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:01:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.002296309219673276 norm:3.2365351216867566e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:01:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0022823805920779705 norm:3.0149347367114387e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:02:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.002270437777042389 norm:2.589233008620795e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:02:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.0022583878599107265 norm:2.3814460291760042e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:03:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.002249956363812089 norm:2.0511108232312836e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:03:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.002243631286546588 norm:1.9671038899105042e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:04:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0022378338035196066 norm:1.7507114534964785e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:04:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.002232880797237158 norm:1.5836614693398587e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:05:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.002228677039965987 norm:1.544445512990933e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:06:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.002225278876721859 norm:1.5094788068381604e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:06:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.00222305441275239 norm:1.4772213944524992e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:07:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.0022194047924131155 norm:1.3212051271693781e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:07:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0022170499432832003 norm:1.2948524272360373e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:08:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.002214644802734256 norm:1.253859227290377e-05 max memory_allocated 22564.74169921875 
[2025-02-19 11:08:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-19 11:08:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.0030966815538704395 norm:0.00041920720832422376 max memory_allocated 22564.91357421875 
[2025-02-19 11:09:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0025842669419944286 norm:0.00010639004176482558 max memory_allocated 22564.91357421875 
[2025-02-19 11:09:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.002504867035895586 norm:6.274350744206458e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:10:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.0024651915300637484 norm:5.185743066249415e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:10:47 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0024341465905308723 norm:4.212220665067434e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:11:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.0024103226605802774 norm:3.565302540664561e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:11:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.0023936182260513306 norm:3.083175033680163e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:12:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0023814369924366474 norm:2.833144026226364e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:12:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.002371902111917734 norm:2.6402656658319756e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:13:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.0023639535065740347 norm:2.2060123228584416e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:13:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.002356508746743202 norm:2.0342858988442458e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:14:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.002350218128412962 norm:1.8558133888291195e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:14:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.0023461906239390373 norm:1.737374986987561e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:15:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.0023422781378030777 norm:1.621174487809185e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:15:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.002339121885597706 norm:1.5415893358294852e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:16:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.002336427802219987 norm:1.3226550436229445e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:16:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.002336015459150076 norm:1.2118933227611706e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:17:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0023330282419919968 norm:1.1991093742835801e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:17:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.0023299059830605984 norm:1.21097991723218e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:18:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0023276004940271378 norm:1.1314435141684953e-05 max memory_allocated 22564.91357421875 
[2025-02-19 11:18:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-19 11:19:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.003425965318456292 norm:0.0004202024429105222 max memory_allocated 22565.08544921875 
[2025-02-19 11:19:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0029030938167124987 norm:0.00012332858750596642 max memory_allocated 22565.08544921875 
[2025-02-19 11:20:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.0028129590209573507 norm:8.241568139055744e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:20:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.0027560675516724586 norm:6.935940473340452e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:21:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.0027105994522571564 norm:5.684069765266031e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:21:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.002675033640116453 norm:4.3737043597502634e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:22:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.0026510206516832113 norm:3.631256186054088e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:22:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.0026338701136410236 norm:3.1247211154550314e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:23:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.0026190539356321096 norm:2.745524216152262e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:23:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.002606748603284359 norm:2.474560278642457e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:24:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.0025972940493375063 norm:2.431315806461498e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:24:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.002589278854429722 norm:2.2858170268591493e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:25:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.002581266453489661 norm:2.0287578081479296e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:25:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.002575793769210577 norm:1.749945295159705e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:26:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.002571005141362548 norm:1.6504003724548966e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:26:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.002566749230027199 norm:1.6402747860411182e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:27:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.0025618101935833693 norm:1.4794679373153485e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:27:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.002558113308623433 norm:1.3980157746118493e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:28:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0025553228333592415 norm:1.3365004633669741e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:28:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.0025532590225338936 norm:1.2547611731861252e-05 max memory_allocated 22565.08544921875 
[2025-02-19 11:28:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-19 11:29:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.00333518348634243 norm:0.0002541515859775245 max memory_allocated 22565.25732421875 
[2025-02-19 11:30:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.0030332074966281652 norm:9.20798847801052e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:30:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.002965770196169615 norm:6.707826105412096e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:31:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.002916052471846342 norm:5.2872146625304595e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:31:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.0028826873749494553 norm:4.185699799563736e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:32:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.0028588036075234413 norm:3.3879419788718224e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:32:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0028411410748958588 norm:2.8999200367252342e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:33:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0028284878935664892 norm:2.6449641154613346e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:33:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.00281672365963459 norm:2.306150236108806e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:34:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.0028073417488485575 norm:2.0028193830512464e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:34:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.0027996059507131577 norm:1.7442693206248805e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:35:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.0027949432842433453 norm:1.5586072549922392e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:35:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.0027897795662283897 norm:1.4934023056412116e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:36:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.0027847979217767715 norm:1.4174008356349077e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:36:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.0027803306002169847 norm:1.324516415479593e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:37:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.0027771065942943096 norm:1.2179915756860282e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:37:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.002775364089757204 norm:1.162567514256807e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:38:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.0027724718675017357 norm:1.1311676644254476e-05 max memory_allocated 22565.25732421875 
[2025-02-19 11:38:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.0027703808154910803 norm:9.86865779850632e-06 max memory_allocated 22565.25732421875 
[2025-02-19 11:39:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0027689491398632526 norm:8.952349162427709e-06 max memory_allocated 22565.25732421875 
[2025-02-19 11:39:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-19 11:39:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.004313645884394646 norm:0.0005986362812109292 max memory_allocated 22565.42919921875 
[2025-02-19 11:40:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.0035408674739301205 norm:0.00017042495892383158 max memory_allocated 22565.42919921875 
[2025-02-19 11:40:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.0033832849003374577 norm:9.772499470273033e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:41:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.0033221382182091475 norm:8.302374044433236e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:41:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.0032792710699141026 norm:7.347382052103058e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:42:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0032428610138595104 norm:6.224886601557955e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:42:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.0032118484377861023 norm:5.106319804326631e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:43:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.003188597271218896 norm:4.228794205118902e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:44:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.0031724192667752504 norm:3.64509760402143e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:44:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.003159348852932453 norm:3.1175171898212284e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:45:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.003149487543851137 norm:2.7761176170315593e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:45:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.003139867214486003 norm:2.77084945992101e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:46:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.0031312357168644667 norm:2.523984403524082e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:46:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.0031247579026967287 norm:2.1867330360691994e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:47:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.0031187788117676973 norm:2.152658998966217e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:47:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.0031131794676184654 norm:1.9445857105893083e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:48:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.0031086693052202463 norm:1.8256465409649536e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:48:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0031047083903104067 norm:1.7673568436293863e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:49:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.003101102542132139 norm:1.7322821804555133e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:49:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.0030980317387729883 norm:1.681724279478658e-05 max memory_allocated 22565.42919921875 
[2025-02-19 11:49:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-19 11:50:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.00424335477873683 norm:0.0004153153859078884 max memory_allocated 22565.60107421875 
[2025-02-19 11:50:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.0037513445131480694 norm:0.00013712095096707344 max memory_allocated 22565.60107421875 
[2025-02-19 11:51:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.0036488394252955914 norm:9.329070599051192e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:51:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.003603828838095069 norm:7.512048614444211e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:52:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.003568098181858659 norm:6.238059722818434e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:52:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.003541071666404605 norm:4.8095833335537463e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:53:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.00352249457500875 norm:3.9600261516170576e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:53:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.0035071540623903275 norm:3.292481414973736e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:54:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.0034970822744071484 norm:2.9494900445570238e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:54:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.0034883657936006784 norm:2.7557256544241682e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:55:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.0034807606134563684 norm:2.3218392016133294e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:55:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.0034732408821582794 norm:2.101965219480917e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:56:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.0034670087043195963 norm:1.929228710650932e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:56:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.0034611430019140244 norm:1.857646566350013e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:57:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.0034566600807011127 norm:1.7290381947532296e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:57:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.003452987177297473 norm:1.6273574146907777e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:58:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.0034503592178225517 norm:1.4546483726007864e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:58:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.003448761999607086 norm:1.4245037164073437e-05 max memory_allocated 22565.60107421875 
[2025-02-19 11:59:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.0034462520852684975 norm:1.3292846233525779e-05 max memory_allocated 22565.60107421875 
[2025-02-19 12:00:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.0034425747580826283 norm:1.2584729120135307e-05 max memory_allocated 22565.60107421875 
[2025-02-19 12:00:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-19 12:00:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.00498585868626833 norm:0.000544403272215277 max memory_allocated 22565.77294921875 
[2025-02-19 12:01:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.004197590984404087 norm:0.00014464036212302744 max memory_allocated 22565.77294921875 
[2025-02-19 12:01:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.0040887794457376 norm:9.268476424040273e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:02:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.004056571517139673 norm:8.239594899350777e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:02:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.004030988551676273 norm:7.582716352771968e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:03:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.004001154098659754 norm:6.0338148614391685e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:03:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.003979384899139404 norm:4.8444846470374614e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:04:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.003964507952332497 norm:4.1329469240736216e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:04:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.003951819147914648 norm:3.2839980121934786e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:05:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.0039418465457856655 norm:3.1512739951722324e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:05:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.003933239262551069 norm:2.7545500415726565e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:06:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.0039262594655156136 norm:2.6494115445530042e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:06:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.003921559080481529 norm:2.18095192394685e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:07:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.003917825873941183 norm:1.9673158021760173e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:07:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.003914953675121069 norm:2.047824818873778e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:08:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.003912256099283695 norm:1.944028917932883e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:08:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.00390870776027441 norm:1.7222671885974705e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:09:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.003906469326466322 norm:1.6662972484482452e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:09:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.0039044474251568317 norm:1.554465779918246e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:10:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.0039014029316604137 norm:1.4371978977578692e-05 max memory_allocated 22565.77294921875 
[2025-02-19 12:10:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-19 12:11:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.004955179989337921 norm:0.00024408807803411037 max memory_allocated 22565.94482421875 
[2025-02-19 12:11:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.004684144631028175 norm:0.00011746475502150133 max memory_allocated 22565.94482421875 
[2025-02-19 12:12:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.004609348252415657 norm:8.80170046002604e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:12:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.004561305977404118 norm:6.479411240434274e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:13:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.0045265499502420425 norm:4.558486398309469e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:13:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.004503487143665552 norm:3.7286397855496034e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:14:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.004486741963773966 norm:3.130893674097024e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:14:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.004472590517252684 norm:2.6017401978606358e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:15:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.00446174805983901 norm:2.2542011720361188e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:15:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.004453361500054598 norm:2.0558067262754776e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:16:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.004446312319487333 norm:1.8035452740150504e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:16:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.004440028220415115 norm:1.6983034583972767e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:17:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.004435157403349876 norm:1.5837038517929614e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:17:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.004431493580341339 norm:1.3902603313908912e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:18:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.004428154788911343 norm:1.3141306226316374e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:18:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.004424241837114096 norm:1.1810741852968931e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:19:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.004422219470143318 norm:1.0938358173007146e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:19:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.004420341458171606 norm:1.0711948561947793e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:20:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.004418742842972279 norm:1.0499196832824964e-05 max memory_allocated 22565.94482421875 
[2025-02-19 12:20:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.004416212905198336 norm:9.961742762243375e-06 max memory_allocated 22565.94482421875 
[2025-02-19 12:20:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-19 12:21:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.005761871580034494 norm:0.0004150828463025391 max memory_allocated 22566.11669921875 
[2025-02-19 12:22:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.005302213132381439 norm:0.00014385362737812102 max memory_allocated 22566.11669921875 
[2025-02-19 12:22:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.005202458705753088 norm:0.00010377018043072894 max memory_allocated 22566.11669921875 
[2025-02-19 12:23:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.005152591038495302 norm:8.206105121644214e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:23:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.005116989836096764 norm:6.622355431318283e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:24:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.005089513957500458 norm:5.327066173776984e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:24:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.005068007856607437 norm:4.1386050725122914e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:25:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.005052703432738781 norm:3.706079951371066e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:25:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.005039844196289778 norm:3.0374681955436245e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:26:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.005031515844166279 norm:2.800733273033984e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:26:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.005024033598601818 norm:2.4739485525060445e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:27:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.005016093607991934 norm:2.2304928279481828e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:27:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.005010677967220545 norm:2.1095162082929164e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:28:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.0050063543021678925 norm:1.957945278263651e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:28:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.005001353099942207 norm:1.8503365936339833e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:29:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.004996730014681816 norm:1.747801070450805e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:29:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.004993055015802383 norm:1.6539039279450662e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:30:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.004990249872207642 norm:1.4757411918253638e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:30:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.004987870343029499 norm:1.4917098269506823e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:31:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.004985439125448465 norm:1.3379401934798807e-05 max memory_allocated 22566.11669921875 
[2025-02-19 12:31:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-19 12:31:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.006835174281150103 norm:0.0003449916257523 max memory_allocated 22566.28857421875 
[2025-02-19 12:32:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.006302296184003353 norm:0.00017954139912035316 max memory_allocated 22566.28857421875 
[2025-02-19 12:32:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.006163003854453564 norm:0.00013594093616120517 max memory_allocated 22566.28857421875 
[2025-02-19 12:33:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.006067064590752125 norm:0.0001067665361915715 max memory_allocated 22566.28857421875 
[2025-02-19 12:33:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.005994787905365229 norm:8.597326814197004e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:34:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.0059501174837350845 norm:7.305594772333279e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:34:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.0059123593382537365 norm:6.13103766227141e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:35:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.005881101358681917 norm:5.4324078519130126e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:35:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.0058554718270897865 norm:4.728385101770982e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:36:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.005833555478602648 norm:4.3884723709197715e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:36:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.005816528107970953 norm:4.005539449281059e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:37:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.005800867453217506 norm:3.6157973227091134e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:38:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.005786917172372341 norm:3.37222772941459e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:38:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.005776910576969385 norm:3.142942296108231e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:39:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.005775724537670612 norm:2.9714046831941232e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:39:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.005764937959611416 norm:2.9541541152866557e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:40:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.005760429427027702 norm:2.9785287551931106e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:40:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.005752769764512777 norm:2.547056647017598e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:41:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.005748821422457695 norm:2.607460191939026e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:41:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.005746836308389902 norm:2.6141999114770442e-05 max memory_allocated 22566.28857421875 
[2025-02-19 12:41:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-19 12:42:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.00732261361554265 norm:0.00032917564385570586 max memory_allocated 22566.46044921875 
[2025-02-19 12:42:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.006840845104306936 norm:0.00014708645176142454 max memory_allocated 22566.46044921875 
[2025-02-19 12:43:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.006722638849169016 norm:0.00011778398766182363 max memory_allocated 22566.46044921875 
[2025-02-19 12:43:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.006658806931227446 norm:9.711176971904933e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:44:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.00661460729315877 norm:8.044885180424899e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:44:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.00658269040286541 norm:6.98992153047584e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:45:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.006555729079991579 norm:4.840553447138518e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:45:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.006540031172335148 norm:3.536040094331838e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:46:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.0065256450325250626 norm:2.955689706141129e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:46:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.006514186505228281 norm:2.7281592338113114e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:47:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.006503798067569733 norm:2.248036071250681e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:47:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.006497103720903397 norm:2.030140967690386e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:48:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.006489679682999849 norm:2.1038111299276352e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:48:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.0064831264317035675 norm:2.0394329112605192e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:49:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.006478184834122658 norm:1.74081669683801e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:49:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.006474122405052185 norm:1.637912828300614e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:50:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.006469746120274067 norm:1.949970464920625e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:50:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.006467319559305906 norm:2.3277636501006782e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:51:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.006464200094342232 norm:2.0064026102772914e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:51:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.006460466422140598 norm:1.6243151549133472e-05 max memory_allocated 22566.46044921875 
[2025-02-19 12:52:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-19 12:52:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.008085113950073719 norm:0.0003164516529068351 max memory_allocated 22566.63232421875 
[2025-02-19 12:53:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.007690655067563057 norm:0.00014413526514545083 max memory_allocated 22566.63232421875 
[2025-02-19 12:53:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.0076056174002587795 norm:0.00012331838661339134 max memory_allocated 22566.63232421875 
[2025-02-19 12:54:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.007545103318989277 norm:9.291803871747106e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:54:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.007501229178160429 norm:9.922985918819904e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:55:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.007467743009328842 norm:4.8328362026950344e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:55:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.007447456941008568 norm:3.7832101952517405e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:56:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.007430566940456629 norm:3.2412324799224734e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:56:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.007416407577693462 norm:2.8014746931148693e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:57:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.007406866643577814 norm:2.3203485397971235e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:57:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.007395458873361349 norm:2.415570088487584e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:58:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.007387569174170494 norm:2.1914951503276825e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:58:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.007382155861705542 norm:1.9206034266971983e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:59:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.007375889923423529 norm:1.8005352103500627e-05 max memory_allocated 22566.63232421875 
[2025-02-19 12:59:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.007369654253125191 norm:1.9582304958021268e-05 max memory_allocated 22566.63232421875 
[2025-02-19 13:00:28 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.007364010438323021 norm:1.7797287000576034e-05 max memory_allocated 22566.63232421875 
[2025-02-19 13:00:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.0073599619790911674 norm:1.57644426508341e-05 max memory_allocated 22566.63232421875 
[2025-02-19 13:01:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.007355482783168554 norm:1.462297404941637e-05 max memory_allocated 22566.63232421875 
[2025-02-19 13:02:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.007352727465331554 norm:1.3162773939257022e-05 max memory_allocated 22566.63232421875 
[2025-02-19 13:02:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.007350500673055649 norm:1.270140455744695e-05 max memory_allocated 22566.63232421875 
[2025-02-19 13:02:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-19 13:02:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:03:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.009498564526438713 norm:0.002254226477816701 max memory_allocated 22566.91943359375 
[2025-02-19 13:03:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.008989155292510986 norm:0.0014000659575685859 max memory_allocated 22566.91943359375 
[2025-02-19 13:04:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.008843537420034409 norm:0.00107881473377347 max memory_allocated 22566.91943359375 
[2025-02-19 13:04:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.008746528066694736 norm:0.0008777686743997037 max memory_allocated 22566.91943359375 
[2025-02-19 13:05:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.008669707924127579 norm:0.0006922531756572425 max memory_allocated 22566.91943359375 
[2025-02-19 13:05:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.008618772029876709 norm:0.0005656145513057709 max memory_allocated 22566.91943359375 
[2025-02-19 13:06:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.008582361042499542 norm:0.0004799836315214634 max memory_allocated 22566.91943359375 
[2025-02-19 13:06:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.008555227890610695 norm:0.00041427291580475867 max memory_allocated 22566.91943359375 
[2025-02-19 13:07:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.008530150167644024 norm:0.00034155292087234557 max memory_allocated 22566.91943359375 
[2025-02-19 13:07:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.008510993793606758 norm:0.0002974198723677546 max memory_allocated 22566.91943359375 
[2025-02-19 13:08:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.008507153019309044 norm:0.0002981480793096125 max memory_allocated 22566.91943359375 
[2025-02-19 13:08:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.008497631177306175 norm:0.000253432837780565 max memory_allocated 22566.91943359375 
[2025-02-19 13:09:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.00848476868122816 norm:0.00024105423653963953 max memory_allocated 22566.91943359375 
[2025-02-19 13:09:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.00848452840000391 norm:0.00029241773881949484 max memory_allocated 22566.91943359375 
[2025-02-19 13:10:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.008612887002527714 norm:0.0006927356589585543 max memory_allocated 22566.91943359375 
[2025-02-19 13:10:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.00909652654081583 norm:0.001242276863195002 max memory_allocated 22566.91943359375 
[2025-02-19 13:11:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.008636172860860825 norm:0.0006617979379370809 max memory_allocated 22566.91943359375 
[2025-02-19 13:11:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.008474708534777164 norm:0.00027697315090335906 max memory_allocated 22566.91943359375 
[2025-02-19 13:12:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.008456232026219368 norm:0.00020042706455569714 max memory_allocated 22566.91943359375 
[2025-02-19 13:12:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.008453533984720707 norm:0.00021533641847781837 max memory_allocated 22566.91943359375 
[2025-02-19 13:13:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-19 13:13:08 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:13:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.010723931714892387 norm:0.001822230638936162 max memory_allocated 22567.09130859375 
[2025-02-19 13:14:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.010262152180075645 norm:0.0010267276084050536 max memory_allocated 22567.09130859375 
[2025-02-19 13:14:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.010110427625477314 norm:0.000969874148722738 max memory_allocated 22567.09130859375 
[2025-02-19 13:15:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.009993204846978188 norm:0.000665100640617311 max memory_allocated 22567.09130859375 
[2025-02-19 13:15:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.009919465519487858 norm:0.0005807650741189718 max memory_allocated 22567.09130859375 
[2025-02-19 13:16:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.009871168062090874 norm:0.000481372611830011 max memory_allocated 22567.09130859375 
[2025-02-19 13:16:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.009835254400968552 norm:0.0005047356826253235 max memory_allocated 22567.09130859375 
[2025-02-19 13:17:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.009802885353565216 norm:0.00040400141733698547 max memory_allocated 22567.09130859375 
[2025-02-19 13:17:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.009773408062756062 norm:0.0003708432777784765 max memory_allocated 22567.09130859375 
[2025-02-19 13:18:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.009755348786711693 norm:0.0003463087778072804 max memory_allocated 22567.09130859375 
[2025-02-19 13:18:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.00973507296293974 norm:0.00029400730272755027 max memory_allocated 22567.09130859375 
[2025-02-19 13:19:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.00971831101924181 norm:0.0002642761683091521 max memory_allocated 22567.09130859375 
[2025-02-19 13:19:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.009711451828479767 norm:0.0002565482282079756 max memory_allocated 22567.09130859375 
[2025-02-19 13:20:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.009733901359140873 norm:0.0003620157658588141 max memory_allocated 22567.09130859375 
[2025-02-19 13:20:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.009726217947900295 norm:0.0004011192359030247 max memory_allocated 22567.09130859375 
[2025-02-19 13:21:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.009675392881035805 norm:0.00022660251124762 max memory_allocated 22567.09130859375 
[2025-02-19 13:21:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.009659834206104279 norm:0.00019717183022294194 max memory_allocated 22567.09130859375 
[2025-02-19 13:22:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.009651007130742073 norm:0.00017871511226985604 max memory_allocated 22567.09130859375 
[2025-02-19 13:22:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.009643832221627235 norm:0.00016621325630694628 max memory_allocated 22567.09130859375 
[2025-02-19 13:23:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.009640008211135864 norm:0.00016280257841572165 max memory_allocated 22567.09130859375 
[2025-02-19 13:23:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-19 13:23:34 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:24:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.020345935598015785 norm:0.004767042584717274 max memory_allocated 22567.26318359375 
[2025-02-19 13:24:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.01545717753469944 norm:0.0033770748414099216 max memory_allocated 22567.26318359375 
[2025-02-19 13:25:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.014690948650240898 norm:0.004321618005633354 max memory_allocated 22567.26318359375 
[2025-02-19 13:25:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.014225417748093605 norm:0.003553896676748991 max memory_allocated 22567.26318359375 
[2025-02-19 13:26:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.01390888448804617 norm:0.0028334022499620914 max memory_allocated 22567.26318359375 
[2025-02-19 13:26:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.013705646619200706 norm:0.002285214141011238 max memory_allocated 22567.26318359375 
[2025-02-19 13:27:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.013529236428439617 norm:0.0018564151832833886 max memory_allocated 22567.26318359375 
[2025-02-19 13:27:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.013369102962315083 norm:0.0016163804102689028 max memory_allocated 22567.26318359375 
[2025-02-19 13:28:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.013305890373885632 norm:0.0014144229935482144 max memory_allocated 22567.26318359375 
[2025-02-19 13:28:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.013245932757854462 norm:0.0012672309530898929 max memory_allocated 22567.26318359375 
[2025-02-19 13:29:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.013156656175851822 norm:0.0011305282823741436 max memory_allocated 22567.26318359375 
[2025-02-19 13:29:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.013114416040480137 norm:0.000888823764398694 max memory_allocated 22567.26318359375 
[2025-02-19 13:30:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.013068237341940403 norm:0.0009207029943354428 max memory_allocated 22567.26318359375 
[2025-02-19 13:30:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.013027634471654892 norm:0.0009253823664039373 max memory_allocated 22567.26318359375 
[2025-02-19 13:31:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.012993581593036652 norm:0.0008252052939496934 max memory_allocated 22567.26318359375 
[2025-02-19 13:31:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.012985000386834145 norm:0.0008307903772220016 max memory_allocated 22567.26318359375 
[2025-02-19 13:32:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.0129358796402812 norm:0.0007672496140003204 max memory_allocated 22567.26318359375 
[2025-02-19 13:32:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.012951422482728958 norm:0.0007272337097674608 max memory_allocated 22567.26318359375 
[2025-02-19 13:33:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.012900713831186295 norm:0.000695652503054589 max memory_allocated 22567.26318359375 
[2025-02-19 13:33:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.012873212806880474 norm:0.0006120736361481249 max memory_allocated 22567.26318359375 
[2025-02-19 13:33:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-19 13:34:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:34:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.030320830643177032 norm:0.00560733349993825 max memory_allocated 22567.43505859375 
[2025-02-19 13:35:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.027677129954099655 norm:0.0033988822251558304 max memory_allocated 22567.43505859375 
[2025-02-19 13:35:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.025990815833210945 norm:0.002745784120634198 max memory_allocated 22567.43505859375 
[2025-02-19 13:36:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.02493671514093876 norm:0.0025676563382148743 max memory_allocated 22567.43505859375 
[2025-02-19 13:36:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.02413300983607769 norm:0.002370184287428856 max memory_allocated 22567.43505859375 
[2025-02-19 13:37:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.023474277928471565 norm:0.0019190679304301739 max memory_allocated 22567.43505859375 
[2025-02-19 13:37:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.02296243980526924 norm:0.0018562929471954703 max memory_allocated 22567.43505859375 
[2025-02-19 13:38:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.022583050653338432 norm:0.0017867815913632512 max memory_allocated 22567.43505859375 
[2025-02-19 13:38:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.022284910082817078 norm:0.0017129566986113787 max memory_allocated 22567.43505859375 
[2025-02-19 13:39:08 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.022006260231137276 norm:0.0015592296840623021 max memory_allocated 22567.43505859375 
[2025-02-19 13:39:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.02183276228606701 norm:0.0016213401686400175 max memory_allocated 22567.43505859375 
[2025-02-19 13:40:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.02169102244079113 norm:0.0015232411678880453 max memory_allocated 22567.43505859375 
[2025-02-19 13:40:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.02160969376564026 norm:0.0016737516270950437 max memory_allocated 22567.43505859375 
[2025-02-19 13:41:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.0216845516115427 norm:0.0023201974108815193 max memory_allocated 22567.43505859375 
[2025-02-19 13:41:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.022168394178152084 norm:0.0034811331424862146 max memory_allocated 22567.43505859375 
[2025-02-19 13:42:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.021691609174013138 norm:0.002306070877239108 max memory_allocated 22567.43505859375 
[2025-02-19 13:42:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.021262533962726593 norm:0.0012888835044577718 max memory_allocated 22567.43505859375 
[2025-02-19 13:43:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.021178457885980606 norm:0.0012944238260388374 max memory_allocated 22567.43505859375 
[2025-02-19 13:43:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.02112399786710739 norm:0.0012805251171812415 max memory_allocated 22567.43505859375 
[2025-02-19 13:44:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.02106645703315735 norm:0.0012291627936065197 max memory_allocated 22567.43505859375 
[2025-02-19 13:44:24 root] (main_calibration.py 365): INFO 19998.88533759117
[2025-02-19 13:45:09 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-19 13:46:12 root] (main_calibration.py 158): INFO wikitext2 : 5.484155178070068
[2025-02-19 13:46:12 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-19 13:47:50 root] (main_calibration.py 158): INFO c4 : 6.9908061027526855
[2025-02-19 15:25:18 root] (main_calibration.py 169): INFO {'wikitext2': 5.484155178070068, 'c4': 6.9908061027526855, 'results': {'hellaswag': {'acc': 0.5670185222067318, 'acc_stderr': 0.004944755230598384, 'acc_norm': 0.7294363672575184, 'acc_norm_stderr': 0.004433430790349411}, 'arc_easy': {'acc': 0.6948653198653199, 'acc_stderr': 0.00944853109416391, 'acc_norm': 0.5340909090909091, 'acc_norm_stderr': 0.010235908103438692}, 'arc_challenge': {'acc': 0.40273037542662116, 'acc_stderr': 0.014332236306790144, 'acc_norm': 0.4035836177474403, 'acc_norm_stderr': 0.014337158914268445}, 'boolq': {'acc': 0.7128440366972477, 'acc_stderr': 0.007913137641689019}, 'piqa': {'acc': 0.7840043525571273, 'acc_stderr': 0.009601236303553555, 'acc_norm': 0.7709466811751904, 'acc_norm_stderr': 0.009804509865175504}, 'winogrande': {'acc': 0.6716653512233622, 'acc_stderr': 0.013198299449717886}}, 'versions': {'hellaswag': 0, 'arc_easy': 0, 'arc_challenge': 0, 'boolq': 1, 'piqa': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
