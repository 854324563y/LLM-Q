[2025-02-20 07:06:40 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-13b-hf-w4a4', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-20 07:06:41 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-20 07:06:41 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-20 07:06:41 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-20 07:06:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-20 07:06:46 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:07:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.05997741222381592 norm:0.04414570331573486 max memory_allocated 29271.02001953125 
[2025-02-20 07:08:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.03728152811527252 norm:0.02424122393131256 max memory_allocated 29271.02001953125 
[2025-02-20 07:09:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.028679821640253067 norm:0.01706257462501526 max memory_allocated 29271.02001953125 
[2025-02-20 07:10:00 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.024935131892561913 norm:0.013966336846351624 max memory_allocated 29271.02001953125 
[2025-02-20 07:10:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.023099267855286598 norm:0.013989821076393127 max memory_allocated 29271.02001953125 
[2025-02-20 07:11:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.021857818588614464 norm:0.010431298054754734 max memory_allocated 29271.02001953125 
[2025-02-20 07:12:27 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.021190999075770378 norm:0.009799000807106495 max memory_allocated 29271.02001953125 
[2025-02-20 07:13:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.020909681916236877 norm:0.008307724259793758 max memory_allocated 29271.02001953125 
[2025-02-20 07:14:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.02071796916425228 norm:0.007261882536113262 max memory_allocated 29271.02001953125 
[2025-02-20 07:14:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.020662466064095497 norm:0.007047866005450487 max memory_allocated 29271.02001953125 
[2025-02-20 07:15:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.020467258989810944 norm:0.006638989318162203 max memory_allocated 29271.02001953125 
[2025-02-20 07:16:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.020495343953371048 norm:0.005672188475728035 max memory_allocated 29271.02001953125 
[2025-02-20 07:17:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.02019079402089119 norm:0.005420382600277662 max memory_allocated 29271.02001953125 
[2025-02-20 07:18:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.02006026916205883 norm:0.004890690091997385 max memory_allocated 29271.02001953125 
[2025-02-20 07:18:58 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.019895365461707115 norm:0.004743028897792101 max memory_allocated 29271.02001953125 
[2025-02-20 07:19:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.019911352545022964 norm:0.004296206869184971 max memory_allocated 29271.02001953125 
[2025-02-20 07:20:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.019909819588065147 norm:0.00407231692224741 max memory_allocated 29271.02001953125 
[2025-02-20 07:21:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.020067254081368446 norm:0.005121965892612934 max memory_allocated 29271.02001953125 
[2025-02-20 07:22:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.02017461135983467 norm:0.0043553742580115795 max memory_allocated 29271.02001953125 
[2025-02-20 07:23:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.019903264939785004 norm:0.004976020660251379 max memory_allocated 29271.02001953125 
[2025-02-20 07:23:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-20 07:23:21 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:24:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.13040828704833984 norm:0.034037426114082336 max memory_allocated 29271.02001953125 
[2025-02-20 07:24:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.10050178319215775 norm:0.024137554690241814 max memory_allocated 29271.02001953125 
[2025-02-20 07:25:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0870552808046341 norm:0.017569389194250107 max memory_allocated 29271.02001953125 
[2025-02-20 07:26:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.08118918538093567 norm:0.013217760249972343 max memory_allocated 29271.02001953125 
[2025-02-20 07:27:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.07793499529361725 norm:0.01031831931322813 max memory_allocated 29271.02001953125 
[2025-02-20 07:28:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.07614807039499283 norm:0.008592428639531136 max memory_allocated 29271.02001953125 
[2025-02-20 07:29:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.07481440901756287 norm:0.007170284166932106 max memory_allocated 29271.02001953125 
[2025-02-20 07:29:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.07402084022760391 norm:0.006260031368583441 max memory_allocated 29271.02001953125 
[2025-02-20 07:30:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.07340502738952637 norm:0.00565233314409852 max memory_allocated 29271.02001953125 
[2025-02-20 07:31:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.07294689118862152 norm:0.005269925110042095 max memory_allocated 29271.02001953125 
[2025-02-20 07:32:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.07270368933677673 norm:0.005249200854450464 max memory_allocated 29271.02001953125 
[2025-02-20 07:33:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.07245737314224243 norm:0.005041530355811119 max memory_allocated 29271.02001953125 
[2025-02-20 07:34:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.07226566970348358 norm:0.005018089897930622 max memory_allocated 29271.02001953125 
[2025-02-20 07:34:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.07210065424442291 norm:0.004974912386387587 max memory_allocated 29271.02001953125 
[2025-02-20 07:35:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.07192745059728622 norm:0.0048799715004861355 max memory_allocated 29271.02001953125 
[2025-02-20 07:36:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.07181545346975327 norm:0.004953944124281406 max memory_allocated 29271.02001953125 
[2025-02-20 07:37:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.07169050723314285 norm:0.004764334298670292 max memory_allocated 29271.02001953125 
[2025-02-20 07:38:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.07167065888643265 norm:0.004760658368468285 max memory_allocated 29271.02001953125 
[2025-02-20 07:38:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0715467631816864 norm:0.004546122159808874 max memory_allocated 29271.02001953125 
[2025-02-20 07:39:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.07152843475341797 norm:0.0045701563358306885 max memory_allocated 29271.02001953125 
[2025-02-20 07:39:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-20 07:40:03 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:40:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.15779559314250946 norm:0.025175876915454865 max memory_allocated 29271.39501953125 
[2025-02-20 07:41:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.13726557791233063 norm:0.02028002217411995 max memory_allocated 29271.39501953125 
[2025-02-20 07:42:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.12367652356624603 norm:0.014284167438745499 max memory_allocated 29271.39501953125 
[2025-02-20 07:43:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.11748969554901123 norm:0.010571436025202274 max memory_allocated 29271.39501953125 
[2025-02-20 07:44:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.11447016894817352 norm:0.008120110258460045 max memory_allocated 29271.39501953125 
[2025-02-20 07:44:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.11244960874319077 norm:0.006470441818237305 max memory_allocated 29271.39501953125 
[2025-02-20 07:45:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.11124736070632935 norm:0.005324278026819229 max memory_allocated 29271.39501953125 
[2025-02-20 07:46:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.1105383113026619 norm:0.005151733756065369 max memory_allocated 29271.39501953125 
[2025-02-20 07:47:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.10982280969619751 norm:0.005124098155647516 max memory_allocated 29271.39501953125 
[2025-02-20 07:48:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.10938158631324768 norm:0.0049569797702133656 max memory_allocated 29271.39501953125 
[2025-02-20 07:49:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.10923726111650467 norm:0.004926371853798628 max memory_allocated 29271.39501953125 
[2025-02-20 07:49:54 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.10893410444259644 norm:0.00487580755725503 max memory_allocated 29271.39501953125 
[2025-02-20 07:50:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.10870098322629929 norm:0.004817149601876736 max memory_allocated 29271.39501953125 
[2025-02-20 07:51:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.10858583450317383 norm:0.004657241050153971 max memory_allocated 29271.39501953125 
[2025-02-20 07:52:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.10842402279376984 norm:0.0045663719065487385 max memory_allocated 29271.39501953125 
[2025-02-20 07:53:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.10842007398605347 norm:0.004410530906170607 max memory_allocated 29271.39501953125 
[2025-02-20 07:54:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.10831785947084427 norm:0.004393451847136021 max memory_allocated 29271.39501953125 
[2025-02-20 07:54:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.10825690627098083 norm:0.004239327739924192 max memory_allocated 29271.39501953125 
[2025-02-20 07:55:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.10821191966533661 norm:0.004213050939142704 max memory_allocated 29271.39501953125 
[2025-02-20 07:56:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.10822029411792755 norm:0.004114226903766394 max memory_allocated 29271.39501953125 
[2025-02-20 07:56:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-20 07:57:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.4022320508956909 norm:0.1959581822156906 max memory_allocated 29271.39501953125 
[2025-02-20 07:58:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.2974311113357544 norm:0.058616697788238525 max memory_allocated 29271.39501953125 
[2025-02-20 07:59:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.24306455254554749 norm:0.024597346782684326 max memory_allocated 29271.39501953125 
[2025-02-20 08:00:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.22666828334331512 norm:0.026299865916371346 max memory_allocated 29271.39501953125 
[2025-02-20 08:00:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.21906334161758423 norm:0.02316989004611969 max memory_allocated 29271.39501953125 
[2025-02-20 08:01:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.21089856326580048 norm:0.021427536383271217 max memory_allocated 29271.39501953125 
[2025-02-20 08:02:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.19962620735168457 norm:0.01830882392823696 max memory_allocated 29271.39501953125 
[2025-02-20 08:03:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.19122634828090668 norm:0.01439134031534195 max memory_allocated 29271.39501953125 
[2025-02-20 08:04:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.18948397040367126 norm:0.013105224817991257 max memory_allocated 29271.39501953125 
[2025-02-20 08:04:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.18620944023132324 norm:0.012850809842348099 max memory_allocated 29271.39501953125 
[2025-02-20 08:05:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.18418414890766144 norm:0.012747776694595814 max memory_allocated 29271.39501953125 
[2025-02-20 08:06:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.18230243027210236 norm:0.012490963563323021 max memory_allocated 29271.39501953125 
[2025-02-20 08:07:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.1816355437040329 norm:0.011211824603378773 max memory_allocated 29271.39501953125 
[2025-02-20 08:08:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.18072274327278137 norm:0.01029879879206419 max memory_allocated 29271.39501953125 
[2025-02-20 08:09:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.1791353076696396 norm:0.009770091623067856 max memory_allocated 29271.39501953125 
[2025-02-20 08:09:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.17895731329917908 norm:0.010213423520326614 max memory_allocated 29271.39501953125 
[2025-02-20 08:10:43 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.17849485576152802 norm:0.009684132412075996 max memory_allocated 29271.39501953125 
[2025-02-20 08:11:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.1770593225955963 norm:0.009602058678865433 max memory_allocated 29271.39501953125 
[2025-02-20 08:12:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.17681342363357544 norm:0.008732193149626255 max memory_allocated 29271.39501953125 
[2025-02-20 08:13:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.17678013443946838 norm:0.008551610633730888 max memory_allocated 29271.39501953125 
[2025-02-20 08:13:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-20 08:14:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.22923128306865692 norm:0.010562426410615444 max memory_allocated 29271.39501953125 
[2025-02-20 08:15:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.20839685201644897 norm:0.004513663239777088 max memory_allocated 29271.39501953125 
[2025-02-20 08:15:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.19545197486877441 norm:0.0027300759684294462 max memory_allocated 29271.39501953125 
[2025-02-20 08:16:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.1898757815361023 norm:0.002049472648650408 max memory_allocated 29271.39501953125 
[2025-02-20 08:17:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.18668706715106964 norm:0.0015998251037672162 max memory_allocated 29271.39501953125 
[2025-02-20 08:18:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.18419089913368225 norm:0.0013163283001631498 max memory_allocated 29271.39501953125 
[2025-02-20 08:19:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.18274208903312683 norm:0.0011696560541167855 max memory_allocated 29271.39501953125 
[2025-02-20 08:20:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.18165263533592224 norm:0.0011098135728389025 max memory_allocated 29271.39501953125 
[2025-02-20 08:20:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.18098843097686768 norm:0.0010886460077017546 max memory_allocated 29271.39501953125 
[2025-02-20 08:21:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.18052661418914795 norm:0.0010812294203788042 max memory_allocated 29271.39501953125 
[2025-02-20 08:22:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.18010365962982178 norm:0.001078840927220881 max memory_allocated 29271.39501953125 
[2025-02-20 08:23:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.17966631054878235 norm:0.0010525687830522656 max memory_allocated 29271.39501953125 
[2025-02-20 08:24:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.17923079431056976 norm:0.001027610502205789 max memory_allocated 29271.39501953125 
[2025-02-20 08:24:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.1790103167295456 norm:0.0010275027016177773 max memory_allocated 29271.39501953125 
[2025-02-20 08:25:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.17870761454105377 norm:0.000996535294689238 max memory_allocated 29271.39501953125 
[2025-02-20 08:26:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.17850418388843536 norm:0.0009985038777813315 max memory_allocated 29271.39501953125 
[2025-02-20 08:27:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.17837803065776825 norm:0.0009864016901701689 max memory_allocated 29271.39501953125 
[2025-02-20 08:28:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.17809225618839264 norm:0.0009692020830698311 max memory_allocated 29271.39501953125 
[2025-02-20 08:29:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.1779003143310547 norm:0.000958564574830234 max memory_allocated 29271.39501953125 
[2025-02-20 08:29:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.1777634620666504 norm:0.0009697572095319629 max memory_allocated 29271.39501953125 
[2025-02-20 08:30:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-20 08:30:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.26096203923225403 norm:0.012168829329311848 max memory_allocated 29271.81298828125 
[2025-02-20 08:31:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.2372194528579712 norm:0.004907811060547829 max memory_allocated 29271.81298828125 
[2025-02-20 08:32:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.22111208736896515 norm:0.0028924690559506416 max memory_allocated 29271.81298828125 
[2025-02-20 08:33:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.21467751264572144 norm:0.0018743841210380197 max memory_allocated 29271.81298828125 
[2025-02-20 08:34:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.2113736867904663 norm:0.0013568990398198366 max memory_allocated 29271.81298828125 
[2025-02-20 08:35:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.20927231013774872 norm:0.0011993789812549949 max memory_allocated 29271.81298828125 
[2025-02-20 08:35:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.2077476680278778 norm:0.001132220379076898 max memory_allocated 29271.81298828125 
[2025-02-20 08:36:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.20663614571094513 norm:0.001064579002559185 max memory_allocated 29271.81298828125 
[2025-02-20 08:37:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.2057226300239563 norm:0.0010191792389377952 max memory_allocated 29271.81298828125 
[2025-02-20 08:38:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.20515747368335724 norm:0.0009846377652138472 max memory_allocated 29271.81298828125 
[2025-02-20 08:39:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.20465880632400513 norm:0.000954260234721005 max memory_allocated 29271.81298828125 
[2025-02-20 08:40:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.20430788397789001 norm:0.0009390562772750854 max memory_allocated 29271.81298828125 
[2025-02-20 08:40:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.2040109783411026 norm:0.0009371921187266707 max memory_allocated 29271.81298828125 
[2025-02-20 08:41:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.20366689562797546 norm:0.0009313273476436734 max memory_allocated 29271.81298828125 
[2025-02-20 08:42:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.20349043607711792 norm:0.0008993666269816458 max memory_allocated 29271.81298828125 
[2025-02-20 08:43:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.20336507260799408 norm:0.0008867622818797827 max memory_allocated 29271.81298828125 
[2025-02-20 08:44:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.20326367020606995 norm:0.000882533669937402 max memory_allocated 29271.81298828125 
[2025-02-20 08:44:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.20305657386779785 norm:0.0008725352818146348 max memory_allocated 29271.81298828125 
[2025-02-20 08:45:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.20302483439445496 norm:0.0008746591047383845 max memory_allocated 29271.81298828125 
[2025-02-20 08:46:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.20269319415092468 norm:0.0008636788697913289 max memory_allocated 29271.81298828125 
[2025-02-20 08:46:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-20 08:47:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.29826992750167847 norm:0.024454878643155098 max memory_allocated 29271.81298828125 
[2025-02-20 08:48:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.2680613100528717 norm:0.011528891511261463 max memory_allocated 29271.81298828125 
[2025-02-20 08:49:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.2451809048652649 norm:0.002901115221902728 max memory_allocated 29271.81298828125 
[2025-02-20 08:50:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.23791739344596863 norm:0.0016647509764879942 max memory_allocated 29271.81298828125 
[2025-02-20 08:50:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.23437348008155823 norm:0.001431155949831009 max memory_allocated 29271.81298828125 
[2025-02-20 08:51:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.2317100167274475 norm:0.0012872660299763083 max memory_allocated 29271.81298828125 
[2025-02-20 08:52:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.22962819039821625 norm:0.0011869287118315697 max memory_allocated 29271.81298828125 
[2025-02-20 08:53:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.22811008989810944 norm:0.0011394929606467485 max memory_allocated 29271.81298828125 
[2025-02-20 08:54:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.2271735519170761 norm:0.0010959679493680596 max memory_allocated 29271.81298828125 
[2025-02-20 08:55:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.22638186812400818 norm:0.0010685657616704702 max memory_allocated 29271.81298828125 
[2025-02-20 08:55:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.22557581961154938 norm:0.0010471534915268421 max memory_allocated 29271.81298828125 
[2025-02-20 08:56:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.22511927783489227 norm:0.001028269063681364 max memory_allocated 29271.81298828125 
[2025-02-20 08:57:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.22474725544452667 norm:0.0010092465672641993 max memory_allocated 29271.81298828125 
[2025-02-20 08:58:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.22451533377170563 norm:0.0010108038550242782 max memory_allocated 29271.81298828125 
[2025-02-20 08:59:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.2243177592754364 norm:0.001005150144919753 max memory_allocated 29271.81298828125 
[2025-02-20 08:59:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.22413823008537292 norm:0.0010060676140710711 max memory_allocated 29271.81298828125 
[2025-02-20 09:00:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.22396919131278992 norm:0.00099687441252172 max memory_allocated 29271.81298828125 
[2025-02-20 09:01:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.2238651067018509 norm:0.0009801582200452685 max memory_allocated 29271.81298828125 
[2025-02-20 09:02:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.22380833327770233 norm:0.000987148261629045 max memory_allocated 29271.81298828125 
[2025-02-20 09:03:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.22379598021507263 norm:0.0009697400964796543 max memory_allocated 29271.81298828125 
[2025-02-20 09:03:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-20 09:04:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.34685179591178894 norm:0.014172994531691074 max memory_allocated 29271.81298828125 
[2025-02-20 09:05:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.30495685338974 norm:0.005163781810551882 max memory_allocated 29271.81298828125 
[2025-02-20 09:06:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.2759290337562561 norm:0.002265469403937459 max memory_allocated 29271.81298828125 
[2025-02-20 09:06:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.26727229356765747 norm:0.0016141687519848347 max memory_allocated 29271.81298828125 
[2025-02-20 09:07:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.26316526532173157 norm:0.0013913416769355536 max memory_allocated 29271.81298828125 
[2025-02-20 09:08:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.2599014639854431 norm:0.001289219711907208 max memory_allocated 29271.81298828125 
[2025-02-20 09:09:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.2575935125350952 norm:0.0012243558885529637 max memory_allocated 29271.81298828125 
[2025-02-20 09:10:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.25609835982322693 norm:0.0011808010749518871 max memory_allocated 29271.81298828125 
[2025-02-20 09:10:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.2550548315048218 norm:0.001161128398962319 max memory_allocated 29271.81298828125 
[2025-02-20 09:11:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.2542625367641449 norm:0.0011288446839898825 max memory_allocated 29271.81298828125 
[2025-02-20 09:12:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.253584086894989 norm:0.001107253017835319 max memory_allocated 29271.81298828125 
[2025-02-20 09:13:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.253221720457077 norm:0.0010920313652604818 max memory_allocated 29271.81298828125 
[2025-02-20 09:14:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.2529012858867645 norm:0.0010888053802773356 max memory_allocated 29271.81298828125 
[2025-02-20 09:15:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.25257810950279236 norm:0.0010751904919743538 max memory_allocated 29271.81298828125 
[2025-02-20 09:15:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.2523820400238037 norm:0.0010584336705505848 max memory_allocated 29271.81298828125 
[2025-02-20 09:16:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.25218966603279114 norm:0.0010440218029543757 max memory_allocated 29271.81298828125 
[2025-02-20 09:17:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.2520678639411926 norm:0.0010418633464723825 max memory_allocated 29271.81298828125 
[2025-02-20 09:18:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.25198036432266235 norm:0.0010296758264303207 max memory_allocated 29271.81298828125 
[2025-02-20 09:19:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.251705527305603 norm:0.0010223727440461516 max memory_allocated 29271.81298828125 
[2025-02-20 09:19:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.2516055703163147 norm:0.0010356866987422109 max memory_allocated 29271.81298828125 
[2025-02-20 09:20:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-20 09:21:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.37056785821914673 norm:0.03230120614171028 max memory_allocated 29272.37548828125 
[2025-02-20 09:21:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.3386369049549103 norm:0.020893078297376633 max memory_allocated 29272.37548828125 
[2025-02-20 09:22:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.30660760402679443 norm:0.013149114325642586 max memory_allocated 29272.37548828125 
[2025-02-20 09:23:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.29373693466186523 norm:0.008304511196911335 max memory_allocated 29272.37548828125 
[2025-02-20 09:24:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.2853355407714844 norm:0.002920531900599599 max memory_allocated 29272.37548828125 
[2025-02-20 09:25:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.2806088626384735 norm:0.0016959055792540312 max memory_allocated 29272.37548828125 
[2025-02-20 09:26:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.27763181924819946 norm:0.001542283920571208 max memory_allocated 29272.37548828125 
[2025-02-20 09:26:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.27545252442359924 norm:0.0013504659291356802 max memory_allocated 29272.37548828125 
[2025-02-20 09:27:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.27408355474472046 norm:0.0012778425589203835 max memory_allocated 29272.37548828125 
[2025-02-20 09:28:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.27284756302833557 norm:0.001177183585241437 max memory_allocated 29272.37548828125 
[2025-02-20 09:29:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.2720472812652588 norm:0.0011221208842471242 max memory_allocated 29272.37548828125 
[2025-02-20 09:30:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.27132391929626465 norm:0.0011050791945308447 max memory_allocated 29272.37548828125 
[2025-02-20 09:30:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.27093416452407837 norm:0.0010926795657724142 max memory_allocated 29272.37548828125 
[2025-02-20 09:31:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.27064576745033264 norm:0.0010618651285767555 max memory_allocated 29272.37548828125 
[2025-02-20 09:32:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.27028971910476685 norm:0.0010593284387141466 max memory_allocated 29272.37548828125 
[2025-02-20 09:33:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.27002811431884766 norm:0.0010440623154863715 max memory_allocated 29272.37548828125 
[2025-02-20 09:34:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.26988548040390015 norm:0.0010439218021929264 max memory_allocated 29272.37548828125 
[2025-02-20 09:35:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.2697872221469879 norm:0.001039220835082233 max memory_allocated 29272.37548828125 
[2025-02-20 09:35:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.26961445808410645 norm:0.0010110092116519809 max memory_allocated 29272.37548828125 
[2025-02-20 09:36:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.2694718837738037 norm:0.0010086465626955032 max memory_allocated 29272.37548828125 
[2025-02-20 09:36:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-20 09:37:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.4353759288787842 norm:0.03076833486557007 max memory_allocated 29272.37548828125 
[2025-02-20 09:38:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.3813903033733368 norm:0.013226182200014591 max memory_allocated 29272.37548828125 
[2025-02-20 09:39:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.3316724896430969 norm:0.005342819727957249 max memory_allocated 29272.37548828125 
[2025-02-20 09:40:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.31402066349983215 norm:0.0029103667475283146 max memory_allocated 29272.37548828125 
[2025-02-20 09:41:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.30732202529907227 norm:0.0023568584583699703 max memory_allocated 29272.37548828125 
[2025-02-20 09:41:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.3033345639705658 norm:0.0019724173471331596 max memory_allocated 29272.37548828125 
[2025-02-20 09:42:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.3003309369087219 norm:0.0017638454446569085 max memory_allocated 29272.37548828125 
[2025-02-20 09:43:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.29807233810424805 norm:0.0016169657465070486 max memory_allocated 29272.37548828125 
[2025-02-20 09:44:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.2963806390762329 norm:0.0015036491677165031 max memory_allocated 29272.37548828125 
[2025-02-20 09:45:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.2951843738555908 norm:0.001408107578754425 max memory_allocated 29272.37548828125 
[2025-02-20 09:46:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.2943178117275238 norm:0.001338740810751915 max memory_allocated 29272.37548828125 
[2025-02-20 09:46:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.29364925622940063 norm:0.0012649421114474535 max memory_allocated 29272.37548828125 
[2025-02-20 09:47:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.29307276010513306 norm:0.0011882786639034748 max memory_allocated 29272.37548828125 
[2025-02-20 09:48:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.29277077317237854 norm:0.0011613802053034306 max memory_allocated 29272.37548828125 
[2025-02-20 09:49:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.29238495230674744 norm:0.0010967719135805964 max memory_allocated 29272.37548828125 
[2025-02-20 09:50:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.29196876287460327 norm:0.0010700214188545942 max memory_allocated 29272.37548828125 
[2025-02-20 09:50:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.2916657626628876 norm:0.001045734970830381 max memory_allocated 29272.37548828125 
[2025-02-20 09:51:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.2912459671497345 norm:0.0010156367206946015 max memory_allocated 29272.37548828125 
[2025-02-20 09:52:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.29112857580184937 norm:0.0010388689115643501 max memory_allocated 29272.37548828125 
[2025-02-20 09:53:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.29088348150253296 norm:0.0010061050998046994 max memory_allocated 29272.37548828125 
[2025-02-20 09:53:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-20 09:54:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.41981974244117737 norm:0.021528838202357292 max memory_allocated 29272.37548828125 
[2025-02-20 09:55:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.37731584906578064 norm:0.008824939839541912 max memory_allocated 29272.37548828125 
[2025-02-20 09:56:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.3404808044433594 norm:0.0035925365518778563 max memory_allocated 29272.37548828125 
[2025-02-20 09:56:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.3272431194782257 norm:0.0022240332327783108 max memory_allocated 29272.37548828125 
[2025-02-20 09:57:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.3216201961040497 norm:0.0017633916577324271 max memory_allocated 29272.37548828125 
[2025-02-20 09:58:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.3178921043872833 norm:0.001471337745897472 max memory_allocated 29272.37548828125 
[2025-02-20 09:59:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.31548425555229187 norm:0.0013443873031064868 max memory_allocated 29272.37548828125 
[2025-02-20 10:00:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.31366580724716187 norm:0.001226179301738739 max memory_allocated 29272.37548828125 
[2025-02-20 10:01:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.3122444152832031 norm:0.0011192891979590058 max memory_allocated 29272.37548828125 
[2025-02-20 10:01:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.311271607875824 norm:0.0010751541703939438 max memory_allocated 29272.37548828125 
[2025-02-20 10:02:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.31062906980514526 norm:0.0010499998461455107 max memory_allocated 29272.37548828125 
[2025-02-20 10:03:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.31006568670272827 norm:0.001024890225380659 max memory_allocated 29272.37548828125 
[2025-02-20 10:04:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.30958324670791626 norm:0.0009909929940477014 max memory_allocated 29272.37548828125 
[2025-02-20 10:05:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.3092121183872223 norm:0.000976971467025578 max memory_allocated 29272.37548828125 
[2025-02-20 10:05:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.3089234530925751 norm:0.0009670524159446359 max memory_allocated 29272.37548828125 
[2025-02-20 10:06:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.3085247874259949 norm:0.000931125192437321 max memory_allocated 29272.37548828125 
[2025-02-20 10:07:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.30827662348747253 norm:0.0009155043517239392 max memory_allocated 29272.37548828125 
[2025-02-20 10:08:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.30791181325912476 norm:0.0009110954124480486 max memory_allocated 29272.37548828125 
[2025-02-20 10:09:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.30767348408699036 norm:0.0008724439539946616 max memory_allocated 29272.37548828125 
[2025-02-20 10:10:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.30760663747787476 norm:0.0008464805432595313 max memory_allocated 29272.37548828125 
[2025-02-20 10:10:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-20 10:11:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.4282101094722748 norm:0.027800776064395905 max memory_allocated 29272.93798828125 
[2025-02-20 10:12:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.39057114720344543 norm:0.010736397467553616 max memory_allocated 29272.93798828125 
[2025-02-20 10:12:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.35478535294532776 norm:0.003922475501894951 max memory_allocated 29272.93798828125 
[2025-02-20 10:13:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.34212353825569153 norm:0.0018669047858566046 max memory_allocated 29272.93798828125 
[2025-02-20 10:14:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.33669450879096985 norm:0.0012870697537437081 max memory_allocated 29272.93798828125 
[2025-02-20 10:15:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.3332022428512573 norm:0.0011656434508040547 max memory_allocated 29272.93798828125 
[2025-02-20 10:16:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.33066219091415405 norm:0.0010770705994218588 max memory_allocated 29272.93798828125 
[2025-02-20 10:16:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.3287905156612396 norm:0.000997291412204504 max memory_allocated 29272.93798828125 
[2025-02-20 10:17:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.32734301686286926 norm:0.0009333128109574318 max memory_allocated 29272.93798828125 
[2025-02-20 10:18:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.32627901434898376 norm:0.0008837071945890784 max memory_allocated 29272.93798828125 
[2025-02-20 10:19:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.32561105489730835 norm:0.000885087763890624 max memory_allocated 29272.93798828125 
[2025-02-20 10:20:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.3251037001609802 norm:0.0008809491991996765 max memory_allocated 29272.93798828125 
[2025-02-20 10:21:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.3247063159942627 norm:0.0008495987858623266 max memory_allocated 29272.93798828125 
[2025-02-20 10:21:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.32434701919555664 norm:0.000823852838948369 max memory_allocated 29272.93798828125 
[2025-02-20 10:22:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.32412201166152954 norm:0.0008105014567263424 max memory_allocated 29272.93798828125 
[2025-02-20 10:23:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.3238750100135803 norm:0.0007939495262689888 max memory_allocated 29272.93798828125 
[2025-02-20 10:24:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.32367241382598877 norm:0.0007851548143662512 max memory_allocated 29272.93798828125 
[2025-02-20 10:25:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.3235379457473755 norm:0.000785027164965868 max memory_allocated 29272.93798828125 
[2025-02-20 10:26:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.3233655095100403 norm:0.0007781608146615326 max memory_allocated 29272.93798828125 
[2025-02-20 10:26:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.323252409696579 norm:0.0007817731238901615 max memory_allocated 29272.93798828125 
[2025-02-20 10:27:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-20 10:27:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.45185577869415283 norm:0.01798972114920616 max memory_allocated 29272.93798828125 
[2025-02-20 10:28:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.40095722675323486 norm:0.006719138473272324 max memory_allocated 29272.93798828125 
[2025-02-20 10:29:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.36268216371536255 norm:0.0027187205851078033 max memory_allocated 29272.93798828125 
[2025-02-20 10:30:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.34951159358024597 norm:0.001652666600421071 max memory_allocated 29272.93798828125 
[2025-02-20 10:31:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.3442154824733734 norm:0.0012898736167699099 max memory_allocated 29272.93798828125 
[2025-02-20 10:32:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.34082016348838806 norm:0.0011665860656648874 max memory_allocated 29272.93798828125 
[2025-02-20 10:32:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.3382742404937744 norm:0.0010801797034218907 max memory_allocated 29272.93798828125 
[2025-02-20 10:33:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.33628156781196594 norm:0.0010219727410003543 max memory_allocated 29272.93798828125 
[2025-02-20 10:34:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.3350367248058319 norm:0.0009796608937904239 max memory_allocated 29272.93798828125 
[2025-02-20 10:35:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.33414509892463684 norm:0.0009440212743356824 max memory_allocated 29272.93798828125 
[2025-02-20 10:36:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.3334951400756836 norm:0.0009144195355474949 max memory_allocated 29272.93798828125 
[2025-02-20 10:36:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.33290860056877136 norm:0.0008813842432573438 max memory_allocated 29272.93798828125 
[2025-02-20 10:37:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.33251750469207764 norm:0.000859365682117641 max memory_allocated 29272.93798828125 
[2025-02-20 10:38:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.33217668533325195 norm:0.0008294685976579785 max memory_allocated 29272.93798828125 
[2025-02-20 10:39:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.33178162574768066 norm:0.0008096586680039763 max memory_allocated 29272.93798828125 
[2025-02-20 10:40:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.3315148651599884 norm:0.0007828888483345509 max memory_allocated 29272.93798828125 
[2025-02-20 10:41:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.33134156465530396 norm:0.0007767644710838795 max memory_allocated 29272.93798828125 
[2025-02-20 10:41:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.3311261534690857 norm:0.0007731185178272426 max memory_allocated 29272.93798828125 
[2025-02-20 10:42:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.33079761266708374 norm:0.0007739015272818506 max memory_allocated 29272.93798828125 
[2025-02-20 10:43:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.3305897116661072 norm:0.0007568586152046919 max memory_allocated 29272.93798828125 
[2025-02-20 10:43:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-20 10:44:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.44290080666542053 norm:0.02220037952065468 max memory_allocated 29272.93798828125 
[2025-02-20 10:45:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.40307241678237915 norm:0.00957542285323143 max memory_allocated 29272.93798828125 
[2025-02-20 10:46:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.37118634581565857 norm:0.004328616429120302 max memory_allocated 29272.93798828125 
[2025-02-20 10:47:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.35827431082725525 norm:0.0017954878276214004 max memory_allocated 29272.93798828125 
[2025-02-20 10:47:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.3530888557434082 norm:0.0013552999589592218 max memory_allocated 29272.93798828125 
[2025-02-20 10:48:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.34960323572158813 norm:0.001165208755992353 max memory_allocated 29272.93798828125 
[2025-02-20 10:49:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.34700340032577515 norm:0.0010597561486065388 max memory_allocated 29272.93798828125 
[2025-02-20 10:50:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.34517917037010193 norm:0.0009837014367803931 max memory_allocated 29272.93798828125 
[2025-02-20 10:51:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.34387317299842834 norm:0.0009474018006585538 max memory_allocated 29272.93798828125 
[2025-02-20 10:52:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.3427773714065552 norm:0.0009060004376806319 max memory_allocated 29272.93798828125 
[2025-02-20 10:52:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.34206146001815796 norm:0.0008708955137990415 max memory_allocated 29272.93798828125 
[2025-02-20 10:53:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.34156668186187744 norm:0.0008619541767984629 max memory_allocated 29272.93798828125 
[2025-02-20 10:54:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.3411686420440674 norm:0.0008481003460474312 max memory_allocated 29272.93798828125 
[2025-02-20 10:55:21 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.3407776355743408 norm:0.0008292435086332262 max memory_allocated 29272.93798828125 
[2025-02-20 10:56:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.340448796749115 norm:0.0008117255638353527 max memory_allocated 29272.93798828125 
[2025-02-20 10:56:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.3402724266052246 norm:0.0007937102345749736 max memory_allocated 29272.93798828125 
[2025-02-20 10:57:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.34003809094429016 norm:0.000779349822551012 max memory_allocated 29272.93798828125 
[2025-02-20 10:58:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.3398486375808716 norm:0.0007716005202382803 max memory_allocated 29272.93798828125 
[2025-02-20 10:59:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.3396786153316498 norm:0.0007710725767537951 max memory_allocated 29272.93798828125 
[2025-02-20 11:00:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.3395332098007202 norm:0.0007641235133633018 max memory_allocated 29272.93798828125 
[2025-02-20 11:00:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-20 11:01:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.44589346647262573 norm:0.018475402146577835 max memory_allocated 29273.50048828125 
[2025-02-20 11:02:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.409733384847641 norm:0.009731946513056755 max memory_allocated 29273.50048828125 
[2025-02-20 11:03:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.3818763196468353 norm:0.0054950835183262825 max memory_allocated 29273.50048828125 
[2025-02-20 11:03:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.3703625202178955 norm:0.003450692631304264 max memory_allocated 29273.50048828125 
[2025-02-20 11:04:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.36503779888153076 norm:0.0024649256374686956 max memory_allocated 29273.50048828125 
[2025-02-20 11:05:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.3614112138748169 norm:0.0018908389611169696 max memory_allocated 29273.50048828125 
[2025-02-20 11:06:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.35885027050971985 norm:0.0016229893080890179 max memory_allocated 29273.50048828125 
[2025-02-20 11:07:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.3569552004337311 norm:0.001249339897185564 max memory_allocated 29273.50048828125 
[2025-02-20 11:07:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.35555070638656616 norm:0.0011077823583036661 max memory_allocated 29273.50048828125 
[2025-02-20 11:08:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.3545282185077667 norm:0.0010079138446599245 max memory_allocated 29273.50048828125 
[2025-02-20 11:09:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.3536897897720337 norm:0.0009409906924702227 max memory_allocated 29273.50048828125 
[2025-02-20 11:10:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.3530901074409485 norm:0.0009149675024673343 max memory_allocated 29273.50048828125 
[2025-02-20 11:11:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.3527146875858307 norm:0.0008836492779664695 max memory_allocated 29273.50048828125 
[2025-02-20 11:12:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.3523673713207245 norm:0.0008629063377156854 max memory_allocated 29273.50048828125 
[2025-02-20 11:12:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.35203817486763 norm:0.0008328325347974896 max memory_allocated 29273.50048828125 
[2025-02-20 11:13:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.35176628828048706 norm:0.0008159415447153151 max memory_allocated 29273.50048828125 
[2025-02-20 11:14:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.3515418469905853 norm:0.0007902054931037128 max memory_allocated 29273.50048828125 
[2025-02-20 11:15:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.3513272702693939 norm:0.0007619527168571949 max memory_allocated 29273.50048828125 
[2025-02-20 11:16:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.35120725631713867 norm:0.0007617356022819877 max memory_allocated 29273.50048828125 
[2025-02-20 11:16:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.35102561116218567 norm:0.0007617651717737317 max memory_allocated 29273.50048828125 
[2025-02-20 11:17:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-20 11:18:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.4392516613006592 norm:0.013448619283735752 max memory_allocated 29273.68798828125 
[2025-02-20 11:18:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.41270142793655396 norm:0.007448161020874977 max memory_allocated 29273.68798828125 
[2025-02-20 11:19:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.38575541973114014 norm:0.0039369938895106316 max memory_allocated 29273.68798828125 
[2025-02-20 11:20:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.3734833300113678 norm:0.001987423514947295 max memory_allocated 29273.68798828125 
[2025-02-20 11:21:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.36830151081085205 norm:0.00137859839014709 max memory_allocated 29273.68798828125 
[2025-02-20 11:22:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.3651539087295532 norm:0.0011115041561424732 max memory_allocated 29273.68798828125 
[2025-02-20 11:23:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.36300885677337646 norm:0.0010380258318036795 max memory_allocated 29273.68798828125 
[2025-02-20 11:23:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.36119344830513 norm:0.000951003807131201 max memory_allocated 29273.68798828125 
[2025-02-20 11:24:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.35969242453575134 norm:0.0008830137667246163 max memory_allocated 29273.68798828125 
[2025-02-20 11:25:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.3586749732494354 norm:0.000846360344439745 max memory_allocated 29273.68798828125 
[2025-02-20 11:26:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.35798412561416626 norm:0.00082457868847996 max memory_allocated 29273.68798828125 
[2025-02-20 11:27:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.3574211895465851 norm:0.0007979797665029764 max memory_allocated 29273.68798828125 
[2025-02-20 11:27:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.3569488823413849 norm:0.0007862016791477799 max memory_allocated 29273.68798828125 
[2025-02-20 11:28:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.3565172851085663 norm:0.000773746520280838 max memory_allocated 29273.68798828125 
[2025-02-20 11:29:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.3560793399810791 norm:0.0007713832892477512 max memory_allocated 29273.68798828125 
[2025-02-20 11:30:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.35559409856796265 norm:0.0007550192531198263 max memory_allocated 29273.68798828125 
[2025-02-20 11:31:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.35537296533584595 norm:0.0007447136449627578 max memory_allocated 29273.68798828125 
[2025-02-20 11:32:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.35527271032333374 norm:0.0007367692305706441 max memory_allocated 29273.68798828125 
[2025-02-20 11:32:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.3551093637943268 norm:0.0007372709806077182 max memory_allocated 29273.68798828125 
[2025-02-20 11:33:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.3550146818161011 norm:0.0007302239537239075 max memory_allocated 29273.68798828125 
[2025-02-20 11:33:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-20 11:34:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.45668143033981323 norm:0.029823223128914833 max memory_allocated 29273.87548828125 
[2025-02-20 11:35:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.42881596088409424 norm:0.014154503121972084 max memory_allocated 29273.87548828125 
[2025-02-20 11:36:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.40088707208633423 norm:0.006599109154194593 max memory_allocated 29273.87548828125 
[2025-02-20 11:37:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.3876774311065674 norm:0.0035605153534561396 max memory_allocated 29273.87548828125 
[2025-02-20 11:38:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.38229748606681824 norm:0.0023121521808207035 max memory_allocated 29273.87548828125 
[2025-02-20 11:38:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.37901416420936584 norm:0.001957557164132595 max memory_allocated 29273.87548828125 
[2025-02-20 11:39:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.3764130473136902 norm:0.0015352268237620592 max memory_allocated 29273.87548828125 
[2025-02-20 11:40:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.37426111102104187 norm:0.001244672341272235 max memory_allocated 29273.87548828125 
[2025-02-20 11:41:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.3728012144565582 norm:0.0011541454587131739 max memory_allocated 29273.87548828125 
[2025-02-20 11:42:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.37174686789512634 norm:0.0011240524472668767 max memory_allocated 29273.87548828125 
[2025-02-20 11:43:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.3708154261112213 norm:0.0010629845783114433 max memory_allocated 29273.87548828125 
[2025-02-20 11:43:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.37011128664016724 norm:0.0010312512749806046 max memory_allocated 29273.87548828125 
[2025-02-20 11:44:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.369467556476593 norm:0.000997351249679923 max memory_allocated 29273.87548828125 
[2025-02-20 11:45:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.3691069185733795 norm:0.0010089883580803871 max memory_allocated 29273.87548828125 
[2025-02-20 11:46:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.3687330484390259 norm:0.001011374988593161 max memory_allocated 29273.87548828125 
[2025-02-20 11:47:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.3683238625526428 norm:0.0009906121995300055 max memory_allocated 29273.87548828125 
[2025-02-20 11:47:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.36804938316345215 norm:0.0009890533983707428 max memory_allocated 29273.87548828125 
[2025-02-20 11:48:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.36784449219703674 norm:0.0009498423896729946 max memory_allocated 29273.87548828125 
[2025-02-20 11:49:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.3676137924194336 norm:0.0009008042979985476 max memory_allocated 29273.87548828125 
[2025-02-20 11:50:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.3673909306526184 norm:0.0008887238800525665 max memory_allocated 29273.87548828125 
[2025-02-20 11:50:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-20 11:51:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.4614197313785553 norm:0.0313597247004509 max memory_allocated 29274.06298828125 
[2025-02-20 11:52:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.4328533709049225 norm:0.013997675850987434 max memory_allocated 29274.06298828125 
[2025-02-20 11:53:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.4045141339302063 norm:0.004931118339300156 max memory_allocated 29274.06298828125 
[2025-02-20 11:53:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.39420685172080994 norm:0.002680836245417595 max memory_allocated 29274.06298828125 
[2025-02-20 11:54:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.3902320861816406 norm:0.0021191996056586504 max memory_allocated 29274.06298828125 
[2025-02-20 11:55:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.3874240517616272 norm:0.0017781713977456093 max memory_allocated 29274.06298828125 
[2025-02-20 11:56:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.38533487915992737 norm:0.0015760635724291205 max memory_allocated 29274.06298828125 
[2025-02-20 11:57:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.38357114791870117 norm:0.0013663690770044923 max memory_allocated 29274.06298828125 
[2025-02-20 11:58:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.3819885849952698 norm:0.0012211690191179514 max memory_allocated 29274.06298828125 
[2025-02-20 11:58:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.38073763251304626 norm:0.0011753508588299155 max memory_allocated 29274.06298828125 
[2025-02-20 11:59:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.3797825574874878 norm:0.001133994315750897 max memory_allocated 29274.06298828125 
[2025-02-20 12:00:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.3790598511695862 norm:0.001104635768570006 max memory_allocated 29274.06298828125 
[2025-02-20 12:01:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.37850189208984375 norm:0.0010820203460752964 max memory_allocated 29274.06298828125 
[2025-02-20 12:02:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.37800660729408264 norm:0.0010473101865500212 max memory_allocated 29274.06298828125 
[2025-02-20 12:03:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.37755244970321655 norm:0.0010253103682771325 max memory_allocated 29274.06298828125 
[2025-02-20 12:03:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.3770710229873657 norm:0.0009733432088978589 max memory_allocated 29274.06298828125 
[2025-02-20 12:04:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.37686917185783386 norm:0.0009869177592918277 max memory_allocated 29274.06298828125 
[2025-02-20 12:05:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.37660542130470276 norm:0.0009660673676989973 max memory_allocated 29274.06298828125 
[2025-02-20 12:06:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.3763374984264374 norm:0.0009364444413222373 max memory_allocated 29274.06298828125 
[2025-02-20 12:07:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.37598347663879395 norm:0.0009137882734648883 max memory_allocated 29274.06298828125 
[2025-02-20 12:07:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-20 12:08:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.459818959236145 norm:0.028663236647844315 max memory_allocated 29274.25048828125 
[2025-02-20 12:09:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.4318481981754303 norm:0.011385595425963402 max memory_allocated 29274.25048828125 
[2025-02-20 12:09:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.41215237975120544 norm:0.0056929923593997955 max memory_allocated 29274.25048828125 
[2025-02-20 12:10:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.4023202955722809 norm:0.002800398040562868 max memory_allocated 29274.25048828125 
[2025-02-20 12:11:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.39834293723106384 norm:0.001878018258139491 max memory_allocated 29274.25048828125 
[2025-02-20 12:12:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.396036297082901 norm:0.0016194357303902507 max memory_allocated 29274.25048828125 
[2025-02-20 12:13:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.3943932056427002 norm:0.0014834270114079118 max memory_allocated 29274.25048828125 
[2025-02-20 12:13:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.3929809033870697 norm:0.0013700914569199085 max memory_allocated 29274.25048828125 
[2025-02-20 12:14:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.39170342683792114 norm:0.0012860666029155254 max memory_allocated 29274.25048828125 
[2025-02-20 12:15:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.3907143771648407 norm:0.0012021753937005997 max memory_allocated 29274.25048828125 
[2025-02-20 12:16:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.38991159200668335 norm:0.0011420734226703644 max memory_allocated 29274.25048828125 
[2025-02-20 12:17:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.38927528262138367 norm:0.0011013897601515055 max memory_allocated 29274.25048828125 
[2025-02-20 12:18:03 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.38857048749923706 norm:0.0010219361865893006 max memory_allocated 29274.25048828125 
[2025-02-20 12:18:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.38792893290519714 norm:0.0009733489132486284 max memory_allocated 29274.25048828125 
[2025-02-20 12:19:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.38745591044425964 norm:0.0009705310221761465 max memory_allocated 29274.25048828125 
[2025-02-20 12:20:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.38706478476524353 norm:0.0009449216886423528 max memory_allocated 29274.25048828125 
[2025-02-20 12:21:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.38679224252700806 norm:0.0009200863423757255 max memory_allocated 29274.25048828125 
[2025-02-20 12:22:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.38651734590530396 norm:0.0009071236709132791 max memory_allocated 29274.25048828125 
[2025-02-20 12:22:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.3862249553203583 norm:0.0009078011498786509 max memory_allocated 29274.25048828125 
[2025-02-20 12:23:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.3859770894050598 norm:0.0008944498258642852 max memory_allocated 29274.25048828125 
[2025-02-20 12:24:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-20 12:24:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.4699288606643677 norm:0.019739778712391853 max memory_allocated 29274.43798828125 
[2025-02-20 12:25:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.44973689317703247 norm:0.009265155531466007 max memory_allocated 29274.43798828125 
[2025-02-20 12:26:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.4301852285861969 norm:0.00460472097620368 max memory_allocated 29274.43798828125 
[2025-02-20 12:27:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.42084020376205444 norm:0.002316061407327652 max memory_allocated 29274.43798828125 
[2025-02-20 12:28:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.4171076714992523 norm:0.0017200325382873416 max memory_allocated 29274.43798828125 
[2025-02-20 12:29:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.4149038791656494 norm:0.0015278770588338375 max memory_allocated 29274.43798828125 
[2025-02-20 12:29:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.41321495175361633 norm:0.0014167275512591004 max memory_allocated 29274.43798828125 
[2025-02-20 12:30:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.41181257367134094 norm:0.0013254738878458738 max memory_allocated 29274.43798828125 
[2025-02-20 12:31:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.4105863571166992 norm:0.0012526767095550895 max memory_allocated 29274.43798828125 
[2025-02-20 12:32:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.40946948528289795 norm:0.0011539822444319725 max memory_allocated 29274.43798828125 
[2025-02-20 12:33:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.40855634212493896 norm:0.0010926933027803898 max memory_allocated 29274.43798828125 
[2025-02-20 12:33:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.4077947735786438 norm:0.0010384675115346909 max memory_allocated 29274.43798828125 
[2025-02-20 12:34:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.4072226881980896 norm:0.0009860852733254433 max memory_allocated 29274.43798828125 
[2025-02-20 12:35:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.40668225288391113 norm:0.00096339569427073 max memory_allocated 29274.43798828125 
[2025-02-20 12:36:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.40633293986320496 norm:0.0009353121276944876 max memory_allocated 29274.43798828125 
[2025-02-20 12:37:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.4061320126056671 norm:0.0009291888563893735 max memory_allocated 29274.43798828125 
[2025-02-20 12:38:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.4058605134487152 norm:0.0009082508622668684 max memory_allocated 29274.43798828125 
[2025-02-20 12:38:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.40551015734672546 norm:0.000880219042301178 max memory_allocated 29274.43798828125 
[2025-02-20 12:39:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.4053288996219635 norm:0.0008691624971106648 max memory_allocated 29274.43798828125 
[2025-02-20 12:40:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.4051724970340729 norm:0.0008679182501509786 max memory_allocated 29274.43798828125 
[2025-02-20 12:40:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-20 12:41:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.48827776312828064 norm:0.019010018557310104 max memory_allocated 29274.62548828125 
[2025-02-20 12:42:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.46816185116767883 norm:0.008540648967027664 max memory_allocated 29274.62548828125 
[2025-02-20 12:43:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.45152992010116577 norm:0.004469395149499178 max memory_allocated 29274.62548828125 
[2025-02-20 12:44:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.44258546829223633 norm:0.0023043896071612835 max memory_allocated 29274.62548828125 
[2025-02-20 12:44:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.43889349699020386 norm:0.0016904746880754828 max memory_allocated 29274.62548828125 
[2025-02-20 12:45:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.4367256760597229 norm:0.0014664093032479286 max memory_allocated 29274.62548828125 
[2025-02-20 12:46:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.43498483300209045 norm:0.0013004394713789225 max memory_allocated 29274.62548828125 
[2025-02-20 12:47:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.4336601495742798 norm:0.0012200671480968595 max memory_allocated 29274.62548828125 
[2025-02-20 12:48:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.43253135681152344 norm:0.0011287261731922626 max memory_allocated 29274.62548828125 
[2025-02-20 12:49:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.43163037300109863 norm:0.0010793616529554129 max memory_allocated 29274.62548828125 
[2025-02-20 12:49:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.4307951331138611 norm:0.0010286979377269745 max memory_allocated 29274.62548828125 
[2025-02-20 12:50:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.43007558584213257 norm:0.0009782511042430997 max memory_allocated 29274.62548828125 
[2025-02-20 12:51:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.4295825958251953 norm:0.00095842668088153 max memory_allocated 29274.62548828125 
[2025-02-20 12:52:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.4290768802165985 norm:0.0009234283934347332 max memory_allocated 29274.62548828125 
[2025-02-20 12:53:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.42863595485687256 norm:0.0009051805245690048 max memory_allocated 29274.62548828125 
[2025-02-20 12:53:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.4282984137535095 norm:0.0008922415436245501 max memory_allocated 29274.62548828125 
[2025-02-20 12:54:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.42797937989234924 norm:0.0008772871224209666 max memory_allocated 29274.62548828125 
[2025-02-20 12:55:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.42776039242744446 norm:0.0008520586998201907 max memory_allocated 29274.62548828125 
[2025-02-20 12:56:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.4275454580783844 norm:0.0008555138483643532 max memory_allocated 29274.62548828125 
[2025-02-20 12:57:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.4273652136325836 norm:0.0008418897050432861 max memory_allocated 29274.62548828125 
[2025-02-20 12:57:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-20 12:58:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.5264502167701721 norm:0.018019365146756172 max memory_allocated 29274.81298828125 
[2025-02-20 12:59:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.5069408416748047 norm:0.009156955406069756 max memory_allocated 29274.81298828125 
[2025-02-20 12:59:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.4893224835395813 norm:0.004798930138349533 max memory_allocated 29274.81298828125 
[2025-02-20 13:00:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.4798164963722229 norm:0.0028474389109760523 max memory_allocated 29274.81298828125 
[2025-02-20 13:01:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.4755209684371948 norm:0.0019345249747857451 max memory_allocated 29274.81298828125 
[2025-02-20 13:02:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.4732653796672821 norm:0.001639431924559176 max memory_allocated 29274.81298828125 
[2025-02-20 13:03:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.47141432762145996 norm:0.0014805349055677652 max memory_allocated 29274.81298828125 
[2025-02-20 13:04:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.47001150250434875 norm:0.0013881649356335402 max memory_allocated 29274.81298828125 
[2025-02-20 13:04:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.4689062237739563 norm:0.0013262140564620495 max memory_allocated 29274.81298828125 
[2025-02-20 13:05:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.4679674804210663 norm:0.001281680422835052 max memory_allocated 29274.81298828125 
[2025-02-20 13:06:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.46720272302627563 norm:0.0012475019320845604 max memory_allocated 29274.81298828125 
[2025-02-20 13:07:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.46654853224754333 norm:0.0011995293898507953 max memory_allocated 29274.81298828125 
[2025-02-20 13:08:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.4660000205039978 norm:0.0011585261672735214 max memory_allocated 29274.81298828125 
[2025-02-20 13:09:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.4654623866081238 norm:0.0011329882545396686 max memory_allocated 29274.81298828125 
[2025-02-20 13:09:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.46503618359565735 norm:0.0011061436962336302 max memory_allocated 29274.81298828125 
[2025-02-20 13:10:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.4646756947040558 norm:0.0011006060522049665 max memory_allocated 29274.81298828125 
[2025-02-20 13:11:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.46435269713401794 norm:0.0010617964435368776 max memory_allocated 29274.81298828125 
[2025-02-20 13:12:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.4639894664287567 norm:0.0010246417950838804 max memory_allocated 29274.81298828125 
[2025-02-20 13:13:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.4637606739997864 norm:0.0010139514924958348 max memory_allocated 29274.81298828125 
[2025-02-20 13:13:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.46352964639663696 norm:0.0010064564412459731 max memory_allocated 29274.81298828125 
[2025-02-20 13:14:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-20 13:15:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.5439147353172302 norm:0.009975100867450237 max memory_allocated 29275.00048828125 
[2025-02-20 13:15:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.5317723751068115 norm:0.006018683314323425 max memory_allocated 29275.00048828125 
[2025-02-20 13:16:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.5193541049957275 norm:0.0037009017542004585 max memory_allocated 29275.00048828125 
[2025-02-20 13:17:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.5133870244026184 norm:0.002318300772458315 max memory_allocated 29275.00048828125 
[2025-02-20 13:18:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.5102277398109436 norm:0.0016573256580159068 max memory_allocated 29275.00048828125 
[2025-02-20 13:19:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.5081881284713745 norm:0.001346425386145711 max memory_allocated 29275.00048828125 
[2025-02-20 13:19:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.5068305134773254 norm:0.0012453217059373856 max memory_allocated 29275.00048828125 
[2025-02-20 13:20:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.505573034286499 norm:0.0011610822984948754 max memory_allocated 29275.00048828125 
[2025-02-20 13:21:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.5044431686401367 norm:0.0010992282768711448 max memory_allocated 29275.00048828125 
[2025-02-20 13:22:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.50356125831604 norm:0.0010639301035553217 max memory_allocated 29275.00048828125 
[2025-02-20 13:23:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.5027537941932678 norm:0.0009928024373948574 max memory_allocated 29275.00048828125 
[2025-02-20 13:24:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.5020977258682251 norm:0.000961181300226599 max memory_allocated 29275.00048828125 
[2025-02-20 13:24:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.5015290379524231 norm:0.0009388590115122497 max memory_allocated 29275.00048828125 
[2025-02-20 13:25:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.5011152625083923 norm:0.0009253522730432451 max memory_allocated 29275.00048828125 
[2025-02-20 13:26:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.5006777048110962 norm:0.0009081370662897825 max memory_allocated 29275.00048828125 
[2025-02-20 13:27:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.5003198981285095 norm:0.0008984571904875338 max memory_allocated 29275.00048828125 
[2025-02-20 13:28:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.500104546546936 norm:0.0008847687859088182 max memory_allocated 29275.00048828125 
[2025-02-20 13:29:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.49992507696151733 norm:0.0008797603659331799 max memory_allocated 29275.00048828125 
[2025-02-20 13:29:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.49976053833961487 norm:0.0008719796896912158 max memory_allocated 29275.00048828125 
[2025-02-20 13:30:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.4994717240333557 norm:0.0008887741714715958 max memory_allocated 29275.00048828125 
[2025-02-20 13:30:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-20 13:31:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.5895447134971619 norm:0.023526586592197418 max memory_allocated 29275.18798828125 
[2025-02-20 13:32:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.5778411030769348 norm:0.01358594186604023 max memory_allocated 29275.18798828125 
[2025-02-20 13:33:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.5661548376083374 norm:0.008361972868442535 max memory_allocated 29275.18798828125 
[2025-02-20 13:34:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.5605541467666626 norm:0.005682616960257292 max memory_allocated 29275.18798828125 
[2025-02-20 13:35:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.5572574138641357 norm:0.004066219553351402 max memory_allocated 29275.18798828125 
[2025-02-20 13:35:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.5551518201828003 norm:0.0033320942893624306 max memory_allocated 29275.18798828125 
[2025-02-20 13:36:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.5532563924789429 norm:0.002827469725161791 max memory_allocated 29275.18798828125 
[2025-02-20 13:37:30 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.5515784025192261 norm:0.0024336869828402996 max memory_allocated 29275.18798828125 
[2025-02-20 13:38:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.5503279566764832 norm:0.002166114281862974 max memory_allocated 29275.18798828125 
[2025-02-20 13:39:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.549325704574585 norm:0.001984782051295042 max memory_allocated 29275.18798828125 
[2025-02-20 13:39:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.5484138131141663 norm:0.0017729934770613909 max memory_allocated 29275.18798828125 
[2025-02-20 13:40:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.5477388501167297 norm:0.0016087554395198822 max memory_allocated 29275.18798828125 
[2025-02-20 13:41:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.5471810698509216 norm:0.0014822876546531916 max memory_allocated 29275.18798828125 
[2025-02-20 13:42:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.5466942191123962 norm:0.001400423003360629 max memory_allocated 29275.18798828125 
[2025-02-20 13:43:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.5462293028831482 norm:0.0012982968473806977 max memory_allocated 29275.18798828125 
[2025-02-20 13:44:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.5459252595901489 norm:0.0012350671458989382 max memory_allocated 29275.18798828125 
[2025-02-20 13:44:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.5456175208091736 norm:0.001176047371700406 max memory_allocated 29275.18798828125 
[2025-02-20 13:45:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.545316219329834 norm:0.001161327469162643 max memory_allocated 29275.18798828125 
[2025-02-20 13:46:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.5449056625366211 norm:0.0009419333073310554 max memory_allocated 29275.18798828125 
[2025-02-20 13:47:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.5445470213890076 norm:0.0008141037542372942 max memory_allocated 29275.18798828125 
[2025-02-20 13:47:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-20 13:48:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.62875896692276 norm:0.00935433804988861 max memory_allocated 29275.37548828125 
[2025-02-20 13:49:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.6207082867622375 norm:0.006749674212187529 max memory_allocated 29275.37548828125 
[2025-02-20 13:50:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.6106811165809631 norm:0.004714303184300661 max memory_allocated 29275.37548828125 
[2025-02-20 13:50:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.6056240797042847 norm:0.003268924541771412 max memory_allocated 29275.37548828125 
[2025-02-20 13:51:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.6025445461273193 norm:0.0023087081499397755 max memory_allocated 29275.37548828125 
[2025-02-20 13:52:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.6006357669830322 norm:0.0019549999851733446 max memory_allocated 29275.37548828125 
[2025-02-20 13:53:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.5990056395530701 norm:0.0017727407393977046 max memory_allocated 29275.37548828125 
[2025-02-20 13:54:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.5975006818771362 norm:0.0015554677229374647 max memory_allocated 29275.37548828125 
[2025-02-20 13:55:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.5962954759597778 norm:0.0014537135139107704 max memory_allocated 29275.37548828125 
[2025-02-20 13:55:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.5952746272087097 norm:0.0013625422725453973 max memory_allocated 29275.37548828125 
[2025-02-20 13:56:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.5945531129837036 norm:0.0013246226590126753 max memory_allocated 29275.37548828125 
[2025-02-20 13:57:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.593877375125885 norm:0.0012840344570577145 max memory_allocated 29275.37548828125 
[2025-02-20 13:58:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.5933851003646851 norm:0.0012844334123656154 max memory_allocated 29275.37548828125 
[2025-02-20 13:59:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.5930420756340027 norm:0.0012662208173424006 max memory_allocated 29275.37548828125 
[2025-02-20 13:59:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.5926499962806702 norm:0.0012037028791382909 max memory_allocated 29275.37548828125 
[2025-02-20 14:00:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.5923808217048645 norm:0.0011747411917895079 max memory_allocated 29275.37548828125 
[2025-02-20 14:01:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.5920807719230652 norm:0.0011179097928106785 max memory_allocated 29275.37548828125 
[2025-02-20 14:02:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.5918314456939697 norm:0.0010849084937945008 max memory_allocated 29275.37548828125 
[2025-02-20 14:03:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.5916271209716797 norm:0.0010410164250060916 max memory_allocated 29275.37548828125 
[2025-02-20 14:04:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.5914888978004456 norm:0.0010096518089994788 max memory_allocated 29275.37548828125 
[2025-02-20 14:04:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-20 14:05:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.6797018051147461 norm:0.005953619722276926 max memory_allocated 29275.56298828125 
[2025-02-20 14:05:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.6706691980361938 norm:0.0036204734351485968 max memory_allocated 29275.56298828125 
[2025-02-20 14:06:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.6609260439872742 norm:0.0025493912398815155 max memory_allocated 29275.56298828125 
[2025-02-20 14:07:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.6571019291877747 norm:0.001820309553295374 max memory_allocated 29275.56298828125 
[2025-02-20 14:08:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.6549626588821411 norm:0.0014157455880194902 max memory_allocated 29275.56298828125 
[2025-02-20 14:09:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.6533433198928833 norm:0.001224336214363575 max memory_allocated 29275.56298828125 
[2025-02-20 14:10:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.6519209146499634 norm:0.0011133261723443866 max memory_allocated 29275.56298828125 
[2025-02-20 14:10:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.6506748199462891 norm:0.001035162596963346 max memory_allocated 29275.56298828125 
[2025-02-20 14:11:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.6496872901916504 norm:0.0009826491586863995 max memory_allocated 29275.56298828125 
[2025-02-20 14:12:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.6488829255104065 norm:0.0009373868815600872 max memory_allocated 29275.56298828125 
[2025-02-20 14:13:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.6482686400413513 norm:0.0008910589967854321 max memory_allocated 29275.56298828125 
[2025-02-20 14:14:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.6477351188659668 norm:0.0008576919790357351 max memory_allocated 29275.56298828125 
[2025-02-20 14:15:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.6473453044891357 norm:0.0008380839717574418 max memory_allocated 29275.56298828125 
[2025-02-20 14:15:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.6470127105712891 norm:0.0008166067418642342 max memory_allocated 29275.56298828125 
[2025-02-20 14:16:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.646669328212738 norm:0.000792890670709312 max memory_allocated 29275.56298828125 
[2025-02-20 14:17:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.6464219093322754 norm:0.0007710658828727901 max memory_allocated 29275.56298828125 
[2025-02-20 14:18:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.6462020874023438 norm:0.0007588833686895669 max memory_allocated 29275.56298828125 
[2025-02-20 14:19:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.6459735631942749 norm:0.0007404309581033885 max memory_allocated 29275.56298828125 
[2025-02-20 14:19:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.6458005309104919 norm:0.0007319530704990029 max memory_allocated 29275.56298828125 
[2025-02-20 14:20:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.6456528902053833 norm:0.000716832815669477 max memory_allocated 29275.56298828125 
[2025-02-20 14:21:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-20 14:21:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.7490630745887756 norm:0.012140126898884773 max memory_allocated 29275.75048828125 
[2025-02-20 14:22:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.7395392656326294 norm:0.007974231615662575 max memory_allocated 29275.75048828125 
[2025-02-20 14:23:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.7282506227493286 norm:0.005591006949543953 max memory_allocated 29275.75048828125 
[2025-02-20 14:24:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.7239155173301697 norm:0.004217313602566719 max memory_allocated 29275.75048828125 
[2025-02-20 14:25:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.7210942506790161 norm:0.0031957407481968403 max memory_allocated 29275.75048828125 
[2025-02-20 14:25:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.7189293503761292 norm:0.0025753944646567106 max memory_allocated 29275.75048828125 
[2025-02-20 14:26:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.7164633274078369 norm:0.0017923732521012425 max memory_allocated 29275.75048828125 
[2025-02-20 14:27:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.7147904634475708 norm:0.0013158917427062988 max memory_allocated 29275.75048828125 
[2025-02-20 14:28:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.7134969830513 norm:0.0012441986473277211 max memory_allocated 29275.75048828125 
[2025-02-20 14:29:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.7124489545822144 norm:0.00118619529530406 max memory_allocated 29275.75048828125 
[2025-02-20 14:30:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.7117272615432739 norm:0.0011567325564101338 max memory_allocated 29275.75048828125 
[2025-02-20 14:30:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.7111486792564392 norm:0.0011785372626036406 max memory_allocated 29275.75048828125 
[2025-02-20 14:31:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.7106966376304626 norm:0.0011447635479271412 max memory_allocated 29275.75048828125 
[2025-02-20 14:32:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.7103649973869324 norm:0.0011573124211281538 max memory_allocated 29275.75048828125 
[2025-02-20 14:33:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.7100470662117004 norm:0.0011632924433797598 max memory_allocated 29275.75048828125 
[2025-02-20 14:34:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.7098067998886108 norm:0.0011527996975928545 max memory_allocated 29275.75048828125 
[2025-02-20 14:35:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.7095092535018921 norm:0.001105551258660853 max memory_allocated 29275.75048828125 
[2025-02-20 14:35:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.7092445492744446 norm:0.0011006564600393176 max memory_allocated 29275.75048828125 
[2025-02-20 14:36:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.7091249227523804 norm:0.0010915420716628432 max memory_allocated 29275.75048828125 
[2025-02-20 14:37:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.7089259028434753 norm:0.0010890094563364983 max memory_allocated 29275.75048828125 
[2025-02-20 14:37:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-20 14:38:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.8024277091026306 norm:0.004568316508084536 max memory_allocated 29275.93798828125 
[2025-02-20 14:39:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.7940237522125244 norm:0.002254350110888481 max memory_allocated 29275.93798828125 
[2025-02-20 14:40:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.7840636968612671 norm:0.0013357376446947455 max memory_allocated 29275.93798828125 
[2025-02-20 14:41:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.7807294726371765 norm:0.001049540936946869 max memory_allocated 29275.93798828125 
[2025-02-20 14:41:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.7788095474243164 norm:0.000798539724200964 max memory_allocated 29275.93798828125 
[2025-02-20 14:42:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.7771691083908081 norm:0.0007419097819365561 max memory_allocated 29275.93798828125 
[2025-02-20 14:43:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.7756162285804749 norm:0.0007090148865245283 max memory_allocated 29275.93798828125 
[2025-02-20 14:44:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.7742895483970642 norm:0.0006910294760018587 max memory_allocated 29275.93798828125 
[2025-02-20 14:45:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.7731897234916687 norm:0.0006798364920541644 max memory_allocated 29275.93798828125 
[2025-02-20 14:45:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.7723104953765869 norm:0.0006689330330118537 max memory_allocated 29275.93798828125 
[2025-02-20 14:46:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.77158123254776 norm:0.000658270379062742 max memory_allocated 29275.93798828125 
[2025-02-20 14:47:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.7709370851516724 norm:0.0006482820608653128 max memory_allocated 29275.93798828125 
[2025-02-20 14:48:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.770454466342926 norm:0.0006489204824902117 max memory_allocated 29275.93798828125 
[2025-02-20 14:49:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.7700191736221313 norm:0.0006487236823886633 max memory_allocated 29275.93798828125 
[2025-02-20 14:50:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.7696800827980042 norm:0.0006494384724646807 max memory_allocated 29275.93798828125 
[2025-02-20 14:50:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.769400954246521 norm:0.0006435802206397057 max memory_allocated 29275.93798828125 
[2025-02-20 14:51:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.7691787481307983 norm:0.0006465309415943921 max memory_allocated 29275.93798828125 
[2025-02-20 14:52:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.7689637541770935 norm:0.0006457637064158916 max memory_allocated 29275.93798828125 
[2025-02-20 14:53:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.7687922716140747 norm:0.0006458942079916596 max memory_allocated 29275.93798828125 
[2025-02-20 14:54:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.768650233745575 norm:0.0006437002448365092 max memory_allocated 29275.93798828125 
[2025-02-20 14:54:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-20 14:55:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.8846806883811951 norm:0.01316773146390915 max memory_allocated 29276.12548828125 
[2025-02-20 14:56:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.8755656480789185 norm:0.008350089192390442 max memory_allocated 29276.12548828125 
[2025-02-20 14:56:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.8647578954696655 norm:0.00578596256673336 max memory_allocated 29276.12548828125 
[2025-02-20 14:57:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.8604299426078796 norm:0.004212039522826672 max memory_allocated 29276.12548828125 
[2025-02-20 14:58:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.8572131395339966 norm:0.0030003604479134083 max memory_allocated 29276.12548828125 
[2025-02-20 14:59:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.8542234897613525 norm:0.0024325556587427855 max memory_allocated 29276.12548828125 
[2025-02-20 15:00:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.8520919680595398 norm:0.002301156520843506 max memory_allocated 29276.12548828125 
[2025-02-20 15:01:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.8502855896949768 norm:0.002022902714088559 max memory_allocated 29276.12548828125 
[2025-02-20 15:01:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.8484556674957275 norm:0.0016839021118357778 max memory_allocated 29276.12548828125 
[2025-02-20 15:02:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.8472820520401001 norm:0.0015933281974866986 max memory_allocated 29276.12548828125 
[2025-02-20 15:03:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.8463008999824524 norm:0.0016189566813409328 max memory_allocated 29276.12548828125 
[2025-02-20 15:04:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.8458301424980164 norm:0.0014762782957404852 max memory_allocated 29276.12548828125 
[2025-02-20 15:05:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.8453096747398376 norm:0.0015187278622761369 max memory_allocated 29276.12548828125 
[2025-02-20 15:05:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.8450606465339661 norm:0.0013837672304362059 max memory_allocated 29276.12548828125 
[2025-02-20 15:06:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.8447257280349731 norm:0.001328510232269764 max memory_allocated 29276.12548828125 
[2025-02-20 15:07:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.8443863391876221 norm:0.0013374598929658532 max memory_allocated 29276.12548828125 
[2025-02-20 15:08:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.8440021872520447 norm:0.0013065049424767494 max memory_allocated 29276.12548828125 
[2025-02-20 15:09:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.8434544205665588 norm:0.0013601992977783084 max memory_allocated 29276.12548828125 
[2025-02-20 15:10:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.8435066342353821 norm:0.0011858558282256126 max memory_allocated 29276.12548828125 
[2025-02-20 15:10:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.8433887362480164 norm:0.001234893104992807 max memory_allocated 29276.12548828125 
[2025-02-20 15:11:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-20 15:12:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.9543342590332031 norm:0.007098018191754818 max memory_allocated 29276.31298828125 
[2025-02-20 15:12:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.9452109336853027 norm:0.00370030733756721 max memory_allocated 29276.31298828125 
[2025-02-20 15:13:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.934058666229248 norm:0.002047128975391388 max memory_allocated 29276.31298828125 
[2025-02-20 15:14:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.929802417755127 norm:0.0012954144040122628 max memory_allocated 29276.31298828125 
[2025-02-20 15:15:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.9274283647537231 norm:0.0009357277886010706 max memory_allocated 29276.31298828125 
[2025-02-20 15:16:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.9255518913269043 norm:0.0008461971883662045 max memory_allocated 29276.31298828125 
[2025-02-20 15:16:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.9238521456718445 norm:0.0007989700534380972 max memory_allocated 29276.31298828125 
[2025-02-20 15:17:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.9223336577415466 norm:0.0007712921360507607 max memory_allocated 29276.31298828125 
[2025-02-20 15:18:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.9210834503173828 norm:0.0007468237308785319 max memory_allocated 29276.31298828125 
[2025-02-20 15:19:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.9201158285140991 norm:0.0007305158069357276 max memory_allocated 29276.31298828125 
[2025-02-20 15:20:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.9194056987762451 norm:0.0007184550631791353 max memory_allocated 29276.31298828125 
[2025-02-20 15:21:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.9187715649604797 norm:0.0007046686951071024 max memory_allocated 29276.31298828125 
[2025-02-20 15:21:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.918293297290802 norm:0.0006964359199628234 max memory_allocated 29276.31298828125 
[2025-02-20 15:22:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.9179832339286804 norm:0.0006838520057499409 max memory_allocated 29276.31298828125 
[2025-02-20 15:23:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.9176754355430603 norm:0.0006725559942424297 max memory_allocated 29276.31298828125 
[2025-02-20 15:24:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.9173953533172607 norm:0.0006649078568443656 max memory_allocated 29276.31298828125 
[2025-02-20 15:25:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.9171643257141113 norm:0.0006569402758032084 max memory_allocated 29276.31298828125 
[2025-02-20 15:25:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.9169896841049194 norm:0.0006536109140142798 max memory_allocated 29276.31298828125 
[2025-02-20 15:26:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.9168258905410767 norm:0.0006529180682264268 max memory_allocated 29276.31298828125 
[2025-02-20 15:27:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.916673481464386 norm:0.0006508626975119114 max memory_allocated 29276.31298828125 
[2025-02-20 15:27:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-20 15:28:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:1.0438127517700195 norm:0.01079384796321392 max memory_allocated 29276.50048828125 
[2025-02-20 15:29:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:1.031753659248352 norm:0.0061162300407886505 max memory_allocated 29276.50048828125 
[2025-02-20 15:30:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:1.0177013874053955 norm:0.0038155093789100647 max memory_allocated 29276.50048828125 
[2025-02-20 15:31:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:1.01169753074646 norm:0.0023334892466664314 max memory_allocated 29276.50048828125 
[2025-02-20 15:32:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:1.0088145732879639 norm:0.0011572272051125765 max memory_allocated 29276.50048828125 
[2025-02-20 15:32:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:1.0066938400268555 norm:0.001022191485390067 max memory_allocated 29276.50048828125 
[2025-02-20 15:33:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:1.004511833190918 norm:0.000943169288802892 max memory_allocated 29276.50048828125 
[2025-02-20 15:34:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:1.0026915073394775 norm:0.000888110778760165 max memory_allocated 29276.50048828125 
[2025-02-20 15:35:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:1.001312017440796 norm:0.0008449350134469569 max memory_allocated 29276.50048828125 
[2025-02-20 15:36:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:1.0003045797348022 norm:0.0008134429808706045 max memory_allocated 29276.50048828125 
[2025-02-20 15:36:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.9995008111000061 norm:0.0007860651821829379 max memory_allocated 29276.50048828125 
[2025-02-20 15:37:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.9989019632339478 norm:0.0007678762776777148 max memory_allocated 29276.50048828125 
[2025-02-20 15:38:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.9984308481216431 norm:0.0007560590747743845 max memory_allocated 29276.50048828125 
[2025-02-20 15:39:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.9980629086494446 norm:0.0007483125082217157 max memory_allocated 29276.50048828125 
[2025-02-20 15:40:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.9977166056632996 norm:0.0007424552459269762 max memory_allocated 29276.50048828125 
[2025-02-20 15:41:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.9974229335784912 norm:0.0007345899357460439 max memory_allocated 29276.50048828125 
[2025-02-20 15:41:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.9971584677696228 norm:0.0007272213697433472 max memory_allocated 29276.50048828125 
[2025-02-20 15:42:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.9968736171722412 norm:0.0007216516532935202 max memory_allocated 29276.50048828125 
[2025-02-20 15:43:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.9966685771942139 norm:0.0007122506503947079 max memory_allocated 29276.50048828125 
[2025-02-20 15:44:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.9964228868484497 norm:0.0007074490422382951 max memory_allocated 29276.50048828125 
[2025-02-20 15:44:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-20 15:45:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:1.1290956735610962 norm:0.010629788041114807 max memory_allocated 29276.68798828125 
[2025-02-20 15:46:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:1.1183524131774902 norm:0.006340814754366875 max memory_allocated 29276.68798828125 
[2025-02-20 15:47:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:1.1038612127304077 norm:0.0030077022965997458 max memory_allocated 29276.68798828125 
[2025-02-20 15:47:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:1.0973970890045166 norm:0.00192092580255121 max memory_allocated 29276.68798828125 
[2025-02-20 15:48:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:1.0937577486038208 norm:0.0009697883506305516 max memory_allocated 29276.68798828125 
[2025-02-20 15:49:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:1.0910629034042358 norm:0.0008051503682509065 max memory_allocated 29276.68798828125 
[2025-02-20 15:50:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:1.0886123180389404 norm:0.0007492431323044002 max memory_allocated 29276.68798828125 
[2025-02-20 15:51:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:1.0865851640701294 norm:0.0007176494691520929 max memory_allocated 29276.68798828125 
[2025-02-20 15:51:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:1.085022211074829 norm:0.0006991127738729119 max memory_allocated 29276.68798828125 
[2025-02-20 15:52:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:1.0840157270431519 norm:0.0006819951231591403 max memory_allocated 29276.68798828125 
[2025-02-20 15:53:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:1.0831679105758667 norm:0.0006725466228090227 max memory_allocated 29276.68798828125 
[2025-02-20 15:54:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:1.0825304985046387 norm:0.0006610546843148768 max memory_allocated 29276.68798828125 
[2025-02-20 15:55:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:1.0820554494857788 norm:0.0006533724954351783 max memory_allocated 29276.68798828125 
[2025-02-20 15:56:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:1.081656813621521 norm:0.000654013711027801 max memory_allocated 29276.68798828125 
[2025-02-20 15:56:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:1.0813244581222534 norm:0.0006512908730655909 max memory_allocated 29276.68798828125 
[2025-02-20 15:57:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:1.0810012817382812 norm:0.0006485595367848873 max memory_allocated 29276.68798828125 
[2025-02-20 15:58:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:1.0808069705963135 norm:0.0006435351679101586 max memory_allocated 29276.68798828125 
[2025-02-20 15:59:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:1.0805996656417847 norm:0.0006468174397014081 max memory_allocated 29276.68798828125 
[2025-02-20 16:00:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:1.0803660154342651 norm:0.0006388459587469697 max memory_allocated 29276.68798828125 
[2025-02-20 16:01:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:1.0801916122436523 norm:0.0006414382951334119 max memory_allocated 29276.68798828125 
[2025-02-20 16:01:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-20 16:02:09 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:1.229690670967102 norm:0.01526813767850399 max memory_allocated 29276.87548828125 
[2025-02-20 16:02:58 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:1.216149091720581 norm:0.008986092172563076 max memory_allocated 29276.87548828125 
[2025-02-20 16:03:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:1.2001252174377441 norm:0.005711008328944445 max memory_allocated 29276.87548828125 
[2025-02-20 16:04:37 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:1.1929086446762085 norm:0.0039072185754776 max memory_allocated 29276.87548828125 
[2025-02-20 16:05:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:1.188604712486267 norm:0.00274589192122221 max memory_allocated 29276.87548828125 
[2025-02-20 16:06:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:1.1853324174880981 norm:0.0022088363766670227 max memory_allocated 29276.87548828125 
[2025-02-20 16:07:04 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:1.1819875240325928 norm:0.0010439560282975435 max memory_allocated 29276.87548828125 
[2025-02-20 16:07:54 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:1.179760456085205 norm:0.0008789314888417721 max memory_allocated 29276.87548828125 
[2025-02-20 16:08:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:1.1783055067062378 norm:0.0008602506131865084 max memory_allocated 29276.87548828125 
[2025-02-20 16:09:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:1.1772154569625854 norm:0.0008462578407488763 max memory_allocated 29276.87548828125 
[2025-02-20 16:10:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:1.1763372421264648 norm:0.0008338873158209026 max memory_allocated 29276.87548828125 
[2025-02-20 16:11:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:1.1757316589355469 norm:0.0008090874180197716 max memory_allocated 29276.87548828125 
[2025-02-20 16:12:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:1.1752535104751587 norm:0.0007986770942807198 max memory_allocated 29276.87548828125 
[2025-02-20 16:12:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:1.1748807430267334 norm:0.0007899457705207169 max memory_allocated 29276.87548828125 
[2025-02-20 16:13:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:1.174538493156433 norm:0.0007814267883077264 max memory_allocated 29276.87548828125 
[2025-02-20 16:14:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:1.1742427349090576 norm:0.0007748397765681148 max memory_allocated 29276.87548828125 
[2025-02-20 16:15:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:1.1740093231201172 norm:0.000777834327891469 max memory_allocated 29276.87548828125 
[2025-02-20 16:16:07 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:1.1737489700317383 norm:0.0007598997326567769 max memory_allocated 29276.87548828125 
[2025-02-20 16:16:56 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:1.173604130744934 norm:0.0007509758579544723 max memory_allocated 29276.87548828125 
[2025-02-20 16:17:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:1.1734665632247925 norm:0.0007407744415104389 max memory_allocated 29276.87548828125 
[2025-02-20 16:17:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-20 16:18:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:1.3320832252502441 norm:0.011539029888808727 max memory_allocated 29277.06298828125 
[2025-02-20 16:19:41 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:1.318249225616455 norm:0.00783944595605135 max memory_allocated 29277.06298828125 
[2025-02-20 16:20:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:1.3012206554412842 norm:0.005480759311467409 max memory_allocated 29277.06298828125 
[2025-02-20 16:21:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:1.2934448719024658 norm:0.004007590934634209 max memory_allocated 29277.06298828125 
[2025-02-20 16:22:09 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:1.2892976999282837 norm:0.003061385825276375 max memory_allocated 29277.06298828125 
[2025-02-20 16:22:58 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:1.285567283630371 norm:0.002427584258839488 max memory_allocated 29277.06298828125 
[2025-02-20 16:23:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:1.2822530269622803 norm:0.0021537276916205883 max memory_allocated 29277.06298828125 
[2025-02-20 16:24:37 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:1.2797473669052124 norm:0.0018736275378614664 max memory_allocated 29277.06298828125 
[2025-02-20 16:25:26 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:1.2780662775039673 norm:0.0016733375377953053 max memory_allocated 29277.06298828125 
[2025-02-20 16:26:15 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:1.27675461769104 norm:0.0015050866641104221 max memory_allocated 29277.06298828125 
[2025-02-20 16:27:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:1.2757742404937744 norm:0.001311885192990303 max memory_allocated 29277.06298828125 
[2025-02-20 16:27:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:1.2750215530395508 norm:0.0012079037260264158 max memory_allocated 29277.06298828125 
[2025-02-20 16:28:43 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:1.274148941040039 norm:0.0011370833963155746 max memory_allocated 29277.06298828125 
[2025-02-20 16:29:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:1.273564338684082 norm:0.0010481528006494045 max memory_allocated 29277.06298828125 
[2025-02-20 16:30:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:1.2733732461929321 norm:0.0009638337069191039 max memory_allocated 29277.06298828125 
[2025-02-20 16:31:10 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:1.273073673248291 norm:0.0009274121839553118 max memory_allocated 29277.06298828125 
[2025-02-20 16:32:00 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:1.272632360458374 norm:0.0008810212602838874 max memory_allocated 29277.06298828125 
[2025-02-20 16:32:49 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:1.2722944021224976 norm:0.0008450964232906699 max memory_allocated 29277.06298828125 
[2025-02-20 16:33:38 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:1.2719851732254028 norm:0.0008073411299847066 max memory_allocated 29277.06298828125 
[2025-02-20 16:34:27 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:1.2717106342315674 norm:0.0007794731645844877 max memory_allocated 29277.06298828125 
[2025-02-20 16:34:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-20 16:35:34 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:1.4541823863983154 norm:0.014010274782776833 max memory_allocated 29277.25048828125 
[2025-02-20 16:36:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:1.4381592273712158 norm:0.008427613414824009 max memory_allocated 29277.25048828125 
[2025-02-20 16:37:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:1.4198147058486938 norm:0.005436374805867672 max memory_allocated 29277.25048828125 
[2025-02-20 16:38:02 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:1.4113805294036865 norm:0.00391467334702611 max memory_allocated 29277.25048828125 
[2025-02-20 16:38:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:1.4068130254745483 norm:0.0031504740472882986 max memory_allocated 29277.25048828125 
[2025-02-20 16:39:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:1.4025596380233765 norm:0.002425088780000806 max memory_allocated 29277.25048828125 
[2025-02-20 16:40:30 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:1.3989771604537964 norm:0.0019525157986208797 max memory_allocated 29277.25048828125 
[2025-02-20 16:41:19 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:1.396233081817627 norm:0.0017067007720470428 max memory_allocated 29277.25048828125 
[2025-02-20 16:42:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:1.394363522529602 norm:0.0015606824308633804 max memory_allocated 29277.25048828125 
[2025-02-20 16:42:57 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:1.392977237701416 norm:0.001421414315700531 max memory_allocated 29277.25048828125 
[2025-02-20 16:43:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:1.3919405937194824 norm:0.001307207508943975 max memory_allocated 29277.25048828125 
[2025-02-20 16:44:36 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:1.3910868167877197 norm:0.0011986896861344576 max memory_allocated 29277.25048828125 
[2025-02-20 16:45:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:1.390411615371704 norm:0.0011163998860865831 max memory_allocated 29277.25048828125 
[2025-02-20 16:46:14 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:1.3899036645889282 norm:0.001052588690072298 max memory_allocated 29277.25048828125 
[2025-02-20 16:47:03 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:1.389378309249878 norm:0.0009939473820850253 max memory_allocated 29277.25048828125 
[2025-02-20 16:47:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:1.3889752626419067 norm:0.0009455038816668093 max memory_allocated 29277.25048828125 
[2025-02-20 16:48:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:1.3885643482208252 norm:0.0007801403407938778 max memory_allocated 29277.25048828125 
[2025-02-20 16:49:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:1.3882578611373901 norm:0.0007485906244255602 max memory_allocated 29277.25048828125 
[2025-02-20 16:50:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:1.3879948854446411 norm:0.00074601179221645 max memory_allocated 29277.25048828125 
[2025-02-20 16:51:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:1.387694239616394 norm:0.0007380573078989983 max memory_allocated 29277.25048828125 
[2025-02-20 16:51:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-20 16:52:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:1.581246256828308 norm:0.005854501388967037 max memory_allocated 29277.43798828125 
[2025-02-20 16:53:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:1.5646567344665527 norm:0.0038419163320213556 max memory_allocated 29277.43798828125 
[2025-02-20 16:53:55 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:1.5444945096969604 norm:0.0025665401481091976 max memory_allocated 29277.43798828125 
[2025-02-20 16:54:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:1.5351946353912354 norm:0.001791453454643488 max memory_allocated 29277.43798828125 
[2025-02-20 16:55:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:1.5300730466842651 norm:0.001404702547006309 max memory_allocated 29277.43798828125 
[2025-02-20 16:56:22 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:1.525965929031372 norm:0.001244395156390965 max memory_allocated 29277.43798828125 
[2025-02-20 16:57:11 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:1.5218325853347778 norm:0.0010572695173323154 max memory_allocated 29277.43798828125 
[2025-02-20 16:58:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:1.5189831256866455 norm:0.0009563685744069517 max memory_allocated 29277.43798828125 
[2025-02-20 16:58:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:1.5171468257904053 norm:0.0009272664901800454 max memory_allocated 29277.43798828125 
[2025-02-20 16:59:39 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:1.515717625617981 norm:0.0008892846526578069 max memory_allocated 29277.43798828125 
[2025-02-20 17:00:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:1.5147156715393066 norm:0.0008598004933446646 max memory_allocated 29277.43798828125 
[2025-02-20 17:01:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:1.513765811920166 norm:0.0008399717044085264 max memory_allocated 29277.43798828125 
[2025-02-20 17:02:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:1.5130343437194824 norm:0.0008218851289711893 max memory_allocated 29277.43798828125 
[2025-02-20 17:02:56 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:1.5125164985656738 norm:0.0008114372612908483 max memory_allocated 29277.43798828125 
[2025-02-20 17:03:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:1.5120761394500732 norm:0.0008026764262467623 max memory_allocated 29277.43798828125 
[2025-02-20 17:04:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:1.5116878747940063 norm:0.0007946988334879279 max memory_allocated 29277.43798828125 
[2025-02-20 17:05:24 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:1.5112910270690918 norm:0.0007814569980837405 max memory_allocated 29277.43798828125 
[2025-02-20 17:06:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:1.5109869241714478 norm:0.0007692920044064522 max memory_allocated 29277.43798828125 
[2025-02-20 17:07:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:1.510642647743225 norm:0.0007605584687553346 max memory_allocated 29277.43798828125 
[2025-02-20 17:07:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:1.5103288888931274 norm:0.0007522394298575819 max memory_allocated 29277.43798828125 
[2025-02-20 17:08:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-20 17:08:10 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:08:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:1.7597836256027222 norm:0.03439315780997276 max memory_allocated 29277.77001953125 
[2025-02-20 17:09:49 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:1.7362903356552124 norm:0.027647200971841812 max memory_allocated 29277.77001953125 
[2025-02-20 17:10:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:1.7099168300628662 norm:0.021100938320159912 max memory_allocated 29277.77001953125 
[2025-02-20 17:11:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:1.6944383382797241 norm:0.016709940508008003 max memory_allocated 29277.77001953125 
[2025-02-20 17:12:17 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:1.684317708015442 norm:0.013706766068935394 max memory_allocated 29277.77001953125 
[2025-02-20 17:13:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:1.6748933792114258 norm:0.010987387038767338 max memory_allocated 29277.77001953125 
[2025-02-20 17:13:55 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:1.6692026853561401 norm:0.010080748237669468 max memory_allocated 29277.77001953125 
[2025-02-20 17:14:45 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:1.6651312112808228 norm:0.0092414366081357 max memory_allocated 29277.77001953125 
[2025-02-20 17:15:34 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:1.6622315645217896 norm:0.008792945183813572 max memory_allocated 29277.77001953125 
[2025-02-20 17:16:23 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:1.6602599620819092 norm:0.008424635976552963 max memory_allocated 29277.77001953125 
[2025-02-20 17:17:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:1.6588643789291382 norm:0.008234699256718159 max memory_allocated 29277.77001953125 
[2025-02-20 17:18:02 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:1.6577789783477783 norm:0.0078321797773242 max memory_allocated 29277.77001953125 
[2025-02-20 17:18:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:1.6569650173187256 norm:0.007659106981009245 max memory_allocated 29277.77001953125 
[2025-02-20 17:19:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:1.6561199426651 norm:0.0072968704625964165 max memory_allocated 29277.77001953125 
[2025-02-20 17:20:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:1.6554745435714722 norm:0.007216829340904951 max memory_allocated 29277.77001953125 
[2025-02-20 17:21:20 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:1.654869556427002 norm:0.007092171814292669 max memory_allocated 29277.77001953125 
[2025-02-20 17:22:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:1.6542887687683105 norm:0.006978229153901339 max memory_allocated 29277.77001953125 
[2025-02-20 17:22:58 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:1.6539227962493896 norm:0.006875617429614067 max memory_allocated 29277.77001953125 
[2025-02-20 17:23:48 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:1.6535108089447021 norm:0.0068479073233902454 max memory_allocated 29277.77001953125 
[2025-02-20 17:24:37 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:1.6531929969787598 norm:0.006795830558985472 max memory_allocated 29277.77001953125 
[2025-02-20 17:24:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-20 17:24:56 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:25:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:1.9321706295013428 norm:0.03386308625340462 max memory_allocated 29277.95751953125 
[2025-02-20 17:26:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:1.9036343097686768 norm:0.028585733845829964 max memory_allocated 29277.95751953125 
[2025-02-20 17:27:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:1.8730831146240234 norm:0.02271552011370659 max memory_allocated 29277.95751953125 
[2025-02-20 17:28:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:1.8533674478530884 norm:0.017753349617123604 max memory_allocated 29277.95751953125 
[2025-02-20 17:29:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:1.8442186117172241 norm:0.015026912093162537 max memory_allocated 29277.95751953125 
[2025-02-20 17:29:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:1.837173342704773 norm:0.012529980391263962 max memory_allocated 29277.95751953125 
[2025-02-20 17:30:41 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:1.8316102027893066 norm:0.010981997475028038 max memory_allocated 29277.95751953125 
[2025-02-20 17:31:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:1.827423334121704 norm:0.010001270100474358 max memory_allocated 29277.95751953125 
[2025-02-20 17:32:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:1.8244816064834595 norm:0.009437203407287598 max memory_allocated 29277.95751953125 
[2025-02-20 17:33:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:1.8225172758102417 norm:0.009247159585356712 max memory_allocated 29277.95751953125 
[2025-02-20 17:33:58 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:1.8207662105560303 norm:0.009313665330410004 max memory_allocated 29277.95751953125 
[2025-02-20 17:34:47 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:1.8196017742156982 norm:0.009401589632034302 max memory_allocated 29277.95751953125 
[2025-02-20 17:35:37 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:1.818071722984314 norm:0.008777663111686707 max memory_allocated 29277.95751953125 
[2025-02-20 17:36:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:1.8168967962265015 norm:0.008477466180920601 max memory_allocated 29277.95751953125 
[2025-02-20 17:37:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:1.8157145977020264 norm:0.00814451277256012 max memory_allocated 29277.95751953125 
[2025-02-20 17:38:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:1.8149791955947876 norm:0.007951061241328716 max memory_allocated 29277.95751953125 
[2025-02-20 17:38:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:1.8141608238220215 norm:0.008060667663812637 max memory_allocated 29277.95751953125 
[2025-02-20 17:39:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:1.8139137029647827 norm:0.008491124026477337 max memory_allocated 29277.95751953125 
[2025-02-20 17:40:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:1.8134758472442627 norm:0.008316605351865292 max memory_allocated 29277.95751953125 
[2025-02-20 17:41:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:1.812943458557129 norm:0.008103126659989357 max memory_allocated 29277.95751953125 
[2025-02-20 17:41:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-20 17:41:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:42:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:2.2354352474212646 norm:0.053944896906614304 max memory_allocated 29278.14501953125 
[2025-02-20 17:43:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:2.1954925060272217 norm:0.04668154567480087 max memory_allocated 29278.14501953125 
[2025-02-20 17:44:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:2.156121015548706 norm:0.0368175134062767 max memory_allocated 29278.14501953125 
[2025-02-20 17:44:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:2.129141330718994 norm:0.029730673879384995 max memory_allocated 29278.14501953125 
[2025-02-20 17:45:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:2.1152169704437256 norm:0.02512306720018387 max memory_allocated 29278.14501953125 
[2025-02-20 17:46:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:2.1055755615234375 norm:0.0213505569845438 max memory_allocated 29278.14501953125 
[2025-02-20 17:47:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:2.0984859466552734 norm:0.018737943843007088 max memory_allocated 29278.14501953125 
[2025-02-20 17:48:17 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:2.0935020446777344 norm:0.017131857573986053 max memory_allocated 29278.14501953125 
[2025-02-20 17:49:06 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:2.089921474456787 norm:0.016302919015288353 max memory_allocated 29278.14501953125 
[2025-02-20 17:49:55 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:2.086371421813965 norm:0.016007089987397194 max memory_allocated 29278.14501953125 
[2025-02-20 17:50:45 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:2.0831644535064697 norm:0.016352979466319084 max memory_allocated 29278.14501953125 
[2025-02-20 17:51:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:2.0798401832580566 norm:0.015924561768770218 max memory_allocated 29278.14501953125 
[2025-02-20 17:52:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:2.0766854286193848 norm:0.015811795368790627 max memory_allocated 29278.14501953125 
[2025-02-20 17:53:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:2.0745513439178467 norm:0.015417065471410751 max memory_allocated 29278.14501953125 
[2025-02-20 17:54:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:2.0725154876708984 norm:0.013813644647598267 max memory_allocated 29278.14501953125 
[2025-02-20 17:54:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:2.0714609622955322 norm:0.014350755140185356 max memory_allocated 29278.14501953125 
[2025-02-20 17:55:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:2.0705764293670654 norm:0.015204422175884247 max memory_allocated 29278.14501953125 
[2025-02-20 17:56:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:2.0690088272094727 norm:0.014360214583575726 max memory_allocated 29278.14501953125 
[2025-02-20 17:57:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:2.0683155059814453 norm:0.014541832730174065 max memory_allocated 29278.14501953125 
[2025-02-20 17:58:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:2.0671868324279785 norm:0.014016300439834595 max memory_allocated 29278.14501953125 
[2025-02-20 17:58:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-20 17:58:27 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:59:16 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:3.2402400970458984 norm:0.17225483059883118 max memory_allocated 29278.33251953125 
[2025-02-20 18:00:05 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:3.0170061588287354 norm:0.12834782898426056 max memory_allocated 29278.33251953125 
[2025-02-20 18:00:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:2.914201021194458 norm:0.10972131788730621 max memory_allocated 29278.33251953125 
[2025-02-20 18:01:44 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:2.840108633041382 norm:0.09305432438850403 max memory_allocated 29278.33251953125 
[2025-02-20 18:02:33 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:2.8018574714660645 norm:0.08338545262813568 max memory_allocated 29278.33251953125 
[2025-02-20 18:03:23 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:2.7741456031799316 norm:0.07068586349487305 max memory_allocated 29278.33251953125 
[2025-02-20 18:04:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:2.755192279815674 norm:0.06361353397369385 max memory_allocated 29278.33251953125 
[2025-02-20 18:05:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:2.7410848140716553 norm:0.058151643723249435 max memory_allocated 29278.33251953125 
[2025-02-20 18:05:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:2.7275097370147705 norm:0.05203777551651001 max memory_allocated 29278.33251953125 
[2025-02-20 18:06:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:2.7163357734680176 norm:0.048473719507455826 max memory_allocated 29278.33251953125 
[2025-02-20 18:07:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:2.7063450813293457 norm:0.045574553310871124 max memory_allocated 29278.33251953125 
[2025-02-20 18:08:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:2.698172092437744 norm:0.04302811250090599 max memory_allocated 29278.33251953125 
[2025-02-20 18:09:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:2.690282106399536 norm:0.04127092286944389 max memory_allocated 29278.33251953125 
[2025-02-20 18:09:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:2.6834614276885986 norm:0.03950927406549454 max memory_allocated 29278.33251953125 
[2025-02-20 18:10:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:2.6775765419006348 norm:0.0377446673810482 max memory_allocated 29278.33251953125 
[2025-02-20 18:11:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:2.6722779273986816 norm:0.03692088648676872 max memory_allocated 29278.33251953125 
[2025-02-20 18:12:26 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:2.668581485748291 norm:0.03585272654891014 max memory_allocated 29278.33251953125 
[2025-02-20 18:13:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:2.665658473968506 norm:0.03506316617131233 max memory_allocated 29278.33251953125 
[2025-02-20 18:14:05 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:2.661956310272217 norm:0.03445853292942047 max memory_allocated 29278.33251953125 
[2025-02-20 18:14:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:2.658554792404175 norm:0.03456815332174301 max memory_allocated 29278.33251953125 
[2025-02-20 18:15:09 root] (main_calibration.py 365): INFO 40108.49095535278
[2025-02-20 18:16:11 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-20 18:18:05 root] (main_calibration.py 158): INFO wikitext2 : 7.453538417816162
[2025-02-20 18:18:05 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-20 18:21:02 root] (main_calibration.py 158): INFO c4 : 10.7589111328125
[2025-02-20 20:16:14 root] (main_calibration.py 169): INFO {'wikitext2': 7.453538417816162, 'c4': 10.7589111328125, 'results': {'hellaswag': {'acc': 0.49362676757618007, 'acc_stderr': 0.004989376044184157, 'acc_norm': 0.642899820752838, 'acc_norm_stderr': 0.0047816546108571355}, 'winogrande': {'acc': 0.56353591160221, 'acc_stderr': 0.013938569465677019}, 'piqa': {'acc': 0.7023939064200218, 'acc_stderr': 0.01066735379238821, 'acc_norm': 0.7100108813928183, 'acc_norm_stderr': 0.010586899128169326}, 'boolq': {'acc': 0.6461773700305811, 'acc_stderr': 0.00836298302090447}, 'arc_challenge': {'acc': 0.3148464163822526, 'acc_stderr': 0.01357265770308495, 'acc_norm': 0.34982935153583616, 'acc_norm_stderr': 0.013936809212158284}, 'arc_easy': {'acc': 0.5997474747474747, 'acc_stderr': 0.010053550119896124, 'acc_norm': 0.4882154882154882, 'acc_norm_stderr': 0.010256933475911008}}, 'versions': {'hellaswag': 0, 'winogrande': 0, 'piqa': 0, 'boolq': 1, 'arc_challenge': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
