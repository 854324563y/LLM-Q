[2025-02-19 23:27:38 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-13b-hf-w4a4', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-19 23:27:39 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-19 23:27:39 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-19 23:27:39 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-19 23:27:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-19 23:27:44 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 23:28:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.05997741222381592 norm:0.04414570331573486 max memory_allocated 29271.02001953125 
[2025-02-19 23:29:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.03728152811527252 norm:0.02424122393131256 max memory_allocated 29271.02001953125 
[2025-02-19 23:30:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.028679821640253067 norm:0.01706257462501526 max memory_allocated 29271.02001953125 
[2025-02-19 23:30:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.024935131892561913 norm:0.013966336846351624 max memory_allocated 29271.02001953125 
[2025-02-19 23:31:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.023099267855286598 norm:0.013989821076393127 max memory_allocated 29271.02001953125 
[2025-02-19 23:32:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.021857818588614464 norm:0.010431298054754734 max memory_allocated 29271.02001953125 
[2025-02-19 23:33:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.021190999075770378 norm:0.009799000807106495 max memory_allocated 29271.02001953125 
[2025-02-19 23:34:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.020909681916236877 norm:0.008307724259793758 max memory_allocated 29271.02001953125 
[2025-02-19 23:35:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.02071796916425228 norm:0.007261882536113262 max memory_allocated 29271.02001953125 
[2025-02-19 23:35:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.020662466064095497 norm:0.007047866005450487 max memory_allocated 29271.02001953125 
[2025-02-19 23:36:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.020467258989810944 norm:0.006638989318162203 max memory_allocated 29271.02001953125 
[2025-02-19 23:37:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.020495343953371048 norm:0.005672188475728035 max memory_allocated 29271.02001953125 
[2025-02-19 23:38:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.02019079402089119 norm:0.005420382600277662 max memory_allocated 29271.02001953125 
[2025-02-19 23:39:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.02006026916205883 norm:0.004890690091997385 max memory_allocated 29271.02001953125 
[2025-02-19 23:39:58 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.019895365461707115 norm:0.004743028897792101 max memory_allocated 29271.02001953125 
[2025-02-19 23:40:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.019911352545022964 norm:0.004296206869184971 max memory_allocated 29271.02001953125 
[2025-02-19 23:41:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.019909819588065147 norm:0.00407231692224741 max memory_allocated 29271.02001953125 
[2025-02-19 23:42:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.020067254081368446 norm:0.005121965892612934 max memory_allocated 29271.02001953125 
[2025-02-19 23:43:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.02017461135983467 norm:0.0043553742580115795 max memory_allocated 29271.02001953125 
[2025-02-19 23:44:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.019903264939785004 norm:0.004976020660251379 max memory_allocated 29271.02001953125 
[2025-02-19 23:44:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-19 23:44:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 23:45:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.13040828704833984 norm:0.034037426114082336 max memory_allocated 29271.02001953125 
[2025-02-19 23:46:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.10050178319215775 norm:0.024137554690241814 max memory_allocated 29271.02001953125 
[2025-02-19 23:46:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0870552808046341 norm:0.017569389194250107 max memory_allocated 29271.02001953125 
[2025-02-19 23:47:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.08118918538093567 norm:0.013217760249972343 max memory_allocated 29271.02001953125 
[2025-02-19 23:48:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.07793499529361725 norm:0.01031831931322813 max memory_allocated 29271.02001953125 
[2025-02-19 23:49:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.07614807039499283 norm:0.008592428639531136 max memory_allocated 29271.02001953125 
[2025-02-19 23:50:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.07481440901756287 norm:0.007170284166932106 max memory_allocated 29271.02001953125 
[2025-02-19 23:50:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.07402084022760391 norm:0.006260031368583441 max memory_allocated 29271.02001953125 
[2025-02-19 23:51:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.07340502738952637 norm:0.00565233314409852 max memory_allocated 29271.02001953125 
[2025-02-19 23:52:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.07294689118862152 norm:0.005269925110042095 max memory_allocated 29271.02001953125 
[2025-02-19 23:53:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.07270368933677673 norm:0.005249200854450464 max memory_allocated 29271.02001953125 
[2025-02-19 23:54:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.07245737314224243 norm:0.005041530355811119 max memory_allocated 29271.02001953125 
[2025-02-19 23:55:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.07226566970348358 norm:0.005018089897930622 max memory_allocated 29271.02001953125 
[2025-02-19 23:55:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.07210065424442291 norm:0.004974912386387587 max memory_allocated 29271.02001953125 
[2025-02-19 23:56:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.07192745059728622 norm:0.0048799715004861355 max memory_allocated 29271.02001953125 
[2025-02-19 23:57:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.07181545346975327 norm:0.004953944124281406 max memory_allocated 29271.02001953125 
[2025-02-19 23:58:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.07169050723314285 norm:0.004764334298670292 max memory_allocated 29271.02001953125 
[2025-02-19 23:59:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.07167065888643265 norm:0.004760658368468285 max memory_allocated 29271.02001953125 
[2025-02-19 23:59:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0715467631816864 norm:0.004546122159808874 max memory_allocated 29271.02001953125 
[2025-02-20 00:00:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.07152843475341797 norm:0.0045701563358306885 max memory_allocated 29271.02001953125 
[2025-02-20 00:01:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-20 00:01:04 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 00:01:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.15779559314250946 norm:0.025175876915454865 max memory_allocated 29271.39501953125 
[2025-02-20 00:02:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.13726557791233063 norm:0.02028002217411995 max memory_allocated 29271.39501953125 
[2025-02-20 00:03:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.12367652356624603 norm:0.014284167438745499 max memory_allocated 29271.39501953125 
[2025-02-20 00:04:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.11748969554901123 norm:0.010571436025202274 max memory_allocated 29271.39501953125 
[2025-02-20 00:05:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.11447016894817352 norm:0.008120110258460045 max memory_allocated 29271.39501953125 
[2025-02-20 00:06:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.11244960874319077 norm:0.006470441818237305 max memory_allocated 29271.39501953125 
[2025-02-20 00:06:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.11124736070632935 norm:0.005324278026819229 max memory_allocated 29271.39501953125 
[2025-02-20 00:07:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.1105383113026619 norm:0.005151733756065369 max memory_allocated 29271.39501953125 
[2025-02-20 00:08:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.10982280969619751 norm:0.005124098155647516 max memory_allocated 29271.39501953125 
[2025-02-20 00:09:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.10938158631324768 norm:0.0049569797702133656 max memory_allocated 29271.39501953125 
[2025-02-20 00:10:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.10923726111650467 norm:0.004926371853798628 max memory_allocated 29271.39501953125 
[2025-02-20 00:10:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.10893410444259644 norm:0.00487580755725503 max memory_allocated 29271.39501953125 
[2025-02-20 00:11:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.10870098322629929 norm:0.004817149601876736 max memory_allocated 29271.39501953125 
[2025-02-20 00:12:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.10858583450317383 norm:0.004657241050153971 max memory_allocated 29271.39501953125 
[2025-02-20 00:13:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.10842402279376984 norm:0.0045663719065487385 max memory_allocated 29271.39501953125 
[2025-02-20 00:14:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.10842007398605347 norm:0.004410530906170607 max memory_allocated 29271.39501953125 
[2025-02-20 00:15:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.10831785947084427 norm:0.004393451847136021 max memory_allocated 29271.39501953125 
[2025-02-20 00:15:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.10825690627098083 norm:0.004239327739924192 max memory_allocated 29271.39501953125 
[2025-02-20 00:16:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.10821191966533661 norm:0.004213050939142704 max memory_allocated 29271.39501953125 
[2025-02-20 00:17:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.10822029411792755 norm:0.004114226903766394 max memory_allocated 29271.39501953125 
[2025-02-20 00:17:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-20 00:18:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.4022320508956909 norm:0.1959581822156906 max memory_allocated 29271.39501953125 
[2025-02-20 00:19:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.2974311113357544 norm:0.058616697788238525 max memory_allocated 29271.39501953125 
[2025-02-20 00:20:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.24306455254554749 norm:0.024597346782684326 max memory_allocated 29271.39501953125 
[2025-02-20 00:21:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.22666828334331512 norm:0.026299865916371346 max memory_allocated 29271.39501953125 
[2025-02-20 00:21:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.21906334161758423 norm:0.02316989004611969 max memory_allocated 29271.39501953125 
[2025-02-20 00:22:43 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.21089856326580048 norm:0.021427536383271217 max memory_allocated 29271.39501953125 
[2025-02-20 00:23:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.19962620735168457 norm:0.01830882392823696 max memory_allocated 29271.39501953125 
[2025-02-20 00:24:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.19122634828090668 norm:0.01439134031534195 max memory_allocated 29271.39501953125 
[2025-02-20 00:25:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.18948397040367126 norm:0.013105224817991257 max memory_allocated 29271.39501953125 
[2025-02-20 00:25:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.18620944023132324 norm:0.012850809842348099 max memory_allocated 29271.39501953125 
[2025-02-20 00:26:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.18418414890766144 norm:0.012747776694595814 max memory_allocated 29271.39501953125 
[2025-02-20 00:27:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.18230243027210236 norm:0.012490963563323021 max memory_allocated 29271.39501953125 
[2025-02-20 00:28:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.1816355437040329 norm:0.011211824603378773 max memory_allocated 29271.39501953125 
[2025-02-20 00:29:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.18072274327278137 norm:0.01029879879206419 max memory_allocated 29271.39501953125 
[2025-02-20 00:30:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.1791353076696396 norm:0.009770091623067856 max memory_allocated 29271.39501953125 
[2025-02-20 00:30:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.17895731329917908 norm:0.010213423520326614 max memory_allocated 29271.39501953125 
[2025-02-20 00:31:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.17849485576152802 norm:0.009684132412075996 max memory_allocated 29271.39501953125 
[2025-02-20 00:32:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.1770593225955963 norm:0.009602058678865433 max memory_allocated 29271.39501953125 
[2025-02-20 00:33:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.17681342363357544 norm:0.008732193149626255 max memory_allocated 29271.39501953125 
[2025-02-20 00:34:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.17678013443946838 norm:0.008551610633730888 max memory_allocated 29271.39501953125 
[2025-02-20 00:34:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-20 00:35:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.22923128306865692 norm:0.010562426410615444 max memory_allocated 29271.39501953125 
[2025-02-20 00:36:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.20839685201644897 norm:0.004513663239777088 max memory_allocated 29271.39501953125 
[2025-02-20 00:36:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.19545197486877441 norm:0.0027300759684294462 max memory_allocated 29271.39501953125 
[2025-02-20 00:37:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.1898757815361023 norm:0.002049472648650408 max memory_allocated 29271.39501953125 
[2025-02-20 00:38:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.18668706715106964 norm:0.0015998251037672162 max memory_allocated 29271.39501953125 
[2025-02-20 00:39:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.18419089913368225 norm:0.0013163283001631498 max memory_allocated 29271.39501953125 
[2025-02-20 00:40:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.18274208903312683 norm:0.0011696560541167855 max memory_allocated 29271.39501953125 
[2025-02-20 00:41:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.18165263533592224 norm:0.0011098135728389025 max memory_allocated 29271.39501953125 
[2025-02-20 00:41:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.18098843097686768 norm:0.0010886460077017546 max memory_allocated 29271.39501953125 
[2025-02-20 00:42:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.18052661418914795 norm:0.0010812294203788042 max memory_allocated 29271.39501953125 
[2025-02-20 00:43:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.18010365962982178 norm:0.001078840927220881 max memory_allocated 29271.39501953125 
[2025-02-20 00:44:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.17966631054878235 norm:0.0010525687830522656 max memory_allocated 29271.39501953125 
[2025-02-20 00:45:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.17923079431056976 norm:0.001027610502205789 max memory_allocated 29271.39501953125 
[2025-02-20 00:45:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.1790103167295456 norm:0.0010275027016177773 max memory_allocated 29271.39501953125 
[2025-02-20 00:46:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.17870761454105377 norm:0.000996535294689238 max memory_allocated 29271.39501953125 
[2025-02-20 00:47:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.17850418388843536 norm:0.0009985038777813315 max memory_allocated 29271.39501953125 
[2025-02-20 00:48:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.17837803065776825 norm:0.0009864016901701689 max memory_allocated 29271.39501953125 
[2025-02-20 00:49:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.17809225618839264 norm:0.0009692020830698311 max memory_allocated 29271.39501953125 
[2025-02-20 00:50:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.1779003143310547 norm:0.000958564574830234 max memory_allocated 29271.39501953125 
[2025-02-20 00:50:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.1777634620666504 norm:0.0009697572095319629 max memory_allocated 29271.39501953125 
[2025-02-20 00:51:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-20 00:51:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.26096203923225403 norm:0.012168829329311848 max memory_allocated 29271.81298828125 
[2025-02-20 00:52:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.2372194528579712 norm:0.004907811060547829 max memory_allocated 29271.81298828125 
[2025-02-20 00:53:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.22111208736896515 norm:0.0028924690559506416 max memory_allocated 29271.81298828125 
[2025-02-20 00:54:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.21467751264572144 norm:0.0018743841210380197 max memory_allocated 29271.81298828125 
[2025-02-20 00:55:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.2113736867904663 norm:0.0013568990398198366 max memory_allocated 29271.81298828125 
[2025-02-20 00:56:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.20927231013774872 norm:0.0011993789812549949 max memory_allocated 29271.81298828125 
[2025-02-20 00:56:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.2077476680278778 norm:0.001132220379076898 max memory_allocated 29271.81298828125 
[2025-02-20 00:57:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.20663614571094513 norm:0.001064579002559185 max memory_allocated 29271.81298828125 
[2025-02-20 00:58:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.2057226300239563 norm:0.0010191792389377952 max memory_allocated 29271.81298828125 
[2025-02-20 00:59:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.20515747368335724 norm:0.0009846377652138472 max memory_allocated 29271.81298828125 
[2025-02-20 01:00:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.20465880632400513 norm:0.000954260234721005 max memory_allocated 29271.81298828125 
[2025-02-20 01:00:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.20430788397789001 norm:0.0009390562772750854 max memory_allocated 29271.81298828125 
[2025-02-20 01:01:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.2040109783411026 norm:0.0009371921187266707 max memory_allocated 29271.81298828125 
[2025-02-20 01:02:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.20366689562797546 norm:0.0009313273476436734 max memory_allocated 29271.81298828125 
[2025-02-20 01:03:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.20349043607711792 norm:0.0008993666269816458 max memory_allocated 29271.81298828125 
[2025-02-20 01:04:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.20336507260799408 norm:0.0008867622818797827 max memory_allocated 29271.81298828125 
[2025-02-20 01:05:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.20326367020606995 norm:0.000882533669937402 max memory_allocated 29271.81298828125 
[2025-02-20 01:05:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.20305657386779785 norm:0.0008725352818146348 max memory_allocated 29271.81298828125 
[2025-02-20 01:06:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.20302483439445496 norm:0.0008746591047383845 max memory_allocated 29271.81298828125 
[2025-02-20 01:07:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.20269319415092468 norm:0.0008636788697913289 max memory_allocated 29271.81298828125 
[2025-02-20 01:07:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-20 01:08:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.29826992750167847 norm:0.024454878643155098 max memory_allocated 29271.81298828125 
[2025-02-20 01:09:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.2680613100528717 norm:0.011528891511261463 max memory_allocated 29271.81298828125 
[2025-02-20 01:10:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.2451809048652649 norm:0.002901115221902728 max memory_allocated 29271.81298828125 
[2025-02-20 01:11:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.23791739344596863 norm:0.0016647509764879942 max memory_allocated 29271.81298828125 
[2025-02-20 01:11:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.23437348008155823 norm:0.001431155949831009 max memory_allocated 29271.81298828125 
[2025-02-20 01:12:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.2317100167274475 norm:0.0012872660299763083 max memory_allocated 29271.81298828125 
[2025-02-20 01:13:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.22962819039821625 norm:0.0011869287118315697 max memory_allocated 29271.81298828125 
[2025-02-20 01:14:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.22811008989810944 norm:0.0011394929606467485 max memory_allocated 29271.81298828125 
[2025-02-20 01:15:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.2271735519170761 norm:0.0010959679493680596 max memory_allocated 29271.81298828125 
[2025-02-20 01:16:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.22638186812400818 norm:0.0010685657616704702 max memory_allocated 29271.81298828125 
[2025-02-20 01:16:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.22557581961154938 norm:0.0010471534915268421 max memory_allocated 29271.81298828125 
[2025-02-20 01:17:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.22511927783489227 norm:0.001028269063681364 max memory_allocated 29271.81298828125 
[2025-02-20 01:18:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.22474725544452667 norm:0.0010092465672641993 max memory_allocated 29271.81298828125 
[2025-02-20 01:19:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.22451533377170563 norm:0.0010108038550242782 max memory_allocated 29271.81298828125 
[2025-02-20 01:20:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.2243177592754364 norm:0.001005150144919753 max memory_allocated 29271.81298828125 
[2025-02-20 01:20:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.22413823008537292 norm:0.0010060676140710711 max memory_allocated 29271.81298828125 
[2025-02-20 01:21:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.22396919131278992 norm:0.00099687441252172 max memory_allocated 29271.81298828125 
[2025-02-20 01:22:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.2238651067018509 norm:0.0009801582200452685 max memory_allocated 29271.81298828125 
[2025-02-20 01:23:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.22380833327770233 norm:0.000987148261629045 max memory_allocated 29271.81298828125 
[2025-02-20 01:24:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.22379598021507263 norm:0.0009697400964796543 max memory_allocated 29271.81298828125 
[2025-02-20 01:24:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-20 01:25:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.34685179591178894 norm:0.014172994531691074 max memory_allocated 29271.81298828125 
[2025-02-20 01:26:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.30495685338974 norm:0.005163781810551882 max memory_allocated 29271.81298828125 
[2025-02-20 01:27:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.2759290337562561 norm:0.002265469403937459 max memory_allocated 29271.81298828125 
[2025-02-20 01:27:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.26727229356765747 norm:0.0016141687519848347 max memory_allocated 29271.81298828125 
[2025-02-20 01:28:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.26316526532173157 norm:0.0013913416769355536 max memory_allocated 29271.81298828125 
[2025-02-20 01:29:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.2599014639854431 norm:0.001289219711907208 max memory_allocated 29271.81298828125 
[2025-02-20 01:30:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.2575935125350952 norm:0.0012243558885529637 max memory_allocated 29271.81298828125 
[2025-02-20 01:31:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.25609835982322693 norm:0.0011808010749518871 max memory_allocated 29271.81298828125 
[2025-02-20 01:31:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.2550548315048218 norm:0.001161128398962319 max memory_allocated 29271.81298828125 
[2025-02-20 01:32:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.2542625367641449 norm:0.0011288446839898825 max memory_allocated 29271.81298828125 
[2025-02-20 01:33:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.253584086894989 norm:0.001107253017835319 max memory_allocated 29271.81298828125 
[2025-02-20 01:34:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.253221720457077 norm:0.0010920313652604818 max memory_allocated 29271.81298828125 
[2025-02-20 01:35:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.2529012858867645 norm:0.0010888053802773356 max memory_allocated 29271.81298828125 
[2025-02-20 01:36:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.25257810950279236 norm:0.0010751904919743538 max memory_allocated 29271.81298828125 
[2025-02-20 01:36:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.2523820400238037 norm:0.0010584336705505848 max memory_allocated 29271.81298828125 
[2025-02-20 01:37:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.25218966603279114 norm:0.0010440218029543757 max memory_allocated 29271.81298828125 
[2025-02-20 01:38:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.2520678639411926 norm:0.0010418633464723825 max memory_allocated 29271.81298828125 
[2025-02-20 01:39:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.25198036432266235 norm:0.0010296758264303207 max memory_allocated 29271.81298828125 
[2025-02-20 01:40:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.251705527305603 norm:0.0010223727440461516 max memory_allocated 29271.81298828125 
[2025-02-20 01:40:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.2516055703163147 norm:0.0010356866987422109 max memory_allocated 29271.81298828125 
[2025-02-20 01:41:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-20 01:42:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.37056785821914673 norm:0.03230120614171028 max memory_allocated 29272.37548828125 
[2025-02-20 01:42:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.3386369049549103 norm:0.020893078297376633 max memory_allocated 29272.37548828125 
[2025-02-20 01:43:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.30660760402679443 norm:0.013149114325642586 max memory_allocated 29272.37548828125 
[2025-02-20 01:44:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.29373693466186523 norm:0.008304511196911335 max memory_allocated 29272.37548828125 
[2025-02-20 01:45:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.2853355407714844 norm:0.002920531900599599 max memory_allocated 29272.37548828125 
[2025-02-20 01:46:09 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.2806088626384735 norm:0.0016959055792540312 max memory_allocated 29272.37548828125 
[2025-02-20 01:46:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.27763181924819946 norm:0.001542283920571208 max memory_allocated 29272.37548828125 
[2025-02-20 01:47:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.27545252442359924 norm:0.0013504659291356802 max memory_allocated 29272.37548828125 
[2025-02-20 01:48:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.27408355474472046 norm:0.0012778425589203835 max memory_allocated 29272.37548828125 
[2025-02-20 01:49:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.27284756302833557 norm:0.001177183585241437 max memory_allocated 29272.37548828125 
[2025-02-20 01:50:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.2720472812652588 norm:0.0011221208842471242 max memory_allocated 29272.37548828125 
[2025-02-20 01:51:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.27132391929626465 norm:0.0011050791945308447 max memory_allocated 29272.37548828125 
[2025-02-20 01:51:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.27093416452407837 norm:0.0010926795657724142 max memory_allocated 29272.37548828125 
[2025-02-20 01:52:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.27064576745033264 norm:0.0010618651285767555 max memory_allocated 29272.37548828125 
[2025-02-20 01:53:32 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.27028971910476685 norm:0.0010593284387141466 max memory_allocated 29272.37548828125 
[2025-02-20 01:54:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.27002811431884766 norm:0.0010440623154863715 max memory_allocated 29272.37548828125 
[2025-02-20 01:55:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.26988548040390015 norm:0.0010439218021929264 max memory_allocated 29272.37548828125 
[2025-02-20 01:56:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.2697872221469879 norm:0.001039220835082233 max memory_allocated 29272.37548828125 
[2025-02-20 01:56:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.26961445808410645 norm:0.0010110092116519809 max memory_allocated 29272.37548828125 
[2025-02-20 01:57:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.2694718837738037 norm:0.0010086465626955032 max memory_allocated 29272.37548828125 
[2025-02-20 01:57:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-20 01:58:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.4353759288787842 norm:0.03076833486557007 max memory_allocated 29272.37548828125 
[2025-02-20 01:59:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.3813903033733368 norm:0.013226182200014591 max memory_allocated 29272.37548828125 
[2025-02-20 02:00:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.3316724896430969 norm:0.005342819727957249 max memory_allocated 29272.37548828125 
[2025-02-20 02:01:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.31402066349983215 norm:0.0029103667475283146 max memory_allocated 29272.37548828125 
[2025-02-20 02:02:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.30732202529907227 norm:0.0023568584583699703 max memory_allocated 29272.37548828125 
[2025-02-20 02:02:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.3033345639705658 norm:0.0019724173471331596 max memory_allocated 29272.37548828125 
[2025-02-20 02:03:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.3003309369087219 norm:0.0017638454446569085 max memory_allocated 29272.37548828125 
[2025-02-20 02:04:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.29807233810424805 norm:0.0016169657465070486 max memory_allocated 29272.37548828125 
[2025-02-20 02:05:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.2963806390762329 norm:0.0015036491677165031 max memory_allocated 29272.37548828125 
[2025-02-20 02:06:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.2951843738555908 norm:0.001408107578754425 max memory_allocated 29272.37548828125 
[2025-02-20 02:06:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.2943178117275238 norm:0.001338740810751915 max memory_allocated 29272.37548828125 
[2025-02-20 02:07:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.29364925622940063 norm:0.0012649421114474535 max memory_allocated 29272.37548828125 
[2025-02-20 02:08:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.29307276010513306 norm:0.0011882786639034748 max memory_allocated 29272.37548828125 
[2025-02-20 02:09:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.29277077317237854 norm:0.0011613802053034306 max memory_allocated 29272.37548828125 
[2025-02-20 02:10:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.29238495230674744 norm:0.0010967719135805964 max memory_allocated 29272.37548828125 
[2025-02-20 02:11:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.29196876287460327 norm:0.0010700214188545942 max memory_allocated 29272.37548828125 
[2025-02-20 02:11:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.2916657626628876 norm:0.001045734970830381 max memory_allocated 29272.37548828125 
[2025-02-20 02:12:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.2912459671497345 norm:0.0010156367206946015 max memory_allocated 29272.37548828125 
[2025-02-20 02:13:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.29112857580184937 norm:0.0010388689115643501 max memory_allocated 29272.37548828125 
[2025-02-20 02:14:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.29088348150253296 norm:0.0010061050998046994 max memory_allocated 29272.37548828125 
[2025-02-20 02:14:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-20 02:15:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.41981974244117737 norm:0.021528838202357292 max memory_allocated 29272.37548828125 
[2025-02-20 02:16:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.37731584906578064 norm:0.008824939839541912 max memory_allocated 29272.37548828125 
[2025-02-20 02:17:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.3404808044433594 norm:0.0035925365518778563 max memory_allocated 29272.37548828125 
[2025-02-20 02:17:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.3272431194782257 norm:0.0022240332327783108 max memory_allocated 29272.37548828125 
[2025-02-20 02:18:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.3216201961040497 norm:0.0017633916577324271 max memory_allocated 29272.37548828125 
[2025-02-20 02:19:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.3178921043872833 norm:0.001471337745897472 max memory_allocated 29272.37548828125 
[2025-02-20 02:20:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.31548425555229187 norm:0.0013443873031064868 max memory_allocated 29272.37548828125 
[2025-02-20 02:21:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.31366580724716187 norm:0.001226179301738739 max memory_allocated 29272.37548828125 
[2025-02-20 02:22:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.3122444152832031 norm:0.0011192891979590058 max memory_allocated 29272.37548828125 
[2025-02-20 02:22:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.311271607875824 norm:0.0010751541703939438 max memory_allocated 29272.37548828125 
[2025-02-20 02:23:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.31062906980514526 norm:0.0010499998461455107 max memory_allocated 29272.37548828125 
[2025-02-20 02:24:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.31006568670272827 norm:0.001024890225380659 max memory_allocated 29272.37548828125 
[2025-02-20 02:25:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.30958324670791626 norm:0.0009909929940477014 max memory_allocated 29272.37548828125 
[2025-02-20 02:26:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.3092121183872223 norm:0.000976971467025578 max memory_allocated 29272.37548828125 
[2025-02-20 02:26:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.3089234530925751 norm:0.0009670524159446359 max memory_allocated 29272.37548828125 
[2025-02-20 02:27:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.3085247874259949 norm:0.000931125192437321 max memory_allocated 29272.37548828125 
[2025-02-20 02:28:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.30827662348747253 norm:0.0009155043517239392 max memory_allocated 29272.37548828125 
[2025-02-20 02:29:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.30791181325912476 norm:0.0009110954124480486 max memory_allocated 29272.37548828125 
[2025-02-20 02:30:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.30767348408699036 norm:0.0008724439539946616 max memory_allocated 29272.37548828125 
[2025-02-20 02:31:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.30760663747787476 norm:0.0008464805432595313 max memory_allocated 29272.37548828125 
[2025-02-20 02:31:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-20 02:32:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.4282101094722748 norm:0.027800776064395905 max memory_allocated 29272.93798828125 
[2025-02-20 02:32:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.39057114720344543 norm:0.010736397467553616 max memory_allocated 29272.93798828125 
[2025-02-20 02:33:48 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.35478535294532776 norm:0.003922475501894951 max memory_allocated 29272.93798828125 
[2025-02-20 02:34:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.34212353825569153 norm:0.0018669047858566046 max memory_allocated 29272.93798828125 
[2025-02-20 02:35:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.33669450879096985 norm:0.0012870697537437081 max memory_allocated 29272.93798828125 
[2025-02-20 02:36:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.3332022428512573 norm:0.0011656434508040547 max memory_allocated 29272.93798828125 
[2025-02-20 02:37:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.33066219091415405 norm:0.0010770705994218588 max memory_allocated 29272.93798828125 
[2025-02-20 02:37:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.3287905156612396 norm:0.000997291412204504 max memory_allocated 29272.93798828125 
[2025-02-20 02:38:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.32734301686286926 norm:0.0009333128109574318 max memory_allocated 29272.93798828125 
[2025-02-20 02:39:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.32627901434898376 norm:0.0008837071945890784 max memory_allocated 29272.93798828125 
[2025-02-20 02:40:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.32561105489730835 norm:0.000885087763890624 max memory_allocated 29272.93798828125 
[2025-02-20 02:41:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.3251037001609802 norm:0.0008809491991996765 max memory_allocated 29272.93798828125 
[2025-02-20 02:42:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.3247063159942627 norm:0.0008495987858623266 max memory_allocated 29272.93798828125 
[2025-02-20 02:42:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.32434701919555664 norm:0.000823852838948369 max memory_allocated 29272.93798828125 
[2025-02-20 02:43:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.32412201166152954 norm:0.0008105014567263424 max memory_allocated 29272.93798828125 
[2025-02-20 02:44:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.3238750100135803 norm:0.0007939495262689888 max memory_allocated 29272.93798828125 
[2025-02-20 02:45:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.32367241382598877 norm:0.0007851548143662512 max memory_allocated 29272.93798828125 
[2025-02-20 02:46:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.3235379457473755 norm:0.000785027164965868 max memory_allocated 29272.93798828125 
[2025-02-20 02:46:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.3233655095100403 norm:0.0007781608146615326 max memory_allocated 29272.93798828125 
[2025-02-20 02:47:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.323252409696579 norm:0.0007817731238901615 max memory_allocated 29272.93798828125 
[2025-02-20 02:47:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-20 02:48:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.45185577869415283 norm:0.01798972114920616 max memory_allocated 29272.93798828125 
[2025-02-20 02:49:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.40095722675323486 norm:0.006719138473272324 max memory_allocated 29272.93798828125 
[2025-02-20 02:50:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.36268216371536255 norm:0.0027187205851078033 max memory_allocated 29272.93798828125 
[2025-02-20 02:51:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.34951159358024597 norm:0.001652666600421071 max memory_allocated 29272.93798828125 
[2025-02-20 02:52:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.3442154824733734 norm:0.0012898736167699099 max memory_allocated 29272.93798828125 
[2025-02-20 02:52:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.34082016348838806 norm:0.0011665860656648874 max memory_allocated 29272.93798828125 
[2025-02-20 02:53:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.3382742404937744 norm:0.0010801797034218907 max memory_allocated 29272.93798828125 
[2025-02-20 02:54:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.33628156781196594 norm:0.0010219727410003543 max memory_allocated 29272.93798828125 
[2025-02-20 02:55:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.3350367248058319 norm:0.0009796608937904239 max memory_allocated 29272.93798828125 
[2025-02-20 02:56:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.33414509892463684 norm:0.0009440212743356824 max memory_allocated 29272.93798828125 
[2025-02-20 02:57:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.3334951400756836 norm:0.0009144195355474949 max memory_allocated 29272.93798828125 
[2025-02-20 02:57:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.33290860056877136 norm:0.0008813842432573438 max memory_allocated 29272.93798828125 
[2025-02-20 02:58:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.33251750469207764 norm:0.000859365682117641 max memory_allocated 29272.93798828125 
[2025-02-20 02:59:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.33217668533325195 norm:0.0008294685976579785 max memory_allocated 29272.93798828125 
[2025-02-20 03:00:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.33178162574768066 norm:0.0008096586680039763 max memory_allocated 29272.93798828125 
[2025-02-20 03:01:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.3315148651599884 norm:0.0007828888483345509 max memory_allocated 29272.93798828125 
[2025-02-20 03:01:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.33134156465530396 norm:0.0007767644710838795 max memory_allocated 29272.93798828125 
[2025-02-20 03:02:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.3311261534690857 norm:0.0007731185178272426 max memory_allocated 29272.93798828125 
[2025-02-20 03:03:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.33079761266708374 norm:0.0007739015272818506 max memory_allocated 29272.93798828125 
[2025-02-20 03:04:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.3305897116661072 norm:0.0007568586152046919 max memory_allocated 29272.93798828125 
[2025-02-20 03:04:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-20 03:05:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.44290080666542053 norm:0.02220037952065468 max memory_allocated 29272.93798828125 
[2025-02-20 03:06:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.40307241678237915 norm:0.00957542285323143 max memory_allocated 29272.93798828125 
[2025-02-20 03:07:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.37118634581565857 norm:0.004328616429120302 max memory_allocated 29272.93798828125 
[2025-02-20 03:08:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.35827431082725525 norm:0.0017954878276214004 max memory_allocated 29272.93798828125 
[2025-02-20 03:08:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.3530888557434082 norm:0.0013552999589592218 max memory_allocated 29272.93798828125 
[2025-02-20 03:09:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.34960323572158813 norm:0.001165208755992353 max memory_allocated 29272.93798828125 
[2025-02-20 03:10:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.34700340032577515 norm:0.0010597561486065388 max memory_allocated 29272.93798828125 
[2025-02-20 03:11:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.34517917037010193 norm:0.0009837014367803931 max memory_allocated 29272.93798828125 
[2025-02-20 03:12:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.34387317299842834 norm:0.0009474018006585538 max memory_allocated 29272.93798828125 
[2025-02-20 03:12:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.3427773714065552 norm:0.0009060004376806319 max memory_allocated 29272.93798828125 
[2025-02-20 03:13:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.34206146001815796 norm:0.0008708955137990415 max memory_allocated 29272.93798828125 
[2025-02-20 03:14:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.34156668186187744 norm:0.0008619541767984629 max memory_allocated 29272.93798828125 
[2025-02-20 03:15:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.3411686420440674 norm:0.0008481003460474312 max memory_allocated 29272.93798828125 
[2025-02-20 03:16:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.3407776355743408 norm:0.0008292435086332262 max memory_allocated 29272.93798828125 
[2025-02-20 03:17:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.340448796749115 norm:0.0008117255638353527 max memory_allocated 29272.93798828125 
[2025-02-20 03:17:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.3402724266052246 norm:0.0007937102345749736 max memory_allocated 29272.93798828125 
[2025-02-20 03:18:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.34003809094429016 norm:0.000779349822551012 max memory_allocated 29272.93798828125 
[2025-02-20 03:19:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.3398486375808716 norm:0.0007716005202382803 max memory_allocated 29272.93798828125 
[2025-02-20 03:20:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.3396786153316498 norm:0.0007710725767537951 max memory_allocated 29272.93798828125 
[2025-02-20 03:21:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.3395332098007202 norm:0.0007641235133633018 max memory_allocated 29272.93798828125 
[2025-02-20 03:21:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-20 03:22:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.44589346647262573 norm:0.018475402146577835 max memory_allocated 29273.50048828125 
[2025-02-20 03:23:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.409733384847641 norm:0.009731946513056755 max memory_allocated 29273.50048828125 
[2025-02-20 03:23:55 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.3818763196468353 norm:0.0054950835183262825 max memory_allocated 29273.50048828125 
[2025-02-20 03:24:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.3703625202178955 norm:0.003450692631304264 max memory_allocated 29273.50048828125 
[2025-02-20 03:25:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.36503779888153076 norm:0.0024649256374686956 max memory_allocated 29273.50048828125 
[2025-02-20 03:26:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.3614112138748169 norm:0.0018908389611169696 max memory_allocated 29273.50048828125 
[2025-02-20 03:27:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.35885027050971985 norm:0.0016229893080890179 max memory_allocated 29273.50048828125 
[2025-02-20 03:28:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.3569552004337311 norm:0.001249339897185564 max memory_allocated 29273.50048828125 
[2025-02-20 03:28:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.35555070638656616 norm:0.0011077823583036661 max memory_allocated 29273.50048828125 
[2025-02-20 03:29:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.3545282185077667 norm:0.0010079138446599245 max memory_allocated 29273.50048828125 
[2025-02-20 03:30:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.3536897897720337 norm:0.0009409906924702227 max memory_allocated 29273.50048828125 
[2025-02-20 03:31:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.3530901074409485 norm:0.0009149675024673343 max memory_allocated 29273.50048828125 
[2025-02-20 03:32:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.3527146875858307 norm:0.0008836492779664695 max memory_allocated 29273.50048828125 
[2025-02-20 03:32:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.3523673713207245 norm:0.0008629063377156854 max memory_allocated 29273.50048828125 
[2025-02-20 03:33:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.35203817486763 norm:0.0008328325347974896 max memory_allocated 29273.50048828125 
[2025-02-20 03:34:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.35176628828048706 norm:0.0008159415447153151 max memory_allocated 29273.50048828125 
[2025-02-20 03:35:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.3515418469905853 norm:0.0007902054931037128 max memory_allocated 29273.50048828125 
[2025-02-20 03:36:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.3513272702693939 norm:0.0007619527168571949 max memory_allocated 29273.50048828125 
[2025-02-20 03:37:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.35120725631713867 norm:0.0007617356022819877 max memory_allocated 29273.50048828125 
[2025-02-20 03:37:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.35102561116218567 norm:0.0007617651717737317 max memory_allocated 29273.50048828125 
[2025-02-20 03:38:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-20 03:39:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.4392516613006592 norm:0.013448619283735752 max memory_allocated 29273.68798828125 
[2025-02-20 03:39:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.41270142793655396 norm:0.007448161020874977 max memory_allocated 29273.68798828125 
[2025-02-20 03:40:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.38575541973114014 norm:0.0039369938895106316 max memory_allocated 29273.68798828125 
[2025-02-20 03:41:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.3734833300113678 norm:0.001987423514947295 max memory_allocated 29273.68798828125 
[2025-02-20 03:42:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.36830151081085205 norm:0.00137859839014709 max memory_allocated 29273.68798828125 
[2025-02-20 03:43:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.3651539087295532 norm:0.0011115041561424732 max memory_allocated 29273.68798828125 
[2025-02-20 03:43:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.36300885677337646 norm:0.0010380258318036795 max memory_allocated 29273.68798828125 
[2025-02-20 03:44:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.36119344830513 norm:0.000951003807131201 max memory_allocated 29273.68798828125 
[2025-02-20 03:45:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.35969242453575134 norm:0.0008830137667246163 max memory_allocated 29273.68798828125 
[2025-02-20 03:46:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.3586749732494354 norm:0.000846360344439745 max memory_allocated 29273.68798828125 
[2025-02-20 03:47:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.35798412561416626 norm:0.00082457868847996 max memory_allocated 29273.68798828125 
[2025-02-20 03:48:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.3574211895465851 norm:0.0007979797665029764 max memory_allocated 29273.68798828125 
[2025-02-20 03:48:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.3569488823413849 norm:0.0007862016791477799 max memory_allocated 29273.68798828125 
[2025-02-20 03:49:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.3565172851085663 norm:0.000773746520280838 max memory_allocated 29273.68798828125 
[2025-02-20 03:50:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.3560793399810791 norm:0.0007713832892477512 max memory_allocated 29273.68798828125 
[2025-02-20 03:51:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.35559409856796265 norm:0.0007550192531198263 max memory_allocated 29273.68798828125 
[2025-02-20 03:52:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.35537296533584595 norm:0.0007447136449627578 max memory_allocated 29273.68798828125 
[2025-02-20 03:52:57 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.35527271032333374 norm:0.0007367692305706441 max memory_allocated 29273.68798828125 
[2025-02-20 03:53:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.3551093637943268 norm:0.0007372709806077182 max memory_allocated 29273.68798828125 
[2025-02-20 03:54:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.3550146818161011 norm:0.0007302239537239075 max memory_allocated 29273.68798828125 
[2025-02-20 03:54:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-20 03:55:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.45668143033981323 norm:0.029823223128914833 max memory_allocated 29273.87548828125 
[2025-02-20 03:56:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.42881596088409424 norm:0.014154503121972084 max memory_allocated 29273.87548828125 
[2025-02-20 03:57:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.40088707208633423 norm:0.006599109154194593 max memory_allocated 29273.87548828125 
[2025-02-20 03:58:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.3876774311065674 norm:0.0035605153534561396 max memory_allocated 29273.87548828125 
[2025-02-20 03:59:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.38229748606681824 norm:0.0023121521808207035 max memory_allocated 29273.87548828125 
[2025-02-20 03:59:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.37901416420936584 norm:0.001957557164132595 max memory_allocated 29273.87548828125 
[2025-02-20 04:00:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.3764130473136902 norm:0.0015352268237620592 max memory_allocated 29273.87548828125 
[2025-02-20 04:01:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.37426111102104187 norm:0.001244672341272235 max memory_allocated 29273.87548828125 
[2025-02-20 04:02:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.3728012144565582 norm:0.0011541454587131739 max memory_allocated 29273.87548828125 
[2025-02-20 04:03:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.37174686789512634 norm:0.0011240524472668767 max memory_allocated 29273.87548828125 
[2025-02-20 04:03:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.3708154261112213 norm:0.0010629845783114433 max memory_allocated 29273.87548828125 
[2025-02-20 04:04:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.37011128664016724 norm:0.0010312512749806046 max memory_allocated 29273.87548828125 
[2025-02-20 04:05:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.369467556476593 norm:0.000997351249679923 max memory_allocated 29273.87548828125 
[2025-02-20 04:06:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.3691069185733795 norm:0.0010089883580803871 max memory_allocated 29273.87548828125 
[2025-02-20 04:07:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.3687330484390259 norm:0.001011374988593161 max memory_allocated 29273.87548828125 
[2025-02-20 04:08:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.3683238625526428 norm:0.0009906121995300055 max memory_allocated 29273.87548828125 
[2025-02-20 04:08:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.36804938316345215 norm:0.0009890533983707428 max memory_allocated 29273.87548828125 
[2025-02-20 04:09:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.36784449219703674 norm:0.0009498423896729946 max memory_allocated 29273.87548828125 
[2025-02-20 04:10:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.3676137924194336 norm:0.0009008042979985476 max memory_allocated 29273.87548828125 
[2025-02-20 04:11:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.3673909306526184 norm:0.0008887238800525665 max memory_allocated 29273.87548828125 
[2025-02-20 04:11:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-20 04:12:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.4614197313785553 norm:0.0313597247004509 max memory_allocated 29274.06298828125 
[2025-02-20 04:13:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.4328533709049225 norm:0.013997675850987434 max memory_allocated 29274.06298828125 
[2025-02-20 04:14:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.4045141339302063 norm:0.004931118339300156 max memory_allocated 29274.06298828125 
[2025-02-20 04:14:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.39420685172080994 norm:0.002680836245417595 max memory_allocated 29274.06298828125 
[2025-02-20 04:15:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.3902320861816406 norm:0.0021191996056586504 max memory_allocated 29274.06298828125 
[2025-02-20 04:16:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.3874240517616272 norm:0.0017781713977456093 max memory_allocated 29274.06298828125 
[2025-02-20 04:17:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.38533487915992737 norm:0.0015760635724291205 max memory_allocated 29274.06298828125 
[2025-02-20 04:18:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.38357114791870117 norm:0.0013663690770044923 max memory_allocated 29274.06298828125 
[2025-02-20 04:18:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.3819885849952698 norm:0.0012211690191179514 max memory_allocated 29274.06298828125 
[2025-02-20 04:19:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.38073763251304626 norm:0.0011753508588299155 max memory_allocated 29274.06298828125 
[2025-02-20 04:20:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.3797825574874878 norm:0.001133994315750897 max memory_allocated 29274.06298828125 
[2025-02-20 04:21:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.3790598511695862 norm:0.001104635768570006 max memory_allocated 29274.06298828125 
[2025-02-20 04:22:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.37850189208984375 norm:0.0010820203460752964 max memory_allocated 29274.06298828125 
[2025-02-20 04:23:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.37800660729408264 norm:0.0010473101865500212 max memory_allocated 29274.06298828125 
[2025-02-20 04:23:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.37755244970321655 norm:0.0010253103682771325 max memory_allocated 29274.06298828125 
[2025-02-20 04:24:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.3770710229873657 norm:0.0009733432088978589 max memory_allocated 29274.06298828125 
[2025-02-20 04:25:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.37686917185783386 norm:0.0009869177592918277 max memory_allocated 29274.06298828125 
[2025-02-20 04:26:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.37660542130470276 norm:0.0009660673676989973 max memory_allocated 29274.06298828125 
[2025-02-20 04:27:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.3763374984264374 norm:0.0009364444413222373 max memory_allocated 29274.06298828125 
[2025-02-20 04:28:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.37598347663879395 norm:0.0009137882734648883 max memory_allocated 29274.06298828125 
[2025-02-20 04:28:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-20 04:29:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.459818959236145 norm:0.028663236647844315 max memory_allocated 29274.25048828125 
[2025-02-20 04:29:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.4318481981754303 norm:0.011385595425963402 max memory_allocated 29274.25048828125 
[2025-02-20 04:30:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.41215237975120544 norm:0.0056929923593997955 max memory_allocated 29274.25048828125 
[2025-02-20 04:31:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.4023202955722809 norm:0.002800398040562868 max memory_allocated 29274.25048828125 
[2025-02-20 04:32:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.39834293723106384 norm:0.001878018258139491 max memory_allocated 29274.25048828125 
[2025-02-20 04:33:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.396036297082901 norm:0.0016194357303902507 max memory_allocated 29274.25048828125 
[2025-02-20 04:34:03 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.3943932056427002 norm:0.0014834270114079118 max memory_allocated 29274.25048828125 
[2025-02-20 04:34:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.3929809033870697 norm:0.0013700914569199085 max memory_allocated 29274.25048828125 
[2025-02-20 04:35:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.39170342683792114 norm:0.0012860666029155254 max memory_allocated 29274.25048828125 
[2025-02-20 04:36:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.3907143771648407 norm:0.0012021753937005997 max memory_allocated 29274.25048828125 
[2025-02-20 04:37:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.38991159200668335 norm:0.0011420734226703644 max memory_allocated 29274.25048828125 
[2025-02-20 04:38:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.38927528262138367 norm:0.0011013897601515055 max memory_allocated 29274.25048828125 
[2025-02-20 04:38:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.38857048749923706 norm:0.0010219361865893006 max memory_allocated 29274.25048828125 
[2025-02-20 04:39:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.38792893290519714 norm:0.0009733489132486284 max memory_allocated 29274.25048828125 
[2025-02-20 04:40:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.38745591044425964 norm:0.0009705310221761465 max memory_allocated 29274.25048828125 
[2025-02-20 04:41:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.38706478476524353 norm:0.0009449216886423528 max memory_allocated 29274.25048828125 
[2025-02-20 04:42:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.38679224252700806 norm:0.0009200863423757255 max memory_allocated 29274.25048828125 
[2025-02-20 04:43:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.38651734590530396 norm:0.0009071236709132791 max memory_allocated 29274.25048828125 
[2025-02-20 04:43:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.3862249553203583 norm:0.0009078011498786509 max memory_allocated 29274.25048828125 
[2025-02-20 04:44:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.3859770894050598 norm:0.0008944498258642852 max memory_allocated 29274.25048828125 
[2025-02-20 04:44:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-20 04:45:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.4699288606643677 norm:0.019739778712391853 max memory_allocated 29274.43798828125 
[2025-02-20 04:46:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.44973689317703247 norm:0.009265155531466007 max memory_allocated 29274.43798828125 
[2025-02-20 04:47:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.4301852285861969 norm:0.00460472097620368 max memory_allocated 29274.43798828125 
[2025-02-20 04:48:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.42084020376205444 norm:0.002316061407327652 max memory_allocated 29274.43798828125 
[2025-02-20 04:49:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.4171076714992523 norm:0.0017200325382873416 max memory_allocated 29274.43798828125 
[2025-02-20 04:49:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.4149038791656494 norm:0.0015278770588338375 max memory_allocated 29274.43798828125 
[2025-02-20 04:50:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.41321495175361633 norm:0.0014167275512591004 max memory_allocated 29274.43798828125 
[2025-02-20 04:51:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.41181257367134094 norm:0.0013254738878458738 max memory_allocated 29274.43798828125 
[2025-02-20 04:52:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.4105863571166992 norm:0.0012526767095550895 max memory_allocated 29274.43798828125 
[2025-02-20 04:53:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.40946948528289795 norm:0.0011539822444319725 max memory_allocated 29274.43798828125 
[2025-02-20 04:54:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.40855634212493896 norm:0.0010926933027803898 max memory_allocated 29274.43798828125 
[2025-02-20 04:54:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.4077947735786438 norm:0.0010384675115346909 max memory_allocated 29274.43798828125 
[2025-02-20 04:55:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.4072226881980896 norm:0.0009860852733254433 max memory_allocated 29274.43798828125 
[2025-02-20 04:56:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.40668225288391113 norm:0.00096339569427073 max memory_allocated 29274.43798828125 
[2025-02-20 04:57:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.40633293986320496 norm:0.0009353121276944876 max memory_allocated 29274.43798828125 
[2025-02-20 04:58:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.4061320126056671 norm:0.0009291888563893735 max memory_allocated 29274.43798828125 
[2025-02-20 04:58:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.4058605134487152 norm:0.0009082508622668684 max memory_allocated 29274.43798828125 
[2025-02-20 04:59:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.40551015734672546 norm:0.000880219042301178 max memory_allocated 29274.43798828125 
[2025-02-20 05:00:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.4053288996219635 norm:0.0008691624971106648 max memory_allocated 29274.43798828125 
[2025-02-20 05:01:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.4051724970340729 norm:0.0008679182501509786 max memory_allocated 29274.43798828125 
[2025-02-20 05:01:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-20 05:02:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.48827776312828064 norm:0.019010018557310104 max memory_allocated 29274.62548828125 
[2025-02-20 05:03:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.46816185116767883 norm:0.008540648967027664 max memory_allocated 29274.62548828125 
[2025-02-20 05:04:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.45152992010116577 norm:0.004469395149499178 max memory_allocated 29274.62548828125 
[2025-02-20 05:04:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.44258546829223633 norm:0.0023043896071612835 max memory_allocated 29274.62548828125 
[2025-02-20 05:05:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.43889349699020386 norm:0.0016904746880754828 max memory_allocated 29274.62548828125 
[2025-02-20 05:06:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.4367256760597229 norm:0.0014664093032479286 max memory_allocated 29274.62548828125 
[2025-02-20 05:07:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.43498483300209045 norm:0.0013004394713789225 max memory_allocated 29274.62548828125 
[2025-02-20 05:08:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.4336601495742798 norm:0.0012200671480968595 max memory_allocated 29274.62548828125 
[2025-02-20 05:09:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.43253135681152344 norm:0.0011287261731922626 max memory_allocated 29274.62548828125 
[2025-02-20 05:09:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.43163037300109863 norm:0.0010793616529554129 max memory_allocated 29274.62548828125 
[2025-02-20 05:10:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.4307951331138611 norm:0.0010286979377269745 max memory_allocated 29274.62548828125 
[2025-02-20 05:11:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.43007558584213257 norm:0.0009782511042430997 max memory_allocated 29274.62548828125 
[2025-02-20 05:12:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.4295825958251953 norm:0.00095842668088153 max memory_allocated 29274.62548828125 
[2025-02-20 05:13:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.4290768802165985 norm:0.0009234283934347332 max memory_allocated 29274.62548828125 
[2025-02-20 05:14:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.42863595485687256 norm:0.0009051805245690048 max memory_allocated 29274.62548828125 
[2025-02-20 05:14:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.4282984137535095 norm:0.0008922415436245501 max memory_allocated 29274.62548828125 
[2025-02-20 05:15:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.42797937989234924 norm:0.0008772871224209666 max memory_allocated 29274.62548828125 
[2025-02-20 05:16:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.42776039242744446 norm:0.0008520586998201907 max memory_allocated 29274.62548828125 
[2025-02-20 05:17:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.4275454580783844 norm:0.0008555138483643532 max memory_allocated 29274.62548828125 
[2025-02-20 05:18:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.4273652136325836 norm:0.0008418897050432861 max memory_allocated 29274.62548828125 
[2025-02-20 05:18:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-20 05:19:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.5264502167701721 norm:0.018019365146756172 max memory_allocated 29274.81298828125 
[2025-02-20 05:20:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.5069408416748047 norm:0.009156955406069756 max memory_allocated 29274.81298828125 
[2025-02-20 05:20:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.4893224835395813 norm:0.004798930138349533 max memory_allocated 29274.81298828125 
[2025-02-20 05:21:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.4798164963722229 norm:0.0028474389109760523 max memory_allocated 29274.81298828125 
[2025-02-20 05:22:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.4755209684371948 norm:0.0019345249747857451 max memory_allocated 29274.81298828125 
[2025-02-20 05:23:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.4732653796672821 norm:0.001639431924559176 max memory_allocated 29274.81298828125 
