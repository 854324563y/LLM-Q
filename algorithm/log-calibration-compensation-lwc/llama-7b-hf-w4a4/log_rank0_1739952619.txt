[2025-02-19 08:10:19 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-7b-hf-w4a4', save_dir='./log-calibration-compensation-lwc/quant/llama-7b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-19 08:11:52 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-19 08:11:53 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-19 08:11:53 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-19 08:11:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-19 08:11:57 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:12:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.06937620788812637 norm:0.04166024923324585 max memory_allocated 22559.10693359375 
[2025-02-19 08:12:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.044969331473112106 norm:0.021348081529140472 max memory_allocated 22559.10693359375 
[2025-02-19 08:13:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.03682670742273331 norm:0.01623043417930603 max memory_allocated 22559.10693359375 
[2025-02-19 08:14:00 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.033432722091674805 norm:0.014253787696361542 max memory_allocated 22559.10693359375 
[2025-02-19 08:14:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0317765511572361 norm:0.012347180396318436 max memory_allocated 22559.10693359375 
[2025-02-19 08:15:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.030749371275305748 norm:0.010723037645220757 max memory_allocated 22559.10693359375 
[2025-02-19 08:15:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.029976841062307358 norm:0.009030572138726711 max memory_allocated 22559.10693359375 
[2025-02-19 08:16:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.02947159856557846 norm:0.008011824451386929 max memory_allocated 22559.10693359375 
[2025-02-19 08:16:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.029242627322673798 norm:0.007074946071952581 max memory_allocated 22559.10693359375 
[2025-02-19 08:17:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.02897818572819233 norm:0.006255342159420252 max memory_allocated 22559.10693359375 
[2025-02-19 08:17:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.02888728305697441 norm:0.0054847770370543 max memory_allocated 22559.10693359375 
[2025-02-19 08:18:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.028662730008363724 norm:0.004919065162539482 max memory_allocated 22559.10693359375 
[2025-02-19 08:18:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.02857528254389763 norm:0.00447606947273016 max memory_allocated 22559.10693359375 
[2025-02-19 08:19:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.02846026048064232 norm:0.00407788809388876 max memory_allocated 22559.10693359375 
[2025-02-19 08:19:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.028461983427405357 norm:0.0039800964295864105 max memory_allocated 22559.10693359375 
[2025-02-19 08:20:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.02836712636053562 norm:0.0037460161838680506 max memory_allocated 22559.10693359375 
[2025-02-19 08:20:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.028490930795669556 norm:0.0036750417202711105 max memory_allocated 22559.10693359375 
[2025-02-19 08:21:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.02832663431763649 norm:0.0034664683043956757 max memory_allocated 22559.10693359375 
[2025-02-19 08:21:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.028329607099294662 norm:0.0034740776754915714 max memory_allocated 22559.10693359375 
[2025-02-19 08:22:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.02821895107626915 norm:0.003331141546368599 max memory_allocated 22559.10693359375 
[2025-02-19 08:22:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-19 08:22:25 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:22:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.24604782462120056 norm:0.128302201628685 max memory_allocated 22559.27880859375 
[2025-02-19 08:23:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.1408122032880783 norm:0.03914559632539749 max memory_allocated 22559.27880859375 
[2025-02-19 08:24:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.10746370255947113 norm:0.020470308139920235 max memory_allocated 22559.27880859375 
[2025-02-19 08:24:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.09386138617992401 norm:0.017719529569149017 max memory_allocated 22559.27880859375 
[2025-02-19 08:25:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.08732368052005768 norm:0.015038356184959412 max memory_allocated 22559.27880859375 
[2025-02-19 08:25:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.08303092420101166 norm:0.013544583693146706 max memory_allocated 22559.27880859375 
[2025-02-19 08:26:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.08013016730546951 norm:0.011858787387609482 max memory_allocated 22559.27880859375 
[2025-02-19 08:26:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.07834194600582123 norm:0.010818921960890293 max memory_allocated 22559.27880859375 
[2025-02-19 08:27:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.07699164003133774 norm:0.009684722870588303 max memory_allocated 22559.27880859375 
[2025-02-19 08:27:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.07589511573314667 norm:0.008547217585146427 max memory_allocated 22559.27880859375 
[2025-02-19 08:28:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.07512257248163223 norm:0.007767778355628252 max memory_allocated 22559.27880859375 
[2025-02-19 08:28:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.07462728768587112 norm:0.007143059745430946 max memory_allocated 22559.27880859375 
[2025-02-19 08:29:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.07409472018480301 norm:0.006640790030360222 max memory_allocated 22559.27880859375 
[2025-02-19 08:29:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.07373502105474472 norm:0.006181344855576754 max memory_allocated 22559.27880859375 
[2025-02-19 08:30:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0735231339931488 norm:0.005811271257698536 max memory_allocated 22559.27880859375 
[2025-02-19 08:30:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.07318540662527084 norm:0.005533947143703699 max memory_allocated 22559.27880859375 
[2025-02-19 08:31:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.07293160259723663 norm:0.005261387676000595 max memory_allocated 22559.27880859375 
[2025-02-19 08:31:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.07269272953271866 norm:0.005093711894005537 max memory_allocated 22559.27880859375 
[2025-02-19 08:32:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.07247696816921234 norm:0.004755716770887375 max memory_allocated 22559.27880859375 
[2025-02-19 08:32:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.07231234014034271 norm:0.004698817618191242 max memory_allocated 22559.27880859375 
[2025-02-19 08:32:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-19 08:32:54 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:33:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.2545289695262909 norm:0.038758356124162674 max memory_allocated 22559.45068359375 
[2025-02-19 08:33:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.20503202080726624 norm:0.03647688776254654 max memory_allocated 22559.45068359375 
[2025-02-19 08:34:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.18192794919013977 norm:0.01960710994899273 max memory_allocated 22559.45068359375 
[2025-02-19 08:34:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.17055082321166992 norm:0.016230786219239235 max memory_allocated 22559.45068359375 
[2025-02-19 08:35:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.1629856377840042 norm:0.014748207293450832 max memory_allocated 22559.45068359375 
[2025-02-19 08:35:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.1582397222518921 norm:0.014844044111669064 max memory_allocated 22559.45068359375 
[2025-02-19 08:36:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.15424412488937378 norm:0.014874586835503578 max memory_allocated 22559.45068359375 
[2025-02-19 08:36:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.15123684704303741 norm:0.014265794306993484 max memory_allocated 22559.45068359375 
[2025-02-19 08:37:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.14764484763145447 norm:0.014030181802809238 max memory_allocated 22559.45068359375 
[2025-02-19 08:38:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.14642731845378876 norm:0.013059518299996853 max memory_allocated 22559.45068359375 
[2025-02-19 08:38:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.14722606539726257 norm:0.01351049356162548 max memory_allocated 22559.45068359375 
[2025-02-19 08:39:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.1443728804588318 norm:0.013409310951828957 max memory_allocated 22559.45068359375 
[2025-02-19 08:39:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.1424752175807953 norm:0.01207205094397068 max memory_allocated 22559.45068359375 
[2025-02-19 08:40:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.14327186346054077 norm:0.011737610213458538 max memory_allocated 22559.45068359375 
[2025-02-19 08:40:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.1434973180294037 norm:0.01213969849050045 max memory_allocated 22559.45068359375 
[2025-02-19 08:41:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.14185817539691925 norm:0.011198737658560276 max memory_allocated 22559.45068359375 
[2025-02-19 08:41:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.14271187782287598 norm:0.012317518703639507 max memory_allocated 22559.45068359375 
[2025-02-19 08:42:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.142277792096138 norm:0.012108569964766502 max memory_allocated 22559.45068359375 
[2025-02-19 08:42:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.14239197969436646 norm:0.011257344856858253 max memory_allocated 22559.45068359375 
[2025-02-19 08:43:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.14156202971935272 norm:0.010981610044836998 max memory_allocated 22559.45068359375 
[2025-02-19 08:43:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-19 08:43:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.24554438889026642 norm:0.05492546036839485 max memory_allocated 22559.50732421875 
[2025-02-19 08:44:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.19863641262054443 norm:0.015711965039372444 max memory_allocated 22559.50732421875 
[2025-02-19 08:44:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.1687058061361313 norm:0.00669662794098258 max memory_allocated 22559.50732421875 
[2025-02-19 08:45:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.15905749797821045 norm:0.0035369086544960737 max memory_allocated 22559.50732421875 
[2025-02-19 08:45:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.15461544692516327 norm:0.0027333665639162064 max memory_allocated 22559.50732421875 
[2025-02-19 08:46:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.1516820192337036 norm:0.002221072092652321 max memory_allocated 22559.50732421875 
[2025-02-19 08:46:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.149912029504776 norm:0.00203995150513947 max memory_allocated 22559.50732421875 
[2025-02-19 08:47:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.14860662817955017 norm:0.001842044061049819 max memory_allocated 22559.50732421875 
[2025-02-19 08:47:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.1477232426404953 norm:0.0016840783646330237 max memory_allocated 22559.50732421875 
[2025-02-19 08:48:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.14693616330623627 norm:0.0016190687892958522 max memory_allocated 22559.50732421875 
[2025-02-19 08:48:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.1465204656124115 norm:0.001645208802074194 max memory_allocated 22559.50732421875 
[2025-02-19 08:49:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.14620600640773773 norm:0.0016155423363670707 max memory_allocated 22559.50732421875 
[2025-02-19 08:49:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.1458645761013031 norm:0.0015101621393114328 max memory_allocated 22559.50732421875 
[2025-02-19 08:50:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.14530214667320251 norm:0.0013989153085276484 max memory_allocated 22559.50732421875 
[2025-02-19 08:50:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.1450309157371521 norm:0.0013462422648444772 max memory_allocated 22559.50732421875 
[2025-02-19 08:51:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.14480865001678467 norm:0.0012780006509274244 max memory_allocated 22559.50732421875 
[2025-02-19 08:52:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.14462634921073914 norm:0.0012349712196737528 max memory_allocated 22559.50732421875 
[2025-02-19 08:52:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.14454445242881775 norm:0.0012288937577977777 max memory_allocated 22559.50732421875 
[2025-02-19 08:53:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.1445688158273697 norm:0.0012160895857959986 max memory_allocated 22559.50732421875 
[2025-02-19 08:53:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.14437837898731232 norm:0.0011686370708048344 max memory_allocated 22559.50732421875 
[2025-02-19 08:53:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-19 08:54:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.2940700948238373 norm:0.04768060892820358 max memory_allocated 22559.67919921875 
[2025-02-19 08:54:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.24513275921344757 norm:0.019099466502666473 max memory_allocated 22559.67919921875 
[2025-02-19 08:55:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.2090199589729309 norm:0.006856804713606834 max memory_allocated 22559.67919921875 
[2025-02-19 08:55:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.19515129923820496 norm:0.0031504544895142317 max memory_allocated 22559.67919921875 
[2025-02-19 08:56:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.1895647495985031 norm:0.002220636699348688 max memory_allocated 22559.67919921875 
[2025-02-19 08:56:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.18665726482868195 norm:0.0019808292854577303 max memory_allocated 22559.67919921875 
[2025-02-19 08:57:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.18474698066711426 norm:0.001839568023569882 max memory_allocated 22559.67919921875 
[2025-02-19 08:57:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.1838952600955963 norm:0.001839023781940341 max memory_allocated 22559.67919921875 
[2025-02-19 08:58:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.18297260999679565 norm:0.0017783415969461203 max memory_allocated 22559.67919921875 
[2025-02-19 08:58:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.18212440609931946 norm:0.0016434738645330071 max memory_allocated 22559.67919921875 
[2025-02-19 08:59:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.18142926692962646 norm:0.0015389100881293416 max memory_allocated 22559.67919921875 
[2025-02-19 08:59:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.18100394308567047 norm:0.0014200315345078707 max memory_allocated 22559.67919921875 
[2025-02-19 09:00:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.18087948858737946 norm:0.0013785286573693156 max memory_allocated 22559.67919921875 
[2025-02-19 09:00:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.18114465475082397 norm:0.0014180594589561224 max memory_allocated 22559.67919921875 
[2025-02-19 09:01:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.18120290338993073 norm:0.0014023593394085765 max memory_allocated 22559.67919921875 
[2025-02-19 09:01:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.18116053938865662 norm:0.0013823886401951313 max memory_allocated 22559.67919921875 
[2025-02-19 09:02:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.18089646100997925 norm:0.0013022528728470206 max memory_allocated 22559.67919921875 
[2025-02-19 09:02:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.18093697726726532 norm:0.001335682114586234 max memory_allocated 22559.67919921875 
[2025-02-19 09:03:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.18086299300193787 norm:0.0012596654705703259 max memory_allocated 22559.67919921875 
[2025-02-19 09:03:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.18093237280845642 norm:0.0012556321453303099 max memory_allocated 22559.67919921875 
[2025-02-19 09:04:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-19 09:04:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.34624484181404114 norm:0.07471577823162079 max memory_allocated 22559.85107421875 
[2025-02-19 09:05:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.2833987772464752 norm:0.02317444048821926 max memory_allocated 22559.85107421875 
[2025-02-19 09:05:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.23565524816513062 norm:0.00705506419762969 max memory_allocated 22559.85107421875 
[2025-02-19 09:06:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.22052988409996033 norm:0.0036587719805538654 max memory_allocated 22559.85107421875 
[2025-02-19 09:06:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.2149357944726944 norm:0.0030380936805158854 max memory_allocated 22559.85107421875 
[2025-02-19 09:07:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.21095463633537292 norm:0.0025257172528654337 max memory_allocated 22559.85107421875 
[2025-02-19 09:07:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.20837430655956268 norm:0.0022343264427036047 max memory_allocated 22559.85107421875 
[2025-02-19 09:08:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.20667140185832977 norm:0.0020686709322035313 max memory_allocated 22559.85107421875 
[2025-02-19 09:08:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.20573639869689941 norm:0.0019803731702268124 max memory_allocated 22559.85107421875 
[2025-02-19 09:09:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.20495733618736267 norm:0.0019016144797205925 max memory_allocated 22559.85107421875 
[2025-02-19 09:09:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.20408004522323608 norm:0.0017192061059176922 max memory_allocated 22559.85107421875 
[2025-02-19 09:10:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.20338685810565948 norm:0.0015954800182953477 max memory_allocated 22559.85107421875 
[2025-02-19 09:10:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.20311838388442993 norm:0.0016060105990618467 max memory_allocated 22559.85107421875 
[2025-02-19 09:11:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.20289355516433716 norm:0.001524397055618465 max memory_allocated 22559.85107421875 
[2025-02-19 09:11:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.20250974595546722 norm:0.0014628421049565077 max memory_allocated 22559.85107421875 
[2025-02-19 09:12:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.20237863063812256 norm:0.0014726464869454503 max memory_allocated 22559.85107421875 
[2025-02-19 09:12:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.20198795199394226 norm:0.0013478911714628339 max memory_allocated 22559.85107421875 
[2025-02-19 09:13:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.2017534375190735 norm:0.0013364613987505436 max memory_allocated 22559.85107421875 
[2025-02-19 09:13:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.20149382948875427 norm:0.0013066857354715466 max memory_allocated 22559.85107421875 
[2025-02-19 09:14:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.20134055614471436 norm:0.001247141626663506 max memory_allocated 22559.85107421875 
[2025-02-19 09:14:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-19 09:14:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.3610647916793823 norm:0.0663372129201889 max memory_allocated 22560.02294921875 
[2025-02-19 09:15:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.3154410123825073 norm:0.028958957642316818 max memory_allocated 22560.02294921875 
[2025-02-19 09:16:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.2639331519603729 norm:0.007916350848972797 max memory_allocated 22560.02294921875 
[2025-02-19 09:16:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.24622681736946106 norm:0.003827393054962158 max memory_allocated 22560.02294921875 
[2025-02-19 09:17:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.24027270078659058 norm:0.0029489840380847454 max memory_allocated 22560.02294921875 
[2025-02-19 09:17:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.23625440895557404 norm:0.0023249217774719 max memory_allocated 22560.02294921875 
[2025-02-19 09:18:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.23342299461364746 norm:0.0019413955742493272 max memory_allocated 22560.02294921875 
[2025-02-19 09:18:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.2313888669013977 norm:0.0017538834363222122 max memory_allocated 22560.02294921875 
[2025-02-19 09:19:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.2299995720386505 norm:0.0016992907039821148 max memory_allocated 22560.02294921875 
[2025-02-19 09:19:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.2292039692401886 norm:0.0016395405400544405 max memory_allocated 22560.02294921875 
[2025-02-19 09:20:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.22871415317058563 norm:0.0015868128975853324 max memory_allocated 22560.02294921875 
[2025-02-19 09:20:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.22826911509037018 norm:0.0015380169497802854 max memory_allocated 22560.02294921875 
[2025-02-19 09:21:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.22745004296302795 norm:0.0014204996405169368 max memory_allocated 22560.02294921875 
[2025-02-19 09:21:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.22722752392292023 norm:0.001389569602906704 max memory_allocated 22560.02294921875 
[2025-02-19 09:22:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.2271219789981842 norm:0.0013602611143141985 max memory_allocated 22560.02294921875 
[2025-02-19 09:22:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.22727137804031372 norm:0.0013679717667400837 max memory_allocated 22560.02294921875 
[2025-02-19 09:23:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.22696778178215027 norm:0.0013698688708245754 max memory_allocated 22560.02294921875 
[2025-02-19 09:23:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.2266487032175064 norm:0.0013234305661171675 max memory_allocated 22560.02294921875 
[2025-02-19 09:24:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.2264205813407898 norm:0.0013027435634285212 max memory_allocated 22560.02294921875 
[2025-02-19 09:24:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.22632309794425964 norm:0.0012571373954415321 max memory_allocated 22560.02294921875 
[2025-02-19 09:24:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-19 09:25:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.3927097022533417 norm:0.044672779738903046 max memory_allocated 22560.19482421875 
[2025-02-19 09:25:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.3423418402671814 norm:0.02058330923318863 max memory_allocated 22560.19482421875 
[2025-02-19 09:26:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.2863205671310425 norm:0.006290342193096876 max memory_allocated 22560.19482421875 
[2025-02-19 09:26:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.2686648368835449 norm:0.003075988730415702 max memory_allocated 22560.19482421875 
[2025-02-19 09:27:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.262225866317749 norm:0.0022557342890650034 max memory_allocated 22560.19482421875 
[2025-02-19 09:27:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.2578415274620056 norm:0.001876126043498516 max memory_allocated 22560.19482421875 
[2025-02-19 09:28:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.25519558787345886 norm:0.0017240913584828377 max memory_allocated 22560.19482421875 
[2025-02-19 09:28:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.25336968898773193 norm:0.0015930257504805923 max memory_allocated 22560.19482421875 
[2025-02-19 09:29:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.25219589471817017 norm:0.0014506463194265962 max memory_allocated 22560.19482421875 
[2025-02-19 09:29:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.25114136934280396 norm:0.0013638818636536598 max memory_allocated 22560.19482421875 
[2025-02-19 09:30:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.2504528760910034 norm:0.001306548248976469 max memory_allocated 22560.19482421875 
[2025-02-19 09:30:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.25018227100372314 norm:0.0012371224584057927 max memory_allocated 22560.19482421875 
[2025-02-19 09:31:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.24987733364105225 norm:0.0012231690343469381 max memory_allocated 22560.19482421875 
[2025-02-19 09:32:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.24938291311264038 norm:0.0011654943227767944 max memory_allocated 22560.19482421875 
[2025-02-19 09:32:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.24936459958553314 norm:0.001173519529402256 max memory_allocated 22560.19482421875 
[2025-02-19 09:33:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.24942490458488464 norm:0.0012166611850261688 max memory_allocated 22560.19482421875 
[2025-02-19 09:33:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.24916288256645203 norm:0.0011732199927791953 max memory_allocated 22560.19482421875 
[2025-02-19 09:34:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.24937845766544342 norm:0.001222156686708331 max memory_allocated 22560.19482421875 
[2025-02-19 09:34:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.24936041235923767 norm:0.001136180479079485 max memory_allocated 22560.19482421875 
[2025-02-19 09:35:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.24928517639636993 norm:0.0011488026939332485 max memory_allocated 22560.19482421875 
[2025-02-19 09:35:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-19 09:35:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.4070379137992859 norm:0.03416125848889351 max memory_allocated 22560.36669921875 
[2025-02-19 09:36:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.3577125370502472 norm:0.01626455783843994 max memory_allocated 22560.36669921875 
[2025-02-19 09:36:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.3057200610637665 norm:0.005467031616717577 max memory_allocated 22560.36669921875 
[2025-02-19 09:37:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.286729633808136 norm:0.0023612966760993004 max memory_allocated 22560.36669921875 
[2025-02-19 09:37:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.28063780069351196 norm:0.0019511420978233218 max memory_allocated 22560.36669921875 
[2025-02-19 09:38:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.27672645449638367 norm:0.0016927876276895404 max memory_allocated 22560.36669921875 
[2025-02-19 09:38:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.27399542927742004 norm:0.0015325489221140742 max memory_allocated 22560.36669921875 
[2025-02-19 09:39:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.2721271514892578 norm:0.0014309443067759275 max memory_allocated 22560.36669921875 
[2025-02-19 09:39:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.27068430185317993 norm:0.0013658127281814814 max memory_allocated 22560.36669921875 
[2025-02-19 09:40:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.2695885896682739 norm:0.0013030520640313625 max memory_allocated 22560.36669921875 
[2025-02-19 09:40:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.268677294254303 norm:0.0012435048120096326 max memory_allocated 22560.36669921875 
[2025-02-19 09:41:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.2681140601634979 norm:0.0011980507988482714 max memory_allocated 22560.36669921875 
[2025-02-19 09:41:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.26760756969451904 norm:0.001142422086559236 max memory_allocated 22560.36669921875 
[2025-02-19 09:42:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.26740792393684387 norm:0.0011117883259430528 max memory_allocated 22560.36669921875 
[2025-02-19 09:42:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.26701679825782776 norm:0.001047409139573574 max memory_allocated 22560.36669921875 
[2025-02-19 09:43:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.2671712040901184 norm:0.0010473928414285183 max memory_allocated 22560.36669921875 
[2025-02-19 09:43:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.2669953405857086 norm:0.001023244927637279 max memory_allocated 22560.36669921875 
[2025-02-19 09:44:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.26680928468704224 norm:0.0009997694287449121 max memory_allocated 22560.36669921875 
[2025-02-19 09:44:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.2666810154914856 norm:0.0010033538565039635 max memory_allocated 22560.36669921875 
[2025-02-19 09:45:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.2664751410484314 norm:0.0010230797342956066 max memory_allocated 22560.36669921875 
[2025-02-19 09:45:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-19 09:46:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.43923449516296387 norm:0.04142184183001518 max memory_allocated 22560.53857421875 
[2025-02-19 09:46:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.39795249700546265 norm:0.02260974794626236 max memory_allocated 22560.53857421875 
[2025-02-19 09:47:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.33369940519332886 norm:0.007332941517233849 max memory_allocated 22560.53857421875 
[2025-02-19 09:47:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.30890968441963196 norm:0.002863067900761962 max memory_allocated 22560.53857421875 
[2025-02-19 09:48:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.30134493112564087 norm:0.0021481686271727085 max memory_allocated 22560.53857421875 
[2025-02-19 09:48:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.29702261090278625 norm:0.0018367822049185634 max memory_allocated 22560.53857421875 
[2025-02-19 09:49:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.29380398988723755 norm:0.001648738980293274 max memory_allocated 22560.53857421875 
[2025-02-19 09:49:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.2917758822441101 norm:0.0015295527409762144 max memory_allocated 22560.53857421875 
[2025-02-19 09:50:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.2905128002166748 norm:0.0014117597602307796 max memory_allocated 22560.53857421875 
[2025-02-19 09:50:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.28902560472488403 norm:0.001333425985649228 max memory_allocated 22560.53857421875 
[2025-02-19 09:51:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.2879447340965271 norm:0.0011962931603193283 max memory_allocated 22560.53857421875 
[2025-02-19 09:51:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.2873467206954956 norm:0.0011881525861099362 max memory_allocated 22560.53857421875 
[2025-02-19 09:52:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.286831259727478 norm:0.0011224228655919433 max memory_allocated 22560.53857421875 
[2025-02-19 09:52:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.2865884304046631 norm:0.0011059533571824431 max memory_allocated 22560.53857421875 
[2025-02-19 09:53:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.28611963987350464 norm:0.001106761978007853 max memory_allocated 22560.53857421875 
[2025-02-19 09:53:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.2859417796134949 norm:0.001066291588358581 max memory_allocated 22560.53857421875 
[2025-02-19 09:54:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.285832941532135 norm:0.001052839565090835 max memory_allocated 22560.53857421875 
[2025-02-19 09:54:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.28555789589881897 norm:0.0010032906429842114 max memory_allocated 22560.53857421875 
[2025-02-19 09:55:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.2854641079902649 norm:0.0010082053486257792 max memory_allocated 22560.53857421875 
[2025-02-19 09:55:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.285293310880661 norm:0.0009791420307010412 max memory_allocated 22560.53857421875 
[2025-02-19 09:56:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-19 09:56:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.40689021348953247 norm:0.022745177149772644 max memory_allocated 22560.71044921875 
[2025-02-19 09:57:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.3743777573108673 norm:0.012582509778439999 max memory_allocated 22560.71044921875 
[2025-02-19 09:57:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.33289259672164917 norm:0.004311271943151951 max memory_allocated 22560.71044921875 
[2025-02-19 09:58:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.31746602058410645 norm:0.002005983842536807 max memory_allocated 22560.71044921875 
[2025-02-19 09:58:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.31174182891845703 norm:0.0014670459786430001 max memory_allocated 22560.71044921875 
[2025-02-19 09:59:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.3085993230342865 norm:0.0013690977357327938 max memory_allocated 22560.71044921875 
[2025-02-19 09:59:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.3063858151435852 norm:0.0012324898270890117 max memory_allocated 22560.71044921875 
[2025-02-19 10:00:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.3048931062221527 norm:0.001186528941616416 max memory_allocated 22560.71044921875 
[2025-02-19 10:00:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.30387020111083984 norm:0.0011302551720291376 max memory_allocated 22560.71044921875 
[2025-02-19 10:01:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.3031657636165619 norm:0.0010931577999144793 max memory_allocated 22560.71044921875 
[2025-02-19 10:01:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.3022546172142029 norm:0.0010569660225883126 max memory_allocated 22560.71044921875 
[2025-02-19 10:02:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.3015773594379425 norm:0.0010202256962656975 max memory_allocated 22560.71044921875 
[2025-02-19 10:02:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.30121228098869324 norm:0.001003215671516955 max memory_allocated 22560.71044921875 
[2025-02-19 10:03:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.3009657561779022 norm:0.0009535813005641103 max memory_allocated 22560.71044921875 
[2025-02-19 10:03:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.30073586106300354 norm:0.0009158875909633934 max memory_allocated 22560.71044921875 
[2025-02-19 10:04:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.30056270956993103 norm:0.0008989233756437898 max memory_allocated 22560.71044921875 
[2025-02-19 10:04:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.30036601424217224 norm:0.0009082392789423466 max memory_allocated 22560.71044921875 
[2025-02-19 10:05:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.2999970316886902 norm:0.0008767712861299515 max memory_allocated 22560.71044921875 
[2025-02-19 10:05:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.2998247742652893 norm:0.0008486930746585131 max memory_allocated 22560.71044921875 
[2025-02-19 10:06:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.2997228801250458 norm:0.000827452982775867 max memory_allocated 22560.71044921875 
[2025-02-19 10:06:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-19 10:06:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.435655802488327 norm:0.03399096429347992 max memory_allocated 22560.88232421875 
[2025-02-19 10:07:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.3996425271034241 norm:0.0175150278955698 max memory_allocated 22560.88232421875 
[2025-02-19 10:07:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.35380464792251587 norm:0.006215827539563179 max memory_allocated 22560.88232421875 
[2025-02-19 10:08:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.334248811006546 norm:0.002654945943504572 max memory_allocated 22560.88232421875 
[2025-02-19 10:08:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.32788974046707153 norm:0.0019048037938773632 max memory_allocated 22560.88232421875 
[2025-02-19 10:09:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.3238702118396759 norm:0.0016607607249170542 max memory_allocated 22560.88232421875 
[2025-02-19 10:10:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.32108956575393677 norm:0.001489127753302455 max memory_allocated 22560.88232421875 
[2025-02-19 10:10:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.31911465525627136 norm:0.0013798766303807497 max memory_allocated 22560.88232421875 
[2025-02-19 10:11:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.3176769018173218 norm:0.0012443158775568008 max memory_allocated 22560.88232421875 
[2025-02-19 10:11:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.3167128264904022 norm:0.0011639301665127277 max memory_allocated 22560.88232421875 
[2025-02-19 10:12:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.31591472029685974 norm:0.0011354918824508786 max memory_allocated 22560.88232421875 
[2025-02-19 10:12:33 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.3153560161590576 norm:0.0011012288741767406 max memory_allocated 22560.88232421875 
[2025-02-19 10:13:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.31488072872161865 norm:0.0010586018906906247 max memory_allocated 22560.88232421875 
[2025-02-19 10:13:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.3145081400871277 norm:0.0010295602260157466 max memory_allocated 22560.88232421875 
[2025-02-19 10:14:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.3143708109855652 norm:0.0010273035150021315 max memory_allocated 22560.88232421875 
[2025-02-19 10:14:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.3141445517539978 norm:0.0010077444603666663 max memory_allocated 22560.88232421875 
[2025-02-19 10:15:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.3139016926288605 norm:0.001013456960208714 max memory_allocated 22560.88232421875 
[2025-02-19 10:15:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.31384095549583435 norm:0.0010401483159512281 max memory_allocated 22560.88232421875 
[2025-02-19 10:16:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.3134267032146454 norm:0.0009846887551248074 max memory_allocated 22560.88232421875 
[2025-02-19 10:16:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.3131592571735382 norm:0.0009538057493045926 max memory_allocated 22560.88232421875 
[2025-02-19 10:16:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-19 10:17:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.425845205783844 norm:0.027982335537672043 max memory_allocated 22561.05419921875 
[2025-02-19 10:17:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.40245339274406433 norm:0.0155557319521904 max memory_allocated 22561.05419921875 
[2025-02-19 10:18:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.37011095881462097 norm:0.007974815554916859 max memory_allocated 22561.05419921875 
[2025-02-19 10:18:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.35123899579048157 norm:0.004249190911650658 max memory_allocated 22561.05419921875 
[2025-02-19 10:19:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.344112366437912 norm:0.0031109661795198917 max memory_allocated 22561.05419921875 
[2025-02-19 10:19:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.33965685963630676 norm:0.0025625661946833134 max memory_allocated 22561.05419921875 
[2025-02-19 10:20:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.3366363048553467 norm:0.0022204890847206116 max memory_allocated 22561.05419921875 
[2025-02-19 10:20:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.33435148000717163 norm:0.0019572589080780745 max memory_allocated 22561.05419921875 
[2025-02-19 10:21:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.33273977041244507 norm:0.0018123204354196787 max memory_allocated 22561.05419921875 
[2025-02-19 10:21:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.33144697546958923 norm:0.001622167183086276 max memory_allocated 22561.05419921875 
[2025-02-19 10:22:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.33038565516471863 norm:0.001512217684648931 max memory_allocated 22561.05419921875 
[2025-02-19 10:22:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.3296848237514496 norm:0.0014787737745791674 max memory_allocated 22561.05419921875 
[2025-02-19 10:23:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.3290906250476837 norm:0.0013897751923650503 max memory_allocated 22561.05419921875 
[2025-02-19 10:23:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.32863718271255493 norm:0.0013280402636155486 max memory_allocated 22561.05419921875 
[2025-02-19 10:24:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.32805588841438293 norm:0.0012541648466140032 max memory_allocated 22561.05419921875 
[2025-02-19 10:24:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.32769590616226196 norm:0.0011867692228406668 max memory_allocated 22561.05419921875 
[2025-02-19 10:25:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.32724687457084656 norm:0.0011524572037160397 max memory_allocated 22561.05419921875 
[2025-02-19 10:26:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.3268977105617523 norm:0.0010826622601598501 max memory_allocated 22561.05419921875 
[2025-02-19 10:26:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.326585978269577 norm:0.0010314193787053227 max memory_allocated 22561.05419921875 
[2025-02-19 10:27:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.3262714743614197 norm:0.0009838358964771032 max memory_allocated 22561.05419921875 
[2025-02-19 10:27:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-19 10:27:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.4337225556373596 norm:0.021201148629188538 max memory_allocated 22561.22607421875 
[2025-02-19 10:28:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.40762996673583984 norm:0.01174026820808649 max memory_allocated 22561.22607421875 
[2025-02-19 10:28:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.3748787045478821 norm:0.005100085400044918 max memory_allocated 22561.22607421875 
[2025-02-19 10:29:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.35996100306510925 norm:0.0022679080720990896 max memory_allocated 22561.22607421875 
[2025-02-19 10:29:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.35402369499206543 norm:0.0016186065040528774 max memory_allocated 22561.22607421875 
[2025-02-19 10:30:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.3507020175457001 norm:0.0014237507712095976 max memory_allocated 22561.22607421875 
[2025-02-19 10:30:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.3483288586139679 norm:0.001291418680921197 max memory_allocated 22561.22607421875 
[2025-02-19 10:31:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.346418172121048 norm:0.0012103533372282982 max memory_allocated 22561.22607421875 
[2025-02-19 10:31:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.3451828360557556 norm:0.001201513921841979 max memory_allocated 22561.22607421875 
[2025-02-19 10:32:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.3442447781562805 norm:0.0011748997494578362 max memory_allocated 22561.22607421875 
[2025-02-19 10:32:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.3430827856063843 norm:0.0011397477937862277 max memory_allocated 22561.22607421875 
[2025-02-19 10:33:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.3424370586872101 norm:0.001133846235461533 max memory_allocated 22561.22607421875 
[2025-02-19 10:33:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.3418814241886139 norm:0.0011158282868564129 max memory_allocated 22561.22607421875 
[2025-02-19 10:34:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.3414555490016937 norm:0.001100777881219983 max memory_allocated 22561.22607421875 
[2025-02-19 10:34:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.34085604548454285 norm:0.0010350426891818643 max memory_allocated 22561.22607421875 
[2025-02-19 10:35:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.3405400812625885 norm:0.0010047550313174725 max memory_allocated 22561.22607421875 
[2025-02-19 10:35:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.3401986360549927 norm:0.000983488280326128 max memory_allocated 22561.22607421875 
[2025-02-19 10:36:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.3400436341762543 norm:0.000941514503210783 max memory_allocated 22561.22607421875 
[2025-02-19 10:36:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.3398824632167816 norm:0.0009499830193817616 max memory_allocated 22561.22607421875 
[2025-02-19 10:37:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.3395923972129822 norm:0.0009159728651866317 max memory_allocated 22561.22607421875 
[2025-02-19 10:37:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-19 10:38:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.48633062839508057 norm:0.05503446236252785 max memory_allocated 22561.39794921875 
[2025-02-19 10:38:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.4571528434753418 norm:0.030541734769940376 max memory_allocated 22561.39794921875 
[2025-02-19 10:39:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.41108939051628113 norm:0.012572706677019596 max memory_allocated 22561.39794921875 
[2025-02-19 10:39:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.39059874415397644 norm:0.0066270241513848305 max memory_allocated 22561.39794921875 
[2025-02-19 10:40:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.38271161913871765 norm:0.005002870690077543 max memory_allocated 22561.39794921875 
[2025-02-19 10:40:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.3782448470592499 norm:0.004283070098608732 max memory_allocated 22561.39794921875 
[2025-02-19 10:41:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.37501251697540283 norm:0.00366904865950346 max memory_allocated 22561.39794921875 
[2025-02-19 10:41:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.3724116384983063 norm:0.003048981074243784 max memory_allocated 22561.39794921875 
[2025-02-19 10:42:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.3704153597354889 norm:0.002733297646045685 max memory_allocated 22561.39794921875 
[2025-02-19 10:42:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.36883315443992615 norm:0.0023932280018925667 max memory_allocated 22561.39794921875 
[2025-02-19 10:43:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.36743199825286865 norm:0.0020504086278378963 max memory_allocated 22561.39794921875 
[2025-02-19 10:43:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.36651167273521423 norm:0.0018940541194751859 max memory_allocated 22561.39794921875 
[2025-02-19 10:44:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.3656666576862335 norm:0.0017258834559470415 max memory_allocated 22561.39794921875 
[2025-02-19 10:44:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.3649892508983612 norm:0.0015792807098478079 max memory_allocated 22561.39794921875 
[2025-02-19 10:45:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.3643885850906372 norm:0.001463159453123808 max memory_allocated 22561.39794921875 
[2025-02-19 10:45:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.3640202283859253 norm:0.0014249051455408335 max memory_allocated 22561.39794921875 
[2025-02-19 10:46:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.36382603645324707 norm:0.0013719748239964247 max memory_allocated 22561.39794921875 
[2025-02-19 10:46:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.36342114210128784 norm:0.0013052697759121656 max memory_allocated 22561.39794921875 
[2025-02-19 10:47:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.36312541365623474 norm:0.0012404131703078747 max memory_allocated 22561.39794921875 
[2025-02-19 10:47:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.36270463466644287 norm:0.001203508349135518 max memory_allocated 22561.39794921875 
[2025-02-19 10:47:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-19 10:48:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.48143237829208374 norm:0.03464684262871742 max memory_allocated 22561.56982421875 
[2025-02-19 10:49:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.45819365978240967 norm:0.018520278856158257 max memory_allocated 22561.56982421875 
[2025-02-19 10:49:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.42872676253318787 norm:0.007745443843305111 max memory_allocated 22561.56982421875 
[2025-02-19 10:50:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.41237497329711914 norm:0.003963346593081951 max memory_allocated 22561.56982421875 
[2025-02-19 10:50:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.4057437777519226 norm:0.0024784652050584555 max memory_allocated 22561.56982421875 
[2025-02-19 10:51:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.40159744024276733 norm:0.0019056538585573435 max memory_allocated 22561.56982421875 
[2025-02-19 10:51:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.3987899422645569 norm:0.0018267066916450858 max memory_allocated 22561.56982421875 
[2025-02-19 10:52:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.3967318832874298 norm:0.0017579692648723722 max memory_allocated 22561.56982421875 
[2025-02-19 10:52:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.39484965801239014 norm:0.0012332898331806064 max memory_allocated 22561.56982421875 
[2025-02-19 10:53:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.393678218126297 norm:0.0011591122020035982 max memory_allocated 22561.56982421875 
[2025-02-19 10:53:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.39264345169067383 norm:0.0011285223299637437 max memory_allocated 22561.56982421875 
[2025-02-19 10:54:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.3919348120689392 norm:0.001111338846385479 max memory_allocated 22561.56982421875 
[2025-02-19 10:54:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.391458123922348 norm:0.0011165692703798413 max memory_allocated 22561.56982421875 
[2025-02-19 10:55:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.39106428623199463 norm:0.001089103170670569 max memory_allocated 22561.56982421875 
[2025-02-19 10:55:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.39069968461990356 norm:0.001065705670043826 max memory_allocated 22561.56982421875 
[2025-02-19 10:56:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.3903326392173767 norm:0.0010579988593235612 max memory_allocated 22561.56982421875 
[2025-02-19 10:56:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.3899874687194824 norm:0.0010596619686111808 max memory_allocated 22561.56982421875 
[2025-02-19 10:57:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.3895432949066162 norm:0.0010205284925177693 max memory_allocated 22561.56982421875 
[2025-02-19 10:57:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.38923102617263794 norm:0.0010110449511557817 max memory_allocated 22561.56982421875 
[2025-02-19 10:58:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.38895511627197266 norm:0.0009897913550958037 max memory_allocated 22561.56982421875 
[2025-02-19 10:58:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-19 10:58:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.5505265593528748 norm:0.04061273857951164 max memory_allocated 22561.74169921875 
[2025-02-19 10:59:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.5181544423103333 norm:0.021923618391156197 max memory_allocated 22561.74169921875 
[2025-02-19 10:59:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.4748936593532562 norm:0.009707599878311157 max memory_allocated 22561.74169921875 
[2025-02-19 11:00:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.4533776640892029 norm:0.005178466439247131 max memory_allocated 22561.74169921875 
[2025-02-19 11:00:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.44545978307724 norm:0.0032477709464728832 max memory_allocated 22561.74169921875 
[2025-02-19 11:01:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.4407629370689392 norm:0.0024392050690948963 max memory_allocated 22561.74169921875 
[2025-02-19 11:01:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.4373757541179657 norm:0.0021366337314248085 max memory_allocated 22561.74169921875 
[2025-02-19 11:02:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.435404896736145 norm:0.002048882655799389 max memory_allocated 22561.74169921875 
[2025-02-19 11:02:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.4336475729942322 norm:0.0018983617192134261 max memory_allocated 22561.74169921875 
[2025-02-19 11:03:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.4322293698787689 norm:0.0017648916691541672 max memory_allocated 22561.74169921875 
[2025-02-19 11:03:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.43075886368751526 norm:0.0015988197410479188 max memory_allocated 22561.74169921875 
[2025-02-19 11:04:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.42978084087371826 norm:0.0015213418519124389 max memory_allocated 22561.74169921875 
[2025-02-19 11:05:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.4289773404598236 norm:0.0014477348886430264 max memory_allocated 22561.74169921875 
[2025-02-19 11:05:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.4283182621002197 norm:0.001446891576051712 max memory_allocated 22561.74169921875 
[2025-02-19 11:06:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.4274067282676697 norm:0.0011754240840673447 max memory_allocated 22561.74169921875 
[2025-02-19 11:06:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.4270145297050476 norm:0.0011827012058347464 max memory_allocated 22561.74169921875 
[2025-02-19 11:07:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.4265859127044678 norm:0.0011846373090520501 max memory_allocated 22561.74169921875 
[2025-02-19 11:07:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.4262517988681793 norm:0.0011688554659485817 max memory_allocated 22561.74169921875 
[2025-02-19 11:08:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.42590704560279846 norm:0.0011446030111983418 max memory_allocated 22561.74169921875 
[2025-02-19 11:08:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.42570969462394714 norm:0.0011240245075896382 max memory_allocated 22561.74169921875 
[2025-02-19 11:08:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-19 11:09:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.5816088914871216 norm:0.03630902245640755 max memory_allocated 22561.91357421875 
[2025-02-19 11:09:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.5529303550720215 norm:0.01937413029372692 max memory_allocated 22561.91357421875 
[2025-02-19 11:10:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.5191089510917664 norm:0.008398214355111122 max memory_allocated 22561.91357421875 
[2025-02-19 11:10:47 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.50113445520401 norm:0.0037523447535932064 max memory_allocated 22561.91357421875 
[2025-02-19 11:11:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.49380308389663696 norm:0.00237432518042624 max memory_allocated 22561.91357421875 
[2025-02-19 11:11:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.48962166905403137 norm:0.001904441975057125 max memory_allocated 22561.91357421875 
[2025-02-19 11:12:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.48686420917510986 norm:0.0017828426789492369 max memory_allocated 22561.91357421875 
[2025-02-19 11:12:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.4847395420074463 norm:0.0016792583046481013 max memory_allocated 22561.91357421875 
[2025-02-19 11:13:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.4829574525356293 norm:0.0015637356555089355 max memory_allocated 22561.91357421875 
[2025-02-19 11:13:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.4816727042198181 norm:0.0014923510607331991 max memory_allocated 22561.91357421875 
[2025-02-19 11:14:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.4807453453540802 norm:0.0014697624137625098 max memory_allocated 22561.91357421875 
[2025-02-19 11:14:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.479989230632782 norm:0.0014242290053516626 max memory_allocated 22561.91357421875 
[2025-02-19 11:15:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.47912997007369995 norm:0.0013636377407237887 max memory_allocated 22561.91357421875 
[2025-02-19 11:15:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.47848138213157654 norm:0.0013424103381112218 max memory_allocated 22561.91357421875 
[2025-02-19 11:16:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.477633535861969 norm:0.0012538526207208633 max memory_allocated 22561.91357421875 
[2025-02-19 11:16:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.4771735370159149 norm:0.0012340773828327656 max memory_allocated 22561.91357421875 
[2025-02-19 11:17:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.476849228143692 norm:0.001208156580105424 max memory_allocated 22561.91357421875 
[2025-02-19 11:17:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.4764823615550995 norm:0.001196454162709415 max memory_allocated 22561.91357421875 
[2025-02-19 11:18:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.4761984646320343 norm:0.0011869234731420875 max memory_allocated 22561.91357421875 
[2025-02-19 11:18:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.47602948546409607 norm:0.0011945848818868399 max memory_allocated 22561.91357421875 
[2025-02-19 11:19:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-19 11:19:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.6402764320373535 norm:0.03705723583698273 max memory_allocated 22562.08544921875 
[2025-02-19 11:20:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.6113643646240234 norm:0.018541626632213593 max memory_allocated 22562.08544921875 
[2025-02-19 11:20:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.578532338142395 norm:0.008367869071662426 max memory_allocated 22562.08544921875 
[2025-02-19 11:21:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.5635618567466736 norm:0.004719652235507965 max memory_allocated 22562.08544921875 
[2025-02-19 11:21:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.5570231676101685 norm:0.0032994768116623163 max memory_allocated 22562.08544921875 
[2025-02-19 11:22:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.5534965991973877 norm:0.0029686877969652414 max memory_allocated 22562.08544921875 
[2025-02-19 11:22:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.5510384440422058 norm:0.002768870210275054 max memory_allocated 22562.08544921875 
[2025-02-19 11:23:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.5490590929985046 norm:0.0024789408780634403 max memory_allocated 22562.08544921875 
[2025-02-19 11:23:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.5475143790245056 norm:0.002243924653157592 max memory_allocated 22562.08544921875 
[2025-02-19 11:24:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.5461971759796143 norm:0.0021041918080300093 max memory_allocated 22562.08544921875 
[2025-02-19 11:24:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.5451949834823608 norm:0.001924775424413383 max memory_allocated 22562.08544921875 
[2025-02-19 11:25:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.5442230701446533 norm:0.0017370071727782488 max memory_allocated 22562.08544921875 
[2025-02-19 11:25:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.5432618260383606 norm:0.0011662079486995935 max memory_allocated 22562.08544921875 
[2025-02-19 11:26:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.5428208708763123 norm:0.0011535828234627843 max memory_allocated 22562.08544921875 
[2025-02-19 11:26:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.5423124432563782 norm:0.0011348387924954295 max memory_allocated 22562.08544921875 
[2025-02-19 11:27:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.5419793725013733 norm:0.0011450025485828519 max memory_allocated 22562.08544921875 
[2025-02-19 11:27:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.5416833162307739 norm:0.0011347344843670726 max memory_allocated 22562.08544921875 
[2025-02-19 11:28:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.5412425398826599 norm:0.0011331640416756272 max memory_allocated 22562.08544921875 
[2025-02-19 11:28:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.5407651662826538 norm:0.0011365092359483242 max memory_allocated 22562.08544921875 
[2025-02-19 11:29:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.5404762029647827 norm:0.0011242039036005735 max memory_allocated 22562.08544921875 
[2025-02-19 11:29:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-19 11:30:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.7047393321990967 norm:0.027383269742131233 max memory_allocated 22562.25732421875 
[2025-02-19 11:30:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.681756854057312 norm:0.014919563196599483 max memory_allocated 22562.25732421875 
[2025-02-19 11:31:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.6572228670120239 norm:0.008307215757668018 max memory_allocated 22562.25732421875 
[2025-02-19 11:31:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.6452142000198364 norm:0.005500822328031063 max memory_allocated 22562.25732421875 
[2025-02-19 11:32:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.6403837203979492 norm:0.0043862382881343365 max memory_allocated 22562.25732421875 
[2025-02-19 11:32:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.6371051073074341 norm:0.0035912711173295975 max memory_allocated 22562.25732421875 
[2025-02-19 11:33:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.6344193816184998 norm:0.0030378822702914476 max memory_allocated 22562.25732421875 
[2025-02-19 11:33:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.6321255564689636 norm:0.002566830487921834 max memory_allocated 22562.25732421875 
[2025-02-19 11:34:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.6303045153617859 norm:0.002215900458395481 max memory_allocated 22562.25732421875 
[2025-02-19 11:34:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.6291223764419556 norm:0.002015845850110054 max memory_allocated 22562.25732421875 
[2025-02-19 11:35:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.6278924942016602 norm:0.001790307927876711 max memory_allocated 22562.25732421875 
[2025-02-19 11:35:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.6266624927520752 norm:0.0015798270469531417 max memory_allocated 22562.25732421875 
[2025-02-19 11:36:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.6254209876060486 norm:0.001110693789087236 max memory_allocated 22562.25732421875 
[2025-02-19 11:36:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.6250200271606445 norm:0.0011288912501186132 max memory_allocated 22562.25732421875 
[2025-02-19 11:37:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.6247334480285645 norm:0.0011197641724720597 max memory_allocated 22562.25732421875 
[2025-02-19 11:37:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.6243111491203308 norm:0.0010871908161789179 max memory_allocated 22562.25732421875 
[2025-02-19 11:38:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.6239939332008362 norm:0.0010967102134600282 max memory_allocated 22562.25732421875 
[2025-02-19 11:38:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.6237689256668091 norm:0.0010871058329939842 max memory_allocated 22562.25732421875 
[2025-02-19 11:39:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.6234260201454163 norm:0.0010991381714120507 max memory_allocated 22562.25732421875 
[2025-02-19 11:39:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.6230934858322144 norm:0.0010897949105128646 max memory_allocated 22562.25732421875 
[2025-02-19 11:39:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-19 11:40:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.8307187557220459 norm:0.02918368950486183 max memory_allocated 22562.42919921875 
[2025-02-19 11:40:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.8060832023620605 norm:0.01647740602493286 max memory_allocated 22562.42919921875 
[2025-02-19 11:41:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.7766610383987427 norm:0.0089995376765728 max memory_allocated 22562.42919921875 
[2025-02-19 11:41:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.7605117559432983 norm:0.005610321648418903 max memory_allocated 22562.42919921875 
[2025-02-19 11:42:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.7551642656326294 norm:0.004434352740645409 max memory_allocated 22562.42919921875 
[2025-02-19 11:42:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.7512935996055603 norm:0.0037175831384956837 max memory_allocated 22562.42919921875 
[2025-02-19 11:43:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.7487296462059021 norm:0.0032125955913215876 max memory_allocated 22562.42919921875 
[2025-02-19 11:43:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.7467518448829651 norm:0.0028833020478487015 max memory_allocated 22562.42919921875 
[2025-02-19 11:44:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.7443724274635315 norm:0.0020122325513511896 max memory_allocated 22562.42919921875 
[2025-02-19 11:44:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.7427377104759216 norm:0.0014707220252603292 max memory_allocated 22562.42919921875 
[2025-02-19 11:45:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.7416873574256897 norm:0.0014351443387567997 max memory_allocated 22562.42919921875 
[2025-02-19 11:46:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.7409483194351196 norm:0.0014245184138417244 max memory_allocated 22562.42919921875 
[2025-02-19 11:46:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.7403290867805481 norm:0.0013929515844210982 max memory_allocated 22562.42919921875 
[2025-02-19 11:47:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.7398310303688049 norm:0.001360270194709301 max memory_allocated 22562.42919921875 
[2025-02-19 11:47:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.7394610047340393 norm:0.0013455884763970971 max memory_allocated 22562.42919921875 
[2025-02-19 11:48:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.7392327785491943 norm:0.0013580957893282175 max memory_allocated 22562.42919921875 
[2025-02-19 11:48:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.7390888333320618 norm:0.0013405380304902792 max memory_allocated 22562.42919921875 
[2025-02-19 11:49:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.7388893365859985 norm:0.0013160555390641093 max memory_allocated 22562.42919921875 
[2025-02-19 11:49:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.7388011813163757 norm:0.0013371821260079741 max memory_allocated 22562.42919921875 
[2025-02-19 11:50:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.7387630939483643 norm:0.0013294430682435632 max memory_allocated 22562.42919921875 
[2025-02-19 11:50:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-19 11:50:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.9402186870574951 norm:0.011721691116690636 max memory_allocated 22562.60107421875 
[2025-02-19 11:51:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.9268198013305664 norm:0.00720646046102047 max memory_allocated 22562.60107421875 
[2025-02-19 11:51:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.9044740796089172 norm:0.0036594243720173836 max memory_allocated 22562.60107421875 
[2025-02-19 11:52:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.8927233815193176 norm:0.002671652939170599 max memory_allocated 22562.60107421875 
[2025-02-19 11:52:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.8865758180618286 norm:0.0019106055842712522 max memory_allocated 22562.60107421875 
[2025-02-19 11:53:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.8835436105728149 norm:0.0017508245073258877 max memory_allocated 22562.60107421875 
[2025-02-19 11:53:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.8814234137535095 norm:0.0016885672230273485 max memory_allocated 22562.60107421875 
[2025-02-19 11:54:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.8798582553863525 norm:0.0016718709375709295 max memory_allocated 22562.60107421875 
[2025-02-19 11:54:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.8785731792449951 norm:0.001633435022085905 max memory_allocated 22562.60107421875 
[2025-02-19 11:55:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.8777223825454712 norm:0.0016171783208847046 max memory_allocated 22562.60107421875 
[2025-02-19 11:55:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.8768700361251831 norm:0.001618906739167869 max memory_allocated 22562.60107421875 
[2025-02-19 11:56:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.8763169646263123 norm:0.0016044361982494593 max memory_allocated 22562.60107421875 
[2025-02-19 11:56:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.8759269714355469 norm:0.001588067738339305 max memory_allocated 22562.60107421875 
[2025-02-19 11:57:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.8755442500114441 norm:0.0015591451665386558 max memory_allocated 22562.60107421875 
[2025-02-19 11:57:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.8750828504562378 norm:0.0015393616631627083 max memory_allocated 22562.60107421875 
[2025-02-19 11:58:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.8749488592147827 norm:0.0015340012032538652 max memory_allocated 22562.60107421875 
[2025-02-19 11:58:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.8744019865989685 norm:0.001525571569800377 max memory_allocated 22562.60107421875 
[2025-02-19 11:59:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.8742862343788147 norm:0.0014519492397084832 max memory_allocated 22562.60107421875 
[2025-02-19 11:59:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.8737972974777222 norm:0.0014880463713780046 max memory_allocated 22562.60107421875 
[2025-02-19 12:00:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.8735008239746094 norm:0.0014964743750169873 max memory_allocated 22562.60107421875 
[2025-02-19 12:00:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-19 12:01:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:1.0780532360076904 norm:0.01190780196338892 max memory_allocated 22562.77294921875 
[2025-02-19 12:01:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:1.0587007999420166 norm:0.007065983954817057 max memory_allocated 22562.77294921875 
[2025-02-19 12:02:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:1.035977840423584 norm:0.0038046243134886026 max memory_allocated 22562.77294921875 
[2025-02-19 12:02:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:1.0267910957336426 norm:0.0026583808939903975 max memory_allocated 22562.77294921875 
[2025-02-19 12:03:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:1.022025227546692 norm:0.001984633505344391 max memory_allocated 22562.77294921875 
[2025-02-19 12:03:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:1.0186612606048584 norm:0.0017326563829556108 max memory_allocated 22562.77294921875 
[2025-02-19 12:04:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:1.0160552263259888 norm:0.0015514056431129575 max memory_allocated 22562.77294921875 
[2025-02-19 12:04:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:1.0139808654785156 norm:0.0013460342306643724 max memory_allocated 22562.77294921875 
[2025-02-19 12:05:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:1.0126094818115234 norm:0.0013212182093411684 max memory_allocated 22562.77294921875 
[2025-02-19 12:05:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:1.0116069316864014 norm:0.0012967422371730208 max memory_allocated 22562.77294921875 
[2025-02-19 12:06:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:1.0109127759933472 norm:0.0012727809371426702 max memory_allocated 22562.77294921875 
[2025-02-19 12:06:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:1.0104031562805176 norm:0.001261094119399786 max memory_allocated 22562.77294921875 
[2025-02-19 12:07:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:1.0100092887878418 norm:0.0012508126674219966 max memory_allocated 22562.77294921875 
[2025-02-19 12:07:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:1.009757161140442 norm:0.0012414382072165608 max memory_allocated 22562.77294921875 
[2025-02-19 12:08:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:1.0095491409301758 norm:0.001232992741279304 max memory_allocated 22562.77294921875 
[2025-02-19 12:08:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:1.0092324018478394 norm:0.0012248728889971972 max memory_allocated 22562.77294921875 
[2025-02-19 12:09:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:1.0090324878692627 norm:0.0012158389436081052 max memory_allocated 22562.77294921875 
[2025-02-19 12:09:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:1.0088571310043335 norm:0.0012077410938218236 max memory_allocated 22562.77294921875 
[2025-02-19 12:10:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:1.0086249113082886 norm:0.0012082969769835472 max memory_allocated 22562.77294921875 
[2025-02-19 12:10:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:1.0085269212722778 norm:0.001203480176627636 max memory_allocated 22562.77294921875 
[2025-02-19 12:11:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-19 12:11:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:1.2630465030670166 norm:0.021274032071232796 max memory_allocated 22562.94482421875 
[2025-02-19 12:12:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:1.239640712738037 norm:0.013769706711173058 max memory_allocated 22562.94482421875 
[2025-02-19 12:12:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:1.2115453481674194 norm:0.0072766696102917194 max memory_allocated 22562.94482421875 
[2025-02-19 12:13:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:1.195784091949463 norm:0.003276732750236988 max memory_allocated 22562.94482421875 
[2025-02-19 12:13:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:1.1892122030258179 norm:0.0026223903987556696 max memory_allocated 22562.94482421875 
[2025-02-19 12:14:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:1.184578776359558 norm:0.0019233369966968894 max memory_allocated 22562.94482421875 
[2025-02-19 12:14:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:1.180977702140808 norm:0.0016058085020631552 max memory_allocated 22562.94482421875 
[2025-02-19 12:15:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:1.1789215803146362 norm:0.001548628555610776 max memory_allocated 22562.94482421875 
[2025-02-19 12:15:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:1.177609920501709 norm:0.0014960343251004815 max memory_allocated 22562.94482421875 
[2025-02-19 12:16:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:1.1764799356460571 norm:0.0014791518915444613 max memory_allocated 22562.94482421875 
[2025-02-19 12:16:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:1.175734281539917 norm:0.0014565972378477454 max memory_allocated 22562.94482421875 
[2025-02-19 12:17:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:1.175196886062622 norm:0.0014411023585125804 max memory_allocated 22562.94482421875 
[2025-02-19 12:17:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:1.1747095584869385 norm:0.001429141964763403 max memory_allocated 22562.94482421875 
[2025-02-19 12:18:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:1.1741859912872314 norm:0.0014201314188539982 max memory_allocated 22562.94482421875 
[2025-02-19 12:18:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:1.1738394498825073 norm:0.0014163678279146552 max memory_allocated 22562.94482421875 
[2025-02-19 12:19:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:1.1736667156219482 norm:0.0014158051926642656 max memory_allocated 22562.94482421875 
[2025-02-19 12:19:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:1.1734473705291748 norm:0.0014056647196412086 max memory_allocated 22562.94482421875 
[2025-02-19 12:20:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:1.1731719970703125 norm:0.0013951289001852274 max memory_allocated 22562.94482421875 
[2025-02-19 12:20:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:1.172978401184082 norm:0.0013907549437135458 max memory_allocated 22562.94482421875 
[2025-02-19 12:21:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:1.1727864742279053 norm:0.0013843468623235822 max memory_allocated 22562.94482421875 
[2025-02-19 12:21:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-19 12:21:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:1.4248132705688477 norm:0.03023260459303856 max memory_allocated 22563.11669921875 
[2025-02-19 12:22:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:1.4047088623046875 norm:0.020709101110696793 max memory_allocated 22563.11669921875 
[2025-02-19 12:22:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:1.378058671951294 norm:0.013706774450838566 max memory_allocated 22563.11669921875 
[2025-02-19 12:23:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:1.3647156953811646 norm:0.010074005462229252 max memory_allocated 22563.11669921875 
[2025-02-19 12:23:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:1.3567031621932983 norm:0.007539776153862476 max memory_allocated 22563.11669921875 
[2025-02-19 12:24:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:1.3507072925567627 norm:0.0060426099225878716 max memory_allocated 22563.11669921875 
[2025-02-19 12:25:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:1.34681236743927 norm:0.0052552795968949795 max memory_allocated 22563.11669921875 
[2025-02-19 12:25:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:1.3443037271499634 norm:0.004662097431719303 max memory_allocated 22563.11669921875 
[2025-02-19 12:26:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:1.3424416780471802 norm:0.004183127079159021 max memory_allocated 22563.11669921875 
[2025-02-19 12:26:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:1.3409390449523926 norm:0.0037384997121989727 max memory_allocated 22563.11669921875 
[2025-02-19 12:27:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:1.33970046043396 norm:0.003415793878957629 max memory_allocated 22563.11669921875 
[2025-02-19 12:27:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:1.338568925857544 norm:0.0031163168605417013 max memory_allocated 22563.11669921875 
[2025-02-19 12:28:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:1.3376274108886719 norm:0.002868486801162362 max memory_allocated 22563.11669921875 
[2025-02-19 12:28:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:1.3368136882781982 norm:0.0026351825799793005 max memory_allocated 22563.11669921875 
[2025-02-19 12:29:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:1.3361637592315674 norm:0.002446342259645462 max memory_allocated 22563.11669921875 
[2025-02-19 12:29:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:1.335637092590332 norm:0.0022865738719701767 max memory_allocated 22563.11669921875 
[2025-02-19 12:30:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:1.334791660308838 norm:0.002025736030191183 max memory_allocated 22563.11669921875 
[2025-02-19 12:30:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:1.3342663049697876 norm:0.0019119482021778822 max memory_allocated 22563.11669921875 
[2025-02-19 12:31:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:1.333819031715393 norm:0.0018370504258200526 max memory_allocated 22563.11669921875 
[2025-02-19 12:31:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:1.333598256111145 norm:0.0017789523117244244 max memory_allocated 22563.11669921875 
[2025-02-19 12:31:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-19 12:32:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:1.597806692123413 norm:0.009659795090556145 max memory_allocated 22563.28857421875 
[2025-02-19 12:32:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:1.5783541202545166 norm:0.005666421260684729 max memory_allocated 22563.28857421875 
[2025-02-19 12:33:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:1.551934838294983 norm:0.00296194339171052 max memory_allocated 22563.28857421875 
[2025-02-19 12:33:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:1.5403002500534058 norm:0.0018065930344164371 max memory_allocated 22563.28857421875 
[2025-02-19 12:34:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:1.534921646118164 norm:0.0015300686936825514 max memory_allocated 22563.28857421875 
[2025-02-19 12:34:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:1.5307315587997437 norm:0.001400061184540391 max memory_allocated 22563.28857421875 
[2025-02-19 12:35:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:1.5277481079101562 norm:0.0013462495990097523 max memory_allocated 22563.28857421875 
[2025-02-19 12:35:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:1.5256786346435547 norm:0.001303817960433662 max memory_allocated 22563.28857421875 
[2025-02-19 12:36:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:1.524131417274475 norm:0.0012748510343953967 max memory_allocated 22563.28857421875 
[2025-02-19 12:36:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:1.5229811668395996 norm:0.001254031085409224 max memory_allocated 22563.28857421875 
[2025-02-19 12:37:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:1.5220893621444702 norm:0.001237576361745596 max memory_allocated 22563.28857421875 
[2025-02-19 12:37:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:1.5213919878005981 norm:0.0012278408976271749 max memory_allocated 22563.28857421875 
[2025-02-19 12:38:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:1.5208401679992676 norm:0.0012248574057593942 max memory_allocated 22563.28857421875 
[2025-02-19 12:38:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:1.5204367637634277 norm:0.0012197920586913824 max memory_allocated 22563.28857421875 
[2025-02-19 12:39:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:1.5200347900390625 norm:0.0012117831502109766 max memory_allocated 22563.28857421875 
[2025-02-19 12:39:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:1.519580364227295 norm:0.001203659106977284 max memory_allocated 22563.28857421875 
[2025-02-19 12:40:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:1.5191948413848877 norm:0.0011995668755844235 max memory_allocated 22563.28857421875 
[2025-02-19 12:41:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:1.5189486742019653 norm:0.0012085456401109695 max memory_allocated 22563.28857421875 
[2025-02-19 12:41:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:1.5186123847961426 norm:0.001200658269226551 max memory_allocated 22563.28857421875 
[2025-02-19 12:42:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:1.5183252096176147 norm:0.0011909008026123047 max memory_allocated 22563.28857421875 
[2025-02-19 12:42:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-19 12:42:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:1.8170357942581177 norm:0.02711523324251175 max memory_allocated 22563.46044921875 
[2025-02-19 12:43:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:1.7877349853515625 norm:0.014010382816195488 max memory_allocated 22563.46044921875 
[2025-02-19 12:43:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:1.7557648420333862 norm:0.006794832646846771 max memory_allocated 22563.46044921875 
[2025-02-19 12:44:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:1.740828037261963 norm:0.0037129162810742855 max memory_allocated 22563.46044921875 
[2025-02-19 12:44:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:1.733052372932434 norm:0.0028674276545643806 max memory_allocated 22563.46044921875 
[2025-02-19 12:45:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:1.7274508476257324 norm:0.0023248065263032913 max memory_allocated 22563.46044921875 
[2025-02-19 12:45:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:1.723591923713684 norm:0.0019760115537792444 max memory_allocated 22563.46044921875 
[2025-02-19 12:46:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:1.720942497253418 norm:0.0017528939060866833 max memory_allocated 22563.46044921875 
[2025-02-19 12:46:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:1.7191247940063477 norm:0.001601498108357191 max memory_allocated 22563.46044921875 
[2025-02-19 12:47:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:1.7177013158798218 norm:0.001486090011894703 max memory_allocated 22563.46044921875 
[2025-02-19 12:47:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:1.7167778015136719 norm:0.0014094841899350286 max memory_allocated 22563.46044921875 
[2025-02-19 12:48:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:1.7160357236862183 norm:0.001364376163110137 max memory_allocated 22563.46044921875 
[2025-02-19 12:48:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:1.7153654098510742 norm:0.0013198720989748836 max memory_allocated 22563.46044921875 
[2025-02-19 12:49:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:1.7147915363311768 norm:0.0012902001617476344 max memory_allocated 22563.46044921875 
[2025-02-19 12:49:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:1.7143089771270752 norm:0.0012626572279259562 max memory_allocated 22563.46044921875 
[2025-02-19 12:50:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:1.7138065099716187 norm:0.001242808299139142 max memory_allocated 22563.46044921875 
[2025-02-19 12:50:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:1.7133225202560425 norm:0.0012334708590060472 max memory_allocated 22563.46044921875 
[2025-02-19 12:51:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:1.7129905223846436 norm:0.001219728379510343 max memory_allocated 22563.46044921875 
[2025-02-19 12:51:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:1.7127540111541748 norm:0.0012150872498750687 max memory_allocated 22563.46044921875 
[2025-02-19 12:52:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:1.7125375270843506 norm:0.0012105741770938039 max memory_allocated 22563.46044921875 
[2025-02-19 12:52:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-19 12:53:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:2.023636817932129 norm:0.023241860792040825 max memory_allocated 22563.63232421875 
[2025-02-19 12:53:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:1.9897819757461548 norm:0.010520507581532001 max memory_allocated 22563.63232421875 
[2025-02-19 12:54:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:1.9573274850845337 norm:0.006281816866248846 max memory_allocated 22563.63232421875 
[2025-02-19 12:54:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:1.9396347999572754 norm:0.0027657162863761187 max memory_allocated 22563.63232421875 
[2025-02-19 12:55:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:1.9322794675827026 norm:0.0021441886201500893 max memory_allocated 22563.63232421875 
[2025-02-19 12:55:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:1.927005410194397 norm:0.0018915556138381362 max memory_allocated 22563.63232421875 
[2025-02-19 12:56:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:1.9230358600616455 norm:0.0017169879283756018 max memory_allocated 22563.63232421875 
[2025-02-19 12:56:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:1.9206581115722656 norm:0.0015677637420594692 max memory_allocated 22563.63232421875 
[2025-02-19 12:57:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:1.9190618991851807 norm:0.0014931082259863615 max memory_allocated 22563.63232421875 
[2025-02-19 12:57:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:1.9178857803344727 norm:0.0014583530137315392 max memory_allocated 22563.63232421875 
[2025-02-19 12:58:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:1.9168736934661865 norm:0.0014336673775687814 max memory_allocated 22563.63232421875 
[2025-02-19 12:58:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:1.9160832166671753 norm:0.0013961123768240213 max memory_allocated 22563.63232421875 
[2025-02-19 12:59:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:1.9153214693069458 norm:0.0013941498473286629 max memory_allocated 22563.63232421875 
[2025-02-19 12:59:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:1.9141571521759033 norm:0.0013769190991297364 max memory_allocated 22563.63232421875 
[2025-02-19 13:00:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:1.9143041372299194 norm:0.0013512842124328017 max memory_allocated 22563.63232421875 
[2025-02-19 13:00:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:1.9138565063476562 norm:0.0013231512857601047 max memory_allocated 22563.63232421875 
[2025-02-19 13:01:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:1.9130884408950806 norm:0.001325089717283845 max memory_allocated 22563.63232421875 
[2025-02-19 13:01:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:1.9129198789596558 norm:0.0013020785991102457 max memory_allocated 22563.63232421875 
[2025-02-19 13:02:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:1.9122741222381592 norm:0.0012885420583188534 max memory_allocated 22563.63232421875 
[2025-02-19 13:02:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:1.911750316619873 norm:0.001297814305871725 max memory_allocated 22563.63232421875 
[2025-02-19 13:03:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-19 13:03:04 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:03:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:2.3186044692993164 norm:0.05251786485314369 max memory_allocated 22563.91943359375 
[2025-02-19 13:04:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:2.2866580486297607 norm:0.04700261354446411 max memory_allocated 22563.91943359375 
[2025-02-19 13:04:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:2.2509102821350098 norm:0.03706507384777069 max memory_allocated 22563.91943359375 
[2025-02-19 13:05:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:2.2307348251342773 norm:0.031162064522504807 max memory_allocated 22563.91943359375 
[2025-02-19 13:05:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:2.2197840213775635 norm:0.026812870055437088 max memory_allocated 22563.91943359375 
[2025-02-19 13:06:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:2.211402416229248 norm:0.02407851442694664 max memory_allocated 22563.91943359375 
[2025-02-19 13:06:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:2.2044448852539062 norm:0.021048272028565407 max memory_allocated 22563.91943359375 
[2025-02-19 13:07:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:2.1992886066436768 norm:0.01885056309401989 max memory_allocated 22563.91943359375 
[2025-02-19 13:07:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:2.1948206424713135 norm:0.017602724954485893 max memory_allocated 22563.91943359375 
[2025-02-19 13:08:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:2.191305637359619 norm:0.016929425299167633 max memory_allocated 22563.91943359375 
[2025-02-19 13:08:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:2.187917709350586 norm:0.01637420989573002 max memory_allocated 22563.91943359375 
[2025-02-19 13:09:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:2.1851062774658203 norm:0.015955397859215736 max memory_allocated 22563.91943359375 
[2025-02-19 13:09:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:2.1825873851776123 norm:0.015839654952287674 max memory_allocated 22563.91943359375 
[2025-02-19 13:10:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:2.1804544925689697 norm:0.015319387428462505 max memory_allocated 22563.91943359375 
[2025-02-19 13:10:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:2.1781904697418213 norm:0.015242006629705429 max memory_allocated 22563.91943359375 
[2025-02-19 13:11:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:2.176689863204956 norm:0.015153793618083 max memory_allocated 22563.91943359375 
[2025-02-19 13:11:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:2.1746246814727783 norm:0.014611512422561646 max memory_allocated 22563.91943359375 
[2025-02-19 13:12:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:2.1732301712036133 norm:0.014129790477454662 max memory_allocated 22563.91943359375 
[2025-02-19 13:12:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:2.1717047691345215 norm:0.0138231897726655 max memory_allocated 22563.91943359375 
[2025-02-19 13:13:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:2.170470952987671 norm:0.013409420847892761 max memory_allocated 22563.91943359375 
[2025-02-19 13:13:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-19 13:13:29 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:14:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:2.6256814002990723 norm:0.05174306407570839 max memory_allocated 22564.09130859375 
[2025-02-19 13:14:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:2.5819756984710693 norm:0.04361160099506378 max memory_allocated 22564.09130859375 
[2025-02-19 13:15:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:2.5327582359313965 norm:0.03330715000629425 max memory_allocated 22564.09130859375 
[2025-02-19 13:15:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:2.5066142082214355 norm:0.02743143029510975 max memory_allocated 22564.09130859375 
[2025-02-19 13:16:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:2.493565082550049 norm:0.023033082485198975 max memory_allocated 22564.09130859375 
[2025-02-19 13:16:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:2.484776496887207 norm:0.019348464906215668 max memory_allocated 22564.09130859375 
[2025-02-19 13:17:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:2.4786407947540283 norm:0.016793547198176384 max memory_allocated 22564.09130859375 
[2025-02-19 13:17:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:2.47436785697937 norm:0.014826769009232521 max memory_allocated 22564.09130859375 
[2025-02-19 13:18:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:2.471501588821411 norm:0.013988670893013477 max memory_allocated 22564.09130859375 
[2025-02-19 13:18:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:2.469484329223633 norm:0.013921303674578667 max memory_allocated 22564.09130859375 
[2025-02-19 13:19:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:2.467820167541504 norm:0.013540693558752537 max memory_allocated 22564.09130859375 
[2025-02-19 13:19:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:2.466461181640625 norm:0.013775822706520557 max memory_allocated 22564.09130859375 
[2025-02-19 13:20:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:2.464843988418579 norm:0.013060920871794224 max memory_allocated 22564.09130859375 
[2025-02-19 13:20:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:2.463527202606201 norm:0.012515573762357235 max memory_allocated 22564.09130859375 
[2025-02-19 13:21:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:2.4623560905456543 norm:0.012180007062852383 max memory_allocated 22564.09130859375 
[2025-02-19 13:21:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:2.4616153240203857 norm:0.012032700702548027 max memory_allocated 22564.09130859375 
[2025-02-19 13:22:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:2.4608099460601807 norm:0.011936192400753498 max memory_allocated 22564.09130859375 
[2025-02-19 13:22:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:2.460111618041992 norm:0.011891558766365051 max memory_allocated 22564.09130859375 
[2025-02-19 13:23:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:2.459667205810547 norm:0.01162143237888813 max memory_allocated 22564.09130859375 
[2025-02-19 13:23:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:2.4593498706817627 norm:0.011931246146559715 max memory_allocated 22564.09130859375 
[2025-02-19 13:23:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-19 13:23:55 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:24:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:3.459787130355835 norm:0.10264003276824951 max memory_allocated 22564.26318359375 
[2025-02-19 13:24:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:3.3656725883483887 norm:0.07202963531017303 max memory_allocated 22564.26318359375 
[2025-02-19 13:25:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:3.215324878692627 norm:0.042189087718725204 max memory_allocated 22564.26318359375 
[2025-02-19 13:25:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:3.1465649604797363 norm:0.033634815365076065 max memory_allocated 22564.26318359375 
[2025-02-19 13:26:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:3.1222102642059326 norm:0.03489702194929123 max memory_allocated 22564.26318359375 
[2025-02-19 13:26:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:3.1009678840637207 norm:0.03317409008741379 max memory_allocated 22564.26318359375 
[2025-02-19 13:27:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:3.086656093597412 norm:0.0350467748939991 max memory_allocated 22564.26318359375 
[2025-02-19 13:28:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:3.0786354541778564 norm:0.03703394532203674 max memory_allocated 22564.26318359375 
[2025-02-19 13:28:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:3.0640931129455566 norm:0.03549353405833244 max memory_allocated 22564.26318359375 
[2025-02-19 13:29:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:3.0571482181549072 norm:0.034096330404281616 max memory_allocated 22564.26318359375 
[2025-02-19 13:29:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:3.055391788482666 norm:0.03775492310523987 max memory_allocated 22564.26318359375 
[2025-02-19 13:30:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:3.042818784713745 norm:0.036610718816518784 max memory_allocated 22564.26318359375 
[2025-02-19 13:30:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:3.040139675140381 norm:0.03633250296115875 max memory_allocated 22564.26318359375 
[2025-02-19 13:31:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:3.037661075592041 norm:0.03656487539410591 max memory_allocated 22564.26318359375 
[2025-02-19 13:31:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:3.040170431137085 norm:0.039771564304828644 max memory_allocated 22564.26318359375 
[2025-02-19 13:32:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:3.0310325622558594 norm:0.039338283240795135 max memory_allocated 22564.26318359375 
[2025-02-19 13:32:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:3.0288379192352295 norm:0.039363838732242584 max memory_allocated 22564.26318359375 
[2025-02-19 13:33:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:3.0290980339050293 norm:0.038564376533031464 max memory_allocated 22564.26318359375 
[2025-02-19 13:33:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:3.02949595451355 norm:0.03974760323762894 max memory_allocated 22564.26318359375 
[2025-02-19 13:34:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:3.026068687438965 norm:0.03885860741138458 max memory_allocated 22564.26318359375 
[2025-02-19 13:34:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-19 13:34:20 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:34:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:6.4945807456970215 norm:0.4204912781715393 max memory_allocated 22564.43505859375 
[2025-02-19 13:35:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:6.0096025466918945 norm:0.3329230546951294 max memory_allocated 22564.43505859375 
[2025-02-19 13:35:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:5.684645175933838 norm:0.2810361087322235 max memory_allocated 22564.43505859375 
[2025-02-19 13:36:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:5.486944198608398 norm:0.27767279744148254 max memory_allocated 22564.43505859375 
[2025-02-19 13:36:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:5.3828301429748535 norm:0.25486835837364197 max memory_allocated 22564.43505859375 
[2025-02-19 13:37:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:5.314882278442383 norm:0.23932680487632751 max memory_allocated 22564.43505859375 
[2025-02-19 13:37:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:5.260315895080566 norm:0.2288198173046112 max memory_allocated 22564.43505859375 
[2025-02-19 13:38:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:5.2178449630737305 norm:0.22225767374038696 max memory_allocated 22564.43505859375 
[2025-02-19 13:38:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:5.173161506652832 norm:0.21286268532276154 max memory_allocated 22564.43505859375 
[2025-02-19 13:39:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:5.1390061378479 norm:0.19891422986984253 max memory_allocated 22564.43505859375 
[2025-02-19 13:39:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:5.113479137420654 norm:0.18723827600479126 max memory_allocated 22564.43505859375 
[2025-02-19 13:40:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:5.09275484085083 norm:0.18286685645580292 max memory_allocated 22564.43505859375 
[2025-02-19 13:41:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:5.07211971282959 norm:0.17475289106369019 max memory_allocated 22564.43505859375 
[2025-02-19 13:41:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:5.0561347007751465 norm:0.16959986090660095 max memory_allocated 22564.43505859375 
[2025-02-19 13:42:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:5.045673370361328 norm:0.16698411107063293 max memory_allocated 22564.43505859375 
[2025-02-19 13:42:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:5.034554958343506 norm:0.1652650535106659 max memory_allocated 22564.43505859375 
[2025-02-19 13:43:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:5.025943756103516 norm:0.1663476675748825 max memory_allocated 22564.43505859375 
[2025-02-19 13:43:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:5.018967151641846 norm:0.16816389560699463 max memory_allocated 22564.43505859375 
[2025-02-19 13:44:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:5.007440567016602 norm:0.16127118468284607 max memory_allocated 22564.43505859375 
[2025-02-19 13:44:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:5.0039963722229 norm:0.16081541776657104 max memory_allocated 22564.43505859375 
[2025-02-19 13:44:43 root] (main_calibration.py 365): INFO 19970.66911506653
[2025-02-19 13:45:27 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-19 13:46:31 root] (main_calibration.py 158): INFO wikitext2 : 8.289040565490723
[2025-02-19 13:46:31 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-19 13:48:08 root] (main_calibration.py 158): INFO c4 : 12.134790420532227
[2025-02-19 15:25:15 root] (main_calibration.py 169): INFO {'wikitext2': 8.289040565490723, 'c4': 12.134790420532227, 'results': {'winogrande': {'acc': 0.5564325177584846, 'acc_stderr': 0.013962694907620397}, 'arc_easy': {'acc': 0.5509259259259259, 'acc_stderr': 0.010206428316323363, 'acc_norm': 0.4692760942760943, 'acc_norm_stderr': 0.01024039558481524}, 'hellaswag': {'acc': 0.4565823541127266, 'acc_stderr': 0.004970933420231929, 'acc_norm': 0.5931089424417447, 'acc_norm_stderr': 0.0049025025147385985}, 'arc_challenge': {'acc': 0.3122866894197952, 'acc_stderr': 0.013542598541688065, 'acc_norm': 0.3506825938566553, 'acc_norm_stderr': 0.013944635930726085}, 'piqa': {'acc': 0.6958650707290533, 'acc_stderr': 0.010733493335721319, 'acc_norm': 0.7007616974972797, 'acc_norm_stderr': 0.010684130673134581}, 'boolq': {'acc': 0.6486238532110091, 'acc_stderr': 0.008349781976603158}}, 'versions': {'winogrande': 0, 'arc_easy': 0, 'hellaswag': 0, 'arc_challenge': 0, 'piqa': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
