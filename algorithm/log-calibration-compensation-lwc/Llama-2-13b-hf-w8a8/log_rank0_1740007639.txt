[2025-02-19 23:27:19 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-13b-hf-w8a8', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-19 23:27:22 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-19 23:27:22 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-19 23:27:22 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-19 23:27:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-19 23:27:35 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 23:28:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.002001421060413122 norm:0.004160563461482525 max memory_allocated 29271.02001953125 
[2025-02-19 23:29:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0010454244911670685 norm:0.0011411660816520452 max memory_allocated 29271.02001953125 
[2025-02-19 23:30:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0008683375199325383 norm:0.0025449409149587154 max memory_allocated 29271.02001953125 
[2025-02-19 23:30:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0007578171789646149 norm:0.0021990740206092596 max memory_allocated 29271.02001953125 
[2025-02-19 23:31:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0006799064576625824 norm:0.0019805938936769962 max memory_allocated 29271.02001953125 
[2025-02-19 23:32:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0006163222133181989 norm:0.0017304378561675549 max memory_allocated 29271.02001953125 
[2025-02-19 23:33:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0005820440128445625 norm:0.0016276491805911064 max memory_allocated 29271.02001953125 
[2025-02-19 23:34:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0005655219429172575 norm:0.0015500657027587295 max memory_allocated 29271.02001953125 
[2025-02-19 23:34:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0005222600884735584 norm:0.0013458331814035773 max memory_allocated 29271.02001953125 
[2025-02-19 23:35:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.000508730998262763 norm:0.0012861017603427172 max memory_allocated 29271.02001953125 
[2025-02-19 23:36:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0004955149488523602 norm:0.0012348899617791176 max memory_allocated 29271.02001953125 
[2025-02-19 23:37:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0004809899546671659 norm:0.0011305897496640682 max memory_allocated 29271.02001953125 
[2025-02-19 23:38:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0004693950468208641 norm:0.0010356067214161158 max memory_allocated 29271.02001953125 
[2025-02-19 23:38:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.00046119786566123366 norm:0.000981912249699235 max memory_allocated 29271.02001953125 
[2025-02-19 23:39:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.00044945828267373145 norm:0.0009085636120289564 max memory_allocated 29271.02001953125 
[2025-02-19 23:40:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0004463261866476387 norm:0.0008807277190499008 max memory_allocated 29271.02001953125 
[2025-02-19 23:41:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00044162999256514013 norm:0.0008471618057228625 max memory_allocated 29271.02001953125 
[2025-02-19 23:42:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0004368955851532519 norm:0.0007828943198546767 max memory_allocated 29271.02001953125 
[2025-02-19 23:43:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00042930181371048093 norm:0.0007117560598999262 max memory_allocated 29271.02001953125 
[2025-02-19 23:43:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.00042757426854223013 norm:0.0006875682156533003 max memory_allocated 29271.02001953125 
[2025-02-19 23:44:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-19 23:44:14 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 23:45:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.002235243795439601 norm:0.0027042250148952007 max memory_allocated 29271.02001953125 
[2025-02-19 23:45:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0015142566990107298 norm:0.0011542135616764426 max memory_allocated 29271.02001953125 
[2025-02-19 23:46:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.001345146680250764 norm:0.0011809548595920205 max memory_allocated 29271.02001953125 
[2025-02-19 23:47:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0012404825538396835 norm:0.0010166163556277752 max memory_allocated 29271.02001953125 
[2025-02-19 23:48:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0011724996147677302 norm:0.0009495888371020555 max memory_allocated 29271.02001953125 
[2025-02-19 23:49:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0011263531632721424 norm:0.0008688923553563654 max memory_allocated 29271.02001953125 
[2025-02-19 23:49:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0010937224142253399 norm:0.0008396738558076322 max memory_allocated 29271.02001953125 
[2025-02-19 23:50:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0010702114086598158 norm:0.0007905650418251753 max memory_allocated 29271.02001953125 
[2025-02-19 23:51:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0010514265159144998 norm:0.0007545023108832538 max memory_allocated 29271.02001953125 
[2025-02-19 23:52:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0010379402665421367 norm:0.0007546751294285059 max memory_allocated 29271.02001953125 
[2025-02-19 23:53:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0010261444840580225 norm:0.0007167515577748418 max memory_allocated 29271.02001953125 
[2025-02-19 23:54:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0010196780785918236 norm:0.0007226974703371525 max memory_allocated 29271.02001953125 
[2025-02-19 23:54:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0010111090959981084 norm:0.0007011176785454154 max memory_allocated 29271.02001953125 
[2025-02-19 23:55:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0010039338376373053 norm:0.0006457701092585921 max memory_allocated 29271.02001953125 
[2025-02-19 23:56:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0009993617422878742 norm:0.0006212068255990744 max memory_allocated 29271.02001953125 
[2025-02-19 23:57:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0009954737033694983 norm:0.000610115472227335 max memory_allocated 29271.02001953125 
[2025-02-19 23:58:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0009925467893481255 norm:0.0005840116646140814 max memory_allocated 29271.02001953125 
[2025-02-19 23:58:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0009904422331601381 norm:0.0005629369406960905 max memory_allocated 29271.02001953125 
[2025-02-19 23:59:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0009920872980728745 norm:0.0005390455480664968 max memory_allocated 29271.02001953125 
[2025-02-20 00:00:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.000989928375929594 norm:0.000495909946039319 max memory_allocated 29271.02001953125 
[2025-02-20 00:00:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-20 00:00:53 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 00:01:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0024911058135330677 norm:0.002431110944598913 max memory_allocated 29271.02001953125 
[2025-02-20 00:02:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0018034287495538592 norm:0.0013725198805332184 max memory_allocated 29271.02001953125 
[2025-02-20 00:03:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0016161862295120955 norm:0.001501352177001536 max memory_allocated 29271.02001953125 
[2025-02-20 00:04:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0014701494947075844 norm:0.0012163983192294836 max memory_allocated 29271.02001953125 
[2025-02-20 00:04:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0013853973941877484 norm:0.0010758007410913706 max memory_allocated 29271.02001953125 
[2025-02-20 00:05:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0013351446250453591 norm:0.0010553481988608837 max memory_allocated 29271.02001953125 
[2025-02-20 00:06:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0013014720752835274 norm:0.001001575030386448 max memory_allocated 29271.02001953125 
[2025-02-20 00:07:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0012792360503226519 norm:0.0009587174281477928 max memory_allocated 29271.02001953125 
[2025-02-20 00:08:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.001262346631847322 norm:0.0009197503095492721 max memory_allocated 29271.02001953125 
[2025-02-20 00:09:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0012507090577855706 norm:0.000889635062776506 max memory_allocated 29271.02001953125 
[2025-02-20 00:09:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.001238923636265099 norm:0.0008332850411534309 max memory_allocated 29271.02001953125 
[2025-02-20 00:10:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0012276222696527839 norm:0.0007802741602063179 max memory_allocated 29271.02001953125 
[2025-02-20 00:11:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0012199286138638854 norm:0.0007337608840316534 max memory_allocated 29271.02001953125 
[2025-02-20 00:12:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0012099158484488726 norm:0.000665030034724623 max memory_allocated 29271.02001953125 
[2025-02-20 00:13:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0012019954156130552 norm:0.0006085415952838957 max memory_allocated 29271.02001953125 
[2025-02-20 00:13:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0011954469373449683 norm:0.0005609866930171847 max memory_allocated 29271.02001953125 
[2025-02-20 00:14:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0011890942696481943 norm:0.0005058324313722551 max memory_allocated 29271.02001953125 
[2025-02-20 00:15:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0011833792086690664 norm:0.00045093963854014874 max memory_allocated 29271.02001953125 
[2025-02-20 00:16:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0011785917449742556 norm:0.0004065103712491691 max memory_allocated 29271.02001953125 
[2025-02-20 00:17:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0012325134593993425 norm:0.0005250646499916911 max memory_allocated 29271.02001953125 
[2025-02-20 00:17:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-20 00:18:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.002319167833775282 norm:0.000758373353164643 max memory_allocated 29271.43798828125 
[2025-02-20 00:19:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0018740387167781591 norm:0.0003718715743161738 max memory_allocated 29271.43798828125 
[2025-02-20 00:20:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0016942934598773718 norm:0.00025826218188740313 max memory_allocated 29271.43798828125 
[2025-02-20 00:20:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0015855649253353477 norm:0.00021353524061851203 max memory_allocated 29271.43798828125 
[2025-02-20 00:21:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.0015205172821879387 norm:0.00018400537373963743 max memory_allocated 29271.43798828125 
[2025-02-20 00:22:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.001466602087020874 norm:0.0001549532316857949 max memory_allocated 29271.43798828125 
[2025-02-20 00:23:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.001430588774383068 norm:0.00013187424337957054 max memory_allocated 29271.43798828125 
[2025-02-20 00:24:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0014076242223381996 norm:0.0001365798234473914 max memory_allocated 29271.43798828125 
[2025-02-20 00:24:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0013913430739194155 norm:0.00013075824244879186 max memory_allocated 29271.43798828125 
[2025-02-20 00:25:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0013780074659734964 norm:0.00012996955774724483 max memory_allocated 29271.43798828125 
[2025-02-20 00:26:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0013681341661140323 norm:0.0001163700144388713 max memory_allocated 29271.43798828125 
[2025-02-20 00:27:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0013628415763378143 norm:0.00011274542339378968 max memory_allocated 29271.43798828125 
[2025-02-20 00:28:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0013593549374490976 norm:0.00010671609925338998 max memory_allocated 29271.43798828125 
[2025-02-20 00:28:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0013543125241994858 norm:0.00010054506128653884 max memory_allocated 29271.43798828125 
[2025-02-20 00:29:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0013499651104211807 norm:9.567415690980852e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:30:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0013475373852998018 norm:8.517055539414287e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:31:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0013452128041535616 norm:8.717673335922882e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:32:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0013434098800644279 norm:0.00010669673793017864 max memory_allocated 29271.43798828125 
[2025-02-20 00:32:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0013389047235250473 norm:0.00010280146670993418 max memory_allocated 29271.43798828125 
[2025-02-20 00:33:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0013375482521951199 norm:0.0001141871907748282 max memory_allocated 29271.43798828125 
[2025-02-20 00:34:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-20 00:34:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.001724408706650138 norm:0.00011663787154247984 max memory_allocated 29271.43798828125 
[2025-02-20 00:35:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0015589825343340635 norm:6.315944483503699e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:36:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0014919675886631012 norm:4.5104941818863153e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:37:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0014548222534358501 norm:3.656728222267702e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:38:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0014344690134748816 norm:3.136586747132242e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:39:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0014194699469953775 norm:2.7924876121687703e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:39:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0014070284087210894 norm:2.5750179702299647e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:40:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0013990828301757574 norm:2.4909317289711908e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:41:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0013939300552010536 norm:2.3932794647407718e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:42:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0013902710052207112 norm:2.4645038138260134e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:43:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0013865710934624076 norm:2.1915086108492687e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:43:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0013839846942573786 norm:2.427175968477968e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:44:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0013813780387863517 norm:2.342637162655592e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:45:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0013794844271615148 norm:2.1093577743158676e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:46:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0013768791686743498 norm:2.26729334826814e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:47:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0013749378267675638 norm:2.1933661628281698e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:47:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0013744569150730968 norm:2.242810296593234e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:48:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0013724718010053039 norm:2.2057807655073702e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:49:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0013726018369197845 norm:2.1096866476000287e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:50:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0013716130051761866 norm:2.333094016648829e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:50:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-20 00:51:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.00179892813321203 norm:0.0001183890417451039 max memory_allocated 29271.43798828125 
[2025-02-20 00:52:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0016474872827529907 norm:5.146753755980171e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:53:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0015852947253733873 norm:3.824082887149416e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:54:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0015513916732743382 norm:3.138685133308172e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:54:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0015300726518034935 norm:2.6090921892318875e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:55:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0015169986290857196 norm:2.3037337086861953e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:56:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.001507035456597805 norm:2.148618659703061e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:57:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0014992954675108194 norm:2.0045319615746848e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:58:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0014944297727197409 norm:1.9367704226169735e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:58:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.001488539739511907 norm:1.781093487807084e-05 max memory_allocated 29271.43798828125 
[2025-02-20 00:59:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.001484665903262794 norm:1.7717124137561768e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:00:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0014816061593592167 norm:1.7494890926172957e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:01:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0014784487430006266 norm:1.7591341020306572e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:02:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.001476246165111661 norm:1.5994881323422305e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:02:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0014739518519490957 norm:1.4901022950652987e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:03:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0014711181866005063 norm:1.4860006558592431e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:04:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.001469550421461463 norm:1.5269612049451098e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:05:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0014673495898023248 norm:1.4939350876375102e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:06:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0014671708922833204 norm:1.4812721019552555e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:07:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0014650049852207303 norm:1.4135546734905802e-05 max memory_allocated 29271.43798828125 
[2025-02-20 01:07:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-20 01:08:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.001918431487865746 norm:0.00015508715296164155 max memory_allocated 29272.00048828125 
[2025-02-20 01:09:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0017431374872103333 norm:6.887810013722628e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:09:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.001672051614150405 norm:4.539807196124457e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:10:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0016324648167937994 norm:3.440359796513803e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:11:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0016093169106170535 norm:2.7683223379426636e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:12:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0015938306460157037 norm:2.4821143597364426e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:13:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0015828560572117567 norm:2.2075953893363476e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:13:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0015751724131405354 norm:2.03315939870663e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:14:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0015673915622755885 norm:1.8086029740516096e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:15:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0015627553220838308 norm:1.7981052224058658e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:16:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.001560622826218605 norm:1.7750668121152557e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:17:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0015570094110444188 norm:1.7225596820935607e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:18:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.001555415685288608 norm:1.7020687664626166e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:18:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0015527853975072503 norm:1.5414945664815605e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:19:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0015477159759029746 norm:1.60867548402166e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:20:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0015509077347815037 norm:1.5711619198555127e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:21:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0015482392627745867 norm:1.54509034473449e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:22:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0015460738213732839 norm:1.4891969840391539e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:22:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0015451236395165324 norm:1.4252580513129942e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:23:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0015440700808539987 norm:1.5057446944410913e-05 max memory_allocated 29272.00048828125 
[2025-02-20 01:23:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-20 01:25:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0019540039356797934 norm:0.00010055277380160987 max memory_allocated 29272.18798828125 
[2025-02-20 01:25:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.001812789123505354 norm:5.0003967771772295e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:26:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.0017548792529851198 norm:3.455502519500442e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:27:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0017215792322531343 norm:2.765276440186426e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:28:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0016984683461487293 norm:2.3300532120629214e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:29:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.001682884176261723 norm:2.1199683033046313e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:29:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0016726210014894605 norm:1.9910945411538705e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:30:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.001666044699959457 norm:1.8110151358996518e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:31:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.001661013811826706 norm:1.6572232198086567e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:32:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0016579556977376342 norm:1.6333300663973205e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:33:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0016539645148441195 norm:1.6594616681686603e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:33:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.001651839236728847 norm:1.6688689356669784e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:34:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0016488989349454641 norm:1.5477929991902784e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:35:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0016462246421724558 norm:1.5002542568254285e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:36:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0016440588515251875 norm:1.3630063222080935e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:37:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0016417618608102202 norm:1.3823153494740836e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:38:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0016404377529397607 norm:1.3863851563655771e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:38:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0016392238903790712 norm:1.358525150862988e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:39:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0016377497231587768 norm:1.3915092495153658e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:40:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0016371917445212603 norm:1.2838057955377735e-05 max memory_allocated 29272.18798828125 
[2025-02-20 01:40:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-20 01:41:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0021363680716603994 norm:0.00021312551689334214 max memory_allocated 29272.37548828125 
[2025-02-20 01:42:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0019318736158311367 norm:9.525749192107469e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:43:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.001849725260399282 norm:6.0112535720691085e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:44:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.001804143888875842 norm:4.43957578681875e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:45:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0017745968652889132 norm:3.441509761614725e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:45:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0017555818194523454 norm:2.92466447717743e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:46:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0017424230463802814 norm:2.4410566766164266e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:47:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0017321272753179073 norm:2.133552698069252e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:48:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0017241861205548048 norm:1.9291694115963764e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:49:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0017181806033477187 norm:1.8133347111870535e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:49:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0017129435436800122 norm:1.5595323930028826e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:50:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0017094484064728022 norm:1.511025766376406e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:51:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0017079015960916877 norm:1.444912686565658e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:52:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0017050503520295024 norm:1.342162795481272e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:53:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0017030586022883654 norm:1.3894151379645336e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:53:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0017006476409733295 norm:1.3328846762306057e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:54:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.001699455315247178 norm:1.2875379979959689e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:55:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0016977108316496015 norm:1.2381205124256667e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:56:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0016968061681836843 norm:1.2377388884488028e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:57:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0016963721718639135 norm:1.210670598084107e-05 max memory_allocated 29272.37548828125 
[2025-02-20 01:57:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-20 01:58:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002088125329464674 norm:7.221912528621033e-05 max memory_allocated 29272.56298828125 
[2025-02-20 01:59:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0019660175312310457 norm:3.664234827738255e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:00:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0019106765976175666 norm:2.8011327231070027e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:00:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0018772367620840669 norm:2.244265670015011e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:01:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0018562055192887783 norm:2.0141966160736047e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:02:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0018431596690788865 norm:1.704641545074992e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:03:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0018326765857636929 norm:1.543108737678267e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:04:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0018244646489620209 norm:1.4437319805438165e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:04:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0018191609997302294 norm:1.3545984984375536e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:05:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0018154860008507967 norm:1.3725742974202149e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:06:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.00181049935054034 norm:1.3217306332080625e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:07:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0018079597502946854 norm:1.318224076385377e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:08:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.001805848442018032 norm:1.2008382327621803e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:09:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0018052541418001056 norm:1.4174654097587336e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:09:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.001803102670237422 norm:1.4123119399300776e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:10:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0018010736675933003 norm:1.2291468010516837e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:11:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0018000260461121798 norm:1.2327162039582618e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:12:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0018001720309257507 norm:1.2212993169669062e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:13:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0017975762020796537 norm:1.2177740245533641e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:13:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0017976898234337568 norm:1.3533733181247953e-05 max memory_allocated 29272.56298828125 
[2025-02-20 02:14:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-20 02:15:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0021510946098715067 norm:6.580065382877365e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:16:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.002037604572251439 norm:3.411348006920889e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:16:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0019880712497979403 norm:2.4272263544844463e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:17:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0019608712755143642 norm:1.982345202122815e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:18:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0019414410926401615 norm:1.6945934476098046e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:19:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0019277868559584022 norm:1.4377509614860173e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:20:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0019205044955015182 norm:1.2643768059206195e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:20:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.001915448228828609 norm:1.2334575330896769e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:21:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.001910315128043294 norm:1.1514045581861865e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:22:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0019068594556301832 norm:1.0748262866400182e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:23:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0019052136922255158 norm:1.0692510841181502e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:24:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0019030378898605704 norm:1.0922481123998296e-05 max memory_allocated 29272.75048828125 
[2025-02-20 02:24:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0019008740782737732 norm:9.946358659362886e-06 max memory_allocated 29272.75048828125 
[2025-02-20 02:25:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0018990059616044164 norm:9.571476766723208e-06 max memory_allocated 29272.75048828125 
[2025-02-20 02:26:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.001897218287922442 norm:9.49412878981093e-06 max memory_allocated 29272.75048828125 
[2025-02-20 02:27:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0018962444737553596 norm:9.365801815874875e-06 max memory_allocated 29272.75048828125 
[2025-02-20 02:28:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0018945737974718213 norm:9.44205021369271e-06 max memory_allocated 29272.75048828125 
[2025-02-20 02:28:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0018946782220155 norm:8.862170943757519e-06 max memory_allocated 29272.75048828125 
[2025-02-20 02:29:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0018934111576527357 norm:9.077419235836715e-06 max memory_allocated 29272.75048828125 
[2025-02-20 02:30:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0018928616773337126 norm:9.955228961189277e-06 max memory_allocated 29272.75048828125 
[2025-02-20 02:30:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-20 02:31:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0022644000127911568 norm:7.429137622239068e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:32:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0021703082602471113 norm:3.3954522223211825e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:33:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.002128630643710494 norm:2.3611353753949516e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:34:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0021037952974438667 norm:1.9437489754636772e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:35:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.002090755384415388 norm:1.7473601474193856e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:35:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002080240985378623 norm:1.6100746506708674e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:36:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.002071523806080222 norm:1.4947171621315647e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:37:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0020663810428231955 norm:1.4185424333845731e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:38:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.002062183804810047 norm:1.3696223504666705e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:39:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0020596664398908615 norm:1.349341073364485e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:39:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0020580573473125696 norm:1.3458652574627195e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:40:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0020546894520521164 norm:1.3222106645116583e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:41:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0020513562485575676 norm:1.2855159184255172e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:42:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0020485343411564827 norm:1.3951571418147068e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:43:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0020470027811825275 norm:1.3044767001701985e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:44:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0020476938225328922 norm:1.328185317106545e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:44:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.002048830036073923 norm:1.392923695675563e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:45:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0020480910316109657 norm:1.3734786080021877e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:46:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0020469133742153645 norm:1.3854926692147274e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:47:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.0020463198889046907 norm:1.3792450772598386e-05 max memory_allocated 29272.93798828125 
[2025-02-20 02:47:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-20 02:48:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0023506730794906616 norm:4.6697063226019964e-05 max memory_allocated 29273.12548828125 
[2025-02-20 02:49:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0022541265934705734 norm:2.4501376174157485e-05 max memory_allocated 29273.12548828125 
[2025-02-20 02:50:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002209107391536236 norm:1.724471258057747e-05 max memory_allocated 29273.12548828125 
[2025-02-20 02:50:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0021835609804838896 norm:1.4550127161783166e-05 max memory_allocated 29273.12548828125 
[2025-02-20 02:51:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0021665184758603573 norm:1.2505195627454668e-05 max memory_allocated 29273.12548828125 
[2025-02-20 02:52:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.002156015019863844 norm:1.0787789506139234e-05 max memory_allocated 29273.12548828125 
[2025-02-20 02:53:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0021469586063176394 norm:9.928172403306235e-06 max memory_allocated 29273.12548828125 
[2025-02-20 02:54:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0021407450549304485 norm:8.979971426015254e-06 max memory_allocated 29273.12548828125 
[2025-02-20 02:54:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.002136340131983161 norm:8.879087545210496e-06 max memory_allocated 29273.12548828125 
[2025-02-20 02:55:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.002132587833330035 norm:8.916525985114276e-06 max memory_allocated 29273.12548828125 
[2025-02-20 02:56:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0021298625506460667 norm:8.694823009136599e-06 max memory_allocated 29273.12548828125 
[2025-02-20 02:57:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0021271223668009043 norm:8.53493475005962e-06 max memory_allocated 29273.12548828125 
[2025-02-20 02:58:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0021250173449516296 norm:8.144676030497067e-06 max memory_allocated 29273.12548828125 
[2025-02-20 02:59:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0021236687898635864 norm:7.731354344286956e-06 max memory_allocated 29273.12548828125 
[2025-02-20 02:59:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002121690195053816 norm:7.324971647904022e-06 max memory_allocated 29273.12548828125 
[2025-02-20 03:00:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.00212082383222878 norm:7.584080776723567e-06 max memory_allocated 29273.12548828125 
[2025-02-20 03:01:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0021187900565564632 norm:7.1551630753674544e-06 max memory_allocated 29273.12548828125 
[2025-02-20 03:02:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0021181809715926647 norm:7.3185319706681184e-06 max memory_allocated 29273.12548828125 
[2025-02-20 03:03:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0021175832953304052 norm:7.274577910720836e-06 max memory_allocated 29273.12548828125 
[2025-02-20 03:03:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0021167618688195944 norm:7.145447398215765e-06 max memory_allocated 29273.12548828125 
[2025-02-20 03:04:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-20 03:05:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.00249110977165401 norm:8.296751911984757e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:06:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.002387581393122673 norm:3.7666795833501965e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:06:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.0023424511309713125 norm:2.4544980988139287e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:07:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.002316155005246401 norm:1.891907049866859e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:08:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.0022978843189775944 norm:1.5655123206670396e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:09:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0022850288078188896 norm:1.3816083082929254e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:10:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.002275441074743867 norm:1.2393219549267087e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:10:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0022688840981572866 norm:1.099433757190127e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:11:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.0022623101249337196 norm:1.0231602573185228e-05 max memory_allocated 29273.31298828125 
[2025-02-20 03:12:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.0022579869255423546 norm:9.844026862992905e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:13:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.00225414102897048 norm:9.06779314391315e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:14:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0022511708084493876 norm:8.873640581441578e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:14:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0022489484399557114 norm:8.225844794651493e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:15:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.002246796851977706 norm:8.47802402859088e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:16:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0022455137223005295 norm:7.916078175185248e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:17:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.0022431504912674427 norm:7.771743185003288e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:18:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.00224215816706419 norm:7.697626642766409e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:19:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0022418976295739412 norm:7.839786121621728e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:19:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.002240746980533004 norm:7.928032573545352e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:20:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.002240198664367199 norm:7.650294719496742e-06 max memory_allocated 29273.31298828125 
[2025-02-20 03:20:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-20 03:21:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.002654409036040306 norm:0.00011301628546789289 max memory_allocated 29273.50048828125 
[2025-02-20 03:22:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.00253856903873384 norm:5.000308374292217e-05 max memory_allocated 29273.50048828125 
[2025-02-20 03:23:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.002484726719558239 norm:2.8063624995411374e-05 max memory_allocated 29273.50048828125 
[2025-02-20 03:24:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0024573809932917356 norm:2.1590170945273712e-05 max memory_allocated 29273.50048828125 
[2025-02-20 03:25:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.0024381536059081554 norm:1.7468533769715577e-05 max memory_allocated 29273.50048828125 
[2025-02-20 03:26:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.002423460129648447 norm:1.4837847629678436e-05 max memory_allocated 29273.50048828125 
[2025-02-20 03:26:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.002413467736914754 norm:1.3051460882707033e-05 max memory_allocated 29273.50048828125 
[2025-02-20 03:27:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.002405450912192464 norm:1.1658410585368983e-05 max memory_allocated 29273.50048828125 
[2025-02-20 03:28:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0023998012766242027 norm:1.0765177648863755e-05 max memory_allocated 29273.50048828125 
[2025-02-20 03:29:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0023948901798576117 norm:9.664642675488722e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:30:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0023910270538181067 norm:9.359729119751137e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:30:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.002388601191341877 norm:9.114648491959088e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:31:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.002385062165558338 norm:8.598601198173128e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:32:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0023838162887841463 norm:8.24536255095154e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:33:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.002381705678999424 norm:7.92347145761596e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:34:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.002380681224167347 norm:8.336726750712842e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:34:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0023784106597304344 norm:7.690939128224272e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:35:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.0023777629248797894 norm:7.742987691017333e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:36:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0023774919100105762 norm:7.471343451470602e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:37:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.002376703079789877 norm:7.601422566949623e-06 max memory_allocated 29273.50048828125 
[2025-02-20 03:37:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-20 03:38:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.0027236950118094683 norm:6.678349745925516e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:39:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.0026212232187390327 norm:3.5443044907879084e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:40:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.002574119484052062 norm:2.4655835659359582e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:41:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.002544649410992861 norm:1.9763327145483345e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:41:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0025260646361857653 norm:1.6058338587754406e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:42:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0025113332085311413 norm:1.3592953109764494e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:43:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0025015075225383043 norm:1.1989530321443453e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:44:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0024933088570833206 norm:1.1168995115440339e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:45:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0024867230094969273 norm:1.0313944585504942e-05 max memory_allocated 29273.68798828125 
[2025-02-20 03:45:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0024818701203912497 norm:9.317344847659115e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:46:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0024782251566648483 norm:9.006558684632182e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:47:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0024744663387537003 norm:8.497102498949971e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:48:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0024723736569285393 norm:8.501480806444306e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:49:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.002470355713739991 norm:7.897771865827963e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:50:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.002468411810696125 norm:7.945248398755211e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:50:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0024669747799634933 norm:7.965430086187553e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:51:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.002465715864673257 norm:7.832517439965159e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:52:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.002465292811393738 norm:7.701379217905924e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:53:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.002464609919115901 norm:7.494482815673109e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:54:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.002462982665747404 norm:7.381088835245464e-06 max memory_allocated 29273.68798828125 
[2025-02-20 03:54:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-20 03:55:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.002938551362603903 norm:0.00010409209062345326 max memory_allocated 29273.87548828125 
[2025-02-20 03:56:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.0028226946014910936 norm:5.1625378546305e-05 max memory_allocated 29273.87548828125 
[2025-02-20 03:57:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0027685617096722126 norm:3.273636684753001e-05 max memory_allocated 29273.87548828125 
[2025-02-20 03:57:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.002737760078161955 norm:2.5677261874079704e-05 max memory_allocated 29273.87548828125 
[2025-02-20 03:58:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0027136695571243763 norm:2.048905662377365e-05 max memory_allocated 29273.87548828125 
[2025-02-20 03:59:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0026977804955095053 norm:1.7396780094713904e-05 max memory_allocated 29273.87548828125 
[2025-02-20 04:00:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.0026852490846067667 norm:1.543010148452595e-05 max memory_allocated 29273.87548828125 
[2025-02-20 04:01:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.002675765659660101 norm:1.4017723515280522e-05 max memory_allocated 29273.87548828125 
[2025-02-20 04:01:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.0026675951667129993 norm:1.2485825209296308e-05 max memory_allocated 29273.87548828125 
[2025-02-20 04:02:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.0026609108317643404 norm:1.1526746675372124e-05 max memory_allocated 29273.87548828125 
[2025-02-20 04:03:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.002656282391399145 norm:1.0741603546193801e-05 max memory_allocated 29273.87548828125 
[2025-02-20 04:04:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.0026519205421209335 norm:1.0162034413951915e-05 max memory_allocated 29273.87548828125 
[2025-02-20 04:05:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0026487153954803944 norm:1.020894524117466e-05 max memory_allocated 29273.87548828125 
[2025-02-20 04:05:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0026457435451447964 norm:9.609841072233394e-06 max memory_allocated 29273.87548828125 
[2025-02-20 04:06:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.002643323503434658 norm:9.065221092896536e-06 max memory_allocated 29273.87548828125 
[2025-02-20 04:07:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.002640981925651431 norm:8.889506716514006e-06 max memory_allocated 29273.87548828125 
[2025-02-20 04:08:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.002639224985614419 norm:8.917472769098822e-06 max memory_allocated 29273.87548828125 
[2025-02-20 04:09:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.002637541852891445 norm:8.197698662115727e-06 max memory_allocated 29273.87548828125 
[2025-02-20 04:10:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0026363132055848837 norm:8.222640644817147e-06 max memory_allocated 29273.87548828125 
[2025-02-20 04:10:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.002635315293446183 norm:8.083209650067147e-06 max memory_allocated 29273.87548828125 
[2025-02-20 04:11:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-20 04:12:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.0029982347041368484 norm:6.822010618634522e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:12:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0029139926191419363 norm:3.521696271491237e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:13:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.002875968348234892 norm:2.5214561901520938e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:14:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.0028524433728307486 norm:2.1046711481176317e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:15:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0028350967913866043 norm:1.7574728190083988e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:16:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.002822266425937414 norm:1.4823115634499118e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:16:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.0028134926687926054 norm:1.3852450138074346e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:17:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0028063782956451178 norm:1.2971657270099968e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:18:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0028001407627016306 norm:1.1492244084365666e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:19:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.002796766348183155 norm:1.1552837349881884e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:20:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.002792424289509654 norm:1.0780909178720322e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:21:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.0027899842243641615 norm:1.0431527698528953e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:21:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.002787380712106824 norm:1.0064171874546446e-05 max memory_allocated 29274.06298828125 
[2025-02-20 04:22:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.0027849781326949596 norm:9.997714187193196e-06 max memory_allocated 29274.06298828125 
[2025-02-20 04:23:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.0027832998894155025 norm:9.589576620783191e-06 max memory_allocated 29274.06298828125 
[2025-02-20 04:24:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.0027816002257168293 norm:9.641778888180852e-06 max memory_allocated 29274.06298828125 
[2025-02-20 04:25:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.00278006074950099 norm:9.225634130416438e-06 max memory_allocated 29274.06298828125 
[2025-02-20 04:25:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.002779326867312193 norm:9.470732038607821e-06 max memory_allocated 29274.06298828125 
[2025-02-20 04:26:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.002777852350845933 norm:9.591209163772874e-06 max memory_allocated 29274.06298828125 
[2025-02-20 04:27:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.00277678738348186 norm:8.992141374619678e-06 max memory_allocated 29274.06298828125 
[2025-02-20 04:27:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-20 04:28:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.003195894183591008 norm:7.401668699458241e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:29:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0030998012516647577 norm:3.63918297807686e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:30:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.003054738510400057 norm:2.4545079213567078e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:31:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.0030265289824455976 norm:1.910220817080699e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:32:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.0030065979808568954 norm:1.566042919876054e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:32:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.002991878893226385 norm:1.371568941976875e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:33:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.002980487886816263 norm:1.2019232599413954e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:34:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.0029720929451286793 norm:1.1186117262695916e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:35:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.002964818850159645 norm:1.024976336339023e-05 max memory_allocated 29274.25048828125 
[2025-02-20 04:36:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.0029592285864055157 norm:9.775716534932144e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:36:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.0029541929252445698 norm:8.906515176931862e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:37:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.002949580317363143 norm:8.63284913066309e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:38:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.0029463875107467175 norm:8.320009328599554e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:39:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.0029433046001940966 norm:7.926581020001322e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:40:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.0029409804847091436 norm:7.958833521115594e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:41:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.0029384680092334747 norm:7.867097338021267e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:41:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.002936077071353793 norm:7.651416126464028e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:42:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.0029341685585677624 norm:7.575871677545365e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:43:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0029328754171729088 norm:7.50685148886987e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:44:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.002931857481598854 norm:7.354452009167289e-06 max memory_allocated 29274.25048828125 
[2025-02-20 04:44:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-20 04:45:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.0034055791329592466 norm:6.464916805271059e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:46:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.0033211768604815006 norm:3.610971907619387e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:47:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.0032791222911328077 norm:2.6477035135030746e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:47:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.003248424269258976 norm:2.0916373614454642e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:48:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.003227926790714264 norm:1.7534237485961057e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:49:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.003210312221199274 norm:1.499032077845186e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:50:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0031960364431142807 norm:1.3329564353625756e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:51:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0031870040111243725 norm:1.2370364856906235e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:51:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0031785299070179462 norm:1.1468061529740226e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:52:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.0031716031953692436 norm:1.0900359484367073e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:53:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.0031657875515520573 norm:1.0134005606232677e-05 max memory_allocated 29274.43798828125 
[2025-02-20 04:54:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.003160839667543769 norm:9.680224138719495e-06 max memory_allocated 29274.43798828125 
[2025-02-20 04:55:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.0031582217197865248 norm:9.165695701085497e-06 max memory_allocated 29274.43798828125 
[2025-02-20 04:56:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.003153995843604207 norm:8.855437954480294e-06 max memory_allocated 29274.43798828125 
[2025-02-20 04:56:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.003151390701532364 norm:8.611876182840206e-06 max memory_allocated 29274.43798828125 
[2025-02-20 04:57:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.0031489720568060875 norm:8.47113460622495e-06 max memory_allocated 29274.43798828125 
[2025-02-20 04:58:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.003145388327538967 norm:8.111314855341334e-06 max memory_allocated 29274.43798828125 
[2025-02-20 04:59:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.0031427815556526184 norm:8.110955604934134e-06 max memory_allocated 29274.43798828125 
[2025-02-20 05:00:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.0031410856172442436 norm:7.801053470757324e-06 max memory_allocated 29274.43798828125 
[2025-02-20 05:00:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0031392835080623627 norm:7.56777944843634e-06 max memory_allocated 29274.43798828125 
[2025-02-20 05:01:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-20 05:02:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.0036251668352633715 norm:6.866565672680736e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:02:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.0035332399420440197 norm:3.466179623501375e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:03:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.003487049136310816 norm:2.4345092242583632e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:04:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.0034573117736727 norm:1.9617231373558752e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:05:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.0034354294184595346 norm:1.670418532739859e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:06:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0034185191616415977 norm:1.4275670764618553e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:06:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.003405567491427064 norm:1.2895938198198564e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:07:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.003394940635189414 norm:1.1746977179427631e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:08:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.0033870378974825144 norm:1.0713565643527545e-05 max memory_allocated 29274.62548828125 
[2025-02-20 05:09:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.0033804536797106266 norm:9.737354048411362e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:10:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.0033754638861864805 norm:9.278192010242492e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:11:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.003370412392541766 norm:9.035665243573021e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:11:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.003365500131621957 norm:8.972003342933021e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:12:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.003362346673384309 norm:8.501577212882694e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:13:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.00335996737703681 norm:8.14376471680589e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:14:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.0033581526950001717 norm:8.010949386516586e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:15:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.003355741733685136 norm:8.150328540068585e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:15:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.003354149404913187 norm:7.851561349525582e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:16:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.0033519272692501545 norm:7.622406428708928e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:17:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.003350527724251151 norm:7.522104624513304e-06 max memory_allocated 29274.62548828125 
[2025-02-20 05:17:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-20 05:18:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.003980622161179781 norm:6.624183879466727e-05 max memory_allocated 29274.81298828125 
[2025-02-20 05:19:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.0038915753830224276 norm:3.797480894718319e-05 max memory_allocated 29274.81298828125 
[2025-02-20 05:20:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.003844986204057932 norm:2.711512024689e-05 max memory_allocated 29274.81298828125 
[2025-02-20 05:21:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.003814694471657276 norm:2.214863707195036e-05 max memory_allocated 29274.81298828125 
[2025-02-20 05:21:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.0037927906960248947 norm:1.897698712127749e-05 max memory_allocated 29274.81298828125 
[2025-02-20 05:22:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.0037764080334454775 norm:1.6534109818167053e-05 max memory_allocated 29274.81298828125 
[2025-02-20 05:23:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.0037626842968165874 norm:1.5026473192847334e-05 max memory_allocated 29274.81298828125 
