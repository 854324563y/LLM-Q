[2025-02-20 07:02:59 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-13b-hf-w8a8', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-20 07:03:03 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-20 07:03:04 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-20 07:03:04 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-20 07:03:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-20 07:03:20 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:04:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.002001421060413122 norm:0.004160563461482525 max memory_allocated 29271.02001953125 
[2025-02-20 07:04:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0010454244911670685 norm:0.0011411660816520452 max memory_allocated 29271.02001953125 
[2025-02-20 07:05:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0008683375199325383 norm:0.0025449409149587154 max memory_allocated 29271.02001953125 
[2025-02-20 07:06:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0007578171789646149 norm:0.0021990740206092596 max memory_allocated 29271.02001953125 
[2025-02-20 07:07:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0006799064576625824 norm:0.0019805938936769962 max memory_allocated 29271.02001953125 
[2025-02-20 07:08:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0006163222133181989 norm:0.0017304378561675549 max memory_allocated 29271.02001953125 
[2025-02-20 07:09:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0005820440128445625 norm:0.0016276491805911064 max memory_allocated 29271.02001953125 
[2025-02-20 07:09:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0005655219429172575 norm:0.0015500657027587295 max memory_allocated 29271.02001953125 
[2025-02-20 07:10:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0005222600884735584 norm:0.0013458331814035773 max memory_allocated 29271.02001953125 
[2025-02-20 07:11:27 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.000508730998262763 norm:0.0012861017603427172 max memory_allocated 29271.02001953125 
[2025-02-20 07:12:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0004955149488523602 norm:0.0012348899617791176 max memory_allocated 29271.02001953125 
[2025-02-20 07:13:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0004809899546671659 norm:0.0011305897496640682 max memory_allocated 29271.02001953125 
[2025-02-20 07:13:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0004693950468208641 norm:0.0010356067214161158 max memory_allocated 29271.02001953125 
[2025-02-20 07:14:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.00046119786566123366 norm:0.000981912249699235 max memory_allocated 29271.02001953125 
[2025-02-20 07:15:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.00044945828267373145 norm:0.0009085636120289564 max memory_allocated 29271.02001953125 
[2025-02-20 07:16:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0004463261866476387 norm:0.0008807277190499008 max memory_allocated 29271.02001953125 
[2025-02-20 07:17:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00044162999256514013 norm:0.0008471618057228625 max memory_allocated 29271.02001953125 
[2025-02-20 07:17:58 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0004368955851532519 norm:0.0007828943198546767 max memory_allocated 29271.02001953125 
[2025-02-20 07:18:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00042930181371048093 norm:0.0007117560598999262 max memory_allocated 29271.02001953125 
[2025-02-20 07:19:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.00042757426854223013 norm:0.0006875682156533003 max memory_allocated 29271.02001953125 
[2025-02-20 07:19:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-20 07:20:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:20:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.002235243795439601 norm:0.0027042250148952007 max memory_allocated 29271.02001953125 
[2025-02-20 07:21:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0015142566990107298 norm:0.0011542135616764426 max memory_allocated 29271.02001953125 
[2025-02-20 07:22:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.001345146680250764 norm:0.0011809548595920205 max memory_allocated 29271.02001953125 
[2025-02-20 07:23:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0012404825538396835 norm:0.0010166163556277752 max memory_allocated 29271.02001953125 
[2025-02-20 07:24:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0011724996147677302 norm:0.0009495888371020555 max memory_allocated 29271.02001953125 
[2025-02-20 07:24:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0011263531632721424 norm:0.0008688923553563654 max memory_allocated 29271.02001953125 
[2025-02-20 07:25:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0010937224142253399 norm:0.0008396738558076322 max memory_allocated 29271.02001953125 
[2025-02-20 07:26:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0010702114086598158 norm:0.0007905650418251753 max memory_allocated 29271.02001953125 
[2025-02-20 07:27:20 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0010514265159144998 norm:0.0007545023108832538 max memory_allocated 29271.02001953125 
[2025-02-20 07:28:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0010379402665421367 norm:0.0007546751294285059 max memory_allocated 29271.02001953125 
[2025-02-20 07:28:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0010261444840580225 norm:0.0007167515577748418 max memory_allocated 29271.02001953125 
[2025-02-20 07:29:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0010196780785918236 norm:0.0007226974703371525 max memory_allocated 29271.02001953125 
[2025-02-20 07:30:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0010111090959981084 norm:0.0007011176785454154 max memory_allocated 29271.02001953125 
[2025-02-20 07:31:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0010039338376373053 norm:0.0006457701092585921 max memory_allocated 29271.02001953125 
[2025-02-20 07:32:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0009993617422878742 norm:0.0006212068255990744 max memory_allocated 29271.02001953125 
[2025-02-20 07:33:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0009954737033694983 norm:0.000610115472227335 max memory_allocated 29271.02001953125 
[2025-02-20 07:33:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0009925467893481255 norm:0.0005840116646140814 max memory_allocated 29271.02001953125 
[2025-02-20 07:34:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0009904422331601381 norm:0.0005629369406960905 max memory_allocated 29271.02001953125 
[2025-02-20 07:35:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0009920872980728745 norm:0.0005390455480664968 max memory_allocated 29271.02001953125 
[2025-02-20 07:36:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.000989928375929594 norm:0.000495909946039319 max memory_allocated 29271.02001953125 
[2025-02-20 07:36:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-20 07:36:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:37:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0024911058135330677 norm:0.002431110944598913 max memory_allocated 29271.39501953125 
[2025-02-20 07:38:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0018034287495538592 norm:0.0013725198805332184 max memory_allocated 29271.39501953125 
[2025-02-20 07:39:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0016161862295120955 norm:0.001501352177001536 max memory_allocated 29271.39501953125 
[2025-02-20 07:39:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0014701494947075844 norm:0.0012163983192294836 max memory_allocated 29271.39501953125 
[2025-02-20 07:40:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0013853973941877484 norm:0.0010758007410913706 max memory_allocated 29271.39501953125 
[2025-02-20 07:41:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0013351446250453591 norm:0.0010553481988608837 max memory_allocated 29271.39501953125 
[2025-02-20 07:42:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0013014720752835274 norm:0.001001575030386448 max memory_allocated 29271.39501953125 
[2025-02-20 07:43:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0012792360503226519 norm:0.0009587174281477928 max memory_allocated 29271.39501953125 
[2025-02-20 07:44:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.001262346631847322 norm:0.0009197503095492721 max memory_allocated 29271.39501953125 
[2025-02-20 07:44:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0012507090577855706 norm:0.000889635062776506 max memory_allocated 29271.39501953125 
[2025-02-20 07:45:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.001238923636265099 norm:0.0008332850411534309 max memory_allocated 29271.39501953125 
[2025-02-20 07:46:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0012276222696527839 norm:0.0007802741602063179 max memory_allocated 29271.39501953125 
[2025-02-20 07:47:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0012199286138638854 norm:0.0007337608840316534 max memory_allocated 29271.39501953125 
[2025-02-20 07:48:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0012099158484488726 norm:0.000665030034724623 max memory_allocated 29271.39501953125 
[2025-02-20 07:48:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0012019954156130552 norm:0.0006085415952838957 max memory_allocated 29271.39501953125 
[2025-02-20 07:49:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0011954469373449683 norm:0.0005609866930171847 max memory_allocated 29271.39501953125 
[2025-02-20 07:50:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0011890942696481943 norm:0.0005058324313722551 max memory_allocated 29271.39501953125 
[2025-02-20 07:51:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0011833792086690664 norm:0.00045093963854014874 max memory_allocated 29271.39501953125 
[2025-02-20 07:52:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0011785917449742556 norm:0.0004065103712491691 max memory_allocated 29271.39501953125 
[2025-02-20 07:53:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0012325134593993425 norm:0.0005250646499916911 max memory_allocated 29271.39501953125 
[2025-02-20 07:53:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-20 07:54:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.002319167833775282 norm:0.000758373353164643 max memory_allocated 29271.39501953125 
[2025-02-20 07:55:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0018740387167781591 norm:0.0003718715743161738 max memory_allocated 29271.39501953125 
[2025-02-20 07:55:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0016942934598773718 norm:0.00025826218188740313 max memory_allocated 29271.39501953125 
[2025-02-20 07:56:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0015855649253353477 norm:0.00021353524061851203 max memory_allocated 29271.39501953125 
[2025-02-20 07:57:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.0015205172821879387 norm:0.00018400537373963743 max memory_allocated 29271.39501953125 
[2025-02-20 07:58:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.001466602087020874 norm:0.0001549532316857949 max memory_allocated 29271.39501953125 
[2025-02-20 07:59:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.001430588774383068 norm:0.00013187424337957054 max memory_allocated 29271.39501953125 
[2025-02-20 07:59:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0014076242223381996 norm:0.0001365798234473914 max memory_allocated 29271.39501953125 
[2025-02-20 08:00:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0013913430739194155 norm:0.00013075824244879186 max memory_allocated 29271.39501953125 
[2025-02-20 08:01:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0013780074659734964 norm:0.00012996955774724483 max memory_allocated 29271.39501953125 
[2025-02-20 08:02:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0013681341661140323 norm:0.0001163700144388713 max memory_allocated 29271.39501953125 
[2025-02-20 08:03:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0013628415763378143 norm:0.00011274542339378968 max memory_allocated 29271.39501953125 
[2025-02-20 08:04:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0013593549374490976 norm:0.00010671609925338998 max memory_allocated 29271.39501953125 
[2025-02-20 08:04:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0013543125241994858 norm:0.00010054506128653884 max memory_allocated 29271.39501953125 
[2025-02-20 08:05:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0013499651104211807 norm:9.567415690980852e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:06:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0013475373852998018 norm:8.517055539414287e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:07:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0013452128041535616 norm:8.717673335922882e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:08:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0013434098800644279 norm:0.00010669673793017864 max memory_allocated 29271.39501953125 
[2025-02-20 08:08:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0013389047235250473 norm:0.00010280146670993418 max memory_allocated 29271.39501953125 
[2025-02-20 08:09:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0013375482521951199 norm:0.0001141871907748282 max memory_allocated 29271.39501953125 
[2025-02-20 08:09:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-20 08:11:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.001724408706650138 norm:0.00011663787154247984 max memory_allocated 29271.39501953125 
[2025-02-20 08:11:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0015589825343340635 norm:6.315944483503699e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:12:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0014919675886631012 norm:4.5104941818863153e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:13:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0014548222534358501 norm:3.656728222267702e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:14:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0014344690134748816 norm:3.136586747132242e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:15:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0014194699469953775 norm:2.7924876121687703e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:15:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0014070284087210894 norm:2.5750179702299647e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:16:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0013990828301757574 norm:2.4909317289711908e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:17:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0013939300552010536 norm:2.3932794647407718e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:18:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0013902710052207112 norm:2.4645038138260134e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:19:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0013865710934624076 norm:2.1915086108492687e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:19:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0013839846942573786 norm:2.427175968477968e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:20:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0013813780387863517 norm:2.342637162655592e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:21:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0013794844271615148 norm:2.1093577743158676e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:22:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0013768791686743498 norm:2.26729334826814e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:23:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0013749378267675638 norm:2.1933661628281698e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:24:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0013744569150730968 norm:2.242810296593234e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:24:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0013724718010053039 norm:2.2057807655073702e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:25:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0013726018369197845 norm:2.1096866476000287e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:26:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0013716130051761866 norm:2.333094016648829e-05 max memory_allocated 29271.39501953125 
[2025-02-20 08:26:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-20 08:27:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.00179892813321203 norm:0.0001183890417451039 max memory_allocated 29271.81298828125 
[2025-02-20 08:28:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0016474872827529907 norm:5.146753755980171e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:29:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0015852947253733873 norm:3.824082887149416e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:30:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0015513916732743382 norm:3.138685133308172e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:30:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0015300726518034935 norm:2.6090921892318875e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:31:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0015169986290857196 norm:2.3037337086861953e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:32:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.001507035456597805 norm:2.148618659703061e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:33:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0014992954675108194 norm:2.0045319615746848e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:34:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0014944297727197409 norm:1.9367704226169735e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:35:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.001488539739511907 norm:1.781093487807084e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:35:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.001484665903262794 norm:1.7717124137561768e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:36:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0014816061593592167 norm:1.7494890926172957e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:37:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0014784487430006266 norm:1.7591341020306572e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:38:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.001476246165111661 norm:1.5994881323422305e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:39:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0014739518519490957 norm:1.4901022950652987e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:39:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0014711181866005063 norm:1.4860006558592431e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:40:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.001469550421461463 norm:1.5269612049451098e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:41:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0014673495898023248 norm:1.4939350876375102e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:42:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0014671708922833204 norm:1.4812721019552555e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:43:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0014650049852207303 norm:1.4135546734905802e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:43:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-20 08:44:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.001918431487865746 norm:0.00015508715296164155 max memory_allocated 29271.81298828125 
[2025-02-20 08:45:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0017431374872103333 norm:6.887810013722628e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:46:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.001672051614150405 norm:4.539807196124457e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:46:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0016324648167937994 norm:3.440359796513803e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:47:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0016093169106170535 norm:2.7683223379426636e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:48:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0015938306460157037 norm:2.4821143597364426e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:49:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0015828560572117567 norm:2.2075953893363476e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:50:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0015751724131405354 norm:2.03315939870663e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:50:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0015673915622755885 norm:1.8086029740516096e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:51:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0015627553220838308 norm:1.7981052224058658e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:52:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.001560622826218605 norm:1.7750668121152557e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:53:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0015570094110444188 norm:1.7225596820935607e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:54:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.001555415685288608 norm:1.7020687664626166e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:55:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0015527853975072503 norm:1.5414945664815605e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:55:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0015477159759029746 norm:1.60867548402166e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:56:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0015509077347815037 norm:1.5711619198555127e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:57:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0015482392627745867 norm:1.54509034473449e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:58:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0015460738213732839 norm:1.4891969840391539e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:59:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0015451236395165324 norm:1.4252580513129942e-05 max memory_allocated 29271.81298828125 
[2025-02-20 08:59:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0015440700808539987 norm:1.5057446944410913e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:00:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-20 09:01:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0019540039356797934 norm:0.00010055277380160987 max memory_allocated 29271.81298828125 
[2025-02-20 09:02:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.001812789123505354 norm:5.0003967771772295e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:02:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.0017548792529851198 norm:3.455502519500442e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:03:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0017215792322531343 norm:2.765276440186426e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:04:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0016984683461487293 norm:2.3300532120629214e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:05:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.001682884176261723 norm:2.1199683033046313e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:06:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0016726210014894605 norm:1.9910945411538705e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:06:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.001666044699959457 norm:1.8110151358996518e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:07:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.001661013811826706 norm:1.6572232198086567e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:08:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0016579556977376342 norm:1.6333300663973205e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:09:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0016539645148441195 norm:1.6594616681686603e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:10:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.001651839236728847 norm:1.6688689356669784e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:10:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0016488989349454641 norm:1.5477929991902784e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:11:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0016462246421724558 norm:1.5002542568254285e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:12:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0016440588515251875 norm:1.3630063222080935e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:13:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0016417618608102202 norm:1.3823153494740836e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:14:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0016404377529397607 norm:1.3863851563655771e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:15:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0016392238903790712 norm:1.358525150862988e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:15:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0016377497231587768 norm:1.3915092495153658e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:16:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0016371917445212603 norm:1.2838057955377735e-05 max memory_allocated 29271.81298828125 
[2025-02-20 09:16:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-20 09:17:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0021363680716603994 norm:0.00021312551689334214 max memory_allocated 29272.37548828125 
[2025-02-20 09:18:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0019318736158311367 norm:9.525749192107469e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:19:32 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.001849725260399282 norm:6.0112535720691085e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:20:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.001804143888875842 norm:4.43957578681875e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:21:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0017745968652889132 norm:3.441509761614725e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:21:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0017555818194523454 norm:2.92466447717743e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:22:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0017424230463802814 norm:2.4410566766164266e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:23:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0017321272753179073 norm:2.133552698069252e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:24:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0017241861205548048 norm:1.9291694115963764e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:25:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0017181806033477187 norm:1.8133347111870535e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:26:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0017129435436800122 norm:1.5595323930028826e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:26:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0017094484064728022 norm:1.511025766376406e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:27:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0017079015960916877 norm:1.444912686565658e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:28:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0017050503520295024 norm:1.342162795481272e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:29:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0017030586022883654 norm:1.3894151379645336e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:30:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0017006476409733295 norm:1.3328846762306057e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:30:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.001699455315247178 norm:1.2875379979959689e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:31:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0016977108316496015 norm:1.2381205124256667e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:32:32 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0016968061681836843 norm:1.2377388884488028e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:33:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0016963721718639135 norm:1.210670598084107e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:33:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-20 09:34:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002088125329464674 norm:7.221912528621033e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:35:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0019660175312310457 norm:3.664234827738255e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:36:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0019106765976175666 norm:2.8011327231070027e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:37:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0018772367620840669 norm:2.244265670015011e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:37:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0018562055192887783 norm:2.0141966160736047e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:38:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0018431596690788865 norm:1.704641545074992e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:39:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0018326765857636929 norm:1.543108737678267e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:40:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0018244646489620209 norm:1.4437319805438165e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:41:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0018191609997302294 norm:1.3545984984375536e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:41:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0018154860008507967 norm:1.3725742974202149e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:42:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.00181049935054034 norm:1.3217306332080625e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:43:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0018079597502946854 norm:1.318224076385377e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:44:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.001805848442018032 norm:1.2008382327621803e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:45:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0018052541418001056 norm:1.4174654097587336e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:45:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.001803102670237422 norm:1.4123119399300776e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:46:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0018010736675933003 norm:1.2291468010516837e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:47:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0018000260461121798 norm:1.2327162039582618e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:48:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0018001720309257507 norm:1.2212993169669062e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:49:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0017975762020796537 norm:1.2177740245533641e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:50:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0017976898234337568 norm:1.3533733181247953e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:50:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-20 09:51:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0021510946098715067 norm:6.580065382877365e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:52:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.002037604572251439 norm:3.411348006920889e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:52:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0019880712497979403 norm:2.4272263544844463e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:53:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0019608712755143642 norm:1.982345202122815e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:54:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0019414410926401615 norm:1.6945934476098046e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:55:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0019277868559584022 norm:1.4377509614860173e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:56:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0019205044955015182 norm:1.2643768059206195e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:56:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.001915448228828609 norm:1.2334575330896769e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:57:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.001910315128043294 norm:1.1514045581861865e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:58:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0019068594556301832 norm:1.0748262866400182e-05 max memory_allocated 29272.37548828125 
[2025-02-20 09:59:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0019052136922255158 norm:1.0692510841181502e-05 max memory_allocated 29272.37548828125 
[2025-02-20 10:00:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0019030378898605704 norm:1.0922481123998296e-05 max memory_allocated 29272.37548828125 
[2025-02-20 10:01:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0019008740782737732 norm:9.946358659362886e-06 max memory_allocated 29272.37548828125 
[2025-02-20 10:01:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0018990059616044164 norm:9.571476766723208e-06 max memory_allocated 29272.37548828125 
[2025-02-20 10:02:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.001897218287922442 norm:9.49412878981093e-06 max memory_allocated 29272.37548828125 
[2025-02-20 10:03:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0018962444737553596 norm:9.365801815874875e-06 max memory_allocated 29272.37548828125 
[2025-02-20 10:04:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0018945737974718213 norm:9.44205021369271e-06 max memory_allocated 29272.37548828125 
[2025-02-20 10:05:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0018946782220155 norm:8.862170943757519e-06 max memory_allocated 29272.37548828125 
[2025-02-20 10:05:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0018934111576527357 norm:9.077419235836715e-06 max memory_allocated 29272.37548828125 
[2025-02-20 10:06:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0018928616773337126 norm:9.955228961189277e-06 max memory_allocated 29272.37548828125 
[2025-02-20 10:06:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-20 10:08:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0022644000127911568 norm:7.429137622239068e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:08:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0021703082602471113 norm:3.3954522223211825e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:09:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.002128630643710494 norm:2.3611353753949516e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:10:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0021037952974438667 norm:1.9437489754636772e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:11:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.002090755384415388 norm:1.7473601474193856e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:12:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002080240985378623 norm:1.6100746506708674e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:12:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.002071523806080222 norm:1.4947171621315647e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:13:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0020663810428231955 norm:1.4185424333845731e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:14:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.002062183804810047 norm:1.3696223504666705e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:15:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0020596664398908615 norm:1.349341073364485e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:16:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0020580573473125696 norm:1.3458652574627195e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:16:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0020546894520521164 norm:1.3222106645116583e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:17:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0020513562485575676 norm:1.2855159184255172e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:18:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0020485343411564827 norm:1.3951571418147068e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:19:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0020470027811825275 norm:1.3044767001701985e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:20:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0020476938225328922 norm:1.328185317106545e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:21:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.002048830036073923 norm:1.392923695675563e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:21:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0020480910316109657 norm:1.3734786080021877e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:22:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0020469133742153645 norm:1.3854926692147274e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:23:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.0020463198889046907 norm:1.3792450772598386e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:23:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-20 10:24:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0023506730794906616 norm:4.6697063226019964e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:25:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0022541265934705734 norm:2.4501376174157485e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:26:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002209107391536236 norm:1.724471258057747e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:27:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0021835609804838896 norm:1.4550127161783166e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:28:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0021665184758603573 norm:1.2505195627454668e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:28:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.002156015019863844 norm:1.0787789506139234e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:29:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0021469586063176394 norm:9.928172403306235e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:30:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0021407450549304485 norm:8.979971426015254e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:31:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.002136340131983161 norm:8.879087545210496e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:32:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.002132587833330035 norm:8.916525985114276e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:32:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0021298625506460667 norm:8.694823009136599e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:33:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0021271223668009043 norm:8.53493475005962e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:34:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0021250173449516296 norm:8.144676030497067e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:35:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0021236687898635864 norm:7.731354344286956e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:36:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002121690195053816 norm:7.324971647904022e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:36:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.00212082383222878 norm:7.584080776723567e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:37:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0021187900565564632 norm:7.1551630753674544e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:38:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0021181809715926647 norm:7.3185319706681184e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:39:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0021175832953304052 norm:7.274577910720836e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:40:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0021167618688195944 norm:7.145447398215765e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:40:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-20 10:41:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.00249110977165401 norm:8.296751911984757e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:42:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.002387581393122673 norm:3.7666795833501965e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:43:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.0023424511309713125 norm:2.4544980988139287e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:43:58 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.002316155005246401 norm:1.891907049866859e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:44:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.0022978843189775944 norm:1.5655123206670396e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:45:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0022850288078188896 norm:1.3816083082929254e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:46:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.002275441074743867 norm:1.2393219549267087e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:47:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0022688840981572866 norm:1.099433757190127e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:48:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.0022623101249337196 norm:1.0231602573185228e-05 max memory_allocated 29272.93798828125 
[2025-02-20 10:48:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.0022579869255423546 norm:9.844026862992905e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:49:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.00225414102897048 norm:9.06779314391315e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:50:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0022511708084493876 norm:8.873640581441578e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:51:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0022489484399557114 norm:8.225844794651493e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:52:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.002246796851977706 norm:8.47802402859088e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:52:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0022455137223005295 norm:7.916078175185248e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:53:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.0022431504912674427 norm:7.771743185003288e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:54:32 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.00224215816706419 norm:7.697626642766409e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:55:21 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0022418976295739412 norm:7.839786121621728e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:56:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.002240746980533004 norm:7.928032573545352e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:56:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.002240198664367199 norm:7.650294719496742e-06 max memory_allocated 29272.93798828125 
[2025-02-20 10:57:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-20 10:58:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.002654409036040306 norm:0.00011301628546789289 max memory_allocated 29273.50048828125 
[2025-02-20 10:59:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.00253856903873384 norm:5.000308374292217e-05 max memory_allocated 29273.50048828125 
[2025-02-20 10:59:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.002484726719558239 norm:2.8063624995411374e-05 max memory_allocated 29273.50048828125 
[2025-02-20 11:00:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0024573809932917356 norm:2.1590170945273712e-05 max memory_allocated 29273.50048828125 
[2025-02-20 11:01:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.0024381536059081554 norm:1.7468533769715577e-05 max memory_allocated 29273.50048828125 
[2025-02-20 11:02:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.002423460129648447 norm:1.4837847629678436e-05 max memory_allocated 29273.50048828125 
[2025-02-20 11:03:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.002413467736914754 norm:1.3051460882707033e-05 max memory_allocated 29273.50048828125 
[2025-02-20 11:03:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.002405450912192464 norm:1.1658410585368983e-05 max memory_allocated 29273.50048828125 
[2025-02-20 11:04:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0023998012766242027 norm:1.0765177648863755e-05 max memory_allocated 29273.50048828125 
[2025-02-20 11:05:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0023948901798576117 norm:9.664642675488722e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:06:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0023910270538181067 norm:9.359729119751137e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:07:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.002388601191341877 norm:9.114648491959088e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:07:56 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.002385062165558338 norm:8.598601198173128e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:08:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0023838162887841463 norm:8.24536255095154e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:09:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.002381705678999424 norm:7.92347145761596e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:10:22 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.002380681224167347 norm:8.336726750712842e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:11:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0023784106597304344 norm:7.690939128224272e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:12:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.0023777629248797894 norm:7.742987691017333e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:12:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0023774919100105762 norm:7.471343451470602e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:13:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.002376703079789877 norm:7.601422566949623e-06 max memory_allocated 29273.50048828125 
[2025-02-20 11:13:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-20 11:14:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.0027236950118094683 norm:6.678349745925516e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:15:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.0026212232187390327 norm:3.5443044907879084e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:16:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.002574119484052062 norm:2.4655835659359582e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:17:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.002544649410992861 norm:1.9763327145483345e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:18:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0025260646361857653 norm:1.6058338587754406e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:18:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0025113332085311413 norm:1.3592953109764494e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:19:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0025015075225383043 norm:1.1989530321443453e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:20:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0024933088570833206 norm:1.1168995115440339e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:21:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0024867230094969273 norm:1.0313944585504942e-05 max memory_allocated 29273.68798828125 
[2025-02-20 11:22:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0024818701203912497 norm:9.317344847659115e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:22:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0024782251566648483 norm:9.006558684632182e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:23:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0024744663387537003 norm:8.497102498949971e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:24:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0024723736569285393 norm:8.501480806444306e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:25:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.002470355713739991 norm:7.897771865827963e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:26:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.002468411810696125 norm:7.945248398755211e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:27:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0024669747799634933 norm:7.965430086187553e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:27:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.002465715864673257 norm:7.832517439965159e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:28:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.002465292811393738 norm:7.701379217905924e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:29:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.002464609919115901 norm:7.494482815673109e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:30:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.002462982665747404 norm:7.381088835245464e-06 max memory_allocated 29273.68798828125 
[2025-02-20 11:30:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-20 11:31:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.002938551362603903 norm:0.00010409209062345326 max memory_allocated 29273.87548828125 
[2025-02-20 11:32:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.0028226946014910936 norm:5.1625378546305e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:33:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0027685617096722126 norm:3.273636684753001e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:33:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.002737760078161955 norm:2.5677261874079704e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:34:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0027136695571243763 norm:2.048905662377365e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:35:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0026977804955095053 norm:1.7396780094713904e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:36:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.0026852490846067667 norm:1.543010148452595e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:37:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.002675765659660101 norm:1.4017723515280522e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:38:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.0026675951667129993 norm:1.2485825209296308e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:38:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.0026609108317643404 norm:1.1526746675372124e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:39:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.002656282391399145 norm:1.0741603546193801e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:40:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.0026519205421209335 norm:1.0162034413951915e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:41:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0026487153954803944 norm:1.020894524117466e-05 max memory_allocated 29273.87548828125 
[2025-02-20 11:42:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0026457435451447964 norm:9.609841072233394e-06 max memory_allocated 29273.87548828125 
[2025-02-20 11:42:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.002643323503434658 norm:9.065221092896536e-06 max memory_allocated 29273.87548828125 
[2025-02-20 11:43:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.002640981925651431 norm:8.889506716514006e-06 max memory_allocated 29273.87548828125 
[2025-02-20 11:44:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.002639224985614419 norm:8.917472769098822e-06 max memory_allocated 29273.87548828125 
[2025-02-20 11:45:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.002637541852891445 norm:8.197698662115727e-06 max memory_allocated 29273.87548828125 
[2025-02-20 11:46:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0026363132055848837 norm:8.222640644817147e-06 max memory_allocated 29273.87548828125 
[2025-02-20 11:46:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.002635315293446183 norm:8.083209650067147e-06 max memory_allocated 29273.87548828125 
[2025-02-20 11:47:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-20 11:48:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.0029982347041368484 norm:6.822010618634522e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:49:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0029139926191419363 norm:3.521696271491237e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:49:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.002875968348234892 norm:2.5214561901520938e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:50:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.0028524433728307486 norm:2.1046711481176317e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:51:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0028350967913866043 norm:1.7574728190083988e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:52:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.002822266425937414 norm:1.4823115634499118e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:53:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.0028134926687926054 norm:1.3852450138074346e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:53:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0028063782956451178 norm:1.2971657270099968e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:54:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0028001407627016306 norm:1.1492244084365666e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:55:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.002796766348183155 norm:1.1552837349881884e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:56:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.002792424289509654 norm:1.0780909178720322e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:57:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.0027899842243641615 norm:1.0431527698528953e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:57:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.002787380712106824 norm:1.0064171874546446e-05 max memory_allocated 29274.06298828125 
[2025-02-20 11:58:47 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.0027849781326949596 norm:9.997714187193196e-06 max memory_allocated 29274.06298828125 
[2025-02-20 11:59:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.0027832998894155025 norm:9.589576620783191e-06 max memory_allocated 29274.06298828125 
[2025-02-20 12:00:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.0027816002257168293 norm:9.641778888180852e-06 max memory_allocated 29274.06298828125 
[2025-02-20 12:01:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.00278006074950099 norm:9.225634130416438e-06 max memory_allocated 29274.06298828125 
[2025-02-20 12:02:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.002779326867312193 norm:9.470732038607821e-06 max memory_allocated 29274.06298828125 
[2025-02-20 12:02:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.002777852350845933 norm:9.591209163772874e-06 max memory_allocated 29274.06298828125 
[2025-02-20 12:03:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.00277678738348186 norm:8.992141374619678e-06 max memory_allocated 29274.06298828125 
[2025-02-20 12:03:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-20 12:04:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.003195894183591008 norm:7.401668699458241e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:05:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0030998012516647577 norm:3.63918297807686e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:06:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.003054738510400057 norm:2.4545079213567078e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:07:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.0030265289824455976 norm:1.910220817080699e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:08:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.0030065979808568954 norm:1.566042919876054e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:08:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.002991878893226385 norm:1.371568941976875e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:09:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.002980487886816263 norm:1.2019232599413954e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:10:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.0029720929451286793 norm:1.1186117262695916e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:11:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.002964818850159645 norm:1.024976336339023e-05 max memory_allocated 29274.25048828125 
[2025-02-20 12:12:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.0029592285864055157 norm:9.775716534932144e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:13:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.0029541929252445698 norm:8.906515176931862e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:13:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.002949580317363143 norm:8.63284913066309e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:14:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.0029463875107467175 norm:8.320009328599554e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:15:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.0029433046001940966 norm:7.926581020001322e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:16:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.0029409804847091436 norm:7.958833521115594e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:17:03 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.0029384680092334747 norm:7.867097338021267e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:17:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.002936077071353793 norm:7.651416126464028e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:18:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.0029341685585677624 norm:7.575871677545365e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:19:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0029328754171729088 norm:7.50685148886987e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:20:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.002931857481598854 norm:7.354452009167289e-06 max memory_allocated 29274.25048828125 
[2025-02-20 12:20:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-20 12:21:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.0034055791329592466 norm:6.464916805271059e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:22:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.0033211768604815006 norm:3.610971907619387e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:23:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.0032791222911328077 norm:2.6477035135030746e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:23:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.003248424269258976 norm:2.0916373614454642e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:24:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.003227926790714264 norm:1.7534237485961057e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:25:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.003210312221199274 norm:1.499032077845186e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:26:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0031960364431142807 norm:1.3329564353625756e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:27:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0031870040111243725 norm:1.2370364856906235e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:28:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0031785299070179462 norm:1.1468061529740226e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:28:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.0031716031953692436 norm:1.0900359484367073e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:29:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.0031657875515520573 norm:1.0134005606232677e-05 max memory_allocated 29274.43798828125 
[2025-02-20 12:30:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.003160839667543769 norm:9.680224138719495e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:31:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.0031582217197865248 norm:9.165695701085497e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:32:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.003153995843604207 norm:8.855437954480294e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:32:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.003151390701532364 norm:8.611876182840206e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:33:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.0031489720568060875 norm:8.47113460622495e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:34:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.003145388327538967 norm:8.111314855341334e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:35:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.0031427815556526184 norm:8.110955604934134e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:36:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.0031410856172442436 norm:7.801053470757324e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:36:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0031392835080623627 norm:7.56777944843634e-06 max memory_allocated 29274.43798828125 
[2025-02-20 12:37:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-20 12:38:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.0036251668352633715 norm:6.866565672680736e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:39:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.0035332399420440197 norm:3.466179623501375e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:39:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.003487049136310816 norm:2.4345092242583632e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:40:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.0034573117736727 norm:1.9617231373558752e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:41:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.0034354294184595346 norm:1.670418532739859e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:42:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0034185191616415977 norm:1.4275670764618553e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:43:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.003405567491427064 norm:1.2895938198198564e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:43:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.003394940635189414 norm:1.1746977179427631e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:44:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.0033870378974825144 norm:1.0713565643527545e-05 max memory_allocated 29274.62548828125 
[2025-02-20 12:45:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.0033804536797106266 norm:9.737354048411362e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:46:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.0033754638861864805 norm:9.278192010242492e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:47:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.003370412392541766 norm:9.035665243573021e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:47:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.003365500131621957 norm:8.972003342933021e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:48:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.003362346673384309 norm:8.501577212882694e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:49:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.00335996737703681 norm:8.14376471680589e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:50:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.0033581526950001717 norm:8.010949386516586e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:51:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.003355741733685136 norm:8.150328540068585e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:52:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.003354149404913187 norm:7.851561349525582e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:52:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.0033519272692501545 norm:7.622406428708928e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:53:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.003350527724251151 norm:7.522104624513304e-06 max memory_allocated 29274.62548828125 
[2025-02-20 12:53:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-20 12:54:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.003980622161179781 norm:6.624183879466727e-05 max memory_allocated 29274.81298828125 
[2025-02-20 12:55:40 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.0038915753830224276 norm:3.797480894718319e-05 max memory_allocated 29274.81298828125 
[2025-02-20 12:56:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.003844986204057932 norm:2.711512024689e-05 max memory_allocated 29274.81298828125 
[2025-02-20 12:57:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.003814694471657276 norm:2.214863707195036e-05 max memory_allocated 29274.81298828125 
[2025-02-20 12:58:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.0037927906960248947 norm:1.897698712127749e-05 max memory_allocated 29274.81298828125 
[2025-02-20 12:58:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.0037764080334454775 norm:1.6534109818167053e-05 max memory_allocated 29274.81298828125 
[2025-02-20 12:59:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.0037626842968165874 norm:1.5026473192847334e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:00:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.0037517636083066463 norm:1.3804989066557027e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:01:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.0037433665711432695 norm:1.2685255569522269e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:02:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.0037358812987804413 norm:1.2058978427376132e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:03:00 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.0037308046594262123 norm:1.1517639904923271e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:03:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.0037255699280649424 norm:1.1279036698397249e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:04:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.003720052307471633 norm:1.0995436241501011e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:05:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.0037158250343054533 norm:1.0159294106415473e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:06:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.0037125826347619295 norm:1.0026300515164621e-05 max memory_allocated 29274.81298828125 
[2025-02-20 13:07:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.0037096343003213406 norm:9.839517588261515e-06 max memory_allocated 29274.81298828125 
[2025-02-20 13:07:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.003706594929099083 norm:9.527788279228844e-06 max memory_allocated 29274.81298828125 
[2025-02-20 13:08:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.003704475238919258 norm:9.317585863755085e-06 max memory_allocated 29274.81298828125 
[2025-02-20 13:09:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.0037025408819317818 norm:9.008577762870118e-06 max memory_allocated 29274.81298828125 
[2025-02-20 13:10:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.003700819332152605 norm:9.05361412151251e-06 max memory_allocated 29274.81298828125 
[2025-02-20 13:10:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-20 13:11:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.004399478435516357 norm:7.319181167986244e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:12:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.004289373755455017 norm:4.369270754978061e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:13:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.004232385661453009 norm:3.124781505903229e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:14:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.0041947741992771626 norm:2.5263616407755762e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:14:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.004167564678937197 norm:2.1110192392370664e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:15:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.004147031810134649 norm:1.8154596546082757e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:16:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.004131292924284935 norm:1.617364978301339e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:17:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.0041184634901583195 norm:1.4441304301726632e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:18:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.004107839427888393 norm:1.333290583716007e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:18:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.004098421428352594 norm:1.228511882800376e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:19:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.004091448150575161 norm:1.167287518910598e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:20:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.00408506253734231 norm:1.106184208765626e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:21:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.004079428501427174 norm:1.0601805115584284e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:22:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.004074421711266041 norm:1.0088855560752563e-05 max memory_allocated 29275.00048828125 
[2025-02-20 13:22:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.004070697817951441 norm:9.77676609181799e-06 max memory_allocated 29275.00048828125 
[2025-02-20 13:23:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.004067030269652605 norm:9.40538211580133e-06 max memory_allocated 29275.00048828125 
[2025-02-20 13:24:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.004064022097736597 norm:9.207213224726729e-06 max memory_allocated 29275.00048828125 
[2025-02-20 13:25:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.004061584826558828 norm:9.26807479117997e-06 max memory_allocated 29275.00048828125 
[2025-02-20 13:26:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.004058780614286661 norm:9.105549906962551e-06 max memory_allocated 29275.00048828125 
[2025-02-20 13:27:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.004056582227349281 norm:8.90368755790405e-06 max memory_allocated 29275.00048828125 
[2025-02-20 13:27:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-20 13:28:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.004839427303522825 norm:0.0001648324978305027 max memory_allocated 29275.18798828125 
[2025-02-20 13:29:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.004714316688477993 norm:8.187032653950155e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:29:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.0046547516249120235 norm:5.2167601097607985e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:30:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.00461915135383606 norm:3.994860526290722e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:31:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.004592979326844215 norm:3.251200541853905e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:32:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.00457348395138979 norm:2.6895151677308604e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:33:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.004556465893983841 norm:2.28861317737028e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:33:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.004543313290923834 norm:1.999744745262433e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:34:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.004533128812909126 norm:1.7679551092442125e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:35:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.00452426029369235 norm:1.5874491509748623e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:36:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.0045172423124313354 norm:1.4537939932779409e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:37:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.004510899540036917 norm:1.283311030420009e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:38:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.004505563993006945 norm:1.1947212442464661e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:38:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.004500724375247955 norm:1.1339422599121463e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:39:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.004496770910918713 norm:1.070688358595362e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:40:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.004493106156587601 norm:1.0141542588826269e-05 max memory_allocated 29275.18798828125 
[2025-02-20 13:41:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.004489603452384472 norm:9.790393050934654e-06 max memory_allocated 29275.18798828125 
[2025-02-20 13:42:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.004487508442252874 norm:9.282580322178546e-06 max memory_allocated 29275.18798828125 
[2025-02-20 13:42:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.004484879318624735 norm:8.954374607128557e-06 max memory_allocated 29275.18798828125 
[2025-02-20 13:43:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.0044831158593297005 norm:8.74571742315311e-06 max memory_allocated 29275.18798828125 
[2025-02-20 13:43:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-20 13:44:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.00527384877204895 norm:0.0012319881934672594 max memory_allocated 29275.37548828125 
[2025-02-20 13:45:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.005127021111547947 norm:0.00010586702410364524 max memory_allocated 29275.37548828125 
[2025-02-20 13:46:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.00507716741412878 norm:4.879590778728016e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:47:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.005046830978244543 norm:3.620665302150883e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:48:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.0050250450149178505 norm:2.909968861786183e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:49:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.005009151995182037 norm:2.436091745039448e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:49:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.0049971421249210835 norm:2.1447971448651515e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:50:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.004987223539501429 norm:1.922055525938049e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:51:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.004978707060217857 norm:1.6910948033910245e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:52:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.004972211085259914 norm:1.5912259186734445e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:53:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.004966471344232559 norm:1.4498340533464216e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:53:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.00496020307764411 norm:1.371839789499063e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:54:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.004955961834639311 norm:1.2820077245123684e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:55:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.0049523417837917805 norm:1.2489136679505464e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:56:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.004948634188622236 norm:1.1935780094063375e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:57:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.004945746622979641 norm:1.1244281267863698e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:58:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.004943354055285454 norm:1.1053308298869524e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:58:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.0049406783655285835 norm:1.0594111699901987e-05 max memory_allocated 29275.37548828125 
[2025-02-20 13:59:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.004937979858368635 norm:1.0377986654930282e-05 max memory_allocated 29275.37548828125 
[2025-02-20 14:00:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.004935843870043755 norm:1.1325999366817996e-05 max memory_allocated 29275.37548828125 
[2025-02-20 14:00:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-20 14:01:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.005645918659865856 norm:9.646167745813727e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:02:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.005568310618400574 norm:4.928944326820783e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:03:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.00553163094446063 norm:3.274874325143173e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:04:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.005510174669325352 norm:2.479641989339143e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:04:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.005494294688105583 norm:2.005847272812389e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:05:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.005482207518070936 norm:1.6818403310026042e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:06:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.005472860764712095 norm:1.4771024325455073e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:07:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.005464689806103706 norm:1.3201676665630657e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:08:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.005458551459014416 norm:1.208514004247263e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:09:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.005452705547213554 norm:1.094334675144637e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:09:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.0054483176209032536 norm:1.0402599400549661e-05 max memory_allocated 29275.56298828125 
[2025-02-20 14:10:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.005444225389510393 norm:9.819827937462833e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:11:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.0054404241964221 norm:9.303294064011425e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:12:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.005437166895717382 norm:9.03587533684913e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:13:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.00543404184281826 norm:8.620613698440138e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:13:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.0054308706894516945 norm:8.279889698314946e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:14:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.005429121665656567 norm:8.014483682927676e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:15:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.005427058786153793 norm:7.747702511551324e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:16:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.0054255700670182705 norm:7.504445875383681e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:17:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.005424137227237225 norm:7.34559307602467e-06 max memory_allocated 29275.56298828125 
[2025-02-20 14:17:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-20 14:18:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.006283491384238005 norm:8.223897020798177e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:19:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.00618639774620533 norm:4.567174255498685e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:20:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.0061383675783872604 norm:3.128664684481919e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:21:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.006107577122747898 norm:2.500636037439108e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:22:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.006084649823606014 norm:2.0486539142439142e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:22:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.006068824790418148 norm:1.7464690245105885e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:23:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.00605624308809638 norm:1.5492059901589528e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:24:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.006046155001968145 norm:1.4123699656920508e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:25:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.006037973333150148 norm:1.3031998605583794e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:26:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.006031712982803583 norm:1.1960859410464764e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:27:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.006025477312505245 norm:1.124805748986546e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:27:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.006020394153892994 norm:1.047082150762435e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:28:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.006016502622514963 norm:1.0443123755976558e-05 max memory_allocated 29275.75048828125 
[2025-02-20 14:29:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.006011761259287596 norm:9.585801308276132e-06 max memory_allocated 29275.75048828125 
[2025-02-20 14:30:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.006007516756653786 norm:9.382480129715987e-06 max memory_allocated 29275.75048828125 
[2025-02-20 14:31:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.00600492162629962 norm:9.093456355913077e-06 max memory_allocated 29275.75048828125 
[2025-02-20 14:31:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.006001277826726437 norm:9.08579932001885e-06 max memory_allocated 29275.75048828125 
[2025-02-20 14:32:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.005999459885060787 norm:8.910092219593935e-06 max memory_allocated 29275.75048828125 
[2025-02-20 14:33:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.0059975008480250835 norm:9.511127245787065e-06 max memory_allocated 29275.75048828125 
[2025-02-20 14:34:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.005995478015393019 norm:8.736240488360636e-06 max memory_allocated 29275.75048828125 
[2025-02-20 14:34:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-20 14:36:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.006697403267025948 norm:2.366457920288667e-05 max memory_allocated 29275.93798828125 
[2025-02-20 14:36:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.006660977844148874 norm:1.497656739957165e-05 max memory_allocated 29275.93798828125 
[2025-02-20 14:37:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.006640150677412748 norm:1.1484744391054846e-05 max memory_allocated 29275.93798828125 
[2025-02-20 14:38:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.00662577711045742 norm:9.735050298331771e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:39:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.006615111138671637 norm:8.684146450832486e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:40:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.006605620961636305 norm:8.036691724555567e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:41:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.006597279105335474 norm:7.584028935525566e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:41:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.006590970791876316 norm:7.206132522696862e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:42:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.006586591247469187 norm:7.063274551910581e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:43:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.006581394001841545 norm:6.888141797389835e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:44:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.006578001659363508 norm:6.830404799984535e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:45:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.0065747820772230625 norm:6.4940568336169235e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:45:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.0065716770477592945 norm:6.494000444945414e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:46:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.006569175980985165 norm:6.3432089518755674e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:47:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.006566895171999931 norm:6.127170763647882e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:48:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.006564407609403133 norm:6.039188519935124e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:49:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.006562337279319763 norm:5.924210654484341e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:49:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.006560812704265118 norm:5.933275133429561e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:50:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.006559140048921108 norm:5.7854958868119866e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:51:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.0065579721704125404 norm:5.878075626242207e-06 max memory_allocated 29275.93798828125 
[2025-02-20 14:51:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-20 14:53:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.00753058260306716 norm:0.0001192079798784107 max memory_allocated 29276.12548828125 
[2025-02-20 14:54:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.007429758086800575 norm:7.583729166071862e-05 max memory_allocated 29276.12548828125 
[2025-02-20 14:54:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.007374707143753767 norm:4.879933476331644e-05 max memory_allocated 29276.12548828125 
[2025-02-20 14:55:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.007346406579017639 norm:3.691093297675252e-05 max memory_allocated 29276.12548828125 
[2025-02-20 14:56:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.007326208986341953 norm:3.0008623070898466e-05 max memory_allocated 29276.12548828125 
[2025-02-20 14:57:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.007310396991670132 norm:2.7534968467080034e-05 max memory_allocated 29276.12548828125 
[2025-02-20 14:58:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.0072990404441952705 norm:2.7412701456341892e-05 max memory_allocated 29276.12548828125 
[2025-02-20 14:58:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.007288506720215082 norm:2.5608593205106445e-05 max memory_allocated 29276.12548828125 
[2025-02-20 14:59:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.007279580924659967 norm:2.4545350242988206e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:00:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.007272755261510611 norm:2.3653581592952833e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:01:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.007267794106155634 norm:2.3370899725705385e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:02:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.0072626108303666115 norm:2.434798443573527e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:02:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.007257243152707815 norm:2.2782503947382793e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:03:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.007253378164023161 norm:2.2152187739266083e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:04:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.007250456139445305 norm:2.1266690964694135e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:05:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.00724783493205905 norm:2.0736575606861152e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:06:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.007245431188493967 norm:2.1659681806340814e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:07:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.0072424886748194695 norm:2.0078001398360357e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:07:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.00724098552018404 norm:1.9383083781576715e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:08:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.007239967584609985 norm:1.9199309463147074e-05 max memory_allocated 29276.12548828125 
[2025-02-20 15:08:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-20 15:10:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.008109933696687222 norm:4.3288870074320585e-05 max memory_allocated 29276.31298828125 
[2025-02-20 15:10:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.008058278821408749 norm:2.4934059183578938e-05 max memory_allocated 29276.31298828125 
[2025-02-20 15:11:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.008029336109757423 norm:1.837837407947518e-05 max memory_allocated 29276.31298828125 
[2025-02-20 15:12:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.008010550402104855 norm:1.4649562217528e-05 max memory_allocated 29276.31298828125 
[2025-02-20 15:13:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.007995354942977428 norm:1.2514779882621951e-05 max memory_allocated 29276.31298828125 
[2025-02-20 15:14:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.007984662428498268 norm:1.1049579370592255e-05 max memory_allocated 29276.31298828125 
[2025-02-20 15:14:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.00797535851597786 norm:1.0193122761847917e-05 max memory_allocated 29276.31298828125 
[2025-02-20 15:15:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.00796679686754942 norm:9.56863095780136e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:16:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.007960174232721329 norm:8.809002792986576e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:17:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.007954818196594715 norm:8.441424142802134e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:18:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.007950029335916042 norm:8.315933882840909e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:18:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.007946140132844448 norm:7.952698979352135e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:19:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.007942971773445606 norm:7.87649696576409e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:20:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.007939761504530907 norm:7.836939403205179e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:21:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.00793792586773634 norm:7.951242878334597e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:22:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.007936730049550533 norm:8.08438562671654e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:23:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.00793382152915001 norm:7.86550299380906e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:23:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.007931734435260296 norm:7.806273060850799e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:24:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.007930420339107513 norm:7.855630428821314e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:25:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.007927825674414635 norm:7.651204214198515e-06 max memory_allocated 29276.31298828125 
[2025-02-20 15:25:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-20 15:26:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.008941438049077988 norm:6.93793990649283e-05 max memory_allocated 29276.50048828125 
[2025-02-20 15:27:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.008861218579113483 norm:3.93603986594826e-05 max memory_allocated 29276.50048828125 
[2025-02-20 15:28:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.008819462731480598 norm:2.6866948246606626e-05 max memory_allocated 29276.50048828125 
[2025-02-20 15:29:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.008792772889137268 norm:2.0510451577138156e-05 max memory_allocated 29276.50048828125 
[2025-02-20 15:30:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.00877290964126587 norm:1.666381285758689e-05 max memory_allocated 29276.50048828125 
[2025-02-20 15:30:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.008757787756621838 norm:1.3936665709479712e-05 max memory_allocated 29276.50048828125 
[2025-02-20 15:31:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.008746504783630371 norm:1.2524901649157982e-05 max memory_allocated 29276.50048828125 
[2025-02-20 15:32:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.008736666291952133 norm:1.1224132322240621e-05 max memory_allocated 29276.50048828125 
[2025-02-20 15:33:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.008728879503905773 norm:9.961994692275766e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:34:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.008722510188817978 norm:9.4188580987975e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:35:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.008716782554984093 norm:8.692559276823886e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:35:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.008711458183825016 norm:8.054655154410284e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:36:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.008707140572369099 norm:7.662529242224991e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:37:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.008703481405973434 norm:7.519776772824116e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:38:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.008699898608028889 norm:7.329154868784826e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:39:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.008697143755853176 norm:7.003250630077673e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:39:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.008695082738995552 norm:6.778832357667852e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:40:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.00869250763207674 norm:6.6602774495549966e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:41:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.008691071532666683 norm:6.7312157625565305e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:42:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.008688690140843391 norm:6.493905857496429e-06 max memory_allocated 29276.50048828125 
[2025-02-20 15:42:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-20 15:43:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.009875135496258736 norm:0.00011851512681460008 max memory_allocated 29276.68798828125 
[2025-02-20 15:44:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.009772762656211853 norm:7.216975063784048e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:45:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.009718617424368858 norm:5.210869858274236e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:46:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.009679310955107212 norm:4.127399733988568e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:46:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.0096519086509943 norm:3.363455107319169e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:47:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.009631332941353321 norm:2.8284228392294608e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:48:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.009614388458430767 norm:2.439146555843763e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:49:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.009600309655070305 norm:2.123935155395884e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:50:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.009588755667209625 norm:1.9000037354999222e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:51:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.00957866758108139 norm:1.6978709027171135e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:51:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.009568421170115471 norm:1.5423676813952625e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:52:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.009561006911098957 norm:1.3904643310524989e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:53:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.009554078802466393 norm:1.2995323231734801e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:54:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.009549188427627087 norm:1.1867930879816413e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:55:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.009544020518660545 norm:1.145875830843579e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:55:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.009538854472339153 norm:1.0804573321365751e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:56:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.00953496154397726 norm:1.0016246051236521e-05 max memory_allocated 29276.68798828125 
[2025-02-20 15:57:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.009531546384096146 norm:9.863885679806117e-06 max memory_allocated 29276.68798828125 
[2025-02-20 15:58:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.009528731927275658 norm:9.158302418654785e-06 max memory_allocated 29276.68798828125 
[2025-02-20 15:59:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.00952589139342308 norm:8.845091542752925e-06 max memory_allocated 29276.68798828125 
[2025-02-20 15:59:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-20 16:00:27 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.010704344138503075 norm:9.447641059523448e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:01:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.01061579491943121 norm:5.251160837360658e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:02:04 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.01057242602109909 norm:3.528136221575551e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:02:53 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.010544123128056526 norm:2.71310218522558e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:03:42 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.010523438453674316 norm:2.23456445382908e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:04:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.010507983155548573 norm:1.8879462004406378e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:05:20 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.010495426133275032 norm:1.6290901839965954e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:06:09 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.010483638383448124 norm:1.4648687283624895e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:06:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.01047519687563181 norm:1.310536390519701e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:07:46 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.010467777960002422 norm:1.1896014257217757e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:08:35 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.010462486185133457 norm:1.0930929420283064e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:09:23 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.010456551797688007 norm:1.0393844604550395e-05 max memory_allocated 29276.87548828125 
[2025-02-20 16:10:12 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.010453296825289726 norm:9.520916137262248e-06 max memory_allocated 29276.87548828125 
[2025-02-20 16:11:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.010448512621223927 norm:9.20832189876819e-06 max memory_allocated 29276.87548828125 
[2025-02-20 16:11:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.010444290935993195 norm:8.600917681178544e-06 max memory_allocated 29276.87548828125 
[2025-02-20 16:12:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.01044237706810236 norm:8.530129889550153e-06 max memory_allocated 29276.87548828125 
[2025-02-20 16:13:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.010437673889100552 norm:7.635111614945345e-06 max memory_allocated 29276.87548828125 
[2025-02-20 16:14:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.010436421260237694 norm:7.567977718281327e-06 max memory_allocated 29276.87548828125 
[2025-02-20 16:15:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.010432710871100426 norm:7.088537131494377e-06 max memory_allocated 29276.87548828125 
[2025-02-20 16:15:54 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.010431218892335892 norm:6.893855243106373e-06 max memory_allocated 29276.87548828125 
[2025-02-20 16:16:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-20 16:17:12 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.01173939649015665 norm:9.264703112421557e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:18:01 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.011637412011623383 norm:5.718610918847844e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:18:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.011577663943171501 norm:4.143890328123234e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:19:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.011537243612110615 norm:3.259629738749936e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:20:28 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.01151042990386486 norm:2.6498455554246902e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:21:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.011489497497677803 norm:2.278766260133125e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:22:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.011473668739199638 norm:1.9436305592535064e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:22:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.011460349895060062 norm:1.6863328710314818e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:23:43 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.011449894867837429 norm:1.5092804460437037e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:24:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.011440601199865341 norm:1.369126039207913e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:25:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.011433216743171215 norm:1.2839544979215134e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:26:09 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.01142614334821701 norm:1.1967200407525524e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:26:58 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.011420468799769878 norm:1.1097728929598816e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:27:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.011414322070777416 norm:1.034825800161343e-05 max memory_allocated 29277.06298828125 
[2025-02-20 16:28:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.011408519931137562 norm:9.847699402598664e-06 max memory_allocated 29277.06298828125 
[2025-02-20 16:29:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.011404396034777164 norm:9.285661690228153e-06 max memory_allocated 29277.06298828125 
[2025-02-20 16:30:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.011399568989872932 norm:8.66271602717461e-06 max memory_allocated 29277.06298828125 
[2025-02-20 16:31:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.011396792717278004 norm:8.474618880427442e-06 max memory_allocated 29277.06298828125 
[2025-02-20 16:31:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.011394615285098553 norm:8.054616046138108e-06 max memory_allocated 29277.06298828125 
[2025-02-20 16:32:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.01139255240559578 norm:7.92385617387481e-06 max memory_allocated 29277.06298828125 
[2025-02-20 16:32:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-20 16:33:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.01284715998917818 norm:6.515704444609582e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:34:46 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.012771563604474068 norm:3.837603799183853e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:35:35 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.012728000059723854 norm:2.6514135242905468e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:36:24 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.012698834761977196 norm:2.0076113287359476e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:37:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.012678987346589565 norm:1.628235258976929e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:38:01 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.012662109918892384 norm:1.3889107322029304e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:38:50 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.012648125179111958 norm:1.2493750546127558e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:39:39 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.012637626379728317 norm:1.1167136108269915e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:40:28 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.012628147378563881 norm:1.0290855243511032e-05 max memory_allocated 29277.25048828125 
[2025-02-20 16:41:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.012621233239769936 norm:9.928384315571748e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:42:05 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.012614519335329533 norm:9.337226401839871e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:42:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.012608455494046211 norm:8.851617167238146e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:43:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.012603776529431343 norm:8.503102435497567e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:44:32 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.012598766945302486 norm:8.145018909999635e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:45:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.01259454246610403 norm:7.772999197186437e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:46:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.012591389939188957 norm:7.6277142397884745e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:46:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.01258889026939869 norm:7.31438285583863e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:47:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.012587451376020908 norm:7.434208328049863e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:48:35 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.012586122378706932 norm:7.857250238885172e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:49:24 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.012583695352077484 norm:7.506781003030483e-06 max memory_allocated 29277.25048828125 
[2025-02-20 16:49:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-20 16:50:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.014046014286577702 norm:2.633616531966254e-05 max memory_allocated 29277.43798828125 
[2025-02-20 16:51:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.013993194326758385 norm:1.8641914721229114e-05 max memory_allocated 29277.43798828125 
[2025-02-20 16:52:22 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.013958980329334736 norm:1.4867678146401886e-05 max memory_allocated 29277.43798828125 
[2025-02-20 16:53:11 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.013934455811977386 norm:1.2609575605893042e-05 max memory_allocated 29277.43798828125 
[2025-02-20 16:54:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.013915877789258957 norm:1.1052633453800809e-05 max memory_allocated 29277.43798828125 
[2025-02-20 16:54:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.013903402723371983 norm:1.0247251339023933e-05 max memory_allocated 29277.43798828125 
[2025-02-20 16:55:37 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.013891431502997875 norm:9.339956704934593e-06 max memory_allocated 29277.43798828125 
[2025-02-20 16:56:26 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.0138816237449646 norm:8.851775419316255e-06 max memory_allocated 29277.43798828125 
[2025-02-20 16:57:14 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.013874217867851257 norm:8.09278844826622e-06 max memory_allocated 29277.43798828125 
[2025-02-20 16:58:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.013867300003767014 norm:8.063895620580297e-06 max memory_allocated 29277.43798828125 
[2025-02-20 16:58:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.013861937448382378 norm:8.060234904405661e-06 max memory_allocated 29277.43798828125 
[2025-02-20 16:59:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.01385611854493618 norm:7.606784947711276e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:00:30 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.013852662406861782 norm:7.219379313028185e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:01:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.013848286122083664 norm:7.325261776713887e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:02:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.013845473527908325 norm:7.016608833509963e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:02:56 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.013842524029314518 norm:7.014989023446105e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:03:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.013839329592883587 norm:6.672938980045728e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:04:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.013837946578860283 norm:6.792100066377316e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:05:22 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.013835402205586433 norm:6.629909876210149e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:06:11 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.013835794292390347 norm:6.659974587819306e-06 max memory_allocated 29277.43798828125 
[2025-02-20 17:06:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-20 17:06:45 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:07:34 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.016041815280914307 norm:0.0011913604103028774 max memory_allocated 29277.77001953125 
[2025-02-20 17:08:23 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.015814241021871567 norm:0.0008913071942515671 max memory_allocated 29277.77001953125 
[2025-02-20 17:09:12 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.01570611260831356 norm:0.0007303530583158135 max memory_allocated 29277.77001953125 
[2025-02-20 17:10:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.015636034309864044 norm:0.0005970183410681784 max memory_allocated 29277.77001953125 
[2025-02-20 17:10:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.015584690496325493 norm:0.0004978110664524138 max memory_allocated 29277.77001953125 
[2025-02-20 17:11:39 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.015549893490970135 norm:0.0004432649875525385 max memory_allocated 29277.77001953125 
[2025-02-20 17:12:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.015522630885243416 norm:0.0004408296081237495 max memory_allocated 29277.77001953125 
[2025-02-20 17:13:17 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.015491719357669353 norm:0.0003892070089932531 max memory_allocated 29277.77001953125 
[2025-02-20 17:14:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.015466837212443352 norm:0.0003205085813533515 max memory_allocated 29277.77001953125 
[2025-02-20 17:14:55 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.015446802601218224 norm:0.00027578743174672127 max memory_allocated 29277.77001953125 
[2025-02-20 17:15:44 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.01543803233653307 norm:0.0002643103071022779 max memory_allocated 29277.77001953125 
[2025-02-20 17:16:33 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.015449265018105507 norm:0.00031378876883536577 max memory_allocated 29277.77001953125 
[2025-02-20 17:17:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.015481165610253811 norm:0.00038956484058871865 max memory_allocated 29277.77001953125 
[2025-02-20 17:18:11 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.01613745652139187 norm:0.0014158798148855567 max memory_allocated 29277.77001953125 
[2025-02-20 17:18:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.01564881019294262 norm:0.0006838136469013989 max memory_allocated 29277.77001953125 
[2025-02-20 17:19:49 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.015500485897064209 norm:0.0004185665457043797 max memory_allocated 29277.77001953125 
[2025-02-20 17:20:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.015438459813594818 norm:0.00039165079942904413 max memory_allocated 29277.77001953125 
[2025-02-20 17:21:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.015417588874697685 norm:0.0003445055044721812 max memory_allocated 29277.77001953125 
[2025-02-20 17:22:16 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.015405673533678055 norm:0.00032451568404212594 max memory_allocated 29277.77001953125 
[2025-02-20 17:23:05 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.015379893593490124 norm:0.00026997915119864047 max memory_allocated 29277.77001953125 
[2025-02-20 17:23:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-20 17:23:37 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:24:26 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.01767592690885067 norm:0.0009458753047510982 max memory_allocated 29277.95751953125 
[2025-02-20 17:25:14 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.017499208450317383 norm:0.0007954956381581724 max memory_allocated 29277.95751953125 
[2025-02-20 17:26:03 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.017411863431334496 norm:0.0006672199233435094 max memory_allocated 29277.95751953125 
[2025-02-20 17:26:52 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.01735427789390087 norm:0.000560526386834681 max memory_allocated 29277.95751953125 
[2025-02-20 17:27:41 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.017308533191680908 norm:0.0005072155618108809 max memory_allocated 29277.95751953125 
[2025-02-20 17:28:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.017277393490076065 norm:0.00044482489465735853 max memory_allocated 29277.95751953125 
[2025-02-20 17:29:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.017247503623366356 norm:0.0003889375366270542 max memory_allocated 29277.95751953125 
[2025-02-20 17:30:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.01721923239529133 norm:0.00033449821057729423 max memory_allocated 29277.95751953125 
[2025-02-20 17:30:57 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.017199818044900894 norm:0.00032101068063639104 max memory_allocated 29277.95751953125 
[2025-02-20 17:31:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.017183417454361916 norm:0.0002780114591587335 max memory_allocated 29277.95751953125 
[2025-02-20 17:32:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.017179075628519058 norm:0.00026942946715280414 max memory_allocated 29277.95751953125 
[2025-02-20 17:33:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.017189230769872665 norm:0.000319468614179641 max memory_allocated 29277.95751953125 
[2025-02-20 17:34:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.017238153144717216 norm:0.0005211998941376805 max memory_allocated 29277.95751953125 
[2025-02-20 17:35:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.017164725810289383 norm:0.00030866035376675427 max memory_allocated 29277.95751953125 
[2025-02-20 17:35:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.017128154635429382 norm:0.0002099711709888652 max memory_allocated 29277.95751953125 
[2025-02-20 17:36:40 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.01711595244705677 norm:0.00018470500071998686 max memory_allocated 29277.95751953125 
[2025-02-20 17:37:29 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.017109695822000504 norm:0.00018057374109048396 max memory_allocated 29277.95751953125 
[2025-02-20 17:38:18 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.017108071595430374 norm:0.00017619473510421813 max memory_allocated 29277.95751953125 
[2025-02-20 17:39:07 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.01710476540029049 norm:0.0001692851074039936 max memory_allocated 29277.95751953125 
[2025-02-20 17:39:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.017102187499403954 norm:0.00016753122326917946 max memory_allocated 29277.95751953125 
[2025-02-20 17:40:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-20 17:40:31 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:41:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.02077600359916687 norm:0.001271429006010294 max memory_allocated 29278.14501953125 
[2025-02-20 17:42:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.020514143630862236 norm:0.0009912976529449224 max memory_allocated 29278.14501953125 
[2025-02-20 17:42:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.020373133942484856 norm:0.0007661842973902822 max memory_allocated 29278.14501953125 
[2025-02-20 17:43:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.020278066396713257 norm:0.0006917901919223368 max memory_allocated 29278.14501953125 
[2025-02-20 17:44:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.020210035145282745 norm:0.0006580798071809113 max memory_allocated 29278.14501953125 
[2025-02-20 17:45:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.02015621028840542 norm:0.0005775869358330965 max memory_allocated 29278.14501953125 
[2025-02-20 17:46:14 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.020122526213526726 norm:0.0005121075082570314 max memory_allocated 29278.14501953125 
[2025-02-20 17:47:03 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.02010081708431244 norm:0.0005265411455184221 max memory_allocated 29278.14501953125 
[2025-02-20 17:47:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.020116174593567848 norm:0.0006515599088743329 max memory_allocated 29278.14501953125 
[2025-02-20 17:48:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.020127831026911736 norm:0.000716721115168184 max memory_allocated 29278.14501953125 
[2025-02-20 17:49:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.02003401890397072 norm:0.0004467876278795302 max memory_allocated 29278.14501953125 
[2025-02-20 17:50:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.01999790407717228 norm:0.000365289713954553 max memory_allocated 29278.14501953125 
[2025-02-20 17:51:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.01997879147529602 norm:0.0003743339912034571 max memory_allocated 29278.14501953125 
[2025-02-20 17:51:57 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.019966281950473785 norm:0.0003462085733190179 max memory_allocated 29278.14501953125 
[2025-02-20 17:52:46 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.019957711920142174 norm:0.0003456705599091947 max memory_allocated 29278.14501953125 
[2025-02-20 17:53:35 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.019947761669754982 norm:0.00031053536804392934 max memory_allocated 29278.14501953125 
[2025-02-20 17:54:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.0199442021548748 norm:0.00035309678060002625 max memory_allocated 29278.14501953125 
[2025-02-20 17:55:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.019935771822929382 norm:0.00029544380959123373 max memory_allocated 29278.14501953125 
[2025-02-20 17:56:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.019922703504562378 norm:0.00028536305762827396 max memory_allocated 29278.14501953125 
[2025-02-20 17:56:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.019914470613002777 norm:0.000275353086180985 max memory_allocated 29278.14501953125 
[2025-02-20 17:57:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-20 17:57:21 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:58:10 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.03561204671859741 norm:0.007042879704385996 max memory_allocated 29278.33251953125 
[2025-02-20 17:58:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.03453613072633743 norm:0.005584996193647385 max memory_allocated 29278.33251953125 
[2025-02-20 17:59:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.03395761549472809 norm:0.0052921948954463005 max memory_allocated 29278.33251953125 
[2025-02-20 18:00:36 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.03348638862371445 norm:0.004905718844383955 max memory_allocated 29278.33251953125 
[2025-02-20 18:01:25 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.033154893666505814 norm:0.004781648982316256 max memory_allocated 29278.33251953125 
[2025-02-20 18:02:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.03291025385260582 norm:0.004845259711146355 max memory_allocated 29278.33251953125 
[2025-02-20 18:03:03 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.03260460123419762 norm:0.004237414803355932 max memory_allocated 29278.33251953125 
[2025-02-20 18:03:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.032330334186553955 norm:0.004110909532755613 max memory_allocated 29278.33251953125 
[2025-02-20 18:04:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.03218308091163635 norm:0.004210690502077341 max memory_allocated 29278.33251953125 
[2025-02-20 18:05:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.03232765942811966 norm:0.004659590777009726 max memory_allocated 29278.33251953125 
[2025-02-20 18:06:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.032756779342889786 norm:0.005180610343813896 max memory_allocated 29278.33251953125 
[2025-02-20 18:07:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.031745750457048416 norm:0.003466419642791152 max memory_allocated 29278.33251953125 
[2025-02-20 18:07:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.0315062515437603 norm:0.0034366848412901163 max memory_allocated 29278.33251953125 
[2025-02-20 18:08:46 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.031319040805101395 norm:0.0035760346800088882 max memory_allocated 29278.33251953125 
[2025-02-20 18:09:35 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.031250566244125366 norm:0.003668219782412052 max memory_allocated 29278.33251953125 
[2025-02-20 18:10:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.031244922429323196 norm:0.0035961163230240345 max memory_allocated 29278.33251953125 
[2025-02-20 18:11:13 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.031080855056643486 norm:0.003489258699119091 max memory_allocated 29278.33251953125 
[2025-02-20 18:12:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.031015636399388313 norm:0.00349922152236104 max memory_allocated 29278.33251953125 
[2025-02-20 18:12:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.03096424788236618 norm:0.00340858893468976 max memory_allocated 29278.33251953125 
[2025-02-20 18:13:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.031003275886178017 norm:0.0035154432989656925 max memory_allocated 29278.33251953125 
[2025-02-20 18:13:55 root] (main_calibration.py 365): INFO 40251.682369709015
[2025-02-20 18:15:03 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-20 18:16:58 root] (main_calibration.py 158): INFO wikitext2 : 4.890997886657715
[2025-02-20 18:16:58 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-20 18:19:56 root] (main_calibration.py 158): INFO c4 : 6.480959892272949
