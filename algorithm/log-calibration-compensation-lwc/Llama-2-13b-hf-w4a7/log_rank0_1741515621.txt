[2025-03-09 10:20:21 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-13b-hf-w4a7', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=7, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-09 10:20:28 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-09 10:20:28 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-09 10:20:29 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-09 10:20:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-09 10:20:43 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:21:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.013954825699329376 norm:0.017135675996541977 max memory_allocated 29271.02001953125 
[2025-03-09 10:22:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.008271979168057442 norm:0.008960429579019547 max memory_allocated 29271.02001953125 
[2025-03-09 10:22:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.006295145954936743 norm:0.0063633183017373085 max memory_allocated 29271.02001953125 
[2025-03-09 10:23:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.005560991354286671 norm:0.00514522660523653 max memory_allocated 29271.02001953125 
[2025-03-09 10:24:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.005379991140216589 norm:0.0044252341613173485 max memory_allocated 29271.02001953125 
[2025-03-09 10:25:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.005073682405054569 norm:0.003676070598885417 max memory_allocated 29271.02001953125 
[2025-03-09 10:26:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0049400124698877335 norm:0.0032570010516792536 max memory_allocated 29271.02001953125 
[2025-03-09 10:26:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0049692667089402676 norm:0.0028253321070224047 max memory_allocated 29271.02001953125 
[2025-03-09 10:27:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.00476142717525363 norm:0.0026342286728322506 max memory_allocated 29271.02001953125 
[2025-03-09 10:28:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.004729012027382851 norm:0.0022239936515688896 max memory_allocated 29271.02001953125 
[2025-03-09 10:29:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.004748868755996227 norm:0.0020929707679897547 max memory_allocated 29271.02001953125 
[2025-03-09 10:29:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.004724778234958649 norm:0.0020272517576813698 max memory_allocated 29271.02001953125 
[2025-03-09 10:30:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.004767784848809242 norm:0.0021967757493257523 max memory_allocated 29271.02001953125 
[2025-03-09 10:31:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.004677243530750275 norm:0.001880254247225821 max memory_allocated 29271.02001953125 
[2025-03-09 10:32:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.004635700024664402 norm:0.001550804590806365 max memory_allocated 29271.02001953125 
[2025-03-09 10:32:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.004634567070752382 norm:0.0015173794236034155 max memory_allocated 29271.02001953125 
[2025-03-09 10:33:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.004579010419547558 norm:0.0015012240037322044 max memory_allocated 29271.02001953125 
[2025-03-09 10:34:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.004562701098620892 norm:0.0012628539698198438 max memory_allocated 29271.02001953125 
[2025-03-09 10:35:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.004545970354229212 norm:0.0012593269348144531 max memory_allocated 29271.02001953125 
[2025-03-09 10:35:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.004519673530012369 norm:0.0011101086856797338 max memory_allocated 29271.02001953125 
[2025-03-09 10:36:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-09 10:36:12 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:36:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.02892220951616764 norm:0.015128210186958313 max memory_allocated 29271.02001953125 
[2025-03-09 10:37:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.019604980945587158 norm:0.009088808670639992 max memory_allocated 29271.02001953125 
[2025-03-09 10:38:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.015344012528657913 norm:0.006223432254046202 max memory_allocated 29271.02001953125 
[2025-03-09 10:39:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.01406547799706459 norm:0.004927853588014841 max memory_allocated 29271.02001953125 
[2025-03-09 10:40:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.01349506713449955 norm:0.00402988912537694 max memory_allocated 29271.02001953125 
[2025-03-09 10:40:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.013192959129810333 norm:0.0034569965209811926 max memory_allocated 29271.02001953125 
[2025-03-09 10:41:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.01299535483121872 norm:0.003079330315813422 max memory_allocated 29271.02001953125 
[2025-03-09 10:42:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.012861809693276882 norm:0.0027140749152749777 max memory_allocated 29271.02001953125 
[2025-03-09 10:43:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.012694019824266434 norm:0.002353793941438198 max memory_allocated 29271.02001953125 
[2025-03-09 10:43:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.012561692856252193 norm:0.002062321873381734 max memory_allocated 29271.02001953125 
[2025-03-09 10:44:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.012427739799022675 norm:0.001792777213267982 max memory_allocated 29271.02001953125 
[2025-03-09 10:45:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.012329209595918655 norm:0.0016311153303831816 max memory_allocated 29271.02001953125 
[2025-03-09 10:46:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.012262942269444466 norm:0.0015397100942209363 max memory_allocated 29271.02001953125 
[2025-03-09 10:46:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0122014619410038 norm:0.0015786730218678713 max memory_allocated 29271.02001953125 
[2025-03-09 10:47:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.012207888066768646 norm:0.001464653410948813 max memory_allocated 29271.02001953125 
[2025-03-09 10:48:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.012218939140439034 norm:0.0015097693540155888 max memory_allocated 29271.02001953125 
[2025-03-09 10:49:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.012346439063549042 norm:0.0014631373342126608 max memory_allocated 29271.02001953125 
[2025-03-09 10:49:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.012218796648085117 norm:0.001315084402449429 max memory_allocated 29271.02001953125 
[2025-03-09 10:50:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.012175839394330978 norm:0.0013944110833108425 max memory_allocated 29271.02001953125 
[2025-03-09 10:51:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.012217756360769272 norm:0.0012859844136983156 max memory_allocated 29271.02001953125 
[2025-03-09 10:51:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-09 10:51:52 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 10:52:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.03243765980005264 norm:0.013105763122439384 max memory_allocated 29271.39501953125 
[2025-03-09 10:53:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.024456150829792023 norm:0.008965520188212395 max memory_allocated 29271.39501953125 
[2025-03-09 10:54:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.020054299384355545 norm:0.006103208288550377 max memory_allocated 29271.39501953125 
[2025-03-09 10:54:54 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.01861874759197235 norm:0.004864382557570934 max memory_allocated 29271.39501953125 
[2025-03-09 10:55:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.017936285585165024 norm:0.004000595770776272 max memory_allocated 29271.39501953125 
[2025-03-09 10:56:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.017493601888418198 norm:0.0033119649160653353 max memory_allocated 29271.39501953125 
[2025-03-09 10:57:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.01713516376912594 norm:0.0028310802299529314 max memory_allocated 29271.39501953125 
[2025-03-09 10:57:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.016873398795723915 norm:0.0023296086583286524 max memory_allocated 29271.39501953125 
[2025-03-09 10:58:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.01673242077231407 norm:0.002015496138483286 max memory_allocated 29271.39501953125 
[2025-03-09 10:59:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.016679735854268074 norm:0.0020032860338687897 max memory_allocated 29271.39501953125 
[2025-03-09 11:00:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.016629446297883987 norm:0.001959261717274785 max memory_allocated 29271.39501953125 
[2025-03-09 11:00:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.01663936674594879 norm:0.0017835720209404826 max memory_allocated 29271.39501953125 
[2025-03-09 11:01:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.01653899811208248 norm:0.001693827100098133 max memory_allocated 29271.39501953125 
[2025-03-09 11:02:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.016515739262104034 norm:0.0016410158714279532 max memory_allocated 29271.39501953125 
[2025-03-09 11:03:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.016543595120310783 norm:0.0016562974778935313 max memory_allocated 29271.39501953125 
[2025-03-09 11:04:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.01655544526875019 norm:0.0015676479088142514 max memory_allocated 29271.39501953125 
[2025-03-09 11:04:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.016575826331973076 norm:0.0015634350711479783 max memory_allocated 29271.39501953125 
[2025-03-09 11:05:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.016594942659139633 norm:0.0014984575100243092 max memory_allocated 29271.39501953125 
[2025-03-09 11:06:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.016589492559432983 norm:0.0015084997285157442 max memory_allocated 29271.39501953125 
[2025-03-09 11:07:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.016606001183390617 norm:0.0014402125962078571 max memory_allocated 29271.39501953125 
[2025-03-09 11:07:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-09 11:08:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.05092295631766319 norm:0.005906213074922562 max memory_allocated 29271.39501953125 
[2025-03-09 11:09:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.036101363599300385 norm:0.0020884601399302483 max memory_allocated 29271.39501953125 
[2025-03-09 11:09:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.029653657227754593 norm:0.0013943956000730395 max memory_allocated 29271.39501953125 
[2025-03-09 11:10:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.02781546674668789 norm:0.0012959796003997326 max memory_allocated 29271.39501953125 
[2025-03-09 11:11:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.026315929368138313 norm:0.0011304819490760565 max memory_allocated 29271.39501953125 
[2025-03-09 11:12:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.02538548782467842 norm:0.0009506568312644958 max memory_allocated 29271.39501953125 
[2025-03-09 11:12:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.02485422044992447 norm:0.0008853615145199001 max memory_allocated 29271.39501953125 
[2025-03-09 11:13:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0254635252058506 norm:0.0010792934335768223 max memory_allocated 29271.39501953125 
[2025-03-09 11:14:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.02469075657427311 norm:0.0010728939669206738 max memory_allocated 29271.39501953125 
[2025-03-09 11:15:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.023685500025749207 norm:0.0007744893664494157 max memory_allocated 29271.39501953125 
[2025-03-09 11:15:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.023839760571718216 norm:0.0008026469731703401 max memory_allocated 29271.39501953125 
[2025-03-09 11:16:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.023391876369714737 norm:0.0006695382762700319 max memory_allocated 29271.39501953125 
[2025-03-09 11:17:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.024453073740005493 norm:0.0009210665011778474 max memory_allocated 29271.39501953125 
[2025-03-09 11:18:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.02439774014055729 norm:0.0011656621936708689 max memory_allocated 29271.39501953125 
[2025-03-09 11:18:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.023916469886898994 norm:0.0010399469174444675 max memory_allocated 29271.39501953125 
[2025-03-09 11:19:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.024375049397349358 norm:0.0009886344196274877 max memory_allocated 29271.39501953125 
[2025-03-09 11:20:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.02408197522163391 norm:0.0009847016772255301 max memory_allocated 29271.39501953125 
[2025-03-09 11:21:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.023636091500520706 norm:0.0006895237020216882 max memory_allocated 29271.39501953125 
[2025-03-09 11:21:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.02366294153034687 norm:0.0006565451039932668 max memory_allocated 29271.39501953125 
[2025-03-09 11:22:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.02365584671497345 norm:0.0009385644225403666 max memory_allocated 29271.39501953125 
[2025-03-09 11:22:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-09 11:23:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.04013359174132347 norm:0.000758579873945564 max memory_allocated 29271.39501953125 
[2025-03-09 11:24:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.03204160928726196 norm:0.00031234940979629755 max memory_allocated 29271.39501953125 
[2025-03-09 11:25:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.02763231471180916 norm:0.00018544730846770108 max memory_allocated 29271.39501953125 
[2025-03-09 11:26:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.02613484300673008 norm:0.00015589460963383317 max memory_allocated 29271.39501953125 
[2025-03-09 11:26:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.025402527302503586 norm:0.00016272664652206004 max memory_allocated 29271.39501953125 
[2025-03-09 11:27:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.02489728294312954 norm:0.00014217454008758068 max memory_allocated 29271.39501953125 
[2025-03-09 11:28:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.024547751992940903 norm:0.00013200646208133548 max memory_allocated 29271.39501953125 
[2025-03-09 11:29:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.024365656077861786 norm:0.00012522311590146273 max memory_allocated 29271.39501953125 
[2025-03-09 11:29:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.024273203685879707 norm:0.0001227127795573324 max memory_allocated 29271.39501953125 
[2025-03-09 11:30:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.02422976680099964 norm:0.0001201001723529771 max memory_allocated 29271.39501953125 
[2025-03-09 11:31:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.024180589243769646 norm:0.0001199608959723264 max memory_allocated 29271.39501953125 
[2025-03-09 11:32:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0241546668112278 norm:0.00012160291953478009 max memory_allocated 29271.39501953125 
[2025-03-09 11:32:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.024121392518281937 norm:0.00011817044287454337 max memory_allocated 29271.39501953125 
[2025-03-09 11:33:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.02414439246058464 norm:0.00012817067909054458 max memory_allocated 29271.39501953125 
[2025-03-09 11:34:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0241532102227211 norm:0.00013441366900224239 max memory_allocated 29271.39501953125 
[2025-03-09 11:35:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.02413109503686428 norm:0.00012961991888005286 max memory_allocated 29271.39501953125 
[2025-03-09 11:35:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.024128897115588188 norm:0.00014703055785503238 max memory_allocated 29271.39501953125 
[2025-03-09 11:36:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.02414971962571144 norm:0.000135012436658144 max memory_allocated 29271.39501953125 
[2025-03-09 11:37:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.02412765473127365 norm:0.00012894527753815055 max memory_allocated 29271.39501953125 
[2025-03-09 11:38:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.024129917845129967 norm:0.0001315627305302769 max memory_allocated 29271.39501953125 
[2025-03-09 11:38:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-09 11:39:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.044568754732608795 norm:0.0012029833160340786 max memory_allocated 29271.81298828125 
[2025-03-09 11:40:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0349537655711174 norm:0.00043764739530161023 max memory_allocated 29271.81298828125 
[2025-03-09 11:40:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.030243132263422012 norm:0.0002327121328562498 max memory_allocated 29271.81298828125 
[2025-03-09 11:41:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.028696633875370026 norm:0.0001640371629036963 max memory_allocated 29271.81298828125 
[2025-03-09 11:42:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.027796704322099686 norm:0.0001383616036036983 max memory_allocated 29271.81298828125 
[2025-03-09 11:43:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.027223479002714157 norm:0.00012462017184589058 max memory_allocated 29271.81298828125 
[2025-03-09 11:43:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.026902606710791588 norm:0.00011382478260202333 max memory_allocated 29271.81298828125 
[2025-03-09 11:44:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.02673625759780407 norm:0.00011368074046913534 max memory_allocated 29271.81298828125 
[2025-03-09 11:45:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.026632439345121384 norm:0.00010636464139679447 max memory_allocated 29271.81298828125 
[2025-03-09 11:46:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.02657126635313034 norm:0.00010346720955567434 max memory_allocated 29271.81298828125 
[2025-03-09 11:46:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.026522314175963402 norm:0.000101464829640463 max memory_allocated 29271.81298828125 
[2025-03-09 11:47:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.02647809125483036 norm:0.00010205940634477884 max memory_allocated 29271.81298828125 
[2025-03-09 11:48:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.02647903747856617 norm:0.00010151635797228664 max memory_allocated 29271.81298828125 
[2025-03-09 11:49:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.026444699615240097 norm:9.967308869818226e-05 max memory_allocated 29271.81298828125 
[2025-03-09 11:49:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.026427261531352997 norm:0.00010338265565223992 max memory_allocated 29271.81298828125 
[2025-03-09 11:50:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.02640472538769245 norm:9.992667037295178e-05 max memory_allocated 29271.81298828125 
[2025-03-09 11:51:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.026400266215205193 norm:0.00010137264325749129 max memory_allocated 29271.81298828125 
[2025-03-09 11:52:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.02639143168926239 norm:0.00010133960313396528 max memory_allocated 29271.81298828125 
[2025-03-09 11:52:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.02636466547846794 norm:0.00010070474672829732 max memory_allocated 29271.81298828125 
[2025-03-09 11:53:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.026352861896157265 norm:0.00010335357364965603 max memory_allocated 29271.81298828125 
[2025-03-09 11:53:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-09 11:54:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.04869399219751358 norm:0.0014749857364222407 max memory_allocated 29271.81298828125 
[2025-03-09 11:55:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.03875738009810448 norm:0.0006029108772054315 max memory_allocated 29271.81298828125 
[2025-03-09 11:56:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.03317675739526749 norm:0.0003286497958470136 max memory_allocated 29271.81298828125 
[2025-03-09 11:57:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.031440895050764084 norm:0.0002140910510206595 max memory_allocated 29271.81298828125 
[2025-03-09 11:57:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.03045932576060295 norm:0.00018382156849838793 max memory_allocated 29271.81298828125 
[2025-03-09 11:58:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.029870903119444847 norm:0.00015933936811052263 max memory_allocated 29271.81298828125 
[2025-03-09 11:59:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.029540667310357094 norm:0.0001511728623881936 max memory_allocated 29271.81298828125 
[2025-03-09 12:00:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.029360756278038025 norm:0.0001377520093228668 max memory_allocated 29271.81298828125 
[2025-03-09 12:00:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.02927509881556034 norm:0.00013622609549202025 max memory_allocated 29271.81298828125 
[2025-03-09 12:01:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.029187869280576706 norm:0.00012979947496205568 max memory_allocated 29271.81298828125 
[2025-03-09 12:02:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.029148202389478683 norm:0.00012528714432846755 max memory_allocated 29271.81298828125 
[2025-03-09 12:03:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.029120057821273804 norm:0.00012972168042324483 max memory_allocated 29271.81298828125 
[2025-03-09 12:03:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.02910161390900612 norm:0.00012069737567799166 max memory_allocated 29271.81298828125 
[2025-03-09 12:04:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.029061630368232727 norm:0.00011852887109853327 max memory_allocated 29271.81298828125 
[2025-03-09 12:05:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.029043054208159447 norm:0.0001199343751068227 max memory_allocated 29271.81298828125 
[2025-03-09 12:06:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.02902885340154171 norm:0.00011751273268600926 max memory_allocated 29271.81298828125 
[2025-03-09 12:06:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.029021916911005974 norm:0.0001140535605372861 max memory_allocated 29271.81298828125 
[2025-03-09 12:07:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.029027268290519714 norm:0.00012210024578962475 max memory_allocated 29271.81298828125 
[2025-03-09 12:08:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.029012085869908333 norm:0.00011733233986888081 max memory_allocated 29271.81298828125 
[2025-03-09 12:09:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.02900567278265953 norm:0.00011823108798125759 max memory_allocated 29271.81298828125 
[2025-03-09 12:09:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-09 12:10:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.05981820076704025 norm:0.001957317115738988 max memory_allocated 29271.81298828125 
[2025-03-09 12:11:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.04504169523715973 norm:0.0007080501527525485 max memory_allocated 29271.81298828125 
[2025-03-09 12:11:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.037641528993844986 norm:0.0003381618298590183 max memory_allocated 29271.81298828125 
[2025-03-09 12:12:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.035532932728528976 norm:0.00026555737713351846 max memory_allocated 29271.81298828125 
[2025-03-09 12:13:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.03447926789522171 norm:0.00022667011944577098 max memory_allocated 29271.81298828125 
[2025-03-09 12:14:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.03379366174340248 norm:0.00020621053408831358 max memory_allocated 29271.81298828125 
[2025-03-09 12:14:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.03336072713136673 norm:0.00018835537775885314 max memory_allocated 29271.81298828125 
[2025-03-09 12:15:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.03315596282482147 norm:0.000193323619896546 max memory_allocated 29271.81298828125 
[2025-03-09 12:16:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.033012498170137405 norm:0.00018631598504725844 max memory_allocated 29271.81298828125 
[2025-03-09 12:17:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.03287925571203232 norm:0.00017950270557776093 max memory_allocated 29271.81298828125 
[2025-03-09 12:17:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.03280062973499298 norm:0.00017797102918848395 max memory_allocated 29271.81298828125 
[2025-03-09 12:18:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.03273576498031616 norm:0.00017067152657546103 max memory_allocated 29271.81298828125 
[2025-03-09 12:19:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.03266626596450806 norm:0.0001719669089652598 max memory_allocated 29271.81298828125 
[2025-03-09 12:20:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.03262678161263466 norm:0.00017034754273481667 max memory_allocated 29271.81298828125 
[2025-03-09 12:20:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.03259969875216484 norm:0.00016821899043861777 max memory_allocated 29271.81298828125 
[2025-03-09 12:21:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.032581791281700134 norm:0.00017083066632039845 max memory_allocated 29271.81298828125 
[2025-03-09 12:22:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0325760543346405 norm:0.00017145267338491976 max memory_allocated 29271.81298828125 
[2025-03-09 12:23:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.03255538269877434 norm:0.00016775299445725977 max memory_allocated 29271.81298828125 
[2025-03-09 12:23:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.03254244476556778 norm:0.0001635148364584893 max memory_allocated 29271.81298828125 
[2025-03-09 12:24:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.03253850340843201 norm:0.00016415031859651208 max memory_allocated 29271.81298828125 
[2025-03-09 12:24:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-09 12:25:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.060556039214134216 norm:0.002659276593476534 max memory_allocated 29272.37548828125 
[2025-03-09 12:26:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.04779370501637459 norm:0.001072628190740943 max memory_allocated 29272.37548828125 
[2025-03-09 12:27:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.040492940694093704 norm:0.0005232857074588537 max memory_allocated 29272.37548828125 
[2025-03-09 12:28:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0382535383105278 norm:0.00036578308208845556 max memory_allocated 29272.37548828125 
[2025-03-09 12:28:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.037015803158283234 norm:0.0002883719280362129 max memory_allocated 29272.37548828125 
[2025-03-09 12:29:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0362941212952137 norm:0.00024722309899516404 max memory_allocated 29272.37548828125 
[2025-03-09 12:30:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.035881031304597855 norm:0.00021616551384795457 max memory_allocated 29272.37548828125 
[2025-03-09 12:31:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.035660356283187866 norm:0.00019915684242732823 max memory_allocated 29272.37548828125 
[2025-03-09 12:31:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.03548738360404968 norm:0.00017409450083505362 max memory_allocated 29272.37548828125 
[2025-03-09 12:32:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.03536975383758545 norm:0.00015731713210698217 max memory_allocated 29272.37548828125 
[2025-03-09 12:33:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.035269297659397125 norm:0.00014538269897457212 max memory_allocated 29272.37548828125 
[2025-03-09 12:34:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0352114662528038 norm:0.00014278761227615178 max memory_allocated 29272.37548828125 
[2025-03-09 12:34:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.035155825316905975 norm:0.00013987149577587843 max memory_allocated 29272.37548828125 
[2025-03-09 12:35:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.03511974960565567 norm:0.00013107608538120985 max memory_allocated 29272.37548828125 
[2025-03-09 12:36:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.03509902209043503 norm:0.00012858309492003173 max memory_allocated 29272.37548828125 
[2025-03-09 12:37:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.03508792445063591 norm:0.0001297460257774219 max memory_allocated 29272.37548828125 
[2025-03-09 12:37:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.035080134868621826 norm:0.0001324936019955203 max memory_allocated 29272.37548828125 
[2025-03-09 12:38:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.035051129758358 norm:0.00012415183300618082 max memory_allocated 29272.37548828125 
[2025-03-09 12:39:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.035009145736694336 norm:0.00012109697127016261 max memory_allocated 29272.37548828125 
[2025-03-09 12:40:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.034986477345228195 norm:0.00012404097651597112 max memory_allocated 29272.37548828125 
[2025-03-09 12:40:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-09 12:41:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.08839047700166702 norm:0.003985055256634951 max memory_allocated 29272.56298828125 
[2025-03-09 12:42:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.06100917607545853 norm:0.0014694007113575935 max memory_allocated 29272.56298828125 
[2025-03-09 12:42:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.04788251221179962 norm:0.0006417753756977618 max memory_allocated 29272.56298828125 
[2025-03-09 12:43:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.04428783059120178 norm:0.0004318842547945678 max memory_allocated 29272.56298828125 
[2025-03-09 12:44:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.04260031506419182 norm:0.00036306524998508394 max memory_allocated 29272.56298828125 
[2025-03-09 12:45:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.041534483432769775 norm:0.00032293327967636287 max memory_allocated 29272.56298828125 
[2025-03-09 12:45:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0409114733338356 norm:0.00029886607080698013 max memory_allocated 29272.56298828125 
[2025-03-09 12:46:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.04049123451113701 norm:0.0002763570810202509 max memory_allocated 29272.56298828125 
[2025-03-09 12:47:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.04023042693734169 norm:0.0002670985704753548 max memory_allocated 29272.56298828125 
[2025-03-09 12:48:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.03997381031513214 norm:0.0002500248374417424 max memory_allocated 29272.56298828125 
[2025-03-09 12:48:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.039748191833496094 norm:0.0002266575611429289 max memory_allocated 29272.56298828125 
[2025-03-09 12:49:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.039601363241672516 norm:0.00021782539261039346 max memory_allocated 29272.56298828125 
[2025-03-09 12:50:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.039469607174396515 norm:0.00020993902580812573 max memory_allocated 29272.56298828125 
[2025-03-09 12:51:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.03938119113445282 norm:0.00019933610747102648 max memory_allocated 29272.56298828125 
[2025-03-09 12:51:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.039314813911914825 norm:0.00019358527788426727 max memory_allocated 29272.56298828125 
[2025-03-09 12:52:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.039260488003492355 norm:0.0001999040978262201 max memory_allocated 29272.56298828125 
[2025-03-09 12:53:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0391688272356987 norm:0.0001954210747499019 max memory_allocated 29272.56298828125 
[2025-03-09 12:54:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.03907858207821846 norm:0.00018312109750695527 max memory_allocated 29272.56298828125 
[2025-03-09 12:54:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.03904033452272415 norm:0.00017857240163721144 max memory_allocated 29272.56298828125 
[2025-03-09 12:55:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.038996484130620956 norm:0.00017713950364850461 max memory_allocated 29272.56298828125 
[2025-03-09 12:55:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-09 12:56:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0729123130440712 norm:0.002441776916384697 max memory_allocated 29272.75048828125 
[2025-03-09 12:57:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.057451680302619934 norm:0.0009794245706871152 max memory_allocated 29272.75048828125 
[2025-03-09 12:58:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.04863576591014862 norm:0.0004707331827376038 max memory_allocated 29272.75048828125 
[2025-03-09 12:59:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.04569493234157562 norm:0.00031892384868115187 max memory_allocated 29272.75048828125 
[2025-03-09 12:59:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.04434937611222267 norm:0.0002582179440651089 max memory_allocated 29272.75048828125 
[2025-03-09 13:00:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.043520793318748474 norm:0.00022603428806178272 max memory_allocated 29272.75048828125 
[2025-03-09 13:01:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.04309307411313057 norm:0.00020427966956049204 max memory_allocated 29272.75048828125 
[2025-03-09 13:02:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.04286516457796097 norm:0.0001946079864865169 max memory_allocated 29272.75048828125 
[2025-03-09 13:02:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.04267106577754021 norm:0.00017441334784962237 max memory_allocated 29272.75048828125 
[2025-03-09 13:03:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.04252588748931885 norm:0.00016777920245658606 max memory_allocated 29272.75048828125 
[2025-03-09 13:04:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.04244381561875343 norm:0.00016457225137855858 max memory_allocated 29272.75048828125 
[2025-03-09 13:05:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.04235931858420372 norm:0.00015789084136486053 max memory_allocated 29272.75048828125 
[2025-03-09 13:05:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.042269956320524216 norm:0.0001464310917071998 max memory_allocated 29272.75048828125 
[2025-03-09 13:06:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.04221431538462639 norm:0.00014265376375988126 max memory_allocated 29272.75048828125 
[2025-03-09 13:07:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.04218393191695213 norm:0.00013984862016513944 max memory_allocated 29272.75048828125 
[2025-03-09 13:08:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.042166706174612045 norm:0.00014006622950546443 max memory_allocated 29272.75048828125 
[2025-03-09 13:08:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.042138513177633286 norm:0.00013152058818377554 max memory_allocated 29272.75048828125 
[2025-03-09 13:09:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0420939140021801 norm:0.00012640382919926196 max memory_allocated 29272.75048828125 
[2025-03-09 13:10:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.04207363724708557 norm:0.00012906716438010335 max memory_allocated 29272.75048828125 
[2025-03-09 13:11:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.04207860305905342 norm:0.00012950618111062795 max memory_allocated 29272.75048828125 
[2025-03-09 13:11:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-09 13:12:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.07170750945806503 norm:0.0017384807579219341 max memory_allocated 29272.93798828125 
[2025-03-09 13:13:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.05798722803592682 norm:0.0006793349166400731 max memory_allocated 29272.93798828125 
[2025-03-09 13:13:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.05034409090876579 norm:0.0003592614666558802 max memory_allocated 29272.93798828125 
[2025-03-09 13:14:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.04784189909696579 norm:0.0002431548637105152 max memory_allocated 29272.93798828125 
[2025-03-09 13:15:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.046699948608875275 norm:0.00020618943381123245 max memory_allocated 29272.93798828125 
[2025-03-09 13:16:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.046015407890081406 norm:0.00018004262528847903 max memory_allocated 29272.93798828125 
[2025-03-09 13:16:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.04566571116447449 norm:0.00016988912830129266 max memory_allocated 29272.93798828125 
[2025-03-09 13:17:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.045464806258678436 norm:0.00016319952555932105 max memory_allocated 29272.93798828125 
[2025-03-09 13:18:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.04531335458159447 norm:0.00015509413788095117 max memory_allocated 29272.93798828125 
[2025-03-09 13:19:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.045182351022958755 norm:0.00014349358389154077 max memory_allocated 29272.93798828125 
[2025-03-09 13:19:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.045099563896656036 norm:0.0001420134649379179 max memory_allocated 29272.93798828125 
[2025-03-09 13:20:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.04501459002494812 norm:0.00013196762301959097 max memory_allocated 29272.93798828125 
[2025-03-09 13:21:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.044954270124435425 norm:0.00012777262600138783 max memory_allocated 29272.93798828125 
[2025-03-09 13:22:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.044885218143463135 norm:0.00012520301970653236 max memory_allocated 29272.93798828125 
[2025-03-09 13:22:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.04484991729259491 norm:0.0001215849697473459 max memory_allocated 29272.93798828125 
[2025-03-09 13:23:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.04481180012226105 norm:0.00012055771367158741 max memory_allocated 29272.93798828125 
[2025-03-09 13:24:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.04479493200778961 norm:0.00011950712359976023 max memory_allocated 29272.93798828125 
[2025-03-09 13:25:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.04477269947528839 norm:0.00011828182323370129 max memory_allocated 29272.93798828125 
[2025-03-09 13:25:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.044729363173246384 norm:0.00011423318210290745 max memory_allocated 29272.93798828125 
[2025-03-09 13:26:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.04472140222787857 norm:0.00011805979738710448 max memory_allocated 29272.93798828125 
[2025-03-09 13:26:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-09 13:27:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.07839592546224594 norm:0.0019491544226184487 max memory_allocated 29273.12548828125 
[2025-03-09 13:28:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.06218402460217476 norm:0.0008253453997895122 max memory_allocated 29273.12548828125 
[2025-03-09 13:29:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.05308231711387634 norm:0.0003971128026023507 max memory_allocated 29273.12548828125 
[2025-03-09 13:30:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.05029306933283806 norm:0.0002685534709598869 max memory_allocated 29273.12548828125 
[2025-03-09 13:30:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.04902375489473343 norm:0.0002181461895816028 max memory_allocated 29273.12548828125 
[2025-03-09 13:31:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.048299387097358704 norm:0.00019100801728200167 max memory_allocated 29273.12548828125 
[2025-03-09 13:32:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.047873690724372864 norm:0.00017073882918339223 max memory_allocated 29273.12548828125 
[2025-03-09 13:33:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.04761161655187607 norm:0.0001574760681251064 max memory_allocated 29273.12548828125 
[2025-03-09 13:33:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.04742142930626869 norm:0.00014765701780561358 max memory_allocated 29273.12548828125 
[2025-03-09 13:34:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.047268372029066086 norm:0.00013890101399738342 max memory_allocated 29273.12548828125 
[2025-03-09 13:35:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.04715198278427124 norm:0.0001318676077062264 max memory_allocated 29273.12548828125 
[2025-03-09 13:36:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.047096122056245804 norm:0.0001328743528574705 max memory_allocated 29273.12548828125 
[2025-03-09 13:36:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.04704760015010834 norm:0.00013271045463625342 max memory_allocated 29273.12548828125 
[2025-03-09 13:37:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.046991508454084396 norm:0.0001247880281880498 max memory_allocated 29273.12548828125 
[2025-03-09 13:38:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.046947263181209564 norm:0.00012277411588001996 max memory_allocated 29273.12548828125 
[2025-03-09 13:39:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.046885281801223755 norm:0.00011379052739357576 max memory_allocated 29273.12548828125 
[2025-03-09 13:39:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.046844206750392914 norm:0.0001091735321097076 max memory_allocated 29273.12548828125 
[2025-03-09 13:40:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.046818267554044724 norm:0.00010932736040558666 max memory_allocated 29273.12548828125 
[2025-03-09 13:41:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.046819791197776794 norm:0.00010671012569218874 max memory_allocated 29273.12548828125 
[2025-03-09 13:42:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.04680102318525314 norm:0.00010745406325440854 max memory_allocated 29273.12548828125 
[2025-03-09 13:42:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-09 13:43:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.07663267850875854 norm:0.001953394617885351 max memory_allocated 29273.31298828125 
[2025-03-09 13:44:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.06289558112621307 norm:0.0008118236437439919 max memory_allocated 29273.31298828125 
[2025-03-09 13:44:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.055149998515844345 norm:0.0004264812159817666 max memory_allocated 29273.31298828125 
[2025-03-09 13:45:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.052496474236249924 norm:0.0003030271327588707 max memory_allocated 29273.31298828125 
[2025-03-09 13:46:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.05121011286973953 norm:0.00024628982646390796 max memory_allocated 29273.31298828125 
[2025-03-09 13:47:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.050401873886585236 norm:0.00021417970128823072 max memory_allocated 29273.31298828125 
[2025-03-09 13:48:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.049975134432315826 norm:0.00020065061107743531 max memory_allocated 29273.31298828125 
[2025-03-09 13:48:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.04965199902653694 norm:0.0001879577903309837 max memory_allocated 29273.31298828125 
[2025-03-09 13:49:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.049435440450906754 norm:0.00017920655955094844 max memory_allocated 29273.31298828125 
[2025-03-09 13:50:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.04927733540534973 norm:0.00016582649550400674 max memory_allocated 29273.31298828125 
[2025-03-09 13:51:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.04918405041098595 norm:0.00015653885202482343 max memory_allocated 29273.31298828125 
[2025-03-09 13:51:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.04908265545964241 norm:0.000145844736834988 max memory_allocated 29273.31298828125 
[2025-03-09 13:52:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.04901277646422386 norm:0.00013951040455140173 max memory_allocated 29273.31298828125 
[2025-03-09 13:53:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.0489589162170887 norm:0.00013460744230542332 max memory_allocated 29273.31298828125 
[2025-03-09 13:54:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.04888840764760971 norm:0.00012553752458188683 max memory_allocated 29273.31298828125 
[2025-03-09 13:54:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.04883524775505066 norm:0.00012022779264952987 max memory_allocated 29273.31298828125 
[2025-03-09 13:55:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.04877384006977081 norm:0.00011684773198794574 max memory_allocated 29273.31298828125 
[2025-03-09 13:56:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.048742569983005524 norm:0.0001161325053544715 max memory_allocated 29273.31298828125 
[2025-03-09 13:57:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0487283356487751 norm:0.00011109984188806266 max memory_allocated 29273.31298828125 
[2025-03-09 13:57:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.048726268112659454 norm:0.00010651631600921974 max memory_allocated 29273.31298828125 
[2025-03-09 13:58:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-09 13:58:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.07981777936220169 norm:0.0018683790694922209 max memory_allocated 29273.50048828125 
[2025-03-09 13:59:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.06658896803855896 norm:0.0008721818448975682 max memory_allocated 29273.50048828125 
[2025-03-09 14:00:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.05791947990655899 norm:0.00045219616731628776 max memory_allocated 29273.50048828125 
[2025-03-09 14:01:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.05490640923380852 norm:0.0002966851752717048 max memory_allocated 29273.50048828125 
[2025-03-09 14:02:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.053674545139074326 norm:0.00024089125508908182 max memory_allocated 29273.50048828125 
[2025-03-09 14:02:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.052937813103199005 norm:0.00021258636843413115 max memory_allocated 29273.50048828125 
[2025-03-09 14:03:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0524844266474247 norm:0.0001926527766045183 max memory_allocated 29273.50048828125 
[2025-03-09 14:04:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.052198804914951324 norm:0.00017296997248195112 max memory_allocated 29273.50048828125 
[2025-03-09 14:05:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.052001792937517166 norm:0.00015937739226501435 max memory_allocated 29273.50048828125 
[2025-03-09 14:05:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.05186392739415169 norm:0.0001501501683378592 max memory_allocated 29273.50048828125 
[2025-03-09 14:06:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.05175452679395676 norm:0.000141622731462121 max memory_allocated 29273.50048828125 
[2025-03-09 14:07:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.05167900770902634 norm:0.00013633900380227715 max memory_allocated 29273.50048828125 
[2025-03-09 14:08:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.051604632288217545 norm:0.00013011711416766047 max memory_allocated 29273.50048828125 
[2025-03-09 14:08:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.05150240659713745 norm:0.00012170903210062534 max memory_allocated 29273.50048828125 
[2025-03-09 14:09:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.05141863599419594 norm:0.00011732493294402957 max memory_allocated 29273.50048828125 
[2025-03-09 14:10:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.05138102173805237 norm:0.00011365749378455803 max memory_allocated 29273.50048828125 
[2025-03-09 14:11:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.05133219063282013 norm:0.00010888272663578391 max memory_allocated 29273.50048828125 
[2025-03-09 14:11:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.051293522119522095 norm:0.00010705634485930204 max memory_allocated 29273.50048828125 
[2025-03-09 14:12:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.05126267299056053 norm:0.00010303184535587206 max memory_allocated 29273.50048828125 
[2025-03-09 14:13:22 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.05124088004231453 norm:0.00010167076834477484 max memory_allocated 29273.50048828125 
[2025-03-09 14:13:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-09 14:14:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.0772731751203537 norm:0.0022079679183661938 max memory_allocated 29273.68798828125 
[2025-03-09 14:15:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.06570658087730408 norm:0.0008684545173309743 max memory_allocated 29273.68798828125 
[2025-03-09 14:16:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0579087994992733 norm:0.00040331316995434463 max memory_allocated 29273.68798828125 
[2025-03-09 14:16:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.055310674011707306 norm:0.00025291836936958134 max memory_allocated 29273.68798828125 
[2025-03-09 14:17:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.05422407388687134 norm:0.00021577021107077599 max memory_allocated 29273.68798828125 
[2025-03-09 14:18:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.05355190485715866 norm:0.00019341171719133854 max memory_allocated 29273.68798828125 
[2025-03-09 14:19:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.05313187837600708 norm:0.00017158317496068776 max memory_allocated 29273.68798828125 
[2025-03-09 14:19:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.052830442786216736 norm:0.0001534034963697195 max memory_allocated 29273.68798828125 
[2025-03-09 14:20:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.052656516432762146 norm:0.0001506131811765954 max memory_allocated 29273.68798828125 
[2025-03-09 14:21:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.05254840478301048 norm:0.00014259234012570232 max memory_allocated 29273.68798828125 
[2025-03-09 14:22:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.05243945121765137 norm:0.00013323033635970205 max memory_allocated 29273.68798828125 
[2025-03-09 14:22:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.05236935615539551 norm:0.0001270989450858906 max memory_allocated 29273.68798828125 
[2025-03-09 14:23:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.05231679975986481 norm:0.00012079882435500622 max memory_allocated 29273.68798828125 
[2025-03-09 14:24:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.052278101444244385 norm:0.00012090837844880298 max memory_allocated 29273.68798828125 
[2025-03-09 14:25:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.05223417282104492 norm:0.00011320765770506114 max memory_allocated 29273.68798828125 
[2025-03-09 14:25:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.05216382071375847 norm:0.00010767798812594265 max memory_allocated 29273.68798828125 
[2025-03-09 14:26:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.05211155489087105 norm:0.00010494489833945408 max memory_allocated 29273.68798828125 
[2025-03-09 14:27:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.052078355103731155 norm:0.00010320734145352617 max memory_allocated 29273.68798828125 
[2025-03-09 14:28:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.052076034247875214 norm:0.00010108089190907776 max memory_allocated 29273.68798828125 
[2025-03-09 14:28:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.05205703526735306 norm:9.743987902766094e-05 max memory_allocated 29273.68798828125 
[2025-03-09 14:29:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-09 14:30:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.08646803349256516 norm:0.004202714655548334 max memory_allocated 29273.87548828125 
[2025-03-09 14:30:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.07089442014694214 norm:0.001484051113948226 max memory_allocated 29273.87548828125 
[2025-03-09 14:31:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.06112923473119736 norm:0.0005976561806164682 max memory_allocated 29273.87548828125 
[2025-03-09 14:32:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.05800189822912216 norm:0.00038338106242008507 max memory_allocated 29273.87548828125 
[2025-03-09 14:33:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.056630510836839676 norm:0.00030505418544635177 max memory_allocated 29273.87548828125 
[2025-03-09 14:33:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.05584331601858139 norm:0.000282693887129426 max memory_allocated 29273.87548828125 
[2025-03-09 14:34:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.0553152859210968 norm:0.0002587239723652601 max memory_allocated 29273.87548828125 
[2025-03-09 14:35:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.05493494123220444 norm:0.0002382938255323097 max memory_allocated 29273.87548828125 
[2025-03-09 14:36:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.05467507988214493 norm:0.00022363611788023263 max memory_allocated 29273.87548828125 
[2025-03-09 14:36:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.05449306219816208 norm:0.0002111796202370897 max memory_allocated 29273.87548828125 
[2025-03-09 14:37:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.054343756288290024 norm:0.00019860309839714319 max memory_allocated 29273.87548828125 
[2025-03-09 14:38:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.05426140874624252 norm:0.00019292699289508164 max memory_allocated 29273.87548828125 
[2025-03-09 14:39:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.05419164523482323 norm:0.0001893791340989992 max memory_allocated 29273.87548828125 
[2025-03-09 14:39:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.054106615483760834 norm:0.00018269571592099965 max memory_allocated 29273.87548828125 
[2025-03-09 14:40:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.05400947108864784 norm:0.0001702070003375411 max memory_allocated 29273.87548828125 
[2025-03-09 14:41:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.053930219262838364 norm:0.00015915636322461069 max memory_allocated 29273.87548828125 
[2025-03-09 14:42:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.053868260234594345 norm:0.00015238802006933838 max memory_allocated 29273.87548828125 
[2025-03-09 14:42:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.05382315814495087 norm:0.0001478589983889833 max memory_allocated 29273.87548828125 
[2025-03-09 14:43:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.05378759652376175 norm:0.0001432912249583751 max memory_allocated 29273.87548828125 
[2025-03-09 14:44:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.05375903099775314 norm:0.00013743843010161072 max memory_allocated 29273.87548828125 
[2025-03-09 14:44:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-09 14:45:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.078646719455719 norm:0.0029926884453743696 max memory_allocated 29274.06298828125 
[2025-03-09 14:46:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.06698356568813324 norm:0.000899149221368134 max memory_allocated 29274.06298828125 
[2025-03-09 14:47:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.06002151593565941 norm:0.0003798655525315553 max memory_allocated 29274.06298828125 
[2025-03-09 14:47:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.05779261142015457 norm:0.0002509877667762339 max memory_allocated 29274.06298828125 
[2025-03-09 14:48:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.05674489960074425 norm:0.0002128495543729514 max memory_allocated 29274.06298828125 
[2025-03-09 14:49:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.056039683520793915 norm:0.00019599440565798432 max memory_allocated 29274.06298828125 
[2025-03-09 14:50:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.055642105638980865 norm:0.00019303559383843094 max memory_allocated 29274.06298828125 
[2025-03-09 14:50:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0553838312625885 norm:0.00017817017214838415 max memory_allocated 29274.06298828125 
[2025-03-09 14:51:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.055220600217580795 norm:0.0001687220938038081 max memory_allocated 29274.06298828125 
[2025-03-09 14:52:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.055078983306884766 norm:0.00015737221110612154 max memory_allocated 29274.06298828125 
[2025-03-09 14:53:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.05498933792114258 norm:0.00015017700206954032 max memory_allocated 29274.06298828125 
[2025-03-09 14:53:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.054908499121665955 norm:0.00014545831072609872 max memory_allocated 29274.06298828125 
[2025-03-09 14:54:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.05485614389181137 norm:0.00013864552602171898 max memory_allocated 29274.06298828125 
[2025-03-09 14:55:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.05479753017425537 norm:0.00013164528354536742 max memory_allocated 29274.06298828125 
[2025-03-09 14:56:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.05473135784268379 norm:0.0001235684030689299 max memory_allocated 29274.06298828125 
[2025-03-09 14:56:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.054691724479198456 norm:0.00011976634414168075 max memory_allocated 29274.06298828125 
[2025-03-09 14:57:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.05466461181640625 norm:0.00011710472608683631 max memory_allocated 29274.06298828125 
[2025-03-09 14:58:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.05463508144021034 norm:0.00011360885400790721 max memory_allocated 29274.06298828125 
[2025-03-09 14:59:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.05461818724870682 norm:0.00010960192594211549 max memory_allocated 29274.06298828125 
[2025-03-09 14:59:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.05460134893655777 norm:0.00010614780330797657 max memory_allocated 29274.06298828125 
[2025-03-09 15:00:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-09 15:01:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.07769127190113068 norm:0.0030200714245438576 max memory_allocated 29274.25048828125 
[2025-03-09 15:01:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.06647097319364548 norm:0.0007844561478123069 max memory_allocated 29274.25048828125 
[2025-03-09 15:02:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.06071871146559715 norm:0.0003575375012587756 max memory_allocated 29274.25048828125 
[2025-03-09 15:03:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.05895073339343071 norm:0.0002461243129801005 max memory_allocated 29274.25048828125 
[2025-03-09 15:04:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.05796399712562561 norm:0.00020611879881471395 max memory_allocated 29274.25048828125 
[2025-03-09 15:04:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.05729632452130318 norm:0.00018886863836087286 max memory_allocated 29274.25048828125 
[2025-03-09 15:05:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.05686371028423309 norm:0.00017241672321688384 max memory_allocated 29274.25048828125 
[2025-03-09 15:06:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.05660584941506386 norm:0.00015822656860109419 max memory_allocated 29274.25048828125 
[2025-03-09 15:07:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.05642818659543991 norm:0.00014915189240127802 max memory_allocated 29274.25048828125 
[2025-03-09 15:07:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.05632202699780464 norm:0.00014343508519232273 max memory_allocated 29274.25048828125 
[2025-03-09 15:08:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.05623188987374306 norm:0.00013289539492689073 max memory_allocated 29274.25048828125 
[2025-03-09 15:09:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.05616876855492592 norm:0.00012854611850343645 max memory_allocated 29274.25048828125 
[2025-03-09 15:10:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.05609719082713127 norm:0.00012262709788046777 max memory_allocated 29274.25048828125 
[2025-03-09 15:10:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.056060321629047394 norm:0.00011915563663933426 max memory_allocated 29274.25048828125 
[2025-03-09 15:11:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.056029390543699265 norm:0.00011070091568399221 max memory_allocated 29274.25048828125 
[2025-03-09 15:12:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.05598095804452896 norm:0.00010752697562566027 max memory_allocated 29274.25048828125 
[2025-03-09 15:13:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.05594493821263313 norm:0.00010536849731579423 max memory_allocated 29274.25048828125 
[2025-03-09 15:13:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.05590640380978584 norm:0.00010107144771609455 max memory_allocated 29274.25048828125 
[2025-03-09 15:14:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.05588381737470627 norm:9.495601989328861e-05 max memory_allocated 29274.25048828125 
[2025-03-09 15:15:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.05586748570203781 norm:9.334383503301069e-05 max memory_allocated 29274.25048828125 
[2025-03-09 15:15:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-09 15:16:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.07852565497159958 norm:0.002340711420401931 max memory_allocated 29274.43798828125 
[2025-03-09 15:17:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.06938101351261139 norm:0.0007242858409881592 max memory_allocated 29274.43798828125 
[2025-03-09 15:18:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.06364968419075012 norm:0.0003522034967318177 max memory_allocated 29274.43798828125 
[2025-03-09 15:18:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.061843812465667725 norm:0.0002435011847410351 max memory_allocated 29274.43798828125 
[2025-03-09 15:19:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.06084195896983147 norm:0.0001985982817132026 max memory_allocated 29274.43798828125 
[2025-03-09 15:20:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.06014322116971016 norm:0.00017263928020838648 max memory_allocated 29274.43798828125 
[2025-03-09 15:21:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.05971427634358406 norm:0.0001604217104613781 max memory_allocated 29274.43798828125 
[2025-03-09 15:21:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.059496454894542694 norm:0.00015163724310696125 max memory_allocated 29274.43798828125 
[2025-03-09 15:22:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.05936543270945549 norm:0.00014571016072295606 max memory_allocated 29274.43798828125 
[2025-03-09 15:23:26 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.05923936516046524 norm:0.00013208523159846663 max memory_allocated 29274.43798828125 
[2025-03-09 15:24:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.05913587659597397 norm:0.00012581930786836892 max memory_allocated 29274.43798828125 
[2025-03-09 15:24:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.059058282524347305 norm:0.00011526388698257506 max memory_allocated 29274.43798828125 
[2025-03-09 15:25:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.058999091386795044 norm:0.00011039392848033458 max memory_allocated 29274.43798828125 
[2025-03-09 15:26:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.05894885212182999 norm:0.00010833106352947652 max memory_allocated 29274.43798828125 
[2025-03-09 15:27:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.058898963034152985 norm:0.00010125523112947121 max memory_allocated 29274.43798828125 
[2025-03-09 15:27:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.058876883238554 norm:0.0001021871721604839 max memory_allocated 29274.43798828125 
[2025-03-09 15:28:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.05885788053274155 norm:9.790019248612225e-05 max memory_allocated 29274.43798828125 
[2025-03-09 15:29:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.05882252752780914 norm:9.236073674401268e-05 max memory_allocated 29274.43798828125 
[2025-03-09 15:30:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.05879194661974907 norm:8.93450778676197e-05 max memory_allocated 29274.43798828125 
[2025-03-09 15:31:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.05876598879694939 norm:8.617022831458598e-05 max memory_allocated 29274.43798828125 
[2025-03-09 15:31:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-09 15:32:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.07999613881111145 norm:0.0019429399399086833 max memory_allocated 29274.62548828125 
[2025-03-09 15:32:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.0718376487493515 norm:0.0005572372465394437 max memory_allocated 29274.62548828125 
[2025-03-09 15:33:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.0665210634469986 norm:0.00029073949554003775 max memory_allocated 29274.62548828125 
[2025-03-09 15:34:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.06481920182704926 norm:0.00019713572692126036 max memory_allocated 29274.62548828125 
[2025-03-09 15:35:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.06387930363416672 norm:0.00016806888743303716 max memory_allocated 29274.62548828125 
[2025-03-09 15:35:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.06325964629650116 norm:0.00015350777539424598 max memory_allocated 29274.62548828125 
[2025-03-09 15:36:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.06286986172199249 norm:0.00014272275438997895 max memory_allocated 29274.62548828125 
[2025-03-09 15:37:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.06265098601579666 norm:0.00013582756218966097 max memory_allocated 29274.62548828125 
[2025-03-09 15:38:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.06250976026058197 norm:0.00012666211114265025 max memory_allocated 29274.62548828125 
[2025-03-09 15:38:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.06239050626754761 norm:0.00012086216156603768 max memory_allocated 29274.62548828125 
[2025-03-09 15:39:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.06231027469038963 norm:0.0001123522815760225 max memory_allocated 29274.62548828125 
[2025-03-09 15:40:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.06224343553185463 norm:0.00010579909576335922 max memory_allocated 29274.62548828125 
[2025-03-09 15:41:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.062187787145376205 norm:0.00010190885222982615 max memory_allocated 29274.62548828125 
[2025-03-09 15:41:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.06215281784534454 norm:9.785652218852192e-05 max memory_allocated 29274.62548828125 
[2025-03-09 15:42:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.06212200969457626 norm:9.487476199865341e-05 max memory_allocated 29274.62548828125 
[2025-03-09 15:43:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.062081847339868546 norm:9.095633868128061e-05 max memory_allocated 29274.62548828125 
[2025-03-09 15:44:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.06206800416111946 norm:8.733205322641879e-05 max memory_allocated 29274.62548828125 
[2025-03-09 15:44:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.062041837722063065 norm:8.643671753816307e-05 max memory_allocated 29274.62548828125 
[2025-03-09 15:45:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.06202177703380585 norm:8.559102570870891e-05 max memory_allocated 29274.62548828125 
[2025-03-09 15:46:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.062001992017030716 norm:8.194165275199339e-05 max memory_allocated 29274.62548828125 
[2025-03-09 15:46:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-09 15:47:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.08739440143108368 norm:0.0015753520419821143 max memory_allocated 29274.81298828125 
[2025-03-09 15:48:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.07868543267250061 norm:0.0005627241916954517 max memory_allocated 29274.81298828125 
[2025-03-09 15:49:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.07280582189559937 norm:0.00033330207224935293 max memory_allocated 29274.81298828125 
[2025-03-09 15:49:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.07086401432752609 norm:0.00021998111333232373 max memory_allocated 29274.81298828125 
[2025-03-09 15:50:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.06989127397537231 norm:0.00019491893181111664 max memory_allocated 29274.81298828125 
[2025-03-09 15:51:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.0692068412899971 norm:0.00017093148198910058 max memory_allocated 29274.81298828125 
[2025-03-09 15:52:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.06876247376203537 norm:0.00015913910465314984 max memory_allocated 29274.81298828125 
[2025-03-09 15:52:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.06850837171077728 norm:0.0001488679408794269 max memory_allocated 29274.81298828125 
[2025-03-09 15:53:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.06833232939243317 norm:0.0001413123682141304 max memory_allocated 29274.81298828125 
[2025-03-09 15:54:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.06822067499160767 norm:0.00013432258856482804 max memory_allocated 29274.81298828125 
[2025-03-09 15:55:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.06813871115446091 norm:0.0001279600546695292 max memory_allocated 29274.81298828125 
[2025-03-09 15:55:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.06806118786334991 norm:0.0001216486853081733 max memory_allocated 29274.81298828125 
[2025-03-09 15:56:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.06799913197755814 norm:0.00011845694825751707 max memory_allocated 29274.81298828125 
[2025-03-09 15:57:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.0679273009300232 norm:0.00011171517689945176 max memory_allocated 29274.81298828125 
[2025-03-09 15:58:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.06788270175457001 norm:0.00010947277041850612 max memory_allocated 29274.81298828125 
[2025-03-09 15:58:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.06784047931432724 norm:0.00010352260869694874 max memory_allocated 29274.81298828125 
[2025-03-09 15:59:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.06780993193387985 norm:0.00010338725405745208 max memory_allocated 29274.81298828125 
[2025-03-09 16:00:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.06778786331415176 norm:0.00010165738058276474 max memory_allocated 29274.81298828125 
[2025-03-09 16:01:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.06776798516511917 norm:0.00010143996769329533 max memory_allocated 29274.81298828125 
[2025-03-09 16:01:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.06773846596479416 norm:0.00010109695722348988 max memory_allocated 29274.81298828125 
[2025-03-09 16:02:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-09 16:03:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.08943170309066772 norm:0.0014400215586647391 max memory_allocated 29275.00048828125 
[2025-03-09 16:04:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.08248727023601532 norm:0.0004725049075204879 max memory_allocated 29275.00048828125 
[2025-03-09 16:04:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.07727186381816864 norm:0.00025007850490510464 max memory_allocated 29275.00048828125 
[2025-03-09 16:05:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.07593447715044022 norm:0.00017589279741514474 max memory_allocated 29275.00048828125 
[2025-03-09 16:06:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.07500254362821579 norm:0.00015216479368973523 max memory_allocated 29275.00048828125 
[2025-03-09 16:07:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.0743093490600586 norm:0.00013540094369091094 max memory_allocated 29275.00048828125 
[2025-03-09 16:07:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.07391808182001114 norm:0.00012484181206673384 max memory_allocated 29275.00048828125 
[2025-03-09 16:08:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.07371121644973755 norm:0.00011622795136645436 max memory_allocated 29275.00048828125 
[2025-03-09 16:09:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.07356573641300201 norm:0.00010742415179265663 max memory_allocated 29275.00048828125 
[2025-03-09 16:10:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.07346167415380478 norm:0.0001008856124826707 max memory_allocated 29275.00048828125 
[2025-03-09 16:10:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.07338239252567291 norm:9.75987350102514e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:11:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.07331909239292145 norm:9.226030670106411e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:12:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.07327896356582642 norm:9.066968050319701e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:13:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.07325148582458496 norm:8.868185977917165e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:13:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.07321635633707047 norm:8.523828000761569e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:14:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.0731697827577591 norm:8.260514005087316e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:15:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.07313620299100876 norm:8.026086288737133e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:16:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.07310885190963745 norm:7.765781629132107e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:16:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.0730852335691452 norm:7.741220179013908e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:17:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.07305620610713959 norm:7.619256211910397e-05 max memory_allocated 29275.00048828125 
[2025-03-09 16:17:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-09 16:18:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.09625392407178879 norm:0.0010930146090686321 max memory_allocated 29275.18798828125 
[2025-03-09 16:19:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.08986392617225647 norm:0.0005462549743242562 max memory_allocated 29275.18798828125 
[2025-03-09 16:20:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.08444971591234207 norm:0.0003047133213840425 max memory_allocated 29275.18798828125 
[2025-03-09 16:21:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.0830070972442627 norm:0.00019216664077248424 max memory_allocated 29275.18798828125 
[2025-03-09 16:21:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.08205696940422058 norm:0.00016276600945275277 max memory_allocated 29275.18798828125 
[2025-03-09 16:22:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.08137258142232895 norm:0.0001503467938164249 max memory_allocated 29275.18798828125 
[2025-03-09 16:23:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.08098220825195312 norm:0.0001349551312159747 max memory_allocated 29275.18798828125 
[2025-03-09 16:24:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.08077726513147354 norm:0.00012484821490943432 max memory_allocated 29275.18798828125 
[2025-03-09 16:24:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.0806342139840126 norm:0.00011526398884598166 max memory_allocated 29275.18798828125 
[2025-03-09 16:25:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.08053489029407501 norm:0.00047942547826096416 max memory_allocated 29275.18798828125 
[2025-03-09 16:26:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.08045382797718048 norm:0.0001020058844005689 max memory_allocated 29275.18798828125 
[2025-03-09 16:27:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.0803976058959961 norm:9.524275083094835e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:27:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.0803399309515953 norm:9.028436033986509e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:28:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.08028412610292435 norm:8.836002234602347e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:29:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.08024676144123077 norm:8.226132194977254e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:30:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.08021172881126404 norm:7.908574480097741e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:30:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.08018016070127487 norm:7.681050192331895e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:31:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.08015371859073639 norm:7.721816655248404e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:32:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.08013437688350677 norm:7.547622226411477e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:33:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.08011524379253387 norm:7.357336289715022e-05 max memory_allocated 29275.18798828125 
[2025-03-09 16:33:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-09 16:34:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.10475032776594162 norm:0.002207780024036765 max memory_allocated 29275.37548828125 
[2025-03-09 16:35:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.09724888205528259 norm:0.000682047801092267 max memory_allocated 29275.37548828125 
[2025-03-09 16:35:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.09171894192695618 norm:0.00034395966213196516 max memory_allocated 29275.37548828125 
[2025-03-09 16:36:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.09043113887310028 norm:0.00024182963534258306 max memory_allocated 29275.37548828125 
[2025-03-09 16:37:21 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.08946686238050461 norm:0.0001964678813237697 max memory_allocated 29275.37548828125 
[2025-03-09 16:38:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.0887499451637268 norm:0.00017271607066504657 max memory_allocated 29275.37548828125 
[2025-03-09 16:38:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.08838094025850296 norm:0.00015982624609023333 max memory_allocated 29275.37548828125 
[2025-03-09 16:39:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.08818566799163818 norm:0.00014889267913531512 max memory_allocated 29275.37548828125 
[2025-03-09 16:40:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.088063545525074 norm:0.00013340114674065262 max memory_allocated 29275.37548828125 
[2025-03-09 16:41:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.08796127885580063 norm:0.00012666684051509947 max memory_allocated 29275.37548828125 
[2025-03-09 16:41:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.08788474649190903 norm:0.00012410810450091958 max memory_allocated 29275.37548828125 
[2025-03-09 16:42:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.08780384063720703 norm:0.00011665294005069882 max memory_allocated 29275.37548828125 
[2025-03-09 16:43:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.08774266391992569 norm:0.00011166265176143497 max memory_allocated 29275.37548828125 
[2025-03-09 16:44:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.0877009928226471 norm:0.00010658349492587149 max memory_allocated 29275.37548828125 
[2025-03-09 16:44:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.0876581072807312 norm:0.00010751857189461589 max memory_allocated 29275.37548828125 
[2025-03-09 16:45:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.08762114495038986 norm:0.00010066916001960635 max memory_allocated 29275.37548828125 
[2025-03-09 16:46:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.08758712559938431 norm:9.591846901457757e-05 max memory_allocated 29275.37548828125 
[2025-03-09 16:47:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.08755765110254288 norm:9.293359471485019e-05 max memory_allocated 29275.37548828125 
[2025-03-09 16:47:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.08753079921007156 norm:9.567072993377224e-05 max memory_allocated 29275.37548828125 
[2025-03-09 16:48:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.08750604838132858 norm:9.480858716415241e-05 max memory_allocated 29275.37548828125 
[2025-03-09 16:48:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-09 16:49:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.11301067471504211 norm:0.0012968209339305758 max memory_allocated 29275.56298828125 
[2025-03-09 16:50:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.10626045614480972 norm:0.0005116972024552524 max memory_allocated 29275.56298828125 
[2025-03-09 16:51:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.10043083876371384 norm:0.00028869672678411007 max memory_allocated 29275.56298828125 
[2025-03-09 16:52:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.09899643063545227 norm:0.0002000826207222417 max memory_allocated 29275.56298828125 
[2025-03-09 16:52:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.09801998734474182 norm:0.00016437987505923957 max memory_allocated 29275.56298828125 
[2025-03-09 16:53:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.09726773202419281 norm:0.00014452432515099645 max memory_allocated 29275.56298828125 
[2025-03-09 16:54:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.09693227708339691 norm:0.00013414111163001508 max memory_allocated 29275.56298828125 
[2025-03-09 16:55:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.09675907343626022 norm:0.000123142046504654 max memory_allocated 29275.56298828125 
[2025-03-09 16:55:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.09663157165050507 norm:0.0001130427117459476 max memory_allocated 29275.56298828125 
[2025-03-09 16:56:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.09654957801103592 norm:0.00010233331704512239 max memory_allocated 29275.56298828125 
[2025-03-09 16:57:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.09647880494594574 norm:9.772777411853895e-05 max memory_allocated 29275.56298828125 
[2025-03-09 16:58:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.09641937911510468 norm:9.476870036451146e-05 max memory_allocated 29275.56298828125 
[2025-03-09 16:58:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.09635066986083984 norm:9.093890548683703e-05 max memory_allocated 29275.56298828125 
[2025-03-09 16:59:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.09629170596599579 norm:8.428742876276374e-05 max memory_allocated 29275.56298828125 
[2025-03-09 17:00:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.09624919295310974 norm:8.208342478610575e-05 max memory_allocated 29275.56298828125 
[2025-03-09 17:01:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.09620866924524307 norm:7.907153485575691e-05 max memory_allocated 29275.56298828125 
[2025-03-09 17:02:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.09618151187896729 norm:7.803937478456646e-05 max memory_allocated 29275.56298828125 
[2025-03-09 17:02:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.0961608812212944 norm:7.95856976765208e-05 max memory_allocated 29275.56298828125 
[2025-03-09 17:03:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.09613896906375885 norm:7.610108150402084e-05 max memory_allocated 29275.56298828125 
[2025-03-09 17:04:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.09611616283655167 norm:7.475927850464359e-05 max memory_allocated 29275.56298828125 
[2025-03-09 17:04:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-09 17:05:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.12418387085199356 norm:0.0008803976816125214 max memory_allocated 29275.75048828125 
[2025-03-09 17:06:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.11707919836044312 norm:0.00042039755498990417 max memory_allocated 29275.75048828125 
[2025-03-09 17:06:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.11106140911579132 norm:0.00026888016145676374 max memory_allocated 29275.75048828125 
[2025-03-09 17:07:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.10948304086923599 norm:0.0002040132094407454 max memory_allocated 29275.75048828125 
[2025-03-09 17:08:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.10838092863559723 norm:0.00017396299517713487 max memory_allocated 29275.75048828125 
[2025-03-09 17:09:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.10757147520780563 norm:0.00014757153985556215 max memory_allocated 29275.75048828125 
[2025-03-09 17:10:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.10720489174127579 norm:0.00013711131759919226 max memory_allocated 29275.75048828125 
[2025-03-09 17:10:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.10701509565114975 norm:0.00013402658805716783 max memory_allocated 29275.75048828125 
[2025-03-09 17:11:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.10686924308538437 norm:0.00012433149095159024 max memory_allocated 29275.75048828125 
[2025-03-09 17:12:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.10675900429487228 norm:0.00011888263543369249 max memory_allocated 29275.75048828125 
[2025-03-09 17:13:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.10667657107114792 norm:0.00011399939103284851 max memory_allocated 29275.75048828125 
[2025-03-09 17:13:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.10662013292312622 norm:0.00011010540765710175 max memory_allocated 29275.75048828125 
[2025-03-09 17:14:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.10655470937490463 norm:0.00010421354090794921 max memory_allocated 29275.75048828125 
[2025-03-09 17:15:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.10650321841239929 norm:0.00010452629794599488 max memory_allocated 29275.75048828125 
[2025-03-09 17:16:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.10644502192735672 norm:0.00010208766616415232 max memory_allocated 29275.75048828125 
[2025-03-09 17:16:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.10639800876379013 norm:0.00010029325494542718 max memory_allocated 29275.75048828125 
[2025-03-09 17:17:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.10635759681463242 norm:9.910438529914245e-05 max memory_allocated 29275.75048828125 
[2025-03-09 17:18:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.10633820295333862 norm:0.00010361288877902552 max memory_allocated 29275.75048828125 
[2025-03-09 17:19:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.10632355511188507 norm:0.00010568391007836908 max memory_allocated 29275.75048828125 
[2025-03-09 17:19:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.10632406175136566 norm:0.00010511980508454144 max memory_allocated 29275.75048828125 
[2025-03-09 17:20:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-09 17:21:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.13207972049713135 norm:0.0003063138574361801 max memory_allocated 29275.93798828125 
[2025-03-09 17:21:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.12622204422950745 norm:0.00018641829956322908 max memory_allocated 29275.93798828125 
[2025-03-09 17:22:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.12043814361095428 norm:0.00012353327474556863 max memory_allocated 29275.93798828125 
[2025-03-09 17:23:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.11907696723937988 norm:0.00010749694047262892 max memory_allocated 29275.93798828125 
[2025-03-09 17:24:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.11800824105739594 norm:9.630428394302726e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:24:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.1172788217663765 norm:9.066384518519044e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:25:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.11697250604629517 norm:8.745853847358376e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:26:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.1168350875377655 norm:8.669678209116682e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:27:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.11672729253768921 norm:7.926859689177945e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:27:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.11663524806499481 norm:7.677095709368587e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:28:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.11656798422336578 norm:7.612866465933621e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:29:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.1165083572268486 norm:7.736402039881796e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:30:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.11644390970468521 norm:7.57195521146059e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:30:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.11639433354139328 norm:7.520607323385775e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:31:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.11634492874145508 norm:7.231033669086173e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:32:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.11630359292030334 norm:7.291783549590036e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:33:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.11626546829938889 norm:7.149349403334782e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:33:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.11622941493988037 norm:7.066973921610042e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:34:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.11619775742292404 norm:0.00017718723393045366 max memory_allocated 29275.93798828125 
[2025-03-09 17:35:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.11618292331695557 norm:7.250523776747286e-05 max memory_allocated 29275.93798828125 
[2025-03-09 17:35:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-09 17:36:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.1469494104385376 norm:0.0009629858541302383 max memory_allocated 29276.12548828125 
[2025-03-09 17:37:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.13980209827423096 norm:0.0005224972264841199 max memory_allocated 29276.12548828125 
[2025-03-09 17:38:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.1334192305803299 norm:0.0003558926982805133 max memory_allocated 29276.12548828125 
[2025-03-09 17:38:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.1316870152950287 norm:0.0002843202091753483 max memory_allocated 29276.12548828125 
[2025-03-09 17:39:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.1304369866847992 norm:0.00025370961520820856 max memory_allocated 29276.12548828125 
[2025-03-09 17:40:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.12970112264156342 norm:0.00022681953851133585 max memory_allocated 29276.12548828125 
[2025-03-09 17:41:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.12937161326408386 norm:0.00021308810391929 max memory_allocated 29276.12548828125 
[2025-03-09 17:41:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.12917375564575195 norm:0.00020250416127964854 max memory_allocated 29276.12548828125 
[2025-03-09 17:42:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.12901932001113892 norm:0.00018208673282060772 max memory_allocated 29276.12548828125 
[2025-03-09 17:43:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.12889105081558228 norm:0.00017695034330245107 max memory_allocated 29276.12548828125 
[2025-03-09 17:44:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.1287699043750763 norm:0.00016402831533923745 max memory_allocated 29276.12548828125 
[2025-03-09 17:44:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.12867535650730133 norm:0.00015016799443401396 max memory_allocated 29276.12548828125 
[2025-03-09 17:45:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.1286003291606903 norm:0.0001456246682209894 max memory_allocated 29276.12548828125 
[2025-03-09 17:46:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.12854363024234772 norm:0.00014080571418162435 max memory_allocated 29276.12548828125 
[2025-03-09 17:47:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.12848667800426483 norm:0.00014838324568700045 max memory_allocated 29276.12548828125 
[2025-03-09 17:47:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.12843617796897888 norm:0.00013474233855959028 max memory_allocated 29276.12548828125 
[2025-03-09 17:48:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.1283942312002182 norm:0.00014405965339392424 max memory_allocated 29276.12548828125 
[2025-03-09 17:49:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.12833836674690247 norm:0.00013847612717654556 max memory_allocated 29276.12548828125 
[2025-03-09 17:50:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.12831395864486694 norm:0.00012954481644555926 max memory_allocated 29276.12548828125 
[2025-03-09 17:50:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.12827542424201965 norm:0.00012234080350026488 max memory_allocated 29276.12548828125 
[2025-03-09 17:51:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-09 17:52:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.1581057757139206 norm:0.00036374820047058165 max memory_allocated 29276.31298828125 
[2025-03-09 17:52:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.15167325735092163 norm:0.00021723196550738066 max memory_allocated 29276.31298828125 
[2025-03-09 17:53:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.14537347853183746 norm:0.0001322220778092742 max memory_allocated 29276.31298828125 
[2025-03-09 17:54:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.1438370943069458 norm:0.00010441705671837553 max memory_allocated 29276.31298828125 
[2025-03-09 17:55:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.1426374316215515 norm:9.24447231227532e-05 max memory_allocated 29276.31298828125 
[2025-03-09 17:55:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.14195415377616882 norm:8.957795216701925e-05 max memory_allocated 29276.31298828125 
[2025-03-09 17:56:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.14166417717933655 norm:8.204762707464397e-05 max memory_allocated 29276.31298828125 
[2025-03-09 17:57:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.1415211707353592 norm:8.003556285984814e-05 max memory_allocated 29276.31298828125 
[2025-03-09 17:58:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.14141806960105896 norm:7.746808114461601e-05 max memory_allocated 29276.31298828125 
[2025-03-09 17:58:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.14132387936115265 norm:7.388758240267634e-05 max memory_allocated 29276.31298828125 
[2025-03-09 17:59:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.14124992489814758 norm:7.347057544393465e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:00:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.14118678867816925 norm:7.620292308274657e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:01:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.14113469421863556 norm:7.590799214085564e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:01:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.1410859227180481 norm:7.387357618426904e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:02:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.14104080200195312 norm:7.35398571123369e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:03:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.1409992277622223 norm:7.546328561147675e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:04:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.14096413552761078 norm:7.48854290577583e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:04:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.14093440771102905 norm:7.519367500208318e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:05:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.14091241359710693 norm:7.571250898763537e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:06:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.1408805549144745 norm:7.448797259712592e-05 max memory_allocated 29276.31298828125 
[2025-03-09 18:06:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-09 18:07:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.1756068915128708 norm:0.0006722796824760735 max memory_allocated 29276.50048828125 
[2025-03-09 18:08:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.1676931381225586 norm:0.00036078455741517246 max memory_allocated 29276.50048828125 
[2025-03-09 18:09:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.1601596623659134 norm:0.0002045838482445106 max memory_allocated 29276.50048828125 
[2025-03-09 18:09:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.15822789072990417 norm:0.00016078715270850807 max memory_allocated 29276.50048828125 
[2025-03-09 18:10:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.1569157838821411 norm:0.00013982123346067965 max memory_allocated 29276.50048828125 
[2025-03-09 18:11:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.15626925230026245 norm:0.00012508434883784503 max memory_allocated 29276.50048828125 
[2025-03-09 18:12:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.15600146353244781 norm:0.00011434979387558997 max memory_allocated 29276.50048828125 
[2025-03-09 18:12:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.1558331847190857 norm:0.00010521322838030756 max memory_allocated 29276.50048828125 
[2025-03-09 18:13:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.15568242967128754 norm:9.968438098439947e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:14:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.15557162463665009 norm:9.640971984481439e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:15:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.15547716617584229 norm:9.261289233108982e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:15:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.1553838700056076 norm:8.946251909947023e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:16:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.15530046820640564 norm:8.646360947750509e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:17:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.15523934364318848 norm:8.517214882886037e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:18:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.15517950057983398 norm:8.263463678304106e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:18:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.1551298201084137 norm:8.120621350826696e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:19:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.15507853031158447 norm:8.027275907807052e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:20:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.155035138130188 norm:7.990930316736922e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:21:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.15499649941921234 norm:7.974564505275339e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:21:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.1549592763185501 norm:7.936243491712958e-05 max memory_allocated 29276.50048828125 
[2025-03-09 18:22:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-09 18:23:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.190717875957489 norm:0.0006428844062611461 max memory_allocated 29276.68798828125 
[2025-03-09 18:24:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.18293797969818115 norm:0.00039144279435276985 max memory_allocated 29276.68798828125 
[2025-03-09 18:24:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.17521198093891144 norm:0.00020873331231996417 max memory_allocated 29276.68798828125 
[2025-03-09 18:25:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.17317795753479004 norm:0.00016886043886188418 max memory_allocated 29276.68798828125 
[2025-03-09 18:26:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.1718301773071289 norm:0.00014887008001096547 max memory_allocated 29276.68798828125 
[2025-03-09 18:27:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.17127138376235962 norm:0.0001382622285746038 max memory_allocated 29276.68798828125 
[2025-03-09 18:27:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.1710493266582489 norm:0.00013083260273560882 max memory_allocated 29276.68798828125 
[2025-03-09 18:28:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.17088647186756134 norm:0.00012392975622788072 max memory_allocated 29276.68798828125 
[2025-03-09 18:29:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.17075873911380768 norm:0.0001192356285173446 max memory_allocated 29276.68798828125 
[2025-03-09 18:30:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.17064973711967468 norm:0.0001106777272070758 max memory_allocated 29276.68798828125 
[2025-03-09 18:30:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.17055991291999817 norm:0.00010617173393256962 max memory_allocated 29276.68798828125 
[2025-03-09 18:31:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.17047831416130066 norm:0.00010393643606221303 max memory_allocated 29276.68798828125 
[2025-03-09 18:32:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.17039670050144196 norm:9.835403761826456e-05 max memory_allocated 29276.68798828125 
[2025-03-09 18:33:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.17034760117530823 norm:9.890731598716229e-05 max memory_allocated 29276.68798828125 
[2025-03-09 18:33:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.17028728127479553 norm:9.876054537016898e-05 max memory_allocated 29276.68798828125 
[2025-03-09 18:34:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.17025169730186462 norm:0.00010448452667333186 max memory_allocated 29276.68798828125 
[2025-03-09 18:35:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.17020291090011597 norm:9.674150351202115e-05 max memory_allocated 29276.68798828125 
[2025-03-09 18:36:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.1701560616493225 norm:9.60553006734699e-05 max memory_allocated 29276.68798828125 
[2025-03-09 18:36:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.17011374235153198 norm:9.45843494264409e-05 max memory_allocated 29276.68798828125 
[2025-03-09 18:37:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.1700909584760666 norm:9.355897782370448e-05 max memory_allocated 29276.68798828125 
[2025-03-09 18:37:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-03-09 18:38:44 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.20976296067237854 norm:0.0007476159371435642 max memory_allocated 29276.87548828125 
[2025-03-09 18:39:30 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.20116320252418518 norm:0.00042684809886850417 max memory_allocated 29276.87548828125 
[2025-03-09 18:40:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.19291052222251892 norm:0.00023493854678235948 max memory_allocated 29276.87548828125 
[2025-03-09 18:41:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.19058933854103088 norm:0.00017628655768930912 max memory_allocated 29276.87548828125 
[2025-03-09 18:41:46 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.18918153643608093 norm:0.0001583201519679278 max memory_allocated 29276.87548828125 
[2025-03-09 18:42:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.18864183127880096 norm:0.00014569060294888914 max memory_allocated 29276.87548828125 
[2025-03-09 18:43:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.18838924169540405 norm:0.00013468963152263314 max memory_allocated 29276.87548828125 
[2025-03-09 18:44:02 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.18821847438812256 norm:0.00012504948244895786 max memory_allocated 29276.87548828125 
[2025-03-09 18:44:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.18806973099708557 norm:0.00011669316154439002 max memory_allocated 29276.87548828125 
[2025-03-09 18:45:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.1879497468471527 norm:0.00011100707342848182 max memory_allocated 29276.87548828125 
[2025-03-09 18:46:19 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.18785908818244934 norm:0.00010404865315649658 max memory_allocated 29276.87548828125 
[2025-03-09 18:47:04 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.18776559829711914 norm:9.914477413985878e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:47:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.1876906007528305 norm:9.550582035444677e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:48:35 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.1876186728477478 norm:9.13168114493601e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:49:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.18755607306957245 norm:9.055111149791628e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:50:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.1874854415655136 norm:8.938487735576928e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:50:51 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.18743419647216797 norm:8.843302930472419e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:51:37 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.18738126754760742 norm:8.748629625188187e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:52:22 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.18733255565166473 norm:8.669011003803462e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:53:08 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.1872933954000473 norm:8.755411545280367e-05 max memory_allocated 29276.87548828125 
[2025-03-09 18:53:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-03-09 18:54:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.22946345806121826 norm:0.0007768294890411198 max memory_allocated 29277.06298828125 
[2025-03-09 18:55:00 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.21988587081432343 norm:0.00042548865894787014 max memory_allocated 29277.06298828125 
[2025-03-09 18:55:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.21099606156349182 norm:0.0002644711930770427 max memory_allocated 29277.06298828125 
[2025-03-09 18:56:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.20853322744369507 norm:0.00019754421373363584 max memory_allocated 29277.06298828125 
[2025-03-09 18:57:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.2071026861667633 norm:0.00016812274407129735 max memory_allocated 29277.06298828125 
[2025-03-09 18:58:01 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.20657753944396973 norm:0.00015467860794160515 max memory_allocated 29277.06298828125 
[2025-03-09 18:58:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.20632857084274292 norm:0.00014191785885486752 max memory_allocated 29277.06298828125 
[2025-03-09 18:59:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.20616087317466736 norm:0.00013286851753946394 max memory_allocated 29277.06298828125 
[2025-03-09 19:00:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.20600928366184235 norm:0.0001225869491463527 max memory_allocated 29277.06298828125 
[2025-03-09 19:01:03 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.20589765906333923 norm:0.00011425599950598553 max memory_allocated 29277.06298828125 
[2025-03-09 19:01:49 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.20579703152179718 norm:0.00010947573173325509 max memory_allocated 29277.06298828125 
[2025-03-09 19:02:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.20571182668209076 norm:0.00010569060395937413 max memory_allocated 29277.06298828125 
[2025-03-09 19:03:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.20562677085399628 norm:0.00010521485091885552 max memory_allocated 29277.06298828125 
[2025-03-09 19:04:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.20554427802562714 norm:0.0001023507647914812 max memory_allocated 29277.06298828125 
[2025-03-09 19:04:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.20548367500305176 norm:0.00010121930245077237 max memory_allocated 29277.06298828125 
[2025-03-09 19:05:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.20545387268066406 norm:9.538716403767467e-05 max memory_allocated 29277.06298828125 
[2025-03-09 19:06:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.20542021095752716 norm:9.779303218238056e-05 max memory_allocated 29277.06298828125 
[2025-03-09 19:07:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.20534837245941162 norm:9.178523760056123e-05 max memory_allocated 29277.06298828125 
[2025-03-09 19:07:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.20529237389564514 norm:9.511747339274734e-05 max memory_allocated 29277.06298828125 
[2025-03-09 19:08:38 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.20526298880577087 norm:9.224965469911695e-05 max memory_allocated 29277.06298828125 
[2025-03-09 19:08:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-03-09 19:09:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.2530609965324402 norm:0.0006499963346868753 max memory_allocated 29277.25048828125 
[2025-03-09 19:10:30 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.24299438297748566 norm:0.00037591258296743035 max memory_allocated 29277.25048828125 
[2025-03-09 19:11:15 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.2339695394039154 norm:0.00023845188843552023 max memory_allocated 29277.25048828125 
[2025-03-09 19:12:00 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.23128913342952728 norm:0.00019247496675234288 max memory_allocated 29277.25048828125 
[2025-03-09 19:12:46 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.22985832393169403 norm:0.00016872504784259945 max memory_allocated 29277.25048828125 
[2025-03-09 19:13:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.2293480932712555 norm:0.0001524288672953844 max memory_allocated 29277.25048828125 
[2025-03-09 19:14:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.22906775772571564 norm:0.00014077743981033564 max memory_allocated 29277.25048828125 
[2025-03-09 19:15:02 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.2288161665201187 norm:0.00012626823445316404 max memory_allocated 29277.25048828125 
[2025-03-09 19:15:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.22863341867923737 norm:0.00011910218017874286 max memory_allocated 29277.25048828125 
[2025-03-09 19:16:33 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.22848916053771973 norm:0.00011322039790684357 max memory_allocated 29277.25048828125 
[2025-03-09 19:17:19 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.22837263345718384 norm:0.00011189377983100712 max memory_allocated 29277.25048828125 
[2025-03-09 19:18:04 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.22825491428375244 norm:0.00010630938777467236 max memory_allocated 29277.25048828125 
[2025-03-09 19:18:50 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.2281433343887329 norm:0.0001017033209791407 max memory_allocated 29277.25048828125 
[2025-03-09 19:19:35 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.2280512899160385 norm:9.930549276759848e-05 max memory_allocated 29277.25048828125 
[2025-03-09 19:20:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.22797326743602753 norm:9.696993220131844e-05 max memory_allocated 29277.25048828125 
[2025-03-09 19:21:06 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.22791288793087006 norm:9.608548862161115e-05 max memory_allocated 29277.25048828125 
[2025-03-09 19:21:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.22785891592502594 norm:0.00010134732292499393 max memory_allocated 29277.25048828125 
[2025-03-09 19:22:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.22779680788516998 norm:9.824455628404394e-05 max memory_allocated 29277.25048828125 
[2025-03-09 19:23:22 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.22775515913963318 norm:9.113001578953117e-05 max memory_allocated 29277.25048828125 
[2025-03-09 19:24:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.22771130502223969 norm:9.159410546999425e-05 max memory_allocated 29277.25048828125 
[2025-03-09 19:24:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-03-09 19:25:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.2774752378463745 norm:0.0003833298687823117 max memory_allocated 29277.43798828125 
[2025-03-09 19:25:59 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.2670173943042755 norm:0.0002411172608844936 max memory_allocated 29277.43798828125 
[2025-03-09 19:26:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.2575420141220093 norm:0.00015944312326610088 max memory_allocated 29277.43798828125 
[2025-03-09 19:27:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.25471386313438416 norm:0.0001347222860204056 max memory_allocated 29277.43798828125 
[2025-03-09 19:28:15 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.253339946269989 norm:0.00012342873378656805 max memory_allocated 29277.43798828125 
[2025-03-09 19:29:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.25286558270454407 norm:0.0001132350298576057 max memory_allocated 29277.43798828125 
[2025-03-09 19:29:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.25257450342178345 norm:0.00010745234612841159 max memory_allocated 29277.43798828125 
[2025-03-09 19:30:31 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.2523764669895172 norm:0.00010417779412819073 max memory_allocated 29277.43798828125 
[2025-03-09 19:31:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.2522122859954834 norm:0.00010181713150814176 max memory_allocated 29277.43798828125 
[2025-03-09 19:32:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.25206509232521057 norm:0.00010079571802634746 max memory_allocated 29277.43798828125 
[2025-03-09 19:32:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.2519463896751404 norm:9.859211422735825e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:33:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.2518678903579712 norm:9.497431165073067e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:34:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.2517712116241455 norm:0.0007559825317002833 max memory_allocated 29277.43798828125 
[2025-03-09 19:35:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.25168490409851074 norm:9.363903518533334e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:35:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.25161558389663696 norm:9.388139005750418e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:36:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.2515571713447571 norm:9.46350337471813e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:37:21 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.25150537490844727 norm:9.392912761541083e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:38:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.2514624297618866 norm:9.349470201414078e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:38:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.2514323890209198 norm:9.437682456336915e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:39:37 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.2513875961303711 norm:9.418815898243338e-05 max memory_allocated 29277.43798828125 
[2025-03-09 19:39:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-03-09 19:39:56 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 19:40:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.31688636541366577 norm:0.010521640069782734 max memory_allocated 29277.77001953125 
[2025-03-09 19:41:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.3014593720436096 norm:0.007947796955704689 max memory_allocated 29277.77001953125 
[2025-03-09 19:42:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.28928619623184204 norm:0.005792232230305672 max memory_allocated 29277.77001953125 
[2025-03-09 19:42:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.2852884829044342 norm:0.0047963629476726055 max memory_allocated 29277.77001953125 
[2025-03-09 19:43:45 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.28358185291290283 norm:0.003958138637244701 max memory_allocated 29277.77001953125 
[2025-03-09 19:44:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.28281915187835693 norm:0.0032713173422962427 max memory_allocated 29277.77001953125 
[2025-03-09 19:45:16 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.28237104415893555 norm:0.002763852011412382 max memory_allocated 29277.77001953125 
[2025-03-09 19:46:02 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.2820693552494049 norm:0.002685823477804661 max memory_allocated 29277.77001953125 
[2025-03-09 19:46:47 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.2818205654621124 norm:0.002589533571153879 max memory_allocated 29277.77001953125 
[2025-03-09 19:47:33 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.2815530300140381 norm:0.002395907649770379 max memory_allocated 29277.77001953125 
[2025-03-09 19:48:18 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.2813537120819092 norm:0.0022271047346293926 max memory_allocated 29277.77001953125 
[2025-03-09 19:49:04 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.2811906635761261 norm:0.0021995806600898504 max memory_allocated 29277.77001953125 
[2025-03-09 19:49:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.28114044666290283 norm:0.0022495745215564966 max memory_allocated 29277.77001953125 
[2025-03-09 19:50:35 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.2810109853744507 norm:0.0022949569392949343 max memory_allocated 29277.77001953125 
[2025-03-09 19:51:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.28089630603790283 norm:0.0020477445796132088 max memory_allocated 29277.77001953125 
[2025-03-09 19:52:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.2807149291038513 norm:0.0020227443892508745 max memory_allocated 29277.77001953125 
[2025-03-09 19:52:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.28070124983787537 norm:0.0020516624208539724 max memory_allocated 29277.77001953125 
[2025-03-09 19:53:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.28053396940231323 norm:0.002023104578256607 max memory_allocated 29277.77001953125 
[2025-03-09 19:54:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.28048616647720337 norm:0.00190207629930228 max memory_allocated 29277.77001953125 
[2025-03-09 19:55:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.2803938388824463 norm:0.0018997829174622893 max memory_allocated 29277.77001953125 
[2025-03-09 19:55:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-03-09 19:55:31 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 19:56:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.3500734567642212 norm:0.010828215628862381 max memory_allocated 29277.95751953125 
[2025-03-09 19:57:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.3346710503101349 norm:0.008350797928869724 max memory_allocated 29277.95751953125 
[2025-03-09 19:57:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.3218842148780823 norm:0.006024586968123913 max memory_allocated 29277.95751953125 
[2025-03-09 19:58:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.3177511692047119 norm:0.0049460288137197495 max memory_allocated 29277.95751953125 
[2025-03-09 19:59:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.31608083844184875 norm:0.004150213208049536 max memory_allocated 29277.95751953125 
[2025-03-09 20:00:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.3153582215309143 norm:0.003527456196025014 max memory_allocated 29277.95751953125 
[2025-03-09 20:00:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.3148626685142517 norm:0.0030026184394955635 max memory_allocated 29277.95751953125 
[2025-03-09 20:01:36 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.31453511118888855 norm:0.0027761785313487053 max memory_allocated 29277.95751953125 
[2025-03-09 20:02:22 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.31435734033584595 norm:0.0027880235575139523 max memory_allocated 29277.95751953125 
[2025-03-09 20:03:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.3142113983631134 norm:0.002767113968729973 max memory_allocated 29277.95751953125 
[2025-03-09 20:03:53 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.31402599811553955 norm:0.002578502055257559 max memory_allocated 29277.95751953125 
[2025-03-09 20:04:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.31391221284866333 norm:0.0025706577580422163 max memory_allocated 29277.95751953125 
[2025-03-09 20:05:25 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.3138352334499359 norm:0.002553086495026946 max memory_allocated 29277.95751953125 
[2025-03-09 20:06:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.31367093324661255 norm:0.002484695753082633 max memory_allocated 29277.95751953125 
[2025-03-09 20:06:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.31349900364875793 norm:0.0023400350473821163 max memory_allocated 29277.95751953125 
[2025-03-09 20:07:42 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.31333038210868835 norm:0.0022112561855465174 max memory_allocated 29277.95751953125 
[2025-03-09 20:08:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.31329694390296936 norm:0.002230520360171795 max memory_allocated 29277.95751953125 
[2025-03-09 20:09:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.31334900856018066 norm:0.002396321389824152 max memory_allocated 29277.95751953125 
[2025-03-09 20:09:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.31324315071105957 norm:0.0022735316306352615 max memory_allocated 29277.95751953125 
[2025-03-09 20:10:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.3131650686264038 norm:0.0021987708751112223 max memory_allocated 29277.95751953125 
[2025-03-09 20:10:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-03-09 20:11:06 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 20:11:51 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.41638287901878357 norm:0.01502115186303854 max memory_allocated 29278.14501953125 
[2025-03-09 20:12:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.3980191946029663 norm:0.007444039918482304 max memory_allocated 29278.14501953125 
[2025-03-09 20:13:23 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.3804851174354553 norm:0.00653207628056407 max memory_allocated 29278.14501953125 
[2025-03-09 20:14:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.3746790289878845 norm:0.007257089018821716 max memory_allocated 29278.14501953125 
[2025-03-09 20:14:54 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.37213918566703796 norm:0.006886427290737629 max memory_allocated 29278.14501953125 
[2025-03-09 20:15:39 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.37114250659942627 norm:0.00665831845253706 max memory_allocated 29278.14501953125 
[2025-03-09 20:16:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.3705996870994568 norm:0.006383485160768032 max memory_allocated 29278.14501953125 
[2025-03-09 20:17:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.3700810372829437 norm:0.006005582399666309 max memory_allocated 29278.14501953125 
[2025-03-09 20:17:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.36969414353370667 norm:0.005804072134196758 max memory_allocated 29278.14501953125 
[2025-03-09 20:18:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.3694269359111786 norm:0.005301482975482941 max memory_allocated 29278.14501953125 
[2025-03-09 20:19:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.3692949414253235 norm:0.005148040130734444 max memory_allocated 29278.14501953125 
[2025-03-09 20:20:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.3693520426750183 norm:0.005376514047384262 max memory_allocated 29278.14501953125 
[2025-03-09 20:20:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.3691408634185791 norm:0.004867737181484699 max memory_allocated 29278.14501953125 
[2025-03-09 20:21:45 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.3690092861652374 norm:0.004681712482124567 max memory_allocated 29278.14501953125 
[2025-03-09 20:22:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.36892762780189514 norm:0.004581020213663578 max memory_allocated 29278.14501953125 
[2025-03-09 20:23:16 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.36898210644721985 norm:0.00444804597645998 max memory_allocated 29278.14501953125 
[2025-03-09 20:24:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.3686804473400116 norm:0.004161515273153782 max memory_allocated 29278.14501953125 
[2025-03-09 20:24:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.36853933334350586 norm:0.0038333951961249113 max memory_allocated 29278.14501953125 
[2025-03-09 20:25:33 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.36845362186431885 norm:0.0037769400514662266 max memory_allocated 29278.14501953125 
[2025-03-09 20:26:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.3684503436088562 norm:0.0037655506748706102 max memory_allocated 29278.14501953125 
[2025-03-09 20:26:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-03-09 20:26:42 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-09 20:27:27 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.6089622378349304 norm:0.06855667382478714 max memory_allocated 29278.33251953125 
[2025-03-09 20:28:13 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.5498551726341248 norm:0.04429737851023674 max memory_allocated 29278.33251953125 
[2025-03-09 20:28:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.5103399157524109 norm:0.02787678688764572 max memory_allocated 29278.33251953125 
[2025-03-09 20:29:44 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.4985124170780182 norm:0.021608849987387657 max memory_allocated 29278.33251953125 
[2025-03-09 20:30:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.4923710525035858 norm:0.018544379621744156 max memory_allocated 29278.33251953125 
[2025-03-09 20:31:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.48849672079086304 norm:0.017603380605578423 max memory_allocated 29278.33251953125 
[2025-03-09 20:32:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.48614513874053955 norm:0.016235938295722008 max memory_allocated 29278.33251953125 
[2025-03-09 20:32:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.48448386788368225 norm:0.015193596482276917 max memory_allocated 29278.33251953125 
[2025-03-09 20:33:32 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.48273587226867676 norm:0.014741683378815651 max memory_allocated 29278.33251953125 
[2025-03-09 20:34:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.48128801584243774 norm:0.013490325771272182 max memory_allocated 29278.33251953125 
[2025-03-09 20:35:04 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.4802461862564087 norm:0.012949462980031967 max memory_allocated 29278.33251953125 
[2025-03-09 20:35:49 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.47873854637145996 norm:0.01193179376423359 max memory_allocated 29278.33251953125 
[2025-03-09 20:36:35 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.4776743948459625 norm:0.01136163529008627 max memory_allocated 29278.33251953125 
[2025-03-09 20:37:20 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.4767179489135742 norm:0.010623903945088387 max memory_allocated 29278.33251953125 
[2025-03-09 20:38:06 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.47633621096611023 norm:0.01033818256109953 max memory_allocated 29278.33251953125 
[2025-03-09 20:38:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.47578367590904236 norm:0.009916640818119049 max memory_allocated 29278.33251953125 
[2025-03-09 20:39:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.4753875136375427 norm:0.009899916127324104 max memory_allocated 29278.33251953125 
[2025-03-09 20:40:23 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.47476932406425476 norm:0.009412359446287155 max memory_allocated 29278.33251953125 
[2025-03-09 20:41:09 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.4748469591140747 norm:0.009535376913845539 max memory_allocated 29278.33251953125 
[2025-03-09 20:41:54 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.47477883100509644 norm:0.009638959541916847 max memory_allocated 29278.33251953125 
[2025-03-09 20:42:10 root] (main_calibration.py 365): INFO 37301.930798769
[2025-03-09 20:42:46 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-09 20:44:29 root] (main_calibration.py 158): INFO wikitext2 : 5.063249111175537
[2025-03-09 20:44:29 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-09 20:47:08 root] (main_calibration.py 158): INFO c4 : 6.707368850708008
[2025-03-09 22:46:40 root] (main_calibration.py 169): INFO {'wikitext2': 5.063249111175537, 'c4': 6.707368850708008, 'results': {'piqa': {'acc': 0.7823721436343852, 'acc_stderr': 0.009627407474840878, 'acc_norm': 0.7867247007616975, 'acc_norm_stderr': 0.00955712122586134}, 'hellaswag': {'acc': 0.5853415654252141, 'acc_stderr': 0.0049165612135912825, 'acc_norm': 0.7546305516829317, 'acc_norm_stderr': 0.004294266090708756}, 'boolq': {'acc': 0.6467889908256881, 'acc_stderr': 0.008359705247064295}, 'arc_easy': {'acc': 0.7247474747474747, 'acc_stderr': 0.009164888895174745, 'acc_norm': 0.5791245791245792, 'acc_norm_stderr': 0.01013050216406633}, 'winogrande': {'acc': 0.6937647987371744, 'acc_stderr': 0.012954385972802459}, 'arc_challenge': {'acc': 0.4300341296928328, 'acc_stderr': 0.014467631559137993, 'acc_norm': 0.4257679180887372, 'acc_norm_stderr': 0.014449464278868805}}, 'versions': {'piqa': 0, 'hellaswag': 0, 'boolq': 1, 'arc_easy': 0, 'winogrande': 0, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
