[2025-03-04 12:47:36 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-7b-hf-w4a6', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-7b-hf', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=6, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-03-04 12:53:10 root] (main_calibration.py 336): INFO === start quantization ===
[2025-03-04 12:53:10 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-04 12:53:11 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-03-04 12:53:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-03-04 12:53:17 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 12:53:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.01548219658434391 norm:0.017144901677966118 max memory_allocated 22562.10693359375 
[2025-03-04 12:54:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.009531083516776562 norm:0.009944610297679901 max memory_allocated 22562.10693359375 
[2025-03-04 12:54:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.007392279803752899 norm:0.0074512287974357605 max memory_allocated 22562.10693359375 
[2025-03-04 12:55:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.006495223846286535 norm:0.00585002638399601 max memory_allocated 22562.10693359375 
[2025-03-04 12:56:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0062221563421189785 norm:0.005032979883253574 max memory_allocated 22562.10693359375 
[2025-03-04 12:56:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.006023949012160301 norm:0.004279030952602625 max memory_allocated 22562.10693359375 
[2025-03-04 12:57:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.005956969223916531 norm:0.003879562485963106 max memory_allocated 22562.10693359375 
[2025-03-04 12:57:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.005859691649675369 norm:0.0034490935504436493 max memory_allocated 22562.10693359375 
[2025-03-04 12:58:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.005816952791064978 norm:0.003006718587130308 max memory_allocated 22562.10693359375 
[2025-03-04 12:58:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.005670396611094475 norm:0.0026441263034939766 max memory_allocated 22562.10693359375 
[2025-03-04 12:59:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.005719000473618507 norm:0.002646193839609623 max memory_allocated 22562.10693359375 
[2025-03-04 12:59:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0056549496948719025 norm:0.002124271122738719 max memory_allocated 22562.10693359375 
[2025-03-04 13:00:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.005567142739892006 norm:0.001959654036909342 max memory_allocated 22562.10693359375 
[2025-03-04 13:01:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.005598307121545076 norm:0.0019155514892190695 max memory_allocated 22562.10693359375 
[2025-03-04 13:01:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0055871945805847645 norm:0.0018656919710338116 max memory_allocated 22562.10693359375 
[2025-03-04 13:02:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.005582177080214024 norm:0.0017913907067850232 max memory_allocated 22562.10693359375 
[2025-03-04 13:02:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.005567001644521952 norm:0.0015805583680048585 max memory_allocated 22562.10693359375 
[2025-03-04 13:03:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.005526082590222359 norm:0.0013986836420372128 max memory_allocated 22562.10693359375 
[2025-03-04 13:03:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0056146252900362015 norm:0.0016467744717374444 max memory_allocated 22562.10693359375 
[2025-03-04 13:04:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.005520938895642757 norm:0.0015052120434120297 max memory_allocated 22562.10693359375 
[2025-03-04 13:04:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-03-04 13:04:37 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 13:05:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.058144934475421906 norm:0.0343707799911499 max memory_allocated 22562.27880859375 
[2025-03-04 13:05:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.04129616916179657 norm:0.02122405543923378 max memory_allocated 22562.27880859375 
[2025-03-04 13:06:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.03669329732656479 norm:0.019413689151406288 max memory_allocated 22562.27880859375 
[2025-03-04 13:06:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.031158387660980225 norm:0.011448418721556664 max memory_allocated 22562.27880859375 
[2025-03-04 13:07:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.030537942424416542 norm:0.01184853259474039 max memory_allocated 22562.27880859375 
[2025-03-04 13:07:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.02999202348291874 norm:0.012616772204637527 max memory_allocated 22562.27880859375 
[2025-03-04 13:08:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.03000454418361187 norm:0.01235821284353733 max memory_allocated 22562.27880859375 
[2025-03-04 13:09:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.02944900654256344 norm:0.011826245114207268 max memory_allocated 22562.27880859375 
[2025-03-04 13:09:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.029274199157953262 norm:0.010607714764773846 max memory_allocated 22562.27880859375 
[2025-03-04 13:10:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.029159091413021088 norm:0.010369276627898216 max memory_allocated 22562.27880859375 
[2025-03-04 13:10:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.028232330456376076 norm:0.010423771105706692 max memory_allocated 22562.27880859375 
[2025-03-04 13:11:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.028222637251019478 norm:0.009327072650194168 max memory_allocated 22562.27880859375 
[2025-03-04 13:11:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.028193838894367218 norm:0.010153756476938725 max memory_allocated 22562.27880859375 
[2025-03-04 13:12:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.028691736981272697 norm:0.00954393483698368 max memory_allocated 22562.27880859375 
[2025-03-04 13:13:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.027989422902464867 norm:0.009285016916692257 max memory_allocated 22562.27880859375 
[2025-03-04 13:13:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.027676040306687355 norm:0.008801756426692009 max memory_allocated 22562.27880859375 
[2025-03-04 13:14:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.028037743642926216 norm:0.009214983321726322 max memory_allocated 22562.27880859375 
[2025-03-04 13:14:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.027249660342931747 norm:0.008208039216697216 max memory_allocated 22562.27880859375 
[2025-03-04 13:15:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.02793748863041401 norm:0.008508779108524323 max memory_allocated 22562.27880859375 
[2025-03-04 13:15:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.027317197993397713 norm:0.0077230362221598625 max memory_allocated 22562.27880859375 
[2025-03-04 13:15:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-03-04 13:16:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 13:16:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.05284566804766655 norm:0.027010539546608925 max memory_allocated 22562.45068359375 
[2025-03-04 13:17:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.04003998637199402 norm:0.01564938947558403 max memory_allocated 22562.45068359375 
[2025-03-04 13:17:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.03381064906716347 norm:0.010281126946210861 max memory_allocated 22562.45068359375 
[2025-03-04 13:18:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.03181988745927811 norm:0.008141182363033295 max memory_allocated 22562.45068359375 
[2025-03-04 13:18:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.030717844143509865 norm:0.006621686741709709 max memory_allocated 22562.45068359375 
[2025-03-04 13:19:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.03001457452774048 norm:0.005595754366368055 max memory_allocated 22562.45068359375 
[2025-03-04 13:19:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.02963738515973091 norm:0.004847445525228977 max memory_allocated 22562.45068359375 
[2025-03-04 13:20:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.029368512332439423 norm:0.004012791905552149 max memory_allocated 22562.45068359375 
[2025-03-04 13:21:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.029200294986367226 norm:0.003335261484608054 max memory_allocated 22562.45068359375 
[2025-03-04 13:21:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.02910129725933075 norm:0.002784663811326027 max memory_allocated 22562.45068359375 
[2025-03-04 13:22:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.029047785326838493 norm:0.002323535270988941 max memory_allocated 22562.45068359375 
[2025-03-04 13:22:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.028994649648666382 norm:0.0021048313938081264 max memory_allocated 22562.45068359375 
[2025-03-04 13:23:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.029059484601020813 norm:0.002280830405652523 max memory_allocated 22562.45068359375 
[2025-03-04 13:23:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0290214903652668 norm:0.0020605428144335747 max memory_allocated 22562.45068359375 
[2025-03-04 13:24:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.02899004891514778 norm:0.0019450531108304858 max memory_allocated 22562.45068359375 
[2025-03-04 13:24:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.028998326510190964 norm:0.0019102221122011542 max memory_allocated 22562.45068359375 
[2025-03-04 13:25:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.02905641868710518 norm:0.0019421542529016733 max memory_allocated 22562.45068359375 
[2025-03-04 13:26:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.02910953015089035 norm:0.0019356888951733708 max memory_allocated 22562.45068359375 
[2025-03-04 13:26:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.02909192629158497 norm:0.0018641272326931357 max memory_allocated 22562.45068359375 
[2025-03-04 13:27:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.029143940657377243 norm:0.0018031037179753184 max memory_allocated 22562.45068359375 
[2025-03-04 13:27:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-03-04 13:27:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.06630373746156693 norm:0.013746429234743118 max memory_allocated 22562.50732421875 
[2025-03-04 13:28:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0505959615111351 norm:0.003762486157938838 max memory_allocated 22562.50732421875 
[2025-03-04 13:29:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.041756853461265564 norm:0.0018456489779055119 max memory_allocated 22562.50732421875 
[2025-03-04 13:29:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.03903574496507645 norm:0.001219573663547635 max memory_allocated 22562.50732421875 
[2025-03-04 13:30:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.037587620317935944 norm:0.000961782643571496 max memory_allocated 22562.50732421875 
[2025-03-04 13:30:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.03692595288157463 norm:0.0008504088618792593 max memory_allocated 22562.50732421875 
[2025-03-04 13:31:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.03661733120679855 norm:0.0007526269182562828 max memory_allocated 22562.50732421875 
[2025-03-04 13:31:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.036424774676561356 norm:0.0006858441047370434 max memory_allocated 22562.50732421875 
[2025-03-04 13:32:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.036225952208042145 norm:0.0005838838405907154 max memory_allocated 22562.50732421875 
[2025-03-04 13:33:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.03617119789123535 norm:0.0005527250468730927 max memory_allocated 22562.50732421875 
[2025-03-04 13:33:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0360756441950798 norm:0.0004970664158463478 max memory_allocated 22562.50732421875 
[2025-03-04 13:34:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.03600284457206726 norm:0.00046339575783349574 max memory_allocated 22562.50732421875 
[2025-03-04 13:34:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.03604908660054207 norm:0.00042837599175982177 max memory_allocated 22562.50732421875 
[2025-03-04 13:35:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.036076027899980545 norm:0.0003998355823569 max memory_allocated 22562.50732421875 
[2025-03-04 13:35:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.03606953099370003 norm:0.00038541306275874376 max memory_allocated 22562.50732421875 
[2025-03-04 13:36:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.036092109978199005 norm:0.0003701186506077647 max memory_allocated 22562.50732421875 
[2025-03-04 13:36:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.03606479614973068 norm:0.00036133744288235903 max memory_allocated 22562.50732421875 
[2025-03-04 13:37:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0360725000500679 norm:0.0003653585445135832 max memory_allocated 22562.50732421875 
[2025-03-04 13:38:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.03611190244555473 norm:0.0003413159283809364 max memory_allocated 22562.50732421875 
[2025-03-04 13:38:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0361565537750721 norm:0.0003367666504345834 max memory_allocated 22562.50732421875 
[2025-03-04 13:38:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-03-04 13:39:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0768546611070633 norm:0.009485425427556038 max memory_allocated 22562.67919921875 
[2025-03-04 13:39:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.05969000607728958 norm:0.0031658161897212267 max memory_allocated 22562.67919921875 
[2025-03-04 13:40:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.04785623028874397 norm:0.0011148164048790932 max memory_allocated 22562.67919921875 
[2025-03-04 13:41:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.04489639401435852 norm:0.0008026262512430549 max memory_allocated 22562.67919921875 
[2025-03-04 13:41:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.043520424515008926 norm:0.0006294201593846083 max memory_allocated 22562.67919921875 
[2025-03-04 13:42:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.04282333329319954 norm:0.0005695386207662523 max memory_allocated 22562.67919921875 
[2025-03-04 13:42:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.042400311678647995 norm:0.0004980167141184211 max memory_allocated 22562.67919921875 
[2025-03-04 13:43:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.042173679918050766 norm:0.00043183338129892945 max memory_allocated 22562.67919921875 
[2025-03-04 13:43:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.04207736998796463 norm:0.0004283239250071347 max memory_allocated 22562.67919921875 
[2025-03-04 13:44:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.04201335459947586 norm:0.00039769007707946 max memory_allocated 22562.67919921875 
[2025-03-04 13:44:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.041844405233860016 norm:0.0003510951646603644 max memory_allocated 22562.67919921875 
[2025-03-04 13:45:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.04179593548178673 norm:0.0003236450720578432 max memory_allocated 22562.67919921875 
[2025-03-04 13:46:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.04174025356769562 norm:0.0003068287915084511 max memory_allocated 22562.67919921875 
[2025-03-04 13:46:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.04188254475593567 norm:0.00030624703504145145 max memory_allocated 22562.67919921875 
[2025-03-04 13:47:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.041919734328985214 norm:0.0002770785358734429 max memory_allocated 22562.67919921875 
[2025-03-04 13:47:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.04191999137401581 norm:0.00028814998222514987 max memory_allocated 22562.67919921875 
[2025-03-04 13:48:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.04192338138818741 norm:0.0002802367671392858 max memory_allocated 22562.67919921875 
[2025-03-04 13:48:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0419253371655941 norm:0.0002693684655241668 max memory_allocated 22562.67919921875 
[2025-03-04 13:49:24 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.04187940061092377 norm:0.00026894500479102135 max memory_allocated 22562.67919921875 
[2025-03-04 13:49:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.041910119354724884 norm:0.0002655830467119813 max memory_allocated 22562.67919921875 
[2025-03-04 13:50:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-03-04 13:50:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.08014662563800812 norm:0.011198937892913818 max memory_allocated 22562.85107421875 
[2025-03-04 13:51:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.06356601417064667 norm:0.0035708737559616566 max memory_allocated 22562.85107421875 
[2025-03-04 13:51:50 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.05181019753217697 norm:0.0009359213290736079 max memory_allocated 22562.85107421875 
[2025-03-04 13:52:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0488857738673687 norm:0.0007046739337965846 max memory_allocated 22562.85107421875 
[2025-03-04 13:52:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.04746367037296295 norm:0.0005620918236672878 max memory_allocated 22562.85107421875 
[2025-03-04 13:53:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.04671535640954971 norm:0.0005196301499381661 max memory_allocated 22562.85107421875 
[2025-03-04 13:54:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.046426575630903244 norm:0.0004561904934234917 max memory_allocated 22562.85107421875 
[2025-03-04 13:54:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.04624580219388008 norm:0.00040482135955244303 max memory_allocated 22562.85107421875 
[2025-03-04 13:55:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0461001992225647 norm:0.0003746612055692822 max memory_allocated 22562.85107421875 
[2025-03-04 13:55:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0460054874420166 norm:0.00036868645111098886 max memory_allocated 22562.85107421875 
[2025-03-04 13:56:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.04593218117952347 norm:0.00035388837568461895 max memory_allocated 22562.85107421875 
[2025-03-04 13:56:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.04591849446296692 norm:0.00034489473910070956 max memory_allocated 22562.85107421875 
[2025-03-04 13:57:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.045885998755693436 norm:0.0003158964682370424 max memory_allocated 22562.85107421875 
[2025-03-04 13:57:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.04587777704000473 norm:0.00030126923229545355 max memory_allocated 22562.85107421875 
[2025-03-04 13:58:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.045860521495342255 norm:0.0002802032686304301 max memory_allocated 22562.85107421875 
[2025-03-04 13:59:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.045811425894498825 norm:0.0002734622685238719 max memory_allocated 22562.85107421875 
[2025-03-04 13:59:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.045756541192531586 norm:0.0002670256362762302 max memory_allocated 22562.85107421875 
[2025-03-04 14:00:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.04574611037969589 norm:0.0002588049683254212 max memory_allocated 22562.85107421875 
[2025-03-04 14:00:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.045771338045597076 norm:0.00025991792790591717 max memory_allocated 22562.85107421875 
[2025-03-04 14:01:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.04572661221027374 norm:0.00025002422626130283 max memory_allocated 22562.85107421875 
[2025-03-04 14:01:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-03-04 14:02:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0981660783290863 norm:0.012420974671840668 max memory_allocated 22563.02294921875 
[2025-03-04 14:02:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.07603125274181366 norm:0.004494986962527037 max memory_allocated 22563.02294921875 
[2025-03-04 14:03:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.06051236391067505 norm:0.0014306243974715471 max memory_allocated 22563.02294921875 
[2025-03-04 14:03:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.05585745349526405 norm:0.0007874936563894153 max memory_allocated 22563.02294921875 
[2025-03-04 14:04:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.05391236022114754 norm:0.0005972895887680352 max memory_allocated 22563.02294921875 
[2025-03-04 14:04:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.052957188338041306 norm:0.0005364137468859553 max memory_allocated 22563.02294921875 
[2025-03-04 14:05:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0525033138692379 norm:0.0005304806982167065 max memory_allocated 22563.02294921875 
[2025-03-04 14:05:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.05225532129406929 norm:0.0004911727737635374 max memory_allocated 22563.02294921875 
[2025-03-04 14:06:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.05214051157236099 norm:0.0004615520592778921 max memory_allocated 22563.02294921875 
[2025-03-04 14:07:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.051988061517477036 norm:0.00043475592974573374 max memory_allocated 22563.02294921875 
[2025-03-04 14:07:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.05177229270339012 norm:0.00039700634079054 max memory_allocated 22563.02294921875 
[2025-03-04 14:08:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.051828447729349136 norm:0.0003657979832496494 max memory_allocated 22563.02294921875 
[2025-03-04 14:08:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.05180606618523598 norm:0.0003373041981831193 max memory_allocated 22563.02294921875 
[2025-03-04 14:09:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.05169824883341789 norm:0.00033684115624055266 max memory_allocated 22563.02294921875 
[2025-03-04 14:09:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.05168721079826355 norm:0.00032707967329770327 max memory_allocated 22563.02294921875 
[2025-03-04 14:10:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.05162543058395386 norm:0.00030825435533188283 max memory_allocated 22563.02294921875 
[2025-03-04 14:11:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.051585353910923004 norm:0.000322130013955757 max memory_allocated 22563.02294921875 
[2025-03-04 14:11:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.051505543291568756 norm:0.0003190385177731514 max memory_allocated 22563.02294921875 
[2025-03-04 14:12:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.051431361585855484 norm:0.00031781449797563255 max memory_allocated 22563.02294921875 
[2025-03-04 14:12:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.051468104124069214 norm:0.00031633832259103656 max memory_allocated 22563.02294921875 
[2025-03-04 14:12:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-03-04 14:13:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.10289131104946136 norm:0.012536085210740566 max memory_allocated 22563.19482421875 
[2025-03-04 14:14:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.08020661771297455 norm:0.004543146584182978 max memory_allocated 22563.19482421875 
[2025-03-04 14:14:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.06363996863365173 norm:0.001020282506942749 max memory_allocated 22563.19482421875 
[2025-03-04 14:15:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.059835322201251984 norm:0.000627820088993758 max memory_allocated 22563.19482421875 
[2025-03-04 14:15:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.05825372412800789 norm:0.0006292420439422131 max memory_allocated 22563.19482421875 
[2025-03-04 14:16:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.057480305433273315 norm:0.0005811852170154452 max memory_allocated 22563.19482421875 
[2025-03-04 14:16:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.057038675993680954 norm:0.0005253132549114525 max memory_allocated 22563.19482421875 
[2025-03-04 14:17:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.05675952136516571 norm:0.0004655678931158036 max memory_allocated 22563.19482421875 
[2025-03-04 14:17:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.05661158263683319 norm:0.00046069954987615347 max memory_allocated 22563.19482421875 
[2025-03-04 14:18:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.056493643671274185 norm:0.00041711016092449427 max memory_allocated 22563.19482421875 
[2025-03-04 14:19:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.05631887540221214 norm:0.0003666376869659871 max memory_allocated 22563.19482421875 
[2025-03-04 14:19:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.05627993121743202 norm:0.00037920326576568186 max memory_allocated 22563.19482421875 
[2025-03-04 14:20:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.05623548477888107 norm:0.00034490571124479175 max memory_allocated 22563.19482421875 
[2025-03-04 14:20:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.05618055909872055 norm:0.0003641875518951565 max memory_allocated 22563.19482421875 
[2025-03-04 14:21:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.05616135522723198 norm:0.00033689310657791793 max memory_allocated 22563.19482421875 
[2025-03-04 14:21:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.05608587712049484 norm:0.0003235015319660306 max memory_allocated 22563.19482421875 
[2025-03-04 14:22:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.05612245947122574 norm:0.000320204853778705 max memory_allocated 22563.19482421875 
[2025-03-04 14:22:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.056122422218322754 norm:0.0003144717775285244 max memory_allocated 22563.19482421875 
[2025-03-04 14:23:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0562056303024292 norm:0.00029940810054540634 max memory_allocated 22563.19482421875 
[2025-03-04 14:24:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.056258127093315125 norm:0.0002876522485166788 max memory_allocated 22563.19482421875 
[2025-03-04 14:24:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-03-04 14:24:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0962386503815651 norm:0.005197917111217976 max memory_allocated 22563.36669921875 
[2025-03-04 14:25:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.07839097082614899 norm:0.0017777126049622893 max memory_allocated 22563.36669921875 
[2025-03-04 14:25:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06668034195899963 norm:0.0005341594805940986 max memory_allocated 22563.36669921875 
[2025-03-04 14:26:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.06352220475673676 norm:0.00037625496042892337 max memory_allocated 22563.36669921875 
[2025-03-04 14:27:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.06206200644373894 norm:0.0003464371548034251 max memory_allocated 22563.36669921875 
[2025-03-04 14:27:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.061431631445884705 norm:0.0003497232683002949 max memory_allocated 22563.36669921875 
[2025-03-04 14:28:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.061047062277793884 norm:0.0003285827406216413 max memory_allocated 22563.36669921875 
[2025-03-04 14:28:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0607687346637249 norm:0.00030763528775423765 max memory_allocated 22563.36669921875 
[2025-03-04 14:29:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.060672078281641006 norm:0.00028355425456538796 max memory_allocated 22563.36669921875 
[2025-03-04 14:29:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.060598187148571014 norm:0.0002881235850509256 max memory_allocated 22563.36669921875 
[2025-03-04 14:30:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.06049865111708641 norm:0.00025808357167989016 max memory_allocated 22563.36669921875 
[2025-03-04 14:31:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0603964626789093 norm:0.0002456397924106568 max memory_allocated 22563.36669921875 
[2025-03-04 14:31:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0603741854429245 norm:0.00024401281552854925 max memory_allocated 22563.36669921875 
[2025-03-04 14:32:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.06041719391942024 norm:0.00024855989613570273 max memory_allocated 22563.36669921875 
[2025-03-04 14:32:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.06047373265028 norm:0.0002664876519702375 max memory_allocated 22563.36669921875 
[2025-03-04 14:33:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.060469914227724075 norm:0.00027712766313925385 max memory_allocated 22563.36669921875 
[2025-03-04 14:33:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.06051240116357803 norm:0.0002674772695172578 max memory_allocated 22563.36669921875 
[2025-03-04 14:34:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.06055291369557381 norm:0.00026614480884745717 max memory_allocated 22563.36669921875 
[2025-03-04 14:34:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.060443948954343796 norm:0.0002579912543296814 max memory_allocated 22563.36669921875 
[2025-03-04 14:35:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.060462478548288345 norm:0.0002493375795893371 max memory_allocated 22563.36669921875 
[2025-03-04 14:35:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-03-04 14:36:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.10138477385044098 norm:0.009350808337330818 max memory_allocated 22563.53857421875 
[2025-03-04 14:36:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.08531960099935532 norm:0.004199164919555187 max memory_allocated 22563.53857421875 
[2025-03-04 14:37:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.07112092524766922 norm:0.0011277557350695133 max memory_allocated 22563.53857421875 
[2025-03-04 14:37:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.06720726191997528 norm:0.0005869954475201666 max memory_allocated 22563.53857421875 
[2025-03-04 14:38:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.06573164463043213 norm:0.00046128398389555514 max memory_allocated 22563.53857421875 
[2025-03-04 14:39:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.06496570259332657 norm:0.0004174419154878706 max memory_allocated 22563.53857421875 
[2025-03-04 14:39:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0645785927772522 norm:0.00036302226362749934 max memory_allocated 22563.53857421875 
[2025-03-04 14:40:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.06433448195457458 norm:0.00032339253812097013 max memory_allocated 22563.53857421875 
[2025-03-04 14:40:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0641842633485794 norm:0.00030469015473499894 max memory_allocated 22563.53857421875 
[2025-03-04 14:41:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.06402348726987839 norm:0.00028686693985946476 max memory_allocated 22563.53857421875 
[2025-03-04 14:41:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.06392526626586914 norm:0.00026710634119808674 max memory_allocated 22563.53857421875 
[2025-03-04 14:42:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.06389997899532318 norm:0.0002694658178370446 max memory_allocated 22563.53857421875 
[2025-03-04 14:42:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.06381259858608246 norm:0.00025513701257295907 max memory_allocated 22563.53857421875 
[2025-03-04 14:43:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.06377903372049332 norm:0.0002579449210315943 max memory_allocated 22563.53857421875 
[2025-03-04 14:44:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.06373327225446701 norm:0.00025258897221647203 max memory_allocated 22563.53857421875 
[2025-03-04 14:44:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0637257769703865 norm:0.0002542597649153322 max memory_allocated 22563.53857421875 
[2025-03-04 14:45:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0637083575129509 norm:0.000252235506195575 max memory_allocated 22563.53857421875 
[2025-03-04 14:45:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06377431005239487 norm:0.00025116096367128193 max memory_allocated 22563.53857421875 
[2025-03-04 14:46:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0637965276837349 norm:0.00025779716088436544 max memory_allocated 22563.53857421875 
[2025-03-04 14:46:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.06383010745048523 norm:0.00025217593065463006 max memory_allocated 22563.53857421875 
[2025-03-04 14:47:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-03-04 14:47:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.1017279103398323 norm:0.00504994485527277 max memory_allocated 22563.71044921875 
[2025-03-04 14:48:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.08704414963722229 norm:0.0022697204258292913 max memory_allocated 22563.71044921875 
[2025-03-04 14:48:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0742744505405426 norm:0.0009392377687618136 max memory_allocated 22563.71044921875 
[2025-03-04 14:49:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.070160411298275 norm:0.0004648587491828948 max memory_allocated 22563.71044921875 
[2025-03-04 14:49:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.06875745952129364 norm:0.0003527216031216085 max memory_allocated 22563.71044921875 
[2025-03-04 14:50:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.06800924241542816 norm:0.00029533024644479156 max memory_allocated 22563.71044921875 
[2025-03-04 14:51:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.06758742034435272 norm:0.0002789149002637714 max memory_allocated 22563.71044921875 
[2025-03-04 14:51:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.06738384813070297 norm:0.0002843509428203106 max memory_allocated 22563.71044921875 
[2025-03-04 14:52:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.06724977493286133 norm:0.0002817395143210888 max memory_allocated 22563.71044921875 
[2025-03-04 14:52:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0671844407916069 norm:0.0002725726808421314 max memory_allocated 22563.71044921875 
[2025-03-04 14:53:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.06712181121110916 norm:0.00025641967658884823 max memory_allocated 22563.71044921875 
[2025-03-04 14:53:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.06711310893297195 norm:0.0002455639187246561 max memory_allocated 22563.71044921875 
[2025-03-04 14:54:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.06708580255508423 norm:0.00022875018476042897 max memory_allocated 22563.71044921875 
[2025-03-04 14:54:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0670345351099968 norm:0.00022438107407651842 max memory_allocated 22563.71044921875 
[2025-03-04 14:55:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.06699632108211517 norm:0.0002251807600259781 max memory_allocated 22563.71044921875 
[2025-03-04 14:56:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.06696269661188126 norm:0.00022483625798486173 max memory_allocated 22563.71044921875 
[2025-03-04 14:56:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.06691751629114151 norm:0.00021785381250083447 max memory_allocated 22563.71044921875 
[2025-03-04 14:57:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.06691122055053711 norm:0.00021539595036301762 max memory_allocated 22563.71044921875 
[2025-03-04 14:57:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0668850764632225 norm:0.0002122355654137209 max memory_allocated 22563.71044921875 
[2025-03-04 14:58:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.06686273962259293 norm:0.00021278197527863085 max memory_allocated 22563.71044921875 
[2025-03-04 14:58:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-03-04 14:59:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.10220475494861603 norm:0.004455817863345146 max memory_allocated 22563.88232421875 
[2025-03-04 14:59:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.08813655376434326 norm:0.0019435370340943336 max memory_allocated 22563.88232421875 
[2025-03-04 15:00:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0758245587348938 norm:0.0006496122223325074 max memory_allocated 22563.88232421875 
[2025-03-04 15:00:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.07192232459783554 norm:0.0003765205619856715 max memory_allocated 22563.88232421875 
[2025-03-04 15:01:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.07054388523101807 norm:0.0003207145200576633 max memory_allocated 22563.88232421875 
[2025-03-04 15:01:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.06982231885194778 norm:0.0002849434386007488 max memory_allocated 22563.88232421875 
[2025-03-04 15:02:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.06944943964481354 norm:0.0002587534545455128 max memory_allocated 22563.88232421875 
[2025-03-04 15:02:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.06924548745155334 norm:0.00024301838129758835 max memory_allocated 22563.88232421875 
[2025-03-04 15:03:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0691225677728653 norm:0.00022781592269893736 max memory_allocated 22563.88232421875 
[2025-03-04 15:04:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.06905671209096909 norm:0.00023118904209695756 max memory_allocated 22563.88232421875 
[2025-03-04 15:04:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.06900137662887573 norm:0.0002214278356404975 max memory_allocated 22563.88232421875 
[2025-03-04 15:05:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.06901088356971741 norm:0.00022717866522725672 max memory_allocated 22563.88232421875 
[2025-03-04 15:05:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.06898617744445801 norm:0.00021592294797301292 max memory_allocated 22563.88232421875 
[2025-03-04 15:06:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.06895256042480469 norm:0.0002063339634332806 max memory_allocated 22563.88232421875 
[2025-03-04 15:06:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.06894292682409286 norm:0.00020483472326304764 max memory_allocated 22563.88232421875 
[2025-03-04 15:07:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.06895323097705841 norm:0.00020303556811995804 max memory_allocated 22563.88232421875 
[2025-03-04 15:07:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.06892773509025574 norm:0.0002112294896505773 max memory_allocated 22563.88232421875 
[2025-03-04 15:08:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.06889872252941132 norm:0.00020233502436894923 max memory_allocated 22563.88232421875 
[2025-03-04 15:09:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.06891757994890213 norm:0.00020205078180879354 max memory_allocated 22563.88232421875 
[2025-03-04 15:09:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.06891226023435593 norm:0.0002012693730648607 max memory_allocated 22563.88232421875 
[2025-03-04 15:09:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-03-04 15:10:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.10079731792211533 norm:0.0042332434095442295 max memory_allocated 22564.05419921875 
[2025-03-04 15:10:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.08838453143835068 norm:0.0019833745900541544 max memory_allocated 22564.05419921875 
[2025-03-04 15:11:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0774155855178833 norm:0.0006679201032966375 max memory_allocated 22564.05419921875 
[2025-03-04 15:12:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.07391978800296783 norm:0.00035595346707850695 max memory_allocated 22564.05419921875 
[2025-03-04 15:12:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.07255873084068298 norm:0.00028901660698466003 max memory_allocated 22564.05419921875 
[2025-03-04 15:13:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.07188107818365097 norm:0.00029535111389122903 max memory_allocated 22564.05419921875 
[2025-03-04 15:13:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.07149290293455124 norm:0.00023218191927298903 max memory_allocated 22564.05419921875 
[2025-03-04 15:14:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.07127562910318375 norm:0.0002415830676909536 max memory_allocated 22564.05419921875 
[2025-03-04 15:14:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.07114195078611374 norm:0.00023988027533050627 max memory_allocated 22564.05419921875 
[2025-03-04 15:15:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.07101505994796753 norm:0.00020959213725291193 max memory_allocated 22564.05419921875 
[2025-03-04 15:16:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.070940762758255 norm:0.00021164001373108476 max memory_allocated 22564.05419921875 
[2025-03-04 15:16:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.07092396914958954 norm:0.00022011861437931657 max memory_allocated 22564.05419921875 
[2025-03-04 15:17:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.07090616226196289 norm:0.00021046039182692766 max memory_allocated 22564.05419921875 
[2025-03-04 15:17:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0709047019481659 norm:0.00019893984426744282 max memory_allocated 22564.05419921875 
[2025-03-04 15:18:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.07087915390729904 norm:0.00020406837575137615 max memory_allocated 22564.05419921875 
[2025-03-04 15:18:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.07083005458116531 norm:0.00020093971397727728 max memory_allocated 22564.05419921875 
[2025-03-04 15:19:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.07080122828483582 norm:0.00019539207278285176 max memory_allocated 22564.05419921875 
[2025-03-04 15:19:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.07079929858446121 norm:0.0001878717594081536 max memory_allocated 22564.05419921875 
[2025-03-04 15:20:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.07078461349010468 norm:0.00018299742077942938 max memory_allocated 22564.05419921875 
[2025-03-04 15:21:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.07083438336849213 norm:0.00018802065460477024 max memory_allocated 22564.05419921875 
[2025-03-04 15:21:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-03-04 15:21:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.10485110431909561 norm:0.008558179251849651 max memory_allocated 22564.22607421875 
[2025-03-04 15:22:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.09017705917358398 norm:0.0038158646784722805 max memory_allocated 22564.22607421875 
[2025-03-04 15:22:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.07727713137865067 norm:0.0009885173058137298 max memory_allocated 22564.22607421875 
[2025-03-04 15:23:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.07389972358942032 norm:0.0005394986364990473 max memory_allocated 22564.22607421875 
[2025-03-04 15:24:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.07254590094089508 norm:0.00046486553037539124 max memory_allocated 22564.22607421875 
[2025-03-04 15:24:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.07174057513475418 norm:0.000396136922063306 max memory_allocated 22564.22607421875 
[2025-03-04 15:25:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.07125154882669449 norm:0.00034800719004124403 max memory_allocated 22564.22607421875 
[2025-03-04 15:25:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.07093319296836853 norm:0.0003381603746674955 max memory_allocated 22564.22607421875 
[2025-03-04 15:26:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.07074001431465149 norm:0.00032136464142240584 max memory_allocated 22564.22607421875 
[2025-03-04 15:26:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.07061953842639923 norm:0.0002952100185211748 max memory_allocated 22564.22607421875 
[2025-03-04 15:27:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.07054223865270615 norm:0.00026537812664173543 max memory_allocated 22564.22607421875 
[2025-03-04 15:27:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.07042323052883148 norm:0.00024268323613796383 max memory_allocated 22564.22607421875 
[2025-03-04 15:28:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.07036042958498001 norm:0.00023845364921726286 max memory_allocated 22564.22607421875 
[2025-03-04 15:29:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.07031753659248352 norm:0.00023290596436709166 max memory_allocated 22564.22607421875 
[2025-03-04 15:29:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.07028253376483917 norm:0.0002372970338910818 max memory_allocated 22564.22607421875 
[2025-03-04 15:30:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.07027007639408112 norm:0.00021806423319503665 max memory_allocated 22564.22607421875 
[2025-03-04 15:30:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.07025585323572159 norm:0.00019764192984439433 max memory_allocated 22564.22607421875 
[2025-03-04 15:31:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.07024043053388596 norm:0.00018812628695741296 max memory_allocated 22564.22607421875 
[2025-03-04 15:31:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0702577605843544 norm:0.00018799383542500436 max memory_allocated 22564.22607421875 
[2025-03-04 15:32:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.07024070620536804 norm:0.00019165125559084117 max memory_allocated 22564.22607421875 
[2025-03-04 15:32:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-03-04 15:33:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.09736564755439758 norm:0.00378625956363976 max memory_allocated 22564.39794921875 
[2025-03-04 15:33:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.08678167313337326 norm:0.0018504300387576222 max memory_allocated 22564.39794921875 
[2025-03-04 15:34:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.07792004197835922 norm:0.0006624262896366417 max memory_allocated 22564.39794921875 
[2025-03-04 15:34:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.07488817721605301 norm:0.0003440801228862256 max memory_allocated 22564.39794921875 
[2025-03-04 15:35:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.07365347445011139 norm:0.0002592821838334203 max memory_allocated 22564.39794921875 
[2025-03-04 15:36:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.0729733407497406 norm:0.0002565575123298913 max memory_allocated 22564.39794921875 
[2025-03-04 15:36:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.07269355654716492 norm:0.0002607684291433543 max memory_allocated 22564.39794921875 
[2025-03-04 15:37:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.07249532639980316 norm:0.0002339401689823717 max memory_allocated 22564.39794921875 
[2025-03-04 15:37:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.07229182869195938 norm:0.00021024294255767018 max memory_allocated 22564.39794921875 
[2025-03-04 15:38:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.07219527661800385 norm:0.00019122325466014445 max memory_allocated 22564.39794921875 
[2025-03-04 15:38:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.07214336842298508 norm:0.00018994181300513446 max memory_allocated 22564.39794921875 
[2025-03-04 15:39:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.07205317169427872 norm:0.0001744866749504581 max memory_allocated 22564.39794921875 
[2025-03-04 15:39:56 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.07202090322971344 norm:0.00018344185082241893 max memory_allocated 22564.39794921875 
[2025-03-04 15:40:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.07198816537857056 norm:0.00016416709695477039 max memory_allocated 22564.39794921875 
[2025-03-04 15:41:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.07197362184524536 norm:0.00015388715837616473 max memory_allocated 22564.39794921875 
[2025-03-04 15:41:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.07201205939054489 norm:0.00014437359641306102 max memory_allocated 22564.39794921875 
[2025-03-04 15:42:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.07203716039657593 norm:0.00014407315757125616 max memory_allocated 22564.39794921875 
[2025-03-04 15:42:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.07202717661857605 norm:0.0001430781849194318 max memory_allocated 22564.39794921875 
[2025-03-04 15:43:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.071983203291893 norm:0.00014583763550035655 max memory_allocated 22564.39794921875 
[2025-03-04 15:43:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.0719907358288765 norm:0.00014505107537843287 max memory_allocated 22564.39794921875 
[2025-03-04 15:44:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-03-04 15:44:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.10763448476791382 norm:0.0075856889598071575 max memory_allocated 22564.56982421875 
[2025-03-04 15:45:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.09240774810314178 norm:0.003159104147925973 max memory_allocated 22564.56982421875 
[2025-03-04 15:45:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.07980357110500336 norm:0.0010855876607820392 max memory_allocated 22564.56982421875 
[2025-03-04 15:46:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.07619920372962952 norm:0.0006027849740348756 max memory_allocated 22564.56982421875 
[2025-03-04 15:46:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.07473117113113403 norm:0.0004737813724204898 max memory_allocated 22564.56982421875 
[2025-03-04 15:47:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0737890899181366 norm:0.0003673285245895386 max memory_allocated 22564.56982421875 
[2025-03-04 15:47:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.07329808175563812 norm:0.0003616797039285302 max memory_allocated 22564.56982421875 
[2025-03-04 15:48:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.07302729785442352 norm:0.00033955214894376695 max memory_allocated 22564.56982421875 
[2025-03-04 15:49:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.07286226749420166 norm:0.00032340644975192845 max memory_allocated 22564.56982421875 
[2025-03-04 15:49:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.07264397293329239 norm:0.0002692990528885275 max memory_allocated 22564.56982421875 
[2025-03-04 15:50:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.07257092744112015 norm:0.00027946275076828897 max memory_allocated 22564.56982421875 
[2025-03-04 15:50:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.07250230014324188 norm:0.0002744953380897641 max memory_allocated 22564.56982421875 
[2025-03-04 15:51:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.07238271832466125 norm:0.0002616473357193172 max memory_allocated 22564.56982421875 
[2025-03-04 15:51:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.07233928143978119 norm:0.0002503278956282884 max memory_allocated 22564.56982421875 
[2025-03-04 15:52:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.07225731015205383 norm:0.00022436122526414692 max memory_allocated 22564.56982421875 
[2025-03-04 15:53:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.07219412177801132 norm:0.0002079803089145571 max memory_allocated 22564.56982421875 
[2025-03-04 15:53:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.07217708230018616 norm:0.0002042038831859827 max memory_allocated 22564.56982421875 
[2025-03-04 15:54:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.07210791110992432 norm:0.0001863319776020944 max memory_allocated 22564.56982421875 
[2025-03-04 15:54:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.07209669798612595 norm:0.00020146576571278274 max memory_allocated 22564.56982421875 
[2025-03-04 15:55:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.07211078703403473 norm:0.00020372247672639787 max memory_allocated 22564.56982421875 
[2025-03-04 15:55:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-03-04 15:56:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.10653871297836304 norm:0.007475856691598892 max memory_allocated 22564.74169921875 
[2025-03-04 15:56:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.0942452996969223 norm:0.0035907593555748463 max memory_allocated 22564.74169921875 
[2025-03-04 15:57:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.08188492059707642 norm:0.0011749661061912775 max memory_allocated 22564.74169921875 
[2025-03-04 15:57:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.07827054709196091 norm:0.0006163152866065502 max memory_allocated 22564.74169921875 
[2025-03-04 15:58:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0768648311495781 norm:0.0004946005647070706 max memory_allocated 22564.74169921875 
[2025-03-04 15:58:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.07603496313095093 norm:0.00040815846296027303 max memory_allocated 22564.74169921875 
[2025-03-04 15:59:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.07548938691616058 norm:0.00037075133877806365 max memory_allocated 22564.74169921875 
[2025-03-04 15:59:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.07517048716545105 norm:0.00035472249146550894 max memory_allocated 22564.74169921875 
[2025-03-04 16:00:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.07504729926586151 norm:0.00036106351763010025 max memory_allocated 22564.74169921875 
[2025-03-04 16:01:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.07486605644226074 norm:0.00033115825499407947 max memory_allocated 22564.74169921875 
[2025-03-04 16:01:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.07475357502698898 norm:0.00031214466434903443 max memory_allocated 22564.74169921875 
[2025-03-04 16:02:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.07462812215089798 norm:0.0002860703389160335 max memory_allocated 22564.74169921875 
[2025-03-04 16:02:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.07451902329921722 norm:0.0002769753918983042 max memory_allocated 22564.74169921875 
[2025-03-04 16:03:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.07444175332784653 norm:0.0002749286650214344 max memory_allocated 22564.74169921875 
[2025-03-04 16:03:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.07432542741298676 norm:0.0002600534353405237 max memory_allocated 22564.74169921875 
[2025-03-04 16:04:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.07425782084465027 norm:0.0002394015755271539 max memory_allocated 22564.74169921875 
[2025-03-04 16:04:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.07420572638511658 norm:0.00021098239812999964 max memory_allocated 22564.74169921875 
[2025-03-04 16:05:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.07414679229259491 norm:0.0001987835712498054 max memory_allocated 22564.74169921875 
[2025-03-04 16:06:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.07407760620117188 norm:0.00019470020197331905 max memory_allocated 22564.74169921875 
[2025-03-04 16:06:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.07404686510562897 norm:0.00019907363457605243 max memory_allocated 22564.74169921875 
[2025-03-04 16:06:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-03-04 16:07:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.10559356212615967 norm:0.00883850734680891 max memory_allocated 22564.91357421875 
[2025-03-04 16:07:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.09333101660013199 norm:0.004054219461977482 max memory_allocated 22564.91357421875 
[2025-03-04 16:08:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.08254918456077576 norm:0.0013254089280962944 max memory_allocated 22564.91357421875 
[2025-03-04 16:09:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.07957647740840912 norm:0.0005584908649325371 max memory_allocated 22564.91357421875 
[2025-03-04 16:09:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.07835656404495239 norm:0.00044350529788061976 max memory_allocated 22564.91357421875 
[2025-03-04 16:10:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.07758975774049759 norm:0.0003904654295183718 max memory_allocated 22564.91357421875 
[2025-03-04 16:10:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.07717739045619965 norm:0.0003683918039314449 max memory_allocated 22564.91357421875 
[2025-03-04 16:11:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.07698442041873932 norm:0.00036016147350892425 max memory_allocated 22564.91357421875 
[2025-03-04 16:11:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.07675822824239731 norm:0.00030890197376720607 max memory_allocated 22564.91357421875 
[2025-03-04 16:12:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.07665769010782242 norm:0.0003222844679839909 max memory_allocated 22564.91357421875 
[2025-03-04 16:13:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.07656995207071304 norm:0.00029951921897009015 max memory_allocated 22564.91357421875 
[2025-03-04 16:13:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.07655152678489685 norm:0.0002818371285684407 max memory_allocated 22564.91357421875 
[2025-03-04 16:14:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.0764494240283966 norm:0.0002542686415836215 max memory_allocated 22564.91357421875 
[2025-03-04 16:14:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.07650433480739594 norm:0.00024883978767320514 max memory_allocated 22564.91357421875 
[2025-03-04 16:15:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.07645287364721298 norm:0.00022297546092886478 max memory_allocated 22564.91357421875 
[2025-03-04 16:15:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.07638971507549286 norm:0.00020904325356241316 max memory_allocated 22564.91357421875 
[2025-03-04 16:16:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.07628728449344635 norm:0.0001962760288733989 max memory_allocated 22564.91357421875 
[2025-03-04 16:16:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.07627546042203903 norm:0.00019535975297912955 max memory_allocated 22564.91357421875 
[2025-03-04 16:17:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.07626178860664368 norm:0.0001849330437835306 max memory_allocated 22564.91357421875 
[2025-03-04 16:18:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.07623612880706787 norm:0.00018405304581392556 max memory_allocated 22564.91357421875 
[2025-03-04 16:18:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-03-04 16:18:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.12354593724012375 norm:0.012715584598481655 max memory_allocated 22565.08544921875 
[2025-03-04 16:19:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.10648667067289352 norm:0.005516338162124157 max memory_allocated 22565.08544921875 
[2025-03-04 16:19:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.09219326823949814 norm:0.002184439916163683 max memory_allocated 22565.08544921875 
[2025-03-04 16:20:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.08728937804698944 norm:0.0009334575734101236 max memory_allocated 22565.08544921875 
[2025-03-04 16:21:03 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.08538826555013657 norm:0.0006533007835969329 max memory_allocated 22565.08544921875 
[2025-03-04 16:21:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.08431418985128403 norm:0.0005554068484343588 max memory_allocated 22565.08544921875 
[2025-03-04 16:22:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.08367092162370682 norm:0.0004684916930273175 max memory_allocated 22565.08544921875 
[2025-03-04 16:22:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.0833323672413826 norm:0.0004418015596456826 max memory_allocated 22565.08544921875 
[2025-03-04 16:23:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.08314523845911026 norm:0.00042219311580993235 max memory_allocated 22565.08544921875 
[2025-03-04 16:23:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.08302991837263107 norm:0.0003955880820285529 max memory_allocated 22565.08544921875 
[2025-03-04 16:24:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.08285275101661682 norm:0.0003680730005726218 max memory_allocated 22565.08544921875 
[2025-03-04 16:24:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.08260542154312134 norm:0.0003331287298351526 max memory_allocated 22565.08544921875 
[2025-03-04 16:25:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.08252564817667007 norm:0.00031751341884955764 max memory_allocated 22565.08544921875 
[2025-03-04 16:26:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.08241070806980133 norm:0.0002864970883820206 max memory_allocated 22565.08544921875 
[2025-03-04 16:26:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.08232095092535019 norm:0.0003102025657426566 max memory_allocated 22565.08544921875 
[2025-03-04 16:27:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.08225877583026886 norm:0.00026560318656265736 max memory_allocated 22565.08544921875 
[2025-03-04 16:27:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.08213205635547638 norm:0.0002406431158306077 max memory_allocated 22565.08544921875 
[2025-03-04 16:28:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.08210805058479309 norm:0.00025321735301986337 max memory_allocated 22565.08544921875 
[2025-03-04 16:28:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.08209885656833649 norm:0.00026038006762973964 max memory_allocated 22565.08544921875 
[2025-03-04 16:29:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.08208580315113068 norm:0.00024410520563833416 max memory_allocated 22565.08544921875 
[2025-03-04 16:29:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-03-04 16:30:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.12166031450033188 norm:0.0094041982665658 max memory_allocated 22565.25732421875 
[2025-03-04 16:30:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.11058209836483002 norm:0.004629771690815687 max memory_allocated 22565.25732421875 
[2025-03-04 16:31:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.09656834602355957 norm:0.001623631571419537 max memory_allocated 22565.25732421875 
[2025-03-04 16:31:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.09265508502721786 norm:0.0006460187141783535 max memory_allocated 22565.25732421875 
[2025-03-04 16:32:26 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.09109070897102356 norm:0.0003978860331699252 max memory_allocated 22565.25732421875 
[2025-03-04 16:32:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.09022103995084763 norm:0.00036086642649024725 max memory_allocated 22565.25732421875 
[2025-03-04 16:33:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.08972519636154175 norm:0.00033685879316180944 max memory_allocated 22565.25732421875 
[2025-03-04 16:34:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.08952344208955765 norm:0.00034171255538240075 max memory_allocated 22565.25732421875 
[2025-03-04 16:34:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.08935251086950302 norm:0.00033413537312299013 max memory_allocated 22565.25732421875 
[2025-03-04 16:35:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.08922575414180756 norm:0.0003186429385095835 max memory_allocated 22565.25732421875 
[2025-03-04 16:35:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.08905894309282303 norm:0.00029289667145349085 max memory_allocated 22565.25732421875 
[2025-03-04 16:36:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.08895940333604813 norm:0.0002668139641173184 max memory_allocated 22565.25732421875 
[2025-03-04 16:36:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.08884262293577194 norm:0.000249630946200341 max memory_allocated 22565.25732421875 
[2025-03-04 16:37:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.08891747891902924 norm:0.00025096419267356396 max memory_allocated 22565.25732421875 
[2025-03-04 16:38:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.08886291086673737 norm:0.00022090556740295142 max memory_allocated 22565.25732421875 
[2025-03-04 16:38:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.0887426882982254 norm:0.00021470182400662452 max memory_allocated 22565.25732421875 
[2025-03-04 16:39:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.08868075907230377 norm:0.00021516962442547083 max memory_allocated 22565.25732421875 
[2025-03-04 16:39:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.08860624581575394 norm:0.00020179548300802708 max memory_allocated 22565.25732421875 
[2025-03-04 16:40:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.088564433157444 norm:0.00018778254161588848 max memory_allocated 22565.25732421875 
[2025-03-04 16:40:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.08854543417692184 norm:0.00017729593673720956 max memory_allocated 22565.25732421875 
[2025-03-04 16:40:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-03-04 16:41:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.12720397114753723 norm:0.007078383583575487 max memory_allocated 22565.42919921875 
[2025-03-04 16:42:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.11688973009586334 norm:0.004006678704172373 max memory_allocated 22565.42919921875 
[2025-03-04 16:42:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.10594500601291656 norm:0.0013923821970820427 max memory_allocated 22565.42919921875 
[2025-03-04 16:43:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.10253975540399551 norm:0.0006541825132444501 max memory_allocated 22565.42919921875 
[2025-03-04 16:43:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.10110345482826233 norm:0.0005209319060668349 max memory_allocated 22565.42919921875 
[2025-03-04 16:44:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.10025877505540848 norm:0.0004620191175490618 max memory_allocated 22565.42919921875 
[2025-03-04 16:44:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.09975962340831757 norm:0.000413781963288784 max memory_allocated 22565.42919921875 
[2025-03-04 16:45:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.0994710624217987 norm:0.00041284141479991376 max memory_allocated 22565.42919921875 
[2025-03-04 16:46:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.09924812614917755 norm:0.000361686252290383 max memory_allocated 22565.42919921875 
[2025-03-04 16:46:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.09909486770629883 norm:0.0003414595266804099 max memory_allocated 22565.42919921875 
[2025-03-04 16:47:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.0989919900894165 norm:0.0003442159213591367 max memory_allocated 22565.42919921875 
[2025-03-04 16:47:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.09889520704746246 norm:0.00030941201839596033 max memory_allocated 22565.42919921875 
[2025-03-04 16:48:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.09881125390529633 norm:0.00028109439881518483 max memory_allocated 22565.42919921875 
[2025-03-04 16:48:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.09873080998659134 norm:0.000254994782153517 max memory_allocated 22565.42919921875 
[2025-03-04 16:49:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.09858521074056625 norm:0.00023718831653241068 max memory_allocated 22565.42919921875 
[2025-03-04 16:49:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.09845699369907379 norm:0.00024372365442104638 max memory_allocated 22565.42919921875 
[2025-03-04 16:50:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.09836265444755554 norm:0.0002364002139074728 max memory_allocated 22565.42919921875 
[2025-03-04 16:51:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.09830603748559952 norm:0.00021909241331741214 max memory_allocated 22565.42919921875 
[2025-03-04 16:51:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.09825639426708221 norm:0.00020521844271570444 max memory_allocated 22565.42919921875 
[2025-03-04 16:52:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.09817348420619965 norm:0.0001975933846551925 max memory_allocated 22565.42919921875 
[2025-03-04 16:52:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-03-04 16:52:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.13301077485084534 norm:0.006381565239280462 max memory_allocated 22565.60107421875 
[2025-03-04 16:53:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.12343503534793854 norm:0.0029120449908077717 max memory_allocated 22565.60107421875 
[2025-03-04 16:54:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.11441118270158768 norm:0.0010165665298700333 max memory_allocated 22565.60107421875 
[2025-03-04 16:54:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.1117236390709877 norm:0.00042028541793115437 max memory_allocated 22565.60107421875 
[2025-03-04 16:55:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.11052600294351578 norm:0.00036620604805648327 max memory_allocated 22565.60107421875 
[2025-03-04 16:55:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.1098484992980957 norm:0.0003348335740156472 max memory_allocated 22565.60107421875 
[2025-03-04 16:56:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.10952310264110565 norm:0.00032633121008984745 max memory_allocated 22565.60107421875 
[2025-03-04 16:56:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.10932037979364395 norm:0.0002884074056055397 max memory_allocated 22565.60107421875 
[2025-03-04 16:57:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.10920048505067825 norm:0.0002896727528423071 max memory_allocated 22565.60107421875 
[2025-03-04 16:57:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.10902482271194458 norm:0.0002359186764806509 max memory_allocated 22565.60107421875 
[2025-03-04 16:58:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.10892653465270996 norm:0.00023470344603992999 max memory_allocated 22565.60107421875 
[2025-03-04 16:59:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.10886602848768234 norm:0.00022142504167277366 max memory_allocated 22565.60107421875 
[2025-03-04 16:59:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.10878968983888626 norm:0.00020812946604564786 max memory_allocated 22565.60107421875 
[2025-03-04 17:00:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.10874440521001816 norm:0.00020256807329133153 max memory_allocated 22565.60107421875 
[2025-03-04 17:00:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.10871004313230515 norm:0.00017981557175517082 max memory_allocated 22565.60107421875 
[2025-03-04 17:01:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.1086929440498352 norm:0.00018130490207113326 max memory_allocated 22565.60107421875 
[2025-03-04 17:01:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.10868248343467712 norm:0.00017302170454058796 max memory_allocated 22565.60107421875 
[2025-03-04 17:02:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.1086510568857193 norm:0.00016316458641085774 max memory_allocated 22565.60107421875 
[2025-03-04 17:02:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.10862959921360016 norm:0.0001628028112463653 max memory_allocated 22565.60107421875 
[2025-03-04 17:03:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.10864052921533585 norm:0.00015605351654812694 max memory_allocated 22565.60107421875 
[2025-03-04 17:03:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-03-04 17:04:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.15175732970237732 norm:0.007286996114999056 max memory_allocated 22565.77294921875 
[2025-03-04 17:04:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.14046545326709747 norm:0.003194150049239397 max memory_allocated 22565.77294921875 
[2025-03-04 17:05:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.13017591834068298 norm:0.0012883890885859728 max memory_allocated 22565.77294921875 
[2025-03-04 17:05:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.1267349123954773 norm:0.0005620671436190605 max memory_allocated 22565.77294921875 
[2025-03-04 17:06:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.12552016973495483 norm:0.0005313327419571579 max memory_allocated 22565.77294921875 
[2025-03-04 17:07:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.12480271607637405 norm:0.00042740869685076177 max memory_allocated 22565.77294921875 
[2025-03-04 17:07:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.12451031804084778 norm:0.00041794360731728375 max memory_allocated 22565.77294921875 
[2025-03-04 17:08:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.12435460835695267 norm:0.000401085038902238 max memory_allocated 22565.77294921875 
[2025-03-04 17:08:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.12414731085300446 norm:0.00036738294875249267 max memory_allocated 22565.77294921875 
[2025-03-04 17:09:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.12396107614040375 norm:0.0003372184291947633 max memory_allocated 22565.77294921875 
[2025-03-04 17:09:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.12382201850414276 norm:0.00031519721960648894 max memory_allocated 22565.77294921875 
[2025-03-04 17:10:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.12377722561359406 norm:0.0003493344411253929 max memory_allocated 22565.77294921875 
[2025-03-04 17:11:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.123783640563488 norm:0.000322264910209924 max memory_allocated 22565.77294921875 
[2025-03-04 17:11:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.12367159873247147 norm:0.0003464123292360455 max memory_allocated 22565.77294921875 
[2025-03-04 17:12:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.12359344214200974 norm:0.00031683474662713706 max memory_allocated 22565.77294921875 
[2025-03-04 17:12:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.12356560677289963 norm:0.00026292307302355766 max memory_allocated 22565.77294921875 
[2025-03-04 17:13:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.12349538505077362 norm:0.0002611562376841903 max memory_allocated 22565.77294921875 
[2025-03-04 17:13:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.12343118339776993 norm:0.00024991121608763933 max memory_allocated 22565.77294921875 
[2025-03-04 17:14:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.12342506647109985 norm:0.00024102628231048584 max memory_allocated 22565.77294921875 
[2025-03-04 17:14:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.12334952503442764 norm:0.00026701384922489524 max memory_allocated 22565.77294921875 
[2025-03-04 17:15:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-03-04 17:15:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.16234980523586273 norm:0.004344872664660215 max memory_allocated 22565.94482421875 
[2025-03-04 17:16:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.1544182002544403 norm:0.002389357890933752 max memory_allocated 22565.94482421875 
[2025-03-04 17:16:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.14486631751060486 norm:0.0008479254320263863 max memory_allocated 22565.94482421875 
[2025-03-04 17:17:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.14214250445365906 norm:0.0004297604609746486 max memory_allocated 22565.94482421875 
[2025-03-04 17:17:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.14080995321273804 norm:0.00033261251519434154 max memory_allocated 22565.94482421875 
[2025-03-04 17:18:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.1402176320552826 norm:0.00032124784775078297 max memory_allocated 22565.94482421875 
[2025-03-04 17:19:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.13996416330337524 norm:0.0002972545917145908 max memory_allocated 22565.94482421875 
[2025-03-04 17:19:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.13983456790447235 norm:0.00028080324409529567 max memory_allocated 22565.94482421875 
[2025-03-04 17:20:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.13971824944019318 norm:0.0002696864539757371 max memory_allocated 22565.94482421875 
[2025-03-04 17:20:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.13964778184890747 norm:0.00025875691790133715 max memory_allocated 22565.94482421875 
[2025-03-04 17:21:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.13957801461219788 norm:0.00022040917247068137 max memory_allocated 22565.94482421875 
[2025-03-04 17:21:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.13945463299751282 norm:0.00020430090080481023 max memory_allocated 22565.94482421875 
[2025-03-04 17:22:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.1393682062625885 norm:0.0001935745240189135 max memory_allocated 22565.94482421875 
[2025-03-04 17:22:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.13933980464935303 norm:0.00019532359146978706 max memory_allocated 22565.94482421875 
[2025-03-04 17:23:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.1393013894557953 norm:0.00018432686920277774 max memory_allocated 22565.94482421875 
[2025-03-04 17:24:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.13928747177124023 norm:0.00017746593221090734 max memory_allocated 22565.94482421875 
[2025-03-04 17:24:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.13924458622932434 norm:0.00016489095287397504 max memory_allocated 22565.94482421875 
[2025-03-04 17:25:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.1392098367214203 norm:0.0001526523701613769 max memory_allocated 22565.94482421875 
[2025-03-04 17:25:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.13918644189834595 norm:0.00014591621584258974 max memory_allocated 22565.94482421875 
[2025-03-04 17:26:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.1391591876745224 norm:0.00014584147720597684 max memory_allocated 22565.94482421875 
[2025-03-04 17:26:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-03-04 17:27:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.18310986459255219 norm:0.005340706557035446 max memory_allocated 22566.11669921875 
[2025-03-04 17:27:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.1722993403673172 norm:0.0020669796504080296 max memory_allocated 22566.11669921875 
[2025-03-04 17:28:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.16311749815940857 norm:0.0010333951795473695 max memory_allocated 22566.11669921875 
[2025-03-04 17:28:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.15991342067718506 norm:0.00045284873340278864 max memory_allocated 22566.11669921875 
[2025-03-04 17:29:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.1585325002670288 norm:0.00040105331572704017 max memory_allocated 22566.11669921875 
[2025-03-04 17:29:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.15792979300022125 norm:0.00036130461376160383 max memory_allocated 22566.11669921875 
[2025-03-04 17:30:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.1576942652463913 norm:0.0003472942335065454 max memory_allocated 22566.11669921875 
[2025-03-04 17:30:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.15751993656158447 norm:0.0003103954950347543 max memory_allocated 22566.11669921875 
[2025-03-04 17:31:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.15738803148269653 norm:0.0003089342499151826 max memory_allocated 22566.11669921875 
[2025-03-04 17:32:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.15723039209842682 norm:0.0002821024681907147 max memory_allocated 22566.11669921875 
[2025-03-04 17:32:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.15709421038627625 norm:0.00027318645152263343 max memory_allocated 22566.11669921875 
[2025-03-04 17:33:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.15703967213630676 norm:0.00024190022668335587 max memory_allocated 22566.11669921875 
[2025-03-04 17:33:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.15699198842048645 norm:0.0002303996734553948 max memory_allocated 22566.11669921875 
[2025-03-04 17:34:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.15693025290966034 norm:0.00022417469881474972 max memory_allocated 22566.11669921875 
[2025-03-04 17:34:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.15684613585472107 norm:0.00021574654965661466 max memory_allocated 22566.11669921875 
[2025-03-04 17:35:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.1567971259355545 norm:0.0002029358729487285 max memory_allocated 22566.11669921875 
[2025-03-04 17:35:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.15674129128456116 norm:0.00020442833192646503 max memory_allocated 22566.11669921875 
[2025-03-04 17:36:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.15673720836639404 norm:0.00020349097030702978 max memory_allocated 22566.11669921875 
[2025-03-04 17:37:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.15677770972251892 norm:0.00020471212337724864 max memory_allocated 22566.11669921875 
[2025-03-04 17:37:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.15672999620437622 norm:0.00019997329218313098 max memory_allocated 22566.11669921875 
[2025-03-04 17:37:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-03-04 17:38:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.2187480479478836 norm:0.007983377203345299 max memory_allocated 22566.28857421875 
[2025-03-04 17:38:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.20796111226081848 norm:0.005082390736788511 max memory_allocated 22566.28857421875 
[2025-03-04 17:39:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.19218532741069794 norm:0.002264460315927863 max memory_allocated 22566.28857421875 
[2025-03-04 17:40:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.1868726909160614 norm:0.001393779180943966 max memory_allocated 22566.28857421875 
[2025-03-04 17:40:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.18445950746536255 norm:0.0010833574924618006 max memory_allocated 22566.28857421875 
[2025-03-04 17:41:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.18269559741020203 norm:0.0007115112966857851 max memory_allocated 22566.28857421875 
[2025-03-04 17:41:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.1822032332420349 norm:0.0007297672564163804 max memory_allocated 22566.28857421875 
[2025-03-04 17:42:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.18199361860752106 norm:0.0007093959138728678 max memory_allocated 22566.28857421875 
[2025-03-04 17:42:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.18184593319892883 norm:0.0006531596300192177 max memory_allocated 22566.28857421875 
[2025-03-04 17:43:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.1813618391752243 norm:0.0005269526154734194 max memory_allocated 22566.28857421875 
[2025-03-04 17:44:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.18122337758541107 norm:0.000549717980902642 max memory_allocated 22566.28857421875 
[2025-03-04 17:44:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.18086133897304535 norm:0.0004433836438693106 max memory_allocated 22566.28857421875 
[2025-03-04 17:45:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.1807800829410553 norm:0.000453588436357677 max memory_allocated 22566.28857421875 
[2025-03-04 17:45:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.18053781986236572 norm:0.00038840234628878534 max memory_allocated 22566.28857421875 
[2025-03-04 17:46:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.1802888959646225 norm:0.00035870622377842665 max memory_allocated 22566.28857421875 
[2025-03-04 17:46:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.18006347119808197 norm:0.0003062437754124403 max memory_allocated 22566.28857421875 
[2025-03-04 17:47:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.17996026575565338 norm:0.00030178582528606057 max memory_allocated 22566.28857421875 
[2025-03-04 17:47:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.17993305623531342 norm:0.00029512643232010305 max memory_allocated 22566.28857421875 
[2025-03-04 17:48:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.17986492812633514 norm:0.00027490241336636245 max memory_allocated 22566.28857421875 
[2025-03-04 17:49:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.1797635853290558 norm:0.00027430104091763496 max memory_allocated 22566.28857421875 
[2025-03-04 17:49:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-03-04 17:49:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.23103448748588562 norm:0.0045466297306120396 max memory_allocated 22566.46044921875 
[2025-03-04 17:50:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.221144437789917 norm:0.002251106547191739 max memory_allocated 22566.46044921875 
[2025-03-04 17:50:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.21034374833106995 norm:0.0009147125529125333 max memory_allocated 22566.46044921875 
[2025-03-04 17:51:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.20676027238368988 norm:0.0005044866702519357 max memory_allocated 22566.46044921875 
[2025-03-04 17:52:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.2052547037601471 norm:0.00044894477468915284 max memory_allocated 22566.46044921875 
[2025-03-04 17:52:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.20464812219142914 norm:0.00037695610080845654 max memory_allocated 22566.46044921875 
[2025-03-04 17:53:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.20435364544391632 norm:0.00035140826366841793 max memory_allocated 22566.46044921875 
[2025-03-04 17:53:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.20414870977401733 norm:0.0003477799182292074 max memory_allocated 22566.46044921875 
[2025-03-04 17:54:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.2039678543806076 norm:0.00029276389977894723 max memory_allocated 22566.46044921875 
[2025-03-04 17:54:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.20379607379436493 norm:0.0002699299657251686 max memory_allocated 22566.46044921875 
[2025-03-04 17:55:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.20365764200687408 norm:0.0002550933859311044 max memory_allocated 22566.46044921875 
[2025-03-04 17:55:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.20356282591819763 norm:0.00025171085144393146 max memory_allocated 22566.46044921875 
[2025-03-04 17:56:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.20346331596374512 norm:0.00024174313875846565 max memory_allocated 22566.46044921875 
[2025-03-04 17:57:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.20343104004859924 norm:0.0002333543379791081 max memory_allocated 22566.46044921875 
[2025-03-04 17:57:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.203401580452919 norm:0.00021271107834763825 max memory_allocated 22566.46044921875 
[2025-03-04 17:58:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.20334628224372864 norm:0.00019399139273446053 max memory_allocated 22566.46044921875 
[2025-03-04 17:58:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.20329757034778595 norm:0.00018912799714598805 max memory_allocated 22566.46044921875 
[2025-03-04 17:59:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.20325668156147003 norm:0.00019107038679067045 max memory_allocated 22566.46044921875 
[2025-03-04 17:59:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.20321448147296906 norm:0.00018883621669374406 max memory_allocated 22566.46044921875 
[2025-03-04 18:00:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.203167125582695 norm:0.00017843383830040693 max memory_allocated 22566.46044921875 
[2025-03-04 18:00:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-03-04 18:01:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.2683306932449341 norm:0.009269867092370987 max memory_allocated 22566.63232421875 
[2025-03-04 18:01:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.25466790795326233 norm:0.004651099443435669 max memory_allocated 22566.63232421875 
[2025-03-04 18:02:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.2402859777212143 norm:0.0014451297465711832 max memory_allocated 22566.63232421875 
[2025-03-04 18:02:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.23534442484378815 norm:0.0006577703170478344 max memory_allocated 22566.63232421875 
[2025-03-04 18:03:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.23379100859165192 norm:0.0004933176678605378 max memory_allocated 22566.63232421875 
[2025-03-04 18:04:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.2332180291414261 norm:0.0004253332153894007 max memory_allocated 22566.63232421875 
[2025-03-04 18:04:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.23291800916194916 norm:0.0004442306817509234 max memory_allocated 22566.63232421875 
[2025-03-04 18:05:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.2326919436454773 norm:0.0004169931635260582 max memory_allocated 22566.63232421875 
[2025-03-04 18:05:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.2324889600276947 norm:0.0003528855449985713 max memory_allocated 22566.63232421875 
[2025-03-04 18:06:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.2323201596736908 norm:0.000340804283041507 max memory_allocated 22566.63232421875 
[2025-03-04 18:06:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.2321816086769104 norm:0.0003200990322511643 max memory_allocated 22566.63232421875 
[2025-03-04 18:07:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.23205837607383728 norm:0.0003063203766942024 max memory_allocated 22566.63232421875 
[2025-03-04 18:07:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.23199225962162018 norm:0.00028310701600275934 max memory_allocated 22566.63232421875 
[2025-03-04 18:08:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.23193050920963287 norm:0.00028148567071184516 max memory_allocated 22566.63232421875 
[2025-03-04 18:09:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.23185661435127258 norm:0.000253074977081269 max memory_allocated 22566.63232421875 
[2025-03-04 18:09:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.23180070519447327 norm:0.00023738884192425758 max memory_allocated 22566.63232421875 
[2025-03-04 18:10:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.23171968758106232 norm:0.00024489453062415123 max memory_allocated 22566.63232421875 
[2025-03-04 18:10:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.23166488111019135 norm:0.00022724687005393207 max memory_allocated 22566.63232421875 
[2025-03-04 18:11:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.23158510029315948 norm:0.00021992054826114327 max memory_allocated 22566.63232421875 
[2025-03-04 18:11:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.23153099417686462 norm:0.00020686010248027742 max memory_allocated 22566.63232421875 
[2025-03-04 18:11:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-03-04 18:12:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 18:12:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.29997456073760986 norm:0.017988385632634163 max memory_allocated 22566.91943359375 
[2025-03-04 18:13:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.2866300046443939 norm:0.013482190668582916 max memory_allocated 22566.91943359375 
[2025-03-04 18:13:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.2743975818157196 norm:0.008501255884766579 max memory_allocated 22566.91943359375 
[2025-03-04 18:14:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.26976868510246277 norm:0.006446518003940582 max memory_allocated 22566.91943359375 
[2025-03-04 18:14:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.2679626941680908 norm:0.005659589543938637 max memory_allocated 22566.91943359375 
[2025-03-04 18:15:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.2673203647136688 norm:0.0049264999106526375 max memory_allocated 22566.91943359375 
[2025-03-04 18:15:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.26692986488342285 norm:0.004365075845271349 max memory_allocated 22566.91943359375 
[2025-03-04 18:16:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.2665858566761017 norm:0.003821840975433588 max memory_allocated 22566.91943359375 
[2025-03-04 18:17:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.26628273725509644 norm:0.0034694187343120575 max memory_allocated 22566.91943359375 
[2025-03-04 18:17:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.2662143409252167 norm:0.003284935839474201 max memory_allocated 22566.91943359375 
[2025-03-04 18:18:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.26615405082702637 norm:0.0032727825455367565 max memory_allocated 22566.91943359375 
[2025-03-04 18:18:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.2660273611545563 norm:0.0031201615929603577 max memory_allocated 22566.91943359375 
[2025-03-04 18:19:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.26594945788383484 norm:0.0030756446067243814 max memory_allocated 22566.91943359375 
[2025-03-04 18:19:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.26585379242897034 norm:0.002917268080636859 max memory_allocated 22566.91943359375 
[2025-03-04 18:20:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.2657862901687622 norm:0.0027533979155123234 max memory_allocated 22566.91943359375 
[2025-03-04 18:21:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.2657212018966675 norm:0.0026922915130853653 max memory_allocated 22566.91943359375 
[2025-03-04 18:21:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.2657856047153473 norm:0.002694171853363514 max memory_allocated 22566.91943359375 
[2025-03-04 18:22:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.26572689414024353 norm:0.0027022112626582384 max memory_allocated 22566.91943359375 
[2025-03-04 18:22:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.2656892240047455 norm:0.0025831416714936495 max memory_allocated 22566.91943359375 
[2025-03-04 18:23:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.2656143307685852 norm:0.0026439595967531204 max memory_allocated 22566.91943359375 
[2025-03-04 18:23:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-03-04 18:23:28 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 18:24:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.345456600189209 norm:0.017008405178785324 max memory_allocated 22567.09130859375 
[2025-03-04 18:24:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.3292921185493469 norm:0.012095004320144653 max memory_allocated 22567.09130859375 
[2025-03-04 18:25:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.31515443325042725 norm:0.007864035665988922 max memory_allocated 22567.09130859375 
[2025-03-04 18:25:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.30995386838912964 norm:0.006357863545417786 max memory_allocated 22567.09130859375 
[2025-03-04 18:26:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.30808526277542114 norm:0.005400223657488823 max memory_allocated 22567.09130859375 
[2025-03-04 18:26:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.307328462600708 norm:0.0046515385620296 max memory_allocated 22567.09130859375 
[2025-03-04 18:27:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.30680322647094727 norm:0.00400240533053875 max memory_allocated 22567.09130859375 
[2025-03-04 18:27:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.30640333890914917 norm:0.0034939046017825603 max memory_allocated 22567.09130859375 
[2025-03-04 18:28:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.30620938539505005 norm:0.0033731632865965366 max memory_allocated 22567.09130859375 
[2025-03-04 18:29:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.3060908317565918 norm:0.00358518841676414 max memory_allocated 22567.09130859375 
[2025-03-04 18:29:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.30595627427101135 norm:0.003453036304563284 max memory_allocated 22567.09130859375 
[2025-03-04 18:30:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.3056820333003998 norm:0.0032006700057536364 max memory_allocated 22567.09130859375 
[2025-03-04 18:30:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.3055703639984131 norm:0.003009867388755083 max memory_allocated 22567.09130859375 
[2025-03-04 18:31:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.30540600419044495 norm:0.0029526473954319954 max memory_allocated 22567.09130859375 
[2025-03-04 18:31:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.3053438663482666 norm:0.0028647880535572767 max memory_allocated 22567.09130859375 
[2025-03-04 18:32:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.30525535345077515 norm:0.0027339474763721228 max memory_allocated 22567.09130859375 
[2025-03-04 18:33:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.3052000105381012 norm:0.002671648282557726 max memory_allocated 22567.09130859375 
[2025-03-04 18:33:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.3052123785018921 norm:0.0026953844353556633 max memory_allocated 22567.09130859375 
[2025-03-04 18:34:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.305242657661438 norm:0.0028051582630723715 max memory_allocated 22567.09130859375 
[2025-03-04 18:34:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.30511677265167236 norm:0.0026775416918098927 max memory_allocated 22567.09130859375 
[2025-03-04 18:34:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-03-04 18:34:54 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 18:35:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:1.5093133449554443 norm:0.322147011756897 max memory_allocated 22567.26318359375 
[2025-03-04 18:36:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.9665952920913696 norm:0.2605808973312378 max memory_allocated 22567.26318359375 
[2025-03-04 18:36:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.6396838426589966 norm:0.12440437823534012 max memory_allocated 22567.26318359375 
[2025-03-04 18:37:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.560698390007019 norm:0.07808917760848999 max memory_allocated 22567.26318359375 
[2025-03-04 18:37:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.5103841423988342 norm:0.057671431452035904 max memory_allocated 22567.26318359375 
[2025-03-04 18:38:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.48884475231170654 norm:0.05563955754041672 max memory_allocated 22567.26318359375 
[2025-03-04 18:38:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.4739033281803131 norm:0.05889548361301422 max memory_allocated 22567.26318359375 
[2025-03-04 18:39:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.46057599782943726 norm:0.06381528824567795 max memory_allocated 22567.26318359375 
[2025-03-04 18:39:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.4554133415222168 norm:0.06364032626152039 max memory_allocated 22567.26318359375 
[2025-03-04 18:40:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.45003068447113037 norm:0.06347117573022842 max memory_allocated 22567.26318359375 
[2025-03-04 18:41:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.4470403790473938 norm:0.06394530087709427 max memory_allocated 22567.26318359375 
[2025-03-04 18:41:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.4435359835624695 norm:0.0644613653421402 max memory_allocated 22567.26318359375 
[2025-03-04 18:42:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.4411965012550354 norm:0.06131015717983246 max memory_allocated 22567.26318359375 
[2025-03-04 18:42:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.43899279832839966 norm:0.06409116834402084 max memory_allocated 22567.26318359375 
[2025-03-04 18:43:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.436308354139328 norm:0.05810036510229111 max memory_allocated 22567.26318359375 
[2025-03-04 18:43:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.43525344133377075 norm:0.05599594861268997 max memory_allocated 22567.26318359375 
[2025-03-04 18:44:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.4319303035736084 norm:0.05549180507659912 max memory_allocated 22567.26318359375 
[2025-03-04 18:45:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.43000704050064087 norm:0.051777493208646774 max memory_allocated 22567.26318359375 
[2025-03-04 18:45:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.42829829454421997 norm:0.049897171556949615 max memory_allocated 22567.26318359375 
[2025-03-04 18:46:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.42841485142707825 norm:0.04904048517346382 max memory_allocated 22567.26318359375 
[2025-03-04 18:46:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-03-04 18:46:20 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-03-04 18:46:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.9562777876853943 norm:0.10165172070264816 max memory_allocated 22567.43505859375 
[2025-03-04 18:47:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.8726045489311218 norm:0.07912708073854446 max memory_allocated 22567.43505859375 
[2025-03-04 18:48:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.7999335527420044 norm:0.06012049317359924 max memory_allocated 22567.43505859375 
[2025-03-04 18:48:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.7770389318466187 norm:0.05387716740369797 max memory_allocated 22567.43505859375 
[2025-03-04 18:49:08 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.7674153447151184 norm:0.048204634338617325 max memory_allocated 22567.43505859375 
[2025-03-04 18:49:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.7578224539756775 norm:0.043584514409303665 max memory_allocated 22567.43505859375 
[2025-03-04 18:50:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.7520928978919983 norm:0.04053892567753792 max memory_allocated 22567.43505859375 
[2025-03-04 18:50:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.7475293874740601 norm:0.03755698353052139 max memory_allocated 22567.43505859375 
[2025-03-04 18:51:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.7438600063323975 norm:0.035941675305366516 max memory_allocated 22567.43505859375 
[2025-03-04 18:51:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.7400364875793457 norm:0.0345052033662796 max memory_allocated 22567.43505859375 
[2025-03-04 18:52:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.7382285594940186 norm:0.034949786961078644 max memory_allocated 22567.43505859375 
[2025-03-04 18:53:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.7366170883178711 norm:0.03463592752814293 max memory_allocated 22567.43505859375 
[2025-03-04 18:53:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.735078752040863 norm:0.03299389034509659 max memory_allocated 22567.43505859375 
[2025-03-04 18:54:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.733222246170044 norm:0.03080933168530464 max memory_allocated 22567.43505859375 
[2025-03-04 18:54:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.7307006120681763 norm:0.028987625613808632 max memory_allocated 22567.43505859375 
[2025-03-04 18:55:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.7302206158638 norm:0.028498895466327667 max memory_allocated 22567.43505859375 
[2025-03-04 18:55:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.731323778629303 norm:0.03138808533549309 max memory_allocated 22567.43505859375 
[2025-03-04 18:56:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.7314047813415527 norm:0.03233223035931587 max memory_allocated 22567.43505859375 
[2025-03-04 18:57:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.7303842306137085 norm:0.032091569155454636 max memory_allocated 22567.43505859375 
[2025-03-04 18:57:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.7292437553405762 norm:0.02947121299803257 max memory_allocated 22567.43505859375 
[2025-03-04 18:57:43 root] (main_calibration.py 365): INFO 21872.894647598267
[2025-03-04 18:58:18 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-04 18:59:29 root] (main_calibration.py 158): INFO wikitext2 : 5.887500286102295
[2025-03-04 18:59:29 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-04 19:01:19 root] (main_calibration.py 158): INFO c4 : 7.558459758758545
[2025-03-04 20:42:21 root] (main_calibration.py 169): INFO {'wikitext2': 5.887500286102295, 'c4': 7.558459758758545, 'results': {'piqa': {'acc': 0.7665941240478781, 'acc_stderr': 0.009869247889521005, 'acc_norm': 0.7600652883569097, 'acc_norm_stderr': 0.009963625892809545}, 'arc_challenge': {'acc': 0.38054607508532423, 'acc_stderr': 0.014188277712349822, 'acc_norm': 0.3916382252559727, 'acc_norm_stderr': 0.014264122124938217}, 'boolq': {'acc': 0.6957186544342507, 'acc_stderr': 0.008047241372069977}, 'hellaswag': {'acc': 0.5416251742680741, 'acc_stderr': 0.004972460206842307, 'acc_norm': 0.7032463652658832, 'acc_norm_stderr': 0.004558933822995536}, 'arc_easy': {'acc': 0.6902356902356902, 'acc_stderr': 0.009488172851903717, 'acc_norm': 0.5273569023569024, 'acc_norm_stderr': 0.010244415164390534}, 'winogrande': {'acc': 0.6669297553275454, 'acc_stderr': 0.013246194028070656}}, 'versions': {'piqa': 0, 'arc_challenge': 0, 'boolq': 1, 'hellaswag': 0, 'arc_easy': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
