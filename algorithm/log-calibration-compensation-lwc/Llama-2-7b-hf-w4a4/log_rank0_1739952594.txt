[2025-02-19 08:09:54 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-7b-hf-w4a4', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-7b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-19 08:11:05 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-19 08:11:05 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-19 08:11:06 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-19 08:11:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-19 08:11:10 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:11:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.056629788130521774 norm:0.048364028334617615 max memory_allocated 22562.10693359375 
[2025-02-19 08:12:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.032433003187179565 norm:0.024003496393561363 max memory_allocated 22562.10693359375 
[2025-02-19 08:12:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.025092143565416336 norm:0.017585059627890587 max memory_allocated 22562.10693359375 
[2025-02-19 08:13:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.022462358698248863 norm:0.015748661011457443 max memory_allocated 22562.10693359375 
[2025-02-19 08:13:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.021059928461909294 norm:0.013750453479588032 max memory_allocated 22562.10693359375 
[2025-02-19 08:14:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.020152032375335693 norm:0.011720444075763226 max memory_allocated 22562.10693359375 
[2025-02-19 08:14:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.019787268713116646 norm:0.010188864544034004 max memory_allocated 22562.10693359375 
[2025-02-19 08:15:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.01961362548172474 norm:0.008767607621848583 max memory_allocated 22562.10693359375 
[2025-02-19 08:15:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.01929781399667263 norm:0.007515368051826954 max memory_allocated 22562.10693359375 
[2025-02-19 08:16:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.01909603551030159 norm:0.0071050808764994144 max memory_allocated 22562.10693359375 
[2025-02-19 08:16:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.018972020596265793 norm:0.006102181039750576 max memory_allocated 22562.10693359375 
[2025-02-19 08:17:19 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.01899273321032524 norm:0.006146910134702921 max memory_allocated 22562.10693359375 
[2025-02-19 08:17:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.01905156299471855 norm:0.005918948445469141 max memory_allocated 22562.10693359375 
[2025-02-19 08:18:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.018971197307109833 norm:0.00503350468352437 max memory_allocated 22562.10693359375 
[2025-02-19 08:18:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0185988899320364 norm:0.004222366027534008 max memory_allocated 22562.10693359375 
[2025-02-19 08:19:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.01872393675148487 norm:0.004693102091550827 max memory_allocated 22562.10693359375 
[2025-02-19 08:19:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.01861049421131611 norm:0.004366063978523016 max memory_allocated 22562.10693359375 
[2025-02-19 08:20:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.018691720440983772 norm:0.004052860662341118 max memory_allocated 22562.10693359375 
[2025-02-19 08:20:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.01870964653789997 norm:0.004234035033732653 max memory_allocated 22562.10693359375 
[2025-02-19 08:21:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.018555112183094025 norm:0.0037946177180856466 max memory_allocated 22562.10693359375 
[2025-02-19 08:21:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-19 08:21:36 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:22:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.2818092107772827 norm:0.11135691404342651 max memory_allocated 22562.27880859375 
[2025-02-19 08:22:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.20040056109428406 norm:0.06780548393726349 max memory_allocated 22562.27880859375 
[2025-02-19 08:23:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.165414959192276 norm:0.04477599263191223 max memory_allocated 22562.27880859375 
[2025-02-19 08:23:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.1487036794424057 norm:0.0373854823410511 max memory_allocated 22562.27880859375 
[2025-02-19 08:24:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.13864506781101227 norm:0.03522889316082001 max memory_allocated 22562.27880859375 
[2025-02-19 08:24:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.13350173830986023 norm:0.034734662622213364 max memory_allocated 22562.27880859375 
[2025-02-19 08:25:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.1299053132534027 norm:0.032859139144420624 max memory_allocated 22562.27880859375 
[2025-02-19 08:25:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.12642434239387512 norm:0.032058801501989365 max memory_allocated 22562.27880859375 
[2025-02-19 08:26:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.12453961372375488 norm:0.031407564878463745 max memory_allocated 22562.27880859375 
[2025-02-19 08:26:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.12351198494434357 norm:0.030652787536382675 max memory_allocated 22562.27880859375 
[2025-02-19 08:27:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.12111775577068329 norm:0.02977362647652626 max memory_allocated 22562.27880859375 
[2025-02-19 08:27:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.12056875228881836 norm:0.02809680998325348 max memory_allocated 22562.27880859375 
[2025-02-19 08:28:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.1199980229139328 norm:0.027808941900730133 max memory_allocated 22562.27880859375 
[2025-02-19 08:28:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.11980671435594559 norm:0.02932216413319111 max memory_allocated 22562.27880859375 
[2025-02-19 08:29:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.11849065124988556 norm:0.02677539363503456 max memory_allocated 22562.27880859375 
[2025-02-19 08:29:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.11863892525434494 norm:0.026069438084959984 max memory_allocated 22562.27880859375 
[2025-02-19 08:30:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.11779970675706863 norm:0.035487156361341476 max memory_allocated 22562.27880859375 
[2025-02-19 08:30:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.11783047020435333 norm:0.02634929120540619 max memory_allocated 22562.27880859375 
[2025-02-19 08:31:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.11613577604293823 norm:0.023809991776943207 max memory_allocated 22562.27880859375 
[2025-02-19 08:31:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.1150125116109848 norm:0.023157138377428055 max memory_allocated 22562.27880859375 
[2025-02-19 08:32:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-19 08:32:02 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 08:32:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.1894896924495697 norm:0.0555623359978199 max memory_allocated 22562.45068359375 
[2025-02-19 08:33:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.16193656623363495 norm:0.03379916399717331 max memory_allocated 22562.45068359375 
[2025-02-19 08:33:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.14546048641204834 norm:0.02312402054667473 max memory_allocated 22562.45068359375 
[2025-02-19 08:34:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.1383287012577057 norm:0.017595941200852394 max memory_allocated 22562.45068359375 
[2025-02-19 08:34:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.13443663716316223 norm:0.014210198074579239 max memory_allocated 22562.45068359375 
[2025-02-19 08:35:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.13174092769622803 norm:0.011007566004991531 max memory_allocated 22562.45068359375 
[2025-02-19 08:35:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.13014380633831024 norm:0.008695274591445923 max memory_allocated 22562.45068359375 
[2025-02-19 08:36:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.1290929764509201 norm:0.007521801628172398 max memory_allocated 22562.45068359375 
[2025-02-19 08:36:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.1282656490802765 norm:0.006422434467822313 max memory_allocated 22562.45068359375 
[2025-02-19 08:37:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.12773393094539642 norm:0.006277470849454403 max memory_allocated 22562.45068359375 
[2025-02-19 08:37:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.1272345930337906 norm:0.0058043887838721275 max memory_allocated 22562.45068359375 
[2025-02-19 08:38:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.1268547773361206 norm:0.005591341760009527 max memory_allocated 22562.45068359375 
[2025-02-19 08:38:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.1266958862543106 norm:0.0052161747589707375 max memory_allocated 22562.45068359375 
[2025-02-19 08:39:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.12667696177959442 norm:0.005156812258064747 max memory_allocated 22562.45068359375 
[2025-02-19 08:39:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.12650907039642334 norm:0.005033033899962902 max memory_allocated 22562.45068359375 
[2025-02-19 08:40:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.12658445537090302 norm:0.0050344085320830345 max memory_allocated 22562.45068359375 
[2025-02-19 08:40:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.1267799288034439 norm:0.005067289341241121 max memory_allocated 22562.45068359375 
[2025-02-19 08:41:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.12657250463962555 norm:0.004780601244419813 max memory_allocated 22562.45068359375 
[2025-02-19 08:41:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.12639448046684265 norm:0.004559835884720087 max memory_allocated 22562.45068359375 
[2025-02-19 08:42:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.12655425071716309 norm:0.004758023656904697 max memory_allocated 22562.45068359375 
[2025-02-19 08:42:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-19 08:43:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.22600853443145752 norm:0.02098691277205944 max memory_allocated 22562.50732421875 
[2025-02-19 08:43:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.19664382934570312 norm:0.007885189726948738 max memory_allocated 22562.50732421875 
[2025-02-19 08:44:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.17665451765060425 norm:0.0037559799384325743 max memory_allocated 22562.50732421875 
[2025-02-19 08:44:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.16967086493968964 norm:0.0025661110412329435 max memory_allocated 22562.50732421875 
[2025-02-19 08:45:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.16671642661094666 norm:0.0022187773138284683 max memory_allocated 22562.50732421875 
[2025-02-19 08:45:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.16499756276607513 norm:0.0021453876979649067 max memory_allocated 22562.50732421875 
[2025-02-19 08:46:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.16350890696048737 norm:0.0017891344614326954 max memory_allocated 22562.50732421875 
[2025-02-19 08:46:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.16244415938854218 norm:0.001609869534149766 max memory_allocated 22562.50732421875 
[2025-02-19 08:47:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.16168232262134552 norm:0.0015005248133093119 max memory_allocated 22562.50732421875 
[2025-02-19 08:47:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.16113004088401794 norm:0.0014129923656582832 max memory_allocated 22562.50732421875 
[2025-02-19 08:48:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.16081558167934418 norm:0.0013682931894436479 max memory_allocated 22562.50732421875 
[2025-02-19 08:48:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.16090241074562073 norm:0.001419438049197197 max memory_allocated 22562.50732421875 
[2025-02-19 08:49:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.16073402762413025 norm:0.0013167596189305186 max memory_allocated 22562.50732421875 
[2025-02-19 08:49:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.1606178879737854 norm:0.0012600765330716968 max memory_allocated 22562.50732421875 
[2025-02-19 08:50:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.16055533289909363 norm:0.0012354961363598704 max memory_allocated 22562.50732421875 
[2025-02-19 08:50:43 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.16053400933742523 norm:0.0012005817843601108 max memory_allocated 22562.50732421875 
[2025-02-19 08:51:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.16054394841194153 norm:0.001203850843012333 max memory_allocated 22562.50732421875 
[2025-02-19 08:51:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.16070327162742615 norm:0.0011983090080320835 max memory_allocated 22562.50732421875 
[2025-02-19 08:52:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.16123318672180176 norm:0.001198965939693153 max memory_allocated 22562.50732421875 
[2025-02-19 08:52:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.1613033413887024 norm:0.001184726133942604 max memory_allocated 22562.50732421875 
[2025-02-19 08:52:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-19 08:53:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.28501272201538086 norm:0.03906412795186043 max memory_allocated 22562.67919921875 
[2025-02-19 08:53:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.24275246262550354 norm:0.01189915556460619 max memory_allocated 22562.67919921875 
[2025-02-19 08:54:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.21462002396583557 norm:0.004629103466868401 max memory_allocated 22562.67919921875 
[2025-02-19 08:54:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.2055572122335434 norm:0.002856218721717596 max memory_allocated 22562.67919921875 
[2025-02-19 08:55:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.20132766664028168 norm:0.002419584896415472 max memory_allocated 22562.67919921875 
[2025-02-19 08:56:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.19815686345100403 norm:0.0019468017853796482 max memory_allocated 22562.67919921875 
[2025-02-19 08:56:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.19617938995361328 norm:0.0016538731288164854 max memory_allocated 22562.67919921875 
[2025-02-19 08:57:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.19485147297382355 norm:0.0014785545645281672 max memory_allocated 22562.67919921875 
[2025-02-19 08:57:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.1939329206943512 norm:0.001424789079464972 max memory_allocated 22562.67919921875 
[2025-02-19 08:58:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.1932799369096756 norm:0.0013190365862101316 max memory_allocated 22562.67919921875 
[2025-02-19 08:58:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.19283340871334076 norm:0.0012789691099897027 max memory_allocated 22562.67919921875 
[2025-02-19 08:59:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.19250763952732086 norm:0.0012494196416810155 max memory_allocated 22562.67919921875 
[2025-02-19 08:59:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.1922072470188141 norm:0.001263362355530262 max memory_allocated 22562.67919921875 
[2025-02-19 09:00:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.1918552964925766 norm:0.0012390248011797667 max memory_allocated 22562.67919921875 
[2025-02-19 09:00:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.19170813262462616 norm:0.0012207854306325316 max memory_allocated 22562.67919921875 
[2025-02-19 09:01:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.19157662987709045 norm:0.0012298367219045758 max memory_allocated 22562.67919921875 
[2025-02-19 09:01:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.1915045529603958 norm:0.001211195020005107 max memory_allocated 22562.67919921875 
[2025-02-19 09:02:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.19157767295837402 norm:0.0011919859098270535 max memory_allocated 22562.67919921875 
[2025-02-19 09:02:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.19135548174381256 norm:0.0011626353953033686 max memory_allocated 22562.67919921875 
[2025-02-19 09:03:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.19127678871154785 norm:0.0011771161807700992 max memory_allocated 22562.67919921875 
[2025-02-19 09:03:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-19 09:03:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.2989778518676758 norm:0.015571465715765953 max memory_allocated 22562.85107421875 
[2025-02-19 09:04:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.26800867915153503 norm:0.006661967374384403 max memory_allocated 22562.85107421875 
[2025-02-19 09:04:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.23870377242565155 norm:0.0026188048068434 max memory_allocated 22562.85107421875 
[2025-02-19 09:05:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.22844967246055603 norm:0.0017400056822225451 max memory_allocated 22562.85107421875 
[2025-02-19 09:05:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.22369155287742615 norm:0.0014213788090273738 max memory_allocated 22562.85107421875 
[2025-02-19 09:06:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.22077807784080505 norm:0.0013551730662584305 max memory_allocated 22562.85107421875 
[2025-02-19 09:06:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.2189740091562271 norm:0.001276891678571701 max memory_allocated 22562.85107421875 
[2025-02-19 09:07:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.21782249212265015 norm:0.0012404350563883781 max memory_allocated 22562.85107421875 
[2025-02-19 09:07:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.21688364446163177 norm:0.0011902842670679092 max memory_allocated 22562.85107421875 
[2025-02-19 09:08:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.21632346510887146 norm:0.0011943099088966846 max memory_allocated 22562.85107421875 
[2025-02-19 09:08:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.2160269021987915 norm:0.0015513228718191385 max memory_allocated 22562.85107421875 
[2025-02-19 09:09:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.21571797132492065 norm:0.0011290590045973659 max memory_allocated 22562.85107421875 
[2025-02-19 09:10:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.2155054807662964 norm:0.001127463998273015 max memory_allocated 22562.85107421875 
[2025-02-19 09:10:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.2155052274465561 norm:0.0011457010405138135 max memory_allocated 22562.85107421875 
[2025-02-19 09:11:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.21520744264125824 norm:0.001116416766308248 max memory_allocated 22562.85107421875 
[2025-02-19 09:11:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.21512505412101746 norm:0.0011072044726461172 max memory_allocated 22562.85107421875 
[2025-02-19 09:12:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.2151208370923996 norm:0.0011392813175916672 max memory_allocated 22562.85107421875 
[2025-02-19 09:12:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.21498116850852966 norm:0.001148652983829379 max memory_allocated 22562.85107421875 
[2025-02-19 09:13:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.21484532952308655 norm:0.0011254689889028668 max memory_allocated 22562.85107421875 
[2025-02-19 09:13:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.21470248699188232 norm:0.0011242254404351115 max memory_allocated 22562.85107421875 
[2025-02-19 09:13:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-19 09:14:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.36362388730049133 norm:0.03426217660307884 max memory_allocated 22563.02294921875 
[2025-02-19 09:14:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.3140513598918915 norm:0.012575734406709671 max memory_allocated 22563.02294921875 
[2025-02-19 09:15:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.27639293670654297 norm:0.0056440141052007675 max memory_allocated 22563.02294921875 
[2025-02-19 09:15:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.26192909479141235 norm:0.0034215059131383896 max memory_allocated 22563.02294921875 
[2025-02-19 09:16:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.25538739562034607 norm:0.00257491716183722 max memory_allocated 22563.02294921875 
[2025-02-19 09:16:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.25104403495788574 norm:0.002172746229916811 max memory_allocated 22563.02294921875 
[2025-02-19 09:17:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.24822579324245453 norm:0.0019099406199529767 max memory_allocated 22563.02294921875 
[2025-02-19 09:17:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.24630846083164215 norm:0.0017549274489283562 max memory_allocated 22563.02294921875 
[2025-02-19 09:18:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.24498862028121948 norm:0.00161714106798172 max memory_allocated 22563.02294921875 
[2025-02-19 09:18:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.24397499859333038 norm:0.001494159922003746 max memory_allocated 22563.02294921875 
[2025-02-19 09:19:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.24307909607887268 norm:0.0014006169512867928 max memory_allocated 22563.02294921875 
[2025-02-19 09:19:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.24258989095687866 norm:0.001350953010842204 max memory_allocated 22563.02294921875 
[2025-02-19 09:20:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.24239179491996765 norm:0.0013014748692512512 max memory_allocated 22563.02294921875 
[2025-02-19 09:20:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.24213635921478271 norm:0.001301734009757638 max memory_allocated 22563.02294921875 
[2025-02-19 09:21:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.2420375496149063 norm:0.001304571982473135 max memory_allocated 22563.02294921875 
[2025-02-19 09:21:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.24188464879989624 norm:0.0012733403127640486 max memory_allocated 22563.02294921875 
[2025-02-19 09:22:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.2418483942747116 norm:0.0012449149508029222 max memory_allocated 22563.02294921875 
[2025-02-19 09:22:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.24135075509548187 norm:0.0011995048262178898 max memory_allocated 22563.02294921875 
[2025-02-19 09:23:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.24122197926044464 norm:0.0011951065389439464 max memory_allocated 22563.02294921875 
[2025-02-19 09:23:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.24109894037246704 norm:0.001160499406978488 max memory_allocated 22563.02294921875 
[2025-02-19 09:24:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-19 09:24:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.4022471606731415 norm:0.037991613149642944 max memory_allocated 22563.19482421875 
[2025-02-19 09:25:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.334208607673645 norm:0.011387499049305916 max memory_allocated 22563.19482421875 
[2025-02-19 09:25:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.2940215766429901 norm:0.0045727090910077095 max memory_allocated 22563.19482421875 
[2025-02-19 09:26:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.28175753355026245 norm:0.0029538043309003115 max memory_allocated 22563.19482421875 
[2025-02-19 09:26:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.27588018774986267 norm:0.002278408035635948 max memory_allocated 22563.19482421875 
[2025-02-19 09:27:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.27199167013168335 norm:0.0019938433542847633 max memory_allocated 22563.19482421875 
[2025-02-19 09:27:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.26953402161598206 norm:0.0019037247402593493 max memory_allocated 22563.19482421875 
[2025-02-19 09:28:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.2676371932029724 norm:0.0016196102369576693 max memory_allocated 22563.19482421875 
[2025-02-19 09:28:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.26639223098754883 norm:0.0015348687302321196 max memory_allocated 22563.19482421875 
[2025-02-19 09:29:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.26536011695861816 norm:0.0014116846723482013 max memory_allocated 22563.19482421875 
[2025-02-19 09:29:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.26479852199554443 norm:0.0014024225529283285 max memory_allocated 22563.19482421875 
[2025-02-19 09:30:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.26426494121551514 norm:0.00129045476205647 max memory_allocated 22563.19482421875 
[2025-02-19 09:30:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.2637300491333008 norm:0.0012366719311103225 max memory_allocated 22563.19482421875 
[2025-02-19 09:31:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.263444185256958 norm:0.0012705859262496233 max memory_allocated 22563.19482421875 
[2025-02-19 09:31:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.26301300525665283 norm:0.0012493003159761429 max memory_allocated 22563.19482421875 
[2025-02-19 09:32:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.2628890573978424 norm:0.001239019795320928 max memory_allocated 22563.19482421875 
[2025-02-19 09:32:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.2627582848072052 norm:0.0012365607544779778 max memory_allocated 22563.19482421875 
[2025-02-19 09:33:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.2624816298484802 norm:0.0012121349573135376 max memory_allocated 22563.19482421875 
[2025-02-19 09:33:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.26237112283706665 norm:0.001184755703434348 max memory_allocated 22563.19482421875 
[2025-02-19 09:34:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.2621346712112427 norm:0.0011737266322597861 max memory_allocated 22563.19482421875 
[2025-02-19 09:34:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-19 09:35:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.3906450867652893 norm:0.023185066878795624 max memory_allocated 22563.36669921875 
[2025-02-19 09:35:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.3452329933643341 norm:0.007947712205350399 max memory_allocated 22563.36669921875 
[2025-02-19 09:36:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.3087373971939087 norm:0.0030168299563229084 max memory_allocated 22563.36669921875 
[2025-02-19 09:36:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.29673781991004944 norm:0.0019112644949927926 max memory_allocated 22563.36669921875 
[2025-02-19 09:37:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.29136043787002563 norm:0.001643985859118402 max memory_allocated 22563.36669921875 
[2025-02-19 09:37:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.28758180141448975 norm:0.0014430778101086617 max memory_allocated 22563.36669921875 
[2025-02-19 09:38:09 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.2851954400539398 norm:0.001326967030763626 max memory_allocated 22563.36669921875 
[2025-02-19 09:38:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.28358396887779236 norm:0.0012698082718998194 max memory_allocated 22563.36669921875 
[2025-02-19 09:39:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.2823573052883148 norm:0.001238707802258432 max memory_allocated 22563.36669921875 
[2025-02-19 09:39:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.2815402150154114 norm:0.0012158481404185295 max memory_allocated 22563.36669921875 
[2025-02-19 09:40:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.2809869945049286 norm:0.0011676526628434658 max memory_allocated 22563.36669921875 
[2025-02-19 09:40:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.2804754078388214 norm:0.0011550048366189003 max memory_allocated 22563.36669921875 
[2025-02-19 09:41:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.2800524830818176 norm:0.0011226738570258021 max memory_allocated 22563.36669921875 
[2025-02-19 09:41:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.27980297803878784 norm:0.0011231069220229983 max memory_allocated 22563.36669921875 
[2025-02-19 09:42:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.2794678807258606 norm:0.0010906131938099861 max memory_allocated 22563.36669921875 
[2025-02-19 09:42:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.2793411910533905 norm:0.0010724414605647326 max memory_allocated 22563.36669921875 
[2025-02-19 09:43:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.27925047278404236 norm:0.0010905134258791804 max memory_allocated 22563.36669921875 
[2025-02-19 09:43:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.2790799140930176 norm:0.0010665119625627995 max memory_allocated 22563.36669921875 
[2025-02-19 09:44:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.2789592146873474 norm:0.0010493636364117265 max memory_allocated 22563.36669921875 
[2025-02-19 09:44:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.27885687351226807 norm:0.0010399683378636837 max memory_allocated 22563.36669921875 
[2025-02-19 09:44:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-19 09:45:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.4299249053001404 norm:0.03736356645822525 max memory_allocated 22563.53857421875 
[2025-02-19 09:46:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.36697328090667725 norm:0.013142649084329605 max memory_allocated 22563.53857421875 
[2025-02-19 09:46:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.3232923150062561 norm:0.00363826472312212 max memory_allocated 22563.53857421875 
[2025-02-19 09:47:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.31063544750213623 norm:0.0021756894420832396 max memory_allocated 22563.53857421875 
[2025-02-19 09:47:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.30502963066101074 norm:0.0018457276746630669 max memory_allocated 22563.53857421875 
[2025-02-19 09:48:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.3014051914215088 norm:0.0016700637061148882 max memory_allocated 22563.53857421875 
[2025-02-19 09:48:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.29896828532218933 norm:0.0015763335395604372 max memory_allocated 22563.53857421875 
[2025-02-19 09:49:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.29718178510665894 norm:0.0014621634036302567 max memory_allocated 22563.53857421875 
[2025-02-19 09:49:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.2960962653160095 norm:0.0014364110538735986 max memory_allocated 22563.53857421875 
[2025-02-19 09:50:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.2951718866825104 norm:0.0013458372559398413 max memory_allocated 22563.53857421875 
[2025-02-19 09:50:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.2944948971271515 norm:0.0013108232524245977 max memory_allocated 22563.53857421875 
[2025-02-19 09:51:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.2938474714756012 norm:0.0013016222510486841 max memory_allocated 22563.53857421875 
[2025-02-19 09:51:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.293511301279068 norm:0.001265684375539422 max memory_allocated 22563.53857421875 
[2025-02-19 09:52:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.29325568675994873 norm:0.0012261406518518925 max memory_allocated 22563.53857421875 
[2025-02-19 09:52:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.2930164337158203 norm:0.0011682927142828703 max memory_allocated 22563.53857421875 
[2025-02-19 09:53:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.2927490770816803 norm:0.0011158521519973874 max memory_allocated 22563.53857421875 
[2025-02-19 09:53:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.2925448417663574 norm:0.0010726358741521835 max memory_allocated 22563.53857421875 
[2025-02-19 09:54:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.29238465428352356 norm:0.0010443114442750812 max memory_allocated 22563.53857421875 
[2025-02-19 09:54:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.2921505868434906 norm:0.0010266794124618173 max memory_allocated 22563.53857421875 
[2025-02-19 09:55:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.2919817566871643 norm:0.0010238655377179384 max memory_allocated 22563.53857421875 
[2025-02-19 09:55:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-19 09:55:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.4132794737815857 norm:0.031066522002220154 max memory_allocated 22563.71044921875 
[2025-02-19 09:56:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.3723425269126892 norm:0.013060495257377625 max memory_allocated 22563.71044921875 
[2025-02-19 09:56:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.33686643838882446 norm:0.00522215012460947 max memory_allocated 22563.71044921875 
[2025-02-19 09:57:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.32151371240615845 norm:0.002151512075215578 max memory_allocated 22563.71044921875 
[2025-02-19 09:57:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.31458616256713867 norm:0.0013480930356308818 max memory_allocated 22563.71044921875 
[2025-02-19 09:58:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.31052836775779724 norm:0.0012357613304629922 max memory_allocated 22563.71044921875 
[2025-02-19 09:58:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.3076469898223877 norm:0.0011481221299618483 max memory_allocated 22563.71044921875 
[2025-02-19 09:59:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.3058018088340759 norm:0.0010902064386755228 max memory_allocated 22563.71044921875 
[2025-02-19 09:59:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.30447837710380554 norm:0.0010526726255193353 max memory_allocated 22563.71044921875 
[2025-02-19 10:00:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.3034861087799072 norm:0.0010243273572996259 max memory_allocated 22563.71044921875 
[2025-02-19 10:01:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.3027925193309784 norm:0.0010092020966112614 max memory_allocated 22563.71044921875 
[2025-02-19 10:01:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.30226200819015503 norm:0.0009783485438674688 max memory_allocated 22563.71044921875 
[2025-02-19 10:02:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.3017745614051819 norm:0.0009523502667434514 max memory_allocated 22563.71044921875 
[2025-02-19 10:02:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.3014305531978607 norm:0.0009497814462520182 max memory_allocated 22563.71044921875 
[2025-02-19 10:03:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.3011687099933624 norm:0.0009494722471572459 max memory_allocated 22563.71044921875 
[2025-02-19 10:03:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.3009442687034607 norm:0.0009470635559409857 max memory_allocated 22563.71044921875 
[2025-02-19 10:04:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.30094149708747864 norm:0.0009303757105953991 max memory_allocated 22563.71044921875 
[2025-02-19 10:04:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.3007679283618927 norm:0.0009202780202031136 max memory_allocated 22563.71044921875 
[2025-02-19 10:05:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.3006688356399536 norm:0.0009171859128400683 max memory_allocated 22563.71044921875 
[2025-02-19 10:05:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.3006783425807953 norm:0.0009059380390681326 max memory_allocated 22563.71044921875 
[2025-02-19 10:05:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-19 10:06:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.41649988293647766 norm:0.02172037400305271 max memory_allocated 22563.88232421875 
[2025-02-19 10:06:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.36587101221084595 norm:0.00798458606004715 max memory_allocated 22563.88232421875 
[2025-02-19 10:07:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.3353198766708374 norm:0.0038717277348041534 max memory_allocated 22563.88232421875 
[2025-02-19 10:07:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.32265567779541016 norm:0.0021506156772375107 max memory_allocated 22563.88232421875 
[2025-02-19 10:08:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.31677573919296265 norm:0.001411230769008398 max memory_allocated 22563.88232421875 
[2025-02-19 10:08:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.3132268488407135 norm:0.001251960638910532 max memory_allocated 22563.88232421875 
[2025-02-19 10:09:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.31092363595962524 norm:0.0011614322429522872 max memory_allocated 22563.88232421875 
[2025-02-19 10:09:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.3093734383583069 norm:0.0011137740220874548 max memory_allocated 22563.88232421875 
[2025-02-19 10:10:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.3082126975059509 norm:0.0010734699899330735 max memory_allocated 22563.88232421875 
[2025-02-19 10:10:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.3073095679283142 norm:0.0010333963437005877 max memory_allocated 22563.88232421875 
[2025-02-19 10:11:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.30669668316841125 norm:0.0009908017236739397 max memory_allocated 22563.88232421875 
[2025-02-19 10:11:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.30624571442604065 norm:0.0010015878360718489 max memory_allocated 22563.88232421875 
[2025-02-19 10:12:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.3058636784553528 norm:0.0009765332797542214 max memory_allocated 22563.88232421875 
[2025-02-19 10:12:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.30561426281929016 norm:0.0009440308203920722 max memory_allocated 22563.88232421875 
[2025-02-19 10:13:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.30533137917518616 norm:0.0009130503167398274 max memory_allocated 22563.88232421875 
[2025-02-19 10:13:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.30517590045928955 norm:0.0009038132848218083 max memory_allocated 22563.88232421875 
[2025-02-19 10:14:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.3050597608089447 norm:0.0008806650876067579 max memory_allocated 22563.88232421875 
[2025-02-19 10:15:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.30490556359291077 norm:0.0008471347391605377 max memory_allocated 22563.88232421875 
[2025-02-19 10:15:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.3048197627067566 norm:0.0008392256568185985 max memory_allocated 22563.88232421875 
[2025-02-19 10:16:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.30481094121932983 norm:0.0008369750576093793 max memory_allocated 22563.88232421875 
[2025-02-19 10:16:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-19 10:16:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.4047861397266388 norm:0.013366969302296638 max memory_allocated 22564.05419921875 
[2025-02-19 10:17:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.3675757050514221 norm:0.005119050852954388 max memory_allocated 22564.05419921875 
[2025-02-19 10:17:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.33969151973724365 norm:0.0023826139513403177 max memory_allocated 22564.05419921875 
[2025-02-19 10:18:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.32809537649154663 norm:0.0014990093186497688 max memory_allocated 22564.05419921875 
[2025-02-19 10:18:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.32283371686935425 norm:0.0011749155819416046 max memory_allocated 22564.05419921875 
[2025-02-19 10:19:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.31967058777809143 norm:0.001049573183991015 max memory_allocated 22564.05419921875 
[2025-02-19 10:19:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.31767675280570984 norm:0.0009818437974900007 max memory_allocated 22564.05419921875 
[2025-02-19 10:20:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.3160616159439087 norm:0.0009174855658784509 max memory_allocated 22564.05419921875 
[2025-02-19 10:20:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.3150276839733124 norm:0.0008933706558309495 max memory_allocated 22564.05419921875 
[2025-02-19 10:21:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.314216673374176 norm:0.0008663226035423577 max memory_allocated 22564.05419921875 
[2025-02-19 10:21:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.3136627674102783 norm:0.0008347989642061293 max memory_allocated 22564.05419921875 
[2025-02-19 10:22:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.31323638558387756 norm:0.0008197824354283512 max memory_allocated 22564.05419921875 
[2025-02-19 10:22:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.31292083859443665 norm:0.0008086314774118364 max memory_allocated 22564.05419921875 
[2025-02-19 10:23:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.3126820921897888 norm:0.0007970408187247813 max memory_allocated 22564.05419921875 
[2025-02-19 10:23:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.31241318583488464 norm:0.0007935712346807122 max memory_allocated 22564.05419921875 
[2025-02-19 10:24:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.3122657835483551 norm:0.0007793442346155643 max memory_allocated 22564.05419921875 
[2025-02-19 10:24:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.3121056854724884 norm:0.0007777571445330977 max memory_allocated 22564.05419921875 
[2025-02-19 10:25:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.3120041489601135 norm:0.0007693135994486511 max memory_allocated 22564.05419921875 
[2025-02-19 10:25:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.3119174540042877 norm:0.0007726085023023188 max memory_allocated 22564.05419921875 
[2025-02-19 10:26:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.3117806017398834 norm:0.0007577968062832952 max memory_allocated 22564.05419921875 
[2025-02-19 10:26:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-19 10:27:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.4338728189468384 norm:0.043344851583242416 max memory_allocated 22564.22607421875 
[2025-02-19 10:27:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.3772546350955963 norm:0.015466850250959396 max memory_allocated 22564.22607421875 
[2025-02-19 10:28:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.3410763740539551 norm:0.00632946053519845 max memory_allocated 22564.22607421875 
[2025-02-19 10:28:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.3294042944908142 norm:0.0034423130564391613 max memory_allocated 22564.22607421875 
[2025-02-19 10:29:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.32312247157096863 norm:0.002117190742865205 max memory_allocated 22564.22607421875 
[2025-02-19 10:29:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.3192908465862274 norm:0.0017238060245290399 max memory_allocated 22564.22607421875 
[2025-02-19 10:30:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.3165004849433899 norm:0.001579450792632997 max memory_allocated 22564.22607421875 
[2025-02-19 10:30:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.31457799673080444 norm:0.0014421032974496484 max memory_allocated 22564.22607421875 
[2025-02-19 10:31:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.31294381618499756 norm:0.0011964187724515796 max memory_allocated 22564.22607421875 
[2025-02-19 10:31:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.3117624521255493 norm:0.0010929355630651116 max memory_allocated 22564.22607421875 
[2025-02-19 10:32:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.31090599298477173 norm:0.0010722408769652247 max memory_allocated 22564.22607421875 
[2025-02-19 10:32:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.31030142307281494 norm:0.0010402568150311708 max memory_allocated 22564.22607421875 
[2025-02-19 10:33:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.3098057806491852 norm:0.0010010466212406754 max memory_allocated 22564.22607421875 
[2025-02-19 10:33:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.3094009757041931 norm:0.0009788883617147803 max memory_allocated 22564.22607421875 
[2025-02-19 10:34:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.3090743124485016 norm:0.0009531039977446198 max memory_allocated 22564.22607421875 
[2025-02-19 10:34:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.30893510580062866 norm:0.0009203896624967456 max memory_allocated 22564.22607421875 
[2025-02-19 10:35:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.30876681208610535 norm:0.0009157555759884417 max memory_allocated 22564.22607421875 
[2025-02-19 10:35:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.30857276916503906 norm:0.0009086168720386922 max memory_allocated 22564.22607421875 
[2025-02-19 10:36:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.3083859384059906 norm:0.0009090149542316794 max memory_allocated 22564.22607421875 
[2025-02-19 10:36:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.3081642687320709 norm:0.0008686569635756314 max memory_allocated 22564.22607421875 
[2025-02-19 10:36:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-19 10:37:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.3996315896511078 norm:0.015215549618005753 max memory_allocated 22564.39794921875 
[2025-02-19 10:38:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.3655908703804016 norm:0.006426665466278791 max memory_allocated 22564.39794921875 
[2025-02-19 10:38:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.3394588828086853 norm:0.002488132333382964 max memory_allocated 22564.39794921875 
[2025-02-19 10:39:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.3296356797218323 norm:0.0013494901359081268 max memory_allocated 22564.39794921875 
[2025-02-19 10:39:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.32542312145233154 norm:0.0011394387111067772 max memory_allocated 22564.39794921875 
[2025-02-19 10:40:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.32273221015930176 norm:0.001048103324137628 max memory_allocated 22564.39794921875 
[2025-02-19 10:40:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.320651650428772 norm:0.0009674871689639986 max memory_allocated 22564.39794921875 
[2025-02-19 10:41:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.3193327784538269 norm:0.0009155297302640975 max memory_allocated 22564.39794921875 
[2025-02-19 10:41:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.31824201345443726 norm:0.0008697505109012127 max memory_allocated 22564.39794921875 
[2025-02-19 10:42:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.31745702028274536 norm:0.0008429251029156148 max memory_allocated 22564.39794921875 
[2025-02-19 10:42:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.31678229570388794 norm:0.0008058195235207677 max memory_allocated 22564.39794921875 
[2025-02-19 10:43:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.3163492679595947 norm:0.0007978844223544002 max memory_allocated 22564.39794921875 
[2025-02-19 10:43:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.3159222900867462 norm:0.0007858462631702423 max memory_allocated 22564.39794921875 
[2025-02-19 10:44:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.3155844807624817 norm:0.0007776689017191529 max memory_allocated 22564.39794921875 
[2025-02-19 10:44:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.31531205773353577 norm:0.0007831590482965112 max memory_allocated 22564.39794921875 
[2025-02-19 10:45:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.31507229804992676 norm:0.0007694709347561002 max memory_allocated 22564.39794921875 
[2025-02-19 10:45:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.3148452043533325 norm:0.0007626201258972287 max memory_allocated 22564.39794921875 
[2025-02-19 10:46:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.3146454691886902 norm:0.0007508529233746231 max memory_allocated 22564.39794921875 
[2025-02-19 10:46:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.31453782320022583 norm:0.0007544690161012113 max memory_allocated 22564.39794921875 
[2025-02-19 10:47:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.3144291043281555 norm:0.0007624520221725106 max memory_allocated 22564.39794921875 
[2025-02-19 10:47:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-19 10:48:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.43924859166145325 norm:0.038260284811258316 max memory_allocated 22564.56982421875 
[2025-02-19 10:48:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.3876478374004364 norm:0.014232281595468521 max memory_allocated 22564.56982421875 
[2025-02-19 10:49:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.347747802734375 norm:0.00490652397274971 max memory_allocated 22564.56982421875 
[2025-02-19 10:49:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.33489078283309937 norm:0.0023668743669986725 max memory_allocated 22564.56982421875 
[2025-02-19 10:50:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.3296644687652588 norm:0.0015485149342566729 max memory_allocated 22564.56982421875 
[2025-02-19 10:50:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.3265511989593506 norm:0.001429711701348424 max memory_allocated 22564.56982421875 
[2025-02-19 10:51:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.32440072298049927 norm:0.0013736701803281903 max memory_allocated 22564.56982421875 
[2025-02-19 10:51:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.32264187932014465 norm:0.0013285854365676641 max memory_allocated 22564.56982421875 
[2025-02-19 10:52:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.32124072313308716 norm:0.0013091877335682511 max memory_allocated 22564.56982421875 
[2025-02-19 10:52:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.3202187418937683 norm:0.0012610837584361434 max memory_allocated 22564.56982421875 
[2025-02-19 10:53:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.3194664418697357 norm:0.0012232728768140078 max memory_allocated 22564.56982421875 
[2025-02-19 10:53:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.31879520416259766 norm:0.0011775094317272305 max memory_allocated 22564.56982421875 
[2025-02-19 10:54:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.3181578814983368 norm:0.0011174631072208285 max memory_allocated 22564.56982421875 
[2025-02-19 10:54:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.31767529249191284 norm:0.0010424073552712798 max memory_allocated 22564.56982421875 
[2025-02-19 10:55:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.3173530697822571 norm:0.0010293526574969292 max memory_allocated 22564.56982421875 
[2025-02-19 10:55:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.31708472967147827 norm:0.0009747231961227953 max memory_allocated 22564.56982421875 
[2025-02-19 10:56:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.3168747127056122 norm:0.0009149941033683717 max memory_allocated 22564.56982421875 
[2025-02-19 10:56:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.31664666533470154 norm:0.0009147662203758955 max memory_allocated 22564.56982421875 
[2025-02-19 10:57:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.31638216972351074 norm:0.0008892800542525947 max memory_allocated 22564.56982421875 
[2025-02-19 10:57:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.3161860704421997 norm:0.000864007044583559 max memory_allocated 22564.56982421875 
[2025-02-19 10:57:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-19 10:58:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.4504544734954834 norm:0.04355279356241226 max memory_allocated 22564.74169921875 
[2025-02-19 10:58:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.4036389887332916 norm:0.0186570193618536 max memory_allocated 22564.74169921875 
[2025-02-19 10:59:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.3626764714717865 norm:0.007190927397459745 max memory_allocated 22564.74169921875 
[2025-02-19 11:00:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.34638309478759766 norm:0.0030698636546730995 max memory_allocated 22564.74169921875 
[2025-02-19 11:00:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.34095317125320435 norm:0.0020804493688046932 max memory_allocated 22564.74169921875 
[2025-02-19 11:01:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.3378303050994873 norm:0.0018421078566461802 max memory_allocated 22564.74169921875 
[2025-02-19 11:01:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.335344135761261 norm:0.0016781034646555781 max memory_allocated 22564.74169921875 
[2025-02-19 11:02:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.3334118127822876 norm:0.0015637892065569758 max memory_allocated 22564.74169921875 
[2025-02-19 11:02:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.3318648338317871 norm:0.001488450332544744 max memory_allocated 22564.74169921875 
[2025-02-19 11:03:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.330582857131958 norm:0.0014070331817492843 max memory_allocated 22564.74169921875 
[2025-02-19 11:03:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.3295460343360901 norm:0.0013429383980110288 max memory_allocated 22564.74169921875 
[2025-02-19 11:04:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.32873696088790894 norm:0.0013059435877949 max memory_allocated 22564.74169921875 
[2025-02-19 11:04:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.3280319273471832 norm:0.0012435039971023798 max memory_allocated 22564.74169921875 
[2025-02-19 11:05:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.32752877473831177 norm:0.0012121140025556087 max memory_allocated 22564.74169921875 
[2025-02-19 11:05:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.3270077109336853 norm:0.0011766457464545965 max memory_allocated 22564.74169921875 
[2025-02-19 11:06:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.32653388381004333 norm:0.0011516204103827477 max memory_allocated 22564.74169921875 
[2025-02-19 11:06:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.3260822892189026 norm:0.001147434115409851 max memory_allocated 22564.74169921875 
[2025-02-19 11:07:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.32566016912460327 norm:0.0011117075337097049 max memory_allocated 22564.74169921875 
[2025-02-19 11:07:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.32544317841529846 norm:0.0010961915832012892 max memory_allocated 22564.74169921875 
[2025-02-19 11:08:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.3252471089363098 norm:0.0010732949012890458 max memory_allocated 22564.74169921875 
[2025-02-19 11:08:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-19 11:08:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.4287031888961792 norm:0.034832194447517395 max memory_allocated 22564.91357421875 
[2025-02-19 11:09:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.39524412155151367 norm:0.015900321304798126 max memory_allocated 22564.91357421875 
[2025-02-19 11:09:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.3613731861114502 norm:0.004806199111044407 max memory_allocated 22564.91357421875 
[2025-02-19 11:10:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.35056570172309875 norm:0.0019048035610467196 max memory_allocated 22564.91357421875 
[2025-02-19 11:10:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.3471083641052246 norm:0.0015880288556218147 max memory_allocated 22564.91357421875 
[2025-02-19 11:11:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.3446524143218994 norm:0.0014299077447503805 max memory_allocated 22564.91357421875 
[2025-02-19 11:11:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.3428709805011749 norm:0.0013402962358668447 max memory_allocated 22564.91357421875 
[2025-02-19 11:12:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.3415451645851135 norm:0.0013287710025906563 max memory_allocated 22564.91357421875 
[2025-02-19 11:12:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.3404047191143036 norm:0.0013103908859193325 max memory_allocated 22564.91357421875 
[2025-02-19 11:13:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.33943748474121094 norm:0.001278810203075409 max memory_allocated 22564.91357421875 
[2025-02-19 11:13:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.33873310685157776 norm:0.0012499222066253424 max memory_allocated 22564.91357421875 
[2025-02-19 11:14:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.3381096124649048 norm:0.0012304645497351885 max memory_allocated 22564.91357421875 
[2025-02-19 11:15:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.3374998867511749 norm:0.001180557650513947 max memory_allocated 22564.91357421875 
[2025-02-19 11:15:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.33710095286369324 norm:0.0011645799968391657 max memory_allocated 22564.91357421875 
[2025-02-19 11:16:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.33665135502815247 norm:0.0011127921752631664 max memory_allocated 22564.91357421875 
[2025-02-19 11:16:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.3363487720489502 norm:0.0010784100741147995 max memory_allocated 22564.91357421875 
[2025-02-19 11:17:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.33606255054473877 norm:0.0010555838234722614 max memory_allocated 22564.91357421875 
[2025-02-19 11:17:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.3358413279056549 norm:0.001033526612445712 max memory_allocated 22564.91357421875 
[2025-02-19 11:18:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.3355450928211212 norm:0.0010042398935183883 max memory_allocated 22564.91357421875 
[2025-02-19 11:18:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.33534812927246094 norm:0.0009801125852391124 max memory_allocated 22564.91357421875 
[2025-02-19 11:18:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-19 11:19:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.45279401540756226 norm:0.04795364290475845 max memory_allocated 22565.08544921875 
[2025-02-19 11:19:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.4217897355556488 norm:0.020324278622865677 max memory_allocated 22565.08544921875 
[2025-02-19 11:20:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.3905608057975769 norm:0.007917974144220352 max memory_allocated 22565.08544921875 
[2025-02-19 11:20:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.37592899799346924 norm:0.003530805930495262 max memory_allocated 22565.08544921875 
[2025-02-19 11:21:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.37005123496055603 norm:0.002035373356193304 max memory_allocated 22565.08544921875 
[2025-02-19 11:21:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.3671202063560486 norm:0.0013928175903856754 max memory_allocated 22565.08544921875 
[2025-02-19 11:22:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.36525461077690125 norm:0.0013285232707858086 max memory_allocated 22565.08544921875 
[2025-02-19 11:22:51 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.3636844754219055 norm:0.0012840741546824574 max memory_allocated 22565.08544921875 
[2025-02-19 11:23:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.36250585317611694 norm:0.0012791630579158664 max memory_allocated 22565.08544921875 
[2025-02-19 11:23:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.3616112768650055 norm:0.0012791728368028998 max memory_allocated 22565.08544921875 
[2025-02-19 11:24:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.36074206233024597 norm:0.0012190925190225244 max memory_allocated 22565.08544921875 
[2025-02-19 11:24:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.35999199748039246 norm:0.001148601179011166 max memory_allocated 22565.08544921875 
[2025-02-19 11:25:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.3595212399959564 norm:0.001119975815527141 max memory_allocated 22565.08544921875 
[2025-02-19 11:25:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.3590943217277527 norm:0.0011122493306174874 max memory_allocated 22565.08544921875 
[2025-02-19 11:26:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.3586951792240143 norm:0.0010860033798962831 max memory_allocated 22565.08544921875 
[2025-02-19 11:26:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.35821953415870667 norm:0.0010521578369662166 max memory_allocated 22565.08544921875 
[2025-02-19 11:27:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.3579432964324951 norm:0.0010252411011606455 max memory_allocated 22565.08544921875 
[2025-02-19 11:27:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.3577224016189575 norm:0.0009900687728077173 max memory_allocated 22565.08544921875 
[2025-02-19 11:28:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.35749876499176025 norm:0.0009693681495264173 max memory_allocated 22565.08544921875 
[2025-02-19 11:28:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.35735613107681274 norm:0.0009595783776603639 max memory_allocated 22565.08544921875 
[2025-02-19 11:29:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-19 11:29:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.45927610993385315 norm:0.03882475197315216 max memory_allocated 22565.25732421875 
[2025-02-19 11:30:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.43378621339797974 norm:0.017484061419963837 max memory_allocated 22565.25732421875 
[2025-02-19 11:30:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.40909063816070557 norm:0.007638772018253803 max memory_allocated 22565.25732421875 
[2025-02-19 11:31:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.3974734842777252 norm:0.0027809327002614737 max memory_allocated 22565.25732421875 
[2025-02-19 11:31:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.39376628398895264 norm:0.0011612685630097985 max memory_allocated 22565.25732421875 
[2025-02-19 11:32:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.391791433095932 norm:0.0010936688631772995 max memory_allocated 22565.25732421875 
[2025-02-19 11:32:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.39018863439559937 norm:0.0010536707704886794 max memory_allocated 22565.25732421875 
[2025-02-19 11:33:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.3888249695301056 norm:0.0010042472276836634 max memory_allocated 22565.25732421875 
[2025-02-19 11:33:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.3877352178096771 norm:0.0009803145658224821 max memory_allocated 22565.25732421875 
[2025-02-19 11:34:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.38690608739852905 norm:0.0009605582454241812 max memory_allocated 22565.25732421875 
[2025-02-19 11:34:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.38623568415641785 norm:0.0009256900520995259 max memory_allocated 22565.25732421875 
[2025-02-19 11:35:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.38563981652259827 norm:0.000878338934853673 max memory_allocated 22565.25732421875 
[2025-02-19 11:35:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.38520902395248413 norm:0.0008598180138505995 max memory_allocated 22565.25732421875 
[2025-02-19 11:36:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.3848693370819092 norm:0.0008542462601326406 max memory_allocated 22565.25732421875 
[2025-02-19 11:36:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.3845449984073639 norm:0.0008182997698895633 max memory_allocated 22565.25732421875 
[2025-02-19 11:37:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.3842471241950989 norm:0.0007959204376675189 max memory_allocated 22565.25732421875 
[2025-02-19 11:37:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.3839898109436035 norm:0.000790419231634587 max memory_allocated 22565.25732421875 
[2025-02-19 11:38:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.38376960158348083 norm:0.0007983860559761524 max memory_allocated 22565.25732421875 
[2025-02-19 11:38:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.3835810422897339 norm:0.0007875863811932504 max memory_allocated 22565.25732421875 
[2025-02-19 11:39:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.3834024667739868 norm:0.000772617815528065 max memory_allocated 22565.25732421875 
[2025-02-19 11:39:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-19 11:40:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.49576592445373535 norm:0.0252634659409523 max memory_allocated 22565.42919921875 
[2025-02-19 11:40:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.4730772078037262 norm:0.012724066153168678 max memory_allocated 22565.42919921875 
[2025-02-19 11:41:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.4495614171028137 norm:0.0057060495018959045 max memory_allocated 22565.42919921875 
[2025-02-19 11:41:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.439336359500885 norm:0.003265331732109189 max memory_allocated 22565.42919921875 
[2025-02-19 11:42:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.4354621171951294 norm:0.002293903846293688 max memory_allocated 22565.42919921875 
[2025-02-19 11:42:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.4324484169483185 norm:0.0015557752922177315 max memory_allocated 22565.42919921875 
[2025-02-19 11:43:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.4304206371307373 norm:0.0014098127139732242 max memory_allocated 22565.42919921875 
[2025-02-19 11:43:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.4288196861743927 norm:0.0012957616709172726 max memory_allocated 22565.42919921875 
[2025-02-19 11:44:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.4275702238082886 norm:0.0012081712484359741 max memory_allocated 22565.42919921875 
[2025-02-19 11:44:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.4265427887439728 norm:0.0011474353959783912 max memory_allocated 22565.42919921875 
[2025-02-19 11:45:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.4257536828517914 norm:0.001097113243304193 max memory_allocated 22565.42919921875 
[2025-02-19 11:45:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.42517074942588806 norm:0.001078255008906126 max memory_allocated 22565.42919921875 
[2025-02-19 11:46:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.42469507455825806 norm:0.0010665912413969636 max memory_allocated 22565.42919921875 
[2025-02-19 11:46:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.42432141304016113 norm:0.001045058248564601 max memory_allocated 22565.42919921875 
[2025-02-19 11:47:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.42392057180404663 norm:0.001022238633595407 max memory_allocated 22565.42919921875 
[2025-02-19 11:47:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.42359980940818787 norm:0.0010119626531377435 max memory_allocated 22565.42919921875 
[2025-02-19 11:48:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.42328572273254395 norm:0.0010065289679914713 max memory_allocated 22565.42919921875 
[2025-02-19 11:48:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.42298710346221924 norm:0.0009888822678476572 max memory_allocated 22565.42919921875 
[2025-02-19 11:49:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.4227583408355713 norm:0.0009844352025538683 max memory_allocated 22565.42919921875 
[2025-02-19 11:49:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.4225558042526245 norm:0.0009632973233237863 max memory_allocated 22565.42919921875 
[2025-02-19 11:49:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-19 11:50:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.5147901177406311 norm:0.018217232078313828 max memory_allocated 22565.60107421875 
[2025-02-19 11:50:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.49767184257507324 norm:0.008092910051345825 max memory_allocated 22565.60107421875 
[2025-02-19 11:51:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.4828535318374634 norm:0.003901239950209856 max memory_allocated 22565.60107421875 
[2025-02-19 11:52:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.47547072172164917 norm:0.001499496283940971 max memory_allocated 22565.60107421875 
[2025-02-19 11:52:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.4726754426956177 norm:0.0009995654691010714 max memory_allocated 22565.60107421875 
[2025-02-19 11:53:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.47105711698532104 norm:0.0009460648288950324 max memory_allocated 22565.60107421875 
[2025-02-19 11:53:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.4696720242500305 norm:0.0009265758562833071 max memory_allocated 22565.60107421875 
[2025-02-19 11:54:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.468269944190979 norm:0.0008842393872328103 max memory_allocated 22565.60107421875 
[2025-02-19 11:54:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.4672754406929016 norm:0.0008568770717829466 max memory_allocated 22565.60107421875 
[2025-02-19 11:55:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.4665910601615906 norm:0.0008282645139843225 max memory_allocated 22565.60107421875 
[2025-02-19 11:55:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.4660102128982544 norm:0.0008040169486775994 max memory_allocated 22565.60107421875 
[2025-02-19 11:56:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.46554169058799744 norm:0.0008000593516044319 max memory_allocated 22565.60107421875 
[2025-02-19 11:56:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.46515601873397827 norm:0.0007836257573217154 max memory_allocated 22565.60107421875 
[2025-02-19 11:57:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.46479880809783936 norm:0.000774692976847291 max memory_allocated 22565.60107421875 
[2025-02-19 11:57:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.4644957184791565 norm:0.0007468948606401682 max memory_allocated 22565.60107421875 
[2025-02-19 11:58:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.4642392694950104 norm:0.0007320322329178452 max memory_allocated 22565.60107421875 
[2025-02-19 11:58:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.46406352519989014 norm:0.0007334843976423144 max memory_allocated 22565.60107421875 
[2025-02-19 11:59:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.4639323055744171 norm:0.0007292895461432636 max memory_allocated 22565.60107421875 
[2025-02-19 11:59:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.4637998044490814 norm:0.0007219606195576489 max memory_allocated 22565.60107421875 
[2025-02-19 12:00:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.4636821150779724 norm:0.000708626233972609 max memory_allocated 22565.60107421875 
[2025-02-19 12:00:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-19 12:00:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.5797941088676453 norm:0.01669193245470524 max memory_allocated 22565.77294921875 
[2025-02-19 12:01:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.5586107969284058 norm:0.0061697508208453655 max memory_allocated 22565.77294921875 
[2025-02-19 12:01:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.5421721339225769 norm:0.0028912480920553207 max memory_allocated 22565.77294921875 
[2025-02-19 12:02:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.5353100299835205 norm:0.0015220193890854716 max memory_allocated 22565.77294921875 
[2025-02-19 12:02:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.532753586769104 norm:0.0012604757212102413 max memory_allocated 22565.77294921875 
[2025-02-19 12:03:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.5309458374977112 norm:0.0011518767569214106 max memory_allocated 22565.77294921875 
[2025-02-19 12:03:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.5294396877288818 norm:0.0010565384291112423 max memory_allocated 22565.77294921875 
[2025-02-19 12:04:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.5282061100006104 norm:0.0010145057458430529 max memory_allocated 22565.77294921875 
[2025-02-19 12:04:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.5272026062011719 norm:0.001026826910674572 max memory_allocated 22565.77294921875 
[2025-02-19 12:05:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.5264761447906494 norm:0.001015320303849876 max memory_allocated 22565.77294921875 
[2025-02-19 12:05:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.5257689356803894 norm:0.000946026761084795 max memory_allocated 22565.77294921875 
[2025-02-19 12:06:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.5252196192741394 norm:0.0010114458855241537 max memory_allocated 22565.77294921875 
[2025-02-19 12:07:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.5250638723373413 norm:0.0009490784723311663 max memory_allocated 22565.77294921875 
[2025-02-19 12:07:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.5245293378829956 norm:0.0009176505263894796 max memory_allocated 22565.77294921875 
[2025-02-19 12:08:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.5242276191711426 norm:0.0008877229993231595 max memory_allocated 22565.77294921875 
[2025-02-19 12:08:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.5241461992263794 norm:0.0008989550406113267 max memory_allocated 22565.77294921875 
[2025-02-19 12:09:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.5238187313079834 norm:0.0008672538097016513 max memory_allocated 22565.77294921875 
[2025-02-19 12:09:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.5234006643295288 norm:0.0008711184491403401 max memory_allocated 22565.77294921875 
[2025-02-19 12:10:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.5234050750732422 norm:0.0008880890090949833 max memory_allocated 22565.77294921875 
[2025-02-19 12:10:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.5231223106384277 norm:0.0008299987530335784 max memory_allocated 22565.77294921875 
[2025-02-19 12:10:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-19 12:11:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.6267691850662231 norm:0.008442588150501251 max memory_allocated 22565.94482421875 
[2025-02-19 12:11:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.6125490665435791 norm:0.004176347050815821 max memory_allocated 22565.94482421875 
[2025-02-19 12:12:18 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.5985711812973022 norm:0.002326339017599821 max memory_allocated 22565.94482421875 
[2025-02-19 12:12:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.5928568840026855 norm:0.001341079594567418 max memory_allocated 22565.94482421875 
[2025-02-19 12:13:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.5900980234146118 norm:0.0010216182563453913 max memory_allocated 22565.94482421875 
[2025-02-19 12:13:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.5880986452102661 norm:0.0008633577963337302 max memory_allocated 22565.94482421875 
[2025-02-19 12:14:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.5865558981895447 norm:0.0008170983055606484 max memory_allocated 22565.94482421875 
[2025-02-19 12:14:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.5853725075721741 norm:0.0007674118387512863 max memory_allocated 22565.94482421875 
[2025-02-19 12:15:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.5844283699989319 norm:0.0007367712096311152 max memory_allocated 22565.94482421875 
[2025-02-19 12:15:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.5837786197662354 norm:0.0007049203268252313 max memory_allocated 22565.94482421875 
[2025-02-19 12:16:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.5833070874214172 norm:0.0006917211576364934 max memory_allocated 22565.94482421875 
[2025-02-19 12:16:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.5829046964645386 norm:0.0006750976317562163 max memory_allocated 22565.94482421875 
[2025-02-19 12:17:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.5825703740119934 norm:0.000668137741740793 max memory_allocated 22565.94482421875 
[2025-02-19 12:17:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.5823454856872559 norm:0.0006707715219818056 max memory_allocated 22565.94482421875 
[2025-02-19 12:18:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.5821300148963928 norm:0.0006697698263451457 max memory_allocated 22565.94482421875 
[2025-02-19 12:18:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.5819891095161438 norm:0.0006652390002273023 max memory_allocated 22565.94482421875 
[2025-02-19 12:19:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.5818558931350708 norm:0.0006580364424735308 max memory_allocated 22565.94482421875 
[2025-02-19 12:19:59 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.5816804766654968 norm:0.0006452484522014856 max memory_allocated 22565.94482421875 
[2025-02-19 12:20:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.5815494656562805 norm:0.0006460793665610254 max memory_allocated 22565.94482421875 
[2025-02-19 12:21:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.5814815163612366 norm:0.0006404074956662953 max memory_allocated 22565.94482421875 
[2025-02-19 12:21:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-19 12:21:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.7013078331947327 norm:0.013361476361751556 max memory_allocated 22566.11669921875 
[2025-02-19 12:22:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.6841945648193359 norm:0.006129630375653505 max memory_allocated 22566.11669921875 
[2025-02-19 12:22:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.6677509546279907 norm:0.0030573569238185883 max memory_allocated 22566.11669921875 
[2025-02-19 12:23:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.6606224775314331 norm:0.0014182557351887226 max memory_allocated 22566.11669921875 
[2025-02-19 12:23:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.6575388312339783 norm:0.0008965344750322402 max memory_allocated 22566.11669921875 
[2025-02-19 12:24:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.6554104089736938 norm:0.0008796914480626583 max memory_allocated 22566.11669921875 
[2025-02-19 12:24:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.6536208987236023 norm:0.0008739917539060116 max memory_allocated 22566.11669921875 
[2025-02-19 12:25:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.6522125005722046 norm:0.0008355084573850036 max memory_allocated 22566.11669921875 
[2025-02-19 12:25:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.6512227654457092 norm:0.0008200795855373144 max memory_allocated 22566.11669921875 
[2025-02-19 12:26:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.6504291892051697 norm:0.0007755672559142113 max memory_allocated 22566.11669921875 
[2025-02-19 12:26:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.6498920917510986 norm:0.0007656915695406497 max memory_allocated 22566.11669921875 
[2025-02-19 12:27:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.6494133472442627 norm:0.0007519794162362814 max memory_allocated 22566.11669921875 
[2025-02-19 12:27:49 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.6491011381149292 norm:0.0007356766145676374 max memory_allocated 22566.11669921875 
[2025-02-19 12:28:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.6488111615180969 norm:0.0007282981532625854 max memory_allocated 22566.11669921875 
[2025-02-19 12:28:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.6485876441001892 norm:0.0007261940627358854 max memory_allocated 22566.11669921875 
[2025-02-19 12:29:21 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.6483483910560608 norm:0.0007197516970336437 max memory_allocated 22566.11669921875 
[2025-02-19 12:29:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.6481013298034668 norm:0.0007168148877099156 max memory_allocated 22566.11669921875 
[2025-02-19 12:30:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.647946834564209 norm:0.0007128299912437797 max memory_allocated 22566.11669921875 
[2025-02-19 12:30:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.6477700471878052 norm:0.0007242246065288782 max memory_allocated 22566.11669921875 
[2025-02-19 12:31:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.6476645469665527 norm:0.0007208433235064149 max memory_allocated 22566.11669921875 
[2025-02-19 12:31:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-19 12:32:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.791285514831543 norm:0.024551132693886757 max memory_allocated 22566.28857421875 
[2025-02-19 12:32:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.775363564491272 norm:0.014896741136908531 max memory_allocated 22566.28857421875 
[2025-02-19 12:33:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.7578743696212769 norm:0.009419763460755348 max memory_allocated 22566.28857421875 
[2025-02-19 12:33:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.7507887482643127 norm:0.006588033866137266 max memory_allocated 22566.28857421875 
[2025-02-19 12:34:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.7473140358924866 norm:0.005238448269665241 max memory_allocated 22566.28857421875 
[2025-02-19 12:34:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.7436124682426453 norm:0.003798033809289336 max memory_allocated 22566.28857421875 
[2025-02-19 12:35:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.73909592628479 norm:0.003044947050511837 max memory_allocated 22566.28857421875 
[2025-02-19 12:35:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.7372740507125854 norm:0.002952088136225939 max memory_allocated 22566.28857421875 
[2025-02-19 12:36:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.7362280488014221 norm:0.002733933972194791 max memory_allocated 22566.28857421875 
[2025-02-19 12:36:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.7353625893592834 norm:0.0025390414521098137 max memory_allocated 22566.28857421875 
[2025-02-19 12:37:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.7342301607131958 norm:0.0019910200498998165 max memory_allocated 22566.28857421875 
[2025-02-19 12:37:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.7337132692337036 norm:0.0022000563330948353 max memory_allocated 22566.28857421875 
[2025-02-19 12:38:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.7329081296920776 norm:0.0017234061378985643 max memory_allocated 22566.28857421875 
[2025-02-19 12:38:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.7327275276184082 norm:0.0019306333269923925 max memory_allocated 22566.28857421875 
[2025-02-19 12:39:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.7318795919418335 norm:0.0014667632058262825 max memory_allocated 22566.28857421875 
[2025-02-19 12:39:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.7316619157791138 norm:0.0016588871367275715 max memory_allocated 22566.28857421875 
[2025-02-19 12:40:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.7311201095581055 norm:0.0013267771573737264 max memory_allocated 22566.28857421875 
[2025-02-19 12:40:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.7307460308074951 norm:0.0014153860975056887 max memory_allocated 22566.28857421875 
[2025-02-19 12:41:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.7303261160850525 norm:0.0011830406729131937 max memory_allocated 22566.28857421875 
[2025-02-19 12:41:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.7301663160324097 norm:0.0012939057778567076 max memory_allocated 22566.28857421875 
[2025-02-19 12:41:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-19 12:42:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.8775542378425598 norm:0.010142592713236809 max memory_allocated 22566.46044921875 
[2025-02-19 12:43:01 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.8592091798782349 norm:0.005158472340553999 max memory_allocated 22566.46044921875 
[2025-02-19 12:43:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.8393936157226562 norm:0.002269146731123328 max memory_allocated 22566.46044921875 
[2025-02-19 12:44:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.8312637209892273 norm:0.0013018944300711155 max memory_allocated 22566.46044921875 
[2025-02-19 12:44:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.8277837038040161 norm:0.0010535212932154536 max memory_allocated 22566.46044921875 
[2025-02-19 12:45:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.8250492215156555 norm:0.0009438759880140424 max memory_allocated 22566.46044921875 
[2025-02-19 12:45:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.8228317499160767 norm:0.0009088749648071826 max memory_allocated 22566.46044921875 
[2025-02-19 12:46:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.8210941553115845 norm:0.0008590018842369318 max memory_allocated 22566.46044921875 
[2025-02-19 12:46:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.8198561072349548 norm:0.0007957009947858751 max memory_allocated 22566.46044921875 
[2025-02-19 12:47:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.8189325332641602 norm:0.0007615094073116779 max memory_allocated 22566.46044921875 
[2025-02-19 12:47:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.8182188272476196 norm:0.0007514094468206167 max memory_allocated 22566.46044921875 
[2025-02-19 12:48:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.8176169395446777 norm:0.0007460615015588701 max memory_allocated 22566.46044921875 
[2025-02-19 12:48:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.8171759843826294 norm:0.0007240427075885236 max memory_allocated 22566.46044921875 
[2025-02-19 12:49:09 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.8167154788970947 norm:0.0007096004555933177 max memory_allocated 22566.46044921875 
[2025-02-19 12:49:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.8163141012191772 norm:0.0007003853679634631 max memory_allocated 22566.46044921875 
[2025-02-19 12:50:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.8160438537597656 norm:0.0007077110349200666 max memory_allocated 22566.46044921875 
[2025-02-19 12:50:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.8157874345779419 norm:0.0007086866535246372 max memory_allocated 22566.46044921875 
[2025-02-19 12:51:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.8155931234359741 norm:0.0007090513245202601 max memory_allocated 22566.46044921875 
[2025-02-19 12:51:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.8154276013374329 norm:0.0007066500838845968 max memory_allocated 22566.46044921875 
[2025-02-19 12:52:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.8152389526367188 norm:0.0007012288551777601 max memory_allocated 22566.46044921875 
[2025-02-19 12:52:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-19 12:53:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.982150673866272 norm:0.014800370670855045 max memory_allocated 22566.63232421875 
[2025-02-19 12:53:30 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.9614937901496887 norm:0.006880952976644039 max memory_allocated 22566.63232421875 
[2025-02-19 12:54:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.9411880373954773 norm:0.0034961302299052477 max memory_allocated 22566.63232421875 
[2025-02-19 12:54:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.9327783584594727 norm:0.0023013711906969547 max memory_allocated 22566.63232421875 
[2025-02-19 12:55:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.9287469387054443 norm:0.0017892610048875213 max memory_allocated 22566.63232421875 
[2025-02-19 12:55:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.924987256526947 norm:0.0009870356880128384 max memory_allocated 22566.63232421875 
[2025-02-19 12:56:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.9222543239593506 norm:0.0007397832814604044 max memory_allocated 22566.63232421875 
[2025-02-19 12:56:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.9204530715942383 norm:0.0007203230634331703 max memory_allocated 22566.63232421875 
[2025-02-19 12:57:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.919251024723053 norm:0.0007137290667742491 max memory_allocated 22566.63232421875 
[2025-02-19 12:57:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.9183827638626099 norm:0.0007033151341602206 max memory_allocated 22566.63232421875 
[2025-02-19 12:58:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.9176806211471558 norm:0.000685951963532716 max memory_allocated 22566.63232421875 
[2025-02-19 12:58:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.91716468334198 norm:0.0006919906591065228 max memory_allocated 22566.63232421875 
[2025-02-19 12:59:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.9167443513870239 norm:0.0006801295094192028 max memory_allocated 22566.63232421875 
[2025-02-19 12:59:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.916379451751709 norm:0.0006709722802042961 max memory_allocated 22566.63232421875 
[2025-02-19 13:00:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.9160770177841187 norm:0.0006663890089839697 max memory_allocated 22566.63232421875 
[2025-02-19 13:00:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.9157978892326355 norm:0.0006659360369667411 max memory_allocated 22566.63232421875 
[2025-02-19 13:01:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.9156016707420349 norm:0.0006603645160794258 max memory_allocated 22566.63232421875 
[2025-02-19 13:01:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.9153658151626587 norm:0.0006546737276948988 max memory_allocated 22566.63232421875 
[2025-02-19 13:02:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.9151031374931335 norm:0.0006545566720888019 max memory_allocated 22566.63232421875 
[2025-02-19 13:02:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.9149484038352966 norm:0.0006479566218331456 max memory_allocated 22566.63232421875 
[2025-02-19 13:02:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-19 13:02:53 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:03:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:1.1123287677764893 norm:0.031078115105628967 max memory_allocated 22566.91943359375 
[2025-02-19 13:03:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:1.089613914489746 norm:0.02384926751255989 max memory_allocated 22566.91943359375 
[2025-02-19 13:04:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:1.0656123161315918 norm:0.016834920272231102 max memory_allocated 22566.91943359375 
[2025-02-19 13:04:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:1.0545144081115723 norm:0.013539258390665054 max memory_allocated 22566.91943359375 
[2025-02-19 13:05:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:1.0491526126861572 norm:0.011442713439464569 max memory_allocated 22566.91943359375 
[2025-02-19 13:05:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:1.0447053909301758 norm:0.009499181061983109 max memory_allocated 22566.91943359375 
[2025-02-19 13:06:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:1.0415451526641846 norm:0.008237898349761963 max memory_allocated 22566.91943359375 
[2025-02-19 13:07:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:1.0391387939453125 norm:0.0073056770488619804 max memory_allocated 22566.91943359375 
[2025-02-19 13:07:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:1.0375769138336182 norm:0.006821461021900177 max memory_allocated 22566.91943359375 
[2025-02-19 13:08:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:1.0365930795669556 norm:0.0068421754986047745 max memory_allocated 22566.91943359375 
[2025-02-19 13:08:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:1.0355713367462158 norm:0.006717672571539879 max memory_allocated 22566.91943359375 
[2025-02-19 13:09:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:1.0347598791122437 norm:0.00654496718198061 max memory_allocated 22566.91943359375 
[2025-02-19 13:09:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:1.0341159105300903 norm:0.006703027989715338 max memory_allocated 22566.91943359375 
[2025-02-19 13:10:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:1.033448576927185 norm:0.00634766835719347 max memory_allocated 22566.91943359375 
[2025-02-19 13:10:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:1.0327361822128296 norm:0.006167457439005375 max memory_allocated 22566.91943359375 
[2025-02-19 13:11:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:1.0323892831802368 norm:0.005914309062063694 max memory_allocated 22566.91943359375 
[2025-02-19 13:11:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:1.0318858623504639 norm:0.005929259117692709 max memory_allocated 22566.91943359375 
[2025-02-19 13:12:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:1.031723976135254 norm:0.006048095412552357 max memory_allocated 22566.91943359375 
[2025-02-19 13:12:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:1.031243085861206 norm:0.0061078197322785854 max memory_allocated 22566.91943359375 
[2025-02-19 13:13:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:1.0309780836105347 norm:0.005754656158387661 max memory_allocated 22566.91943359375 
[2025-02-19 13:13:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-19 13:13:21 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:13:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:1.2594592571258545 norm:0.02969937212765217 max memory_allocated 22567.09130859375 
[2025-02-19 13:14:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:1.2299516201019287 norm:0.023015175014734268 max memory_allocated 22567.09130859375 
[2025-02-19 13:14:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:1.201431155204773 norm:0.01695593260228634 max memory_allocated 22567.09130859375 
[2025-02-19 13:15:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:1.1869473457336426 norm:0.014005407691001892 max memory_allocated 22567.09130859375 
[2025-02-19 13:15:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:1.1787766218185425 norm:0.011267983354628086 max memory_allocated 22567.09130859375 
[2025-02-19 13:16:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:1.1733382940292358 norm:0.00971553847193718 max memory_allocated 22567.09130859375 
[2025-02-19 13:16:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:1.169655442237854 norm:0.008981643244624138 max memory_allocated 22567.09130859375 
[2025-02-19 13:17:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:1.1673920154571533 norm:0.009017635136842728 max memory_allocated 22567.09130859375 
[2025-02-19 13:17:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:1.1655679941177368 norm:0.008878210559487343 max memory_allocated 22567.09130859375 
[2025-02-19 13:18:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:1.1641172170639038 norm:0.008040106855332851 max memory_allocated 22567.09130859375 
[2025-02-19 13:18:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:1.1629307270050049 norm:0.008010377176105976 max memory_allocated 22567.09130859375 
[2025-02-19 13:19:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:1.162428855895996 norm:0.008156548254191875 max memory_allocated 22567.09130859375 
[2025-02-19 13:20:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:1.1614493131637573 norm:0.007896036840975285 max memory_allocated 22567.09130859375 
[2025-02-19 13:20:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:1.1605987548828125 norm:0.007371216081082821 max memory_allocated 22567.09130859375 
[2025-02-19 13:21:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:1.1599136590957642 norm:0.007195387035608292 max memory_allocated 22567.09130859375 
[2025-02-19 13:21:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:1.1597015857696533 norm:0.007424376904964447 max memory_allocated 22567.09130859375 
[2025-02-19 13:22:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:1.1592732667922974 norm:0.007484924513846636 max memory_allocated 22567.09130859375 
[2025-02-19 13:22:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:1.1585125923156738 norm:0.0070840297266840935 max memory_allocated 22567.09130859375 
[2025-02-19 13:23:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:1.1581650972366333 norm:0.0069363233633339405 max memory_allocated 22567.09130859375 
[2025-02-19 13:23:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:1.1579208374023438 norm:0.007019422948360443 max memory_allocated 22567.09130859375 
[2025-02-19 13:23:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-19 13:23:48 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:24:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:2.8126513957977295 norm:0.6675138473510742 max memory_allocated 22567.26318359375 
[2025-02-19 13:24:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:2.9106826782226562 norm:1.1173324584960938 max memory_allocated 22567.26318359375 
[2025-02-19 13:25:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:2.4494287967681885 norm:0.4519607126712799 max memory_allocated 22567.26318359375 
[2025-02-19 13:25:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:1.9132939577102661 norm:0.2596237361431122 max memory_allocated 22567.26318359375 
[2025-02-19 13:26:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:1.8124831914901733 norm:0.19601351022720337 max memory_allocated 22567.26318359375 
[2025-02-19 13:26:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:1.780616044998169 norm:0.17632246017456055 max memory_allocated 22567.26318359375 
[2025-02-19 13:27:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:1.757171630859375 norm:0.15357092022895813 max memory_allocated 22567.26318359375 
[2025-02-19 13:27:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:1.7409528493881226 norm:0.14882685244083405 max memory_allocated 22567.26318359375 
[2025-02-19 13:28:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:1.7256466150283813 norm:0.14056411385536194 max memory_allocated 22567.26318359375 
[2025-02-19 13:28:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:1.7187082767486572 norm:0.14339357614517212 max memory_allocated 22567.26318359375 
[2025-02-19 13:29:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:1.714889407157898 norm:0.1548498570919037 max memory_allocated 22567.26318359375 
[2025-02-19 13:29:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:1.7155510187149048 norm:0.1449398249387741 max memory_allocated 22567.26318359375 
[2025-02-19 13:30:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:1.7157833576202393 norm:0.14324256777763367 max memory_allocated 22567.26318359375 
[2025-02-19 13:30:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:1.7160203456878662 norm:0.12536337971687317 max memory_allocated 22567.26318359375 
[2025-02-19 13:31:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:1.704765796661377 norm:0.10723967105150223 max memory_allocated 22567.26318359375 
[2025-02-19 13:32:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:1.7126606702804565 norm:0.10695282369852066 max memory_allocated 22567.26318359375 
[2025-02-19 13:32:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:1.6998062133789062 norm:0.10400320589542389 max memory_allocated 22567.26318359375 
[2025-02-19 13:33:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:1.7036101818084717 norm:0.10268440842628479 max memory_allocated 22567.26318359375 
[2025-02-19 13:33:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:1.693004846572876 norm:0.09895429015159607 max memory_allocated 22567.26318359375 
[2025-02-19 13:34:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:1.6996724605560303 norm:0.10712442547082901 max memory_allocated 22567.26318359375 
[2025-02-19 13:34:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-19 13:34:14 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 13:34:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:3.927689552307129 norm:0.327191025018692 max memory_allocated 22567.43505859375 
[2025-02-19 13:35:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:3.4434256553649902 norm:0.2377694696187973 max memory_allocated 22567.43505859375 
[2025-02-19 13:35:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:3.0843605995178223 norm:0.16440041363239288 max memory_allocated 22567.43505859375 
[2025-02-19 13:36:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:2.9788639545440674 norm:0.16280993819236755 max memory_allocated 22567.43505859375 
[2025-02-19 13:36:48 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:2.9309260845184326 norm:0.16104787588119507 max memory_allocated 22567.43505859375 
[2025-02-19 13:37:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:2.8989195823669434 norm:0.15528114140033722 max memory_allocated 22567.43505859375 
[2025-02-19 13:37:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:2.870286703109741 norm:0.14597834646701813 max memory_allocated 22567.43505859375 
[2025-02-19 13:38:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:2.833845376968384 norm:0.13326899707317352 max memory_allocated 22567.43505859375 
[2025-02-19 13:38:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:2.802011013031006 norm:0.12261146306991577 max memory_allocated 22567.43505859375 
[2025-02-19 13:39:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:2.778883934020996 norm:0.11613941937685013 max memory_allocated 22567.43505859375 
[2025-02-19 13:39:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:2.7603204250335693 norm:0.10812210291624069 max memory_allocated 22567.43505859375 
[2025-02-19 13:40:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:2.745929718017578 norm:0.10180246084928513 max memory_allocated 22567.43505859375 
[2025-02-19 13:40:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:2.7299463748931885 norm:0.10109783709049225 max memory_allocated 22567.43505859375 
[2025-02-19 13:41:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:2.7203803062438965 norm:0.10016823559999466 max memory_allocated 22567.43505859375 
[2025-02-19 13:41:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:2.710380792617798 norm:0.09945135563611984 max memory_allocated 22567.43505859375 
[2025-02-19 13:42:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:2.699779510498047 norm:0.09602423757314682 max memory_allocated 22567.43505859375 
[2025-02-19 13:42:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:2.68973970413208 norm:0.09348403662443161 max memory_allocated 22567.43505859375 
[2025-02-19 13:43:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:2.6804747581481934 norm:0.0909910574555397 max memory_allocated 22567.43505859375 
[2025-02-19 13:43:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:2.6735403537750244 norm:0.09066419303417206 max memory_allocated 22567.43505859375 
[2025-02-19 13:44:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:2.6660304069519043 norm:0.08971841633319855 max memory_allocated 22567.43505859375 
[2025-02-19 13:44:38 root] (main_calibration.py 365): INFO 20013.100596427917
[2025-02-19 13:45:26 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-19 13:46:30 root] (main_calibration.py 158): INFO wikitext2 : 8.808492660522461
[2025-02-19 13:46:30 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-19 13:48:07 root] (main_calibration.py 158): INFO c4 : 12.157540321350098
[2025-02-19 15:26:49 root] (main_calibration.py 169): INFO {'wikitext2': 8.808492660522461, 'c4': 12.157540321350098, 'results': {'arc_easy': {'acc': 0.5509259259259259, 'acc_stderr': 0.010206428316323363, 'acc_norm': 0.4393939393939394, 'acc_norm_stderr': 0.010184134315437668}, 'piqa': {'acc': 0.6920565832426551, 'acc_stderr': 0.01077089236746368, 'acc_norm': 0.6871599564744287, 'acc_norm_stderr': 0.010817714425701095}, 'winogrande': {'acc': 0.5714285714285714, 'acc_stderr': 0.013908353814606693}, 'hellaswag': {'acc': 0.44592710615415254, 'acc_stderr': 0.004960516570284905, 'acc_norm': 0.5920135431189006, 'acc_norm_stderr': 0.004904561795919018}, 'arc_challenge': {'acc': 0.2696245733788396, 'acc_stderr': 0.012968040686869154, 'acc_norm': 0.3319112627986348, 'acc_norm_stderr': 0.013760988200880541}, 'boolq': {'acc': 0.6302752293577981, 'acc_stderr': 0.008443002801337144}}, 'versions': {'arc_easy': 0, 'piqa': 0, 'winogrande': 0, 'hellaswag': 0, 'arc_challenge': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
