[2025-02-19 23:28:20 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-13b-hf-w4a4', save_dir='./log-calibration-compensation-lwc/quant/llama-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-19 23:32:54 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-19 23:32:54 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-19 23:32:55 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-19 23:32:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-19 23:33:00 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 23:33:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.10205390304327011 norm:0.06527844816446304 max memory_allocated 29268.02001953125 
[2025-02-19 23:34:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.05535279959440231 norm:0.02681726962327957 max memory_allocated 29268.02001953125 
[2025-02-19 23:35:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.04366803914308548 norm:0.017306668683886528 max memory_allocated 29268.02001953125 
[2025-02-19 23:36:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.039161670953035355 norm:0.015515518374741077 max memory_allocated 29268.02001953125 
[2025-02-19 23:37:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.03696703165769577 norm:0.013387969695031643 max memory_allocated 29268.02001953125 
[2025-02-19 23:37:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.035384126007556915 norm:0.01234502624720335 max memory_allocated 29268.02001953125 
[2025-02-19 23:38:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.03436865285038948 norm:0.011319002136588097 max memory_allocated 29268.02001953125 
[2025-02-19 23:39:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.03367835655808449 norm:0.009998105466365814 max memory_allocated 29268.02001953125 
[2025-02-19 23:40:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.03314495086669922 norm:0.009105172008275986 max memory_allocated 29268.02001953125 
[2025-02-19 23:41:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.03254038095474243 norm:0.008235041052103043 max memory_allocated 29268.02001953125 
[2025-02-19 23:41:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.03245782479643822 norm:0.007581972982734442 max memory_allocated 29268.02001953125 
[2025-02-19 23:42:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.032217856496572495 norm:0.006955277174711227 max memory_allocated 29268.02001953125 
[2025-02-19 23:43:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.03198258951306343 norm:0.006389599293470383 max memory_allocated 29268.02001953125 
[2025-02-19 23:44:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.03180200234055519 norm:0.0058177318423986435 max memory_allocated 29268.02001953125 
[2025-02-19 23:45:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.03172936290502548 norm:0.005377507768571377 max memory_allocated 29268.02001953125 
[2025-02-19 23:46:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.03160170838236809 norm:0.005133677739650011 max memory_allocated 29268.02001953125 
[2025-02-19 23:46:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.03168760612607002 norm:0.004962107166647911 max memory_allocated 29268.02001953125 
[2025-02-19 23:47:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.03166119009256363 norm:0.004687406122684479 max memory_allocated 29268.02001953125 
[2025-02-19 23:48:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.031694598495960236 norm:0.004475962836295366 max memory_allocated 29268.02001953125 
[2025-02-19 23:49:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.03160644322633743 norm:0.004326259717345238 max memory_allocated 29268.02001953125 
[2025-02-19 23:49:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-19 23:49:35 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-19 23:50:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.26761505007743835 norm:0.11430663615465164 max memory_allocated 29268.02001953125 
[2025-02-19 23:51:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.17197470366954803 norm:0.04691406339406967 max memory_allocated 29268.02001953125 
[2025-02-19 23:52:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.1246895119547844 norm:0.020923884585499763 max memory_allocated 29268.02001953125 
[2025-02-19 23:52:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.10593301802873611 norm:0.016500817611813545 max memory_allocated 29268.02001953125 
[2025-02-19 23:53:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.09656106680631638 norm:0.013991158455610275 max memory_allocated 29268.02001953125 
[2025-02-19 23:54:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.09120998531579971 norm:0.011953994631767273 max memory_allocated 29268.02001953125 
[2025-02-19 23:55:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.08759702742099762 norm:0.010551221668720245 max memory_allocated 29268.02001953125 
[2025-02-19 23:56:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0852440819144249 norm:0.009207716211676598 max memory_allocated 29268.02001953125 
[2025-02-19 23:56:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.08373009413480759 norm:0.00832348596304655 max memory_allocated 29268.02001953125 
[2025-02-19 23:57:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.08247315138578415 norm:0.007477389648556709 max memory_allocated 29268.02001953125 
[2025-02-19 23:58:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.08156080543994904 norm:0.006842724047601223 max memory_allocated 29268.02001953125 
[2025-02-19 23:59:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.08081042766571045 norm:0.006314151920378208 max memory_allocated 29268.02001953125 
[2025-02-20 00:00:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.08018248528242111 norm:0.005680823232978582 max memory_allocated 29268.02001953125 
[2025-02-20 00:01:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.07966475188732147 norm:0.005246972199529409 max memory_allocated 29268.02001953125 
[2025-02-20 00:01:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.07932420819997787 norm:0.005019348580390215 max memory_allocated 29268.02001953125 
[2025-02-20 00:02:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.07907173037528992 norm:0.005103331990540028 max memory_allocated 29268.02001953125 
[2025-02-20 00:03:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.07893187552690506 norm:0.005084139760583639 max memory_allocated 29268.02001953125 
[2025-02-20 00:04:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.07868459820747375 norm:0.005066840443760157 max memory_allocated 29268.02001953125 
[2025-02-20 00:05:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.07858221977949142 norm:0.004730550572276115 max memory_allocated 29268.02001953125 
[2025-02-20 00:05:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.07839637249708176 norm:0.004651103168725967 max memory_allocated 29268.02001953125 
[2025-02-20 00:06:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-20 00:06:15 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 00:07:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.30244648456573486 norm:0.05403566360473633 max memory_allocated 29268.02001953125 
[2025-02-20 00:07:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.23937220871448517 norm:0.03557470440864563 max memory_allocated 29268.02001953125 
[2025-02-20 00:08:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.20568828284740448 norm:0.028127791360020638 max memory_allocated 29268.02001953125 
[2025-02-20 00:09:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.1875944882631302 norm:0.02310148999094963 max memory_allocated 29268.02001953125 
[2025-02-20 00:10:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.1792381852865219 norm:0.020188957452774048 max memory_allocated 29268.02001953125 
[2025-02-20 00:11:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.17193682491779327 norm:0.01781540736556053 max memory_allocated 29268.02001953125 
[2025-02-20 00:11:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.16682946681976318 norm:0.01697465591132641 max memory_allocated 29268.02001953125 
[2025-02-20 00:12:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.1642831563949585 norm:0.016584061086177826 max memory_allocated 29268.02001953125 
[2025-02-20 00:13:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.15940183401107788 norm:0.016885753720998764 max memory_allocated 29268.02001953125 
[2025-02-20 00:14:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.15762756764888763 norm:0.017157215625047684 max memory_allocated 29268.02001953125 
[2025-02-20 00:15:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.15460358560085297 norm:0.01690220646560192 max memory_allocated 29268.02001953125 
[2025-02-20 00:16:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.15149298310279846 norm:0.015347648411989212 max memory_allocated 29268.02001953125 
[2025-02-20 00:16:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.1521599143743515 norm:0.015826286748051643 max memory_allocated 29268.02001953125 
[2025-02-20 00:17:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.14851145446300507 norm:0.015958484262228012 max memory_allocated 29268.02001953125 
[2025-02-20 00:18:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.14721016585826874 norm:0.015120547264814377 max memory_allocated 29268.02001953125 
[2025-02-20 00:19:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.14791922271251678 norm:0.014359557069838047 max memory_allocated 29268.02001953125 
[2025-02-20 00:20:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.14614105224609375 norm:0.014848990365862846 max memory_allocated 29268.02001953125 
[2025-02-20 00:20:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.1464197337627411 norm:0.014854056760668755 max memory_allocated 29268.02001953125 
[2025-02-20 00:21:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.14664527773857117 norm:0.015284793451428413 max memory_allocated 29268.02001953125 
[2025-02-20 00:22:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.14562976360321045 norm:0.014322829432785511 max memory_allocated 29268.02001953125 
[2025-02-20 00:22:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-20 00:23:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.24803906679153442 norm:0.028817541897296906 max memory_allocated 29268.43798828125 
[2025-02-20 00:24:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.22008652985095978 norm:0.013669364154338837 max memory_allocated 29268.43798828125 
[2025-02-20 00:25:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.1940380185842514 norm:0.006595858838409185 max memory_allocated 29268.43798828125 
[2025-02-20 00:26:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.18378791213035583 norm:0.004299106542021036 max memory_allocated 29268.43798828125 
[2025-02-20 00:27:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.1792352944612503 norm:0.0033901671413332224 max memory_allocated 29268.43798828125 
[2025-02-20 00:27:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.17676475644111633 norm:0.0029539139941334724 max memory_allocated 29268.43798828125 
[2025-02-20 00:28:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.17437539994716644 norm:0.0025639915838837624 max memory_allocated 29268.43798828125 
[2025-02-20 00:29:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.1724160611629486 norm:0.0021892013028264046 max memory_allocated 29268.43798828125 
[2025-02-20 00:30:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.17137150466442108 norm:0.00196471088565886 max memory_allocated 29268.43798828125 
[2025-02-20 00:31:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.17057955265045166 norm:0.00189639488235116 max memory_allocated 29268.43798828125 
[2025-02-20 00:31:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.16954301297664642 norm:0.001733316108584404 max memory_allocated 29268.43798828125 
[2025-02-20 00:32:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.16934418678283691 norm:0.0016832067631185055 max memory_allocated 29268.43798828125 
[2025-02-20 00:33:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.16943727433681488 norm:0.0017856401391327381 max memory_allocated 29268.43798828125 
[2025-02-20 00:34:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.16918961703777313 norm:0.0017596333054825664 max memory_allocated 29268.43798828125 
[2025-02-20 00:35:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.16930213570594788 norm:0.0016960162902250886 max memory_allocated 29268.43798828125 
[2025-02-20 00:35:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.16918346285820007 norm:0.0016300047282129526 max memory_allocated 29268.43798828125 
[2025-02-20 00:36:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.16860350966453552 norm:0.001566113205626607 max memory_allocated 29268.43798828125 
[2025-02-20 00:37:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.1683109700679779 norm:0.0015113973058760166 max memory_allocated 29268.43798828125 
[2025-02-20 00:38:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.16795271635055542 norm:0.0015155822038650513 max memory_allocated 29268.43798828125 
[2025-02-20 00:39:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.16809296607971191 norm:0.0015233810991048813 max memory_allocated 29268.43798828125 
[2025-02-20 00:39:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-20 00:40:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.2998506724834442 norm:0.04128115624189377 max memory_allocated 29268.62548828125 
[2025-02-20 00:41:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.2645477056503296 norm:0.012871543876826763 max memory_allocated 29268.62548828125 
[2025-02-20 00:41:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.23713502287864685 norm:0.006190015003085136 max memory_allocated 29268.62548828125 
[2025-02-20 00:42:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.22902309894561768 norm:0.004748024977743626 max memory_allocated 29268.62548828125 
[2025-02-20 00:43:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.2245498150587082 norm:0.003860191907733679 max memory_allocated 29268.62548828125 
[2025-02-20 00:44:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.22119590640068054 norm:0.0031758432742208242 max memory_allocated 29268.62548828125 
[2025-02-20 00:45:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.21878916025161743 norm:0.0027490085922181606 max memory_allocated 29268.62548828125 
[2025-02-20 00:46:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.2172044962644577 norm:0.0023458460345864296 max memory_allocated 29268.62548828125 
[2025-02-20 00:46:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.21632730960845947 norm:0.0022252846974879503 max memory_allocated 29268.62548828125 
[2025-02-20 00:47:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.2159673273563385 norm:0.0022895338479429483 max memory_allocated 29268.62548828125 
[2025-02-20 00:48:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.21496665477752686 norm:0.001996866427361965 max memory_allocated 29268.62548828125 
[2025-02-20 00:49:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.21513307094573975 norm:0.002078392542898655 max memory_allocated 29268.62548828125 
[2025-02-20 00:50:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.21542350947856903 norm:0.002161447424441576 max memory_allocated 29268.62548828125 
[2025-02-20 00:50:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.21563997864723206 norm:0.0022098813205957413 max memory_allocated 29268.62548828125 
[2025-02-20 00:51:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.21532988548278809 norm:0.002064129337668419 max memory_allocated 29268.62548828125 
[2025-02-20 00:52:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.21388942003250122 norm:0.0016594256740063429 max memory_allocated 29268.62548828125 
[2025-02-20 00:53:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.2130439579486847 norm:0.0015015254030004144 max memory_allocated 29268.62548828125 
[2025-02-20 00:54:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.21290281414985657 norm:0.0014720747712999582 max memory_allocated 29268.62548828125 
[2025-02-20 00:55:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.21282166242599487 norm:0.0014495881041511893 max memory_allocated 29268.62548828125 
[2025-02-20 00:55:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.2129785418510437 norm:0.0015397754032164812 max memory_allocated 29268.62548828125 
[2025-02-20 00:56:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-20 00:56:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.3575107455253601 norm:0.05807121843099594 max memory_allocated 29268.81298828125 
[2025-02-20 00:57:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.3092370331287384 norm:0.018033064901828766 max memory_allocated 29268.81298828125 
[2025-02-20 00:58:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.27206966280937195 norm:0.006257287226617336 max memory_allocated 29268.81298828125 
[2025-02-20 00:59:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.2611567974090576 norm:0.004425464663654566 max memory_allocated 29268.81298828125 
[2025-02-20 01:00:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.2557453513145447 norm:0.003554397728294134 max memory_allocated 29268.81298828125 
[2025-02-20 01:01:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.25224465131759644 norm:0.003053793916478753 max memory_allocated 29268.81298828125 
[2025-02-20 01:01:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.24962562322616577 norm:0.002681470476090908 max memory_allocated 29268.81298828125 
[2025-02-20 01:02:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.24808458983898163 norm:0.0026644570752978325 max memory_allocated 29268.81298828125 
[2025-02-20 01:03:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.24723035097122192 norm:0.0027238165494054556 max memory_allocated 29268.81298828125 
[2025-02-20 01:04:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.24666853249073029 norm:0.002691426081582904 max memory_allocated 29268.81298828125 
[2025-02-20 01:05:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.24591413140296936 norm:0.002341232029721141 max memory_allocated 29268.81298828125 
[2025-02-20 01:05:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.24495096504688263 norm:0.0021800147369503975 max memory_allocated 29268.81298828125 
[2025-02-20 01:06:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.24460609257221222 norm:0.0020411666482686996 max memory_allocated 29268.81298828125 
[2025-02-20 01:07:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.24423298239707947 norm:0.0019477455643936992 max memory_allocated 29268.81298828125 
[2025-02-20 01:08:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.24337229132652283 norm:0.0017233053222298622 max memory_allocated 29268.81298828125 
[2025-02-20 01:09:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.24291540682315826 norm:0.0016470245318487287 max memory_allocated 29268.81298828125 
[2025-02-20 01:10:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.2427975982427597 norm:0.0015970730455592275 max memory_allocated 29268.81298828125 
[2025-02-20 01:10:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.24215100705623627 norm:0.0014879504451528192 max memory_allocated 29268.81298828125 
[2025-02-20 01:11:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.2417834997177124 norm:0.0014017540961503983 max memory_allocated 29268.81298828125 
[2025-02-20 01:12:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.24170589447021484 norm:0.0013548079878091812 max memory_allocated 29268.81298828125 
[2025-02-20 01:12:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-20 01:13:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.42937207221984863 norm:0.015179409645497799 max memory_allocated 29269.00048828125 
[2025-02-20 01:14:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.39525720477104187 norm:0.008345828391611576 max memory_allocated 29269.00048828125 
[2025-02-20 01:15:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.36320826411247253 norm:0.005284921266138554 max memory_allocated 29269.00048828125 
[2025-02-20 01:16:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.35107532143592834 norm:0.004067942034453154 max memory_allocated 29269.00048828125 
[2025-02-20 01:16:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.344641774892807 norm:0.0036041438579559326 max memory_allocated 29269.00048828125 
[2025-02-20 01:17:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.3389476239681244 norm:0.003438306972384453 max memory_allocated 29269.00048828125 
[2025-02-20 01:18:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.3345985412597656 norm:0.003269708016887307 max memory_allocated 29269.00048828125 
[2025-02-20 01:19:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.33512723445892334 norm:0.004176293965429068 max memory_allocated 29269.00048828125 
[2025-02-20 01:20:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.3302420377731323 norm:0.003482045605778694 max memory_allocated 29269.00048828125 
[2025-02-20 01:21:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.32812589406967163 norm:0.0031936538871377707 max memory_allocated 29269.00048828125 
[2025-02-20 01:21:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.3268716335296631 norm:0.0032420463394373655 max memory_allocated 29269.00048828125 
[2025-02-20 01:22:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.32564738392829895 norm:0.003100327216088772 max memory_allocated 29269.00048828125 
[2025-02-20 01:23:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.32499930262565613 norm:0.0032313033007085323 max memory_allocated 29269.00048828125 
[2025-02-20 01:24:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.323263555765152 norm:0.003121598158031702 max memory_allocated 29269.00048828125 
[2025-02-20 01:25:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.32350975275039673 norm:0.0031484272330999374 max memory_allocated 29269.00048828125 
[2025-02-20 01:25:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.3230563700199127 norm:0.0032683524768799543 max memory_allocated 29269.00048828125 
[2025-02-20 01:26:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.3228002190589905 norm:0.003218150231987238 max memory_allocated 29269.00048828125 
[2025-02-20 01:27:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.32258814573287964 norm:0.003320215502753854 max memory_allocated 29269.00048828125 
[2025-02-20 01:28:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.32226645946502686 norm:0.0034356475807726383 max memory_allocated 29269.00048828125 
[2025-02-20 01:29:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.3219980001449585 norm:0.0036110645160079002 max memory_allocated 29269.00048828125 
[2025-02-20 01:29:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-20 01:30:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.44895312190055847 norm:0.020558521151542664 max memory_allocated 29269.18798828125 
[2025-02-20 01:31:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.41732966899871826 norm:0.010209239087998867 max memory_allocated 29269.18798828125 
[2025-02-20 01:31:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.3757936656475067 norm:0.0040080915205180645 max memory_allocated 29269.18798828125 
[2025-02-20 01:32:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.3594013452529907 norm:0.002286894479766488 max memory_allocated 29269.18798828125 
[2025-02-20 01:33:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.3528493642807007 norm:0.0017231408273801208 max memory_allocated 29269.18798828125 
[2025-02-20 01:34:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.34847861528396606 norm:0.0014273753622546792 max memory_allocated 29269.18798828125 
[2025-02-20 01:35:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.3452688753604889 norm:0.0012422353029251099 max memory_allocated 29269.18798828125 
[2025-02-20 01:36:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.34302327036857605 norm:0.001149881398305297 max memory_allocated 29269.18798828125 
[2025-02-20 01:36:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.34145238995552063 norm:0.0010985600529238582 max memory_allocated 29269.18798828125 
[2025-02-20 01:37:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.3401346802711487 norm:0.0010499487398192286 max memory_allocated 29269.18798828125 
[2025-02-20 01:38:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.3392283320426941 norm:0.0010125458939000964 max memory_allocated 29269.18798828125 
[2025-02-20 01:39:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.338579922914505 norm:0.0009998875902965665 max memory_allocated 29269.18798828125 
[2025-02-20 01:40:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.33824852108955383 norm:0.001014521112665534 max memory_allocated 29269.18798828125 
[2025-02-20 01:40:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.3377607464790344 norm:0.001019775401800871 max memory_allocated 29269.18798828125 
[2025-02-20 01:41:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.3373039960861206 norm:0.0010178625816479325 max memory_allocated 29269.18798828125 
[2025-02-20 01:42:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.3370359539985657 norm:0.0010071521392092109 max memory_allocated 29269.18798828125 
[2025-02-20 01:43:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.3367610275745392 norm:0.0009717149659991264 max memory_allocated 29269.18798828125 
[2025-02-20 01:44:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.3366457223892212 norm:0.0009642642689868808 max memory_allocated 29269.18798828125 
[2025-02-20 01:45:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.3364931643009186 norm:0.0009683467214927077 max memory_allocated 29269.18798828125 
[2025-02-20 01:45:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.33629754185676575 norm:0.0009678318165242672 max memory_allocated 29269.18798828125 
[2025-02-20 01:46:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-20 01:46:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.4707965850830078 norm:0.018993327394127846 max memory_allocated 29269.37548828125 
[2025-02-20 01:47:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.43664127588272095 norm:0.009980019181966782 max memory_allocated 29269.37548828125 
[2025-02-20 01:48:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.3963247835636139 norm:0.004220366943627596 max memory_allocated 29269.37548828125 
[2025-02-20 01:49:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.37912285327911377 norm:0.0022311522625386715 max memory_allocated 29269.37548828125 
[2025-02-20 01:50:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.3727523684501648 norm:0.0017370766727253795 max memory_allocated 29269.37548828125 
[2025-02-20 01:51:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.36862027645111084 norm:0.0014231395907700062 max memory_allocated 29269.37548828125 
[2025-02-20 01:51:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.36551475524902344 norm:0.001179942162707448 max memory_allocated 29269.37548828125 
[2025-02-20 01:52:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.36317357420921326 norm:0.001073804683983326 max memory_allocated 29269.37548828125 
[2025-02-20 01:53:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.3615749776363373 norm:0.001032595755532384 max memory_allocated 29269.37548828125 
[2025-02-20 01:54:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.3605188727378845 norm:0.0010128351859748363 max memory_allocated 29269.37548828125 
[2025-02-20 01:55:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.3595820665359497 norm:0.0009778945241123438 max memory_allocated 29269.37548828125 
[2025-02-20 01:55:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.35890719294548035 norm:0.000968986249063164 max memory_allocated 29269.37548828125 
[2025-02-20 01:56:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.35838010907173157 norm:0.0009310535970143974 max memory_allocated 29269.37548828125 
[2025-02-20 01:57:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.35800227522850037 norm:0.0009179130429401994 max memory_allocated 29269.37548828125 
[2025-02-20 01:58:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.35773566365242004 norm:0.0009168874239549041 max memory_allocated 29269.37548828125 
[2025-02-20 01:59:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.35749852657318115 norm:0.0009236605255864561 max memory_allocated 29269.37548828125 
[2025-02-20 02:00:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.3572424054145813 norm:0.0009183375514112413 max memory_allocated 29269.37548828125 
[2025-02-20 02:00:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.3571315407752991 norm:0.0009237047634087503 max memory_allocated 29269.37548828125 
[2025-02-20 02:01:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.3570232689380646 norm:0.0009197192848660052 max memory_allocated 29269.37548828125 
[2025-02-20 02:02:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.35685622692108154 norm:0.0009219766361638904 max memory_allocated 29269.37548828125 
[2025-02-20 02:02:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-20 02:03:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.5022712349891663 norm:0.01887224242091179 max memory_allocated 29269.56298828125 
[2025-02-20 02:04:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.46171844005584717 norm:0.009817436337471008 max memory_allocated 29269.56298828125 
[2025-02-20 02:05:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.41186490654945374 norm:0.003461610060185194 max memory_allocated 29269.56298828125 
[2025-02-20 02:06:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.39637911319732666 norm:0.0018369245808571577 max memory_allocated 29269.56298828125 
[2025-02-20 02:06:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.39090821146965027 norm:0.0013998019276186824 max memory_allocated 29269.56298828125 
[2025-02-20 02:07:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.38723093271255493 norm:0.001206303364597261 max memory_allocated 29269.56298828125 
[2025-02-20 02:08:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.3845178186893463 norm:0.0010711062932386994 max memory_allocated 29269.56298828125 
[2025-02-20 02:09:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.3825257122516632 norm:0.0009979960741475224 max memory_allocated 29269.56298828125 
[2025-02-20 02:10:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.38111239671707153 norm:0.000968027045018971 max memory_allocated 29269.56298828125 
[2025-02-20 02:10:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.3800681531429291 norm:0.0009476930717937648 max memory_allocated 29269.56298828125 
[2025-02-20 02:11:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.379199743270874 norm:0.0009244393440894783 max memory_allocated 29269.56298828125 
[2025-02-20 02:12:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.3785025477409363 norm:0.0009169029071927071 max memory_allocated 29269.56298828125 
[2025-02-20 02:13:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.3779773712158203 norm:0.0008979822741821408 max memory_allocated 29269.56298828125 
[2025-02-20 02:14:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.37760689854621887 norm:0.0008817124180495739 max memory_allocated 29269.56298828125 
[2025-02-20 02:15:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.3773269057273865 norm:0.0008706957451067865 max memory_allocated 29269.56298828125 
[2025-02-20 02:15:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.37701714038848877 norm:0.0008678919402882457 max memory_allocated 29269.56298828125 
[2025-02-20 02:16:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.3768126964569092 norm:0.0008563185692764819 max memory_allocated 29269.56298828125 
[2025-02-20 02:17:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.3765377402305603 norm:0.0008448968292213976 max memory_allocated 29269.56298828125 
[2025-02-20 02:18:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.376470685005188 norm:0.0008311119745485485 max memory_allocated 29269.56298828125 
[2025-02-20 02:19:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.37640416622161865 norm:0.000836893857922405 max memory_allocated 29269.56298828125 
[2025-02-20 02:19:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-20 02:20:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.5021861791610718 norm:0.028938405215740204 max memory_allocated 29269.75048828125 
[2025-02-20 02:21:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.47272852063179016 norm:0.01267944648861885 max memory_allocated 29269.75048828125 
[2025-02-20 02:21:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.43392592668533325 norm:0.004471133463084698 max memory_allocated 29269.75048828125 
[2025-02-20 02:22:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.4170893132686615 norm:0.002111610723659396 max memory_allocated 29269.75048828125 
[2025-02-20 02:23:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.41107332706451416 norm:0.0015647717518731952 max memory_allocated 29269.75048828125 
[2025-02-20 02:24:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.40731292963027954 norm:0.0013364595361053944 max memory_allocated 29269.75048828125 
[2025-02-20 02:25:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.4046284854412079 norm:0.0012133853742852807 max memory_allocated 29269.75048828125 
[2025-02-20 02:25:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.4026176333427429 norm:0.0011027875589206815 max memory_allocated 29269.75048828125 
[2025-02-20 02:26:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.4010899066925049 norm:0.001051206374540925 max memory_allocated 29269.75048828125 
[2025-02-20 02:27:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.3997889459133148 norm:0.0009554776479490101 max memory_allocated 29269.75048828125 
[2025-02-20 02:28:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.3988570272922516 norm:0.0009211181895807385 max memory_allocated 29269.75048828125 
[2025-02-20 02:29:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.398243248462677 norm:0.0008903065463528037 max memory_allocated 29269.75048828125 
[2025-02-20 02:30:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.3977912664413452 norm:0.0008678389713168144 max memory_allocated 29269.75048828125 
[2025-02-20 02:30:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.3974446654319763 norm:0.0008652518736198545 max memory_allocated 29269.75048828125 
[2025-02-20 02:31:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.3969976305961609 norm:0.000837220810353756 max memory_allocated 29269.75048828125 
[2025-02-20 02:32:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.3967559039592743 norm:0.0008183239260688424 max memory_allocated 29269.75048828125 
[2025-02-20 02:33:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.3965429663658142 norm:0.0008069564355537295 max memory_allocated 29269.75048828125 
[2025-02-20 02:34:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.39646753668785095 norm:0.0008054858772084117 max memory_allocated 29269.75048828125 
[2025-02-20 02:34:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.39623555541038513 norm:0.0007900302298367023 max memory_allocated 29269.75048828125 
[2025-02-20 02:35:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.39611658453941345 norm:0.0007941554067656398 max memory_allocated 29269.75048828125 
[2025-02-20 02:36:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-20 02:36:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.5357296466827393 norm:0.021764224395155907 max memory_allocated 29269.93798828125 
[2025-02-20 02:37:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.5010756850242615 norm:0.009964138269424438 max memory_allocated 29269.93798828125 
[2025-02-20 02:38:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.4566103518009186 norm:0.0038519357331097126 max memory_allocated 29269.93798828125 
[2025-02-20 02:39:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.4389496147632599 norm:0.0017724940553307533 max memory_allocated 29269.93798828125 
[2025-02-20 02:40:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.4331505000591278 norm:0.001313803717494011 max memory_allocated 29269.93798828125 
[2025-02-20 02:40:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.4294430613517761 norm:0.001114305341616273 max memory_allocated 29269.93798828125 
[2025-02-20 02:41:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.42673707008361816 norm:0.001003751764073968 max memory_allocated 29269.93798828125 
[2025-02-20 02:42:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.42472052574157715 norm:0.0009461671579629183 max memory_allocated 29269.93798828125 
[2025-02-20 02:43:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.42321449518203735 norm:0.0009040205040946603 max memory_allocated 29269.93798828125 
[2025-02-20 02:44:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.4221607446670532 norm:0.0008903429261408746 max memory_allocated 29269.93798828125 
[2025-02-20 02:45:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.42138004302978516 norm:0.0008649232913739979 max memory_allocated 29269.93798828125 
[2025-02-20 02:45:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.42065346240997314 norm:0.0008481943514198065 max memory_allocated 29269.93798828125 
[2025-02-20 02:46:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.42011165618896484 norm:0.0008358951890841126 max memory_allocated 29269.93798828125 
[2025-02-20 02:47:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.4196084141731262 norm:0.0008076392114162445 max memory_allocated 29269.93798828125 
[2025-02-20 02:48:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.41929250955581665 norm:0.0008020687964744866 max memory_allocated 29269.93798828125 
[2025-02-20 02:49:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.41902878880500793 norm:0.0007905589300207794 max memory_allocated 29269.93798828125 
[2025-02-20 02:49:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.4189390540122986 norm:0.0007860710029490292 max memory_allocated 29269.93798828125 
[2025-02-20 02:50:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.41878199577331543 norm:0.0007816141005605459 max memory_allocated 29269.93798828125 
[2025-02-20 02:51:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.41871121525764465 norm:0.0007803440093994141 max memory_allocated 29269.93798828125 
[2025-02-20 02:52:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.41864871978759766 norm:0.0007850429392419755 max memory_allocated 29269.93798828125 
[2025-02-20 02:52:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-20 02:53:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.5475963950157166 norm:0.024372728541493416 max memory_allocated 29270.12548828125 
[2025-02-20 02:54:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.5159999132156372 norm:0.009687981568276882 max memory_allocated 29270.12548828125 
[2025-02-20 02:55:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.4804273247718811 norm:0.004118659999221563 max memory_allocated 29270.12548828125 
[2025-02-20 02:55:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.46335023641586304 norm:0.0019473027205094695 max memory_allocated 29270.12548828125 
[2025-02-20 02:56:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.4567471742630005 norm:0.0013754786923527718 max memory_allocated 29270.12548828125 
[2025-02-20 02:57:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.45266541838645935 norm:0.0011949443724006414 max memory_allocated 29270.12548828125 
[2025-02-20 02:58:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.44971397519111633 norm:0.0010836271103471518 max memory_allocated 29270.12548828125 
[2025-02-20 02:59:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.44756871461868286 norm:0.0010234229266643524 max memory_allocated 29270.12548828125 
[2025-02-20 03:00:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.44580429792404175 norm:0.0009816645178943872 max memory_allocated 29270.12548828125 
[2025-02-20 03:00:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.4445200562477112 norm:0.0009608016698621213 max memory_allocated 29270.12548828125 
[2025-02-20 03:01:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.44345688819885254 norm:0.0009391541825607419 max memory_allocated 29270.12548828125 
[2025-02-20 03:02:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.4425901174545288 norm:0.0009318448137491941 max memory_allocated 29270.12548828125 
[2025-02-20 03:03:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.44189712405204773 norm:0.0009131044498644769 max memory_allocated 29270.12548828125 
[2025-02-20 03:04:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.4413754343986511 norm:0.0009110441897064447 max memory_allocated 29270.12548828125 
[2025-02-20 03:04:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.440957635641098 norm:0.0008904459537006915 max memory_allocated 29270.12548828125 
[2025-02-20 03:05:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.440574586391449 norm:0.0008778571500442922 max memory_allocated 29270.12548828125 
[2025-02-20 03:06:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.44032472372055054 norm:0.0008678027661517262 max memory_allocated 29270.12548828125 
[2025-02-20 03:07:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.44003769755363464 norm:0.0008583846502006054 max memory_allocated 29270.12548828125 
[2025-02-20 03:08:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.43980199098587036 norm:0.0008615266415290534 max memory_allocated 29270.12548828125 
[2025-02-20 03:09:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.4396214485168457 norm:0.0008618170395493507 max memory_allocated 29270.12548828125 
[2025-02-20 03:09:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-20 03:10:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.5503565073013306 norm:0.008945358917117119 max memory_allocated 29270.31298828125 
[2025-02-20 03:11:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.5283252596855164 norm:0.0057565015740692616 max memory_allocated 29270.31298828125 
[2025-02-20 03:11:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.4983300268650055 norm:0.002925494220107794 max memory_allocated 29270.31298828125 
[2025-02-20 03:12:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.48395463824272156 norm:0.001635521650314331 max memory_allocated 29270.31298828125 
[2025-02-20 03:13:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.47781920433044434 norm:0.0012830374762415886 max memory_allocated 29270.31298828125 
[2025-02-20 03:14:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.47404563426971436 norm:0.0011564246378839016 max memory_allocated 29270.31298828125 
[2025-02-20 03:15:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.4713006019592285 norm:0.0011017372598871589 max memory_allocated 29270.31298828125 
[2025-02-20 03:15:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.4691894054412842 norm:0.0010668353643268347 max memory_allocated 29270.31298828125 
[2025-02-20 03:16:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.4677572250366211 norm:0.0010358772706240416 max memory_allocated 29270.31298828125 
[2025-02-20 03:17:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.4663926362991333 norm:0.0009935327107086778 max memory_allocated 29270.31298828125 
[2025-02-20 03:18:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.4653507471084595 norm:0.0009650276624597609 max memory_allocated 29270.31298828125 
[2025-02-20 03:19:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.4646455943584442 norm:0.0009597883326932788 max memory_allocated 29270.31298828125 
[2025-02-20 03:20:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.46408042311668396 norm:0.0009432907681912184 max memory_allocated 29270.31298828125 
[2025-02-20 03:20:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.4637078046798706 norm:0.0009416308021172881 max memory_allocated 29270.31298828125 
[2025-02-20 03:21:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.4631170630455017 norm:0.0009339700918644667 max memory_allocated 29270.31298828125 
[2025-02-20 03:22:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.462828129529953 norm:0.0009366382146254182 max memory_allocated 29270.31298828125 
[2025-02-20 03:23:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.46248888969421387 norm:0.0009309871238656342 max memory_allocated 29270.31298828125 
[2025-02-20 03:24:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.46219611167907715 norm:0.0009263730025850236 max memory_allocated 29270.31298828125 
[2025-02-20 03:24:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.4620290696620941 norm:0.0009232104639522731 max memory_allocated 29270.31298828125 
[2025-02-20 03:25:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.46176663041114807 norm:0.0009207471739500761 max memory_allocated 29270.31298828125 
[2025-02-20 03:25:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-20 03:26:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.6060971021652222 norm:0.03611947223544121 max memory_allocated 29270.50048828125 
[2025-02-20 03:27:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.573955774307251 norm:0.018475761637091637 max memory_allocated 29270.50048828125 
[2025-02-20 03:28:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.5348544716835022 norm:0.009219673462212086 max memory_allocated 29270.50048828125 
[2025-02-20 03:29:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.5151880383491516 norm:0.005219562910497189 max memory_allocated 29270.50048828125 
[2025-02-20 03:30:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.5060849785804749 norm:0.003282197518274188 max memory_allocated 29270.50048828125 
[2025-02-20 03:30:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.5005642771720886 norm:0.0020190891809761524 max memory_allocated 29270.50048828125 
[2025-02-20 03:31:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.49720463156700134 norm:0.0016950202407315373 max memory_allocated 29270.50048828125 
[2025-02-20 03:32:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.4945877194404602 norm:0.0015167603269219398 max memory_allocated 29270.50048828125 
[2025-02-20 03:33:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.49237582087516785 norm:0.0012405785964801908 max memory_allocated 29270.50048828125 
[2025-02-20 03:34:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.49077358841896057 norm:0.00116325996350497 max memory_allocated 29270.50048828125 
[2025-02-20 03:35:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.4894959032535553 norm:0.0011168423807248473 max memory_allocated 29270.50048828125 
[2025-02-20 03:35:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.4883888065814972 norm:0.0010952672455459833 max memory_allocated 29270.50048828125 
[2025-02-20 03:36:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.4876443147659302 norm:0.0010835243156179786 max memory_allocated 29270.50048828125 
[2025-02-20 03:37:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.4868184030056 norm:0.0010593555634841323 max memory_allocated 29270.50048828125 
[2025-02-20 03:38:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.48629236221313477 norm:0.0010419958271086216 max memory_allocated 29270.50048828125 
[2025-02-20 03:39:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.48579680919647217 norm:0.0010112383170053363 max memory_allocated 29270.50048828125 
[2025-02-20 03:39:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.485431045293808 norm:0.000996412942185998 max memory_allocated 29270.50048828125 
[2025-02-20 03:40:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.48519545793533325 norm:0.0009874359238892794 max memory_allocated 29270.50048828125 
[2025-02-20 03:41:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.4849770665168762 norm:0.000976841663941741 max memory_allocated 29270.50048828125 
[2025-02-20 03:42:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.4847154915332794 norm:0.000969438930042088 max memory_allocated 29270.50048828125 
[2025-02-20 03:42:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-20 03:43:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.6223286986351013 norm:0.06914941221475601 max memory_allocated 29270.68798828125 
[2025-02-20 03:44:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.6073017120361328 norm:0.04125652462244034 max memory_allocated 29270.68798828125 
[2025-02-20 03:45:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.5724051594734192 norm:0.01687716133892536 max memory_allocated 29270.68798828125 
[2025-02-20 03:45:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.5512488484382629 norm:0.0068726311437785625 max memory_allocated 29270.68798828125 
[2025-02-20 03:46:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.5432390570640564 norm:0.004634532146155834 max memory_allocated 29270.68798828125 
[2025-02-20 03:47:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.5379731059074402 norm:0.0031298391986638308 max memory_allocated 29270.68798828125 
[2025-02-20 03:48:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.5346873998641968 norm:0.0028171672020107508 max memory_allocated 29270.68798828125 
[2025-02-20 03:49:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.5322997570037842 norm:0.0026281606405973434 max memory_allocated 29270.68798828125 
[2025-02-20 03:50:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.5302518606185913 norm:0.002454042434692383 max memory_allocated 29270.68798828125 
[2025-02-20 03:50:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.5283509492874146 norm:0.0021435818634927273 max memory_allocated 29270.68798828125 
[2025-02-20 03:51:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.5265259742736816 norm:0.0014839143259450793 max memory_allocated 29270.68798828125 
[2025-02-20 03:52:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.5251746773719788 norm:0.001419985550455749 max memory_allocated 29270.68798828125 
[2025-02-20 03:53:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.5242109894752502 norm:0.001393125276081264 max memory_allocated 29270.68798828125 
[2025-02-20 03:54:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.5234088897705078 norm:0.0013718681875616312 max memory_allocated 29270.68798828125 
[2025-02-20 03:54:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.5228520631790161 norm:0.001344032702036202 max memory_allocated 29270.68798828125 
[2025-02-20 03:55:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.5223820209503174 norm:0.001316014677286148 max memory_allocated 29270.68798828125 
[2025-02-20 03:56:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.5218018889427185 norm:0.0012588109821081161 max memory_allocated 29270.68798828125 
[2025-02-20 03:57:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.5213913917541504 norm:0.0012412869837135077 max memory_allocated 29270.68798828125 
[2025-02-20 03:58:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.5209547877311707 norm:0.0012131124967709184 max memory_allocated 29270.68798828125 
[2025-02-20 03:59:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.5207799077033997 norm:0.0012023858726024628 max memory_allocated 29270.68798828125 
[2025-02-20 03:59:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-20 04:00:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.6364648938179016 norm:0.018156692385673523 max memory_allocated 29270.87548828125 
[2025-02-20 04:01:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.6119291186332703 norm:0.009977329522371292 max memory_allocated 29270.87548828125 
[2025-02-20 04:01:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.5846467018127441 norm:0.004741990473121405 max memory_allocated 29270.87548828125 
[2025-02-20 04:02:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.5720558166503906 norm:0.002526748925447464 max memory_allocated 29270.87548828125 
[2025-02-20 04:03:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.5670551061630249 norm:0.0017744675278663635 max memory_allocated 29270.87548828125 
[2025-02-20 04:04:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.5640429258346558 norm:0.0014801959041506052 max memory_allocated 29270.87548828125 
[2025-02-20 04:05:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.5617620348930359 norm:0.0013719168491661549 max memory_allocated 29270.87548828125 
[2025-02-20 04:05:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.559865415096283 norm:0.0013013603165745735 max memory_allocated 29270.87548828125 
[2025-02-20 04:06:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.5584781169891357 norm:0.0012439490528777242 max memory_allocated 29270.87548828125 
[2025-02-20 04:07:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.5573498010635376 norm:0.0011944384314119816 max memory_allocated 29270.87548828125 
[2025-02-20 04:08:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.5561363101005554 norm:0.0011088005267083645 max memory_allocated 29270.87548828125 
[2025-02-20 04:09:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.5552283525466919 norm:0.0010497220791876316 max memory_allocated 29270.87548828125 
[2025-02-20 04:10:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.554474413394928 norm:0.001025253557600081 max memory_allocated 29270.87548828125 
[2025-02-20 04:10:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.5538485646247864 norm:0.0009998345049098134 max memory_allocated 29270.87548828125 
[2025-02-20 04:11:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.5532854795455933 norm:0.0009705859120003879 max memory_allocated 29270.87548828125 
[2025-02-20 04:12:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.552797794342041 norm:0.0009551821276545525 max memory_allocated 29270.87548828125 
[2025-02-20 04:13:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.5524648427963257 norm:0.000940090452786535 max memory_allocated 29270.87548828125 
[2025-02-20 04:14:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.552172064781189 norm:0.0009211881551891565 max memory_allocated 29270.87548828125 
[2025-02-20 04:14:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.5519332885742188 norm:0.0009160405024886131 max memory_allocated 29270.87548828125 
[2025-02-20 04:15:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.5517013669013977 norm:0.0009179317275993526 max memory_allocated 29270.87548828125 
[2025-02-20 04:15:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-20 04:16:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.6802140474319458 norm:0.014406519941985607 max memory_allocated 29271.06298828125 
[2025-02-20 04:17:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.6596783399581909 norm:0.008794454857707024 max memory_allocated 29271.06298828125 
[2025-02-20 04:18:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.6281174421310425 norm:0.00428537605330348 max memory_allocated 29271.06298828125 
[2025-02-20 04:19:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.6153470873832703 norm:0.002565403701737523 max memory_allocated 29271.06298828125 
[2025-02-20 04:20:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.610289990901947 norm:0.001964607974514365 max memory_allocated 29271.06298828125 
[2025-02-20 04:20:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.6068863272666931 norm:0.0016282742144539952 max memory_allocated 29271.06298828125 
[2025-02-20 04:21:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.6043523550033569 norm:0.0014397712657228112 max memory_allocated 29271.06298828125 
[2025-02-20 04:22:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.6021859645843506 norm:0.0013151813764125109 max memory_allocated 29271.06298828125 
[2025-02-20 04:23:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.6005489230155945 norm:0.0012489247601479292 max memory_allocated 29271.06298828125 
[2025-02-20 04:24:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.5992966890335083 norm:0.0011959206312894821 max memory_allocated 29271.06298828125 
[2025-02-20 04:25:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.5983574390411377 norm:0.0011620459845289588 max memory_allocated 29271.06298828125 
[2025-02-20 04:25:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.5974528789520264 norm:0.001129589625634253 max memory_allocated 29271.06298828125 
[2025-02-20 04:26:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.5966823697090149 norm:0.0010997527278959751 max memory_allocated 29271.06298828125 
[2025-02-20 04:27:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.5959683656692505 norm:0.0010560748632997274 max memory_allocated 29271.06298828125 
[2025-02-20 04:28:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.5954601168632507 norm:0.001039423979818821 max memory_allocated 29271.06298828125 
[2025-02-20 04:29:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.5949554443359375 norm:0.0010082981316372752 max memory_allocated 29271.06298828125 
[2025-02-20 04:29:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.5946135520935059 norm:0.0009943143231794238 max memory_allocated 29271.06298828125 
[2025-02-20 04:30:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.5941506624221802 norm:0.000977715477347374 max memory_allocated 29271.06298828125 
[2025-02-20 04:31:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.5938904285430908 norm:0.0009827938629314303 max memory_allocated 29271.06298828125 
[2025-02-20 04:32:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.5936776399612427 norm:0.0009756876970641315 max memory_allocated 29271.06298828125 
[2025-02-20 04:32:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-20 04:33:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.7686751484870911 norm:0.06207380071282387 max memory_allocated 29271.25048828125 
[2025-02-20 04:34:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.7383534908294678 norm:0.03383273631334305 max memory_allocated 29271.25048828125 
[2025-02-20 04:35:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.702629804611206 norm:0.01632966846227646 max memory_allocated 29271.25048828125 
[2025-02-20 04:35:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.6845220327377319 norm:0.008749104104936123 max memory_allocated 29271.25048828125 
[2025-02-20 04:36:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.6769917607307434 norm:0.005800655111670494 max memory_allocated 29271.25048828125 
[2025-02-20 04:37:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.672315239906311 norm:0.004298247396945953 max memory_allocated 29271.25048828125 
[2025-02-20 04:38:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.6690041422843933 norm:0.003698362037539482 max memory_allocated 29271.25048828125 
[2025-02-20 04:39:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.6662458181381226 norm:0.0031397745478898287 max memory_allocated 29271.25048828125 
[2025-02-20 04:40:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.6638312935829163 norm:0.0025350013747811317 max memory_allocated 29271.25048828125 
[2025-02-20 04:40:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.6622992157936096 norm:0.0025442196056246758 max memory_allocated 29271.25048828125 
[2025-02-20 04:41:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.6610801219940186 norm:0.0023874114267528057 max memory_allocated 29271.25048828125 
[2025-02-20 04:42:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.6600561738014221 norm:0.002262067748233676 max memory_allocated 29271.25048828125 
[2025-02-20 04:43:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.6593557596206665 norm:0.0021538673900067806 max memory_allocated 29271.25048828125 
[2025-02-20 04:44:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.6588745713233948 norm:0.0020842908415943384 max memory_allocated 29271.25048828125 
[2025-02-20 04:44:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.6582737565040588 norm:0.0019598519429564476 max memory_allocated 29271.25048828125 
[2025-02-20 04:45:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.6575352549552917 norm:0.0018731753807514906 max memory_allocated 29271.25048828125 
[2025-02-20 04:46:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.6569364070892334 norm:0.0017912710318341851 max memory_allocated 29271.25048828125 
[2025-02-20 04:47:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.6564023494720459 norm:0.0017994486261159182 max memory_allocated 29271.25048828125 
[2025-02-20 04:48:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.6554134488105774 norm:0.0013424723874777555 max memory_allocated 29271.25048828125 
[2025-02-20 04:49:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.6547077894210815 norm:0.0011950613697990775 max memory_allocated 29271.25048828125 
[2025-02-20 04:49:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-20 04:50:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.8015389442443848 norm:0.018241656944155693 max memory_allocated 29271.43798828125 
[2025-02-20 04:50:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.7825004458427429 norm:0.010198398493230343 max memory_allocated 29271.43798828125 
[2025-02-20 04:51:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.7586692571640015 norm:0.005475251004099846 max memory_allocated 29271.43798828125 
[2025-02-20 04:52:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.7467280626296997 norm:0.0030996729619801044 max memory_allocated 29271.43798828125 
[2025-02-20 04:53:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.741863489151001 norm:0.0025464266072958708 max memory_allocated 29271.43798828125 
[2025-02-20 04:54:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.7387176752090454 norm:0.0022026386577636003 max memory_allocated 29271.43798828125 
[2025-02-20 04:55:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.7355465888977051 norm:0.0014482978731393814 max memory_allocated 29271.43798828125 
[2025-02-20 04:55:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.7334941625595093 norm:0.0014119134284555912 max memory_allocated 29271.43798828125 
[2025-02-20 04:56:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.7320481538772583 norm:0.0013817177386954427 max memory_allocated 29271.43798828125 
[2025-02-20 04:57:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.7307978272438049 norm:0.001347799552604556 max memory_allocated 29271.43798828125 
[2025-02-20 04:58:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.7297378182411194 norm:0.0012864969903603196 max memory_allocated 29271.43798828125 
[2025-02-20 04:59:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.728952944278717 norm:0.0012591725680977106 max memory_allocated 29271.43798828125 
[2025-02-20 04:59:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.728356122970581 norm:0.0012316169450059533 max memory_allocated 29271.43798828125 
[2025-02-20 05:00:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.7277448177337646 norm:0.0012179022887721658 max memory_allocated 29271.43798828125 
[2025-02-20 05:01:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.7272893190383911 norm:0.0011921761324629188 max memory_allocated 29271.43798828125 
[2025-02-20 05:02:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.7268551588058472 norm:0.0011769197881221771 max memory_allocated 29271.43798828125 
[2025-02-20 05:03:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.7263256311416626 norm:0.0011703709606081247 max memory_allocated 29271.43798828125 
[2025-02-20 05:04:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.7259492874145508 norm:0.0011628817301243544 max memory_allocated 29271.43798828125 
[2025-02-20 05:04:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.7256267666816711 norm:0.0011598453857004642 max memory_allocated 29271.43798828125 
[2025-02-20 05:05:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.7254068851470947 norm:0.0011504009598866105 max memory_allocated 29271.43798828125 
[2025-02-20 05:05:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-20 05:06:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.8899354934692383 norm:0.03279275447130203 max memory_allocated 29271.62548828125 
[2025-02-20 05:07:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.8683682680130005 norm:0.019775118678808212 max memory_allocated 29271.62548828125 
[2025-02-20 05:08:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.8398642539978027 norm:0.011217496357858181 max memory_allocated 29271.62548828125 
[2025-02-20 05:09:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.8246889710426331 norm:0.005784026347100735 max memory_allocated 29271.62548828125 
[2025-02-20 05:10:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.8188875913619995 norm:0.00460091745480895 max memory_allocated 29271.62548828125 
[2025-02-20 05:10:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.815229058265686 norm:0.0037635357584804296 max memory_allocated 29271.62548828125 
[2025-02-20 05:11:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.8125413060188293 norm:0.003256598487496376 max memory_allocated 29271.62548828125 
[2025-02-20 05:12:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.8104374408721924 norm:0.002993782050907612 max memory_allocated 29271.62548828125 
[2025-02-20 05:13:20 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.8086509108543396 norm:0.002727976767346263 max memory_allocated 29271.62548828125 
[2025-02-20 05:14:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.8069911003112793 norm:0.002374873496592045 max memory_allocated 29271.62548828125 
[2025-02-20 05:14:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.805597186088562 norm:0.0021310029551386833 max memory_allocated 29271.62548828125 
[2025-02-20 05:15:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.8046677708625793 norm:0.0020103624556213617 max memory_allocated 29271.62548828125 
[2025-02-20 05:16:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.8037267327308655 norm:0.0019476128509268165 max memory_allocated 29271.62548828125 
[2025-02-20 05:17:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.8023369312286377 norm:0.0013185428688302636 max memory_allocated 29271.62548828125 
[2025-02-20 05:18:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.8017614483833313 norm:0.0012442133156582713 max memory_allocated 29271.62548828125 
[2025-02-20 05:19:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.8014064431190491 norm:0.001231634640134871 max memory_allocated 29271.62548828125 
[2025-02-20 05:19:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.80084627866745 norm:0.0011936704395338893 max memory_allocated 29271.62548828125 
[2025-02-20 05:20:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.8004252910614014 norm:0.0011874365154653788 max memory_allocated 29271.62548828125 
[2025-02-20 05:21:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.8001865148544312 norm:0.0011683558113873005 max memory_allocated 29271.62548828125 
[2025-02-20 05:22:20 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.7999406456947327 norm:0.0011515135411173105 max memory_allocated 29271.62548828125 
[2025-02-20 05:22:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-20 05:23:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.9763161540031433 norm:0.011279702186584473 max memory_allocated 29271.81298828125 
