[2025-02-20 07:15:00 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/llama-13b-hf-w4a4', save_dir='./log-calibration-compensation-lwc/quant/llama-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-20 07:15:14 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-20 07:15:14 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-20 07:15:14 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-20 07:15:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-20 07:15:19 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:16:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.10205390304327011 norm:0.06527844816446304 max memory_allocated 29268.02001953125 
[2025-02-20 07:16:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.05535279959440231 norm:0.02681726962327957 max memory_allocated 29268.02001953125 
[2025-02-20 07:17:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.04366803914308548 norm:0.017306668683886528 max memory_allocated 29268.02001953125 
[2025-02-20 07:18:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.039161670953035355 norm:0.015515518374741077 max memory_allocated 29268.02001953125 
[2025-02-20 07:19:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.03696703165769577 norm:0.013387969695031643 max memory_allocated 29268.02001953125 
[2025-02-20 07:20:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.035384126007556915 norm:0.01234502624720335 max memory_allocated 29268.02001953125 
[2025-02-20 07:20:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.03436865285038948 norm:0.011319002136588097 max memory_allocated 29268.02001953125 
[2025-02-20 07:21:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.03367835655808449 norm:0.009998105466365814 max memory_allocated 29268.02001953125 
[2025-02-20 07:22:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.03314495086669922 norm:0.009105172008275986 max memory_allocated 29268.02001953125 
[2025-02-20 07:23:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.03254038095474243 norm:0.008235041052103043 max memory_allocated 29268.02001953125 
[2025-02-20 07:24:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.03245782479643822 norm:0.007581972982734442 max memory_allocated 29268.02001953125 
[2025-02-20 07:25:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.032217856496572495 norm:0.006955277174711227 max memory_allocated 29268.02001953125 
[2025-02-20 07:25:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.03198258951306343 norm:0.006389599293470383 max memory_allocated 29268.02001953125 
[2025-02-20 07:26:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.03180200234055519 norm:0.0058177318423986435 max memory_allocated 29268.02001953125 
[2025-02-20 07:27:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.03172936290502548 norm:0.005377507768571377 max memory_allocated 29268.02001953125 
[2025-02-20 07:28:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.03160170838236809 norm:0.005133677739650011 max memory_allocated 29268.02001953125 
[2025-02-20 07:29:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.03168760612607002 norm:0.004962107166647911 max memory_allocated 29268.02001953125 
[2025-02-20 07:29:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.03166119009256363 norm:0.004687406122684479 max memory_allocated 29268.02001953125 
[2025-02-20 07:30:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.031694598495960236 norm:0.004475962836295366 max memory_allocated 29268.02001953125 
[2025-02-20 07:31:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.03160644322633743 norm:0.004326259717345238 max memory_allocated 29268.02001953125 
[2025-02-20 07:31:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-20 07:31:55 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:32:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.26761505007743835 norm:0.11430663615465164 max memory_allocated 29268.02001953125 
[2025-02-20 07:33:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.17197470366954803 norm:0.04691406339406967 max memory_allocated 29268.02001953125 
[2025-02-20 07:34:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.1246895119547844 norm:0.020923884585499763 max memory_allocated 29268.02001953125 
[2025-02-20 07:35:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.10593301802873611 norm:0.016500817611813545 max memory_allocated 29268.02001953125 
[2025-02-20 07:35:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.09656106680631638 norm:0.013991158455610275 max memory_allocated 29268.02001953125 
[2025-02-20 07:36:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.09120998531579971 norm:0.011953994631767273 max memory_allocated 29268.02001953125 
[2025-02-20 07:37:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.08759702742099762 norm:0.010551221668720245 max memory_allocated 29268.02001953125 
[2025-02-20 07:38:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0852440819144249 norm:0.009207716211676598 max memory_allocated 29268.02001953125 
[2025-02-20 07:39:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.08373009413480759 norm:0.00832348596304655 max memory_allocated 29268.02001953125 
[2025-02-20 07:40:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.08247315138578415 norm:0.007477389648556709 max memory_allocated 29268.02001953125 
[2025-02-20 07:40:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.08156080543994904 norm:0.006842724047601223 max memory_allocated 29268.02001953125 
[2025-02-20 07:41:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.08081042766571045 norm:0.006314151920378208 max memory_allocated 29268.02001953125 
[2025-02-20 07:42:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.08018248528242111 norm:0.005680823232978582 max memory_allocated 29268.02001953125 
[2025-02-20 07:43:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.07966475188732147 norm:0.005246972199529409 max memory_allocated 29268.02001953125 
[2025-02-20 07:44:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.07932420819997787 norm:0.005019348580390215 max memory_allocated 29268.02001953125 
[2025-02-20 07:45:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.07907173037528992 norm:0.005103331990540028 max memory_allocated 29268.02001953125 
[2025-02-20 07:45:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.07893187552690506 norm:0.005084139760583639 max memory_allocated 29268.02001953125 
[2025-02-20 07:46:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.07868459820747375 norm:0.005066840443760157 max memory_allocated 29268.02001953125 
[2025-02-20 07:47:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.07858221977949142 norm:0.004730550572276115 max memory_allocated 29268.02001953125 
[2025-02-20 07:48:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.07839637249708176 norm:0.004651103168725967 max memory_allocated 29268.02001953125 
[2025-02-20 07:48:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-20 07:48:34 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 07:49:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.30244648456573486 norm:0.05403566360473633 max memory_allocated 29268.02001953125 
[2025-02-20 07:50:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.23937220871448517 norm:0.03557470440864563 max memory_allocated 29268.02001953125 
[2025-02-20 07:51:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.20568828284740448 norm:0.028127791360020638 max memory_allocated 29268.02001953125 
[2025-02-20 07:51:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.1875944882631302 norm:0.02310148999094963 max memory_allocated 29268.02001953125 
[2025-02-20 07:52:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.1792381852865219 norm:0.020188957452774048 max memory_allocated 29268.02001953125 
[2025-02-20 07:53:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.17193682491779327 norm:0.01781540736556053 max memory_allocated 29268.02001953125 
[2025-02-20 07:54:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.16682946681976318 norm:0.01697465591132641 max memory_allocated 29268.02001953125 
[2025-02-20 07:55:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.1642831563949585 norm:0.016584061086177826 max memory_allocated 29268.02001953125 
[2025-02-20 07:55:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.15940183401107788 norm:0.016885753720998764 max memory_allocated 29268.02001953125 
[2025-02-20 07:56:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.15762756764888763 norm:0.017157215625047684 max memory_allocated 29268.02001953125 
[2025-02-20 07:57:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.15460358560085297 norm:0.01690220646560192 max memory_allocated 29268.02001953125 
[2025-02-20 07:58:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.15149298310279846 norm:0.015347648411989212 max memory_allocated 29268.02001953125 
[2025-02-20 07:59:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.1521599143743515 norm:0.015826286748051643 max memory_allocated 29268.02001953125 
[2025-02-20 08:00:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.14851145446300507 norm:0.015958484262228012 max memory_allocated 29268.02001953125 
[2025-02-20 08:00:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.14721016585826874 norm:0.015120547264814377 max memory_allocated 29268.02001953125 
[2025-02-20 08:01:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.14791922271251678 norm:0.014359557069838047 max memory_allocated 29268.02001953125 
[2025-02-20 08:02:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.14614105224609375 norm:0.014848990365862846 max memory_allocated 29268.02001953125 
[2025-02-20 08:03:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.1464197337627411 norm:0.014854056760668755 max memory_allocated 29268.02001953125 
[2025-02-20 08:04:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.14664527773857117 norm:0.015284793451428413 max memory_allocated 29268.02001953125 
[2025-02-20 08:04:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.14562976360321045 norm:0.014322829432785511 max memory_allocated 29268.02001953125 
[2025-02-20 08:05:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-20 08:06:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.24803906679153442 norm:0.028817541897296906 max memory_allocated 29268.43798828125 
[2025-02-20 08:06:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.22008652985095978 norm:0.013669364154338837 max memory_allocated 29268.43798828125 
[2025-02-20 08:07:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.1940380185842514 norm:0.006595858838409185 max memory_allocated 29268.43798828125 
[2025-02-20 08:08:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.18378791213035583 norm:0.004299106542021036 max memory_allocated 29268.43798828125 
[2025-02-20 08:09:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.1792352944612503 norm:0.0033901671413332224 max memory_allocated 29268.43798828125 
[2025-02-20 08:10:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.17676475644111633 norm:0.0029539139941334724 max memory_allocated 29268.43798828125 
[2025-02-20 08:11:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.17437539994716644 norm:0.0025639915838837624 max memory_allocated 29268.43798828125 
[2025-02-20 08:11:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.1724160611629486 norm:0.0021892013028264046 max memory_allocated 29268.43798828125 
[2025-02-20 08:12:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.17137150466442108 norm:0.00196471088565886 max memory_allocated 29268.43798828125 
[2025-02-20 08:13:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.17057955265045166 norm:0.00189639488235116 max memory_allocated 29268.43798828125 
[2025-02-20 08:14:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.16954301297664642 norm:0.001733316108584404 max memory_allocated 29268.43798828125 
[2025-02-20 08:15:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.16934418678283691 norm:0.0016832067631185055 max memory_allocated 29268.43798828125 
[2025-02-20 08:15:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.16943727433681488 norm:0.0017856401391327381 max memory_allocated 29268.43798828125 
[2025-02-20 08:16:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.16918961703777313 norm:0.0017596333054825664 max memory_allocated 29268.43798828125 
[2025-02-20 08:17:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.16930213570594788 norm:0.0016960162902250886 max memory_allocated 29268.43798828125 
[2025-02-20 08:18:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.16918346285820007 norm:0.0016300047282129526 max memory_allocated 29268.43798828125 
[2025-02-20 08:19:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.16860350966453552 norm:0.001566113205626607 max memory_allocated 29268.43798828125 
[2025-02-20 08:20:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.1683109700679779 norm:0.0015113973058760166 max memory_allocated 29268.43798828125 
[2025-02-20 08:20:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.16795271635055542 norm:0.0015155822038650513 max memory_allocated 29268.43798828125 
[2025-02-20 08:21:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.16809296607971191 norm:0.0015233810991048813 max memory_allocated 29268.43798828125 
[2025-02-20 08:21:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-20 08:22:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.2998506724834442 norm:0.04128115624189377 max memory_allocated 29268.62548828125 
[2025-02-20 08:23:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.2645477056503296 norm:0.012871543876826763 max memory_allocated 29268.62548828125 
[2025-02-20 08:24:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.23713502287864685 norm:0.006190015003085136 max memory_allocated 29268.62548828125 
[2025-02-20 08:25:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.22902309894561768 norm:0.004748024977743626 max memory_allocated 29268.62548828125 
[2025-02-20 08:26:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.2245498150587082 norm:0.003860191907733679 max memory_allocated 29268.62548828125 
[2025-02-20 08:26:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.22119590640068054 norm:0.0031758432742208242 max memory_allocated 29268.62548828125 
[2025-02-20 08:27:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.21878916025161743 norm:0.0027490085922181606 max memory_allocated 29268.62548828125 
[2025-02-20 08:28:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.2172044962644577 norm:0.0023458460345864296 max memory_allocated 29268.62548828125 
[2025-02-20 08:29:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.21632730960845947 norm:0.0022252846974879503 max memory_allocated 29268.62548828125 
[2025-02-20 08:30:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.2159673273563385 norm:0.0022895338479429483 max memory_allocated 29268.62548828125 
[2025-02-20 08:30:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.21496665477752686 norm:0.001996866427361965 max memory_allocated 29268.62548828125 
[2025-02-20 08:31:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.21513307094573975 norm:0.002078392542898655 max memory_allocated 29268.62548828125 
[2025-02-20 08:32:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.21542350947856903 norm:0.002161447424441576 max memory_allocated 29268.62548828125 
[2025-02-20 08:33:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.21563997864723206 norm:0.0022098813205957413 max memory_allocated 29268.62548828125 
[2025-02-20 08:34:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.21532988548278809 norm:0.002064129337668419 max memory_allocated 29268.62548828125 
[2025-02-20 08:35:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.21388942003250122 norm:0.0016594256740063429 max memory_allocated 29268.62548828125 
[2025-02-20 08:35:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.2130439579486847 norm:0.0015015254030004144 max memory_allocated 29268.62548828125 
[2025-02-20 08:36:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.21290281414985657 norm:0.0014720747712999582 max memory_allocated 29268.62548828125 
[2025-02-20 08:37:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.21282166242599487 norm:0.0014495881041511893 max memory_allocated 29268.62548828125 
[2025-02-20 08:38:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.2129785418510437 norm:0.0015397754032164812 max memory_allocated 29268.62548828125 
[2025-02-20 08:38:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-20 08:39:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.3575107455253601 norm:0.05807121843099594 max memory_allocated 29268.81298828125 
[2025-02-20 08:40:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.3092370331287384 norm:0.018033064901828766 max memory_allocated 29268.81298828125 
[2025-02-20 08:41:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.27206966280937195 norm:0.006257287226617336 max memory_allocated 29268.81298828125 
[2025-02-20 08:41:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.2611567974090576 norm:0.004425464663654566 max memory_allocated 29268.81298828125 
[2025-02-20 08:42:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.2557453513145447 norm:0.003554397728294134 max memory_allocated 29268.81298828125 
[2025-02-20 08:43:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.25224465131759644 norm:0.003053793916478753 max memory_allocated 29268.81298828125 
[2025-02-20 08:44:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.24962562322616577 norm:0.002681470476090908 max memory_allocated 29268.81298828125 
[2025-02-20 08:45:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.24808458983898163 norm:0.0026644570752978325 max memory_allocated 29268.81298828125 
[2025-02-20 08:45:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.24723035097122192 norm:0.0027238165494054556 max memory_allocated 29268.81298828125 
[2025-02-20 08:46:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.24666853249073029 norm:0.002691426081582904 max memory_allocated 29268.81298828125 
[2025-02-20 08:47:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.24591413140296936 norm:0.002341232029721141 max memory_allocated 29268.81298828125 
[2025-02-20 08:48:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.24495096504688263 norm:0.0021800147369503975 max memory_allocated 29268.81298828125 
[2025-02-20 08:49:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.24460609257221222 norm:0.0020411666482686996 max memory_allocated 29268.81298828125 
[2025-02-20 08:50:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.24423298239707947 norm:0.0019477455643936992 max memory_allocated 29268.81298828125 
[2025-02-20 08:50:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.24337229132652283 norm:0.0017233053222298622 max memory_allocated 29268.81298828125 
[2025-02-20 08:51:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.24291540682315826 norm:0.0016470245318487287 max memory_allocated 29268.81298828125 
[2025-02-20 08:52:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.2427975982427597 norm:0.0015970730455592275 max memory_allocated 29268.81298828125 
[2025-02-20 08:53:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.24215100705623627 norm:0.0014879504451528192 max memory_allocated 29268.81298828125 
[2025-02-20 08:54:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.2417834997177124 norm:0.0014017540961503983 max memory_allocated 29268.81298828125 
[2025-02-20 08:54:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.24170589447021484 norm:0.0013548079878091812 max memory_allocated 29268.81298828125 
[2025-02-20 08:55:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-20 08:56:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.42937207221984863 norm:0.015179409645497799 max memory_allocated 29269.00048828125 
[2025-02-20 08:56:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.39525720477104187 norm:0.008345828391611576 max memory_allocated 29269.00048828125 
[2025-02-20 08:57:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.36320826411247253 norm:0.005284921266138554 max memory_allocated 29269.00048828125 
[2025-02-20 08:58:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.35107532143592834 norm:0.004067942034453154 max memory_allocated 29269.00048828125 
[2025-02-20 08:59:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.344641774892807 norm:0.0036041438579559326 max memory_allocated 29269.00048828125 
[2025-02-20 09:00:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.3389476239681244 norm:0.003438306972384453 max memory_allocated 29269.00048828125 
[2025-02-20 09:01:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.3345985412597656 norm:0.003269708016887307 max memory_allocated 29269.00048828125 
[2025-02-20 09:01:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.33512723445892334 norm:0.004176293965429068 max memory_allocated 29269.00048828125 
[2025-02-20 09:02:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.3302420377731323 norm:0.003482045605778694 max memory_allocated 29269.00048828125 
[2025-02-20 09:03:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.32812589406967163 norm:0.0031936538871377707 max memory_allocated 29269.00048828125 
[2025-02-20 09:04:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.3268716335296631 norm:0.0032420463394373655 max memory_allocated 29269.00048828125 
[2025-02-20 09:05:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.32564738392829895 norm:0.003100327216088772 max memory_allocated 29269.00048828125 
[2025-02-20 09:05:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.32499930262565613 norm:0.0032313033007085323 max memory_allocated 29269.00048828125 
[2025-02-20 09:06:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.323263555765152 norm:0.003121598158031702 max memory_allocated 29269.00048828125 
[2025-02-20 09:07:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.32350975275039673 norm:0.0031484272330999374 max memory_allocated 29269.00048828125 
[2025-02-20 09:08:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.3230563700199127 norm:0.0032683524768799543 max memory_allocated 29269.00048828125 
[2025-02-20 09:09:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.3228002190589905 norm:0.003218150231987238 max memory_allocated 29269.00048828125 
[2025-02-20 09:10:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.32258814573287964 norm:0.003320215502753854 max memory_allocated 29269.00048828125 
[2025-02-20 09:10:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.32226645946502686 norm:0.0034356475807726383 max memory_allocated 29269.00048828125 
[2025-02-20 09:11:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.3219980001449585 norm:0.0036110645160079002 max memory_allocated 29269.00048828125 
[2025-02-20 09:11:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-20 09:12:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.44895312190055847 norm:0.020558521151542664 max memory_allocated 29269.18798828125 
[2025-02-20 09:13:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.41732966899871826 norm:0.010209239087998867 max memory_allocated 29269.18798828125 
[2025-02-20 09:14:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.3757936656475067 norm:0.0040080915205180645 max memory_allocated 29269.18798828125 
[2025-02-20 09:15:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.3594013452529907 norm:0.002286894479766488 max memory_allocated 29269.18798828125 
[2025-02-20 09:16:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.3528493642807007 norm:0.0017231408273801208 max memory_allocated 29269.18798828125 
[2025-02-20 09:16:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.34847861528396606 norm:0.0014273753622546792 max memory_allocated 29269.18798828125 
[2025-02-20 09:17:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.3452688753604889 norm:0.0012422353029251099 max memory_allocated 29269.18798828125 
[2025-02-20 09:18:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.34302327036857605 norm:0.001149881398305297 max memory_allocated 29269.18798828125 
[2025-02-20 09:19:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.34145238995552063 norm:0.0010985600529238582 max memory_allocated 29269.18798828125 
[2025-02-20 09:20:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.3401346802711487 norm:0.0010499487398192286 max memory_allocated 29269.18798828125 
[2025-02-20 09:20:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.3392283320426941 norm:0.0010125458939000964 max memory_allocated 29269.18798828125 
[2025-02-20 09:21:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.338579922914505 norm:0.0009998875902965665 max memory_allocated 29269.18798828125 
[2025-02-20 09:22:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.33824852108955383 norm:0.001014521112665534 max memory_allocated 29269.18798828125 
[2025-02-20 09:23:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.3377607464790344 norm:0.001019775401800871 max memory_allocated 29269.18798828125 
[2025-02-20 09:24:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.3373039960861206 norm:0.0010178625816479325 max memory_allocated 29269.18798828125 
[2025-02-20 09:25:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.3370359539985657 norm:0.0010071521392092109 max memory_allocated 29269.18798828125 
[2025-02-20 09:25:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.3367610275745392 norm:0.0009717149659991264 max memory_allocated 29269.18798828125 
[2025-02-20 09:26:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.3366457223892212 norm:0.0009642642689868808 max memory_allocated 29269.18798828125 
[2025-02-20 09:27:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.3364931643009186 norm:0.0009683467214927077 max memory_allocated 29269.18798828125 
[2025-02-20 09:28:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.33629754185676575 norm:0.0009678318165242672 max memory_allocated 29269.18798828125 
[2025-02-20 09:28:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-20 09:29:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.4707965850830078 norm:0.018993327394127846 max memory_allocated 29269.37548828125 
[2025-02-20 09:30:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.43664127588272095 norm:0.009980019181966782 max memory_allocated 29269.37548828125 
[2025-02-20 09:31:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.3963247835636139 norm:0.004220366943627596 max memory_allocated 29269.37548828125 
[2025-02-20 09:31:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.37912285327911377 norm:0.0022311522625386715 max memory_allocated 29269.37548828125 
[2025-02-20 09:32:44 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.3727523684501648 norm:0.0017370766727253795 max memory_allocated 29269.37548828125 
[2025-02-20 09:33:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.36862027645111084 norm:0.0014231395907700062 max memory_allocated 29269.37548828125 
[2025-02-20 09:34:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.36551475524902344 norm:0.001179942162707448 max memory_allocated 29269.37548828125 
[2025-02-20 09:35:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.36317357420921326 norm:0.001073804683983326 max memory_allocated 29269.37548828125 
[2025-02-20 09:36:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.3615749776363373 norm:0.001032595755532384 max memory_allocated 29269.37548828125 
[2025-02-20 09:36:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.3605188727378845 norm:0.0010128351859748363 max memory_allocated 29269.37548828125 
[2025-02-20 09:37:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.3595820665359497 norm:0.0009778945241123438 max memory_allocated 29269.37548828125 
[2025-02-20 09:38:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.35890719294548035 norm:0.000968986249063164 max memory_allocated 29269.37548828125 
[2025-02-20 09:39:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.35838010907173157 norm:0.0009310535970143974 max memory_allocated 29269.37548828125 
[2025-02-20 09:40:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.35800227522850037 norm:0.0009179130429401994 max memory_allocated 29269.37548828125 
[2025-02-20 09:40:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.35773566365242004 norm:0.0009168874239549041 max memory_allocated 29269.37548828125 
[2025-02-20 09:41:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.35749852657318115 norm:0.0009236605255864561 max memory_allocated 29269.37548828125 
[2025-02-20 09:42:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.3572424054145813 norm:0.0009183375514112413 max memory_allocated 29269.37548828125 
[2025-02-20 09:43:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.3571315407752991 norm:0.0009237047634087503 max memory_allocated 29269.37548828125 
[2025-02-20 09:44:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.3570232689380646 norm:0.0009197192848660052 max memory_allocated 29269.37548828125 
[2025-02-20 09:45:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.35685622692108154 norm:0.0009219766361638904 max memory_allocated 29269.37548828125 
[2025-02-20 09:45:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-20 09:46:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.5022712349891663 norm:0.01887224242091179 max memory_allocated 29269.56298828125 
[2025-02-20 09:46:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.46171844005584717 norm:0.009817436337471008 max memory_allocated 29269.56298828125 
[2025-02-20 09:47:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.41186490654945374 norm:0.003461610060185194 max memory_allocated 29269.56298828125 
[2025-02-20 09:48:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.39637911319732666 norm:0.0018369245808571577 max memory_allocated 29269.56298828125 
[2025-02-20 09:49:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.39090821146965027 norm:0.0013998019276186824 max memory_allocated 29269.56298828125 
[2025-02-20 09:50:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.38723093271255493 norm:0.001206303364597261 max memory_allocated 29269.56298828125 
[2025-02-20 09:51:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.3845178186893463 norm:0.0010711062932386994 max memory_allocated 29269.56298828125 
[2025-02-20 09:51:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.3825257122516632 norm:0.0009979960741475224 max memory_allocated 29269.56298828125 
[2025-02-20 09:52:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.38111239671707153 norm:0.000968027045018971 max memory_allocated 29269.56298828125 
[2025-02-20 09:53:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.3800681531429291 norm:0.0009476930717937648 max memory_allocated 29269.56298828125 
[2025-02-20 09:54:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.379199743270874 norm:0.0009244393440894783 max memory_allocated 29269.56298828125 
[2025-02-20 09:55:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.3785025477409363 norm:0.0009169029071927071 max memory_allocated 29269.56298828125 
[2025-02-20 09:55:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.3779773712158203 norm:0.0008979822741821408 max memory_allocated 29269.56298828125 
[2025-02-20 09:56:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.37760689854621887 norm:0.0008817124180495739 max memory_allocated 29269.56298828125 
[2025-02-20 09:57:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.3773269057273865 norm:0.0008706957451067865 max memory_allocated 29269.56298828125 
[2025-02-20 09:58:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.37701714038848877 norm:0.0008678919402882457 max memory_allocated 29269.56298828125 
[2025-02-20 09:59:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.3768126964569092 norm:0.0008563185692764819 max memory_allocated 29269.56298828125 
[2025-02-20 10:00:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.3765377402305603 norm:0.0008448968292213976 max memory_allocated 29269.56298828125 
[2025-02-20 10:00:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.376470685005188 norm:0.0008311119745485485 max memory_allocated 29269.56298828125 
[2025-02-20 10:01:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.37640416622161865 norm:0.000836893857922405 max memory_allocated 29269.56298828125 
[2025-02-20 10:01:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-20 10:02:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.5021861791610718 norm:0.028938405215740204 max memory_allocated 29269.75048828125 
[2025-02-20 10:03:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.47272852063179016 norm:0.01267944648861885 max memory_allocated 29269.75048828125 
[2025-02-20 10:04:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.43392592668533325 norm:0.004471133463084698 max memory_allocated 29269.75048828125 
[2025-02-20 10:05:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.4170893132686615 norm:0.002111610723659396 max memory_allocated 29269.75048828125 
[2025-02-20 10:06:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.41107332706451416 norm:0.0015647717518731952 max memory_allocated 29269.75048828125 
[2025-02-20 10:06:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.40731292963027954 norm:0.0013364595361053944 max memory_allocated 29269.75048828125 
[2025-02-20 10:07:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.4046284854412079 norm:0.0012133853742852807 max memory_allocated 29269.75048828125 
[2025-02-20 10:08:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.4026176333427429 norm:0.0011027875589206815 max memory_allocated 29269.75048828125 
[2025-02-20 10:09:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.4010899066925049 norm:0.001051206374540925 max memory_allocated 29269.75048828125 
[2025-02-20 10:10:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.3997889459133148 norm:0.0009554776479490101 max memory_allocated 29269.75048828125 
[2025-02-20 10:11:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.3988570272922516 norm:0.0009211181895807385 max memory_allocated 29269.75048828125 
[2025-02-20 10:11:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.398243248462677 norm:0.0008903065463528037 max memory_allocated 29269.75048828125 
[2025-02-20 10:12:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.3977912664413452 norm:0.0008678389713168144 max memory_allocated 29269.75048828125 
[2025-02-20 10:13:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.3974446654319763 norm:0.0008652518736198545 max memory_allocated 29269.75048828125 
[2025-02-20 10:14:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.3969976305961609 norm:0.000837220810353756 max memory_allocated 29269.75048828125 
[2025-02-20 10:15:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.3967559039592743 norm:0.0008183239260688424 max memory_allocated 29269.75048828125 
[2025-02-20 10:15:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.3965429663658142 norm:0.0008069564355537295 max memory_allocated 29269.75048828125 
[2025-02-20 10:16:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.39646753668785095 norm:0.0008054858772084117 max memory_allocated 29269.75048828125 
[2025-02-20 10:17:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.39623555541038513 norm:0.0007900302298367023 max memory_allocated 29269.75048828125 
[2025-02-20 10:18:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.39611658453941345 norm:0.0007941554067656398 max memory_allocated 29269.75048828125 
[2025-02-20 10:18:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-20 10:19:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.5357296466827393 norm:0.021764224395155907 max memory_allocated 29269.93798828125 
[2025-02-20 10:20:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.5010756850242615 norm:0.009964138269424438 max memory_allocated 29269.93798828125 
[2025-02-20 10:21:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.4566103518009186 norm:0.0038519357331097126 max memory_allocated 29269.93798828125 
[2025-02-20 10:21:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.4389496147632599 norm:0.0017724940553307533 max memory_allocated 29269.93798828125 
[2025-02-20 10:22:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.4331505000591278 norm:0.001313803717494011 max memory_allocated 29269.93798828125 
[2025-02-20 10:23:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.4294430613517761 norm:0.001114305341616273 max memory_allocated 29269.93798828125 
[2025-02-20 10:24:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.42673707008361816 norm:0.001003751764073968 max memory_allocated 29269.93798828125 
[2025-02-20 10:25:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.42472052574157715 norm:0.0009461671579629183 max memory_allocated 29269.93798828125 
[2025-02-20 10:26:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.42321449518203735 norm:0.0009040205040946603 max memory_allocated 29269.93798828125 
[2025-02-20 10:26:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.4221607446670532 norm:0.0008903429261408746 max memory_allocated 29269.93798828125 
[2025-02-20 10:27:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.42138004302978516 norm:0.0008649232913739979 max memory_allocated 29269.93798828125 
[2025-02-20 10:28:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.42065346240997314 norm:0.0008481943514198065 max memory_allocated 29269.93798828125 
[2025-02-20 10:29:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.42011165618896484 norm:0.0008358951890841126 max memory_allocated 29269.93798828125 
[2025-02-20 10:30:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.4196084141731262 norm:0.0008076392114162445 max memory_allocated 29269.93798828125 
[2025-02-20 10:30:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.41929250955581665 norm:0.0008020687964744866 max memory_allocated 29269.93798828125 
[2025-02-20 10:31:48 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.41902878880500793 norm:0.0007905589300207794 max memory_allocated 29269.93798828125 
[2025-02-20 10:32:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.4189390540122986 norm:0.0007860710029490292 max memory_allocated 29269.93798828125 
[2025-02-20 10:33:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.41878199577331543 norm:0.0007816141005605459 max memory_allocated 29269.93798828125 
[2025-02-20 10:34:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.41871121525764465 norm:0.0007803440093994141 max memory_allocated 29269.93798828125 
[2025-02-20 10:35:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.41864871978759766 norm:0.0007850429392419755 max memory_allocated 29269.93798828125 
[2025-02-20 10:35:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-20 10:36:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.5475963950157166 norm:0.024372728541493416 max memory_allocated 29270.12548828125 
[2025-02-20 10:37:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.5159999132156372 norm:0.009687981568276882 max memory_allocated 29270.12548828125 
[2025-02-20 10:37:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.4804273247718811 norm:0.004118659999221563 max memory_allocated 29270.12548828125 
[2025-02-20 10:38:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.46335023641586304 norm:0.0019473027205094695 max memory_allocated 29270.12548828125 
[2025-02-20 10:39:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.4567471742630005 norm:0.0013754786923527718 max memory_allocated 29270.12548828125 
[2025-02-20 10:40:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.45266541838645935 norm:0.0011949443724006414 max memory_allocated 29270.12548828125 
[2025-02-20 10:41:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.44971397519111633 norm:0.0010836271103471518 max memory_allocated 29270.12548828125 
[2025-02-20 10:41:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.44756871461868286 norm:0.0010234229266643524 max memory_allocated 29270.12548828125 
[2025-02-20 10:42:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.44580429792404175 norm:0.0009816645178943872 max memory_allocated 29270.12548828125 
[2025-02-20 10:43:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.4445200562477112 norm:0.0009608016698621213 max memory_allocated 29270.12548828125 
[2025-02-20 10:44:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.44345688819885254 norm:0.0009391541825607419 max memory_allocated 29270.12548828125 
[2025-02-20 10:45:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.4425901174545288 norm:0.0009318448137491941 max memory_allocated 29270.12548828125 
[2025-02-20 10:46:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.44189712405204773 norm:0.0009131044498644769 max memory_allocated 29270.12548828125 
[2025-02-20 10:46:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.4413754343986511 norm:0.0009110441897064447 max memory_allocated 29270.12548828125 
[2025-02-20 10:47:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.440957635641098 norm:0.0008904459537006915 max memory_allocated 29270.12548828125 
[2025-02-20 10:48:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.440574586391449 norm:0.0008778571500442922 max memory_allocated 29270.12548828125 
[2025-02-20 10:49:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.44032472372055054 norm:0.0008678027661517262 max memory_allocated 29270.12548828125 
[2025-02-20 10:50:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.44003769755363464 norm:0.0008583846502006054 max memory_allocated 29270.12548828125 
[2025-02-20 10:50:58 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.43980199098587036 norm:0.0008615266415290534 max memory_allocated 29270.12548828125 
[2025-02-20 10:51:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.4396214485168457 norm:0.0008618170395493507 max memory_allocated 29270.12548828125 
[2025-02-20 10:52:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-20 10:52:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.5503565073013306 norm:0.008945358917117119 max memory_allocated 29270.31298828125 
[2025-02-20 10:53:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.5283252596855164 norm:0.0057565015740692616 max memory_allocated 29270.31298828125 
[2025-02-20 10:54:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.4983300268650055 norm:0.002925494220107794 max memory_allocated 29270.31298828125 
[2025-02-20 10:55:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.48395463824272156 norm:0.001635521650314331 max memory_allocated 29270.31298828125 
[2025-02-20 10:56:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.47781920433044434 norm:0.0012830374762415886 max memory_allocated 29270.31298828125 
[2025-02-20 10:57:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.47404563426971436 norm:0.0011564246378839016 max memory_allocated 29270.31298828125 
[2025-02-20 10:57:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.4713006019592285 norm:0.0011017372598871589 max memory_allocated 29270.31298828125 
[2025-02-20 10:58:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.4691894054412842 norm:0.0010668353643268347 max memory_allocated 29270.31298828125 
[2025-02-20 10:59:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.4677572250366211 norm:0.0010358772706240416 max memory_allocated 29270.31298828125 
[2025-02-20 11:00:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.4663926362991333 norm:0.0009935327107086778 max memory_allocated 29270.31298828125 
[2025-02-20 11:01:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.4653507471084595 norm:0.0009650276624597609 max memory_allocated 29270.31298828125 
[2025-02-20 11:01:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.4646455943584442 norm:0.0009597883326932788 max memory_allocated 29270.31298828125 
[2025-02-20 11:02:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.46408042311668396 norm:0.0009432907681912184 max memory_allocated 29270.31298828125 
[2025-02-20 11:03:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.4637078046798706 norm:0.0009416308021172881 max memory_allocated 29270.31298828125 
[2025-02-20 11:04:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.4631170630455017 norm:0.0009339700918644667 max memory_allocated 29270.31298828125 
[2025-02-20 11:05:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.462828129529953 norm:0.0009366382146254182 max memory_allocated 29270.31298828125 
[2025-02-20 11:06:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.46248888969421387 norm:0.0009309871238656342 max memory_allocated 29270.31298828125 
[2025-02-20 11:06:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.46219611167907715 norm:0.0009263730025850236 max memory_allocated 29270.31298828125 
[2025-02-20 11:07:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.4620290696620941 norm:0.0009232104639522731 max memory_allocated 29270.31298828125 
[2025-02-20 11:08:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.46176663041114807 norm:0.0009207471739500761 max memory_allocated 29270.31298828125 
[2025-02-20 11:08:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-20 11:09:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.6060971021652222 norm:0.03611947223544121 max memory_allocated 29270.50048828125 
[2025-02-20 11:10:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.573955774307251 norm:0.018475761637091637 max memory_allocated 29270.50048828125 
[2025-02-20 11:11:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.5348544716835022 norm:0.009219673462212086 max memory_allocated 29270.50048828125 
[2025-02-20 11:12:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.5151880383491516 norm:0.005219562910497189 max memory_allocated 29270.50048828125 
[2025-02-20 11:12:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.5060849785804749 norm:0.003282197518274188 max memory_allocated 29270.50048828125 
[2025-02-20 11:13:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.5005642771720886 norm:0.0020190891809761524 max memory_allocated 29270.50048828125 
[2025-02-20 11:14:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.49720463156700134 norm:0.0016950202407315373 max memory_allocated 29270.50048828125 
[2025-02-20 11:15:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.4945877194404602 norm:0.0015167603269219398 max memory_allocated 29270.50048828125 
[2025-02-20 11:16:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.49237582087516785 norm:0.0012405785964801908 max memory_allocated 29270.50048828125 
[2025-02-20 11:16:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.49077358841896057 norm:0.00116325996350497 max memory_allocated 29270.50048828125 
[2025-02-20 11:17:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.4894959032535553 norm:0.0011168423807248473 max memory_allocated 29270.50048828125 
[2025-02-20 11:18:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.4883888065814972 norm:0.0010952672455459833 max memory_allocated 29270.50048828125 
[2025-02-20 11:19:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.4876443147659302 norm:0.0010835243156179786 max memory_allocated 29270.50048828125 
[2025-02-20 11:20:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.4868184030056 norm:0.0010593555634841323 max memory_allocated 29270.50048828125 
[2025-02-20 11:21:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.48629236221313477 norm:0.0010419958271086216 max memory_allocated 29270.50048828125 
[2025-02-20 11:21:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.48579680919647217 norm:0.0010112383170053363 max memory_allocated 29270.50048828125 
[2025-02-20 11:22:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.485431045293808 norm:0.000996412942185998 max memory_allocated 29270.50048828125 
[2025-02-20 11:23:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.48519545793533325 norm:0.0009874359238892794 max memory_allocated 29270.50048828125 
[2025-02-20 11:24:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.4849770665168762 norm:0.000976841663941741 max memory_allocated 29270.50048828125 
[2025-02-20 11:25:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.4847154915332794 norm:0.000969438930042088 max memory_allocated 29270.50048828125 
[2025-02-20 11:25:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-20 11:26:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.6223286986351013 norm:0.06914941221475601 max memory_allocated 29270.68798828125 
[2025-02-20 11:27:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.6073017120361328 norm:0.04125652462244034 max memory_allocated 29270.68798828125 
[2025-02-20 11:27:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.5724051594734192 norm:0.01687716133892536 max memory_allocated 29270.68798828125 
[2025-02-20 11:28:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.5512488484382629 norm:0.0068726311437785625 max memory_allocated 29270.68798828125 
[2025-02-20 11:29:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.5432390570640564 norm:0.004634532146155834 max memory_allocated 29270.68798828125 
[2025-02-20 11:30:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.5379731059074402 norm:0.0031298391986638308 max memory_allocated 29270.68798828125 
[2025-02-20 11:31:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.5346873998641968 norm:0.0028171672020107508 max memory_allocated 29270.68798828125 
[2025-02-20 11:31:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.5322997570037842 norm:0.0026281606405973434 max memory_allocated 29270.68798828125 
[2025-02-20 11:32:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.5302518606185913 norm:0.002454042434692383 max memory_allocated 29270.68798828125 
[2025-02-20 11:33:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.5283509492874146 norm:0.0021435818634927273 max memory_allocated 29270.68798828125 
[2025-02-20 11:34:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.5265259742736816 norm:0.0014839143259450793 max memory_allocated 29270.68798828125 
[2025-02-20 11:35:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.5251746773719788 norm:0.001419985550455749 max memory_allocated 29270.68798828125 
[2025-02-20 11:36:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.5242109894752502 norm:0.001393125276081264 max memory_allocated 29270.68798828125 
[2025-02-20 11:36:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.5234088897705078 norm:0.0013718681875616312 max memory_allocated 29270.68798828125 
[2025-02-20 11:37:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.5228520631790161 norm:0.001344032702036202 max memory_allocated 29270.68798828125 
[2025-02-20 11:38:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.5223820209503174 norm:0.001316014677286148 max memory_allocated 29270.68798828125 
[2025-02-20 11:39:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.5218018889427185 norm:0.0012588109821081161 max memory_allocated 29270.68798828125 
[2025-02-20 11:40:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.5213913917541504 norm:0.0012412869837135077 max memory_allocated 29270.68798828125 
[2025-02-20 11:40:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.5209547877311707 norm:0.0012131124967709184 max memory_allocated 29270.68798828125 
[2025-02-20 11:41:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.5207799077033997 norm:0.0012023858726024628 max memory_allocated 29270.68798828125 
[2025-02-20 11:42:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-20 11:42:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.6364648938179016 norm:0.018156692385673523 max memory_allocated 29270.87548828125 
[2025-02-20 11:43:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.6119291186332703 norm:0.009977329522371292 max memory_allocated 29270.87548828125 
[2025-02-20 11:44:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.5846467018127441 norm:0.004741990473121405 max memory_allocated 29270.87548828125 
[2025-02-20 11:45:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.5720558166503906 norm:0.002526748925447464 max memory_allocated 29270.87548828125 
[2025-02-20 11:46:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.5670551061630249 norm:0.0017744675278663635 max memory_allocated 29270.87548828125 
[2025-02-20 11:47:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.5640429258346558 norm:0.0014801959041506052 max memory_allocated 29270.87548828125 
[2025-02-20 11:47:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.5617620348930359 norm:0.0013719168491661549 max memory_allocated 29270.87548828125 
[2025-02-20 11:48:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.559865415096283 norm:0.0013013603165745735 max memory_allocated 29270.87548828125 
[2025-02-20 11:49:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.5584781169891357 norm:0.0012439490528777242 max memory_allocated 29270.87548828125 
[2025-02-20 11:50:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.5573498010635376 norm:0.0011944384314119816 max memory_allocated 29270.87548828125 
[2025-02-20 11:51:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.5561363101005554 norm:0.0011088005267083645 max memory_allocated 29270.87548828125 
[2025-02-20 11:51:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.5552283525466919 norm:0.0010497220791876316 max memory_allocated 29270.87548828125 
[2025-02-20 11:52:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.554474413394928 norm:0.001025253557600081 max memory_allocated 29270.87548828125 
[2025-02-20 11:53:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.5538485646247864 norm:0.0009998345049098134 max memory_allocated 29270.87548828125 
[2025-02-20 11:54:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.5532854795455933 norm:0.0009705859120003879 max memory_allocated 29270.87548828125 
[2025-02-20 11:55:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.552797794342041 norm:0.0009551821276545525 max memory_allocated 29270.87548828125 
[2025-02-20 11:56:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.5524648427963257 norm:0.000940090452786535 max memory_allocated 29270.87548828125 
[2025-02-20 11:56:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.552172064781189 norm:0.0009211881551891565 max memory_allocated 29270.87548828125 
[2025-02-20 11:57:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.5519332885742188 norm:0.0009160405024886131 max memory_allocated 29270.87548828125 
[2025-02-20 11:58:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.5517013669013977 norm:0.0009179317275993526 max memory_allocated 29270.87548828125 
[2025-02-20 11:58:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-20 11:59:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.6802140474319458 norm:0.014406519941985607 max memory_allocated 29271.06298828125 
[2025-02-20 12:00:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.6596783399581909 norm:0.008794454857707024 max memory_allocated 29271.06298828125 
[2025-02-20 12:01:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.6281174421310425 norm:0.00428537605330348 max memory_allocated 29271.06298828125 
[2025-02-20 12:02:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.6153470873832703 norm:0.002565403701737523 max memory_allocated 29271.06298828125 
[2025-02-20 12:02:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.610289990901947 norm:0.001964607974514365 max memory_allocated 29271.06298828125 
[2025-02-20 12:03:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.6068863272666931 norm:0.0016282742144539952 max memory_allocated 29271.06298828125 
[2025-02-20 12:04:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.6043523550033569 norm:0.0014397712657228112 max memory_allocated 29271.06298828125 
[2025-02-20 12:05:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.6021859645843506 norm:0.0013151813764125109 max memory_allocated 29271.06298828125 
[2025-02-20 12:06:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.6005489230155945 norm:0.0012489247601479292 max memory_allocated 29271.06298828125 
[2025-02-20 12:06:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.5992966890335083 norm:0.0011959206312894821 max memory_allocated 29271.06298828125 
[2025-02-20 12:07:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.5983574390411377 norm:0.0011620459845289588 max memory_allocated 29271.06298828125 
[2025-02-20 12:08:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.5974528789520264 norm:0.001129589625634253 max memory_allocated 29271.06298828125 
[2025-02-20 12:09:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.5966823697090149 norm:0.0010997527278959751 max memory_allocated 29271.06298828125 
[2025-02-20 12:10:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.5959683656692505 norm:0.0010560748632997274 max memory_allocated 29271.06298828125 
[2025-02-20 12:11:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.5954601168632507 norm:0.001039423979818821 max memory_allocated 29271.06298828125 
[2025-02-20 12:11:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.5949554443359375 norm:0.0010082981316372752 max memory_allocated 29271.06298828125 
[2025-02-20 12:12:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.5946135520935059 norm:0.0009943143231794238 max memory_allocated 29271.06298828125 
[2025-02-20 12:13:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.5941506624221802 norm:0.000977715477347374 max memory_allocated 29271.06298828125 
[2025-02-20 12:14:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.5938904285430908 norm:0.0009827938629314303 max memory_allocated 29271.06298828125 
[2025-02-20 12:15:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.5936776399612427 norm:0.0009756876970641315 max memory_allocated 29271.06298828125 
[2025-02-20 12:15:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-20 12:16:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.7686751484870911 norm:0.06207380071282387 max memory_allocated 29271.25048828125 
[2025-02-20 12:17:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.7383534908294678 norm:0.03383273631334305 max memory_allocated 29271.25048828125 
[2025-02-20 12:17:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.702629804611206 norm:0.01632966846227646 max memory_allocated 29271.25048828125 
[2025-02-20 12:18:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.6845220327377319 norm:0.008749104104936123 max memory_allocated 29271.25048828125 
[2025-02-20 12:19:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.6769917607307434 norm:0.005800655111670494 max memory_allocated 29271.25048828125 
[2025-02-20 12:20:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.672315239906311 norm:0.004298247396945953 max memory_allocated 29271.25048828125 
[2025-02-20 12:21:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.6690041422843933 norm:0.003698362037539482 max memory_allocated 29271.25048828125 
[2025-02-20 12:21:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.6662458181381226 norm:0.0031397745478898287 max memory_allocated 29271.25048828125 
[2025-02-20 12:22:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.6638312935829163 norm:0.0025350013747811317 max memory_allocated 29271.25048828125 
[2025-02-20 12:23:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.6622992157936096 norm:0.0025442196056246758 max memory_allocated 29271.25048828125 
[2025-02-20 12:24:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.6610801219940186 norm:0.0023874114267528057 max memory_allocated 29271.25048828125 
[2025-02-20 12:25:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.6600561738014221 norm:0.002262067748233676 max memory_allocated 29271.25048828125 
[2025-02-20 12:26:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.6593557596206665 norm:0.0021538673900067806 max memory_allocated 29271.25048828125 
[2025-02-20 12:26:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.6588745713233948 norm:0.0020842908415943384 max memory_allocated 29271.25048828125 
[2025-02-20 12:27:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.6582737565040588 norm:0.0019598519429564476 max memory_allocated 29271.25048828125 
[2025-02-20 12:28:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.6575352549552917 norm:0.0018731753807514906 max memory_allocated 29271.25048828125 
[2025-02-20 12:29:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.6569364070892334 norm:0.0017912710318341851 max memory_allocated 29271.25048828125 
[2025-02-20 12:30:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.6564023494720459 norm:0.0017994486261159182 max memory_allocated 29271.25048828125 
[2025-02-20 12:31:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.6554134488105774 norm:0.0013424723874777555 max memory_allocated 29271.25048828125 
[2025-02-20 12:31:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.6547077894210815 norm:0.0011950613697990775 max memory_allocated 29271.25048828125 
[2025-02-20 12:32:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-20 12:32:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.8015389442443848 norm:0.018241656944155693 max memory_allocated 29271.43798828125 
[2025-02-20 12:33:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.7825004458427429 norm:0.010198398493230343 max memory_allocated 29271.43798828125 
[2025-02-20 12:34:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.7586692571640015 norm:0.005475251004099846 max memory_allocated 29271.43798828125 
[2025-02-20 12:35:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.7467280626296997 norm:0.0030996729619801044 max memory_allocated 29271.43798828125 
[2025-02-20 12:36:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.741863489151001 norm:0.0025464266072958708 max memory_allocated 29271.43798828125 
[2025-02-20 12:37:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.7387176752090454 norm:0.0022026386577636003 max memory_allocated 29271.43798828125 
[2025-02-20 12:37:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.7355465888977051 norm:0.0014482978731393814 max memory_allocated 29271.43798828125 
[2025-02-20 12:38:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.7334941625595093 norm:0.0014119134284555912 max memory_allocated 29271.43798828125 
[2025-02-20 12:39:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.7320481538772583 norm:0.0013817177386954427 max memory_allocated 29271.43798828125 
[2025-02-20 12:40:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.7307978272438049 norm:0.001347799552604556 max memory_allocated 29271.43798828125 
[2025-02-20 12:41:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.7297378182411194 norm:0.0012864969903603196 max memory_allocated 29271.43798828125 
[2025-02-20 12:41:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.728952944278717 norm:0.0012591725680977106 max memory_allocated 29271.43798828125 
[2025-02-20 12:42:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.728356122970581 norm:0.0012316169450059533 max memory_allocated 29271.43798828125 
[2025-02-20 12:43:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.7277448177337646 norm:0.0012179022887721658 max memory_allocated 29271.43798828125 
[2025-02-20 12:44:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.7272893190383911 norm:0.0011921761324629188 max memory_allocated 29271.43798828125 
[2025-02-20 12:45:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.7268551588058472 norm:0.0011769197881221771 max memory_allocated 29271.43798828125 
[2025-02-20 12:46:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.7263256311416626 norm:0.0011703709606081247 max memory_allocated 29271.43798828125 
[2025-02-20 12:46:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.7259492874145508 norm:0.0011628817301243544 max memory_allocated 29271.43798828125 
[2025-02-20 12:47:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.7256267666816711 norm:0.0011598453857004642 max memory_allocated 29271.43798828125 
[2025-02-20 12:48:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.7254068851470947 norm:0.0011504009598866105 max memory_allocated 29271.43798828125 
[2025-02-20 12:48:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-20 12:49:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.8899354934692383 norm:0.03279275447130203 max memory_allocated 29271.62548828125 
[2025-02-20 12:50:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.8683682680130005 norm:0.019775118678808212 max memory_allocated 29271.62548828125 
[2025-02-20 12:51:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.8398642539978027 norm:0.011217496357858181 max memory_allocated 29271.62548828125 
[2025-02-20 12:52:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.8246889710426331 norm:0.005784026347100735 max memory_allocated 29271.62548828125 
[2025-02-20 12:52:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.8188875913619995 norm:0.00460091745480895 max memory_allocated 29271.62548828125 
[2025-02-20 12:53:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.815229058265686 norm:0.0037635357584804296 max memory_allocated 29271.62548828125 
[2025-02-20 12:54:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.8125413060188293 norm:0.003256598487496376 max memory_allocated 29271.62548828125 
[2025-02-20 12:55:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.8104374408721924 norm:0.002993782050907612 max memory_allocated 29271.62548828125 
[2025-02-20 12:56:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.8086509108543396 norm:0.002727976767346263 max memory_allocated 29271.62548828125 
[2025-02-20 12:57:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.8069911003112793 norm:0.002374873496592045 max memory_allocated 29271.62548828125 
[2025-02-20 12:57:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.805597186088562 norm:0.0021310029551386833 max memory_allocated 29271.62548828125 
[2025-02-20 12:58:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.8046677708625793 norm:0.0020103624556213617 max memory_allocated 29271.62548828125 
[2025-02-20 12:59:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.8037267327308655 norm:0.0019476128509268165 max memory_allocated 29271.62548828125 
[2025-02-20 13:00:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.8023369312286377 norm:0.0013185428688302636 max memory_allocated 29271.62548828125 
[2025-02-20 13:01:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.8017614483833313 norm:0.0012442133156582713 max memory_allocated 29271.62548828125 
[2025-02-20 13:01:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.8014064431190491 norm:0.001231634640134871 max memory_allocated 29271.62548828125 
[2025-02-20 13:02:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.80084627866745 norm:0.0011936704395338893 max memory_allocated 29271.62548828125 
[2025-02-20 13:03:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.8004252910614014 norm:0.0011874365154653788 max memory_allocated 29271.62548828125 
[2025-02-20 13:04:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.8001865148544312 norm:0.0011683558113873005 max memory_allocated 29271.62548828125 
[2025-02-20 13:05:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.7999406456947327 norm:0.0011515135411173105 max memory_allocated 29271.62548828125 
[2025-02-20 13:05:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-20 13:06:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.9763161540031433 norm:0.011279702186584473 max memory_allocated 29271.81298828125 
[2025-02-20 13:07:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.9569652080535889 norm:0.007132196798920631 max memory_allocated 29271.81298828125 
[2025-02-20 13:07:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.9305043816566467 norm:0.0035467883571982384 max memory_allocated 29271.81298828125 
[2025-02-20 13:08:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.9205334186553955 norm:0.0020462963730096817 max memory_allocated 29271.81298828125 
[2025-02-20 13:09:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.9163745641708374 norm:0.0017416169866919518 max memory_allocated 29271.81298828125 
[2025-02-20 13:10:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.9134570956230164 norm:0.0016220875550061464 max memory_allocated 29271.81298828125 
[2025-02-20 13:11:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.9109270572662354 norm:0.0015826476737856865 max memory_allocated 29271.81298828125 
[2025-02-20 13:12:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.9090715050697327 norm:0.0015239155618473887 max memory_allocated 29271.81298828125 
[2025-02-20 13:12:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.9075302481651306 norm:0.0015078037977218628 max memory_allocated 29271.81298828125 
[2025-02-20 13:13:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.9064928889274597 norm:0.0014923505950719118 max memory_allocated 29271.81298828125 
[2025-02-20 13:14:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.9054009318351746 norm:0.0014551817439496517 max memory_allocated 29271.81298828125 
[2025-02-20 13:15:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.9044802188873291 norm:0.001424154033884406 max memory_allocated 29271.81298828125 
[2025-02-20 13:16:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.9037370681762695 norm:0.0013879782054573298 max memory_allocated 29271.81298828125 
[2025-02-20 13:16:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.9032133221626282 norm:0.0013549672439694405 max memory_allocated 29271.81298828125 
[2025-02-20 13:17:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.902643084526062 norm:0.0013236196245998144 max memory_allocated 29271.81298828125 
[2025-02-20 13:18:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.9022260904312134 norm:0.0013070243876427412 max memory_allocated 29271.81298828125 
[2025-02-20 13:19:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.901884138584137 norm:0.0012904714094474912 max memory_allocated 29271.81298828125 
[2025-02-20 13:20:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.9015994071960449 norm:0.0012729238951578736 max memory_allocated 29271.81298828125 
[2025-02-20 13:21:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.9013165235519409 norm:0.001259583979845047 max memory_allocated 29271.81298828125 
[2025-02-20 13:21:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.9010952115058899 norm:0.001245151157490909 max memory_allocated 29271.81298828125 
[2025-02-20 13:22:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-20 13:22:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:1.100390076637268 norm:0.019341351464390755 max memory_allocated 29272.00048828125 
[2025-02-20 13:23:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:1.0771650075912476 norm:0.009801149368286133 max memory_allocated 29272.00048828125 
[2025-02-20 13:24:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:1.0522189140319824 norm:0.005295582581311464 max memory_allocated 29272.00048828125 
[2025-02-20 13:25:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:1.0409082174301147 norm:0.0027192814741283655 max memory_allocated 29272.00048828125 
[2025-02-20 13:26:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:1.0361318588256836 norm:0.0019322928274050355 max memory_allocated 29272.00048828125 
[2025-02-20 13:27:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:1.0330321788787842 norm:0.0017939931713044643 max memory_allocated 29272.00048828125 
[2025-02-20 13:27:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:1.0306732654571533 norm:0.0017181664006784558 max memory_allocated 29272.00048828125 
[2025-02-20 13:28:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:1.028771162033081 norm:0.001638174639083445 max memory_allocated 29272.00048828125 
[2025-02-20 13:29:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:1.027214527130127 norm:0.001589445979334414 max memory_allocated 29272.00048828125 
[2025-02-20 13:30:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:1.025902509689331 norm:0.0015450501814484596 max memory_allocated 29272.00048828125 
[2025-02-20 13:31:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:1.0248833894729614 norm:0.001499631442129612 max memory_allocated 29272.00048828125 
[2025-02-20 13:31:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:1.0240181684494019 norm:0.0014644170878455043 max memory_allocated 29272.00048828125 
[2025-02-20 13:32:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:1.0233848094940186 norm:0.0014536682283505797 max memory_allocated 29272.00048828125 
[2025-02-20 13:33:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:1.0228934288024902 norm:0.001440605497919023 max memory_allocated 29272.00048828125 
[2025-02-20 13:34:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:1.022459626197815 norm:0.0014130740892142057 max memory_allocated 29272.00048828125 
[2025-02-20 13:35:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:1.0220757722854614 norm:0.0014041406102478504 max memory_allocated 29272.00048828125 
[2025-02-20 13:36:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:1.0217235088348389 norm:0.0013874088181182742 max memory_allocated 29272.00048828125 
[2025-02-20 13:36:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:1.0214847326278687 norm:0.0013829409144818783 max memory_allocated 29272.00048828125 
[2025-02-20 13:37:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:1.0211964845657349 norm:0.0013723918236792088 max memory_allocated 29272.00048828125 
[2025-02-20 13:38:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:1.0208667516708374 norm:0.0013728539925068617 max memory_allocated 29272.00048828125 
[2025-02-20 13:38:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-20 13:39:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:1.239270567893982 norm:0.019534947350621223 max memory_allocated 29272.18798828125 
[2025-02-20 13:40:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:1.2193031311035156 norm:0.011631329543888569 max memory_allocated 29272.18798828125 
[2025-02-20 13:41:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:1.1970546245574951 norm:0.006247747223824263 max memory_allocated 29272.18798828125 
[2025-02-20 13:42:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:1.1873276233673096 norm:0.002462667180225253 max memory_allocated 29272.18798828125 
[2025-02-20 13:42:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:1.1831154823303223 norm:0.0020978518296033144 max memory_allocated 29272.18798828125 
[2025-02-20 13:43:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:1.1800822019577026 norm:0.0019726864993572235 max memory_allocated 29272.18798828125 
[2025-02-20 13:44:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:1.1776981353759766 norm:0.0019091313006356359 max memory_allocated 29272.18798828125 
[2025-02-20 13:45:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:1.1757404804229736 norm:0.0018350681057199836 max memory_allocated 29272.18798828125 
[2025-02-20 13:46:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:1.174235224723816 norm:0.0018057937268167734 max memory_allocated 29272.18798828125 
[2025-02-20 13:47:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:1.1728222370147705 norm:0.001762673957273364 max memory_allocated 29272.18798828125 
[2025-02-20 13:47:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:1.1717653274536133 norm:0.001742903608828783 max memory_allocated 29272.18798828125 
[2025-02-20 13:48:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:1.1710485219955444 norm:0.0017252773977816105 max memory_allocated 29272.18798828125 
[2025-02-20 13:49:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:1.1701951026916504 norm:0.001672902493737638 max memory_allocated 29272.18798828125 
[2025-02-20 13:50:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:1.169446349143982 norm:0.0016532711451873183 max memory_allocated 29272.18798828125 
[2025-02-20 13:51:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:1.1688820123672485 norm:0.0016403916524723172 max memory_allocated 29272.18798828125 
[2025-02-20 13:51:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:1.1683204174041748 norm:0.001631660619750619 max memory_allocated 29272.18798828125 
[2025-02-20 13:52:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:1.1679240465164185 norm:0.001611983054317534 max memory_allocated 29272.18798828125 
[2025-02-20 13:53:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:1.1675372123718262 norm:0.0016100405482575297 max memory_allocated 29272.18798828125 
[2025-02-20 13:54:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:1.1672135591506958 norm:0.0016228985041379929 max memory_allocated 29272.18798828125 
[2025-02-20 13:55:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:1.1667804718017578 norm:0.0016083859372884035 max memory_allocated 29272.18798828125 
[2025-02-20 13:55:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-20 13:56:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:1.375272512435913 norm:0.01266384869813919 max memory_allocated 29272.37548828125 
[2025-02-20 13:57:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:1.3573209047317505 norm:0.00727669894695282 max memory_allocated 29272.37548828125 
[2025-02-20 13:57:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:1.3379818201065063 norm:0.004861902445554733 max memory_allocated 29272.37548828125 
[2025-02-20 13:58:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:1.3308346271514893 norm:0.0035412798170000315 max memory_allocated 29272.37548828125 
[2025-02-20 13:59:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:1.3266327381134033 norm:0.0030233734287321568 max memory_allocated 29272.37548828125 
[2025-02-20 14:00:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:1.3235867023468018 norm:0.002671930007636547 max memory_allocated 29272.37548828125 
[2025-02-20 14:01:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:1.32071852684021 norm:0.0023544887080788612 max memory_allocated 29272.37548828125 
[2025-02-20 14:02:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:1.3186150789260864 norm:0.0021461888682097197 max memory_allocated 29272.37548828125 
[2025-02-20 14:02:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:1.3168338537216187 norm:0.0019884693901985884 max memory_allocated 29272.37548828125 
[2025-02-20 14:03:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:1.3155338764190674 norm:0.001855402602814138 max memory_allocated 29272.37548828125 
[2025-02-20 14:04:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:1.314483880996704 norm:0.0017766323871910572 max memory_allocated 29272.37548828125 
[2025-02-20 14:05:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:1.3136310577392578 norm:0.0017038836376741529 max memory_allocated 29272.37548828125 
[2025-02-20 14:06:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:1.312903881072998 norm:0.0016476456075906754 max memory_allocated 29272.37548828125 
[2025-02-20 14:06:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:1.3122206926345825 norm:0.0016039866022765636 max memory_allocated 29272.37548828125 
[2025-02-20 14:07:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:1.3115392923355103 norm:0.0015746313147246838 max memory_allocated 29272.37548828125 
[2025-02-20 14:08:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:1.3110156059265137 norm:0.0015460725408047438 max memory_allocated 29272.37548828125 
[2025-02-20 14:09:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:1.3107249736785889 norm:0.0015270472504198551 max memory_allocated 29272.37548828125 
[2025-02-20 14:10:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:1.3103537559509277 norm:0.0015049254288896918 max memory_allocated 29272.37548828125 
[2025-02-20 14:11:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:1.3099647760391235 norm:0.0014969611074775457 max memory_allocated 29272.37548828125 
[2025-02-20 14:11:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:1.309638261795044 norm:0.0014805897371843457 max memory_allocated 29272.37548828125 
[2025-02-20 14:12:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-20 14:12:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:1.5428478717803955 norm:0.019063735380768776 max memory_allocated 29272.56298828125 
[2025-02-20 14:13:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:1.521216630935669 norm:0.011140173301100731 max memory_allocated 29272.56298828125 
[2025-02-20 14:14:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:1.50002920627594 norm:0.006559767760336399 max memory_allocated 29272.56298828125 
[2025-02-20 14:15:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:1.4904625415802002 norm:0.003927889280021191 max memory_allocated 29272.56298828125 
[2025-02-20 14:16:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:1.484692931175232 norm:0.001819811062887311 max memory_allocated 29272.56298828125 
[2025-02-20 14:17:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:1.4811874628067017 norm:0.0016898452304303646 max memory_allocated 29272.56298828125 
[2025-02-20 14:17:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:1.4785324335098267 norm:0.0016050933627411723 max memory_allocated 29272.56298828125 
[2025-02-20 14:18:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:1.476427435874939 norm:0.001544647617265582 max memory_allocated 29272.56298828125 
[2025-02-20 14:19:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:1.4748296737670898 norm:0.0015191346174106002 max memory_allocated 29272.56298828125 
[2025-02-20 14:20:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:1.4736324548721313 norm:0.0014983369037508965 max memory_allocated 29272.56298828125 
[2025-02-20 14:21:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:1.472862720489502 norm:0.0014938239473849535 max memory_allocated 29272.56298828125 
[2025-02-20 14:22:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:1.4719401597976685 norm:0.0014819271164014935 max memory_allocated 29272.56298828125 
[2025-02-20 14:22:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:1.4711711406707764 norm:0.0014746151864528656 max memory_allocated 29272.56298828125 
[2025-02-20 14:23:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:1.4705908298492432 norm:0.0014712833799421787 max memory_allocated 29272.56298828125 
[2025-02-20 14:24:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:1.469948172569275 norm:0.0014675261918455362 max memory_allocated 29272.56298828125 
[2025-02-20 14:25:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:1.4692864418029785 norm:0.001456694328226149 max memory_allocated 29272.56298828125 
[2025-02-20 14:26:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:1.4690489768981934 norm:0.0014548087492585182 max memory_allocated 29272.56298828125 
[2025-02-20 14:26:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:1.4689099788665771 norm:0.0014564660377800465 max memory_allocated 29272.56298828125 
[2025-02-20 14:27:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:1.4687070846557617 norm:0.0014548671897500753 max memory_allocated 29272.56298828125 
[2025-02-20 14:28:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:1.4684749841690063 norm:0.0014508113963529468 max memory_allocated 29272.56298828125 
[2025-02-20 14:28:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-20 14:29:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:1.6997051239013672 norm:0.014704463072121143 max memory_allocated 29272.75048828125 
[2025-02-20 14:30:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:1.680023431777954 norm:0.00801569502800703 max memory_allocated 29272.75048828125 
[2025-02-20 14:31:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:1.658952236175537 norm:0.004932353273034096 max memory_allocated 29272.75048828125 
[2025-02-20 14:32:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:1.649832844734192 norm:0.002464030869305134 max memory_allocated 29272.75048828125 
[2025-02-20 14:32:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:1.645141363143921 norm:0.0017336001619696617 max memory_allocated 29272.75048828125 
[2025-02-20 14:33:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:1.641784429550171 norm:0.0015238240594044328 max memory_allocated 29272.75048828125 
[2025-02-20 14:34:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:1.6391111612319946 norm:0.0014370062854140997 max memory_allocated 29272.75048828125 
[2025-02-20 14:35:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:1.6370244026184082 norm:0.0014239887241274118 max memory_allocated 29272.75048828125 
[2025-02-20 14:36:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:1.635479211807251 norm:0.0014199712313711643 max memory_allocated 29272.75048828125 
[2025-02-20 14:37:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:1.6342875957489014 norm:0.0014073371421545744 max memory_allocated 29272.75048828125 
[2025-02-20 14:37:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:1.6335558891296387 norm:0.0013990350998938084 max memory_allocated 29272.75048828125 
[2025-02-20 14:38:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:1.632983922958374 norm:0.0014015422202646732 max memory_allocated 29272.75048828125 
[2025-02-20 14:39:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:1.6324466466903687 norm:0.0013978536007925868 max memory_allocated 29272.75048828125 
[2025-02-20 14:40:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:1.631847620010376 norm:0.0013922742800787091 max memory_allocated 29272.75048828125 
[2025-02-20 14:41:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:1.631245732307434 norm:0.001379793044179678 max memory_allocated 29272.75048828125 
[2025-02-20 14:41:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:1.630852222442627 norm:0.001376416883431375 max memory_allocated 29272.75048828125 
[2025-02-20 14:42:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:1.6305439472198486 norm:0.0013777645071968436 max memory_allocated 29272.75048828125 
[2025-02-20 14:43:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:1.630330204963684 norm:0.0013736286200582981 max memory_allocated 29272.75048828125 
[2025-02-20 14:44:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:1.630123496055603 norm:0.0013711178908124566 max memory_allocated 29272.75048828125 
[2025-02-20 14:45:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:1.6299192905426025 norm:0.0013694516383111477 max memory_allocated 29272.75048828125 
[2025-02-20 14:45:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-20 14:46:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:1.8861722946166992 norm:0.006029017269611359 max memory_allocated 29272.93798828125 
[2025-02-20 14:47:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:1.8678159713745117 norm:0.003693504026159644 max memory_allocated 29272.93798828125 
[2025-02-20 14:47:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:1.8459265232086182 norm:0.0025862816255539656 max memory_allocated 29272.93798828125 
[2025-02-20 14:48:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:1.837384819984436 norm:0.0016968989511951804 max memory_allocated 29272.93798828125 
[2025-02-20 14:49:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:1.832729697227478 norm:0.0014944311697036028 max memory_allocated 29272.93798828125 
[2025-02-20 14:50:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:1.82919180393219 norm:0.0014443949330598116 max memory_allocated 29272.93798828125 
[2025-02-20 14:51:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:1.8263545036315918 norm:0.001415872946381569 max memory_allocated 29272.93798828125 
[2025-02-20 14:52:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:1.8240195512771606 norm:0.001398001448251307 max memory_allocated 29272.93798828125 
[2025-02-20 14:52:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:1.8223391771316528 norm:0.0013936115428805351 max memory_allocated 29272.93798828125 
[2025-02-20 14:53:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:1.8209470510482788 norm:0.0013855649158358574 max memory_allocated 29272.93798828125 
[2025-02-20 14:54:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:1.8198050260543823 norm:0.0013817287981510162 max memory_allocated 29272.93798828125 
[2025-02-20 14:55:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:1.818953514099121 norm:0.0013810042291879654 max memory_allocated 29272.93798828125 
[2025-02-20 14:56:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:1.8182748556137085 norm:0.0013783754548057914 max memory_allocated 29272.93798828125 
[2025-02-20 14:56:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:1.8177098035812378 norm:0.0013822149485349655 max memory_allocated 29272.93798828125 
[2025-02-20 14:57:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:1.817223072052002 norm:0.0013889104593545198 max memory_allocated 29272.93798828125 
[2025-02-20 14:58:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:1.816806435585022 norm:0.0013888096436858177 max memory_allocated 29272.93798828125 
[2025-02-20 14:59:26 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:1.8166394233703613 norm:0.0013943552039563656 max memory_allocated 29272.93798828125 
[2025-02-20 15:00:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:1.8162517547607422 norm:0.0013846044894307852 max memory_allocated 29272.93798828125 
[2025-02-20 15:01:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:1.8159973621368408 norm:0.0013896034797653556 max memory_allocated 29272.93798828125 
[2025-02-20 15:01:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:1.815825343132019 norm:0.0013909105909988284 max memory_allocated 29272.93798828125 
[2025-02-20 15:02:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-20 15:03:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:2.080806016921997 norm:0.007088634185492992 max memory_allocated 29273.12548828125 
[2025-02-20 15:03:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:2.058628559112549 norm:0.0038438858464360237 max memory_allocated 29273.12548828125 
[2025-02-20 15:04:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:2.0361380577087402 norm:0.002739459043368697 max memory_allocated 29273.12548828125 
[2025-02-20 15:05:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:2.0286202430725098 norm:0.002211695071309805 max memory_allocated 29273.12548828125 
[2025-02-20 15:06:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:2.024600028991699 norm:0.001991985598579049 max memory_allocated 29273.12548828125 
[2025-02-20 15:07:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:2.020988702774048 norm:0.0018745434936136007 max memory_allocated 29273.12548828125 
[2025-02-20 15:07:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:2.0180931091308594 norm:0.001839547185227275 max memory_allocated 29273.12548828125 
[2025-02-20 15:08:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:2.015599489212036 norm:0.0017329563852399588 max memory_allocated 29273.12548828125 
[2025-02-20 15:09:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:2.013932228088379 norm:0.001680559478700161 max memory_allocated 29273.12548828125 
[2025-02-20 15:10:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:2.012601137161255 norm:0.0016128052957355976 max memory_allocated 29273.12548828125 
[2025-02-20 15:11:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:2.011561393737793 norm:0.0016349725192412734 max memory_allocated 29273.12548828125 
[2025-02-20 15:12:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:2.010913848876953 norm:0.0016423650085926056 max memory_allocated 29273.12548828125 
[2025-02-20 15:12:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:2.010298728942871 norm:0.0016896153101697564 max memory_allocated 29273.12548828125 
[2025-02-20 15:13:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:2.0096848011016846 norm:0.0016765780746936798 max memory_allocated 29273.12548828125 
[2025-02-20 15:14:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:2.0089635848999023 norm:0.001652235398069024 max memory_allocated 29273.12548828125 
[2025-02-20 15:15:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:2.0084567070007324 norm:0.001638848683796823 max memory_allocated 29273.12548828125 
[2025-02-20 15:16:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:2.0079519748687744 norm:0.001622652867808938 max memory_allocated 29273.12548828125 
[2025-02-20 15:16:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:2.007408618927002 norm:0.0016471664421260357 max memory_allocated 29273.12548828125 
[2025-02-20 15:17:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:2.00708270072937 norm:0.001627522986382246 max memory_allocated 29273.12548828125 
[2025-02-20 15:18:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:2.006859540939331 norm:0.001610499108210206 max memory_allocated 29273.12548828125 
[2025-02-20 15:18:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-20 15:19:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:2.2612030506134033 norm:0.011629663407802582 max memory_allocated 29273.31298828125 
[2025-02-20 15:20:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:2.240884780883789 norm:0.006306803319603205 max memory_allocated 29273.31298828125 
[2025-02-20 15:21:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:2.2178375720977783 norm:0.0037825636100023985 max memory_allocated 29273.31298828125 
[2025-02-20 15:22:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:2.209136962890625 norm:0.0026452657766640186 max memory_allocated 29273.31298828125 
[2025-02-20 15:22:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:2.2037668228149414 norm:0.0016899704933166504 max memory_allocated 29273.31298828125 
[2025-02-20 15:23:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:2.1998305320739746 norm:0.0014887339202687144 max memory_allocated 29273.31298828125 
[2025-02-20 15:24:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:2.196770668029785 norm:0.0014323563082143664 max memory_allocated 29273.31298828125 
[2025-02-20 15:25:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:2.194293260574341 norm:0.0013844461645931005 max memory_allocated 29273.31298828125 
[2025-02-20 15:26:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:2.1924993991851807 norm:0.00137057073879987 max memory_allocated 29273.31298828125 
[2025-02-20 15:27:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:2.1911966800689697 norm:0.0013569977600127459 max memory_allocated 29273.31298828125 
[2025-02-20 15:27:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:2.190195083618164 norm:0.001343714538961649 max memory_allocated 29273.31298828125 
[2025-02-20 15:28:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:2.1894962787628174 norm:0.001326108118519187 max memory_allocated 29273.31298828125 
[2025-02-20 15:29:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:2.188814163208008 norm:0.0013201027177274227 max memory_allocated 29273.31298828125 
[2025-02-20 15:30:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:2.18833589553833 norm:0.0013306026812642813 max memory_allocated 29273.31298828125 
[2025-02-20 15:31:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:2.187865972518921 norm:0.0013310960493981838 max memory_allocated 29273.31298828125 
[2025-02-20 15:31:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:2.1875104904174805 norm:0.0013296151300892234 max memory_allocated 29273.31298828125 
[2025-02-20 15:32:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:2.1872034072875977 norm:0.0013260134728625417 max memory_allocated 29273.31298828125 
[2025-02-20 15:33:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:2.1869821548461914 norm:0.0013223417336121202 max memory_allocated 29273.31298828125 
[2025-02-20 15:34:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:2.186800956726074 norm:0.0013186063151806593 max memory_allocated 29273.31298828125 
[2025-02-20 15:35:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:2.186579465866089 norm:0.0013231434859335423 max memory_allocated 29273.31298828125 
[2025-02-20 15:35:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-20 15:36:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:2.479149103164673 norm:0.01576630398631096 max memory_allocated 29273.50048828125 
[2025-02-20 15:37:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:2.4594228267669678 norm:0.011084548197686672 max memory_allocated 29273.50048828125 
[2025-02-20 15:38:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:2.4351186752319336 norm:0.007929094135761261 max memory_allocated 29273.50048828125 
[2025-02-20 15:38:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:2.425415515899658 norm:0.006107711233198643 max memory_allocated 29273.50048828125 
[2025-02-20 15:39:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:2.4181902408599854 norm:0.004594423808157444 max memory_allocated 29273.50048828125 
[2025-02-20 15:40:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:2.4111742973327637 norm:0.003651171224191785 max memory_allocated 29273.50048828125 
[2025-02-20 15:41:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:2.4070417881011963 norm:0.003505539847537875 max memory_allocated 29273.50048828125 
[2025-02-20 15:42:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:2.4041781425476074 norm:0.0031757992692291737 max memory_allocated 29273.50048828125 
[2025-02-20 15:42:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:2.4018936157226562 norm:0.0028928606770932674 max memory_allocated 29273.50048828125 
[2025-02-20 15:43:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:2.4003005027770996 norm:0.0027215941809117794 max memory_allocated 29273.50048828125 
[2025-02-20 15:44:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:2.3988492488861084 norm:0.0025207968428730965 max memory_allocated 29273.50048828125 
[2025-02-20 15:45:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:2.3977723121643066 norm:0.002348931273445487 max memory_allocated 29273.50048828125 
[2025-02-20 15:46:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:2.396799087524414 norm:0.0021841702982783318 max memory_allocated 29273.50048828125 
[2025-02-20 15:47:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:2.3959665298461914 norm:0.0020942878909409046 max memory_allocated 29273.50048828125 
[2025-02-20 15:47:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:2.3948025703430176 norm:0.0019095528405159712 max memory_allocated 29273.50048828125 
[2025-02-20 15:48:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:2.394341230392456 norm:0.0019227503798902035 max memory_allocated 29273.50048828125 
[2025-02-20 15:49:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:2.3937973976135254 norm:0.0018587980885058641 max memory_allocated 29273.50048828125 
[2025-02-20 15:50:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:2.3931984901428223 norm:0.0018054808024317026 max memory_allocated 29273.50048828125 
[2025-02-20 15:51:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:2.392730951309204 norm:0.0017580126877874136 max memory_allocated 29273.50048828125 
[2025-02-20 15:51:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:2.3923604488372803 norm:0.0017213583923876286 max memory_allocated 29273.50048828125 
[2025-02-20 15:52:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-20 15:53:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:2.678500175476074 norm:0.006618882529437542 max memory_allocated 29273.68798828125 
[2025-02-20 15:53:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:2.656897783279419 norm:0.004004506394267082 max memory_allocated 29273.68798828125 
[2025-02-20 15:54:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:2.6324617862701416 norm:0.002909889677539468 max memory_allocated 29273.68798828125 
[2025-02-20 15:55:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:2.622735023498535 norm:0.002414305927231908 max memory_allocated 29273.68798828125 
[2025-02-20 15:56:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:2.617058753967285 norm:0.002253515413030982 max memory_allocated 29273.68798828125 
[2025-02-20 15:57:08 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:2.6122183799743652 norm:0.0021611384581774473 max memory_allocated 29273.68798828125 
[2025-02-20 15:57:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:2.6082851886749268 norm:0.0019964207895100117 max memory_allocated 29273.68798828125 
[2025-02-20 15:58:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:2.6055233478546143 norm:0.0019279099069535732 max memory_allocated 29273.68798828125 
[2025-02-20 15:59:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:2.603515625 norm:0.0018945258343592286 max memory_allocated 29273.68798828125 
[2025-02-20 16:00:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:2.6019909381866455 norm:0.0018278979696333408 max memory_allocated 29273.68798828125 
[2025-02-20 16:01:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:2.6008949279785156 norm:0.0018140526954084635 max memory_allocated 29273.68798828125 
[2025-02-20 16:02:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:2.599891424179077 norm:0.0017775667365640402 max memory_allocated 29273.68798828125 
[2025-02-20 16:02:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:2.599071979522705 norm:0.0017355974996462464 max memory_allocated 29273.68798828125 
[2025-02-20 16:03:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:2.5984177589416504 norm:0.001721510081551969 max memory_allocated 29273.68798828125 
[2025-02-20 16:04:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:2.597860813140869 norm:0.0016840845346450806 max memory_allocated 29273.68798828125 
[2025-02-20 16:05:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:2.5973119735717773 norm:0.0016656212974339724 max memory_allocated 29273.68798828125 
[2025-02-20 16:06:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:2.5968782901763916 norm:0.0016567696584388614 max memory_allocated 29273.68798828125 
[2025-02-20 16:06:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:2.596466064453125 norm:0.0016321754083037376 max memory_allocated 29273.68798828125 
[2025-02-20 16:07:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:2.5962002277374268 norm:0.0016223752172663808 max memory_allocated 29273.68798828125 
[2025-02-20 16:08:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:2.595935106277466 norm:0.001613855711184442 max memory_allocated 29273.68798828125 
[2025-02-20 16:08:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-20 16:09:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:2.8916268348693848 norm:0.011410585604608059 max memory_allocated 29273.87548828125 
[2025-02-20 16:10:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:2.8687386512756348 norm:0.00676098233088851 max memory_allocated 29273.87548828125 
[2025-02-20 16:11:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:2.841427803039551 norm:0.004400305915623903 max memory_allocated 29273.87548828125 
[2025-02-20 16:12:11 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:2.8295814990997314 norm:0.003002190263941884 max memory_allocated 29273.87548828125 
[2025-02-20 16:13:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:2.824005603790283 norm:0.002350498689338565 max memory_allocated 29273.87548828125 
[2025-02-20 16:13:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:2.8184759616851807 norm:0.0020105172879993916 max memory_allocated 29273.87548828125 
[2025-02-20 16:14:38 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:2.813840866088867 norm:0.0017537148669362068 max memory_allocated 29273.87548828125 
[2025-02-20 16:15:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:2.8106255531311035 norm:0.0016436104197055101 max memory_allocated 29273.87548828125 
[2025-02-20 16:16:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:2.808082103729248 norm:0.001589036313816905 max memory_allocated 29273.87548828125 
[2025-02-20 16:17:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:2.806319236755371 norm:0.0015488506760448217 max memory_allocated 29273.87548828125 
[2025-02-20 16:17:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:2.8049607276916504 norm:0.0014705481007695198 max memory_allocated 29273.87548828125 
[2025-02-20 16:18:44 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:2.803819179534912 norm:0.0014441196108236909 max memory_allocated 29273.87548828125 
[2025-02-20 16:19:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:2.8031246662139893 norm:0.0014376372564584017 max memory_allocated 29273.87548828125 
[2025-02-20 16:20:22 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:2.802530288696289 norm:0.0014168355846777558 max memory_allocated 29273.87548828125 
[2025-02-20 16:21:11 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:2.801919937133789 norm:0.0013927562395110726 max memory_allocated 29273.87548828125 
[2025-02-20 16:22:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:2.8014321327209473 norm:0.0013963517267256975 max memory_allocated 29273.87548828125 
[2025-02-20 16:22:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:2.8008692264556885 norm:0.0013737838016822934 max memory_allocated 29273.87548828125 
[2025-02-20 16:23:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:2.8004302978515625 norm:0.001356569118797779 max memory_allocated 29273.87548828125 
[2025-02-20 16:24:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:2.7999157905578613 norm:0.0013515972532331944 max memory_allocated 29273.87548828125 
[2025-02-20 16:25:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:2.7995352745056152 norm:0.0013379458105191588 max memory_allocated 29273.87548828125 
[2025-02-20 16:25:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-20 16:26:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:3.1347522735595703 norm:0.011043407022953033 max memory_allocated 29274.06298828125 
[2025-02-20 16:27:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:3.109532356262207 norm:0.00608528358861804 max memory_allocated 29274.06298828125 
[2025-02-20 16:28:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:3.0798189640045166 norm:0.004154634661972523 max memory_allocated 29274.06298828125 
[2025-02-20 16:28:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:3.067413806915283 norm:0.002941422164440155 max memory_allocated 29274.06298828125 
[2025-02-20 16:29:40 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:3.0603537559509277 norm:0.002290876116603613 max memory_allocated 29274.06298828125 
[2025-02-20 16:30:30 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:3.0540013313293457 norm:0.0019322929438203573 max memory_allocated 29274.06298828125 
[2025-02-20 16:31:19 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:3.0491108894348145 norm:0.0018352542538195848 max memory_allocated 29274.06298828125 
[2025-02-20 16:32:08 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:3.045501947402954 norm:0.0017338257748633623 max memory_allocated 29274.06298828125 
[2025-02-20 16:32:57 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:3.0428998470306396 norm:0.0016598746879026294 max memory_allocated 29274.06298828125 
[2025-02-20 16:33:46 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:3.04105544090271 norm:0.0016159527003765106 max memory_allocated 29274.06298828125 
[2025-02-20 16:34:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:3.0395874977111816 norm:0.0015914831310510635 max memory_allocated 29274.06298828125 
[2025-02-20 16:35:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:3.03843355178833 norm:0.001567730912938714 max memory_allocated 29274.06298828125 
[2025-02-20 16:36:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:3.037402629852295 norm:0.0015445826575160027 max memory_allocated 29274.06298828125 
[2025-02-20 16:37:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:3.036494731903076 norm:0.001520290388725698 max memory_allocated 29274.06298828125 
[2025-02-20 16:37:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:3.035705327987671 norm:0.0014773588627576828 max memory_allocated 29274.06298828125 
[2025-02-20 16:38:40 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:3.0351297855377197 norm:0.0014664717018604279 max memory_allocated 29274.06298828125 
[2025-02-20 16:39:30 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:3.0347251892089844 norm:0.0014552504289895296 max memory_allocated 29274.06298828125 
[2025-02-20 16:40:19 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:3.0342578887939453 norm:0.001441262662410736 max memory_allocated 29274.06298828125 
[2025-02-20 16:41:08 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:3.0338268280029297 norm:0.0014423996908590198 max memory_allocated 29274.06298828125 
[2025-02-20 16:41:57 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:3.0334181785583496 norm:0.0014394649770110846 max memory_allocated 29274.06298828125 
[2025-02-20 16:42:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-20 16:43:04 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:3.4530439376831055 norm:0.033824291080236435 max memory_allocated 29274.25048828125 
[2025-02-20 16:43:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:3.4190640449523926 norm:0.02227558009326458 max memory_allocated 29274.25048828125 
[2025-02-20 16:44:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:3.378570556640625 norm:0.014562509022653103 max memory_allocated 29274.25048828125 
[2025-02-20 16:45:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:3.356900215148926 norm:0.009557120501995087 max memory_allocated 29274.25048828125 
[2025-02-20 16:46:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:3.3443636894226074 norm:0.007370776496827602 max memory_allocated 29274.25048828125 
[2025-02-20 16:47:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:3.336245536804199 norm:0.006264996714890003 max memory_allocated 29274.25048828125 
[2025-02-20 16:47:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:3.3302454948425293 norm:0.005386974196881056 max memory_allocated 29274.25048828125 
[2025-02-20 16:48:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:3.3249902725219727 norm:0.004572727717459202 max memory_allocated 29274.25048828125 
[2025-02-20 16:49:36 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:3.320624589920044 norm:0.003979748580604792 max memory_allocated 29274.25048828125 
[2025-02-20 16:50:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:3.315774917602539 norm:0.0018915944965556264 max memory_allocated 29274.25048828125 
[2025-02-20 16:51:15 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:3.313077926635742 norm:0.0018648937111720443 max memory_allocated 29274.25048828125 
[2025-02-20 16:52:04 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:3.31105899810791 norm:0.0018462992738932371 max memory_allocated 29274.25048828125 
[2025-02-20 16:52:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:3.3097543716430664 norm:0.001860019750893116 max memory_allocated 29274.25048828125 
[2025-02-20 16:53:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:3.3085474967956543 norm:0.001846620929427445 max memory_allocated 29274.25048828125 
[2025-02-20 16:54:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:3.307492971420288 norm:0.0018434971570968628 max memory_allocated 29274.25048828125 
[2025-02-20 16:55:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:3.306469440460205 norm:0.0018126033246517181 max memory_allocated 29274.25048828125 
[2025-02-20 16:56:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:3.306088924407959 norm:0.0017835567705333233 max memory_allocated 29274.25048828125 
[2025-02-20 16:56:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:3.3064379692077637 norm:0.0017802667571231723 max memory_allocated 29274.25048828125 
[2025-02-20 16:57:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:3.3061447143554688 norm:0.00178571417927742 max memory_allocated 29274.25048828125 
[2025-02-20 16:58:36 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:3.3054535388946533 norm:0.0017841216176748276 max memory_allocated 29274.25048828125 
[2025-02-20 16:58:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-20 16:59:43 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:3.743372678756714 norm:0.022664062678813934 max memory_allocated 29274.43798828125 
[2025-02-20 17:00:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:3.703801155090332 norm:0.015301131643354893 max memory_allocated 29274.43798828125 
[2025-02-20 17:01:22 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:3.6603245735168457 norm:0.010439842939376831 max memory_allocated 29274.43798828125 
[2025-02-20 17:02:11 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:3.63786244392395 norm:0.007415455300360918 max memory_allocated 29274.43798828125 
[2025-02-20 17:03:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:3.6246869564056396 norm:0.005689689889550209 max memory_allocated 29274.43798828125 
[2025-02-20 17:03:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:3.61590838432312 norm:0.005093490704894066 max memory_allocated 29274.43798828125 
[2025-02-20 17:04:38 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:3.6086134910583496 norm:0.004436349030584097 max memory_allocated 29274.43798828125 
[2025-02-20 17:05:27 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:3.6029374599456787 norm:0.004037041217088699 max memory_allocated 29274.43798828125 
[2025-02-20 17:06:16 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:3.598559617996216 norm:0.003733624704182148 max memory_allocated 29274.43798828125 
[2025-02-20 17:07:05 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:3.595421075820923 norm:0.003452727571129799 max memory_allocated 29274.43798828125 
[2025-02-20 17:07:54 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:3.5927157402038574 norm:0.003220277838408947 max memory_allocated 29274.43798828125 
[2025-02-20 17:08:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:3.590580940246582 norm:0.003050483064725995 max memory_allocated 29274.43798828125 
[2025-02-20 17:09:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:3.5884604454040527 norm:0.0030372245237231255 max memory_allocated 29274.43798828125 
[2025-02-20 17:10:22 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:3.5866990089416504 norm:0.003108329838141799 max memory_allocated 29274.43798828125 
[2025-02-20 17:11:11 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:3.5853419303894043 norm:0.003213081043213606 max memory_allocated 29274.43798828125 
[2025-02-20 17:12:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:3.5844459533691406 norm:0.0031777149997651577 max memory_allocated 29274.43798828125 
[2025-02-20 17:12:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:3.583848476409912 norm:0.0031506509985774755 max memory_allocated 29274.43798828125 
[2025-02-20 17:13:38 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:3.5825116634368896 norm:0.0030862519051879644 max memory_allocated 29274.43798828125 
[2025-02-20 17:14:28 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:3.581666946411133 norm:0.003134678350761533 max memory_allocated 29274.43798828125 
[2025-02-20 17:15:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:3.5806455612182617 norm:0.0031541124917566776 max memory_allocated 29274.43798828125 
[2025-02-20 17:15:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-20 17:15:35 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:16:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:4.049612998962402 norm:0.04394238442182541 max memory_allocated 29274.77001953125 
[2025-02-20 17:17:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:4.002161502838135 norm:0.04087120294570923 max memory_allocated 29274.77001953125 
[2025-02-20 17:18:02 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:3.9503936767578125 norm:0.03338740020990372 max memory_allocated 29274.77001953125 
[2025-02-20 17:18:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:3.92755126953125 norm:0.029215505346655846 max memory_allocated 29274.77001953125 
[2025-02-20 17:19:40 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:3.9126453399658203 norm:0.026097774505615234 max memory_allocated 29274.77001953125 
[2025-02-20 17:20:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:3.9009387493133545 norm:0.022962065413594246 max memory_allocated 29274.77001953125 
[2025-02-20 17:21:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:3.8920862674713135 norm:0.021329903975129128 max memory_allocated 29274.77001953125 
[2025-02-20 17:22:08 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:3.8857483863830566 norm:0.020259682089090347 max memory_allocated 29274.77001953125 
[2025-02-20 17:22:58 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:3.8808250427246094 norm:0.019309144467115402 max memory_allocated 29274.77001953125 
[2025-02-20 17:23:47 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:3.877135992050171 norm:0.018324213102459908 max memory_allocated 29274.77001953125 
[2025-02-20 17:24:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:3.874314785003662 norm:0.017734656110405922 max memory_allocated 29274.77001953125 
[2025-02-20 17:25:26 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:3.8722987174987793 norm:0.017416302114725113 max memory_allocated 29274.77001953125 
[2025-02-20 17:26:15 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:3.870511054992676 norm:0.017085717990994453 max memory_allocated 29274.77001953125 
[2025-02-20 17:27:04 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:3.8677690029144287 norm:0.01686432585120201 max memory_allocated 29274.77001953125 
[2025-02-20 17:27:53 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:3.8661935329437256 norm:0.016716308891773224 max memory_allocated 29274.77001953125 
[2025-02-20 17:28:43 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:3.8648266792297363 norm:0.016424251720309258 max memory_allocated 29274.77001953125 
[2025-02-20 17:29:32 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:3.863649368286133 norm:0.01624825969338417 max memory_allocated 29274.77001953125 
[2025-02-20 17:30:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:3.862715005874634 norm:0.016155604273080826 max memory_allocated 29274.77001953125 
[2025-02-20 17:31:10 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:3.8618271350860596 norm:0.016127174720168114 max memory_allocated 29274.77001953125 
[2025-02-20 17:32:00 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:3.861199140548706 norm:0.016363130882382393 max memory_allocated 29274.77001953125 
[2025-02-20 17:32:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-20 17:32:18 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:33:07 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:4.512546539306641 norm:0.06285922229290009 max memory_allocated 29274.95751953125 
[2025-02-20 17:33:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:4.443753719329834 norm:0.05333935096859932 max memory_allocated 29274.95751953125 
[2025-02-20 17:34:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:4.371285915374756 norm:0.041043151170015335 max memory_allocated 29274.95751953125 
[2025-02-20 17:35:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:4.338657379150391 norm:0.032234881073236465 max memory_allocated 29274.95751953125 
[2025-02-20 17:36:25 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:4.31843376159668 norm:0.025649134069681168 max memory_allocated 29274.95751953125 
[2025-02-20 17:37:14 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:4.305009841918945 norm:0.0216224268078804 max memory_allocated 29274.95751953125 
[2025-02-20 17:38:03 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:4.296239376068115 norm:0.0205900389701128 max memory_allocated 29274.95751953125 
[2025-02-20 17:38:52 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:4.289885520935059 norm:0.020037375390529633 max memory_allocated 29274.95751953125 
[2025-02-20 17:39:42 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:4.283638000488281 norm:0.020164113491773605 max memory_allocated 29274.95751953125 
[2025-02-20 17:40:31 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:4.278758525848389 norm:0.019439587369561195 max memory_allocated 29274.95751953125 
[2025-02-20 17:41:21 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:4.275352478027344 norm:0.019078614190220833 max memory_allocated 29274.95751953125 
[2025-02-20 17:42:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:4.271771430969238 norm:0.0184231735765934 max memory_allocated 29274.95751953125 
[2025-02-20 17:42:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:4.268910884857178 norm:0.017770512029528618 max memory_allocated 29274.95751953125 
[2025-02-20 17:43:49 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:4.267539024353027 norm:0.018672514706850052 max memory_allocated 29274.95751953125 
[2025-02-20 17:44:38 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:4.26531982421875 norm:0.018947260454297066 max memory_allocated 29274.95751953125 
[2025-02-20 17:45:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:4.2627997398376465 norm:0.018211835995316505 max memory_allocated 29274.95751953125 
[2025-02-20 17:46:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:4.261976718902588 norm:0.019009752199053764 max memory_allocated 29274.95751953125 
[2025-02-20 17:47:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:4.2601637840271 norm:0.0183534137904644 max memory_allocated 29274.95751953125 
[2025-02-20 17:47:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:4.258472919464111 norm:0.017725592479109764 max memory_allocated 29274.95751953125 
[2025-02-20 17:48:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:4.257655143737793 norm:0.017627807334065437 max memory_allocated 29274.95751953125 
[2025-02-20 17:48:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-20 17:49:03 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 17:49:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:6.047424793243408 norm:0.40691208839416504 max memory_allocated 29275.14501953125 
[2025-02-20 17:50:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:5.739704132080078 norm:0.36967238783836365 max memory_allocated 29275.14501953125 
[2025-02-20 17:51:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:5.58119010925293 norm:0.2868536412715912 max memory_allocated 29275.14501953125 
[2025-02-20 17:52:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:5.481527328491211 norm:0.22030656039714813 max memory_allocated 29275.14501953125 
[2025-02-20 17:53:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:5.421618461608887 norm:0.16734158992767334 max memory_allocated 29275.14501953125 
[2025-02-20 17:53:57 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:5.359582901000977 norm:0.12941665947437286 max memory_allocated 29275.14501953125 
[2025-02-20 17:54:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:5.293112754821777 norm:0.0979151725769043 max memory_allocated 29275.14501953125 
[2025-02-20 17:55:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:5.255756855010986 norm:0.0863083153963089 max memory_allocated 29275.14501953125 
[2025-02-20 17:56:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:5.232063293457031 norm:0.07652006298303604 max memory_allocated 29275.14501953125 
[2025-02-20 17:57:14 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:5.211984634399414 norm:0.06986984610557556 max memory_allocated 29275.14501953125 
[2025-02-20 17:58:03 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:5.193816184997559 norm:0.06840713322162628 max memory_allocated 29275.14501953125 
[2025-02-20 17:58:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:5.180126190185547 norm:0.06647991389036179 max memory_allocated 29275.14501953125 
[2025-02-20 17:59:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:5.16865348815918 norm:0.06595582515001297 max memory_allocated 29275.14501953125 
[2025-02-20 18:00:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:5.159767150878906 norm:0.06789150834083557 max memory_allocated 29275.14501953125 
[2025-02-20 18:01:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:5.156830787658691 norm:0.06936298310756683 max memory_allocated 29275.14501953125 
[2025-02-20 18:02:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:5.15477180480957 norm:0.07008275389671326 max memory_allocated 29275.14501953125 
[2025-02-20 18:02:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:5.147683620452881 norm:0.06484422087669373 max memory_allocated 29275.14501953125 
[2025-02-20 18:03:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:5.142230987548828 norm:0.062072377651929855 max memory_allocated 29275.14501953125 
[2025-02-20 18:04:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:5.143166542053223 norm:0.06416746973991394 max memory_allocated 29275.14501953125 
[2025-02-20 18:05:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:5.140327453613281 norm:0.06274781376123428 max memory_allocated 29275.14501953125 
[2025-02-20 18:05:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-20 18:05:45 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-20 18:06:34 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:9.806861877441406 norm:0.4423363208770752 max memory_allocated 29275.33251953125 
[2025-02-20 18:07:23 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:9.37714672088623 norm:0.365411639213562 max memory_allocated 29275.33251953125 
[2025-02-20 18:08:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:8.951984405517578 norm:0.3106802701950073 max memory_allocated 29275.33251953125 
[2025-02-20 18:09:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:8.70010757446289 norm:0.33084607124328613 max memory_allocated 29275.33251953125 
[2025-02-20 18:09:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:8.567022323608398 norm:0.31434354186058044 max memory_allocated 29275.33251953125 
[2025-02-20 18:10:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:8.478919982910156 norm:0.29584595561027527 max memory_allocated 29275.33251953125 
[2025-02-20 18:11:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:8.383332252502441 norm:0.3032551407814026 max memory_allocated 29275.33251953125 
[2025-02-20 18:12:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:8.303089141845703 norm:0.29259443283081055 max memory_allocated 29275.33251953125 
[2025-02-20 18:13:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:8.255640029907227 norm:0.2678737938404083 max memory_allocated 29275.33251953125 
[2025-02-20 18:13:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:8.21471118927002 norm:0.26040542125701904 max memory_allocated 29275.33251953125 
[2025-02-20 18:14:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:8.181172370910645 norm:0.2562568187713623 max memory_allocated 29275.33251953125 
[2025-02-20 18:15:36 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:8.153840065002441 norm:0.2518705129623413 max memory_allocated 29275.33251953125 
[2025-02-20 18:16:26 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:8.131818771362305 norm:0.24387744069099426 max memory_allocated 29275.33251953125 
[2025-02-20 18:17:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:8.117534637451172 norm:0.24085527658462524 max memory_allocated 29275.33251953125 
[2025-02-20 18:18:05 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:8.10746955871582 norm:0.24088618159294128 max memory_allocated 29275.33251953125 
[2025-02-20 18:18:54 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:8.086640357971191 norm:0.23513394594192505 max memory_allocated 29275.33251953125 
[2025-02-20 18:19:43 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:8.071976661682129 norm:0.22403165698051453 max memory_allocated 29275.33251953125 
[2025-02-20 18:20:32 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:8.075115203857422 norm:0.22900718450546265 max memory_allocated 29275.33251953125 
[2025-02-20 18:21:22 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:8.05923080444336 norm:0.2303830087184906 max memory_allocated 29275.33251953125 
[2025-02-20 18:22:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:8.046581268310547 norm:0.2238350510597229 max memory_allocated 29275.33251953125 
[2025-02-20 18:22:26 root] (main_calibration.py 365): INFO 40031.97243356705
[2025-02-20 18:23:29 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-20 18:25:22 root] (main_calibration.py 158): INFO wikitext2 : 7.5731425285339355
[2025-02-20 18:25:22 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-20 18:28:19 root] (main_calibration.py 158): INFO c4 : 10.688876152038574
[2025-02-20 20:25:09 root] (main_calibration.py 169): INFO {'wikitext2': 7.5731425285339355, 'c4': 10.688876152038574, 'results': {'winogrande': {'acc': 0.5619573796369376, 'acc_stderr': 0.013944181296470806}, 'piqa': {'acc': 0.6670293797606094, 'acc_stderr': 0.01099564882261907, 'acc_norm': 0.6692056583242655, 'acc_norm_stderr': 0.01097752058471444}, 'arc_challenge': {'acc': 0.27047781569965873, 'acc_stderr': 0.012980954547659554, 'acc_norm': 0.3165529010238908, 'acc_norm_stderr': 0.01359243151906808}, 'arc_easy': {'acc': 0.5345117845117845, 'acc_stderr': 0.010235314238969395, 'acc_norm': 0.44234006734006737, 'acc_norm_stderr': 0.01019133444422086}, 'boolq': {'acc': 0.6308868501529052, 'acc_stderr': 0.008440108625231398}, 'hellaswag': {'acc': 0.4872535351523601, 'acc_stderr': 0.004988159744742518, 'acc_norm': 0.6447918741286597, 'acc_norm_stderr': 0.0047759826503559165}}, 'versions': {'winogrande': 0, 'piqa': 0, 'arc_challenge': 0, 'arc_easy': 0, 'boolq': 1, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
