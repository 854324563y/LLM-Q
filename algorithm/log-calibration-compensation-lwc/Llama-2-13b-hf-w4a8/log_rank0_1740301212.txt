[2025-02-23 09:00:12 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-13b-hf-w4a8', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-13b-hf-w4a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-23 09:00:14 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-23 09:00:14 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-23 09:00:14 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-23 09:00:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-23 09:00:26 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:01:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.011576879769563675 norm:0.0153965400531888 max memory_allocated 29271.02001953125 
[2025-02-23 09:02:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.006536663044244051 norm:0.008162187412381172 max memory_allocated 29271.02001953125 
[2025-02-23 09:02:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.00471703615039587 norm:0.005599129479378462 max memory_allocated 29271.02001953125 
[2025-02-23 09:03:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0040421620942652225 norm:0.004169669467955828 max memory_allocated 29271.02001953125 
[2025-02-23 09:04:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0038503166288137436 norm:0.003583055455237627 max memory_allocated 29271.02001953125 
[2025-02-23 09:05:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0036669601686298847 norm:0.0031054019927978516 max memory_allocated 29271.02001953125 
[2025-02-23 09:06:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.003582392120733857 norm:0.0027186935767531395 max memory_allocated 29271.02001953125 
[2025-02-23 09:07:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.003516065888106823 norm:0.002324796747416258 max memory_allocated 29271.02001953125 
[2025-02-23 09:07:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.003464682959020138 norm:0.002015303820371628 max memory_allocated 29271.02001953125 
[2025-02-23 09:08:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.003357913112267852 norm:0.0017635237891227007 max memory_allocated 29271.02001953125 
[2025-02-23 09:09:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0033121018204838037 norm:0.0016697528772056103 max memory_allocated 29271.02001953125 
[2025-02-23 09:10:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.003325608093291521 norm:0.00162134924903512 max memory_allocated 29271.02001953125 
[2025-02-23 09:11:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.003309739287942648 norm:0.0014810709981247783 max memory_allocated 29271.02001953125 
[2025-02-23 09:11:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.00329217198304832 norm:0.001381825190037489 max memory_allocated 29271.02001953125 
[2025-02-23 09:12:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0032827677205204964 norm:0.0012804263969883323 max memory_allocated 29271.02001953125 
[2025-02-23 09:13:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0032862438820302486 norm:0.0011930072214454412 max memory_allocated 29271.02001953125 
[2025-02-23 09:14:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0032462216913700104 norm:0.0010585854761302471 max memory_allocated 29271.02001953125 
[2025-02-23 09:15:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0032784361392259598 norm:0.0011353329755365849 max memory_allocated 29271.02001953125 
[2025-02-23 09:16:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0033120049629360437 norm:0.0010334025137126446 max memory_allocated 29271.02001953125 
[2025-02-23 09:16:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.003303463803604245 norm:0.000996684655547142 max memory_allocated 29271.02001953125 
[2025-02-23 09:17:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-23 09:17:14 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:18:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.026768440380692482 norm:0.014773604460060596 max memory_allocated 29271.02001953125 
[2025-02-23 09:18:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.01698959432542324 norm:0.00925660040229559 max memory_allocated 29271.02001953125 
[2025-02-23 09:19:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.01282787136733532 norm:0.006282084621489048 max memory_allocated 29271.02001953125 
[2025-02-23 09:20:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.011614578776061535 norm:0.0048821428790688515 max memory_allocated 29271.02001953125 
[2025-02-23 09:21:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.011124620214104652 norm:0.004127390682697296 max memory_allocated 29271.02001953125 
[2025-02-23 09:22:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.010815638117492199 norm:0.0035040797665715218 max memory_allocated 29271.02001953125 
[2025-02-23 09:23:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.010591619648039341 norm:0.002978042233735323 max memory_allocated 29271.02001953125 
[2025-02-23 09:23:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.01038459874689579 norm:0.002573728794232011 max memory_allocated 29271.02001953125 
[2025-02-23 09:24:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.010247524827718735 norm:0.0022540828213095665 max memory_allocated 29271.02001953125 
[2025-02-23 09:25:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.010133770294487476 norm:0.0019910018891096115 max memory_allocated 29271.02001953125 
[2025-02-23 09:26:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.010057145729660988 norm:0.0017521092668175697 max memory_allocated 29271.02001953125 
[2025-02-23 09:27:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.009952818974852562 norm:0.0016020595794543624 max memory_allocated 29271.02001953125 
[2025-02-23 09:27:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.009939203038811684 norm:0.001635268097743392 max memory_allocated 29271.02001953125 
[2025-02-23 09:28:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.009899207390844822 norm:0.0016189380548894405 max memory_allocated 29271.02001953125 
[2025-02-23 09:29:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.009878807701170444 norm:0.0014496400253847241 max memory_allocated 29271.02001953125 
[2025-02-23 09:30:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.00981138739734888 norm:0.001418406143784523 max memory_allocated 29271.02001953125 
[2025-02-23 09:31:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.009833984076976776 norm:0.001289642183110118 max memory_allocated 29271.02001953125 
[2025-02-23 09:32:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.009755024686455727 norm:0.0013045264640823007 max memory_allocated 29271.02001953125 
[2025-02-23 09:32:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.009772366844117641 norm:0.0012342439731583 max memory_allocated 29271.02001953125 
[2025-02-23 09:33:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.009722234681248665 norm:0.001244227634742856 max memory_allocated 29271.02001953125 
[2025-02-23 09:34:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-23 09:34:07 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:34:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.028854303061962128 norm:0.013307686895132065 max memory_allocated 29271.02001953125 
[2025-02-23 09:35:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.020757894963026047 norm:0.008703658357262611 max memory_allocated 29271.02001953125 
[2025-02-23 09:36:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.016414685174822807 norm:0.005744012538343668 max memory_allocated 29271.02001953125 
[2025-02-23 09:37:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.015004022978246212 norm:0.004541059490293264 max memory_allocated 29271.02001953125 
[2025-02-23 09:38:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.014387615956366062 norm:0.003747241571545601 max memory_allocated 29271.02001953125 
[2025-02-23 09:39:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.013951505534350872 norm:0.0031915800645947456 max memory_allocated 29271.02001953125 
[2025-02-23 09:39:54 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.013615002855658531 norm:0.002750458661466837 max memory_allocated 29271.02001953125 
[2025-02-23 09:40:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.01335799228399992 norm:0.002340365666896105 max memory_allocated 29271.02001953125 
[2025-02-23 09:41:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.01308903656899929 norm:0.0019427065271884203 max memory_allocated 29271.02001953125 
[2025-02-23 09:42:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.013038315810263157 norm:0.0018656962784007192 max memory_allocated 29271.02001953125 
[2025-02-23 09:43:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.013019160367548466 norm:0.0018205982632935047 max memory_allocated 29271.02001953125 
[2025-02-23 09:44:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.01294251810759306 norm:0.001709247357212007 max memory_allocated 29271.02001953125 
[2025-02-23 09:44:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.012936092913150787 norm:0.0016009154496714473 max memory_allocated 29271.02001953125 
[2025-02-23 09:45:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.012892790138721466 norm:0.001570871565490961 max memory_allocated 29271.02001953125 
[2025-02-23 09:46:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.012929377146065235 norm:0.0014741828199476004 max memory_allocated 29271.02001953125 
[2025-02-23 09:47:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.01291561033576727 norm:0.001536163967102766 max memory_allocated 29271.02001953125 
[2025-02-23 09:48:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.012923949398100376 norm:0.001419090200215578 max memory_allocated 29271.02001953125 
[2025-02-23 09:48:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.012868549674749374 norm:0.0014662081375718117 max memory_allocated 29271.02001953125 
[2025-02-23 09:49:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.012791023589670658 norm:0.001232474111020565 max memory_allocated 29271.02001953125 
[2025-02-23 09:50:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.012822973541915417 norm:0.001271469984203577 max memory_allocated 29271.02001953125 
[2025-02-23 09:50:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-23 09:51:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.046911440789699554 norm:0.004658474586904049 max memory_allocated 29271.43798828125 
[2025-02-23 09:52:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.032297391444444656 norm:0.0017840361688286066 max memory_allocated 29271.43798828125 
[2025-02-23 09:53:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.024849385023117065 norm:0.0014478156808763742 max memory_allocated 29271.43798828125 
[2025-02-23 09:54:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.02269114926457405 norm:0.0013095114845782518 max memory_allocated 29271.43798828125 
[2025-02-23 09:55:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.02206336334347725 norm:0.0015593350399285555 max memory_allocated 29271.43798828125 
[2025-02-23 09:55:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.021176286041736603 norm:0.0011394607136026025 max memory_allocated 29271.43798828125 
[2025-02-23 09:56:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.020869070664048195 norm:0.0011720338370651007 max memory_allocated 29271.43798828125 
[2025-02-23 09:57:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.020946934819221497 norm:0.002086990512907505 max memory_allocated 29271.43798828125 
[2025-02-23 09:58:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0199652761220932 norm:0.0010572989704087377 max memory_allocated 29271.43798828125 
[2025-02-23 09:59:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.020660538226366043 norm:0.0013799442676827312 max memory_allocated 29271.43798828125 
[2025-02-23 10:00:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.01952025666832924 norm:0.0008189039654098451 max memory_allocated 29271.43798828125 
[2025-02-23 10:00:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.01981814019382 norm:0.0007788337534293532 max memory_allocated 29271.43798828125 
[2025-02-23 10:01:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.019837066531181335 norm:0.0008340023341588676 max memory_allocated 29271.43798828125 
[2025-02-23 10:02:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.01966080069541931 norm:0.0009340521646663547 max memory_allocated 29271.43798828125 
[2025-02-23 10:03:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.018907366320490837 norm:0.0007593330810777843 max memory_allocated 29271.43798828125 
[2025-02-23 10:04:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.019796811044216156 norm:0.0008705219370312989 max memory_allocated 29271.43798828125 
[2025-02-23 10:05:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.019714904949069023 norm:0.0010546048870310187 max memory_allocated 29271.43798828125 
[2025-02-23 10:05:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.019500406458973885 norm:0.0008692487026564777 max memory_allocated 29271.43798828125 
[2025-02-23 10:06:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.020162563771009445 norm:0.001124743139371276 max memory_allocated 29271.43798828125 
[2025-02-23 10:07:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.019010979682207108 norm:0.0008270337712019682 max memory_allocated 29271.43798828125 
[2025-02-23 10:07:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-23 10:08:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.03579793870449066 norm:0.0006856960244476795 max memory_allocated 29271.43798828125 
[2025-02-23 10:09:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.027871739119291306 norm:0.00028910840046592057 max memory_allocated 29271.43798828125 
[2025-02-23 10:10:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.023560822010040283 norm:0.00017429376021027565 max memory_allocated 29271.43798828125 
[2025-02-23 10:11:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.022079844027757645 norm:0.00015383862773887813 max memory_allocated 29271.43798828125 
[2025-02-23 10:12:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.02134673111140728 norm:0.00014616912812925875 max memory_allocated 29271.43798828125 
[2025-02-23 10:12:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.020818760618567467 norm:0.00013588373258244246 max memory_allocated 29271.43798828125 
[2025-02-23 10:13:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.020492935553193092 norm:0.00013114158355165273 max memory_allocated 29271.43798828125 
[2025-02-23 10:14:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.020284565165638924 norm:0.00011968840408371761 max memory_allocated 29271.43798828125 
[2025-02-23 10:15:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.020187482237815857 norm:0.00012013022205792367 max memory_allocated 29271.43798828125 
[2025-02-23 10:16:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.020137876272201538 norm:0.00012466988118831068 max memory_allocated 29271.43798828125 
[2025-02-23 10:16:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.020107455551624298 norm:0.00012790595064871013 max memory_allocated 29271.43798828125 
[2025-02-23 10:17:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.020099977031350136 norm:0.00011099960829596967 max memory_allocated 29271.43798828125 
[2025-02-23 10:18:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.020091306418180466 norm:0.00010772686800919473 max memory_allocated 29271.43798828125 
[2025-02-23 10:19:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0200542863458395 norm:0.0001042242074618116 max memory_allocated 29271.43798828125 
[2025-02-23 10:20:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.020021960139274597 norm:0.00010425815708003938 max memory_allocated 29271.43798828125 
[2025-02-23 10:21:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.020038669928908348 norm:0.00010356344864703715 max memory_allocated 29271.43798828125 
[2025-02-23 10:21:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.020016316324472427 norm:0.00010648221359588206 max memory_allocated 29271.43798828125 
[2025-02-23 10:22:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.020018160343170166 norm:0.00011240047024330124 max memory_allocated 29271.43798828125 
[2025-02-23 10:23:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.020036369562149048 norm:0.00011898275988642126 max memory_allocated 29271.43798828125 
[2025-02-23 10:24:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.020039064809679985 norm:0.00011959810217376798 max memory_allocated 29271.43798828125 
[2025-02-23 10:24:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-23 10:25:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.04000124707818031 norm:0.0011260537430644035 max memory_allocated 29271.43798828125 
[2025-02-23 10:26:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.03044896014034748 norm:0.00042926036985591054 max memory_allocated 29271.43798828125 
[2025-02-23 10:27:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.02570180781185627 norm:0.00021906256733927876 max memory_allocated 29271.43798828125 
[2025-02-23 10:28:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.024112924933433533 norm:0.00015650838031433523 max memory_allocated 29271.43798828125 
[2025-02-23 10:28:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.023256376385688782 norm:0.00013542143278755248 max memory_allocated 29271.43798828125 
[2025-02-23 10:29:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.02271805703639984 norm:0.00012262712698429823 max memory_allocated 29271.43798828125 
[2025-02-23 10:30:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.022362947463989258 norm:0.00011310674017295241 max memory_allocated 29271.43798828125 
[2025-02-23 10:31:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0221769567579031 norm:0.00010852148989215493 max memory_allocated 29271.43798828125 
[2025-02-23 10:32:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.022051988169550896 norm:0.00010141228267457336 max memory_allocated 29271.43798828125 
[2025-02-23 10:33:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.021996431052684784 norm:0.00010072276927530766 max memory_allocated 29271.43798828125 
[2025-02-23 10:33:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.021936319768428802 norm:9.645683167036623e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:34:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.02188580483198166 norm:9.482329915044829e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:35:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.02186001092195511 norm:9.205251990351826e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:36:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.021843448281288147 norm:8.913275087252259e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:37:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.021810883656144142 norm:9.005676110973582e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:37:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.02177489921450615 norm:8.87348796823062e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:38:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.02177334576845169 norm:9.423791925655678e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:39:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.02175823599100113 norm:8.868623262969777e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:40:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.021741565316915512 norm:8.886600699042901e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:41:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.021747590973973274 norm:8.900226384866983e-05 max memory_allocated 29271.43798828125 
[2025-02-23 10:41:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-23 10:42:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.043811798095703125 norm:0.0011730343103408813 max memory_allocated 29272.00048828125 
[2025-02-23 10:43:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.03398195654153824 norm:0.0004509017162490636 max memory_allocated 29272.00048828125 
[2025-02-23 10:44:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.028506362810730934 norm:0.0002610194496810436 max memory_allocated 29272.00048828125 
[2025-02-23 10:44:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.026792732998728752 norm:0.00019872587290592492 max memory_allocated 29272.00048828125 
[2025-02-23 10:45:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.02579580992460251 norm:0.0001721749868011102 max memory_allocated 29272.00048828125 
[2025-02-23 10:46:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.025196034461259842 norm:0.0001518366188975051 max memory_allocated 29272.00048828125 
[2025-02-23 10:47:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.024865642189979553 norm:0.00013972051965538412 max memory_allocated 29272.00048828125 
[2025-02-23 10:48:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.024673955515027046 norm:0.0001360045571345836 max memory_allocated 29272.00048828125 
[2025-02-23 10:49:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.024549007415771484 norm:0.00012632724246941507 max memory_allocated 29272.00048828125 
[2025-02-23 10:49:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.024463538080453873 norm:0.0001263697922695428 max memory_allocated 29272.00048828125 
[2025-02-23 10:50:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0244002602994442 norm:0.0001266094477614388 max memory_allocated 29272.00048828125 
[2025-02-23 10:51:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0243376512080431 norm:0.00011586995969992131 max memory_allocated 29272.00048828125 
[2025-02-23 10:52:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.02430255152285099 norm:0.00011726281809387729 max memory_allocated 29272.00048828125 
[2025-02-23 10:53:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.024284832179546356 norm:0.00011747599637601525 max memory_allocated 29272.00048828125 
[2025-02-23 10:53:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.02424350380897522 norm:0.00011398137576179579 max memory_allocated 29272.00048828125 
[2025-02-23 10:54:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.02420888841152191 norm:0.00010652865603333339 max memory_allocated 29272.00048828125 
[2025-02-23 10:55:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.024193594232201576 norm:0.00010788446525111794 max memory_allocated 29272.00048828125 
[2025-02-23 10:56:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.02417600527405739 norm:0.0001068144993041642 max memory_allocated 29272.00048828125 
[2025-02-23 10:57:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.02417389303445816 norm:0.00010039578774012625 max memory_allocated 29272.00048828125 
[2025-02-23 10:58:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.024165727198123932 norm:0.00010255983215756714 max memory_allocated 29272.00048828125 
[2025-02-23 10:58:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-23 10:59:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.054672837257385254 norm:0.001580148353241384 max memory_allocated 29272.18798828125 
[2025-02-23 11:00:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.040296006947755814 norm:0.0005925491568632424 max memory_allocated 29272.18798828125 
[2025-02-23 11:00:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.03328727185726166 norm:0.00031268742168322206 max memory_allocated 29272.18798828125 
[2025-02-23 11:01:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.031242886558175087 norm:0.0002453236375004053 max memory_allocated 29272.18798828125 
[2025-02-23 11:02:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.030181996524333954 norm:0.00021853797079529613 max memory_allocated 29272.18798828125 
[2025-02-23 11:03:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.029463404789566994 norm:0.0001984917907975614 max memory_allocated 29272.18798828125 
[2025-02-23 11:04:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0290633924305439 norm:0.00018301642558071762 max memory_allocated 29272.18798828125 
[2025-02-23 11:05:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.02880486100912094 norm:0.00017630580987315625 max memory_allocated 29272.18798828125 
[2025-02-23 11:05:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.028651466593146324 norm:0.00017997511895373464 max memory_allocated 29272.18798828125 
[2025-02-23 11:06:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.028548285365104675 norm:0.00017135735834017396 max memory_allocated 29272.18798828125 
[2025-02-23 11:07:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.028429899364709854 norm:0.00016560569929424673 max memory_allocated 29272.18798828125 
[2025-02-23 11:08:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.02838263288140297 norm:0.00016746488108765334 max memory_allocated 29272.18798828125 
[2025-02-23 11:09:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.028324555605649948 norm:0.00015630955749657005 max memory_allocated 29272.18798828125 
[2025-02-23 11:09:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.028269769623875618 norm:0.00016080735076684505 max memory_allocated 29272.18798828125 
[2025-02-23 11:10:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.028232570737600327 norm:0.00014828922576270998 max memory_allocated 29272.18798828125 
[2025-02-23 11:11:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0282143447548151 norm:0.0001585145655553788 max memory_allocated 29272.18798828125 
[2025-02-23 11:12:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.02817695587873459 norm:0.00016171897004824132 max memory_allocated 29272.18798828125 
[2025-02-23 11:13:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.028139742091298103 norm:0.00015377085946965963 max memory_allocated 29272.18798828125 
[2025-02-23 11:14:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.028117502108216286 norm:0.00014724193897563964 max memory_allocated 29272.18798828125 
[2025-02-23 11:14:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.028113605454564095 norm:0.00014838430797681212 max memory_allocated 29272.18798828125 
[2025-02-23 11:15:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-23 11:16:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.05265487730503082 norm:0.0014144122833386064 max memory_allocated 29272.37548828125 
[2025-02-23 11:16:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.04157276824116707 norm:0.0005916150403209031 max memory_allocated 29272.37548828125 
[2025-02-23 11:17:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.03538687527179718 norm:0.0003307711740490049 max memory_allocated 29272.37548828125 
[2025-02-23 11:18:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.03335068002343178 norm:0.0002542090369388461 max memory_allocated 29272.37548828125 
[2025-02-23 11:19:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.03216060250997543 norm:0.00021366498549468815 max memory_allocated 29272.37548828125 
[2025-02-23 11:20:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.03147781640291214 norm:0.00018380979599896818 max memory_allocated 29272.37548828125 
[2025-02-23 11:21:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.031081344932317734 norm:0.00017031078459694982 max memory_allocated 29272.37548828125 
[2025-02-23 11:21:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.03083682805299759 norm:0.00015977701696101576 max memory_allocated 29272.37548828125 
[2025-02-23 11:22:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.030690107494592667 norm:0.00014774248120374978 max memory_allocated 29272.37548828125 
[2025-02-23 11:23:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.03055909276008606 norm:0.00014005416596774012 max memory_allocated 29272.37548828125 
[2025-02-23 11:24:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.030450424179434776 norm:0.00013010836846660823 max memory_allocated 29272.37548828125 
[2025-02-23 11:25:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.03036465309560299 norm:0.00012555401190184057 max memory_allocated 29272.37548828125 
[2025-02-23 11:25:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.030313219875097275 norm:0.00012045318726450205 max memory_allocated 29272.37548828125 
[2025-02-23 11:26:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.030264457687735558 norm:0.00012066861381754279 max memory_allocated 29272.37548828125 
[2025-02-23 11:27:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.030253726989030838 norm:0.00011825902765849605 max memory_allocated 29272.37548828125 
[2025-02-23 11:28:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.030215293169021606 norm:0.00011702234041877091 max memory_allocated 29272.37548828125 
[2025-02-23 11:29:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.030195564031600952 norm:0.00011545633606147021 max memory_allocated 29272.37548828125 
[2025-02-23 11:30:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.030174147337675095 norm:0.00011561610153876245 max memory_allocated 29272.37548828125 
[2025-02-23 11:30:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.030164070427417755 norm:0.00011414327309466898 max memory_allocated 29272.37548828125 
[2025-02-23 11:31:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.030144894495606422 norm:0.0001133306504925713 max memory_allocated 29272.37548828125 
[2025-02-23 11:32:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-23 11:32:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.07755487412214279 norm:0.003232436254620552 max memory_allocated 29272.56298828125 
[2025-02-23 11:33:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.05289134383201599 norm:0.0012562476331368089 max memory_allocated 29272.56298828125 
[2025-02-23 11:34:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.041804783046245575 norm:0.00057271181140095 max memory_allocated 29272.56298828125 
[2025-02-23 11:35:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.03861168771982193 norm:0.00040120581979863346 max memory_allocated 29272.56298828125 
[2025-02-23 11:36:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.03701417148113251 norm:0.0003294197376817465 max memory_allocated 29272.56298828125 
[2025-02-23 11:37:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.03605812042951584 norm:0.0002930087794084102 max memory_allocated 29272.56298828125 
[2025-02-23 11:37:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.03543824702501297 norm:0.00027857982786372304 max memory_allocated 29272.56298828125 
[2025-02-23 11:38:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.03504554182291031 norm:0.0002632346877362579 max memory_allocated 29272.56298828125 
[2025-02-23 11:39:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.034739233553409576 norm:0.00024680772912688553 max memory_allocated 29272.56298828125 
[2025-02-23 11:40:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.03450086712837219 norm:0.00023236738343257457 max memory_allocated 29272.56298828125 
[2025-02-23 11:41:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.034350551664829254 norm:0.00021881269640289247 max memory_allocated 29272.56298828125 
[2025-02-23 11:42:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.034219615161418915 norm:0.00021282577654346824 max memory_allocated 29272.56298828125 
[2025-02-23 11:42:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.03411956876516342 norm:0.00019948705448769033 max memory_allocated 29272.56298828125 
[2025-02-23 11:43:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.033978451043367386 norm:0.00019768108904827386 max memory_allocated 29272.56298828125 
[2025-02-23 11:44:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0338931567966938 norm:0.00018840980192180723 max memory_allocated 29272.56298828125 
[2025-02-23 11:45:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.03381025791168213 norm:0.0001824068312998861 max memory_allocated 29272.56298828125 
[2025-02-23 11:46:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.033739570528268814 norm:0.00017779847257770598 max memory_allocated 29272.56298828125 
[2025-02-23 11:46:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.03366715833544731 norm:0.0001669164194026962 max memory_allocated 29272.56298828125 
[2025-02-23 11:47:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.03362230956554413 norm:0.0001684920280240476 max memory_allocated 29272.56298828125 
[2025-02-23 11:48:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.03359495848417282 norm:0.00016991599113680422 max memory_allocated 29272.56298828125 
[2025-02-23 11:48:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-23 11:49:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.06376896798610687 norm:0.0014154050732031465 max memory_allocated 29272.75048828125 
[2025-02-23 11:50:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.04999591410160065 norm:0.0006692992174066603 max memory_allocated 29272.75048828125 
[2025-02-23 11:51:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.042407117784023285 norm:0.0003890036605298519 max memory_allocated 29272.75048828125 
[2025-02-23 11:52:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.03968663513660431 norm:0.00027465555467642844 max memory_allocated 29272.75048828125 
[2025-02-23 11:53:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.038351885974407196 norm:0.00022155452461447567 max memory_allocated 29272.75048828125 
[2025-02-23 11:53:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0376010499894619 norm:0.00019473893917165697 max memory_allocated 29272.75048828125 
[2025-02-23 11:54:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.03715379536151886 norm:0.00017708470113575459 max memory_allocated 29272.75048828125 
[2025-02-23 11:55:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.036917805671691895 norm:0.00016328477067872882 max memory_allocated 29272.75048828125 
[2025-02-23 11:56:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.03675314784049988 norm:0.00015189562691375613 max memory_allocated 29272.75048828125 
[2025-02-23 11:57:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.03662458434700966 norm:0.00014267755614127964 max memory_allocated 29272.75048828125 
[2025-02-23 11:58:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.036525413393974304 norm:0.00013926869723945856 max memory_allocated 29272.75048828125 
[2025-02-23 11:58:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.03645078465342522 norm:0.00013188819866627455 max memory_allocated 29272.75048828125 
[2025-02-23 11:59:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0364089198410511 norm:0.00013100597425363958 max memory_allocated 29272.75048828125 
[2025-02-23 12:00:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.03634362295269966 norm:0.00012342794798314571 max memory_allocated 29272.75048828125 
[2025-02-23 12:01:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.03629232197999954 norm:0.0001192224444821477 max memory_allocated 29272.75048828125 
[2025-02-23 12:02:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.036246076226234436 norm:0.00011788038682425395 max memory_allocated 29272.75048828125 
[2025-02-23 12:02:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.03621813282370567 norm:0.00011798769992310554 max memory_allocated 29272.75048828125 
[2025-02-23 12:03:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.03618583455681801 norm:0.00011408796854084358 max memory_allocated 29272.75048828125 
[2025-02-23 12:04:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.03614510968327522 norm:0.00011119962437078357 max memory_allocated 29272.75048828125 
[2025-02-23 12:05:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.03609630838036537 norm:0.00011269353854004294 max memory_allocated 29272.75048828125 
[2025-02-23 12:05:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-23 12:06:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.06306668370962143 norm:0.0011702380143105984 max memory_allocated 29272.93798828125 
[2025-02-23 12:07:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0507567822933197 norm:0.0005395941552706063 max memory_allocated 29272.93798828125 
[2025-02-23 12:08:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.04372258111834526 norm:0.0003032782697118819 max memory_allocated 29272.93798828125 
[2025-02-23 12:09:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.04136108234524727 norm:0.0002160590374842286 max memory_allocated 29272.93798828125 
[2025-02-23 12:09:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.040297918021678925 norm:0.00018180834013037384 max memory_allocated 29272.93798828125 
[2025-02-23 12:10:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.03964129835367203 norm:0.00016147896531037986 max memory_allocated 29272.93798828125 
[2025-02-23 12:11:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.039249811321496964 norm:0.00015015115786809474 max memory_allocated 29272.93798828125 
[2025-02-23 12:12:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.039046768099069595 norm:0.00014383716916199774 max memory_allocated 29272.93798828125 
[2025-02-23 12:13:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.03888605535030365 norm:0.00013483619841281325 max memory_allocated 29272.93798828125 
[2025-02-23 12:14:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.03879585862159729 norm:0.00013123919779900461 max memory_allocated 29272.93798828125 
[2025-02-23 12:14:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.038714099675416946 norm:0.0001277312112506479 max memory_allocated 29272.93798828125 
[2025-02-23 12:15:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.03864586725831032 norm:0.00012131499534007162 max memory_allocated 29272.93798828125 
[2025-02-23 12:16:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.03858523070812225 norm:0.00011355341121088713 max memory_allocated 29272.93798828125 
[2025-02-23 12:17:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.03852351754903793 norm:0.00011280475155217573 max memory_allocated 29272.93798828125 
[2025-02-23 12:18:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.03845367208123207 norm:0.00011114712106063962 max memory_allocated 29272.93798828125 
[2025-02-23 12:19:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0384298712015152 norm:0.00010914595623034984 max memory_allocated 29272.93798828125 
[2025-02-23 12:19:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.038416534662246704 norm:0.0001082179369404912 max memory_allocated 29272.93798828125 
[2025-02-23 12:20:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0383627749979496 norm:0.00010843294148799032 max memory_allocated 29272.93798828125 
[2025-02-23 12:21:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.038339439779520035 norm:0.00010489726264495403 max memory_allocated 29272.93798828125 
[2025-02-23 12:22:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.038327306509017944 norm:0.00010620787361403927 max memory_allocated 29272.93798828125 
[2025-02-23 12:22:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-23 12:23:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0680905431509018 norm:0.001379066496156156 max memory_allocated 29273.12548828125 
[2025-02-23 12:24:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0539976991713047 norm:0.000622797233518213 max memory_allocated 29273.12548828125 
[2025-02-23 12:25:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0459299273788929 norm:0.00033533366513438523 max memory_allocated 29273.12548828125 
[2025-02-23 12:25:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.04336990416049957 norm:0.00023177513503469527 max memory_allocated 29273.12548828125 
[2025-02-23 12:26:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.04221147298812866 norm:0.000191819533938542 max memory_allocated 29273.12548828125 
[2025-02-23 12:27:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.04152322933077812 norm:0.0001701154833426699 max memory_allocated 29273.12548828125 
[2025-02-23 12:28:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.04111108183860779 norm:0.00015290893497876823 max memory_allocated 29273.12548828125 
[2025-02-23 12:29:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0408770777285099 norm:0.00014292758714873344 max memory_allocated 29273.12548828125 
[2025-02-23 12:30:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.04069337621331215 norm:0.00013310230860952288 max memory_allocated 29273.12548828125 
[2025-02-23 12:30:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.04053798317909241 norm:0.00012751219037454575 max memory_allocated 29273.12548828125 
[2025-02-23 12:31:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.040416471660137177 norm:0.00011622491001617163 max memory_allocated 29273.12548828125 
[2025-02-23 12:32:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.04034731909632683 norm:0.00011540892592165619 max memory_allocated 29273.12548828125 
[2025-02-23 12:33:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.04028201848268509 norm:0.0001111690144171007 max memory_allocated 29273.12548828125 
[2025-02-23 12:34:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.040229737758636475 norm:0.00010580598609521985 max memory_allocated 29273.12548828125 
[2025-02-23 12:35:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.04020144045352936 norm:0.0001051498984452337 max memory_allocated 29273.12548828125 
[2025-02-23 12:35:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.040155597031116486 norm:0.00010097668564412743 max memory_allocated 29273.12548828125 
[2025-02-23 12:36:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.04010234400629997 norm:9.76626033661887e-05 max memory_allocated 29273.12548828125 
[2025-02-23 12:37:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.04005136340856552 norm:9.542253974359483e-05 max memory_allocated 29273.12548828125 
[2025-02-23 12:38:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.040023960173130035 norm:9.40706449910067e-05 max memory_allocated 29273.12548828125 
[2025-02-23 12:39:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.040008366107940674 norm:9.033686365000904e-05 max memory_allocated 29273.12548828125 
[2025-02-23 12:39:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-23 12:40:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.06635747849941254 norm:0.0012676327023655176 max memory_allocated 29273.31298828125 
[2025-02-23 12:41:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.05450909957289696 norm:0.0005939365364611149 max memory_allocated 29273.31298828125 
[2025-02-23 12:41:58 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.04757548123598099 norm:0.000349608511896804 max memory_allocated 29273.31298828125 
[2025-02-23 12:42:47 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.045138195157051086 norm:0.0002571656077634543 max memory_allocated 29273.31298828125 
[2025-02-23 12:43:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.043866727501153946 norm:0.00021527352510020137 max memory_allocated 29273.31298828125 
[2025-02-23 12:44:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.043159421533346176 norm:0.0001906143588712439 max memory_allocated 29273.31298828125 
[2025-02-23 12:45:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.04274137318134308 norm:0.0001814335846574977 max memory_allocated 29273.31298828125 
[2025-02-23 12:46:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0424540713429451 norm:0.0001711469958536327 max memory_allocated 29273.31298828125 
[2025-02-23 12:46:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.04228280484676361 norm:0.00016195733041968197 max memory_allocated 29273.31298828125 
[2025-02-23 12:47:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.04212980717420578 norm:0.00014840293442830443 max memory_allocated 29273.31298828125 
[2025-02-23 12:48:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.04204010218381882 norm:0.00013953181041870266 max memory_allocated 29273.31298828125 
[2025-02-23 12:49:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.04193459078669548 norm:0.0001320696756010875 max memory_allocated 29273.31298828125 
[2025-02-23 12:50:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.04186621308326721 norm:0.00012703757965937257 max memory_allocated 29273.31298828125 
[2025-02-23 12:51:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.04180978611111641 norm:0.0001203077263198793 max memory_allocated 29273.31298828125 
[2025-02-23 12:51:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.041725967079401016 norm:0.00011315212759654969 max memory_allocated 29273.31298828125 
[2025-02-23 12:52:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.04165801405906677 norm:0.00011194098624400795 max memory_allocated 29273.31298828125 
[2025-02-23 12:53:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.04162369668483734 norm:0.00010551439481787384 max memory_allocated 29273.31298828125 
[2025-02-23 12:54:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.04157866910099983 norm:0.00010016448504757136 max memory_allocated 29273.31298828125 
[2025-02-23 12:55:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.041565265506505966 norm:9.875603427644819e-05 max memory_allocated 29273.31298828125 
[2025-02-23 12:55:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.04154415428638458 norm:9.792511991690844e-05 max memory_allocated 29273.31298828125 
[2025-02-23 12:56:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-23 12:57:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.06890106201171875 norm:0.0012268334394320846 max memory_allocated 29273.50048828125 
[2025-02-23 12:57:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.05712530016899109 norm:0.0006284194532781839 max memory_allocated 29273.50048828125 
[2025-02-23 12:58:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.04970795288681984 norm:0.000357183744199574 max memory_allocated 29273.50048828125 
[2025-02-23 12:59:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.047039736062288284 norm:0.00023973816132638603 max memory_allocated 29273.50048828125 
[2025-02-23 13:00:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.04588717594742775 norm:0.00019511213758960366 max memory_allocated 29273.50048828125 
[2025-02-23 13:01:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.04523131996393204 norm:0.00017337509780190885 max memory_allocated 29273.50048828125 
[2025-02-23 13:02:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.04480433836579323 norm:0.00015876781253609806 max memory_allocated 29273.50048828125 
[2025-02-23 13:02:55 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.04450519382953644 norm:0.00014570140046998858 max memory_allocated 29273.50048828125 
[2025-02-23 13:03:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.04432431608438492 norm:0.00013691367348656058 max memory_allocated 29273.50048828125 
[2025-02-23 13:04:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.04420355334877968 norm:0.0001295640249736607 max memory_allocated 29273.50048828125 
[2025-02-23 13:05:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.044108372181653976 norm:0.00011961920245084912 max memory_allocated 29273.50048828125 
[2025-02-23 13:06:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.04400206357240677 norm:0.00011234359408263117 max memory_allocated 29273.50048828125 
[2025-02-23 13:07:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.04390927776694298 norm:0.00010719002602854744 max memory_allocated 29273.50048828125 
[2025-02-23 13:07:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.04384535923600197 norm:0.00010504855890758336 max memory_allocated 29273.50048828125 
[2025-02-23 13:08:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.04381730034947395 norm:9.771953045856208e-05 max memory_allocated 29273.50048828125 
[2025-02-23 13:09:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.04378875717520714 norm:9.606865933164954e-05 max memory_allocated 29273.50048828125 
[2025-02-23 13:10:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.04372360184788704 norm:9.401559509569779e-05 max memory_allocated 29273.50048828125 
[2025-02-23 13:11:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.04368476942181587 norm:9.064516052603722e-05 max memory_allocated 29273.50048828125 
[2025-02-23 13:11:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.04364009201526642 norm:8.640799205750227e-05 max memory_allocated 29273.50048828125 
[2025-02-23 13:12:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.043611880391836166 norm:8.576698019169271e-05 max memory_allocated 29273.50048828125 
[2025-02-23 13:13:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-23 13:14:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.06521202623844147 norm:0.0009803090943023562 max memory_allocated 29273.68798828125 
[2025-02-23 13:14:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.05583616718649864 norm:0.00046751333866268396 max memory_allocated 29273.68798828125 
[2025-02-23 13:15:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.04932596534490585 norm:0.0002678636519704014 max memory_allocated 29273.68798828125 
[2025-02-23 13:16:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.04711063951253891 norm:0.0001897350448416546 max memory_allocated 29273.68798828125 
[2025-02-23 13:17:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.04608742147684097 norm:0.00016179952945094556 max memory_allocated 29273.68798828125 
[2025-02-23 13:18:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0454789362847805 norm:0.00014610483776777983 max memory_allocated 29273.68798828125 
[2025-02-23 13:19:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.04509532451629639 norm:0.00012978841550648212 max memory_allocated 29273.68798828125 
[2025-02-23 13:19:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0448741689324379 norm:0.00012141745537519455 max memory_allocated 29273.68798828125 
[2025-02-23 13:20:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.04472513869404793 norm:0.00011290328984614462 max memory_allocated 29273.68798828125 
[2025-02-23 13:21:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.04459232836961746 norm:0.00010753964306786656 max memory_allocated 29273.68798828125 
[2025-02-23 13:22:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.044500868767499924 norm:0.0001029680497595109 max memory_allocated 29273.68798828125 
[2025-02-23 13:23:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.04441593214869499 norm:9.958866576198488e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:23:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.04435849189758301 norm:9.460423461860046e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:24:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.04430694878101349 norm:9.176608000416309e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:25:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.04425923526287079 norm:8.657759462948889e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:26:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.044226355850696564 norm:8.377950871363282e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:27:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.044183436781167984 norm:8.12951911939308e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:28:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.044161610305309296 norm:8.14031736808829e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:28:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.04414058476686478 norm:8.02420690888539e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:29:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.04410539194941521 norm:8.030653407331556e-05 max memory_allocated 29273.68798828125 
[2025-02-23 13:29:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-23 13:30:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.07101734727621078 norm:0.0015339868841692805 max memory_allocated 29273.87548828125 
[2025-02-23 13:31:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.059178829193115234 norm:0.0006653536111116409 max memory_allocated 29273.87548828125 
[2025-02-23 13:32:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.051606032997369766 norm:0.0003681292000692338 max memory_allocated 29273.87548828125 
[2025-02-23 13:33:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.049047522246837616 norm:0.0002543223381508142 max memory_allocated 29273.87548828125 
[2025-02-23 13:34:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.047838371247053146 norm:0.00021087682398501784 max memory_allocated 29273.87548828125 
[2025-02-23 13:35:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.047105759382247925 norm:0.00019036259618587792 max memory_allocated 29273.87548828125 
[2025-02-23 13:35:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.04667752981185913 norm:0.00017943458806257695 max memory_allocated 29273.87548828125 
[2025-02-23 13:36:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.046361736953258514 norm:0.0001713362435111776 max memory_allocated 29273.87548828125 
[2025-02-23 13:37:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.046145249158144 norm:0.00015764603449497372 max memory_allocated 29273.87548828125 
[2025-02-23 13:38:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.04597137123346329 norm:0.00014770332199987024 max memory_allocated 29273.87548828125 
[2025-02-23 13:39:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.04585753008723259 norm:0.00014193943934515119 max memory_allocated 29273.87548828125 
[2025-02-23 13:39:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.0457560196518898 norm:0.00013486944953911006 max memory_allocated 29273.87548828125 
[2025-02-23 13:40:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.04565667733550072 norm:0.00012863273150287569 max memory_allocated 29273.87548828125 
[2025-02-23 13:41:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.045572344213724136 norm:0.00012352445628494024 max memory_allocated 29273.87548828125 
[2025-02-23 13:42:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.04548554867506027 norm:0.00011892538896063343 max memory_allocated 29273.87548828125 
[2025-02-23 13:43:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.04542982205748558 norm:0.00011248118244111538 max memory_allocated 29273.87548828125 
[2025-02-23 13:44:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.04538794606924057 norm:0.0001087789423763752 max memory_allocated 29273.87548828125 
[2025-02-23 13:44:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.045346323400735855 norm:0.0001072006270987913 max memory_allocated 29273.87548828125 
[2025-02-23 13:45:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.045318715274333954 norm:0.00010241459676763043 max memory_allocated 29273.87548828125 
[2025-02-23 13:46:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.04527938365936279 norm:9.859984857030213e-05 max memory_allocated 29273.87548828125 
[2025-02-23 13:46:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-23 13:47:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.06438758224248886 norm:0.0008285765652544796 max memory_allocated 29274.06298828125 
[2025-02-23 13:48:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.056159116327762604 norm:0.00036608101800084114 max memory_allocated 29274.06298828125 
[2025-02-23 13:49:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.050642624497413635 norm:0.00022816676937509328 max memory_allocated 29274.06298828125 
[2025-02-23 13:50:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.04875196889042854 norm:0.0001712241064524278 max memory_allocated 29274.06298828125 
[2025-02-23 13:51:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.04777916520833969 norm:0.0001495170290581882 max memory_allocated 29274.06298828125 
[2025-02-23 13:51:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.04716763645410538 norm:0.00013888506509829313 max memory_allocated 29274.06298828125 
[2025-02-23 13:52:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.046785444021224976 norm:0.0001289695646846667 max memory_allocated 29274.06298828125 
[2025-02-23 13:53:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.046566300094127655 norm:0.00012154728756286204 max memory_allocated 29274.06298828125 
[2025-02-23 13:54:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.04639515280723572 norm:0.00011451033788034692 max memory_allocated 29274.06298828125 
[2025-02-23 13:55:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.04627915099263191 norm:0.00010885001393035054 max memory_allocated 29274.06298828125 
[2025-02-23 13:55:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.0461924709379673 norm:0.00010570917220320553 max memory_allocated 29274.06298828125 
[2025-02-23 13:56:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.04611164703965187 norm:0.00010035128070740029 max memory_allocated 29274.06298828125 
[2025-02-23 13:57:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.046051301062107086 norm:9.629678970668465e-05 max memory_allocated 29274.06298828125 
[2025-02-23 13:58:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.046020057052373886 norm:9.526589565211907e-05 max memory_allocated 29274.06298828125 
[2025-02-23 13:59:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.04595406726002693 norm:9.123922063736245e-05 max memory_allocated 29274.06298828125 
[2025-02-23 14:00:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.04591358080506325 norm:8.518202957930043e-05 max memory_allocated 29274.06298828125 
[2025-02-23 14:00:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.04588531702756882 norm:8.340257772943005e-05 max memory_allocated 29274.06298828125 
[2025-02-23 14:01:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.04587390273809433 norm:8.206011261790991e-05 max memory_allocated 29274.06298828125 
[2025-02-23 14:02:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.045859962701797485 norm:8.068284660112113e-05 max memory_allocated 29274.06298828125 
[2025-02-23 14:03:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.04583904519677162 norm:7.966734119690955e-05 max memory_allocated 29274.06298828125 
[2025-02-23 14:03:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-23 14:04:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.0635194480419159 norm:0.0008414454641751945 max memory_allocated 29274.25048828125 
[2025-02-23 14:05:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.05579884350299835 norm:0.00033691537100821733 max memory_allocated 29274.25048828125 
[2025-02-23 14:06:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.05106043443083763 norm:0.00020967361342627555 max memory_allocated 29274.25048828125 
[2025-02-23 14:07:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.049477193504571915 norm:0.00016509367560502142 max memory_allocated 29274.25048828125 
[2025-02-23 14:07:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.048554759472608566 norm:0.00014277774607762694 max memory_allocated 29274.25048828125 
[2025-02-23 14:08:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.047947634011507034 norm:0.00012965717178303748 max memory_allocated 29274.25048828125 
[2025-02-23 14:09:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.04757565259933472 norm:0.00012027876800857484 max memory_allocated 29274.25048828125 
[2025-02-23 14:10:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.04733363538980484 norm:0.00011234607518417761 max memory_allocated 29274.25048828125 
[2025-02-23 14:11:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.04720253497362137 norm:0.00010706577450037003 max memory_allocated 29274.25048828125 
[2025-02-23 14:12:03 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.04711093381047249 norm:9.884770406642929e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:12:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.04703088849782944 norm:9.611955465516075e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:13:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.04694889485836029 norm:9.090680396184325e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:14:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.04688544571399689 norm:8.798584894975647e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:15:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.04684224724769592 norm:8.51438962854445e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:16:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.04677223041653633 norm:8.016631909413263e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:17:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.04672117158770561 norm:7.626667502336204e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:17:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.046685025095939636 norm:7.471083517884836e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:18:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.046657346189022064 norm:7.151044701458886e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:19:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.046653591096401215 norm:6.886632036184892e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:20:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.0466480553150177 norm:6.593286525458097e-05 max memory_allocated 29274.25048828125 
[2025-02-23 14:20:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-23 14:21:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.06484614312648773 norm:0.0007367157959379256 max memory_allocated 29274.43798828125 
[2025-02-23 14:22:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.058051615953445435 norm:0.0003351590712554753 max memory_allocated 29274.43798828125 
[2025-02-23 14:23:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.053228020668029785 norm:0.00020856312767136842 max memory_allocated 29274.43798828125 
[2025-02-23 14:23:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.05167534947395325 norm:0.00015832777717150748 max memory_allocated 29274.43798828125 
[2025-02-23 14:24:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.05073600634932518 norm:0.00013737047265749425 max memory_allocated 29274.43798828125 
[2025-02-23 14:25:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.050127267837524414 norm:0.0001234613300766796 max memory_allocated 29274.43798828125 
[2025-02-23 14:26:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.04978290945291519 norm:0.00011390582221793011 max memory_allocated 29274.43798828125 
[2025-02-23 14:27:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.04958931356668472 norm:0.00010658853716449812 max memory_allocated 29274.43798828125 
[2025-02-23 14:28:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0494401678442955 norm:9.983877680497244e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:28:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.04932904988527298 norm:9.191945719067007e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:29:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.04921460524201393 norm:8.529152546543628e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:30:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.0491652637720108 norm:8.282435010187328e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:31:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.04911503195762634 norm:8.075896766968071e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:32:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.04905600845813751 norm:7.60699767852202e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:33:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.049012310802936554 norm:7.356098649324849e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:33:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.04897266998887062 norm:7.007591193541884e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:34:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.048937585204839706 norm:6.779513932997361e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:35:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.04893219843506813 norm:6.672117888228968e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:36:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.04892423376441002 norm:6.696992204524577e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:37:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.04890170693397522 norm:6.665084947599098e-05 max memory_allocated 29274.43798828125 
[2025-02-23 14:37:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-23 14:38:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.06681802868843079 norm:0.0006380333215929568 max memory_allocated 29274.62548828125 
[2025-02-23 14:39:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.060381654649972916 norm:0.00027517651324160397 max memory_allocated 29274.62548828125 
[2025-02-23 14:39:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.0557047463953495 norm:0.00017591280629858375 max memory_allocated 29274.62548828125 
[2025-02-23 14:40:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.05419961363077164 norm:0.00013985804980620742 max memory_allocated 29274.62548828125 
[2025-02-23 14:41:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.05331830680370331 norm:0.0001193419229821302 max memory_allocated 29274.62548828125 
[2025-02-23 14:42:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.052752792835235596 norm:0.00011121583520434797 max memory_allocated 29274.62548828125 
[2025-02-23 14:43:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.05237396061420441 norm:0.0001007082246360369 max memory_allocated 29274.62548828125 
[2025-02-23 14:44:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.052160024642944336 norm:9.389342449139804e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:44:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.052041634917259216 norm:9.069463703781366e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:45:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.051945727318525314 norm:8.453881309833378e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:46:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.051862239837646484 norm:8.053400961216539e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:47:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.05179252848029137 norm:7.5937932706438e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:48:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.05174960196018219 norm:7.584639388369396e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:49:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.05170672759413719 norm:7.239374099299312e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:49:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.05166686698794365 norm:6.968869274714962e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:50:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.05164100229740143 norm:6.917335849720985e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:51:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.05160433053970337 norm:6.641109212068841e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:52:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.05157577618956566 norm:6.637019396293908e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:53:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.05154566094279289 norm:6.369844049913809e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:53:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.051532723009586334 norm:6.747405859641731e-05 max memory_allocated 29274.62548828125 
[2025-02-23 14:54:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-23 14:55:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.07387876510620117 norm:0.0007603157428093255 max memory_allocated 29274.81298828125 
[2025-02-23 14:55:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.066375732421875 norm:0.0003602804208640009 max memory_allocated 29274.81298828125 
[2025-02-23 14:56:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.06094970181584358 norm:0.0002361483930144459 max memory_allocated 29274.81298828125 
[2025-02-23 14:57:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.05919888615608215 norm:0.00017504864081274718 max memory_allocated 29274.81298828125 
[2025-02-23 14:58:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.058280277997255325 norm:0.0001547363499412313 max memory_allocated 29274.81298828125 
[2025-02-23 14:59:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.05761399120092392 norm:0.00014033236948307604 max memory_allocated 29274.81298828125 
[2025-02-23 15:00:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.057194337248802185 norm:0.00013104981917422265 max memory_allocated 29274.81298828125 
[2025-02-23 15:00:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.05695994198322296 norm:0.0001250373461516574 max memory_allocated 29274.81298828125 
[2025-02-23 15:01:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.05679966136813164 norm:0.00011766985699068755 max memory_allocated 29274.81298828125 
[2025-02-23 15:02:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.056686583906412125 norm:0.0001127471768995747 max memory_allocated 29274.81298828125 
[2025-02-23 15:03:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.05658417195081711 norm:0.00010875615407712758 max memory_allocated 29274.81298828125 
[2025-02-23 15:04:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.05650654435157776 norm:0.00010584147821646184 max memory_allocated 29274.81298828125 
[2025-02-23 15:05:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.05641485005617142 norm:0.00010182729602092877 max memory_allocated 29274.81298828125 
[2025-02-23 15:05:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.05633523315191269 norm:9.760548709891737e-05 max memory_allocated 29274.81298828125 
[2025-02-23 15:06:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.05628266930580139 norm:9.440710709895939e-05 max memory_allocated 29274.81298828125 
[2025-02-23 15:07:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.05623660609126091 norm:8.928014722187072e-05 max memory_allocated 29274.81298828125 
[2025-02-23 15:08:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.05620903894305229 norm:8.778883056947961e-05 max memory_allocated 29274.81298828125 
[2025-02-23 15:09:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.056171562522649765 norm:8.204799814848229e-05 max memory_allocated 29274.81298828125 
[2025-02-23 15:09:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.05613218992948532 norm:8.235599671024829e-05 max memory_allocated 29274.81298828125 
[2025-02-23 15:10:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.05611082911491394 norm:8.100050035864115e-05 max memory_allocated 29274.81298828125 
[2025-02-23 15:11:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-23 15:11:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.0745047777891159 norm:0.0004636115918401629 max memory_allocated 29275.00048828125 
[2025-02-23 15:12:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.06891127675771713 norm:0.0002282200730405748 max memory_allocated 29275.00048828125 
[2025-02-23 15:13:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.06426125764846802 norm:0.00014183681923896074 max memory_allocated 29275.00048828125 
[2025-02-23 15:14:28 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.06297455728054047 norm:0.00012133142445236444 max memory_allocated 29275.00048828125 
[2025-02-23 15:15:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.062083467841148376 norm:0.00010693872900446877 max memory_allocated 29275.00048828125 
[2025-02-23 15:16:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.06144249439239502 norm:9.701628005132079e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:16:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.06107855588197708 norm:9.158537432085723e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:17:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.060889966785907745 norm:9.001985745271668e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:18:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.060761768370866776 norm:9.402533032698557e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:19:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.06066448241472244 norm:7.976494816830382e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:20:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.060592442750930786 norm:7.751505472697318e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:21:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.060532864183187485 norm:7.670939521631226e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:21:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.060459159314632416 norm:7.244608423206955e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:22:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.06041768565773964 norm:6.935104465810582e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:23:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.06038524955511093 norm:6.866319745313376e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:24:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.06034408137202263 norm:6.793667853344232e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:25:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.06030333414673805 norm:6.666181434411556e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:26:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.060280539095401764 norm:6.585642404388636e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:26:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.06025594845414162 norm:6.502354517579079e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:27:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.060221921652555466 norm:6.603873771382496e-05 max memory_allocated 29275.00048828125 
[2025-02-23 15:27:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-23 15:28:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.08069124817848206 norm:0.0004778824222739786 max memory_allocated 29275.18798828125 
[2025-02-23 15:29:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.07515207678079605 norm:0.00027778392541222274 max memory_allocated 29275.18798828125 
[2025-02-23 15:30:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.07021097093820572 norm:0.00016027943638619035 max memory_allocated 29275.18798828125 
[2025-02-23 15:31:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.06887707114219666 norm:0.00013199135719332844 max memory_allocated 29275.18798828125 
[2025-02-23 15:32:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.06796394288539886 norm:0.00011548701149877161 max memory_allocated 29275.18798828125 
[2025-02-23 15:33:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.06729535758495331 norm:0.00010595193452900276 max memory_allocated 29275.18798828125 
[2025-02-23 15:33:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.0669323056936264 norm:9.865235188044608e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:34:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.06674306839704514 norm:9.095232235267758e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:35:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.06661175191402435 norm:8.614672697149217e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:36:18 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.06650322675704956 norm:8.251448889495805e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:37:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.06639454513788223 norm:7.79319743742235e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:37:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.06632143259048462 norm:7.484412344638258e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:38:47 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.06625539064407349 norm:7.207207818282768e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:39:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.0662013590335846 norm:6.995977309998125e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:40:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.0661473274230957 norm:6.839457637397572e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:41:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.0661027729511261 norm:0.00015031726798042655 max memory_allocated 29275.18798828125 
[2025-02-23 15:42:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.06607945263385773 norm:6.709757872158661e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:42:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.06604686379432678 norm:6.477154238382354e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:43:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.06602446734905243 norm:6.61775775370188e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:44:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.06600554287433624 norm:6.374310032697394e-05 max memory_allocated 29275.18798828125 
[2025-02-23 15:44:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-23 15:45:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.08688554912805557 norm:0.00046022230526432395 max memory_allocated 29275.37548828125 
[2025-02-23 15:46:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.08120214939117432 norm:0.0002608549839351326 max memory_allocated 29275.37548828125 
[2025-02-23 15:47:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.07617709785699844 norm:0.0001710690266918391 max memory_allocated 29275.37548828125 
[2025-02-23 15:48:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.0749846026301384 norm:0.00014258481678552926 max memory_allocated 29275.37548828125 
[2025-02-23 15:49:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.07406046986579895 norm:0.00012578166206367314 max memory_allocated 29275.37548828125 
[2025-02-23 15:49:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.07335915416479111 norm:0.00011507690214784816 max memory_allocated 29275.37548828125 
[2025-02-23 15:50:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.07300030440092087 norm:0.0001062092196661979 max memory_allocated 29275.37548828125 
[2025-02-23 15:51:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.0728098526597023 norm:0.00010217318776994944 max memory_allocated 29275.37548828125 
[2025-02-23 15:52:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.07268530130386353 norm:9.894368122331798e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:53:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.07259044051170349 norm:9.98990872176364e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:54:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.0725022554397583 norm:9.792100900085643e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:54:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.07241401076316833 norm:9.410582424607128e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:55:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.07236030697822571 norm:9.468996722716838e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:56:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.07233384251594543 norm:9.92528221104294e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:57:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.07228533923625946 norm:9.299562952946872e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:58:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.072244793176651 norm:9.424826566828415e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:58:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.07220728695392609 norm:8.822858944768086e-05 max memory_allocated 29275.37548828125 
[2025-02-23 15:59:49 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.07216235995292664 norm:8.900831744540483e-05 max memory_allocated 29275.37548828125 
[2025-02-23 16:00:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.07212744653224945 norm:8.883999544195831e-05 max memory_allocated 29275.37548828125 
[2025-02-23 16:01:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.07209751754999161 norm:9.1650101239793e-05 max memory_allocated 29275.37548828125 
[2025-02-23 16:01:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-23 16:02:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.09488262236118317 norm:0.00041535007767379284 max memory_allocated 29275.56298828125 
[2025-02-23 16:03:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.08904046565294266 norm:0.00024745712289586663 max memory_allocated 29275.56298828125 
[2025-02-23 16:04:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.08357937633991241 norm:0.0001652312057558447 max memory_allocated 29275.56298828125 
[2025-02-23 16:05:09 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.08222425729036331 norm:0.00012987789523322135 max memory_allocated 29275.56298828125 
[2025-02-23 16:05:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.08123354613780975 norm:0.00011341244680806994 max memory_allocated 29275.56298828125 
[2025-02-23 16:06:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.08050557971000671 norm:0.00010320452565792948 max memory_allocated 29275.56298828125 
[2025-02-23 16:07:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.08016923815011978 norm:9.559040336171165e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:08:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.08000286668539047 norm:8.839509973768145e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:09:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.0798727497458458 norm:8.402847015531734e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:10:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.07975979894399643 norm:7.953885506140068e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:10:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.07967398315668106 norm:7.864445797167718e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:11:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.07960163056850433 norm:7.608381565660238e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:12:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.07953669875860214 norm:7.454739534296095e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:13:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.0794835165143013 norm:7.246275345096365e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:14:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.07942962646484375 norm:6.973783456487581e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:15:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.07937084138393402 norm:6.737504008924589e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:15:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.07933253049850464 norm:6.623878289246932e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:16:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.07929952442646027 norm:6.393603689502925e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:17:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.07926493138074875 norm:6.276418571360409e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:18:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.0792321264743805 norm:6.170188135001808e-05 max memory_allocated 29275.56298828125 
[2025-02-23 16:18:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-23 16:19:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.10433951020240784 norm:0.00045345613034442067 max memory_allocated 29275.75048828125 
[2025-02-23 16:20:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.09808863699436188 norm:0.0002612280077300966 max memory_allocated 29275.75048828125 
[2025-02-23 16:21:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.09232841432094574 norm:0.0001777484139893204 max memory_allocated 29275.75048828125 
[2025-02-23 16:22:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.09083686023950577 norm:0.0001415371662005782 max memory_allocated 29275.75048828125 
[2025-02-23 16:22:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.08972492069005966 norm:0.00013582032988779247 max memory_allocated 29275.75048828125 
[2025-02-23 16:23:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.08896410465240479 norm:0.000127240942674689 max memory_allocated 29275.75048828125 
[2025-02-23 16:24:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.08862505853176117 norm:0.00012202834477648139 max memory_allocated 29275.75048828125 
[2025-02-23 16:25:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.08842579275369644 norm:0.00011641594755928963 max memory_allocated 29275.75048828125 
[2025-02-23 16:26:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.08828574419021606 norm:0.0001141901666414924 max memory_allocated 29275.75048828125 
[2025-02-23 16:27:01 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.08817265182733536 norm:0.00010478436888661236 max memory_allocated 29275.75048828125 
[2025-02-23 16:27:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.08807137608528137 norm:9.854885865934193e-05 max memory_allocated 29275.75048828125 
[2025-02-23 16:28:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.08800113946199417 norm:0.00010254242806695402 max memory_allocated 29275.75048828125 
[2025-02-23 16:29:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.08793731778860092 norm:0.0001030656712828204 max memory_allocated 29275.75048828125 
[2025-02-23 16:30:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.08786572515964508 norm:9.789738396648318e-05 max memory_allocated 29275.75048828125 
[2025-02-23 16:31:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.08779606223106384 norm:9.519328159512952e-05 max memory_allocated 29275.75048828125 
[2025-02-23 16:31:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.08775486797094345 norm:9.999604662880301e-05 max memory_allocated 29275.75048828125 
[2025-02-23 16:32:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.0877106636762619 norm:0.00010708616900956258 max memory_allocated 29275.75048828125 
[2025-02-23 16:33:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.08766631036996841 norm:9.799528925213963e-05 max memory_allocated 29275.75048828125 
[2025-02-23 16:34:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.08763636648654938 norm:8.953726501204073e-05 max memory_allocated 29275.75048828125 
[2025-02-23 16:35:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.08759623765945435 norm:8.895636710803956e-05 max memory_allocated 29275.75048828125 
[2025-02-23 16:35:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-23 16:36:28 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.11181911081075668 norm:0.0002667079970706254 max memory_allocated 29275.93798828125 
[2025-02-23 16:37:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.10595449060201645 norm:0.00016748021880630404 max memory_allocated 29275.93798828125 
[2025-02-23 16:38:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.10026586800813675 norm:0.00012113696720916778 max memory_allocated 29275.93798828125 
[2025-02-23 16:38:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.09888182580471039 norm:0.0001052671141223982 max memory_allocated 29275.93798828125 
[2025-02-23 16:39:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.09779296070337296 norm:0.00013029483670834452 max memory_allocated 29275.93798828125 
[2025-02-23 16:40:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.09706968069076538 norm:9.223468077834696e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:41:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.09676139056682587 norm:8.848260767990723e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:42:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.09660128504037857 norm:8.344990783371031e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:43:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.09644480049610138 norm:7.753906538709998e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:43:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.09635097533464432 norm:7.799069135216996e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:44:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.09626218676567078 norm:7.566294516436756e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:45:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.09619031846523285 norm:7.410618127323687e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:46:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.09612827003002167 norm:7.341954915318638e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:47:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.09605731815099716 norm:7.132595783332363e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:48:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.09599728882312775 norm:7.064291276037693e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:48:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.09593968838453293 norm:6.789560575271025e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:49:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.0958888977766037 norm:6.698114157188684e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:50:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.09584686905145645 norm:6.51252776151523e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:51:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.09580178558826447 norm:6.40594371361658e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:52:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.09576240926980972 norm:6.41064762021415e-05 max memory_allocated 29275.93798828125 
[2025-02-23 16:52:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-23 16:53:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.1234922707080841 norm:0.0005019498057663441 max memory_allocated 29276.12548828125 
[2025-02-23 16:54:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.1169823482632637 norm:0.0003296200593467802 max memory_allocated 29276.12548828125 
[2025-02-23 16:55:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.11090070009231567 norm:0.0002502360148355365 max memory_allocated 29276.12548828125 
[2025-02-23 16:55:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.10928251594305038 norm:0.00020197869162075222 max memory_allocated 29276.12548828125 
[2025-02-23 16:56:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.10805133730173111 norm:0.00018709083087742329 max memory_allocated 29276.12548828125 
[2025-02-23 16:57:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.10730161517858505 norm:0.00017733318964019418 max memory_allocated 29276.12548828125 
[2025-02-23 16:58:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.10697558522224426 norm:0.0001669208868406713 max memory_allocated 29276.12548828125 
[2025-02-23 16:59:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.10676995664834976 norm:0.00016128529387060553 max memory_allocated 29276.12548828125 
[2025-02-23 16:59:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.1066267192363739 norm:0.0001550123270135373 max memory_allocated 29276.12548828125 
[2025-02-23 17:00:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.10648860782384872 norm:0.0001574541092850268 max memory_allocated 29276.12548828125 
[2025-02-23 17:01:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.10637359321117401 norm:0.0001582149270689115 max memory_allocated 29276.12548828125 
[2025-02-23 17:02:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.10629084706306458 norm:0.00013792453682981431 max memory_allocated 29276.12548828125 
[2025-02-23 17:03:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.10618826746940613 norm:0.00013147493882570416 max memory_allocated 29276.12548828125 
[2025-02-23 17:04:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.1061156839132309 norm:0.00013592126197181642 max memory_allocated 29276.12548828125 
[2025-02-23 17:04:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.10604673624038696 norm:0.00011546062887646258 max memory_allocated 29276.12548828125 
[2025-02-23 17:05:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.1059872955083847 norm:0.00012033764505758882 max memory_allocated 29276.12548828125 
[2025-02-23 17:06:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.10592159628868103 norm:0.00011158103734487668 max memory_allocated 29276.12548828125 
[2025-02-23 17:07:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.1058603823184967 norm:0.00010877769091166556 max memory_allocated 29276.12548828125 
[2025-02-23 17:08:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.10581525415182114 norm:0.00011282027116976678 max memory_allocated 29276.12548828125 
[2025-02-23 17:09:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.10578163713216782 norm:0.00011676616122713313 max memory_allocated 29276.12548828125 
[2025-02-23 17:09:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-23 17:10:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.13356155157089233 norm:0.00026998177054338157 max memory_allocated 29276.31298828125 
[2025-02-23 17:11:04 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.12715961039066315 norm:0.00017022635438479483 max memory_allocated 29276.31298828125 
[2025-02-23 17:11:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.12093580514192581 norm:0.00011450832971604541 max memory_allocated 29276.31298828125 
[2025-02-23 17:12:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.1193745881319046 norm:9.895102266455069e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:13:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.1181415319442749 norm:9.012137888930738e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:14:23 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.11746671795845032 norm:8.617184357717633e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:15:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.11718378216028214 norm:8.219352457672358e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:16:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.11701194196939468 norm:7.766664202790707e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:16:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.1168825626373291 norm:7.639999239472672e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:17:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.11678063124418259 norm:7.597420335514471e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:18:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.11668050289154053 norm:7.198891398729756e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:19:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.11659936606884003 norm:7.273585652001202e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:20:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.11652728170156479 norm:7.15825444785878e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:20:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.11646320670843124 norm:6.919720181031153e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:21:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.11641412228345871 norm:6.934982229722664e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:22:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.11636260151863098 norm:7.000108598731458e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:23:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.11631203442811966 norm:6.954438867978752e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:24:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.1162700206041336 norm:6.84722326695919e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:25:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.11624329537153244 norm:6.961134204175323e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:25:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.11620922386646271 norm:7.02096804161556e-05 max memory_allocated 29276.31298828125 
[2025-02-23 17:26:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-23 17:27:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.14851762354373932 norm:0.0004543634713627398 max memory_allocated 29276.50048828125 
[2025-02-23 17:27:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.14077046513557434 norm:0.00025542781804688275 max memory_allocated 29276.50048828125 
[2025-02-23 17:28:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.13334280252456665 norm:0.00015402588178403676 max memory_allocated 29276.50048828125 
[2025-02-23 17:29:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.13140524923801422 norm:0.00012526656792033464 max memory_allocated 29276.50048828125 
[2025-02-23 17:30:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.13009220361709595 norm:0.00013205214054323733 max memory_allocated 29276.50048828125 
[2025-02-23 17:31:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.1294402778148651 norm:0.00010548444697633386 max memory_allocated 29276.50048828125 
[2025-02-23 17:32:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.12919005751609802 norm:9.976026194635779e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:32:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.12901785969734192 norm:9.292853064835072e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:33:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.12886355817317963 norm:8.740957855479792e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:34:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.12874291837215424 norm:8.4442894149106e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:35:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.1286245733499527 norm:8.169775537680835e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:36:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.1285187155008316 norm:8.053188503254205e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:37:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.1284453570842743 norm:7.916260801721364e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:37:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.12836448848247528 norm:7.374918641289696e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:38:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.12828274071216583 norm:7.338566501857713e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:39:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.1282179057598114 norm:7.183454727055505e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:40:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.12815968692302704 norm:7.177825318649411e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:41:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.12811078131198883 norm:7.083217496983707e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:41:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.12805798649787903 norm:7.043130608508363e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:42:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.12801538407802582 norm:6.920095620444044e-05 max memory_allocated 29276.50048828125 
[2025-02-23 17:43:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-23 17:44:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.16098450124263763 norm:0.0004048986593261361 max memory_allocated 29276.68798828125 
[2025-02-23 17:44:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.15330928564071655 norm:0.0002574432292021811 max memory_allocated 29276.68798828125 
[2025-02-23 17:45:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.14573831856250763 norm:0.00014153876691125333 max memory_allocated 29276.68798828125 
[2025-02-23 17:46:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.1436855047941208 norm:0.00011746962263714522 max memory_allocated 29276.68798828125 
[2025-02-23 17:47:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.14235998690128326 norm:0.00011096457456005737 max memory_allocated 29276.68798828125 
[2025-02-23 17:48:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.141823410987854 norm:0.0001045268727466464 max memory_allocated 29276.68798828125 
[2025-02-23 17:48:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.141597181558609 norm:9.937478898791596e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:49:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.14144442975521088 norm:9.226035763276741e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:50:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.14129739999771118 norm:8.917207014746964e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:51:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.1411781907081604 norm:9.353934729006141e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:52:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.1410779058933258 norm:8.424234692938626e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:53:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.14098133146762848 norm:8.287759555969387e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:53:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.140900656580925 norm:8.052425255300477e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:54:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.14082548022270203 norm:7.914091111160815e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:55:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.1407628357410431 norm:7.95711821410805e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:56:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.140712171792984 norm:7.926981197670102e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:57:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.14064614474773407 norm:8.066527516348287e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:58:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.1405964493751526 norm:7.844567153370008e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:58:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.14055033028125763 norm:8.017638901947066e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:59:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.14051197469234467 norm:7.854151772335172e-05 max memory_allocated 29276.68798828125 
[2025-02-23 17:59:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-23 18:00:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.17724275588989258 norm:0.00048622858594171703 max memory_allocated 29276.87548828125 
[2025-02-23 18:01:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.16877135634422302 norm:0.0002828675787895918 max memory_allocated 29276.87548828125 
[2025-02-23 18:02:36 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.16066581010818481 norm:0.00017030889284797013 max memory_allocated 29276.87548828125 
[2025-02-23 18:03:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.15836137533187866 norm:0.00013984987162984908 max memory_allocated 29276.87548828125 
[2025-02-23 18:04:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.15695953369140625 norm:0.00012765407154802233 max memory_allocated 29276.87548828125 
[2025-02-23 18:05:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.1564176380634308 norm:0.000117324641905725 max memory_allocated 29276.87548828125 
[2025-02-23 18:05:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.15615126490592957 norm:0.00011072080815210938 max memory_allocated 29276.87548828125 
[2025-02-23 18:06:44 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.15594559907913208 norm:0.00010688100883271545 max memory_allocated 29276.87548828125 
[2025-02-23 18:07:34 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.15575480461120605 norm:0.00010056774044642225 max memory_allocated 29276.87548828125 
[2025-02-23 18:08:24 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.15562821924686432 norm:0.00012982034240849316 max memory_allocated 29276.87548828125 
[2025-02-23 18:09:13 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.1555098444223404 norm:8.924643771024421e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:10:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.1554126739501953 norm:8.708763198228553e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:10:52 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.15531541407108307 norm:8.463986159767956e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:11:41 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.15523287653923035 norm:8.10402343631722e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:12:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.1551537960767746 norm:8.116413664538413e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:13:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.1550818383693695 norm:7.713981904089451e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:14:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.15502741932868958 norm:7.566239219158888e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:15:00 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.15496817231178284 norm:7.583016122225672e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:15:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.15491703152656555 norm:7.508630369557068e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:16:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.15488030016422272 norm:7.36523506930098e-05 max memory_allocated 29276.87548828125 
[2025-02-23 18:16:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-23 18:17:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.1931152045726776 norm:0.0004014318692497909 max memory_allocated 29277.06298828125 
[2025-02-23 18:18:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.18408606946468353 norm:0.0002562633599154651 max memory_allocated 29277.06298828125 
[2025-02-23 18:19:29 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.17543284595012665 norm:0.00015321391401812434 max memory_allocated 29277.06298828125 
[2025-02-23 18:20:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.17310157418251038 norm:0.00012499169679358602 max memory_allocated 29277.06298828125 
[2025-02-23 18:21:08 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.17170462012290955 norm:0.00011073783389292657 max memory_allocated 29277.06298828125 
[2025-02-23 18:21:57 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.1711871325969696 norm:0.00010455361189087853 max memory_allocated 29277.06298828125 
[2025-02-23 18:22:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.17094199359416962 norm:0.00010310554353054613 max memory_allocated 29277.06298828125 
[2025-02-23 18:23:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.17073091864585876 norm:9.634042362449691e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:24:25 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.17057017982006073 norm:9.275876072933897e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:25:15 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.17042581737041473 norm:8.684718341100961e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:26:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.17030750215053558 norm:8.631575474282727e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:26:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.17021116614341736 norm:8.33744925330393e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:27:44 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.17011891305446625 norm:8.361537766177207e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:28:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.1700240671634674 norm:8.456456998828799e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:29:23 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.16995513439178467 norm:8.474029891658574e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:30:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.16989071667194366 norm:8.349795098183677e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:31:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.16981810331344604 norm:8.645370689919218e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:31:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.16976188123226166 norm:8.35609098430723e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:32:42 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.16972003877162933 norm:8.341961802216247e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:33:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.16968007385730743 norm:8.483886631438509e-05 max memory_allocated 29277.06298828125 
[2025-02-23 18:33:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-23 18:34:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.21376986801624298 norm:0.0004973043687641621 max memory_allocated 29277.25048828125 
[2025-02-23 18:35:32 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.20375297963619232 norm:0.0002869511372409761 max memory_allocated 29277.25048828125 
[2025-02-23 18:36:22 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.1948281228542328 norm:0.00018885846657212824 max memory_allocated 29277.25048828125 
[2025-02-23 18:37:11 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.19214710593223572 norm:0.00015939641161821783 max memory_allocated 29277.25048828125 
[2025-02-23 18:38:01 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.1907440423965454 norm:0.0001452048891223967 max memory_allocated 29277.25048828125 
[2025-02-23 18:38:50 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.19022244215011597 norm:0.00013417445006780326 max memory_allocated 29277.25048828125 
[2025-02-23 18:39:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.18991953134536743 norm:0.0001270898210350424 max memory_allocated 29277.25048828125 
[2025-02-23 18:40:30 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.1896439790725708 norm:0.00012047977361362427 max memory_allocated 29277.25048828125 
[2025-02-23 18:41:19 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.18939447402954102 norm:0.00011682043987093493 max memory_allocated 29277.25048828125 
[2025-02-23 18:42:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.18919356167316437 norm:0.00010873607971007004 max memory_allocated 29277.25048828125 
[2025-02-23 18:42:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.18907597661018372 norm:0.00010557266068644822 max memory_allocated 29277.25048828125 
[2025-02-23 18:43:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.18893314898014069 norm:0.00010169950837735087 max memory_allocated 29277.25048828125 
[2025-02-23 18:44:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.18881019949913025 norm:9.836125536821783e-05 max memory_allocated 29277.25048828125 
[2025-02-23 18:45:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.18870709836483002 norm:9.775639773579314e-05 max memory_allocated 29277.25048828125 
[2025-02-23 18:46:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.18861696124076843 norm:9.232989395968616e-05 max memory_allocated 29277.25048828125 
[2025-02-23 18:47:06 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.18851196765899658 norm:8.988576155388728e-05 max memory_allocated 29277.25048828125 
[2025-02-23 18:47:56 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.18842093646526337 norm:8.925969450501725e-05 max memory_allocated 29277.25048828125 
[2025-02-23 18:48:46 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.18834158778190613 norm:8.747948595555499e-05 max memory_allocated 29277.25048828125 
[2025-02-23 18:49:35 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.18828733265399933 norm:8.835139306029305e-05 max memory_allocated 29277.25048828125 
[2025-02-23 18:50:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.1882283091545105 norm:8.484289719490334e-05 max memory_allocated 29277.25048828125 
[2025-02-23 18:50:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-23 18:51:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.23424191772937775 norm:0.00034383012098260224 max memory_allocated 29277.43798828125 
[2025-02-23 18:52:25 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.2238110899925232 norm:0.00022089597769081593 max memory_allocated 29277.43798828125 
[2025-02-23 18:53:15 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.21441654860973358 norm:0.0001506758708273992 max memory_allocated 29277.43798828125 
[2025-02-23 18:54:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.21153023838996887 norm:0.0001288679923163727 max memory_allocated 29277.43798828125 
[2025-02-23 18:54:54 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.21018996834754944 norm:0.00011772854486480355 max memory_allocated 29277.43798828125 
[2025-02-23 18:55:43 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.20973430573940277 norm:0.00011275608267169446 max memory_allocated 29277.43798828125 
[2025-02-23 18:56:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.20940817892551422 norm:0.00010407922673039138 max memory_allocated 29277.43798828125 
[2025-02-23 18:57:23 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.20916539430618286 norm:9.826765017351136e-05 max memory_allocated 29277.43798828125 
[2025-02-23 18:58:12 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.20897310972213745 norm:9.416511602466926e-05 max memory_allocated 29277.43798828125 
[2025-02-23 18:59:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.20881034433841705 norm:9.410746133653447e-05 max memory_allocated 29277.43798828125 
[2025-02-23 18:59:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.20865210890769958 norm:9.065209451364353e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:00:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.2085222601890564 norm:8.857024658937007e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:01:31 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.20839866995811462 norm:8.517109381500632e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:02:20 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.20829644799232483 norm:8.343904482899234e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:03:10 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.20821133255958557 norm:8.376485493499786e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:03:59 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.20812490582466125 norm:8.30966018838808e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:04:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.20804628729820251 norm:8.368913404410705e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:05:38 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.20797179639339447 norm:8.37154220789671e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:06:28 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.20790284872055054 norm:8.331766002811491e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:07:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.2078426331281662 norm:8.372833690373227e-05 max memory_allocated 29277.43798828125 
[2025-02-23 19:07:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-23 19:07:40 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 19:08:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.26593637466430664 norm:0.009350567124783993 max memory_allocated 29277.77001953125 
[2025-02-23 19:09:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.2520490288734436 norm:0.0071314661763608456 max memory_allocated 29277.77001953125 
[2025-02-23 19:10:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.2405712902545929 norm:0.00516536645591259 max memory_allocated 29277.77001953125 
[2025-02-23 19:10:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.23687556385993958 norm:0.004224179312586784 max memory_allocated 29277.77001953125 
[2025-02-23 19:11:49 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.23530788719654083 norm:0.0034866526257246733 max memory_allocated 29277.77001953125 
[2025-02-23 19:12:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.23466387391090393 norm:0.00286550959572196 max memory_allocated 29277.77001953125 
[2025-02-23 19:13:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.23419877886772156 norm:0.0024732735473662615 max memory_allocated 29277.77001953125 
[2025-02-23 19:14:18 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.23391546308994293 norm:0.0024295062758028507 max memory_allocated 29277.77001953125 
[2025-02-23 19:15:08 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.2336403727531433 norm:0.0022933827713131905 max memory_allocated 29277.77001953125 
[2025-02-23 19:15:57 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.23336677253246307 norm:0.002115927403792739 max memory_allocated 29277.77001953125 
[2025-02-23 19:16:47 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.23314523696899414 norm:0.0020661200396716595 max memory_allocated 29277.77001953125 
[2025-02-23 19:17:37 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.2329508662223816 norm:0.0019651446491479874 max memory_allocated 29277.77001953125 
[2025-02-23 19:18:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.23272430896759033 norm:0.0019308716291561723 max memory_allocated 29277.77001953125 
[2025-02-23 19:19:17 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.2325678914785385 norm:0.0018182129133492708 max memory_allocated 29277.77001953125 
[2025-02-23 19:20:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.23242421448230743 norm:0.0017865190748125315 max memory_allocated 29277.77001953125 
[2025-02-23 19:20:56 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.232319638133049 norm:0.0017388924024999142 max memory_allocated 29277.77001953125 
[2025-02-23 19:21:46 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.2321949005126953 norm:0.0017055643256753683 max memory_allocated 29277.77001953125 
[2025-02-23 19:22:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.23209944367408752 norm:0.0016974147874861956 max memory_allocated 29277.77001953125 
[2025-02-23 19:23:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.23196479678153992 norm:0.0015995358116924763 max memory_allocated 29277.77001953125 
[2025-02-23 19:24:15 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.23189766705036163 norm:0.0015840706182643771 max memory_allocated 29277.77001953125 
[2025-02-23 19:24:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-23 19:24:36 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 19:25:26 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.2946159839630127 norm:0.00972655601799488 max memory_allocated 29277.95751953125 
[2025-02-23 19:26:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.28028005361557007 norm:0.007818743586540222 max memory_allocated 29277.95751953125 
[2025-02-23 19:27:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.26789698004722595 norm:0.005460623186081648 max memory_allocated 29277.95751953125 
[2025-02-23 19:27:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.2639245390892029 norm:0.004473465494811535 max memory_allocated 29277.95751953125 
[2025-02-23 19:28:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.26236408948898315 norm:0.003728513838723302 max memory_allocated 29277.95751953125 
[2025-02-23 19:29:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.26167380809783936 norm:0.003136648330837488 max memory_allocated 29277.95751953125 
[2025-02-23 19:30:25 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.26116007566452026 norm:0.0026617427356541157 max memory_allocated 29277.95751953125 
[2025-02-23 19:31:15 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.26080575585365295 norm:0.00242137280292809 max memory_allocated 29277.95751953125 
[2025-02-23 19:32:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.26064687967300415 norm:0.002485961187630892 max memory_allocated 29277.95751953125 
[2025-02-23 19:32:54 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.2604616582393646 norm:0.0025285212323069572 max memory_allocated 29277.95751953125 
[2025-02-23 19:33:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.260200560092926 norm:0.0022826059721410275 max memory_allocated 29277.95751953125 
[2025-02-23 19:34:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.2599124014377594 norm:0.0020674436818808317 max memory_allocated 29277.95751953125 
[2025-02-23 19:35:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.2597975730895996 norm:0.0020226561464369297 max memory_allocated 29277.95751953125 
[2025-02-23 19:36:14 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.25965484976768494 norm:0.002004422713071108 max memory_allocated 29277.95751953125 
[2025-02-23 19:37:04 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.2596431374549866 norm:0.001991619821637869 max memory_allocated 29277.95751953125 
[2025-02-23 19:37:53 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.25955724716186523 norm:0.0020598997361958027 max memory_allocated 29277.95751953125 
[2025-02-23 19:38:43 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.25950953364372253 norm:0.002009115181863308 max memory_allocated 29277.95751953125 
[2025-02-23 19:39:33 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.2594337463378906 norm:0.0019961409270763397 max memory_allocated 29277.95751953125 
[2025-02-23 19:40:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.2593013644218445 norm:0.0019183552358299494 max memory_allocated 29277.95751953125 
[2025-02-23 19:41:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.2592702805995941 norm:0.0019721118733286858 max memory_allocated 29277.95751953125 
[2025-02-23 19:41:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-23 19:41:34 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 19:42:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.34556543827056885 norm:0.015246475115418434 max memory_allocated 29278.14501953125 
[2025-02-23 19:43:14 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.3269440531730652 norm:0.011023152619600296 max memory_allocated 29278.14501953125 
[2025-02-23 19:44:04 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.3126797378063202 norm:0.007497172802686691 max memory_allocated 29278.14501953125 
[2025-02-23 19:44:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.30822357535362244 norm:0.006183994002640247 max memory_allocated 29278.14501953125 
[2025-02-23 19:45:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.3065776824951172 norm:0.005435851402580738 max memory_allocated 29278.14501953125 
[2025-02-23 19:46:33 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.3057580292224884 norm:0.004760001786053181 max memory_allocated 29278.14501953125 
[2025-02-23 19:47:23 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.3052200973033905 norm:0.00423737708479166 max memory_allocated 29278.14501953125 
[2025-02-23 19:48:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.30482053756713867 norm:0.004128130618482828 max memory_allocated 29278.14501953125 
[2025-02-23 19:49:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.30441850423812866 norm:0.0039696055464446545 max memory_allocated 29278.14501953125 
[2025-02-23 19:49:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.3042052090167999 norm:0.003878608113154769 max memory_allocated 29278.14501953125 
[2025-02-23 19:50:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.3042803406715393 norm:0.003934904467314482 max memory_allocated 29278.14501953125 
[2025-02-23 19:51:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.3041061758995056 norm:0.0038440364878624678 max memory_allocated 29278.14501953125 
[2025-02-23 19:52:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.30391955375671387 norm:0.0035966152790933847 max memory_allocated 29278.14501953125 
[2025-02-23 19:53:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.30379247665405273 norm:0.003580200718715787 max memory_allocated 29278.14501953125 
[2025-02-23 19:54:00 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.3035871982574463 norm:0.0034941337071359158 max memory_allocated 29278.14501953125 
[2025-02-23 19:54:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.30341747403144836 norm:0.003198944730684161 max memory_allocated 29278.14501953125 
[2025-02-23 19:55:39 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.30333051085472107 norm:0.0031444448977708817 max memory_allocated 29278.14501953125 
[2025-02-23 19:56:29 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.3032892346382141 norm:0.003121499205008149 max memory_allocated 29278.14501953125 
[2025-02-23 19:57:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.3033014237880707 norm:0.003079366870224476 max memory_allocated 29278.14501953125 
[2025-02-23 19:58:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.30341941118240356 norm:0.0032469758298248053 max memory_allocated 29278.14501953125 
[2025-02-23 19:58:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-23 19:58:31 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 19:59:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.5114687085151672 norm:0.053643450140953064 max memory_allocated 29278.33251953125 
[2025-02-23 20:00:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.46283870935440063 norm:0.03727434203028679 max memory_allocated 29278.33251953125 
[2025-02-23 20:01:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.4259726405143738 norm:0.02342293970286846 max memory_allocated 29278.33251953125 
[2025-02-23 20:01:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.4156412184238434 norm:0.019295288249850273 max memory_allocated 29278.33251953125 
[2025-02-23 20:02:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.4106280207633972 norm:0.016616852954030037 max memory_allocated 29278.33251953125 
[2025-02-23 20:03:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.4070315957069397 norm:0.014437452889978886 max memory_allocated 29278.33251953125 
[2025-02-23 20:04:20 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.4047115445137024 norm:0.01356557197868824 max memory_allocated 29278.33251953125 
[2025-02-23 20:05:09 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.4030294418334961 norm:0.012287884950637817 max memory_allocated 29278.33251953125 
[2025-02-23 20:05:59 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.4015327990055084 norm:0.011421959847211838 max memory_allocated 29278.33251953125 
[2025-02-23 20:06:49 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.4001336693763733 norm:0.01080393511801958 max memory_allocated 29278.33251953125 
[2025-02-23 20:07:39 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.39917129278182983 norm:0.009906160645186901 max memory_allocated 29278.33251953125 
[2025-02-23 20:08:28 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.39855384826660156 norm:0.009825028479099274 max memory_allocated 29278.33251953125 
[2025-02-23 20:09:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.39787787199020386 norm:0.009218800812959671 max memory_allocated 29278.33251953125 
[2025-02-23 20:10:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.3973778486251831 norm:0.008974164724349976 max memory_allocated 29278.33251953125 
[2025-02-23 20:10:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.397171288728714 norm:0.008984924294054508 max memory_allocated 29278.33251953125 
[2025-02-23 20:11:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.39719706773757935 norm:0.008881348185241222 max memory_allocated 29278.33251953125 
[2025-02-23 20:12:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.3973136842250824 norm:0.008804287761449814 max memory_allocated 29278.33251953125 
[2025-02-23 20:13:27 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.3965074121952057 norm:0.008585578761994839 max memory_allocated 29278.33251953125 
[2025-02-23 20:14:16 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.39596033096313477 norm:0.007943305186927319 max memory_allocated 29278.33251953125 
[2025-02-23 20:15:06 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.39570268988609314 norm:0.008253046311438084 max memory_allocated 29278.33251953125 
[2025-02-23 20:15:21 root] (main_calibration.py 365): INFO 40506.649102926254
[2025-02-23 20:16:24 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-23 20:18:20 root] (main_calibration.py 158): INFO wikitext2 : 5.016936302185059
[2025-02-23 20:18:20 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-23 20:21:21 root] (main_calibration.py 158): INFO c4 : 6.654295444488525
[2025-02-23 22:18:14 root] (main_calibration.py 169): INFO {'wikitext2': 5.016936302185059, 'c4': 6.654295444488525, 'results': {'arc_easy': {'acc': 0.7251683501683501, 'acc_stderr': 0.009160538115254958, 'acc_norm': 0.5698653198653199, 'acc_norm_stderr': 0.010159130445178513}, 'hellaswag': {'acc': 0.5898227444732125, 'acc_stderr': 0.00490860473208281, 'acc_norm': 0.7532364070902211, 'acc_norm_stderr': 0.004302468610485184}, 'boolq': {'acc': 0.6474006116207951, 'acc_stderr': 0.00835641249356213}, 'piqa': {'acc': 0.7812840043525572, 'acc_stderr': 0.009644731932667558, 'acc_norm': 0.7872687704026116, 'acc_norm_stderr': 0.009548223123047343}, 'arc_challenge': {'acc': 0.4453924914675768, 'acc_stderr': 0.01452398763834408, 'acc_norm': 0.42662116040955633, 'acc_norm_stderr': 0.014453185592920293}, 'winogrande': {'acc': 0.6874506708760852, 'acc_stderr': 0.013027563620748838}}, 'versions': {'arc_easy': 0, 'hellaswag': 0, 'boolq': 1, 'piqa': 0, 'arc_challenge': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
