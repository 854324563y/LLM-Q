[2025-02-23 08:59:20 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc/Llama-2-7b-hf-w4a8', save_dir='./log-calibration-compensation-lwc/quant/Llama-2-7b-hf-w4a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True)
[2025-02-23 09:00:52 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-23 09:00:52 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-23 09:00:52 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-23 09:00:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-23 09:00:59 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:01:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.00906220730394125 norm:0.012926136143505573 max memory_allocated 22562.10693359375 
[2025-02-23 09:02:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.005133308935910463 norm:0.007724015973508358 max memory_allocated 22562.10693359375 
[2025-02-23 09:02:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0036271989811211824 norm:0.005405806005001068 max memory_allocated 22562.10693359375 
[2025-02-23 09:03:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0030294053722172976 norm:0.004220253322273493 max memory_allocated 22562.10693359375 
[2025-02-23 09:03:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.002906515495851636 norm:0.0036271093413233757 max memory_allocated 22562.10693359375 
[2025-02-23 09:04:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0028332090005278587 norm:0.0031607206910848618 max memory_allocated 22562.10693359375 
[2025-02-23 09:04:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0027279281057417393 norm:0.0026813303120434284 max memory_allocated 22562.10693359375 
[2025-02-23 09:05:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0026601566933095455 norm:0.0023791419807821512 max memory_allocated 22562.10693359375 
[2025-02-23 09:06:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0026269354857504368 norm:0.0021519141737371683 max memory_allocated 22562.10693359375 
[2025-02-23 09:06:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0025843018665909767 norm:0.001890946296043694 max memory_allocated 22562.10693359375 
[2025-02-23 09:07:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.002551085315644741 norm:0.001623261021450162 max memory_allocated 22562.10693359375 
[2025-02-23 09:07:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0025538813788443804 norm:0.001466889982111752 max memory_allocated 22562.10693359375 
[2025-02-23 09:08:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.002531114500015974 norm:0.001290192361921072 max memory_allocated 22562.10693359375 
[2025-02-23 09:08:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0024795320350676775 norm:0.0011738869361579418 max memory_allocated 22562.10693359375 
[2025-02-23 09:09:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0024850741028785706 norm:0.0011309615802019835 max memory_allocated 22562.10693359375 
[2025-02-23 09:09:58 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.002459663897752762 norm:0.0010785622289404273 max memory_allocated 22562.10693359375 
[2025-02-23 09:10:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0024364758282899857 norm:0.000987136852927506 max memory_allocated 22562.10693359375 
[2025-02-23 09:11:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.002441895194351673 norm:0.0009354664944112301 max memory_allocated 22562.10693359375 
[2025-02-23 09:11:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.002480581868439913 norm:0.0009597927564755082 max memory_allocated 22562.10693359375 
[2025-02-23 09:12:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.002466624602675438 norm:0.000906798173673451 max memory_allocated 22562.10693359375 
[2025-02-23 09:12:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-23 09:12:25 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:12:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.03405214101076126 norm:0.027721792459487915 max memory_allocated 22562.27880859375 
[2025-02-23 09:13:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.021275607869029045 norm:0.017179546877741814 max memory_allocated 22562.27880859375 
[2025-02-23 09:14:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.015271232463419437 norm:0.010211411863565445 max memory_allocated 22562.27880859375 
[2025-02-23 09:14:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.015538482926785946 norm:0.011792642064392567 max memory_allocated 22562.27880859375 
[2025-02-23 09:15:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.014593925327062607 norm:0.010515723377466202 max memory_allocated 22562.27880859375 
[2025-02-23 09:15:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.013607250526547432 norm:0.010541195049881935 max memory_allocated 22562.27880859375 
[2025-02-23 09:16:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.013217531144618988 norm:0.009250415489077568 max memory_allocated 22562.27880859375 
[2025-02-23 09:16:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.012685833498835564 norm:0.008575594052672386 max memory_allocated 22562.27880859375 
[2025-02-23 09:17:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.012854148633778095 norm:0.008341732434928417 max memory_allocated 22562.27880859375 
[2025-02-23 09:18:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.012834696099162102 norm:0.008123883977532387 max memory_allocated 22562.27880859375 
[2025-02-23 09:18:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.012690321542322636 norm:0.00791865587234497 max memory_allocated 22562.27880859375 
[2025-02-23 09:19:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.01288978848606348 norm:0.008029692806303501 max memory_allocated 22562.27880859375 
[2025-02-23 09:19:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.01273653656244278 norm:0.007808094844222069 max memory_allocated 22562.27880859375 
[2025-02-23 09:20:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.012261883355677128 norm:0.007179817650467157 max memory_allocated 22562.27880859375 
[2025-02-23 09:20:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.012451756745576859 norm:0.00727934530004859 max memory_allocated 22562.27880859375 
[2025-02-23 09:21:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.012386439368128777 norm:0.006821421906352043 max memory_allocated 22562.27880859375 
[2025-02-23 09:22:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.013047999702394009 norm:0.007467126473784447 max memory_allocated 22562.27880859375 
[2025-02-23 09:22:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.012043639086186886 norm:0.00659843347966671 max memory_allocated 22562.27880859375 
[2025-02-23 09:23:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.012216612696647644 norm:0.006403593346476555 max memory_allocated 22562.27880859375 
[2025-02-23 09:23:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.012457695789635181 norm:0.006824760232120752 max memory_allocated 22562.27880859375 
[2025-02-23 09:23:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-23 09:23:54 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 09:24:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.029819583520293236 norm:0.014752032235264778 max memory_allocated 22562.45068359375 
[2025-02-23 09:25:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.02078009583055973 norm:0.009272044524550438 max memory_allocated 22562.45068359375 
[2025-02-23 09:25:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.016972042620182037 norm:0.006352717988193035 max memory_allocated 22562.45068359375 
[2025-02-23 09:26:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.015646014362573624 norm:0.004884240683168173 max memory_allocated 22562.45068359375 
[2025-02-23 09:26:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.014925195835530758 norm:0.004035545513033867 max memory_allocated 22562.45068359375 
[2025-02-23 09:27:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.014392252080142498 norm:0.003331943415105343 max memory_allocated 22562.45068359375 
[2025-02-23 09:27:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.014072246849536896 norm:0.0028565973043441772 max memory_allocated 22562.45068359375 
[2025-02-23 09:28:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.013891072943806648 norm:0.0023917939979583025 max memory_allocated 22562.45068359375 
[2025-02-23 09:28:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.013791984878480434 norm:0.0019820400048047304 max memory_allocated 22562.45068359375 
[2025-02-23 09:29:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.013733083382248878 norm:0.001635189400985837 max memory_allocated 22562.45068359375 
[2025-02-23 09:30:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.013665257953107357 norm:0.0014042581897228956 max memory_allocated 22562.45068359375 
[2025-02-23 09:30:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.013689159415662289 norm:0.0014798546908423305 max memory_allocated 22562.45068359375 
[2025-02-23 09:31:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.013723907992243767 norm:0.001479289960116148 max memory_allocated 22562.45068359375 
[2025-02-23 09:31:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.013766536489129066 norm:0.0013550597941502929 max memory_allocated 22562.45068359375 
[2025-02-23 09:32:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.013636007905006409 norm:0.0011624330654740334 max memory_allocated 22562.45068359375 
[2025-02-23 09:32:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.013618418015539646 norm:0.0011720535112544894 max memory_allocated 22562.45068359375 
[2025-02-23 09:33:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.013647164218127728 norm:0.0011381329968571663 max memory_allocated 22562.45068359375 
[2025-02-23 09:34:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.013672737404704094 norm:0.0011306792730465531 max memory_allocated 22562.45068359375 
[2025-02-23 09:34:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.013697854243218899 norm:0.0010426952503621578 max memory_allocated 22562.45068359375 
[2025-02-23 09:35:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.013652763329446316 norm:0.0010349894873797894 max memory_allocated 22562.45068359375 
[2025-02-23 09:35:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-23 09:35:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.03292178735136986 norm:0.0035705752670764923 max memory_allocated 22562.50732421875 
[2025-02-23 09:36:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.025048628449440002 norm:0.0009152895072475076 max memory_allocated 22562.50732421875 
[2025-02-23 09:37:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.020820794627070427 norm:0.0006667627603746951 max memory_allocated 22562.50732421875 
[2025-02-23 09:37:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.01921982690691948 norm:0.0004296375554986298 max memory_allocated 22562.50732421875 
[2025-02-23 09:38:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.018409691751003265 norm:0.00035088506410829723 max memory_allocated 22562.50732421875 
[2025-02-23 09:38:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.017955312505364418 norm:0.0002934487129095942 max memory_allocated 22562.50732421875 
[2025-02-23 09:39:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.017739394679665565 norm:0.00024753075558692217 max memory_allocated 22562.50732421875 
[2025-02-23 09:39:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.01770334132015705 norm:0.0002533498627599329 max memory_allocated 22562.50732421875 
[2025-02-23 09:40:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.01770423911511898 norm:0.0002693991700652987 max memory_allocated 22562.50732421875 
[2025-02-23 09:41:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.017647797241806984 norm:0.0002089274494210258 max memory_allocated 22562.50732421875 
[2025-02-23 09:41:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.017614176496863365 norm:0.00020279498130548745 max memory_allocated 22562.50732421875 
[2025-02-23 09:42:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.017567098140716553 norm:0.00019692220666911453 max memory_allocated 22562.50732421875 
[2025-02-23 09:42:43 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.01768602430820465 norm:0.00022196014469955117 max memory_allocated 22562.50732421875 
[2025-02-23 09:43:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.01761331781744957 norm:0.00020085633150301874 max memory_allocated 22562.50732421875 
[2025-02-23 09:43:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.017649218440055847 norm:0.0002085593150695786 max memory_allocated 22562.50732421875 
[2025-02-23 09:44:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.017620157450437546 norm:0.0001870010164566338 max memory_allocated 22562.50732421875 
[2025-02-23 09:44:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.01759638637304306 norm:0.0001733157696435228 max memory_allocated 22562.50732421875 
[2025-02-23 09:45:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.017585404217243195 norm:0.00019819874432869256 max memory_allocated 22562.50732421875 
[2025-02-23 09:46:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.017724841833114624 norm:0.00021447330072987825 max memory_allocated 22562.50732421875 
[2025-02-23 09:46:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.01764771342277527 norm:0.00019779225112870336 max memory_allocated 22562.50732421875 
[2025-02-23 09:46:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-23 09:47:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.039222925901412964 norm:0.001855563372373581 max memory_allocated 22562.67919921875 
[2025-02-23 09:47:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.029839958995580673 norm:0.0006734216003678739 max memory_allocated 22562.67919921875 
[2025-02-23 09:48:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.024531936272978783 norm:0.00043089280370622873 max memory_allocated 22562.67919921875 
[2025-02-23 09:49:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.022735198959708214 norm:0.00030301077640615404 max memory_allocated 22562.67919921875 
[2025-02-23 09:49:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.02174699679017067 norm:0.00022511932183988392 max memory_allocated 22562.67919921875 
[2025-02-23 09:50:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.021304748952388763 norm:0.00020679675799328834 max memory_allocated 22562.67919921875 
[2025-02-23 09:50:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.021147245541214943 norm:0.00020189161296002567 max memory_allocated 22562.67919921875 
[2025-02-23 09:51:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.021011091768741608 norm:0.00017705114441923797 max memory_allocated 22562.67919921875 
[2025-02-23 09:51:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.021021010354161263 norm:0.00019207550212740898 max memory_allocated 22562.67919921875 
[2025-02-23 09:52:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.02099810726940632 norm:0.00016800902085378766 max memory_allocated 22562.67919921875 
[2025-02-23 09:53:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.020942572504281998 norm:0.00015857174003031105 max memory_allocated 22562.67919921875 
[2025-02-23 09:53:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.020896241068840027 norm:0.00015715767221990973 max memory_allocated 22562.67919921875 
[2025-02-23 09:54:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.020891770720481873 norm:0.00015868009359110147 max memory_allocated 22562.67919921875 
[2025-02-23 09:54:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.020881937816739082 norm:0.0001765185734257102 max memory_allocated 22562.67919921875 
[2025-02-23 09:55:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.020824410021305084 norm:0.00015980290481820703 max memory_allocated 22562.67919921875 
[2025-02-23 09:55:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.020792027935385704 norm:0.00016234040958806872 max memory_allocated 22562.67919921875 
[2025-02-23 09:56:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.020797664299607277 norm:0.00016682763816788793 max memory_allocated 22562.67919921875 
[2025-02-23 09:56:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.020845433697104454 norm:0.00015491730300709605 max memory_allocated 22562.67919921875 
[2025-02-23 09:57:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.020846029743552208 norm:0.00016985532420221716 max memory_allocated 22562.67919921875 
[2025-02-23 09:58:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.020820489153265953 norm:0.0001591175823705271 max memory_allocated 22562.67919921875 
[2025-02-23 09:58:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-23 09:58:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.039009008556604385 norm:0.001363110262900591 max memory_allocated 22562.85107421875 
[2025-02-23 09:59:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.03129787743091583 norm:0.0004121568927075714 max memory_allocated 22562.85107421875 
[2025-02-23 10:00:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0266178697347641 norm:0.0002844605769496411 max memory_allocated 22562.85107421875 
[2025-02-23 10:00:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0249259602278471 norm:0.00020796971512027085 max memory_allocated 22562.85107421875 
[2025-02-23 10:01:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.02410360798239708 norm:0.00018917056149803102 max memory_allocated 22562.85107421875 
[2025-02-23 10:01:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.023728936910629272 norm:0.00016306989709846675 max memory_allocated 22562.85107421875 
[2025-02-23 10:02:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.023557158187031746 norm:0.00017182852025143802 max memory_allocated 22562.85107421875 
[2025-02-23 10:02:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.023566393181681633 norm:0.00018127996008843184 max memory_allocated 22562.85107421875 
[2025-02-23 10:03:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.023490021005272865 norm:0.00015490129590034485 max memory_allocated 22562.85107421875 
[2025-02-23 10:03:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.023387333378195763 norm:0.00013915587624069303 max memory_allocated 22562.85107421875 
[2025-02-23 10:04:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.023354999721050262 norm:0.00013650582695845515 max memory_allocated 22562.85107421875 
[2025-02-23 10:05:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.023324137553572655 norm:0.0001452328433515504 max memory_allocated 22562.85107421875 
[2025-02-23 10:05:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.02331651747226715 norm:0.00015682048979215324 max memory_allocated 22562.85107421875 
[2025-02-23 10:06:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.023300331085920334 norm:0.0001531235029688105 max memory_allocated 22562.85107421875 
[2025-02-23 10:06:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.023333167657256126 norm:0.00015979169984348118 max memory_allocated 22562.85107421875 
[2025-02-23 10:07:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.023330718278884888 norm:0.00014186043699737638 max memory_allocated 22562.85107421875 
[2025-02-23 10:07:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.023306390270590782 norm:0.00013956493057776242 max memory_allocated 22562.85107421875 
[2025-02-23 10:08:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.023296905681490898 norm:0.00013433564163278788 max memory_allocated 22562.85107421875 
[2025-02-23 10:09:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.02332880161702633 norm:0.0001550395681988448 max memory_allocated 22562.85107421875 
[2025-02-23 10:09:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0233258418738842 norm:0.00013906913227401674 max memory_allocated 22562.85107421875 
[2025-02-23 10:09:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-23 10:10:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.04577028378844261 norm:0.0009085824131034315 max memory_allocated 22563.02294921875 
[2025-02-23 10:10:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.03633555397391319 norm:0.0003905117337126285 max memory_allocated 22563.02294921875 
[2025-02-23 10:11:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.030645806342363358 norm:0.0002883033885154873 max memory_allocated 22563.02294921875 
[2025-02-23 10:12:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.028622282668948174 norm:0.00022490553965326399 max memory_allocated 22563.02294921875 
[2025-02-23 10:12:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.02766367420554161 norm:0.00019899080507457256 max memory_allocated 22563.02294921875 
[2025-02-23 10:13:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.02722075954079628 norm:0.00019841239554807544 max memory_allocated 22563.02294921875 
[2025-02-23 10:13:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.026939917355775833 norm:0.00017155556997749954 max memory_allocated 22563.02294921875 
[2025-02-23 10:14:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.02681918442249298 norm:0.00017635896801948547 max memory_allocated 22563.02294921875 
[2025-02-23 10:14:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.026722637936472893 norm:0.0001760058949002996 max memory_allocated 22563.02294921875 
[2025-02-23 10:15:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.02661689929664135 norm:0.00015821300621610135 max memory_allocated 22563.02294921875 
[2025-02-23 10:15:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.026516452431678772 norm:0.0001609583559911698 max memory_allocated 22563.02294921875 
[2025-02-23 10:16:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.02644539065659046 norm:0.0001546136918477714 max memory_allocated 22563.02294921875 
[2025-02-23 10:17:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.026408726349473 norm:0.00016263687575701624 max memory_allocated 22563.02294921875 
[2025-02-23 10:17:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.02637612260878086 norm:0.0001643797440920025 max memory_allocated 22563.02294921875 
[2025-02-23 10:18:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.026379944756627083 norm:0.00018356855434831232 max memory_allocated 22563.02294921875 
[2025-02-23 10:18:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.026392441242933273 norm:0.0001998519292101264 max memory_allocated 22563.02294921875 
[2025-02-23 10:19:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.026378387585282326 norm:0.00017484990530647337 max memory_allocated 22563.02294921875 
[2025-02-23 10:19:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.026402581483125687 norm:0.00017987590399570763 max memory_allocated 22563.02294921875 
[2025-02-23 10:20:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.02638726495206356 norm:0.0001768210349837318 max memory_allocated 22563.02294921875 
[2025-02-23 10:21:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.026387296617031097 norm:0.0001634655345696956 max memory_allocated 22563.02294921875 
[2025-02-23 10:21:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-23 10:21:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.05101115629076958 norm:0.002015438163653016 max memory_allocated 22563.19482421875 
[2025-02-23 10:22:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.039752714335918427 norm:0.0015640145866200328 max memory_allocated 22563.19482421875 
[2025-02-23 10:22:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.033582594245672226 norm:0.000336469616740942 max memory_allocated 22563.19482421875 
[2025-02-23 10:23:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.031334053725004196 norm:0.00023505999706685543 max memory_allocated 22563.19482421875 
[2025-02-23 10:24:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.030375275760889053 norm:0.00022141041699796915 max memory_allocated 22563.19482421875 
[2025-02-23 10:24:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.029896682128310204 norm:0.00018737719801720232 max memory_allocated 22563.19482421875 
[2025-02-23 10:25:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.029688606038689613 norm:0.0002018852683249861 max memory_allocated 22563.19482421875 
[2025-02-23 10:25:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.02954583242535591 norm:0.00017690681852400303 max memory_allocated 22563.19482421875 
[2025-02-23 10:26:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.02939424104988575 norm:0.00017777617904357612 max memory_allocated 22563.19482421875 
[2025-02-23 10:26:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.029303330928087234 norm:0.00017629082140047103 max memory_allocated 22563.19482421875 
[2025-02-23 10:27:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.029284894466400146 norm:0.0001870491250883788 max memory_allocated 22563.19482421875 
[2025-02-23 10:27:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.02922930009663105 norm:0.00016922342183534056 max memory_allocated 22563.19482421875 
[2025-02-23 10:28:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.02918028086423874 norm:0.0001691556826699525 max memory_allocated 22563.19482421875 
[2025-02-23 10:29:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.029129181057214737 norm:0.0001657487009651959 max memory_allocated 22563.19482421875 
[2025-02-23 10:29:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.029130831360816956 norm:0.0001708509516902268 max memory_allocated 22563.19482421875 
[2025-02-23 10:30:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.029103167355060577 norm:0.00015960109885782003 max memory_allocated 22563.19482421875 
[2025-02-23 10:30:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.02906215935945511 norm:0.00015515688573941588 max memory_allocated 22563.19482421875 
[2025-02-23 10:31:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.02903667651116848 norm:0.0001487781701143831 max memory_allocated 22563.19482421875 
[2025-02-23 10:31:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.02903497964143753 norm:0.00015164559590630233 max memory_allocated 22563.19482421875 
[2025-02-23 10:32:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.029014786705374718 norm:0.0001587563892826438 max memory_allocated 22563.19482421875 
[2025-02-23 10:32:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-23 10:33:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.050791285932064056 norm:0.000650072586722672 max memory_allocated 22563.36669921875 
[2025-02-23 10:33:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.04181750863790512 norm:0.0003593524161260575 max memory_allocated 22563.36669921875 
[2025-02-23 10:34:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.03603377565741539 norm:0.00025124443345703185 max memory_allocated 22563.36669921875 
[2025-02-23 10:34:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.033971771597862244 norm:0.00018638720212038606 max memory_allocated 22563.36669921875 
[2025-02-23 10:35:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.033126793801784515 norm:0.00017486375872977078 max memory_allocated 22563.36669921875 
[2025-02-23 10:36:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0326794870197773 norm:0.00017437311180401593 max memory_allocated 22563.36669921875 
[2025-02-23 10:36:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.03249729797244072 norm:0.00017011311138048768 max memory_allocated 22563.36669921875 
[2025-02-23 10:37:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.03235051408410072 norm:0.0001456245081499219 max memory_allocated 22563.36669921875 
[2025-02-23 10:37:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.03221278265118599 norm:0.0001395770232193172 max memory_allocated 22563.36669921875 
[2025-02-23 10:38:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.03214765712618828 norm:0.0001490545109845698 max memory_allocated 22563.36669921875 
[2025-02-23 10:38:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.03208814933896065 norm:0.0001483968080719933 max memory_allocated 22563.36669921875 
[2025-02-23 10:39:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.03201674297451973 norm:0.00013516652688849717 max memory_allocated 22563.36669921875 
[2025-02-23 10:40:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.03196156769990921 norm:0.00013405333447735757 max memory_allocated 22563.36669921875 
[2025-02-23 10:40:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.031972676515579224 norm:0.0001372412807540968 max memory_allocated 22563.36669921875 
[2025-02-23 10:41:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.031944550573825836 norm:0.00014426854613702744 max memory_allocated 22563.36669921875 
[2025-02-23 10:41:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.03198152035474777 norm:0.00015534559497609735 max memory_allocated 22563.36669921875 
[2025-02-23 10:42:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.032092977315187454 norm:0.00017565210873726755 max memory_allocated 22563.36669921875 
[2025-02-23 10:42:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.032041072845458984 norm:0.00015757876099087298 max memory_allocated 22563.36669921875 
[2025-02-23 10:43:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.03198903799057007 norm:0.00014642623136751354 max memory_allocated 22563.36669921875 
[2025-02-23 10:43:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.031982697546482086 norm:0.00014133549120742828 max memory_allocated 22563.36669921875 
[2025-02-23 10:44:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-23 10:44:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.05606721341609955 norm:0.002417539246380329 max memory_allocated 22563.53857421875 
[2025-02-23 10:45:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.04567629098892212 norm:0.0008962558349594474 max memory_allocated 22563.53857421875 
[2025-02-23 10:45:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.038938406854867935 norm:0.0003651109291240573 max memory_allocated 22563.53857421875 
[2025-02-23 10:46:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.03668481856584549 norm:0.00023403388331644237 max memory_allocated 22563.53857421875 
[2025-02-23 10:46:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.03571194410324097 norm:0.00019067544781137258 max memory_allocated 22563.53857421875 
[2025-02-23 10:47:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.035242628306150436 norm:0.00018722550885286182 max memory_allocated 22563.53857421875 
[2025-02-23 10:48:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.03501315414905548 norm:0.0001629386533750221 max memory_allocated 22563.53857421875 
[2025-02-23 10:48:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.03487067669630051 norm:0.00015257028280757368 max memory_allocated 22563.53857421875 
[2025-02-23 10:49:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.034787263721227646 norm:0.000140087038744241 max memory_allocated 22563.53857421875 
[2025-02-23 10:49:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.03471912816166878 norm:0.00014041054237168282 max memory_allocated 22563.53857421875 
[2025-02-23 10:50:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.034645963460206985 norm:0.0001453962759114802 max memory_allocated 22563.53857421875 
[2025-02-23 10:50:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.034607335925102234 norm:0.00013714506349060684 max memory_allocated 22563.53857421875 
[2025-02-23 10:51:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.034542910754680634 norm:0.00013837033475283533 max memory_allocated 22563.53857421875 
[2025-02-23 10:52:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.03449968621134758 norm:0.00013234125799499452 max memory_allocated 22563.53857421875 
[2025-02-23 10:52:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0345066636800766 norm:0.0001310805673711002 max memory_allocated 22563.53857421875 
[2025-02-23 10:53:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0344880074262619 norm:0.00013003475032746792 max memory_allocated 22563.53857421875 
[2025-02-23 10:53:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.03445544093847275 norm:0.00013310940994415432 max memory_allocated 22563.53857421875 
[2025-02-23 10:54:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.03442062810063362 norm:0.00013208971358835697 max memory_allocated 22563.53857421875 
[2025-02-23 10:54:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.03439316153526306 norm:0.00012611408601514995 max memory_allocated 22563.53857421875 
[2025-02-23 10:55:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.034367695450782776 norm:0.00013179339293856174 max memory_allocated 22563.53857421875 
[2025-02-23 10:55:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-23 10:56:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.057361043989658356 norm:0.0011125491000711918 max memory_allocated 22563.71044921875 
[2025-02-23 10:56:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.04833649843931198 norm:0.0006469520158134401 max memory_allocated 22563.71044921875 
[2025-02-23 10:57:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.041429243981838226 norm:0.0003211213042959571 max memory_allocated 22563.71044921875 
[2025-02-23 10:57:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.039003729820251465 norm:0.00018811164773069322 max memory_allocated 22563.71044921875 
[2025-02-23 10:58:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.03806709498167038 norm:0.00015070057997945696 max memory_allocated 22563.71044921875 
[2025-02-23 10:59:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.037657611072063446 norm:0.00014499531243927777 max memory_allocated 22563.71044921875 
[2025-02-23 10:59:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0374639518558979 norm:0.00012982834596186876 max memory_allocated 22563.71044921875 
[2025-02-23 11:00:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.03736138343811035 norm:0.00013099263014737517 max memory_allocated 22563.71044921875 
[2025-02-23 11:00:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.03730010613799095 norm:0.00012814346700906754 max memory_allocated 22563.71044921875 
[2025-02-23 11:01:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.03718888759613037 norm:0.00012718630023300648 max memory_allocated 22563.71044921875 
[2025-02-23 11:01:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.03707490488886833 norm:0.00012601050548255444 max memory_allocated 22563.71044921875 
[2025-02-23 11:02:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.03703591972589493 norm:0.0001268269115826115 max memory_allocated 22563.71044921875 
[2025-02-23 11:02:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.03699563443660736 norm:0.0001271758956136182 max memory_allocated 22563.71044921875 
[2025-02-23 11:03:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.03697722405195236 norm:0.00012868631165474653 max memory_allocated 22563.71044921875 
[2025-02-23 11:04:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.036945901811122894 norm:0.000128318220959045 max memory_allocated 22563.71044921875 
[2025-02-23 11:04:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.036907076835632324 norm:0.0001268445630557835 max memory_allocated 22563.71044921875 
[2025-02-23 11:05:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.036908745765686035 norm:0.00012750679161399603 max memory_allocated 22563.71044921875 
[2025-02-23 11:05:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0368860587477684 norm:0.00012850646453443915 max memory_allocated 22563.71044921875 
[2025-02-23 11:06:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.036834876984357834 norm:0.00012469386274460703 max memory_allocated 22563.71044921875 
[2025-02-23 11:06:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.036778923124074936 norm:0.00012017237168038264 max memory_allocated 22563.71044921875 
[2025-02-23 11:07:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-23 11:07:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.06018761172890663 norm:0.001486227847635746 max memory_allocated 22563.88232421875 
[2025-02-23 11:08:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.049986280500888824 norm:0.0007188162999227643 max memory_allocated 22563.88232421875 
[2025-02-23 11:08:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.04315149039030075 norm:0.00033841858385130763 max memory_allocated 22563.88232421875 
[2025-02-23 11:09:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.04068997874855995 norm:0.00021196181478444487 max memory_allocated 22563.88232421875 
[2025-02-23 11:09:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.03972533345222473 norm:0.00017881541862152517 max memory_allocated 22563.88232421875 
[2025-02-23 11:10:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.03923170268535614 norm:0.00016595832130406052 max memory_allocated 22563.88232421875 
[2025-02-23 11:11:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.03897585719823837 norm:0.0001605431898497045 max memory_allocated 22563.88232421875 
[2025-02-23 11:11:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.038854341953992844 norm:0.0001567935396451503 max memory_allocated 22563.88232421875 
[2025-02-23 11:12:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.03877392038702965 norm:0.00015607722161803395 max memory_allocated 22563.88232421875 
[2025-02-23 11:12:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.03871673345565796 norm:0.00014817742339801043 max memory_allocated 22563.88232421875 
[2025-02-23 11:13:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.03864196687936783 norm:0.00014580527204088867 max memory_allocated 22563.88232421875 
[2025-02-23 11:13:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.03861503303050995 norm:0.00014152578660286963 max memory_allocated 22563.88232421875 
[2025-02-23 11:14:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.03857234865427017 norm:0.0001385522773489356 max memory_allocated 22563.88232421875 
[2025-02-23 11:14:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.038530588150024414 norm:0.00013656321971211582 max memory_allocated 22563.88232421875 
[2025-02-23 11:15:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.038516879081726074 norm:0.00013468100223690271 max memory_allocated 22563.88232421875 
[2025-02-23 11:16:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.03849807381629944 norm:0.0001337690482614562 max memory_allocated 22563.88232421875 
[2025-02-23 11:16:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.03846397250890732 norm:0.0001400750916218385 max memory_allocated 22563.88232421875 
[2025-02-23 11:17:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0384288914501667 norm:0.0001378290035063401 max memory_allocated 22563.88232421875 
[2025-02-23 11:17:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.038393110036849976 norm:0.0001385823416057974 max memory_allocated 22563.88232421875 
[2025-02-23 11:18:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.03836309537291527 norm:0.0001395170111209154 max memory_allocated 22563.88232421875 
[2025-02-23 11:18:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-23 11:19:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.05736703798174858 norm:0.000931324961129576 max memory_allocated 22564.05419921875 
[2025-02-23 11:19:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.04935545474290848 norm:0.000505979114677757 max memory_allocated 22564.05419921875 
[2025-02-23 11:20:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.04365227371454239 norm:0.00028703227872028947 max memory_allocated 22564.05419921875 
[2025-02-23 11:20:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.041601844131946564 norm:0.00018748882575891912 max memory_allocated 22564.05419921875 
[2025-02-23 11:21:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.040719494223594666 norm:0.00014985425514169037 max memory_allocated 22564.05419921875 
[2025-02-23 11:21:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.040271274745464325 norm:0.00013953185407444835 max memory_allocated 22564.05419921875 
[2025-02-23 11:22:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.04006187617778778 norm:0.00014850472507532686 max memory_allocated 22564.05419921875 
[2025-02-23 11:23:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.03990808129310608 norm:0.0001306371414102614 max memory_allocated 22564.05419921875 
[2025-02-23 11:23:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.03985336422920227 norm:0.00013574030890595168 max memory_allocated 22564.05419921875 
[2025-02-23 11:24:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.03973085805773735 norm:0.00012574523861985654 max memory_allocated 22564.05419921875 
[2025-02-23 11:24:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.03972920775413513 norm:0.00012819301628042012 max memory_allocated 22564.05419921875 
[2025-02-23 11:25:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.03963863104581833 norm:0.00011997549154330045 max memory_allocated 22564.05419921875 
[2025-02-23 11:25:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.039593156427145004 norm:0.0001200754995807074 max memory_allocated 22564.05419921875 
[2025-02-23 11:26:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0395519882440567 norm:0.00011395764886401594 max memory_allocated 22564.05419921875 
[2025-02-23 11:26:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.039531342685222626 norm:0.00010495517199160531 max memory_allocated 22564.05419921875 
[2025-02-23 11:27:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.039506230503320694 norm:0.00010711731010815129 max memory_allocated 22564.05419921875 
[2025-02-23 11:28:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.039472684264183044 norm:0.00010561124508967623 max memory_allocated 22564.05419921875 
[2025-02-23 11:28:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.03944258764386177 norm:0.00010337466665077955 max memory_allocated 22564.05419921875 
[2025-02-23 11:29:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.03941638767719269 norm:0.00010419668251415715 max memory_allocated 22564.05419921875 
[2025-02-23 11:29:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0393914133310318 norm:0.00010831891995621845 max memory_allocated 22564.05419921875 
[2025-02-23 11:29:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-23 11:30:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.05627540126442909 norm:0.0016355180414393544 max memory_allocated 22564.22607421875 
[2025-02-23 11:31:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.04822210967540741 norm:0.0005742923240177333 max memory_allocated 22564.22607421875 
[2025-02-23 11:31:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.043007560074329376 norm:0.0002693773712962866 max memory_allocated 22564.22607421875 
[2025-02-23 11:32:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.04110126942396164 norm:0.00017264712369069457 max memory_allocated 22564.22607421875 
[2025-02-23 11:32:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.040239546447992325 norm:0.00015403761062771082 max memory_allocated 22564.22607421875 
[2025-02-23 11:33:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.039787229150533676 norm:0.000152090797200799 max memory_allocated 22564.22607421875 
[2025-02-23 11:33:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.03951149433851242 norm:0.0001427837269147858 max memory_allocated 22564.22607421875 
[2025-02-23 11:34:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.03935864567756653 norm:0.00014721829211339355 max memory_allocated 22564.22607421875 
[2025-02-23 11:35:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.03925717994570732 norm:0.00013089140702504665 max memory_allocated 22564.22607421875 
[2025-02-23 11:35:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.03918109089136124 norm:0.00011702349002007395 max memory_allocated 22564.22607421875 
[2025-02-23 11:36:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.03911829739809036 norm:0.00011805591202573851 max memory_allocated 22564.22607421875 
[2025-02-23 11:36:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.039090223610401154 norm:0.00011918287782464176 max memory_allocated 22564.22607421875 
[2025-02-23 11:37:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.03903850540518761 norm:0.0001214658550452441 max memory_allocated 22564.22607421875 
[2025-02-23 11:37:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.03902125731110573 norm:0.00011079436808358878 max memory_allocated 22564.22607421875 
[2025-02-23 11:38:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.03900537267327309 norm:0.00010185786231886595 max memory_allocated 22564.22607421875 
[2025-02-23 11:39:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.03898968547582626 norm:9.92596906144172e-05 max memory_allocated 22564.22607421875 
[2025-02-23 11:39:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.038995906710624695 norm:0.00011195128899998963 max memory_allocated 22564.22607421875 
[2025-02-23 11:40:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.03895527869462967 norm:0.00010909493721555918 max memory_allocated 22564.22607421875 
[2025-02-23 11:40:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.038911011070013046 norm:0.00010171849862672389 max memory_allocated 22564.22607421875 
[2025-02-23 11:41:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.038901932537555695 norm:0.00010378191655036062 max memory_allocated 22564.22607421875 
[2025-02-23 11:41:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-23 11:42:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.055205654352903366 norm:0.0006840366404503584 max memory_allocated 22564.39794921875 
[2025-02-23 11:42:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.04841630160808563 norm:0.00036035673110745847 max memory_allocated 22564.39794921875 
[2025-02-23 11:43:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.043452613055706024 norm:0.00021630219998769462 max memory_allocated 22564.39794921875 
[2025-02-23 11:43:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.041735876351594925 norm:0.000152607390191406 max memory_allocated 22564.39794921875 
[2025-02-23 11:44:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.04099663719534874 norm:0.00012960188905708492 max memory_allocated 22564.39794921875 
[2025-02-23 11:44:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.040590960532426834 norm:0.000127557257656008 max memory_allocated 22564.39794921875 
[2025-02-23 11:45:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.040351346135139465 norm:0.0001231905771419406 max memory_allocated 22564.39794921875 
[2025-02-23 11:45:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.04020611569285393 norm:0.00011303689097985625 max memory_allocated 22564.39794921875 
[2025-02-23 11:46:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.040104009211063385 norm:0.00010035747254732996 max memory_allocated 22564.39794921875 
[2025-02-23 11:47:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.040029481053352356 norm:9.316112118540332e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:47:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.039958879351615906 norm:9.55434879870154e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:48:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.03991641104221344 norm:9.739658707985654e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:48:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.03991448134183884 norm:9.405267337569967e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:49:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.039898261427879333 norm:8.40722641441971e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:49:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.03986668959259987 norm:8.264237840194255e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:50:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.03982875868678093 norm:8.17495965748094e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:51:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.03980448096990585 norm:8.221040479838848e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:51:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.03978210687637329 norm:8.27746553113684e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:52:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.03977273404598236 norm:8.115740638459101e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:52:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.039751093834638596 norm:7.827388617442921e-05 max memory_allocated 22564.39794921875 
[2025-02-23 11:52:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-23 11:53:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.05836555361747742 norm:0.001486120279878378 max memory_allocated 22564.56982421875 
[2025-02-23 11:54:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.049573786556720734 norm:0.0004508259298745543 max memory_allocated 22564.56982421875 
[2025-02-23 11:54:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.043847810477018356 norm:0.00026566555607132614 max memory_allocated 22564.56982421875 
[2025-02-23 11:55:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.041950564831495285 norm:0.00019342672021593899 max memory_allocated 22564.56982421875 
[2025-02-23 11:55:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.041063740849494934 norm:0.00016326371405739337 max memory_allocated 22564.56982421875 
[2025-02-23 11:56:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.04054930806159973 norm:0.00014976129750721157 max memory_allocated 22564.56982421875 
[2025-02-23 11:56:51 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0402846559882164 norm:0.00014023232506588101 max memory_allocated 22564.56982421875 
[2025-02-23 11:57:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.04010758921504021 norm:0.0001341956085525453 max memory_allocated 22564.56982421875 
[2025-02-23 11:57:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.039986949414014816 norm:0.00012633237929549068 max memory_allocated 22564.56982421875 
[2025-02-23 11:58:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.039870284497737885 norm:0.00011729995458154008 max memory_allocated 22564.56982421875 
[2025-02-23 11:59:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0397934764623642 norm:0.00010658898099791259 max memory_allocated 22564.56982421875 
[2025-02-23 11:59:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.039742037653923035 norm:0.00011446711141616106 max memory_allocated 22564.56982421875 
[2025-02-23 12:00:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.03966357186436653 norm:0.00011110164632555097 max memory_allocated 22564.56982421875 
[2025-02-23 12:00:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.03960110619664192 norm:0.00010160388046642765 max memory_allocated 22564.56982421875 
[2025-02-23 12:01:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.039554521441459656 norm:9.830761700868607e-05 max memory_allocated 22564.56982421875 
[2025-02-23 12:01:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.03950140252709389 norm:9.415596286999062e-05 max memory_allocated 22564.56982421875 
[2025-02-23 12:02:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.039465513080358505 norm:9.094392589759082e-05 max memory_allocated 22564.56982421875 
[2025-02-23 12:03:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.03941460698843002 norm:9.108208178076893e-05 max memory_allocated 22564.56982421875 
[2025-02-23 12:03:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.03938887268304825 norm:8.994743984658271e-05 max memory_allocated 22564.56982421875 
[2025-02-23 12:04:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.03937380015850067 norm:8.621036249678582e-05 max memory_allocated 22564.56982421875 
[2025-02-23 12:04:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-23 12:04:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.05923029035329819 norm:0.0017697579460218549 max memory_allocated 22564.74169921875 
[2025-02-23 12:05:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.05063368380069733 norm:0.0005891065229661763 max memory_allocated 22564.74169921875 
[2025-02-23 12:06:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.04473075270652771 norm:0.0002703665813896805 max memory_allocated 22564.74169921875 
[2025-02-23 12:06:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.042900461703538895 norm:0.00021944301261100918 max memory_allocated 22564.74169921875 
[2025-02-23 12:07:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.04206039756536484 norm:0.00020145683083683252 max memory_allocated 22564.74169921875 
[2025-02-23 12:07:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0415118932723999 norm:0.00017454242333769798 max memory_allocated 22564.74169921875 
[2025-02-23 12:08:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.04121098294854164 norm:0.00016138539649546146 max memory_allocated 22564.74169921875 
[2025-02-23 12:08:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.04103076085448265 norm:0.000154029912664555 max memory_allocated 22564.74169921875 
[2025-02-23 12:09:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.04089915007352829 norm:0.00014932887279428542 max memory_allocated 22564.74169921875 
[2025-02-23 12:10:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.04078782722353935 norm:0.00013445538934320211 max memory_allocated 22564.74169921875 
[2025-02-23 12:10:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.04069806635379791 norm:0.00012551146210171282 max memory_allocated 22564.74169921875 
[2025-02-23 12:11:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.04061897099018097 norm:0.0001226018212037161 max memory_allocated 22564.74169921875 
[2025-02-23 12:11:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.040529947727918625 norm:0.00011588286724872887 max memory_allocated 22564.74169921875 
[2025-02-23 12:12:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.04046854376792908 norm:0.0001139973319368437 max memory_allocated 22564.74169921875 
[2025-02-23 12:12:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.040436092764139175 norm:0.00011098242976004258 max memory_allocated 22564.74169921875 
[2025-02-23 12:13:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.040381837636232376 norm:0.00010872496204683557 max memory_allocated 22564.74169921875 
[2025-02-23 12:13:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.04034357890486717 norm:9.96915769064799e-05 max memory_allocated 22564.74169921875 
[2025-02-23 12:14:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.04031844809651375 norm:9.708893048809841e-05 max memory_allocated 22564.74169921875 
[2025-02-23 12:15:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.040295474231243134 norm:0.00010281975846737623 max memory_allocated 22564.74169921875 
[2025-02-23 12:15:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.04026744142174721 norm:9.413006773684174e-05 max memory_allocated 22564.74169921875 
[2025-02-23 12:15:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-23 12:16:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.05537238344550133 norm:0.0015685564139857888 max memory_allocated 22564.91357421875 
[2025-02-23 12:16:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.04905369132757187 norm:0.0005106024909764528 max memory_allocated 22564.91357421875 
[2025-02-23 12:17:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.04456515237689018 norm:0.0002453891502227634 max memory_allocated 22564.91357421875 
[2025-02-23 12:18:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.043109793215990067 norm:0.00018879835261031985 max memory_allocated 22564.91357421875 
[2025-02-23 12:18:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.04231828451156616 norm:0.00015716598136350513 max memory_allocated 22564.91357421875 
[2025-02-23 12:19:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.0418727844953537 norm:0.00013169244630262256 max memory_allocated 22564.91357421875 
[2025-02-23 12:19:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.04165844991803169 norm:0.00012852097279392183 max memory_allocated 22564.91357421875 
[2025-02-23 12:20:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.041511569172143936 norm:0.0001286410551983863 max memory_allocated 22564.91357421875 
[2025-02-23 12:20:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.04138911888003349 norm:0.00011192303645657375 max memory_allocated 22564.91357421875 
[2025-02-23 12:21:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.04130110889673233 norm:9.487503120908514e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:22:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.041224155575037 norm:9.14341289899312e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:22:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.04118786379694939 norm:7.80391856096685e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:23:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.04115283116698265 norm:8.934242941904813e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:23:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.04110048711299896 norm:8.06673924671486e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:24:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.041075218468904495 norm:7.950052531668916e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:24:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.04104854539036751 norm:7.868195825722069e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:25:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.0410393625497818 norm:7.4622621468734e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:25:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.04102819040417671 norm:7.43891650927253e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:26:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.04097937047481537 norm:7.603569247294217e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:27:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.040975023061037064 norm:7.287385960808024e-05 max memory_allocated 22564.91357421875 
[2025-02-23 12:27:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-23 12:27:51 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.060665313154459 norm:0.0012901520822197199 max memory_allocated 22565.08544921875 
[2025-02-23 12:28:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.05309183895587921 norm:0.00044674435048364103 max memory_allocated 22565.08544921875 
[2025-02-23 12:28:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.047952763736248016 norm:0.00024743544054217637 max memory_allocated 22565.08544921875 
[2025-02-23 12:29:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.04637874662876129 norm:0.00019244910799898207 max memory_allocated 22565.08544921875 
[2025-02-23 12:30:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.045416079461574554 norm:0.00015889831411186606 max memory_allocated 22565.08544921875 
[2025-02-23 12:30:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.04491600766777992 norm:0.0001575029018567875 max memory_allocated 22565.08544921875 
[2025-02-23 12:31:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.04464966431260109 norm:0.00014598485722672194 max memory_allocated 22565.08544921875 
[2025-02-23 12:31:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.04449494555592537 norm:0.00013301995932124555 max memory_allocated 22565.08544921875 
[2025-02-23 12:32:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.044383980333805084 norm:0.00012489547953009605 max memory_allocated 22565.08544921875 
[2025-02-23 12:32:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.04428023099899292 norm:0.00011875930067617446 max memory_allocated 22565.08544921875 
[2025-02-23 12:33:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.04418828710913658 norm:0.00011099036055384204 max memory_allocated 22565.08544921875 
[2025-02-23 12:34:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.044119782745838165 norm:0.00010576623026281595 max memory_allocated 22565.08544921875 
[2025-02-23 12:34:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.044042572379112244 norm:9.932529064826667e-05 max memory_allocated 22565.08544921875 
[2025-02-23 12:35:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.043991606682538986 norm:9.964123455574736e-05 max memory_allocated 22565.08544921875 
[2025-02-23 12:35:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.043934453278779984 norm:9.642123040976003e-05 max memory_allocated 22565.08544921875 
[2025-02-23 12:36:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.04388396814465523 norm:9.283778490498662e-05 max memory_allocated 22565.08544921875 
[2025-02-23 12:36:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.043842069804668427 norm:8.732515561860055e-05 max memory_allocated 22565.08544921875 
[2025-02-23 12:37:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.043807484209537506 norm:8.219642040785402e-05 max memory_allocated 22565.08544921875 
[2025-02-23 12:37:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.043774694204330444 norm:8.351405995199457e-05 max memory_allocated 22565.08544921875 
[2025-02-23 12:38:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.04375384747982025 norm:8.478084055241197e-05 max memory_allocated 22565.08544921875 
[2025-02-23 12:38:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-23 12:39:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.06381313502788544 norm:0.0013922100188210607 max memory_allocated 22565.25732421875 
[2025-02-23 12:39:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.05703043192625046 norm:0.0005284920334815979 max memory_allocated 22565.25732421875 
[2025-02-23 12:40:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.051366303116083145 norm:0.00019949732813984156 max memory_allocated 22565.25732421875 
[2025-02-23 12:40:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.049861084669828415 norm:0.00016304382006637752 max memory_allocated 22565.25732421875 
[2025-02-23 12:41:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.04898509383201599 norm:0.00014574354281648993 max memory_allocated 22565.25732421875 
[2025-02-23 12:42:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.04847124591469765 norm:0.00013191414473112673 max memory_allocated 22565.25732421875 
[2025-02-23 12:42:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.048211339861154556 norm:0.0001281428267247975 max memory_allocated 22565.25732421875 
[2025-02-23 12:43:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.048055075109004974 norm:0.00011688897939166054 max memory_allocated 22565.25732421875 
[2025-02-23 12:43:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.04795413836836815 norm:0.00012029728532070294 max memory_allocated 22565.25732421875 
[2025-02-23 12:44:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.04784109815955162 norm:0.00010321845911676064 max memory_allocated 22565.25732421875 
[2025-02-23 12:44:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.04777276888489723 norm:9.6721007139422e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:45:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.04769010841846466 norm:8.7750653619878e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:46:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.04762989282608032 norm:9.144889918388799e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:46:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.047584209591150284 norm:9.115612192545086e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:47:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.047532275319099426 norm:8.739445183891803e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:47:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.04746551066637039 norm:7.949436258058995e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:48:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.0474202036857605 norm:7.228816684801131e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:48:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.04738793149590492 norm:7.824512431398034e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:49:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.04737049341201782 norm:8.099919796222821e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:49:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0473577156662941 norm:7.307440682779998e-05 max memory_allocated 22565.25732421875 
[2025-02-23 12:50:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-23 12:50:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.07070524990558624 norm:0.002169113140553236 max memory_allocated 22565.42919921875 
[2025-02-23 12:51:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.06278355419635773 norm:0.0008129487396217883 max memory_allocated 22565.42919921875 
[2025-02-23 12:51:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.05692509561777115 norm:0.0003236917546018958 max memory_allocated 22565.42919921875 
[2025-02-23 12:52:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.055348221212625504 norm:0.00022197794169187546 max memory_allocated 22565.42919921875 
[2025-02-23 12:53:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.05438847467303276 norm:0.00019892632553819567 max memory_allocated 22565.42919921875 
[2025-02-23 12:53:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.05381801724433899 norm:0.00017654812836553901 max memory_allocated 22565.42919921875 
[2025-02-23 12:54:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.05351655185222626 norm:0.00017311204283032566 max memory_allocated 22565.42919921875 
[2025-02-23 12:54:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.053343433886766434 norm:0.00016650959150865674 max memory_allocated 22565.42919921875 
[2025-02-23 12:55:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.05319874733686447 norm:0.00015500860172323883 max memory_allocated 22565.42919921875 
[2025-02-23 12:55:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.05308432877063751 norm:0.00015493016690015793 max memory_allocated 22565.42919921875 
[2025-02-23 12:56:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.05298038199543953 norm:0.0001403600035700947 max memory_allocated 22565.42919921875 
[2025-02-23 12:56:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.05290396139025688 norm:0.0001328794314758852 max memory_allocated 22565.42919921875 
[2025-02-23 12:57:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.052819158881902695 norm:0.00011695063585648313 max memory_allocated 22565.42919921875 
[2025-02-23 12:58:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.052715882658958435 norm:0.00011559696577023715 max memory_allocated 22565.42919921875 
[2025-02-23 12:58:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.05265749245882034 norm:0.00011719853500835598 max memory_allocated 22565.42919921875 
[2025-02-23 12:59:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.05261776223778725 norm:0.00011361335782567039 max memory_allocated 22565.42919921875 
[2025-02-23 12:59:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.05257957801222801 norm:0.00011881629325216636 max memory_allocated 22565.42919921875 
[2025-02-23 13:00:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.05252561345696449 norm:0.00010442388884257525 max memory_allocated 22565.42919921875 
[2025-02-23 13:00:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.05248641595244408 norm:0.00010099152859766036 max memory_allocated 22565.42919921875 
[2025-02-23 13:01:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.052457544952631 norm:0.00010021551861427724 max memory_allocated 22565.42919921875 
[2025-02-23 13:01:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-23 13:02:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.07395528256893158 norm:0.0017000732477754354 max memory_allocated 22565.60107421875 
[2025-02-23 13:02:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.06712045520544052 norm:0.0005262535414658487 max memory_allocated 22565.60107421875 
[2025-02-23 13:03:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.062132734805345535 norm:0.00024978985311463475 max memory_allocated 22565.60107421875 
[2025-02-23 13:03:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.06079830974340439 norm:0.00019050744595006108 max memory_allocated 22565.60107421875 
[2025-02-23 13:04:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.059855908155441284 norm:0.00017757879686541855 max memory_allocated 22565.60107421875 
[2025-02-23 13:05:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.059326838701963425 norm:0.00016398944717366248 max memory_allocated 22565.60107421875 
[2025-02-23 13:05:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.05908976122736931 norm:0.0001556489587528631 max memory_allocated 22565.60107421875 
[2025-02-23 13:06:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.05891364440321922 norm:0.0001382888585794717 max memory_allocated 22565.60107421875 
[2025-02-23 13:06:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.05879371613264084 norm:0.00012083053297828883 max memory_allocated 22565.60107421875 
[2025-02-23 13:07:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.058695368468761444 norm:0.00011898827506229281 max memory_allocated 22565.60107421875 
[2025-02-23 13:07:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.058601584285497665 norm:0.00012271510786376894 max memory_allocated 22565.60107421875 
[2025-02-23 13:08:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.058513518422842026 norm:0.00011282382911304012 max memory_allocated 22565.60107421875 
[2025-02-23 13:08:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.05845508351922035 norm:0.00010097200720338151 max memory_allocated 22565.60107421875 
[2025-02-23 13:09:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.05839657783508301 norm:0.00010125904373126104 max memory_allocated 22565.60107421875 
[2025-02-23 13:10:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.05833882465958595 norm:9.89955515251495e-05 max memory_allocated 22565.60107421875 
[2025-02-23 13:10:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.058278586715459824 norm:9.554398275213316e-05 max memory_allocated 22565.60107421875 
[2025-02-23 13:11:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.058240603655576706 norm:9.088403021451086e-05 max memory_allocated 22565.60107421875 
[2025-02-23 13:11:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.058205801993608475 norm:8.735437586437911e-05 max memory_allocated 22565.60107421875 
[2025-02-23 13:12:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.058167438954114914 norm:8.377229096367955e-05 max memory_allocated 22565.60107421875 
[2025-02-23 13:12:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.05813704803586006 norm:7.963727694004774e-05 max memory_allocated 22565.60107421875 
[2025-02-23 13:13:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-23 13:13:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.08470005542039871 norm:0.0013951087603345513 max memory_allocated 22565.77294921875 
[2025-02-23 13:14:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.07684159278869629 norm:0.0004226389864925295 max memory_allocated 22565.77294921875 
[2025-02-23 13:14:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.07101303339004517 norm:0.00023912472533993423 max memory_allocated 22565.77294921875 
[2025-02-23 13:15:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.06921236962080002 norm:0.00020809259149245918 max memory_allocated 22565.77294921875 
[2025-02-23 13:15:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.0681615024805069 norm:0.0002077002718579024 max memory_allocated 22565.77294921875 
[2025-02-23 13:16:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.06760706007480621 norm:0.00017934136849362403 max memory_allocated 22565.77294921875 
[2025-02-23 13:17:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.06737755239009857 norm:0.00017865844711195678 max memory_allocated 22565.77294921875 
[2025-02-23 13:17:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.06723445653915405 norm:0.00015711854211986065 max memory_allocated 22565.77294921875 
[2025-02-23 13:18:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.06706119328737259 norm:0.00014683914196211845 max memory_allocated 22565.77294921875 
[2025-02-23 13:18:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.06697210669517517 norm:0.00015556298603769392 max memory_allocated 22565.77294921875 
[2025-02-23 13:19:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.06686298549175262 norm:0.00014615994587074965 max memory_allocated 22565.77294921875 
[2025-02-23 13:19:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.06677794456481934 norm:0.00013260566629469395 max memory_allocated 22565.77294921875 
[2025-02-23 13:20:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.06670697778463364 norm:0.0001261180150322616 max memory_allocated 22565.77294921875 
[2025-02-23 13:20:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.06663569062948227 norm:0.00012848743062932044 max memory_allocated 22565.77294921875 
[2025-02-23 13:21:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.06657936424016953 norm:0.00012277820496819913 max memory_allocated 22565.77294921875 
[2025-02-23 13:22:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.06651400029659271 norm:0.00011327351967338473 max memory_allocated 22565.77294921875 
[2025-02-23 13:22:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.06643103063106537 norm:0.00011531733616720885 max memory_allocated 22565.77294921875 
[2025-02-23 13:23:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.06640362739562988 norm:0.00012069433432770893 max memory_allocated 22565.77294921875 
[2025-02-23 13:23:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.06636149436235428 norm:0.0001102952373912558 max memory_allocated 22565.77294921875 
[2025-02-23 13:24:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.06630007922649384 norm:0.0001280582946492359 max memory_allocated 22565.77294921875 
[2025-02-23 13:24:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-23 13:25:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.09314300864934921 norm:0.0014761675847694278 max memory_allocated 22565.94482421875 
[2025-02-23 13:25:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.0858638733625412 norm:0.0005002430989407003 max memory_allocated 22565.94482421875 
[2025-02-23 13:26:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.07955797016620636 norm:0.0002070355840260163 max memory_allocated 22565.94482421875 
[2025-02-23 13:26:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.07792002707719803 norm:0.0001897170441225171 max memory_allocated 22565.94482421875 
[2025-02-23 13:27:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.07689113914966583 norm:0.0001770763919921592 max memory_allocated 22565.94482421875 
[2025-02-23 13:27:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.07642954587936401 norm:0.0001615444925846532 max memory_allocated 22565.94482421875 
[2025-02-23 13:28:30 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.0762149915099144 norm:0.00014333792205434293 max memory_allocated 22565.94482421875 
[2025-02-23 13:29:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.07605154812335968 norm:0.0001370734244119376 max memory_allocated 22565.94482421875 
[2025-02-23 13:29:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.07593302428722382 norm:0.00013611376925837249 max memory_allocated 22565.94482421875 
[2025-02-23 13:30:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.07582902163267136 norm:0.00011853425530716777 max memory_allocated 22565.94482421875 
[2025-02-23 13:30:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.0757438987493515 norm:9.807194874156266e-05 max memory_allocated 22565.94482421875 
[2025-02-23 13:31:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.07563342154026031 norm:0.00010409284732304513 max memory_allocated 22565.94482421875 
[2025-02-23 13:31:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.07555197924375534 norm:0.00011184396862518042 max memory_allocated 22565.94482421875 
[2025-02-23 13:32:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.07548365741968155 norm:0.00010146085696760565 max memory_allocated 22565.94482421875 
[2025-02-23 13:33:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.07543863356113434 norm:9.873865928966552e-05 max memory_allocated 22565.94482421875 
[2025-02-23 13:33:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.07539065927267075 norm:9.966154175344855e-05 max memory_allocated 22565.94482421875 
[2025-02-23 13:34:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.07534802705049515 norm:9.452042286284268e-05 max memory_allocated 22565.94482421875 
[2025-02-23 13:34:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.07531504333019257 norm:9.306373976869509e-05 max memory_allocated 22565.94482421875 
[2025-02-23 13:35:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.07528286427259445 norm:9.13350231712684e-05 max memory_allocated 22565.94482421875 
[2025-02-23 13:35:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.07524237036705017 norm:8.691033872310072e-05 max memory_allocated 22565.94482421875 
[2025-02-23 13:35:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-23 13:36:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.10568711161613464 norm:0.0021180417388677597 max memory_allocated 22566.11669921875 
[2025-02-23 13:37:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.09654685109853745 norm:0.0004976828931830823 max memory_allocated 22566.11669921875 
[2025-02-23 13:37:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.09026343375444412 norm:0.00036493627703748643 max memory_allocated 22566.11669921875 
[2025-02-23 13:38:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.08823730796575546 norm:0.0002346317342016846 max memory_allocated 22566.11669921875 
[2025-02-23 13:38:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.08707194775342941 norm:0.0002236171712866053 max memory_allocated 22566.11669921875 
[2025-02-23 13:39:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.08658593147993088 norm:0.00020506132568698376 max memory_allocated 22566.11669921875 
[2025-02-23 13:39:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.08638215810060501 norm:0.00019534016610123217 max memory_allocated 22566.11669921875 
[2025-02-23 13:40:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.08621646463871002 norm:0.0001640134578337893 max memory_allocated 22566.11669921875 
[2025-02-23 13:41:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.0860380306839943 norm:0.0001486199616920203 max memory_allocated 22566.11669921875 
[2025-02-23 13:41:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.08591168373823166 norm:0.00015802893904037774 max memory_allocated 22566.11669921875 
[2025-02-23 13:42:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.08580200374126434 norm:0.00014925553114153445 max memory_allocated 22566.11669921875 
[2025-02-23 13:42:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.08571483939886093 norm:0.0001502011000411585 max memory_allocated 22566.11669921875 
[2025-02-23 13:43:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.0856538787484169 norm:0.00015956186689436436 max memory_allocated 22566.11669921875 
[2025-02-23 13:43:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.08557131141424179 norm:0.00013847705849912018 max memory_allocated 22566.11669921875 
[2025-02-23 13:44:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.0855134129524231 norm:0.0001404542854288593 max memory_allocated 22566.11669921875 
[2025-02-23 13:45:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.0854581966996193 norm:0.00012823434371966869 max memory_allocated 22566.11669921875 
[2025-02-23 13:45:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.08541493117809296 norm:0.00012761169637087733 max memory_allocated 22566.11669921875 
[2025-02-23 13:46:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.08535969257354736 norm:0.00013122978270985186 max memory_allocated 22566.11669921875 
[2025-02-23 13:46:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.08531394600868225 norm:0.00012496740964706987 max memory_allocated 22566.11669921875 
[2025-02-23 13:47:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.08528855443000793 norm:0.00011917258962057531 max memory_allocated 22566.11669921875 
[2025-02-23 13:47:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-23 13:48:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.1195572167634964 norm:0.0011238857405260205 max memory_allocated 22566.28857421875 
[2025-02-23 13:48:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.11093107610940933 norm:0.0005372679443098605 max memory_allocated 22566.28857421875 
[2025-02-23 13:49:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.10332876443862915 norm:0.0003202092775609344 max memory_allocated 22566.28857421875 
[2025-02-23 13:49:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.10112984478473663 norm:0.00026586203603073955 max memory_allocated 22566.28857421875 
[2025-02-23 13:50:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.09997120499610901 norm:0.0002434970228932798 max memory_allocated 22566.28857421875 
[2025-02-23 13:50:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.09948793053627014 norm:0.00021787744481116533 max memory_allocated 22566.28857421875 
[2025-02-23 13:51:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.09922847151756287 norm:0.00020116116502322257 max memory_allocated 22566.28857421875 
[2025-02-23 13:51:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.0990254282951355 norm:0.00018724428082350641 max memory_allocated 22566.28857421875 
[2025-02-23 13:52:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.09886288642883301 norm:0.00017423488316126168 max memory_allocated 22566.28857421875 
[2025-02-23 13:53:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.09872424602508545 norm:0.0001706534530967474 max memory_allocated 22566.28857421875 
[2025-02-23 13:53:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.09866578876972198 norm:0.00018200103659182787 max memory_allocated 22566.28857421875 
[2025-02-23 13:54:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.09853002429008484 norm:0.00015192892169579864 max memory_allocated 22566.28857421875 
[2025-02-23 13:54:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.09839053452014923 norm:0.0001457703037885949 max memory_allocated 22566.28857421875 
[2025-02-23 13:55:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.09828631579875946 norm:0.00014369565178640187 max memory_allocated 22566.28857421875 
[2025-02-23 13:55:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.09823303669691086 norm:0.00013266093446873128 max memory_allocated 22566.28857421875 
[2025-02-23 13:56:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.09818091988563538 norm:0.00012568922829814255 max memory_allocated 22566.28857421875 
[2025-02-23 13:57:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.09808792918920517 norm:0.00012690381845459342 max memory_allocated 22566.28857421875 
[2025-02-23 13:57:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.09805598855018616 norm:0.0001266704493900761 max memory_allocated 22566.28857421875 
[2025-02-23 13:58:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.09799402207136154 norm:0.00011990981874987483 max memory_allocated 22566.28857421875 
[2025-02-23 13:58:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.09794124215841293 norm:0.0001205035368911922 max memory_allocated 22566.28857421875 
[2025-02-23 13:58:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-23 13:59:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.13419124484062195 norm:0.001808656146749854 max memory_allocated 22566.46044921875 
[2025-02-23 14:00:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.1251731663942337 norm:0.000720397976692766 max memory_allocated 22566.46044921875 
[2025-02-23 14:00:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.1173507571220398 norm:0.000272840668912977 max memory_allocated 22566.46044921875 
[2025-02-23 14:01:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.1149425283074379 norm:0.0002744595694821328 max memory_allocated 22566.46044921875 
[2025-02-23 14:01:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.11375195533037186 norm:0.00023377469915430993 max memory_allocated 22566.46044921875 
[2025-02-23 14:02:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.11332287639379501 norm:0.0002075307274935767 max memory_allocated 22566.46044921875 
[2025-02-23 14:02:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.11303246766328812 norm:0.000197480883798562 max memory_allocated 22566.46044921875 
[2025-02-23 14:03:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.11281868815422058 norm:0.0001893969310913235 max memory_allocated 22566.46044921875 
[2025-02-23 14:04:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.11263962090015411 norm:0.00017597517580725253 max memory_allocated 22566.46044921875 
[2025-02-23 14:04:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.11247918009757996 norm:0.00017018696235027164 max memory_allocated 22566.46044921875 
[2025-02-23 14:05:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.1123659610748291 norm:0.00015424078446812928 max memory_allocated 22566.46044921875 
[2025-02-23 14:05:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.11225200444459915 norm:0.00015406677266582847 max memory_allocated 22566.46044921875 
[2025-02-23 14:06:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.11211559921503067 norm:0.00013676474918611348 max memory_allocated 22566.46044921875 
[2025-02-23 14:06:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.11199725419282913 norm:0.00013841174950357527 max memory_allocated 22566.46044921875 
[2025-02-23 14:07:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.1119067445397377 norm:0.00013412149564828724 max memory_allocated 22566.46044921875 
[2025-02-23 14:07:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.11187107115983963 norm:0.000136737828142941 max memory_allocated 22566.46044921875 
[2025-02-23 14:08:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.111808180809021 norm:0.00013185229909140617 max memory_allocated 22566.46044921875 
[2025-02-23 14:09:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.1117643415927887 norm:0.00013533985475078225 max memory_allocated 22566.46044921875 
[2025-02-23 14:09:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.11173368990421295 norm:0.00011581762373680249 max memory_allocated 22566.46044921875 
[2025-02-23 14:10:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.11171155422925949 norm:0.00012316415086388588 max memory_allocated 22566.46044921875 
[2025-02-23 14:10:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-23 14:10:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.15125462412834167 norm:0.0020476863719522953 max memory_allocated 22566.63232421875 
[2025-02-23 14:11:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.14123442769050598 norm:0.0007081485236994922 max memory_allocated 22566.63232421875 
[2025-02-23 14:12:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.13310293853282928 norm:0.00028906072839163244 max memory_allocated 22566.63232421875 
[2025-02-23 14:12:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.13065791130065918 norm:0.00020734971622005105 max memory_allocated 22566.63232421875 
[2025-02-23 14:13:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.12965723872184753 norm:0.00020620881696231663 max memory_allocated 22566.63232421875 
[2025-02-23 14:13:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.1292785406112671 norm:0.0001911377185024321 max memory_allocated 22566.63232421875 
[2025-02-23 14:14:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.12898766994476318 norm:0.0001658162655076012 max memory_allocated 22566.63232421875 
[2025-02-23 14:14:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.12876786291599274 norm:0.0001663060102146119 max memory_allocated 22566.63232421875 
[2025-02-23 14:15:28 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.1285955011844635 norm:0.00015576029545627534 max memory_allocated 22566.63232421875 
[2025-02-23 14:16:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.1284601241350174 norm:0.0001444884983357042 max memory_allocated 22566.63232421875 
[2025-02-23 14:16:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.12833771109580994 norm:0.0001434547739336267 max memory_allocated 22566.63232421875 
[2025-02-23 14:17:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.12821584939956665 norm:0.000138249815790914 max memory_allocated 22566.63232421875 
[2025-02-23 14:17:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.12810733914375305 norm:0.0001330226514255628 max memory_allocated 22566.63232421875 
[2025-02-23 14:18:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.12802469730377197 norm:0.00012866062752436846 max memory_allocated 22566.63232421875 
[2025-02-23 14:18:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.1279517561197281 norm:0.00013249408220872283 max memory_allocated 22566.63232421875 
[2025-02-23 14:19:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.12789446115493774 norm:0.00012130614049965516 max memory_allocated 22566.63232421875 
[2025-02-23 14:19:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.1278190314769745 norm:0.00012048768257955089 max memory_allocated 22566.63232421875 
[2025-02-23 14:20:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.12777534127235413 norm:0.00011805087706306949 max memory_allocated 22566.63232421875 
[2025-02-23 14:21:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.12776589393615723 norm:0.00012031442020088434 max memory_allocated 22566.63232421875 
[2025-02-23 14:21:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.12773483991622925 norm:0.00011544681910891086 max memory_allocated 22566.63232421875 
[2025-02-23 14:21:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-23 14:21:52 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 14:22:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.17275820672512054 norm:0.011904304847121239 max memory_allocated 22566.91943359375 
[2025-02-23 14:22:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.1622646450996399 norm:0.008616847917437553 max memory_allocated 22566.91943359375 
[2025-02-23 14:23:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.15369608998298645 norm:0.005873400717973709 max memory_allocated 22566.91943359375 
[2025-02-23 14:24:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.15080004930496216 norm:0.004711411893367767 max memory_allocated 22566.91943359375 
[2025-02-23 14:24:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.14968520402908325 norm:0.004000519867986441 max memory_allocated 22566.91943359375 
[2025-02-23 14:25:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.14922954142093658 norm:0.0033940670546144247 max memory_allocated 22566.91943359375 
[2025-02-23 14:25:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.14884740114212036 norm:0.002878775354474783 max memory_allocated 22566.91943359375 
[2025-02-23 14:26:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.14856751263141632 norm:0.0025261633563786745 max memory_allocated 22566.91943359375 
[2025-02-23 14:26:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.14839765429496765 norm:0.0024062665179371834 max memory_allocated 22566.91943359375 
[2025-02-23 14:27:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.14829106628894806 norm:0.0023433100432157516 max memory_allocated 22566.91943359375 
[2025-02-23 14:28:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.1481325626373291 norm:0.0022953259758651257 max memory_allocated 22566.91943359375 
[2025-02-23 14:28:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.14802323281764984 norm:0.0020634473767131567 max memory_allocated 22566.91943359375 
[2025-02-23 14:29:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.1479334980249405 norm:0.0020487289875745773 max memory_allocated 22566.91943359375 
[2025-02-23 14:29:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.14782701432704926 norm:0.0019355417462065816 max memory_allocated 22566.91943359375 
[2025-02-23 14:30:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.14774742722511292 norm:0.0018621705239638686 max memory_allocated 22566.91943359375 
[2025-02-23 14:30:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.14774180948734283 norm:0.0019234023056924343 max memory_allocated 22566.91943359375 
[2025-02-23 14:31:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.14769501984119415 norm:0.0018928885692730546 max memory_allocated 22566.91943359375 
[2025-02-23 14:32:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.14760567247867584 norm:0.0017868800787255168 max memory_allocated 22566.91943359375 
[2025-02-23 14:32:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.14756202697753906 norm:0.0017696929862722754 max memory_allocated 22566.91943359375 
[2025-02-23 14:33:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.14753185212612152 norm:0.0016843521734699607 max memory_allocated 22566.91943359375 
[2025-02-23 14:33:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-23 14:33:22 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 14:33:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.20105423033237457 norm:0.01260684709995985 max memory_allocated 22567.09130859375 
[2025-02-23 14:34:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.18764732778072357 norm:0.009506301023066044 max memory_allocated 22567.09130859375 
[2025-02-23 14:35:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.17717774212360382 norm:0.006016246974468231 max memory_allocated 22567.09130859375 
[2025-02-23 14:35:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.17391979694366455 norm:0.005033582448959351 max memory_allocated 22567.09130859375 
[2025-02-23 14:36:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.1727631688117981 norm:0.004255380481481552 max memory_allocated 22567.09130859375 
[2025-02-23 14:36:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.17224755883216858 norm:0.003716836916282773 max memory_allocated 22567.09130859375 
[2025-02-23 14:37:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.17180462181568146 norm:0.0032077115029096603 max memory_allocated 22567.09130859375 
[2025-02-23 14:37:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.17150500416755676 norm:0.0027982050087302923 max memory_allocated 22567.09130859375 
[2025-02-23 14:38:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.17127709090709686 norm:0.002567667281255126 max memory_allocated 22567.09130859375 
[2025-02-23 14:39:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.1711292266845703 norm:0.0025588853750377893 max memory_allocated 22567.09130859375 
[2025-02-23 14:39:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.17105725407600403 norm:0.002481685485690832 max memory_allocated 22567.09130859375 
[2025-02-23 14:40:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.1709640622138977 norm:0.0026182434521615505 max memory_allocated 22567.09130859375 
[2025-02-23 14:40:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.17084936797618866 norm:0.002393379108980298 max memory_allocated 22567.09130859375 
[2025-02-23 14:41:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.17068102955818176 norm:0.002335060853511095 max memory_allocated 22567.09130859375 
[2025-02-23 14:41:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.1705760657787323 norm:0.0021682207006961107 max memory_allocated 22567.09130859375 
[2025-02-23 14:42:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.17046120762825012 norm:0.002119706943631172 max memory_allocated 22567.09130859375 
[2025-02-23 14:42:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.17040275037288666 norm:0.0020693817641586065 max memory_allocated 22567.09130859375 
[2025-02-23 14:43:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.17028482258319855 norm:0.0019272766076028347 max memory_allocated 22567.09130859375 
[2025-02-23 14:44:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.1703055202960968 norm:0.0020043423864990473 max memory_allocated 22567.09130859375 
[2025-02-23 14:44:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.17023102939128876 norm:0.001951297977939248 max memory_allocated 22567.09130859375 
[2025-02-23 14:44:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-23 14:44:52 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 14:45:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.29632997512817383 norm:0.03322373330593109 max memory_allocated 22567.26318359375 
[2025-02-23 14:45:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.26605224609375 norm:0.02472418174147606 max memory_allocated 22567.26318359375 
[2025-02-23 14:46:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.24379046261310577 norm:0.015799054875969887 max memory_allocated 22567.26318359375 
[2025-02-23 14:47:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.23798087239265442 norm:0.01377551257610321 max memory_allocated 22567.26318359375 
[2025-02-23 14:47:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.2360992133617401 norm:0.012448997236788273 max memory_allocated 22567.26318359375 
[2025-02-23 14:48:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.234402596950531 norm:0.011825880035758018 max memory_allocated 22567.26318359375 
[2025-02-23 14:48:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.2331942915916443 norm:0.010839933529496193 max memory_allocated 22567.26318359375 
[2025-02-23 14:49:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.23243670165538788 norm:0.010311676189303398 max memory_allocated 22567.26318359375 
[2025-02-23 14:49:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.231721892952919 norm:0.009892220608890057 max memory_allocated 22567.26318359375 
[2025-02-23 14:50:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.23154740035533905 norm:0.009889944456517696 max memory_allocated 22567.26318359375 
[2025-02-23 14:51:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.2316378802061081 norm:0.009544959291815758 max memory_allocated 22567.26318359375 
[2025-02-23 14:51:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.23109707236289978 norm:0.00933963805437088 max memory_allocated 22567.26318359375 
[2025-02-23 14:52:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.2305290848016739 norm:0.00933478306978941 max memory_allocated 22567.26318359375 
[2025-02-23 14:52:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.23058082163333893 norm:0.009326749481260777 max memory_allocated 22567.26318359375 
[2025-02-23 14:53:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.23038363456726074 norm:0.009375466965138912 max memory_allocated 22567.26318359375 
[2025-02-23 14:53:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.23022902011871338 norm:0.009168659336864948 max memory_allocated 22567.26318359375 
[2025-02-23 14:54:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.22997605800628662 norm:0.009191355668008327 max memory_allocated 22567.26318359375 
[2025-02-23 14:55:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.23022794723510742 norm:0.008759214542806149 max memory_allocated 22567.26318359375 
[2025-02-23 14:55:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.23033717274665833 norm:0.009116926230490208 max memory_allocated 22567.26318359375 
[2025-02-23 14:56:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.22994466125965118 norm:0.008972235955297947 max memory_allocated 22567.26318359375 
[2025-02-23 14:56:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-23 14:56:21 root] (abq_llm_calibration.py 276): INFO use compensation vector
[2025-02-23 14:56:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.49591296911239624 norm:0.06284965574741364 max memory_allocated 22567.43505859375 
[2025-02-23 14:57:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.44113603234291077 norm:0.045439742505550385 max memory_allocated 22567.43505859375 
[2025-02-23 14:58:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.3994932770729065 norm:0.029108351096510887 max memory_allocated 22567.43505859375 
[2025-02-23 14:58:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.38892558217048645 norm:0.025154827162623405 max memory_allocated 22567.43505859375 
[2025-02-23 14:59:10 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.3841697573661804 norm:0.023021141067147255 max memory_allocated 22567.43505859375 
[2025-02-23 14:59:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.3810559809207916 norm:0.020343435928225517 max memory_allocated 22567.43505859375 
[2025-02-23 15:00:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.3784993886947632 norm:0.018388554453849792 max memory_allocated 22567.43505859375 
[2025-02-23 15:00:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.3766742944717407 norm:0.017907723784446716 max memory_allocated 22567.43505859375 
[2025-02-23 15:01:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.37574630975723267 norm:0.016916800290346146 max memory_allocated 22567.43505859375 
[2025-02-23 15:01:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.37518826127052307 norm:0.016668586060404778 max memory_allocated 22567.43505859375 
[2025-02-23 15:02:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.3744044303894043 norm:0.01604374498128891 max memory_allocated 22567.43505859375 
[2025-02-23 15:03:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.37358659505844116 norm:0.015189201571047306 max memory_allocated 22567.43505859375 
[2025-02-23 15:03:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.3726712465286255 norm:0.014852629043161869 max memory_allocated 22567.43505859375 
[2025-02-23 15:04:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.371977299451828 norm:0.014536291360855103 max memory_allocated 22567.43505859375 
[2025-02-23 15:04:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.3716198801994324 norm:0.0136532261967659 max memory_allocated 22567.43505859375 
[2025-02-23 15:05:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.37103188037872314 norm:0.013392431661486626 max memory_allocated 22567.43505859375 
[2025-02-23 15:05:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.3715002238750458 norm:0.013631800189614296 max memory_allocated 22567.43505859375 
[2025-02-23 15:06:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.37176525592803955 norm:0.01420710887759924 max memory_allocated 22567.43505859375 
[2025-02-23 15:07:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.3713725209236145 norm:0.013690158724784851 max memory_allocated 22567.43505859375 
[2025-02-23 15:07:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.3700129985809326 norm:0.012471561320126057 max memory_allocated 22567.43505859375 
[2025-02-23 15:07:48 root] (main_calibration.py 365): INFO 22015.146661758423
[2025-02-23 15:08:19 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-23 15:09:31 root] (main_calibration.py 158): INFO wikitext2 : 5.671237468719482
[2025-02-23 15:09:31 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-23 15:11:22 root] (main_calibration.py 158): INFO c4 : 7.238714218139648
