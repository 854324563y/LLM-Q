[2025-02-17 14:00:16 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration/llama-13b-hf-w8a8', save_dir='./log-calibration/quant/llama-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-17 14:07:02 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-17 14:07:02 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-17 14:07:03 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-17 14:07:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-17 14:07:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0013696836540475488 norm:0.00034143426455557346 max memory_allocated 29226.177734375 
[2025-02-17 14:08:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0009430283680558205 norm:0.00011158392590004951 max memory_allocated 29226.177734375 
[2025-02-17 14:09:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0007876841118559241 norm:6.583247159142047e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:10:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0007087045814841986 norm:4.681158679886721e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:10:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0006647167028859258 norm:3.6857651139143854e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:11:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.00063851207960397 norm:3.1342202419182286e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:12:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0006194536690600216 norm:2.9437644116114825e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:13:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0006054296973161399 norm:3.069780723308213e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:13:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0005970891797915101 norm:2.4824974389048293e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:14:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0005892181070521474 norm:2.3955401047714986e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:15:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.000585447414778173 norm:2.3570180928800255e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:16:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0005821090890094638 norm:2.359387872274965e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:16:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0005790609284304082 norm:2.319867053302005e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:17:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0005767342518083751 norm:2.301770291524008e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:18:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0005750952404923737 norm:2.3097205485100858e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:19:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0005745035596191883 norm:2.3176147806225345e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:19:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0005718626780435443 norm:2.3175613023340702e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:20:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0005704183713532984 norm:2.305129419255536e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:21:25 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0005675306310877204 norm:2.2939388145459816e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:22:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0005669294623658061 norm:2.308620241819881e-05 max memory_allocated 29226.177734375 
[2025-02-17 14:22:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-17 14:23:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0012949316296726465 norm:0.00020150409545749426 max memory_allocated 29226.365234375 
[2025-02-17 14:24:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0010349886724725366 norm:6.454061804106459e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:24:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0009411296341568232 norm:3.788020330830477e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:25:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.000894608034286648 norm:2.674805000424385e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:26:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0008674963028170168 norm:2.1058105630800128e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:27:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0008501596748828888 norm:1.7707361621432938e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:27:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0008369075949303806 norm:1.5590341718052514e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:28:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0008292697020806372 norm:1.4330309568322264e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:29:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0008220925228670239 norm:1.3554023098549806e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:30:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0008162777521647513 norm:1.2954707926837727e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:30:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0008181515149772167 norm:1.2668514500546735e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:31:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0008205615449696779 norm:1.2532314940472133e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:32:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0008157943375408649 norm:1.2380392945487984e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:33:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0008124765590764582 norm:1.2248063285369426e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:33:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0008112179348245263 norm:1.2199396223877557e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:34:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0008092622156254947 norm:1.2161019185441546e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:35:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0008101263083517551 norm:1.2211272405693308e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:36:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.000810188998002559 norm:1.2210621207486838e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:36:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0008093024371191859 norm:1.221650654770201e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:37:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0008090414921753109 norm:1.2255959518370219e-05 max memory_allocated 29226.365234375 
[2025-02-17 14:37:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-17 14:38:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0015321505488827825 norm:0.00015015588724054396 max memory_allocated 29226.552734375 
[2025-02-17 14:39:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0012923548929393291 norm:6.984020001254976e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:40:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0012011346407234669 norm:5.1877479563700035e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:40:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0011424871627241373 norm:4.061816071043722e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:41:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.00110733846668154 norm:3.606931568356231e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:42:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0010891658021137118 norm:3.44765285262838e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:43:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0010714026866480708 norm:3.2405983802163973e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:43:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0010582837276160717 norm:3.1871782994130626e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:44:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0010599145898595452 norm:3.2818628824315965e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:45:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.001049706945195794 norm:3.129346441710368e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:46:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0010400288738310337 norm:3.2368316169595346e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:46:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0010327110067009926 norm:3.0074712412897497e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:47:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0010291426442563534 norm:3.0166429496603087e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:48:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.001032110070809722 norm:3.095240026596002e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:49:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0010296270484104753 norm:3.115305662504397e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:49:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0010315962135791779 norm:3.2665480830473825e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:50:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.001029872801154852 norm:3.1885305361356586e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:51:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0010285800090059638 norm:3.3407690352760255e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:52:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0010188368614763021 norm:3.078052759519778e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:52:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0010260418057441711 norm:3.300318348919973e-05 max memory_allocated 29226.552734375 
[2025-02-17 14:53:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-17 14:53:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0020656937267631292 norm:0.0006142990896478295 max memory_allocated 29226.740234375 
[2025-02-17 14:54:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0015653884038329124 norm:0.00024147865769919008 max memory_allocated 29226.740234375 
[2025-02-17 14:55:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0014133884105831385 norm:0.00014335841115098447 max memory_allocated 29226.740234375 
[2025-02-17 14:56:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0013441669289022684 norm:9.508644870948046e-05 max memory_allocated 29226.740234375 
[2025-02-17 14:56:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.001310193445533514 norm:7.034176087472588e-05 max memory_allocated 29226.740234375 
[2025-02-17 14:57:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0012903236784040928 norm:5.505895387614146e-05 max memory_allocated 29226.740234375 
[2025-02-17 14:58:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0012749895686283708 norm:4.412646376295015e-05 max memory_allocated 29226.740234375 
[2025-02-17 14:59:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0012667719274759293 norm:3.652254235930741e-05 max memory_allocated 29226.740234375 
[2025-02-17 14:59:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0012587318196892738 norm:3.1108684197533876e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:00:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0012502686586230993 norm:2.598647552076727e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:01:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0012470014626160264 norm:2.1574647689703852e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:02:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0012436185497790575 norm:1.8397160602035e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:02:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0012389071052893996 norm:1.5884654203546233e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:03:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0012359145330265164 norm:1.3748441233474296e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:04:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0012346830917522311 norm:1.2484693797887303e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:05:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0012347579468041658 norm:1.1362296390871052e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:05:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0012334334896877408 norm:1.0860712791327387e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:06:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0012323857517912984 norm:1.0590219062578399e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:07:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0012305853888392448 norm:1.0446608939673752e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:08:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0012328819138929248 norm:1.0068499250337481e-05 max memory_allocated 29226.740234375 
[2025-02-17 15:08:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-17 15:09:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0038242789451032877 norm:0.0027999908197671175 max memory_allocated 29226.927734375 
[2025-02-17 15:09:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.00208462355658412 norm:0.0006316839717328548 max memory_allocated 29226.927734375 
[2025-02-17 15:10:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0017930702306330204 norm:0.0003638708731159568 max memory_allocated 29226.927734375 
[2025-02-17 15:11:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.001701071858406067 norm:0.00027935593971051276 max memory_allocated 29226.927734375 
[2025-02-17 15:12:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0016086735995486379 norm:0.00019266204617451876 max memory_allocated 29226.927734375 
[2025-02-17 15:12:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0015564074274152517 norm:0.00014639331493526697 max memory_allocated 29226.927734375 
[2025-02-17 15:13:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.001525085885077715 norm:0.00011465685383882374 max memory_allocated 29226.927734375 
[2025-02-17 15:14:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.001509491354227066 norm:9.831071656662971e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:15:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0014966664602980018 norm:8.582553709857166e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:15:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0014830303844064474 norm:7.535612530773506e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:16:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0014732885174453259 norm:6.68484135530889e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:17:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0014679243322461843 norm:6.0228281654417515e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:18:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0014632400125265121 norm:5.3984396799933165e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:18:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0014605887699872255 norm:4.792778781848028e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:19:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0014582648873329163 norm:4.349870505393483e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:20:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0014537799870595336 norm:3.879803989548236e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:21:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.001450785668566823 norm:3.43007777701132e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:21:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0014466828433796763 norm:2.9364256988628767e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:22:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0014429416041821241 norm:2.565247268648818e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:23:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0014413832686841488 norm:2.231874896096997e-05 max memory_allocated 29226.927734375 
[2025-02-17 15:23:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-17 15:24:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0030982522293925285 norm:0.00163803412579 max memory_allocated 29227.115234375 
[2025-02-17 15:25:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0019943828228861094 norm:0.0003661970840767026 max memory_allocated 29227.115234375 
[2025-02-17 15:25:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0018527591601014137 norm:0.00026240816805511713 max memory_allocated 29227.115234375 
[2025-02-17 15:26:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.00176332239061594 norm:0.00018793308117892593 max memory_allocated 29227.115234375 
[2025-02-17 15:27:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0016980450600385666 norm:0.00013464019866660237 max memory_allocated 29227.115234375 
[2025-02-17 15:28:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0016616616630926728 norm:0.00010562119132373482 max memory_allocated 29227.115234375 
[2025-02-17 15:28:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.001638331450521946 norm:8.72289965627715e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:29:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.001621273229829967 norm:7.50531762605533e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:30:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0016083813970908523 norm:6.536857108585536e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:31:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0015978700248524547 norm:5.750363925471902e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:31:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0015896293334662914 norm:5.1062015700154006e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:32:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0015848011244088411 norm:4.6451095840893686e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:33:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0015797519590705633 norm:4.2230771214235574e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:34:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0015738166403025389 norm:3.7095334846526384e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:34:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.001569948042742908 norm:3.326435762573965e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:35:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0015658248448744416 norm:3.056181230931543e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:36:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.001562818419188261 norm:2.7805181161966175e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:37:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.001560803852044046 norm:2.411270725133363e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:37:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.001558623043820262 norm:2.080439662677236e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:38:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0015563410706818104 norm:1.812454138416797e-05 max memory_allocated 29227.115234375 
[2025-02-17 15:38:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-17 15:39:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0026353432331234217 norm:0.00034920440521091223 max memory_allocated 29227.302734375 
[2025-02-17 15:40:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0022085723467171192 norm:0.00011851283488795161 max memory_allocated 29227.302734375 
[2025-02-17 15:41:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.002124497899785638 norm:8.485243597533554e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:41:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0020657654386013746 norm:5.680972753907554e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:42:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.002024289220571518 norm:4.3360945710446686e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:43:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0020037032663822174 norm:3.565070073818788e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:44:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.001986600225791335 norm:3.105362338828854e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:44:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0019727086182683706 norm:2.8008464141748846e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:45:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0019666666630655527 norm:2.4780329113127664e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:46:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.001954368781298399 norm:2.2981710571912117e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:47:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0019475063309073448 norm:2.0924788259435445e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:47:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0019483278738334775 norm:1.923914169310592e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:48:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.0019478171598166227 norm:1.8897624613600783e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:49:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.001948971301317215 norm:1.7750511688063852e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:50:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0019382431637495756 norm:1.6818441508803517e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:50:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0019391474779695272 norm:1.6960226275841706e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:51:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0019380120793357491 norm:1.5782678019604646e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:52:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0019380649318918586 norm:1.525137122371234e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:53:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0019419523887336254 norm:1.4669360098196194e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:53:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0019383926410228014 norm:1.4930242286936846e-05 max memory_allocated 29227.302734375 
[2025-02-17 15:54:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-17 15:54:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0031544941011816263 norm:0.00044521497329697013 max memory_allocated 29227.490234375 
[2025-02-17 15:55:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0022836122661828995 norm:9.997173037845641e-05 max memory_allocated 29227.490234375 
[2025-02-17 15:56:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.002117870608344674 norm:6.37135817669332e-05 max memory_allocated 29227.490234375 
[2025-02-17 15:57:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.00207641557790339 norm:5.8980822359444574e-05 max memory_allocated 29227.490234375 
[2025-02-17 15:57:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0020285320933908224 norm:4.096324846614152e-05 max memory_allocated 29227.490234375 
[2025-02-17 15:58:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.0020042695105075836 norm:3.0308880013762973e-05 max memory_allocated 29227.490234375 
[2025-02-17 15:59:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.001990910852327943 norm:2.378356111876201e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:00:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.001981760375201702 norm:2.0619656424969435e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:00:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.0019746606703847647 norm:1.784415144356899e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:01:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.001970150275155902 norm:1.5416233509313315e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:02:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.001966154668480158 norm:1.4244813428376801e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:03:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0019645877182483673 norm:1.3234819562057965e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:03:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.001961459405720234 norm:1.2231913387950044e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:04:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0019585653208196163 norm:1.1113600521639455e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:05:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0019574316684156656 norm:1.0277069122821558e-05 max memory_allocated 29227.490234375 
[2025-02-17 16:06:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0019559503998607397 norm:9.885787221719511e-06 max memory_allocated 29227.490234375 
[2025-02-17 16:06:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.001954874722287059 norm:9.205857168126386e-06 max memory_allocated 29227.490234375 
[2025-02-17 16:07:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0019528336124494672 norm:8.967523172032088e-06 max memory_allocated 29227.490234375 
[2025-02-17 16:08:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0019528153352439404 norm:8.537616849935148e-06 max memory_allocated 29227.490234375 
[2025-02-17 16:09:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0019517175387591124 norm:8.184725629689638e-06 max memory_allocated 29227.490234375 
[2025-02-17 16:09:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-17 16:10:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.002771571045741439 norm:0.00020136084640398622 max memory_allocated 29227.677734375 
[2025-02-17 16:11:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0023022980894893408 norm:6.048698560334742e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:11:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.00221073254942894 norm:4.3802701839013025e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:12:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0021649699192494154 norm:3.314476634841412e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:13:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.00213813129812479 norm:2.3652217350900173e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:14:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0021241011563688517 norm:1.9670495021273382e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:14:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.00211317278444767 norm:1.639599577174522e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:15:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.002106259809806943 norm:1.3916655007051304e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:16:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0020999093540012836 norm:1.2525914826255757e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:17:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.002096825512126088 norm:1.1114194421679713e-05 max memory_allocated 29227.677734375 
[2025-02-17 16:17:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0020931698381900787 norm:9.984677490137983e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:18:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0020914478227496147 norm:9.11004826775752e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:19:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.002089998684823513 norm:8.484480531478766e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:20:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0020893956534564495 norm:8.126648026518524e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:20:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.002089513698592782 norm:7.442076821462251e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:21:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0020894063636660576 norm:7.17395141691668e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:22:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.002088555134832859 norm:7.078820544847986e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:23:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0020866827107965946 norm:6.919568022567546e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:23:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0020860345102846622 norm:6.841015419922769e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:24:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.002085249638184905 norm:6.697080152662238e-06 max memory_allocated 29227.677734375 
[2025-02-17 16:24:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-17 16:25:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002831272082403302 norm:0.0001702313602436334 max memory_allocated 29227.865234375 
[2025-02-17 16:26:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0024091461673378944 norm:4.729200009023771e-05 max memory_allocated 29227.865234375 
[2025-02-17 16:27:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.00231955386698246 norm:3.231321170460433e-05 max memory_allocated 29227.865234375 
[2025-02-17 16:27:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0022866418585181236 norm:2.903792483266443e-05 max memory_allocated 29227.865234375 
[2025-02-17 16:28:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0022531449794769287 norm:1.8708677089307457e-05 max memory_allocated 29227.865234375 
[2025-02-17 16:29:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0022430073004215956 norm:1.3738125744566787e-05 max memory_allocated 29227.865234375 
[2025-02-17 16:30:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.002238686429336667 norm:1.1419556358305272e-05 max memory_allocated 29227.865234375 
[2025-02-17 16:30:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0022354458924382925 norm:1.028224596666405e-05 max memory_allocated 29227.865234375 
[2025-02-17 16:31:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.002230808138847351 norm:8.945045919972472e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:32:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0022284334991127253 norm:8.052938937908038e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:33:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0022273524664342403 norm:7.429730430885684e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:33:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.002225827192887664 norm:7.2468683356419206e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:34:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0022255287040024996 norm:6.933656095498009e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:35:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.002223882358521223 norm:6.6099510149797425e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:36:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0022230297327041626 norm:6.216554083948722e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:36:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0022217915393412113 norm:5.9123444771103095e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:37:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0022202106192708015 norm:5.838681772729615e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:38:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0022192085161805153 norm:5.6991575547726825e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:39:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0022184052504599094 norm:5.664240234182216e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:39:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0022176713682711124 norm:5.628967301163357e-06 max memory_allocated 29227.865234375 
[2025-02-17 16:40:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-17 16:40:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0035609095357358456 norm:0.00039540472789667547 max memory_allocated 29228.052734375 
[2025-02-17 16:41:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0027111603412777185 norm:9.945163037627935e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:42:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.002529972931370139 norm:5.194506229599938e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:43:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.002483983291313052 norm:4.4371223339112476e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:43:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0024540650192648172 norm:3.592681969166733e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:44:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.002425492275506258 norm:2.643883999553509e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:45:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0024099666625261307 norm:2.0956767912139185e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:46:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.002403195248916745 norm:1.763245563779492e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:46:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0023985288571566343 norm:1.5214281120279338e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:47:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0023945020511746407 norm:1.3492183825292159e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:48:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.002391589805483818 norm:1.2196809620945714e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:49:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.002388130407780409 norm:1.10109231172828e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:49:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0023871769662946463 norm:1.006253205559915e-05 max memory_allocated 29228.052734375 
[2025-02-17 16:50:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.002385703381150961 norm:9.166806194116361e-06 max memory_allocated 29228.052734375 
[2025-02-17 16:51:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.002384273102506995 norm:8.72297187015647e-06 max memory_allocated 29228.052734375 
[2025-02-17 16:52:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.00238370755687356 norm:8.434426490566693e-06 max memory_allocated 29228.052734375 
[2025-02-17 16:52:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0023819534108042717 norm:8.012117177713662e-06 max memory_allocated 29228.052734375 
[2025-02-17 16:53:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0023802570067346096 norm:7.350285613938468e-06 max memory_allocated 29228.052734375 
[2025-02-17 16:54:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.002379366662353277 norm:7.009351520537166e-06 max memory_allocated 29228.052734375 
[2025-02-17 16:55:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0023793468717485666 norm:6.6743095885613e-06 max memory_allocated 29228.052734375 
[2025-02-17 16:55:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-17 16:56:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0031990911811590195 norm:0.00015098614676389843 max memory_allocated 29228.240234375 
[2025-02-17 16:56:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.002782912226393819 norm:4.958333738613874e-05 max memory_allocated 29228.240234375 
[2025-02-17 16:57:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0026748492382466793 norm:2.984890124935191e-05 max memory_allocated 29228.240234375 
[2025-02-17 16:58:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0026420638896524906 norm:2.4942983145592734e-05 max memory_allocated 29228.240234375 
[2025-02-17 16:59:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0026138820685446262 norm:1.834385511756409e-05 max memory_allocated 29228.240234375 
[2025-02-17 16:59:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002599047962576151 norm:1.385595078318147e-05 max memory_allocated 29228.240234375 
[2025-02-17 17:00:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0025899438187479973 norm:1.1237745638936758e-05 max memory_allocated 29228.240234375 
[2025-02-17 17:01:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0025844143237918615 norm:9.780443178897258e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:02:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.002580119064077735 norm:8.681059625814669e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:02:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0025758566334843636 norm:7.674781045352574e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:03:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0025741704739630222 norm:7.131960501283174e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:04:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.002573953475803137 norm:6.343657787510892e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:05:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0025736200623214245 norm:5.932543899689335e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:05:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0025711050257086754 norm:5.702247108274605e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:06:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.002571644727140665 norm:5.410927315097069e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:07:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0025707969907671213 norm:5.28246982867131e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:08:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0025699324905872345 norm:5.2315008360892534e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:08:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.002569513162598014 norm:5.039788447902538e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:09:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0025681823026388884 norm:4.979770892532542e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:10:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.002567294519394636 norm:4.905142304778565e-06 max memory_allocated 29228.240234375 
[2025-02-17 17:10:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-17 17:11:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0033158890437334776 norm:0.00015583931235596538 max memory_allocated 29228.427734375 
[2025-02-17 17:12:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.002933943411335349 norm:5.103780495119281e-05 max memory_allocated 29228.427734375 
[2025-02-17 17:12:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0028423359617590904 norm:3.0578492442145944e-05 max memory_allocated 29228.427734375 
[2025-02-17 17:13:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.002807898446917534 norm:2.5545507014612667e-05 max memory_allocated 29228.427734375 
[2025-02-17 17:14:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.002781794872134924 norm:1.9186141798854806e-05 max memory_allocated 29228.427734375 
[2025-02-17 17:15:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.0027673563454300165 norm:1.465457353333477e-05 max memory_allocated 29228.427734375 
[2025-02-17 17:15:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0027607742231339216 norm:1.1920581528102048e-05 max memory_allocated 29228.427734375 
[2025-02-17 17:16:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.002756188390776515 norm:1.0444811778143048e-05 max memory_allocated 29228.427734375 
[2025-02-17 17:17:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.002753450535237789 norm:9.447526281292085e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:18:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.002749915700405836 norm:8.882852853275836e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:18:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.002747884253039956 norm:8.129151865432505e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:19:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0027466025203466415 norm:7.651171472389251e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:20:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.002744043245911598 norm:7.379863291134825e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:21:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.002744309836998582 norm:7.060843472572742e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:21:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002743417862802744 norm:7.043935511319432e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:22:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0027423021383583546 norm:6.799722086725524e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:23:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.002741588046774268 norm:6.662185114691965e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:24:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0027414162177592516 norm:6.546034910570597e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:24:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.002740747295320034 norm:6.494048648164608e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:25:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.002741097239777446 norm:6.405656222341349e-06 max memory_allocated 29228.427734375 
[2025-02-17 17:25:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-17 17:26:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.00343377236276865 norm:0.00010066798859043047 max memory_allocated 29228.615234375 
[2025-02-17 17:27:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.003161950269713998 norm:3.5159391700290143e-05 max memory_allocated 29228.615234375 
[2025-02-17 17:28:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.003099876455962658 norm:2.5728069886099547e-05 max memory_allocated 29228.615234375 
[2025-02-17 17:28:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0030679276678711176 norm:2.0723433408420533e-05 max memory_allocated 29228.615234375 
[2025-02-17 17:29:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.003046344965696335 norm:1.607847116247285e-05 max memory_allocated 29228.615234375 
[2025-02-17 17:30:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0030334966722875834 norm:1.3175377716834191e-05 max memory_allocated 29228.615234375 
[2025-02-17 17:31:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.0030256733298301697 norm:1.1403872122173198e-05 max memory_allocated 29228.615234375 
[2025-02-17 17:31:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0030196821317076683 norm:1.0346515409764834e-05 max memory_allocated 29228.615234375 
[2025-02-17 17:32:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.003015943570062518 norm:9.698926987766754e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:33:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.0030119491275399923 norm:9.011142537929118e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:34:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0030094904359430075 norm:8.436526513833087e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:34:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0030076061375439167 norm:8.134584277286194e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:35:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0030059919226914644 norm:7.961857590998989e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:36:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.003004638943821192 norm:7.75341504777316e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:37:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0030043430160731077 norm:7.613186426169705e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:37:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.003003736026585102 norm:7.54753682485898e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:38:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0030032387003302574 norm:7.456025286955992e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:39:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0030033409129828215 norm:7.451550118275918e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:40:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0030029958579689264 norm:7.414032097585732e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:40:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0030028210021555424 norm:7.381599061773159e-06 max memory_allocated 29228.615234375 
[2025-02-17 17:41:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-17 17:41:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.004137812182307243 norm:0.0002243325434392318 max memory_allocated 29228.802734375 
[2025-02-17 17:42:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0035669244825839996 norm:7.813964475644752e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:43:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0034071204718202353 norm:4.5603148464579135e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:44:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.003354316111654043 norm:3.614267552620731e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:44:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.003320510732010007 norm:2.982359910674859e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:45:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.0032903810497373343 norm:2.3444212274625897e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:46:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.003271870780736208 norm:1.906870056700427e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:47:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0032611670903861523 norm:1.6178953956114128e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:47:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.003253770060837269 norm:1.4517108866130002e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:48:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0032475064508616924 norm:1.301212341786595e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:49:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0032428919803351164 norm:1.1853628166136332e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:50:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0032380789052695036 norm:1.1106866622867528e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:50:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.00323581718839705 norm:1.0014353392762132e-05 max memory_allocated 29228.802734375 
[2025-02-17 17:51:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0032332190312445164 norm:9.34042691369541e-06 max memory_allocated 29228.802734375 
[2025-02-17 17:52:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0032311659306287766 norm:8.996757969725877e-06 max memory_allocated 29228.802734375 
[2025-02-17 17:53:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.0032294036354869604 norm:8.623344911029562e-06 max memory_allocated 29228.802734375 
[2025-02-17 17:53:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.003227220382541418 norm:8.347225957550108e-06 max memory_allocated 29228.802734375 
[2025-02-17 17:54:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.003226931905373931 norm:7.933001143101137e-06 max memory_allocated 29228.802734375 
[2025-02-17 17:55:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0032273402903229 norm:7.678883775952272e-06 max memory_allocated 29228.802734375 
[2025-02-17 17:56:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.0032275766134262085 norm:7.48937918615411e-06 max memory_allocated 29228.802734375 
[2025-02-17 17:56:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-17 17:57:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.005048453342169523 norm:0.0005366138066165149 max memory_allocated 29228.990234375 
[2025-02-17 17:57:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.004050071816891432 norm:0.00014053632912691683 max memory_allocated 29228.990234375 
[2025-02-17 17:58:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0038305390626192093 norm:7.818449375918135e-05 max memory_allocated 29228.990234375 
[2025-02-17 17:59:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0037637418136000633 norm:6.521635077660903e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:00:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0037322749849408865 norm:5.880362732568756e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:01:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0036886318121105433 norm:4.3729738536057994e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:01:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.003664040472358465 norm:3.523475243127905e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:02:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.003648243611678481 norm:3.0473580409307033e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:03:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0036365867126733065 norm:2.666146792762447e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:04:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.003626949153840542 norm:2.3787790269125253e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:04:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0036180648021399975 norm:2.1944706531940028e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:05:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0036115827970206738 norm:1.956002233782783e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:06:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0036076558753848076 norm:1.804766543500591e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:07:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0036052013747394085 norm:1.7484826457803138e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:07:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.003601035103201866 norm:1.5715117115178145e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:08:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.003598572686314583 norm:1.4363014088303316e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:09:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.0035960867535322905 norm:1.3511582437786274e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:10:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.003594190813601017 norm:1.2767783118761145e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:10:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0035929898731410503 norm:1.2239090210641734e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:11:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.003591591026633978 norm:1.1686419384204783e-05 max memory_allocated 29228.990234375 
[2025-02-17 18:11:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-17 18:12:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.004800908733159304 norm:0.0002608197391964495 max memory_allocated 29229.177734375 
[2025-02-17 18:13:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.004219126887619495 norm:7.609309250256047e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:14:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.004086334723979235 norm:4.5862008846597746e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:14:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.004046346060931683 norm:4.01256256736815e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:15:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.004012316465377808 norm:3.24129214277491e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:16:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.003984851762652397 norm:2.565639078966342e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:17:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.003964850213378668 norm:2.069359652523417e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:17:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.003953120205551386 norm:1.7835083781392314e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:18:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.003943887073546648 norm:1.597629852767568e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:19:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.003938676789402962 norm:1.4323355571832508e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:20:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.003932454623281956 norm:1.3129060789651703e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:20:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.003928682766854763 norm:1.207443347084336e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:21:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.00392550602555275 norm:1.1119788723590318e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:22:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.003923462238162756 norm:1.0415542419650592e-05 max memory_allocated 29229.177734375 
[2025-02-17 18:23:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.003921921364963055 norm:9.699891961645335e-06 max memory_allocated 29229.177734375 
[2025-02-17 18:23:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.0039199781604111195 norm:9.188855983666144e-06 max memory_allocated 29229.177734375 
[2025-02-17 18:24:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.0039180065505206585 norm:8.844054718792904e-06 max memory_allocated 29229.177734375 
[2025-02-17 18:25:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.003916603513062 norm:8.412809620494954e-06 max memory_allocated 29229.177734375 
[2025-02-17 18:26:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.003915784880518913 norm:8.119626727420837e-06 max memory_allocated 29229.177734375 
[2025-02-17 18:26:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.003915715496987104 norm:7.93465915194247e-06 max memory_allocated 29229.177734375 
[2025-02-17 18:27:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-17 18:27:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.005161758977919817 norm:0.0002440373646095395 max memory_allocated 29229.365234375 
[2025-02-17 18:28:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0046075028367340565 norm:7.795846613589674e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:29:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.004462872631847858 norm:4.5627304643858224e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:30:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.004415902309119701 norm:3.83856167900376e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:30:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.004381492733955383 norm:3.108093369519338e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:31:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.004354757256805897 norm:2.393268732703291e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:32:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.004336963873356581 norm:2.0365336240502074e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:33:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.004323720000684261 norm:1.7291207768721506e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:33:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.004315230064094067 norm:1.5437914044014178e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:34:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.004307376220822334 norm:1.4182524864736479e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:35:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.00430157408118248 norm:1.3117851267452352e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:36:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.004296465776860714 norm:1.2259974027983844e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:36:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.004292627796530724 norm:1.146313934441423e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:37:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.004290456883609295 norm:1.0812200343934819e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:38:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.004288771189749241 norm:1.027087819238659e-05 max memory_allocated 29229.365234375 
[2025-02-17 18:39:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.004286412615329027 norm:9.855652024270967e-06 max memory_allocated 29229.365234375 
[2025-02-17 18:39:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.0042848712764680386 norm:9.530396710033529e-06 max memory_allocated 29229.365234375 
[2025-02-17 18:40:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0042841448448598385 norm:9.248299647879321e-06 max memory_allocated 29229.365234375 
[2025-02-17 18:41:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.004282200708985329 norm:9.048185347637627e-06 max memory_allocated 29229.365234375 
[2025-02-17 18:42:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.004281466361135244 norm:8.814874490781222e-06 max memory_allocated 29229.365234375 
[2025-02-17 18:42:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-17 18:43:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.006102981045842171 norm:0.00039491348434239626 max memory_allocated 29229.552734375 
[2025-02-17 18:43:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.00529417535290122 norm:0.00014335602463688701 max memory_allocated 29229.552734375 
[2025-02-17 18:44:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.005075337830930948 norm:8.52005323395133e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:45:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.005000954493880272 norm:6.705674604745582e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:46:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.004956081975251436 norm:5.506439629243687e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:47:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.004915892146527767 norm:4.400372563395649e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:47:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.004887926392257214 norm:3.6313671444077045e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:48:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.004864789545536041 norm:3.0368091756827198e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:49:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.004849160090088844 norm:2.5876594008877873e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:50:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.0048401872627437115 norm:2.2828906367067248e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:50:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.004832090809941292 norm:2.0372533981571905e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:51:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.004826345480978489 norm:1.8355538486503065e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:52:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.004823037888854742 norm:1.643539872020483e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:53:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.004820757079869509 norm:1.4819926946074702e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:53:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.004818364512175322 norm:1.3766913980362006e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:54:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.004815761465579271 norm:1.2887028788099997e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:55:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.00481297867372632 norm:1.203921692649601e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:56:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.004810656886547804 norm:1.1327218089718372e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:56:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.004808906931430101 norm:1.0940799256786704e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:57:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.004808854311704636 norm:1.0408683920104522e-05 max memory_allocated 29229.552734375 
[2025-02-17 18:57:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-17 18:58:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.006538071669638157 norm:0.0003481487510725856 max memory_allocated 29229.740234375 
[2025-02-17 18:59:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.005832366179674864 norm:0.00011073529458371922 max memory_allocated 29229.740234375 
[2025-02-17 19:00:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.005641919560730457 norm:6.310365279205143e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:00:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.005579711403697729 norm:5.249048990663141e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:01:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.005538613069802523 norm:4.306637856643647e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:02:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.005502298008650541 norm:3.4447875805199146e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:03:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.005474867299199104 norm:2.8105310775572434e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:03:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.005459970328956842 norm:2.3791741114109755e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:04:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0054473718628287315 norm:2.122140176652465e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:05:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.005436460487544537 norm:1.926287768583279e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:06:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.005427319090813398 norm:1.7527670934214257e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:06:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.005419792141765356 norm:1.631826307857409e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:07:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.005414845887571573 norm:1.5244295354932547e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:08:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.005409234203398228 norm:1.4416956219065469e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:09:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.005405288189649582 norm:1.361228532914538e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:09:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.005403262563049793 norm:1.2868079465988558e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:10:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.005400693975389004 norm:1.2281655472179409e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:11:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.005398303270339966 norm:1.1760768757085316e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:12:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.005396021064370871 norm:1.116372186515946e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:12:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.005394488573074341 norm:1.0959740393445827e-05 max memory_allocated 29229.740234375 
[2025-02-17 19:13:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-17 19:13:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.007595008704811335 norm:0.000521709444001317 max memory_allocated 29229.927734375 
[2025-02-17 19:14:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.006613460835069418 norm:0.00016882744967006147 max memory_allocated 29229.927734375 
[2025-02-17 19:15:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.006345568224787712 norm:9.531899559078738e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:16:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.006246574223041534 norm:7.274850213434547e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:16:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.006195740308612585 norm:6.132198905106634e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:17:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.006151889450848103 norm:5.028702798881568e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:18:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.00611469941213727 norm:3.967832890339196e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:19:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.00608788849785924 norm:3.3011318009812385e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:19:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.00606834702193737 norm:2.844779555744026e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:20:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.006055057514458895 norm:2.5041514163604006e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:21:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.006044607609510422 norm:2.241465335828252e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:22:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.006037200801074505 norm:2.033745295193512e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:22:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.006031163968145847 norm:1.865976992121432e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:23:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.006025590002536774 norm:1.7148682672996074e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:24:20 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.006021400447934866 norm:1.5817488019820303e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:25:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.0060188053175807 norm:1.4628121789428405e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:25:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.006014413665980101 norm:1.377067110297503e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:26:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0060110995545983315 norm:1.295644324272871e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:27:20 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.006009845528751612 norm:1.2132172741985414e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:28:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.006007108371704817 norm:1.159560360974865e-05 max memory_allocated 29229.927734375 
[2025-02-17 19:28:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-17 19:29:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.008614313788712025 norm:0.000777016335632652 max memory_allocated 29230.115234375 
[2025-02-17 19:29:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.007466226350516081 norm:0.00021214570733718574 max memory_allocated 29230.115234375 
[2025-02-17 19:30:36 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.007183818146586418 norm:0.00011013678886229172 max memory_allocated 29230.115234375 
[2025-02-17 19:31:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.00708827655762434 norm:8.652668475406244e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:32:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.007040885277092457 norm:7.659902621526271e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:32:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.006997596472501755 norm:6.311394099611789e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:33:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.006955733988434076 norm:4.902789441985078e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:34:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.006929098162800074 norm:4.124621773371473e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:35:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.0069090318866074085 norm:3.5592951462604105e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:35:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.00689424155279994 norm:3.1411502277478576e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:36:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.006881547626107931 norm:2.8239144739927724e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:37:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.006871764548122883 norm:2.5955809178412892e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:38:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.0068637533113360405 norm:2.395909723418299e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:38:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.006856368854641914 norm:2.1986539650242776e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:39:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.006850878242403269 norm:2.0189370843581855e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:40:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.006847994402050972 norm:1.8372997146798298e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:41:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.006843768060207367 norm:1.7162914446089417e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:41:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.0068401675671339035 norm:1.590860847500153e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:42:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.006837703753262758 norm:1.4942676898499485e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:43:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.0068363891914486885 norm:1.423053618054837e-05 max memory_allocated 29230.115234375 
[2025-02-17 19:43:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-17 19:44:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.009001137688755989 norm:0.00036797739448957145 max memory_allocated 29230.302734375 
[2025-02-17 19:45:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.008304609917104244 norm:0.0001303312019445002 max memory_allocated 29230.302734375 
[2025-02-17 19:45:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.008100847713649273 norm:7.937235932331532e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:46:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.00801891554147005 norm:6.239021604415029e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:47:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.007964382879436016 norm:5.035204958403483e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:48:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.007922064512968063 norm:3.999188629677519e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:48:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.007891288958489895 norm:3.329038372612558e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:49:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.007871605455875397 norm:2.801933078444563e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:50:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.007854312658309937 norm:2.4992339604068547e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:51:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.007841532118618488 norm:2.2467020244221203e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:51:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.00783027708530426 norm:2.0651870727306232e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:52:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.007823066785931587 norm:1.927942139445804e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:53:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.00781712494790554 norm:1.7863385437522084e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:54:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.007813890464603901 norm:1.662507747823838e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:54:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.007809195201843977 norm:1.583404082339257e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:55:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.0078064026311039925 norm:1.5055406038300134e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:56:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.007803915534168482 norm:1.4391175682249013e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:57:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.007802064996212721 norm:1.389227054460207e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:57:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.00780020747333765 norm:1.3660011973115616e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:58:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.007798728533089161 norm:1.342604446108453e-05 max memory_allocated 29230.302734375 
[2025-02-17 19:58:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-17 19:59:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.010442495346069336 norm:0.0005014105117879808 max memory_allocated 29230.490234375 
[2025-02-17 20:00:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.009640389122068882 norm:0.0001857680908869952 max memory_allocated 29230.490234375 
[2025-02-17 20:01:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.009408630430698395 norm:0.0001193496718769893 max memory_allocated 29230.490234375 
[2025-02-17 20:01:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.009302830323576927 norm:9.475422120885924e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:02:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.009226691909134388 norm:7.58807800593786e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:03:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.00916827842593193 norm:6.0819780628662556e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:04:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.009123450145125389 norm:4.9826790927909315e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:04:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.009086403995752335 norm:4.226068267598748e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:05:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.009060990065336227 norm:3.6626908695325255e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:06:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.009038261137902737 norm:3.2434465538244694e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:07:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.009024903178215027 norm:2.9116039513610303e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:07:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.009011580608785152 norm:2.613744800328277e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:08:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.009003464132547379 norm:2.3656892153667286e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:09:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.008995722979307175 norm:2.178378235839773e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:10:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.008991214446723461 norm:2.0050156308570877e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:10:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.008985892869532108 norm:1.86886918527307e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:11:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.008981258608400822 norm:1.7841553926700726e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:12:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.008978600613772869 norm:1.7064157873392105e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:13:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.008973847143352032 norm:1.641300150367897e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:13:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.008973593823611736 norm:1.5915164112811908e-05 max memory_allocated 29230.490234375 
[2025-02-17 20:14:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-17 20:14:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.011234971694648266 norm:0.00047665383317507803 max memory_allocated 29230.677734375 
[2025-02-17 20:15:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.010562708601355553 norm:0.00015791822806932032 max memory_allocated 29230.677734375 
[2025-02-17 20:16:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.010401124134659767 norm:9.737877553561702e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:17:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.010333343409001827 norm:7.518813072238117e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:17:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.010287287645041943 norm:5.7926459703594446e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:18:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.010254931636154652 norm:4.5359975047176704e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:19:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.010229013860225677 norm:3.632661173469387e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:20:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.010212674736976624 norm:2.9731525501119904e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:20:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.010199010372161865 norm:2.5728806576807983e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:21:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.010190058499574661 norm:2.282950299559161e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:22:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.010190099477767944 norm:2.0443942048586905e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:23:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.010186922736465931 norm:1.878901821328327e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:23:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.010180536657571793 norm:1.7894753909786232e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:24:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.01017358060926199 norm:1.722646084090229e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:25:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.010169945657253265 norm:1.6533986126887612e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:26:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.0101693756878376 norm:1.6050205886131153e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:26:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.010167194530367851 norm:1.572963265061844e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:27:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.010164504870772362 norm:1.547446481708903e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:28:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.010163613595068455 norm:1.5294695913325995e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:29:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.010161873884499073 norm:1.5106775208550971e-05 max memory_allocated 29230.677734375 
[2025-02-17 20:29:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-17 20:30:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.012635650113224983 norm:0.0004814903368242085 max memory_allocated 29230.865234375 
[2025-02-17 20:31:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.011903759092092514 norm:0.00016542253433726728 max memory_allocated 29230.865234375 
[2025-02-17 20:31:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.011733457446098328 norm:0.00011363312660250813 max memory_allocated 29230.865234375 
[2025-02-17 20:32:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.011654194444417953 norm:9.060540469363332e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:33:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.01159772090613842 norm:7.266578904818743e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:34:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.011548434384167194 norm:5.6063930969685316e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:34:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.011514337733387947 norm:4.482689837459475e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:35:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.011492040008306503 norm:3.8469835999421775e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:36:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.011472832411527634 norm:3.3688080293359235e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:37:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.011461036279797554 norm:2.934712938440498e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:37:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.011453057639300823 norm:2.579953616077546e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:38:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.011449011042714119 norm:2.251800469821319e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:39:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.011445334181189537 norm:2.021682848862838e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:40:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.011439302936196327 norm:1.8619242837303318e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:40:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.011434483341872692 norm:1.70774419530062e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:41:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.011430483311414719 norm:1.5845196685404517e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:42:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.0114286495372653 norm:1.4933624697732739e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:43:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.011427771300077438 norm:1.4231627574190497e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:43:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.011424902826547623 norm:1.3716866305912845e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:44:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.011423682793974876 norm:1.3363109246711247e-05 max memory_allocated 29230.865234375 
[2025-02-17 20:44:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-17 20:45:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.013333232142031193 norm:0.00021284035756252706 max memory_allocated 29231.052734375 
[2025-02-17 20:46:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.013037781231105328 norm:0.00010424036008771509 max memory_allocated 29231.052734375 
[2025-02-17 20:47:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.01292876061052084 norm:7.199935498647392e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:47:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.012858257628977299 norm:5.271996997180395e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:48:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.012809599749743938 norm:4.095116310054436e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:49:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.01277774479240179 norm:3.327461308799684e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:50:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.012754352763295174 norm:2.8070699045201764e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:50:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.012736789882183075 norm:2.392072565271519e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:51:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.012722917832434177 norm:2.0690409655799158e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:52:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.012713944539427757 norm:1.8192920833826065e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:53:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.012708285823464394 norm:1.608410821063444e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:53:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.012705469503998756 norm:1.440687628928572e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:54:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.012701847590506077 norm:1.3302448678587098e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:55:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.012698827311396599 norm:1.2505988706834614e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:56:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.012697728350758553 norm:1.1953714420087636e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:56:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.012696526944637299 norm:1.1705975339282304e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:57:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.012695932760834694 norm:1.1442447430454195e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:58:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.012694109231233597 norm:1.1267652553215157e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:59:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.012692708522081375 norm:1.1181286026840098e-05 max memory_allocated 29231.052734375 
[2025-02-17 20:59:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.012691473588347435 norm:1.1146707038278691e-05 max memory_allocated 29231.052734375 
[2025-02-17 21:00:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-17 21:00:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.014863969758152962 norm:0.00023961762781254947 max memory_allocated 29231.240234375 
[2025-02-17 21:01:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.014552447013556957 norm:0.00011349339911248535 max memory_allocated 29231.240234375 
[2025-02-17 21:02:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.014436333440244198 norm:7.932737935334444e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:03:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.014364930801093578 norm:5.973449515295215e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:03:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.014315130189061165 norm:4.699464625446126e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:04:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.014278383925557137 norm:3.8524762203451246e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:05:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.014253459870815277 norm:3.3037125831469893e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:06:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.014237450435757637 norm:2.869445779651869e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:06:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.014224085956811905 norm:2.5406989152543247e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:07:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.014215239323675632 norm:2.2526937755174004e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:08:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.014205686748027802 norm:2.026102265517693e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:09:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.014198435470461845 norm:1.8825798179022968e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:09:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.014190858229994774 norm:1.7662052414380014e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:10:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.014183628372848034 norm:1.6572292224736884e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:11:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.014175448566675186 norm:1.6062836948549375e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:12:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.014171539805829525 norm:1.5175439330050722e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:12:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.014167852699756622 norm:1.4794124581385404e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:13:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.014163866639137268 norm:1.4254317648010328e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:14:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.014158358797430992 norm:1.4046734577277675e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:15:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.014153863303363323 norm:1.3894761650590226e-05 max memory_allocated 29231.240234375 
[2025-02-17 21:15:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-17 21:16:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.017420034855604172 norm:0.0009856362594291568 max memory_allocated 29231.427734375 
[2025-02-17 21:16:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.01622939109802246 norm:0.0002706000814214349 max memory_allocated 29231.427734375 
[2025-02-17 21:17:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.015969259664416313 norm:0.00014944042777642608 max memory_allocated 29231.427734375 
[2025-02-17 21:18:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.015889223664999008 norm:0.0001214129151776433 max memory_allocated 29231.427734375 
[2025-02-17 21:19:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.01584932766854763 norm:0.00010603419650578871 max memory_allocated 29231.427734375 
[2025-02-17 21:19:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.01580921746790409 norm:8.66396221681498e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:20:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.01577097363770008 norm:6.918763392604887e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:21:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.015737324953079224 norm:5.707893433282152e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:22:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.015718163922429085 norm:4.8235320718958974e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:22:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.015704063698649406 norm:4.2520241549937055e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:23:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.01569056697189808 norm:3.834802919300273e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:24:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.01568187028169632 norm:3.525983265717514e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:25:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.015672896057367325 norm:3.253824252169579e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:25:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.015668779611587524 norm:2.981167745019775e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:26:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.01566130854189396 norm:2.758750088105444e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:27:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.01565813645720482 norm:2.5669505703262985e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:28:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.015654737129807472 norm:2.3455435439245775e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:28:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.01564980298280716 norm:2.1810910766362213e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:29:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.01564658433198929 norm:2.0530935216811486e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:30:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.01564423367381096 norm:1.935674845299218e-05 max memory_allocated 29231.427734375 
[2025-02-17 21:30:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-17 21:31:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.017795268446207047 norm:0.0002467425656504929 max memory_allocated 29231.615234375 
[2025-02-17 21:32:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.01749938353896141 norm:0.00011585192260099575 max memory_allocated 29231.615234375 
[2025-02-17 21:32:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.017404286190867424 norm:8.167896885424852e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:33:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.01734124682843685 norm:6.0138841945445165e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:34:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.01729625277221203 norm:4.5845434215152636e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:35:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.017263807356357574 norm:3.728082447196357e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:35:55 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.017239976674318314 norm:3.1494251743424684e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:36:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.017223456874489784 norm:2.6731451725936495e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:37:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.017213614657521248 norm:2.272637175337877e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:38:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.017205802723765373 norm:1.929712379933335e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:38:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.01720418967306614 norm:1.6373749531339854e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:39:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.01720091700553894 norm:1.4322955394163728e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:40:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.017201248556375504 norm:1.2967491784365848e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:41:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.01719864457845688 norm:1.2168043213023338e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:41:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.017196375876665115 norm:1.1785925380536355e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:42:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.017193488776683807 norm:1.1488274139992427e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:43:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.017192166298627853 norm:1.1165981049998663e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:44:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.017191851511597633 norm:1.0994360309268814e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:44:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.017187833786010742 norm:1.094221261155326e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:45:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.01718645542860031 norm:1.0823016054928303e-05 max memory_allocated 29231.615234375 
[2025-02-17 21:45:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-17 21:46:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.019572414457798004 norm:0.00022719208209309727 max memory_allocated 29231.802734375 
[2025-02-17 21:47:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.019317427650094032 norm:0.0001263946178369224 max memory_allocated 29231.802734375 
[2025-02-17 21:48:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.019200166687369347 norm:8.667416113894433e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:48:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.019127588719129562 norm:6.533544365083799e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:49:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.019076788797974586 norm:5.134794628247619e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:50:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.01904691383242607 norm:4.210516999592073e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:51:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.01902158558368683 norm:3.550035762600601e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:51:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.019001945853233337 norm:3.052013198612258e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:52:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.01899014413356781 norm:2.6501120373723097e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:53:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.018977846950292587 norm:2.400832272542175e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:54:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.018966251984238625 norm:2.1658102923538536e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:54:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.01895756646990776 norm:2.001448410737794e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:55:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.018949417397379875 norm:1.880993295344524e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:56:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.018944870680570602 norm:1.750303454173263e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:57:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.018939506262540817 norm:1.6111242075567134e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:57:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.018935730680823326 norm:1.520455589343328e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:58:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.018929461017251015 norm:1.4835261026746593e-05 max memory_allocated 29231.802734375 
[2025-02-17 21:59:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.01892801746726036 norm:1.4586374163627625e-05 max memory_allocated 29231.802734375 
[2025-02-17 22:00:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.018926145508885384 norm:1.3727643818128854e-05 max memory_allocated 29231.802734375 
[2025-02-17 22:00:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.018923480063676834 norm:1.3636547919304576e-05 max memory_allocated 29231.802734375 
[2025-02-17 22:01:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-17 22:02:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.021288525313138962 norm:0.0002261247136630118 max memory_allocated 29231.990234375 
[2025-02-17 22:02:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.021021675318479538 norm:0.00011328300752211362 max memory_allocated 29231.990234375 
[2025-02-17 22:03:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.020920302718877792 norm:7.771415403112769e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:04:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.020856717601418495 norm:5.671495455317199e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:05:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.020814264193177223 norm:4.437367897480726e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:05:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.020786035805940628 norm:3.5970824683317915e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:06:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.020766422152519226 norm:2.95561840175651e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:07:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.020750684663653374 norm:2.4668061087140813e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:08:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.0207461379468441 norm:1.968170727195684e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:08:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.020741697400808334 norm:1.5872396033955738e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:09:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.020740142092108727 norm:1.3467049939208664e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:10:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.02073669619858265 norm:1.2001492905255873e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:11:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.02073223888874054 norm:1.1290063412161544e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:11:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.020727746188640594 norm:1.083893766917754e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:12:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.020723633468151093 norm:1.0563607247604523e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:13:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.020722927525639534 norm:1.0326592928322498e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:14:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.020721446722745895 norm:1.0202639714407269e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:14:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.02072235196828842 norm:1.0087892405863386e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:15:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.02071969583630562 norm:1.0158699296880513e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:16:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.02071860432624817 norm:1.0031328201876022e-05 max memory_allocated 29231.990234375 
[2025-02-17 22:16:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-17 22:17:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.023138204589486122 norm:0.00022883998462930322 max memory_allocated 29232.177734375 
[2025-02-17 22:18:02 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.022904664278030396 norm:0.00012377290113363415 max memory_allocated 29232.177734375 
[2025-02-17 22:18:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.02278953604400158 norm:8.251206600107253e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:19:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.022721048444509506 norm:6.1017963162157685e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:20:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.022674908861517906 norm:4.8388479626737535e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:21:02 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.02263832464814186 norm:3.922973701264709e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:21:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.02261374145746231 norm:3.227201523259282e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:22:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.02259647101163864 norm:2.697473428270314e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:23:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.022582881152629852 norm:2.2663414711132646e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:24:02 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.022571422159671783 norm:1.9360486476216465e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:24:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.02256196364760399 norm:1.6659450920997187e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:25:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.022561268880963326 norm:1.378899014525814e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:26:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.02256223000586033 norm:1.2082669854862615e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:27:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.022560184821486473 norm:1.1169935532961972e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:27:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.02255670353770256 norm:1.071936458174605e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:28:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.022557370364665985 norm:1.0483634468982928e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:29:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.022554395720362663 norm:1.0293037121300586e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:30:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.02255217544734478 norm:1.0143098734261002e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:30:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.0225506778806448 norm:1.0079561434395146e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:31:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.022549787536263466 norm:1.012378834275296e-05 max memory_allocated 29232.177734375 
[2025-02-17 22:31:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-17 22:32:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.025374561548233032 norm:0.00022253574570640922 max memory_allocated 29232.365234375 
[2025-02-17 22:33:19 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.025128444656729698 norm:0.00012127031368436292 max memory_allocated 29232.365234375 
[2025-02-17 22:34:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.025002840906381607 norm:8.028822776395828e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:34:49 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.02493101730942726 norm:5.922435593674891e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:35:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.02488541603088379 norm:4.653532960219309e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:36:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.024858977645635605 norm:3.760524850804359e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:37:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.02484055981040001 norm:3.1038958695717156e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:37:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.02482544630765915 norm:2.6758856620290317e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:38:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.024806687608361244 norm:2.38740994973341e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:39:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.024797970429062843 norm:2.1160389223950915e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:40:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.02478604204952717 norm:1.9323488231748343e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:40:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.024775495752692223 norm:1.787359178706538e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:41:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.02476721815764904 norm:1.6638283341308124e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:42:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.024762803688645363 norm:1.5711580999777652e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:43:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.02475789561867714 norm:1.5019133570604026e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:43:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.024757035076618195 norm:1.4142349755275063e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:44:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.024753008037805557 norm:1.354491450911155e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:45:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.02474956586956978 norm:1.3358713658817578e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:46:06 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.024747854098677635 norm:1.2951793905813247e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:46:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.024746637791395187 norm:1.2523948498710524e-05 max memory_allocated 29232.365234375 
[2025-02-17 22:47:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-17 22:47:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.028171544894576073 norm:0.00026516561047174037 max memory_allocated 29232.552734375 
[2025-02-17 22:48:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.02782246470451355 norm:0.00016200894606299698 max memory_allocated 29232.552734375 
[2025-02-17 22:49:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.02763606235384941 norm:0.00011307316890452057 max memory_allocated 29232.552734375 
[2025-02-17 22:50:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.02752416580915451 norm:8.522533607902005e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:50:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.02744881436228752 norm:6.727066647727042e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:51:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.027395276352763176 norm:5.464690548251383e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:52:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.027358636260032654 norm:4.478947084862739e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:53:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.027333252131938934 norm:3.755676152650267e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:53:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.02731211483478546 norm:3.234696123399772e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:54:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.02729519084095955 norm:2.82281544059515e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:55:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.027280500158667564 norm:2.518308065191377e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:56:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.02726546861231327 norm:2.2769972929381765e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:56:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.02725912258028984 norm:2.06732456717873e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:57:39 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.02724575251340866 norm:1.9237410015193745e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:58:24 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.02723812870681286 norm:1.807215448934585e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:59:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.027231864631175995 norm:1.7063604900613427e-05 max memory_allocated 29232.552734375 
[2025-02-17 22:59:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.027226530015468597 norm:1.627323217689991e-05 max memory_allocated 29232.552734375 
[2025-02-17 23:00:39 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.027223296463489532 norm:1.5533529222011566e-05 max memory_allocated 29232.552734375 
[2025-02-17 23:01:24 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.027218125760555267 norm:1.5062255442899186e-05 max memory_allocated 29232.552734375 
[2025-02-17 23:02:09 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.027216915041208267 norm:1.4574787201127037e-05 max memory_allocated 29232.552734375 
[2025-02-17 23:02:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-17 23:03:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.03107261098921299 norm:0.0002556132385507226 max memory_allocated 29232.740234375 
[2025-02-17 23:04:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.0306280255317688 norm:0.00015174203144852072 max memory_allocated 29232.740234375 
[2025-02-17 23:04:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.030394481495022774 norm:0.00010907994874287397 max memory_allocated 29232.740234375 
[2025-02-17 23:05:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.03024332784116268 norm:8.445743151241913e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:06:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.030137185007333755 norm:6.895595288369805e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:07:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.030062377452850342 norm:5.810643051518127e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:07:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.030003618448972702 norm:4.9195285100722685e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:08:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.029958022758364677 norm:4.2976720578735694e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:09:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.02991689182817936 norm:3.737012229976244e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:10:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.029887648299336433 norm:3.279934753663838e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:10:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.029868070036172867 norm:2.9401442589005455e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:11:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.029853960499167442 norm:2.6426972908666357e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:12:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.029841002076864243 norm:2.4219949409598485e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:13:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.02983112260699272 norm:2.2697990061715245e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:13:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.029820118099451065 norm:2.1322155589587055e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:14:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.02981407195329666 norm:2.0257855794625357e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:15:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.0298056248575449 norm:1.94357089640107e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:16:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.02980111911892891 norm:1.873365545179695e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:16:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.02980009652674198 norm:1.8003056538873352e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:17:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.029798313975334167 norm:1.750140290823765e-05 max memory_allocated 29232.740234375 
[2025-02-17 23:17:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-17 23:18:35 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.033952441066503525 norm:0.0001851495762821287 max memory_allocated 29232.927734375 
[2025-02-17 23:19:20 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.033538948744535446 norm:0.00011454000195953995 max memory_allocated 29232.927734375 
[2025-02-17 23:20:05 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.03329400345683098 norm:8.066555892582983e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:20:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.03314550593495369 norm:6.179139018058777e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:21:35 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.03303481265902519 norm:4.919800267089158e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:22:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.03295648843050003 norm:4.043642184115015e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:23:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.03290519490838051 norm:3.309757448732853e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:23:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.03286684677004814 norm:2.7782380129792728e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:24:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.03283781185746193 norm:2.4119086447171867e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:25:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.03281112760305405 norm:2.1219659174676053e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:26:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.03279421478509903 norm:1.9464245269773528e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:26:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.032782815396785736 norm:1.8013764929492027e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:27:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.03276985138654709 norm:1.7206168195116334e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:28:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.032759398221969604 norm:1.6967778719845228e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:29:06 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.03275366127490997 norm:1.586973485245835e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:29:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.032747555524110794 norm:1.545320264995098e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:30:37 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.03274139389395714 norm:1.54322824528208e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:31:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.03273290395736694 norm:1.492985484219389e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:32:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.03273112699389458 norm:1.486634664615849e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:32:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.03272358328104019 norm:1.4723449567100033e-05 max memory_allocated 29232.927734375 
[2025-02-17 23:33:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-17 23:33:53 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.03924998641014099 norm:0.0002749189152382314 max memory_allocated 29233.115234375 
[2025-02-17 23:34:38 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.03839334845542908 norm:0.00016629145829938352 max memory_allocated 29233.115234375 
[2025-02-17 23:35:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.03791089728474617 norm:0.0001207977402373217 max memory_allocated 29233.115234375 
[2025-02-17 23:36:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.037602946162223816 norm:8.834210893837735e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:36:53 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.037397272884845734 norm:7.13925837771967e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:37:38 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.03725779056549072 norm:5.876692375750281e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:38:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.03715190291404724 norm:4.9185277021024376e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:39:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.037071190774440765 norm:4.189707397017628e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:39:54 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.037012308835983276 norm:3.6967041523894295e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:40:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.03696557506918907 norm:3.278829899500124e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:41:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.036933548748493195 norm:3.053321415791288e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:42:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.03690139576792717 norm:2.8503836801974103e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:42:54 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.036878760904073715 norm:2.739556657616049e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:43:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.03685852885246277 norm:2.611435229482595e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:44:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.036844633519649506 norm:2.5897536033880897e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:45:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.036832306534051895 norm:2.5462679332122207e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:45:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.03682269528508186 norm:2.5153161914204247e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:46:40 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.036812569946050644 norm:2.4734623366384767e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:47:25 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.03680221736431122 norm:2.447546285111457e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:48:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.03680166229605675 norm:2.4429331460851245e-05 max memory_allocated 29233.115234375 
[2025-02-17 23:48:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-17 23:49:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.07409409433603287 norm:0.006586810573935509 max memory_allocated 29233.302734375 
[2025-02-17 23:49:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.05607180297374725 norm:0.0015775450738146901 max memory_allocated 29233.302734375 
[2025-02-17 23:50:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.05203204229474068 norm:0.0010217567905783653 max memory_allocated 29233.302734375 
[2025-02-17 23:51:26 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.05071333795785904 norm:0.0008034435450099409 max memory_allocated 29233.302734375 
[2025-02-17 23:52:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.04995270073413849 norm:0.0007257857359945774 max memory_allocated 29233.302734375 
[2025-02-17 23:52:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.04940224066376686 norm:0.0006474551628343761 max memory_allocated 29233.302734375 
[2025-02-17 23:53:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.048892877995967865 norm:0.0005423012771643698 max memory_allocated 29233.302734375 
[2025-02-17 23:54:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.04853995889425278 norm:0.0004560261149890721 max memory_allocated 29233.302734375 
[2025-02-17 23:55:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.04838162288069725 norm:0.00044476616312749684 max memory_allocated 29233.302734375 
[2025-02-17 23:55:57 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.04821810871362686 norm:0.0003980553010478616 max memory_allocated 29233.302734375 
[2025-02-17 23:56:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.04812834411859512 norm:0.00040650952723808587 max memory_allocated 29233.302734375 
[2025-02-17 23:57:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.04804646968841553 norm:0.00043391319923102856 max memory_allocated 29233.302734375 
[2025-02-17 23:58:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.04789223149418831 norm:0.0004111484158784151 max memory_allocated 29233.302734375 
[2025-02-17 23:58:57 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.04782549664378166 norm:0.0003866524784825742 max memory_allocated 29233.302734375 
[2025-02-17 23:59:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.04775693640112877 norm:0.00038171777850948274 max memory_allocated 29233.302734375 
[2025-02-18 00:00:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.0477069653570652 norm:0.00036020996049046516 max memory_allocated 29233.302734375 
[2025-02-18 00:01:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.04766751080751419 norm:0.000365706451702863 max memory_allocated 29233.302734375 
[2025-02-18 00:01:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.04756603017449379 norm:0.0003481766034383327 max memory_allocated 29233.302734375 
[2025-02-18 00:02:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.04746892675757408 norm:0.00033702675136737525 max memory_allocated 29233.302734375 
[2025-02-18 00:03:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.047464728355407715 norm:0.00034203779068775475 max memory_allocated 29233.302734375 
[2025-02-18 00:03:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-18 00:04:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.09842006862163544 norm:0.002865099348127842 max memory_allocated 29233.490234375 
[2025-02-18 00:05:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.09399941563606262 norm:0.0018681662622839212 max memory_allocated 29233.490234375 
[2025-02-18 00:05:59 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.0911240205168724 norm:0.001284456462599337 max memory_allocated 29233.490234375 
[2025-02-18 00:06:44 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.0889991968870163 norm:0.0009446603944525123 max memory_allocated 29233.490234375 
[2025-02-18 00:07:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.08748052269220352 norm:0.000714707188308239 max memory_allocated 29233.490234375 
[2025-02-18 00:08:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.08629751950502396 norm:0.0005560131976380944 max memory_allocated 29233.490234375 
[2025-02-18 00:08:59 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.08541902899742126 norm:0.0004505599499680102 max memory_allocated 29233.490234375 
[2025-02-18 00:09:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.0848027765750885 norm:0.0003788790199905634 max memory_allocated 29233.490234375 
[2025-02-18 00:10:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.08434946089982986 norm:0.0003343028365634382 max memory_allocated 29233.490234375 
[2025-02-18 00:11:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.08390476554632187 norm:0.00030673356377519667 max memory_allocated 29233.490234375 
[2025-02-18 00:12:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.08357278257608414 norm:0.0002891774638555944 max memory_allocated 29233.490234375 
[2025-02-18 00:12:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.08333200961351395 norm:0.00027778526418842375 max memory_allocated 29233.490234375 
[2025-02-18 00:13:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.0831083431839943 norm:0.0002734794979915023 max memory_allocated 29233.490234375 
[2025-02-18 00:14:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.08288442343473434 norm:0.00026414799503982067 max memory_allocated 29233.490234375 
[2025-02-18 00:15:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.08270450681447983 norm:0.00025948823895305395 max memory_allocated 29233.490234375 
[2025-02-18 00:15:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.08262770622968674 norm:0.00026683800388127565 max memory_allocated 29233.490234375 
[2025-02-18 00:16:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.08259320259094238 norm:0.00026931081083603203 max memory_allocated 29233.490234375 
[2025-02-18 00:17:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.08257020264863968 norm:0.0002575127291493118 max memory_allocated 29233.490234375 
[2025-02-18 00:18:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.0825805515050888 norm:0.0002544193703215569 max memory_allocated 29233.490234375 
[2025-02-18 00:18:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.08254934847354889 norm:0.0002512720529921353 max memory_allocated 29233.490234375 
[2025-02-18 00:18:58 root] (main_calibration.py 365): INFO 36715.98779153824
