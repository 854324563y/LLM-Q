[2025-02-18 04:40:32 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration/llama-13b-hf-w8a8', save_dir='./log-calibration/quant/llama-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-18 04:49:16 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 04:49:16 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-18 04:49:16 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 04:49:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 04:50:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0013696836540475488 norm:0.00034143426455557346 max memory_allocated 29226.177734375 
[2025-02-18 04:51:00 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0009430283680558205 norm:0.00011158392590004951 max memory_allocated 29226.177734375 
[2025-02-18 04:51:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0007876841118559241 norm:6.583247159142047e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:52:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0007087045814841986 norm:4.681158679886721e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:53:27 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0006647167028859258 norm:3.6857651139143854e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:54:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.00063851207960397 norm:3.1342202419182286e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:55:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0006194536690600216 norm:2.9437644116114825e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:55:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0006054296973161399 norm:3.069780723308213e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:56:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0005970891797915101 norm:2.4824974389048293e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:57:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.0005892181070521474 norm:2.3955401047714986e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:58:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.000585447414778173 norm:2.3570180928800255e-05 max memory_allocated 29226.177734375 
[2025-02-18 04:59:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0005821090890094638 norm:2.359387872274965e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:00:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0005790609284304082 norm:2.319867053302005e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:00:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0005767342518083751 norm:2.301770291524008e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:01:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0005750952404923737 norm:2.3097205485100858e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:02:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0005745035596191883 norm:2.3176147806225345e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:03:19 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0005718626780435443 norm:2.3175613023340702e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:04:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.0005704183713532984 norm:2.305129419255536e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:04:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0005675306310877204 norm:2.2939388145459816e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:05:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.0005669294623658061 norm:2.308620241819881e-05 max memory_allocated 29226.177734375 
[2025-02-18 05:06:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 05:06:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0012949316296726465 norm:0.00020150409545749426 max memory_allocated 29226.365234375 
[2025-02-18 05:07:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0010349886724725366 norm:6.454061804106459e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:08:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0009411296341568232 norm:3.788020330830477e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:09:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.000894608034286648 norm:2.674805000424385e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:10:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0008674963028170168 norm:2.1058105630800128e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:11:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0008501596748828888 norm:1.7707361621432938e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:11:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0008369075949303806 norm:1.5590341718052514e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:12:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0008292697020806372 norm:1.4330309568322264e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:13:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0008220925228670239 norm:1.3554023098549806e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:14:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0008162777521647513 norm:1.2954707926837727e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:15:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0008181515149772167 norm:1.2668514500546735e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:15:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0008205615449696779 norm:1.2532314940472133e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:16:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0008157943375408649 norm:1.2380392945487984e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:17:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0008124765590764582 norm:1.2248063285369426e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:18:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0008112179348245263 norm:1.2199396223877557e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:19:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0008092622156254947 norm:1.2161019185441546e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:20:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0008101263083517551 norm:1.2211272405693308e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:20:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.000810188998002559 norm:1.2210621207486838e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:21:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0008093024371191859 norm:1.221650654770201e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:22:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0008090414921753109 norm:1.2255959518370219e-05 max memory_allocated 29226.365234375 
[2025-02-18 05:22:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 05:23:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0015321505488827825 norm:0.00015015588724054396 max memory_allocated 29226.552734375 
[2025-02-18 05:24:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0012923548929393291 norm:6.984020001254976e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:25:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0012011346407234669 norm:5.1877479563700035e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:26:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0011424871627241373 norm:4.061816071043722e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:26:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.00110733846668154 norm:3.606931568356231e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:27:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0010891658021137118 norm:3.44765285262838e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:28:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0010714026866480708 norm:3.2405983802163973e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:29:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0010582837276160717 norm:3.1871782994130626e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:30:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0010599145898595452 norm:3.2818628824315965e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:31:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.001049706945195794 norm:3.129346441710368e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:31:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0010400288738310337 norm:3.2368316169595346e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:32:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0010327110067009926 norm:3.0074712412897497e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:33:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0010291426442563534 norm:3.0166429496603087e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:34:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.001032110070809722 norm:3.095240026596002e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:35:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0010296270484104753 norm:3.115305662504397e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:36:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0010315962135791779 norm:3.2665480830473825e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:36:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.001029872801154852 norm:3.1885305361356586e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:37:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0010285800090059638 norm:3.3407690352760255e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:38:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0010188368614763021 norm:3.078052759519778e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:39:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0010260418057441711 norm:3.300318348919973e-05 max memory_allocated 29226.552734375 
[2025-02-18 05:39:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 05:40:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0020656937267631292 norm:0.0006142990896478295 max memory_allocated 29226.740234375 
[2025-02-18 05:41:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0015653884038329124 norm:0.00024147865769919008 max memory_allocated 29226.740234375 
[2025-02-18 05:42:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0014133884105831385 norm:0.00014335841115098447 max memory_allocated 29226.740234375 
[2025-02-18 05:42:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0013441669289022684 norm:9.508644870948046e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:43:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.001310193445533514 norm:7.034176087472588e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:44:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0012903236784040928 norm:5.505895387614146e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:45:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0012749895686283708 norm:4.412646376295015e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:46:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0012667719274759293 norm:3.652254235930741e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:46:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0012587318196892738 norm:3.1108684197533876e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:47:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0012502686586230993 norm:2.598647552076727e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:48:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0012470014626160264 norm:2.1574647689703852e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:49:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0012436185497790575 norm:1.8397160602035e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:50:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0012389071052893996 norm:1.5884654203546233e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:51:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0012359145330265164 norm:1.3748441233474296e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:51:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0012346830917522311 norm:1.2484693797887303e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:52:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0012347579468041658 norm:1.1362296390871052e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:53:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0012334334896877408 norm:1.0860712791327387e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:54:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0012323857517912984 norm:1.0590219062578399e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:55:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0012305853888392448 norm:1.0446608939673752e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:56:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0012328819138929248 norm:1.0068499250337481e-05 max memory_allocated 29226.740234375 
[2025-02-18 05:56:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 05:57:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0038242789451032877 norm:0.0027999908197671175 max memory_allocated 29226.927734375 
[2025-02-18 05:57:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.00208462355658412 norm:0.0006316839717328548 max memory_allocated 29226.927734375 
[2025-02-18 05:58:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0017930702306330204 norm:0.0003638708731159568 max memory_allocated 29226.927734375 
[2025-02-18 05:59:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.001701071858406067 norm:0.00027935593971051276 max memory_allocated 29226.927734375 
[2025-02-18 06:00:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0016086735995486379 norm:0.00019266204617451876 max memory_allocated 29226.927734375 
[2025-02-18 06:01:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0015564074274152517 norm:0.00014639331493526697 max memory_allocated 29226.927734375 
[2025-02-18 06:02:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.001525085885077715 norm:0.00011465685383882374 max memory_allocated 29226.927734375 
[2025-02-18 06:02:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.001509491354227066 norm:9.831071656662971e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:03:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0014966664602980018 norm:8.582553709857166e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:04:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0014830303844064474 norm:7.535612530773506e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:05:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0014732885174453259 norm:6.68484135530889e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:06:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0014679243322461843 norm:6.0228281654417515e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:07:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0014632400125265121 norm:5.3984396799933165e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:07:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0014605887699872255 norm:4.792778781848028e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:08:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0014582648873329163 norm:4.349870505393483e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:09:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0014537799870595336 norm:3.879803989548236e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:10:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.001450785668566823 norm:3.43007777701132e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:11:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0014466828433796763 norm:2.9364256988628767e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:11:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0014429416041821241 norm:2.565247268648818e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:12:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0014413832686841488 norm:2.231874896096997e-05 max memory_allocated 29226.927734375 
[2025-02-18 06:13:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 06:13:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0030982522293925285 norm:0.00163803412579 max memory_allocated 29227.115234375 
[2025-02-18 06:14:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0019943828228861094 norm:0.0003661970840767026 max memory_allocated 29227.115234375 
[2025-02-18 06:15:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0018527591601014137 norm:0.00026240816805511713 max memory_allocated 29227.115234375 
[2025-02-18 06:16:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.00176332239061594 norm:0.00018793308117892593 max memory_allocated 29227.115234375 
[2025-02-18 06:17:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0016980450600385666 norm:0.00013464019866660237 max memory_allocated 29227.115234375 
[2025-02-18 06:18:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0016616616630926728 norm:0.00010562119132373482 max memory_allocated 29227.115234375 
[2025-02-18 06:18:50 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.001638331450521946 norm:8.72289965627715e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:19:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.001621273229829967 norm:7.50531762605533e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:20:28 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0016083813970908523 norm:6.536857108585536e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:21:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0015978700248524547 norm:5.750363925471902e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:22:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0015896293334662914 norm:5.1062015700154006e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:22:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0015848011244088411 norm:4.6451095840893686e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:23:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0015797519590705633 norm:4.2230771214235574e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:24:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0015738166403025389 norm:3.7095334846526384e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:25:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.001569948042742908 norm:3.326435762573965e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:26:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0015658248448744416 norm:3.056181230931543e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:27:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.001562818419188261 norm:2.7805181161966175e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:27:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.001560803852044046 norm:2.411270725133363e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:28:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.001558623043820262 norm:2.080439662677236e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:29:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0015563410706818104 norm:1.812454138416797e-05 max memory_allocated 29227.115234375 
[2025-02-18 06:29:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 06:30:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0026353432331234217 norm:0.00034920440521091223 max memory_allocated 29227.302734375 
[2025-02-18 06:31:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0022085723467171192 norm:0.00011851283488795161 max memory_allocated 29227.302734375 
[2025-02-18 06:32:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.002124497899785638 norm:8.485243597533554e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:33:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0020657654386013746 norm:5.680972753907554e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:33:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.002024289220571518 norm:4.3360945710446686e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:34:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0020037032663822174 norm:3.565070073818788e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:35:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.001986600225791335 norm:3.105362338828854e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:36:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0019727086182683706 norm:2.8008464141748846e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:37:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0019666666630655527 norm:2.4780329113127664e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:38:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.001954368781298399 norm:2.2981710571912117e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:38:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0019475063309073448 norm:2.0924788259435445e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:39:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0019483278738334775 norm:1.923914169310592e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:40:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.0019478171598166227 norm:1.8897624613600783e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:41:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.001948971301317215 norm:1.7750511688063852e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:42:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0019382431637495756 norm:1.6818441508803517e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:42:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0019391474779695272 norm:1.6960226275841706e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:43:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0019380120793357491 norm:1.5782678019604646e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:44:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0019380649318918586 norm:1.525137122371234e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:45:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0019419523887336254 norm:1.4669360098196194e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:46:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0019383926410228014 norm:1.4930242286936846e-05 max memory_allocated 29227.302734375 
[2025-02-18 06:46:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 06:47:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0031544941011816263 norm:0.00044521497329697013 max memory_allocated 29227.490234375 
[2025-02-18 06:48:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0022836122661828995 norm:9.997173037845641e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:49:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.002117870608344674 norm:6.37135817669332e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:49:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.00207641557790339 norm:5.8980822359444574e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:50:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0020285320933908224 norm:4.096324846614152e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:51:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.0020042695105075836 norm:3.0308880013762973e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:52:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.001990910852327943 norm:2.378356111876201e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:53:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.001981760375201702 norm:2.0619656424969435e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:53:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.0019746606703847647 norm:1.784415144356899e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:54:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.001970150275155902 norm:1.5416233509313315e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:55:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.001966154668480158 norm:1.4244813428376801e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:56:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0019645877182483673 norm:1.3234819562057965e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:57:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.001961459405720234 norm:1.2231913387950044e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:58:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0019585653208196163 norm:1.1113600521639455e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:58:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0019574316684156656 norm:1.0277069122821558e-05 max memory_allocated 29227.490234375 
[2025-02-18 06:59:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0019559503998607397 norm:9.885787221719511e-06 max memory_allocated 29227.490234375 
[2025-02-18 07:00:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.001954874722287059 norm:9.205857168126386e-06 max memory_allocated 29227.490234375 
[2025-02-18 07:01:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0019528336124494672 norm:8.967523172032088e-06 max memory_allocated 29227.490234375 
[2025-02-18 07:02:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0019528153352439404 norm:8.537616849935148e-06 max memory_allocated 29227.490234375 
[2025-02-18 07:02:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.0019517175387591124 norm:8.184725629689638e-06 max memory_allocated 29227.490234375 
[2025-02-18 07:03:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 07:04:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.002771571045741439 norm:0.00020136084640398622 max memory_allocated 29227.677734375 
[2025-02-18 07:04:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0023022980894893408 norm:6.048698560334742e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:05:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.00221073254942894 norm:4.3802701839013025e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:06:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0021649699192494154 norm:3.314476634841412e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:07:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.00213813129812479 norm:2.3652217350900173e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:08:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0021241011563688517 norm:1.9670495021273382e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:09:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.00211317278444767 norm:1.639599577174522e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:09:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.002106259809806943 norm:1.3916655007051304e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:10:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0020999093540012836 norm:1.2525914826255757e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:11:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.002096825512126088 norm:1.1114194421679713e-05 max memory_allocated 29227.677734375 
[2025-02-18 07:12:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0020931698381900787 norm:9.984677490137983e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:13:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0020914478227496147 norm:9.11004826775752e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:13:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.002089998684823513 norm:8.484480531478766e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:14:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0020893956534564495 norm:8.126648026518524e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:15:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.002089513698592782 norm:7.442076821462251e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:16:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0020894063636660576 norm:7.17395141691668e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:17:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.002088555134832859 norm:7.078820544847986e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:18:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0020866827107965946 norm:6.919568022567546e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:18:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0020860345102846622 norm:6.841015419922769e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:19:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.002085249638184905 norm:6.697080152662238e-06 max memory_allocated 29227.677734375 
[2025-02-18 07:19:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 07:20:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002831272082403302 norm:0.0001702313602436334 max memory_allocated 29227.865234375 
[2025-02-18 07:21:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0024091461673378944 norm:4.729200009023771e-05 max memory_allocated 29227.865234375 
[2025-02-18 07:22:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.00231955386698246 norm:3.231321170460433e-05 max memory_allocated 29227.865234375 
[2025-02-18 07:23:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0022866418585181236 norm:2.903792483266443e-05 max memory_allocated 29227.865234375 
[2025-02-18 07:24:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0022531449794769287 norm:1.8708677089307457e-05 max memory_allocated 29227.865234375 
[2025-02-18 07:24:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0022430073004215956 norm:1.3738125744566787e-05 max memory_allocated 29227.865234375 
[2025-02-18 07:25:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.002238686429336667 norm:1.1419556358305272e-05 max memory_allocated 29227.865234375 
[2025-02-18 07:26:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0022354458924382925 norm:1.028224596666405e-05 max memory_allocated 29227.865234375 
[2025-02-18 07:27:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.002230808138847351 norm:8.945045919972472e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:28:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0022284334991127253 norm:8.052938937908038e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:29:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0022273524664342403 norm:7.429730430885684e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:29:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.002225827192887664 norm:7.2468683356419206e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:30:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0022255287040024996 norm:6.933656095498009e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:31:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.002223882358521223 norm:6.6099510149797425e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:32:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0022230297327041626 norm:6.216554083948722e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:33:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0022217915393412113 norm:5.9123444771103095e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:33:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0022202106192708015 norm:5.838681772729615e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:34:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0022192085161805153 norm:5.6991575547726825e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:35:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0022184052504599094 norm:5.664240234182216e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:36:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0022176713682711124 norm:5.628967301163357e-06 max memory_allocated 29227.865234375 
[2025-02-18 07:36:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 07:37:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0035609095357358456 norm:0.00039540472789667547 max memory_allocated 29228.052734375 
[2025-02-18 07:38:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0027111603412777185 norm:9.945163037627935e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:39:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.002529972931370139 norm:5.194506229599938e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:40:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.002483983291313052 norm:4.4371223339112476e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:40:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0024540650192648172 norm:3.592681969166733e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:41:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.002425492275506258 norm:2.643883999553509e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:42:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0024099666625261307 norm:2.0956767912139185e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:43:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.002403195248916745 norm:1.763245563779492e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:44:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0023985288571566343 norm:1.5214281120279338e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:44:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0023945020511746407 norm:1.3492183825292159e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:45:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.002391589805483818 norm:1.2196809620945714e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:46:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.002388130407780409 norm:1.10109231172828e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:47:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0023871769662946463 norm:1.006253205559915e-05 max memory_allocated 29228.052734375 
[2025-02-18 07:48:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.002385703381150961 norm:9.166806194116361e-06 max memory_allocated 29228.052734375 
[2025-02-18 07:49:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.002384273102506995 norm:8.72297187015647e-06 max memory_allocated 29228.052734375 
[2025-02-18 07:49:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.00238370755687356 norm:8.434426490566693e-06 max memory_allocated 29228.052734375 
[2025-02-18 07:50:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0023819534108042717 norm:8.012117177713662e-06 max memory_allocated 29228.052734375 
[2025-02-18 07:51:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0023802570067346096 norm:7.350285613938468e-06 max memory_allocated 29228.052734375 
[2025-02-18 07:52:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.002379366662353277 norm:7.009351520537166e-06 max memory_allocated 29228.052734375 
[2025-02-18 07:53:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0023793468717485666 norm:6.6743095885613e-06 max memory_allocated 29228.052734375 
[2025-02-18 07:53:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 07:54:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0031990911811590195 norm:0.00015098614676389843 max memory_allocated 29228.240234375 
[2025-02-18 07:55:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.002782912226393819 norm:4.958333738613874e-05 max memory_allocated 29228.240234375 
[2025-02-18 07:55:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0026748492382466793 norm:2.984890124935191e-05 max memory_allocated 29228.240234375 
[2025-02-18 07:56:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0026420638896524906 norm:2.4942983145592734e-05 max memory_allocated 29228.240234375 
[2025-02-18 07:57:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0026138820685446262 norm:1.834385511756409e-05 max memory_allocated 29228.240234375 
[2025-02-18 07:58:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002599047962576151 norm:1.385595078318147e-05 max memory_allocated 29228.240234375 
[2025-02-18 07:59:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0025899438187479973 norm:1.1237745638936758e-05 max memory_allocated 29228.240234375 
[2025-02-18 08:00:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0025844143237918615 norm:9.780443178897258e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:00:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.002580119064077735 norm:8.681059625814669e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:01:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0025758566334843636 norm:7.674781045352574e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:02:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0025741704739630222 norm:7.131960501283174e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:03:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.002573953475803137 norm:6.343657787510892e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:04:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0025736200623214245 norm:5.932543899689335e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:05:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0025711050257086754 norm:5.702247108274605e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:05:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.002571644727140665 norm:5.410927315097069e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:06:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0025707969907671213 norm:5.28246982867131e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:07:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0025699324905872345 norm:5.2315008360892534e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:08:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.002569513162598014 norm:5.039788447902538e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:09:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0025681823026388884 norm:4.979770892532542e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:09:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.002567294519394636 norm:4.905142304778565e-06 max memory_allocated 29228.240234375 
[2025-02-18 08:10:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 08:11:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0033158890437334776 norm:0.00015583931235596538 max memory_allocated 29228.427734375 
[2025-02-18 08:11:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.002933943411335349 norm:5.103780495119281e-05 max memory_allocated 29228.427734375 
[2025-02-18 08:12:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0028423359617590904 norm:3.0578492442145944e-05 max memory_allocated 29228.427734375 
[2025-02-18 08:13:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.002807898446917534 norm:2.5545507014612667e-05 max memory_allocated 29228.427734375 
[2025-02-18 08:14:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.002781794872134924 norm:1.9186141798854806e-05 max memory_allocated 29228.427734375 
[2025-02-18 08:15:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.0027673563454300165 norm:1.465457353333477e-05 max memory_allocated 29228.427734375 
[2025-02-18 08:16:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0027607742231339216 norm:1.1920581528102048e-05 max memory_allocated 29228.427734375 
[2025-02-18 08:16:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.002756188390776515 norm:1.0444811778143048e-05 max memory_allocated 29228.427734375 
[2025-02-18 08:17:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.002753450535237789 norm:9.447526281292085e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:18:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.002749915700405836 norm:8.882852853275836e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:19:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.002747884253039956 norm:8.129151865432505e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:20:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0027466025203466415 norm:7.651171472389251e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:20:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.002744043245911598 norm:7.379863291134825e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:21:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.002744309836998582 norm:7.060843472572742e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:22:34 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002743417862802744 norm:7.043935511319432e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:23:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0027423021383583546 norm:6.799722086725524e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:24:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.002741588046774268 norm:6.662185114691965e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:25:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0027414162177592516 norm:6.546034910570597e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:25:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.002740747295320034 norm:6.494048648164608e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:26:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.002741097239777446 norm:6.405656222341349e-06 max memory_allocated 29228.427734375 
[2025-02-18 08:26:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 08:27:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.00343377236276865 norm:0.00010066798859043047 max memory_allocated 29228.615234375 
[2025-02-18 08:28:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.003161950269713998 norm:3.5159391700290143e-05 max memory_allocated 29228.615234375 
[2025-02-18 08:29:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.003099876455962658 norm:2.5728069886099547e-05 max memory_allocated 29228.615234375 
[2025-02-18 08:30:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0030679276678711176 norm:2.0723433408420533e-05 max memory_allocated 29228.615234375 
[2025-02-18 08:31:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.003046344965696335 norm:1.607847116247285e-05 max memory_allocated 29228.615234375 
[2025-02-18 08:31:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0030334966722875834 norm:1.3175377716834191e-05 max memory_allocated 29228.615234375 
[2025-02-18 08:32:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.0030256733298301697 norm:1.1403872122173198e-05 max memory_allocated 29228.615234375 
[2025-02-18 08:33:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0030196821317076683 norm:1.0346515409764834e-05 max memory_allocated 29228.615234375 
[2025-02-18 08:34:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.003015943570062518 norm:9.698926987766754e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:35:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.0030119491275399923 norm:9.011142537929118e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:36:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0030094904359430075 norm:8.436526513833087e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:36:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0030076061375439167 norm:8.134584277286194e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:37:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0030059919226914644 norm:7.961857590998989e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:38:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.003004638943821192 norm:7.75341504777316e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:39:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0030043430160731077 norm:7.613186426169705e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:40:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.003003736026585102 norm:7.54753682485898e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:40:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0030032387003302574 norm:7.456025286955992e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:41:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0030033409129828215 norm:7.451550118275918e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:42:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0030029958579689264 norm:7.414032097585732e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:43:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0030028210021555424 norm:7.381599061773159e-06 max memory_allocated 29228.615234375 
[2025-02-18 08:43:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 08:44:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.004137812182307243 norm:0.0002243325434392318 max memory_allocated 29228.802734375 
[2025-02-18 08:45:22 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0035669244825839996 norm:7.813964475644752e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:46:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0034071204718202353 norm:4.5603148464579135e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:47:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.003354316111654043 norm:3.614267552620731e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:47:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.003320510732010007 norm:2.982359910674859e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:48:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.0032903810497373343 norm:2.3444212274625897e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:49:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.003271870780736208 norm:1.906870056700427e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:50:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0032611670903861523 norm:1.6178953956114128e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:51:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.003253770060837269 norm:1.4517108866130002e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:51:56 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0032475064508616924 norm:1.301212341786595e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:52:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0032428919803351164 norm:1.1853628166136332e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:53:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0032380789052695036 norm:1.1106866622867528e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:54:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.00323581718839705 norm:1.0014353392762132e-05 max memory_allocated 29228.802734375 
[2025-02-18 08:55:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0032332190312445164 norm:9.34042691369541e-06 max memory_allocated 29228.802734375 
[2025-02-18 08:56:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0032311659306287766 norm:8.996757969725877e-06 max memory_allocated 29228.802734375 
[2025-02-18 08:56:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.0032294036354869604 norm:8.623344911029562e-06 max memory_allocated 29228.802734375 
[2025-02-18 08:57:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.003227220382541418 norm:8.347225957550108e-06 max memory_allocated 29228.802734375 
[2025-02-18 08:58:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.003226931905373931 norm:7.933001143101137e-06 max memory_allocated 29228.802734375 
[2025-02-18 08:59:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0032273402903229 norm:7.678883775952272e-06 max memory_allocated 29228.802734375 
[2025-02-18 09:00:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.0032275766134262085 norm:7.48937918615411e-06 max memory_allocated 29228.802734375 
[2025-02-18 09:00:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 09:01:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.005048453342169523 norm:0.0005366138066165149 max memory_allocated 29228.990234375 
[2025-02-18 09:02:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.004050071816891432 norm:0.00014053632912691683 max memory_allocated 29228.990234375 
[2025-02-18 09:02:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0038305390626192093 norm:7.818449375918135e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:03:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0037637418136000633 norm:6.521635077660903e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:04:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0037322749849408865 norm:5.880362732568756e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:05:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0036886318121105433 norm:4.3729738536057994e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:06:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.003664040472358465 norm:3.523475243127905e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:07:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.003648243611678481 norm:3.0473580409307033e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:07:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0036365867126733065 norm:2.666146792762447e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:08:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.003626949153840542 norm:2.3787790269125253e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:09:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0036180648021399975 norm:2.1944706531940028e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:10:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0036115827970206738 norm:1.956002233782783e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:11:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0036076558753848076 norm:1.804766543500591e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:11:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0036052013747394085 norm:1.7484826457803138e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:12:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.003601035103201866 norm:1.5715117115178145e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:13:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.003598572686314583 norm:1.4363014088303316e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:14:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.0035960867535322905 norm:1.3511582437786274e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:15:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.003594190813601017 norm:1.2767783118761145e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:16:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0035929898731410503 norm:1.2239090210641734e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:16:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.003591591026633978 norm:1.1686419384204783e-05 max memory_allocated 29228.990234375 
[2025-02-18 09:17:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 09:18:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.004800908733159304 norm:0.0002608197391964495 max memory_allocated 29229.177734375 
[2025-02-18 09:18:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.004219126887619495 norm:7.609309250256047e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:19:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.004086334723979235 norm:4.5862008846597746e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:20:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.004046346060931683 norm:4.01256256736815e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:21:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.004012316465377808 norm:3.24129214277491e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:22:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.003984851762652397 norm:2.565639078966342e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:22:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.003964850213378668 norm:2.069359652523417e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:23:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.003953120205551386 norm:1.7835083781392314e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:24:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.003943887073546648 norm:1.597629852767568e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:25:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.003938676789402962 norm:1.4323355571832508e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:26:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.003932454623281956 norm:1.3129060789651703e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:27:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.003928682766854763 norm:1.207443347084336e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:27:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.00392550602555275 norm:1.1119788723590318e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:28:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.003923462238162756 norm:1.0415542419650592e-05 max memory_allocated 29229.177734375 
[2025-02-18 09:29:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.003921921364963055 norm:9.699891961645335e-06 max memory_allocated 29229.177734375 
[2025-02-18 09:30:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.0039199781604111195 norm:9.188855983666144e-06 max memory_allocated 29229.177734375 
[2025-02-18 09:31:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.0039180065505206585 norm:8.844054718792904e-06 max memory_allocated 29229.177734375 
[2025-02-18 09:32:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.003916603513062 norm:8.412809620494954e-06 max memory_allocated 29229.177734375 
[2025-02-18 09:32:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.003915784880518913 norm:8.119626727420837e-06 max memory_allocated 29229.177734375 
[2025-02-18 09:33:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.003915715496987104 norm:7.93465915194247e-06 max memory_allocated 29229.177734375 
[2025-02-18 09:33:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 09:34:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.005161758977919817 norm:0.0002440373646095395 max memory_allocated 29229.365234375 
[2025-02-18 09:35:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0046075028367340565 norm:7.795846613589674e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:36:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.004462872631847858 norm:4.5627304643858224e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:37:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.004415902309119701 norm:3.83856167900376e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:38:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.004381492733955383 norm:3.108093369519338e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:38:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.004354757256805897 norm:2.393268732703291e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:39:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.004336963873356581 norm:2.0365336240502074e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:40:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.004323720000684261 norm:1.7291207768721506e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:41:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.004315230064094067 norm:1.5437914044014178e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:42:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.004307376220822334 norm:1.4182524864736479e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:42:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.00430157408118248 norm:1.3117851267452352e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:43:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.004296465776860714 norm:1.2259974027983844e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:44:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.004292627796530724 norm:1.146313934441423e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:45:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.004290456883609295 norm:1.0812200343934819e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:46:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.004288771189749241 norm:1.027087819238659e-05 max memory_allocated 29229.365234375 
[2025-02-18 09:47:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.004286412615329027 norm:9.855652024270967e-06 max memory_allocated 29229.365234375 
[2025-02-18 09:47:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.0042848712764680386 norm:9.530396710033529e-06 max memory_allocated 29229.365234375 
[2025-02-18 09:48:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0042841448448598385 norm:9.248299647879321e-06 max memory_allocated 29229.365234375 
[2025-02-18 09:49:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.004282200708985329 norm:9.048185347637627e-06 max memory_allocated 29229.365234375 
[2025-02-18 09:50:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.004281466361135244 norm:8.814874490781222e-06 max memory_allocated 29229.365234375 
[2025-02-18 09:50:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 09:51:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.006102981045842171 norm:0.00039491348434239626 max memory_allocated 29229.552734375 
[2025-02-18 09:52:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.00529417535290122 norm:0.00014335602463688701 max memory_allocated 29229.552734375 
[2025-02-18 09:53:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.005075337830930948 norm:8.52005323395133e-05 max memory_allocated 29229.552734375 
[2025-02-18 09:53:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.005000954493880272 norm:6.705674604745582e-05 max memory_allocated 29229.552734375 
[2025-02-18 09:54:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.004956081975251436 norm:5.506439629243687e-05 max memory_allocated 29229.552734375 
[2025-02-18 09:55:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.004915892146527767 norm:4.400372563395649e-05 max memory_allocated 29229.552734375 
[2025-02-18 09:56:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.004887926392257214 norm:3.6313671444077045e-05 max memory_allocated 29229.552734375 
[2025-02-18 09:57:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.004864789545536041 norm:3.0368091756827198e-05 max memory_allocated 29229.552734375 
[2025-02-18 09:58:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.004849160090088844 norm:2.5876594008877873e-05 max memory_allocated 29229.552734375 
[2025-02-18 09:58:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.0048401872627437115 norm:2.2828906367067248e-05 max memory_allocated 29229.552734375 
[2025-02-18 09:59:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.004832090809941292 norm:2.0372533981571905e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:00:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.004826345480978489 norm:1.8355538486503065e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:01:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.004823037888854742 norm:1.643539872020483e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:02:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.004820757079869509 norm:1.4819926946074702e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:03:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.004818364512175322 norm:1.3766913980362006e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:03:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.004815761465579271 norm:1.2887028788099997e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:04:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.00481297867372632 norm:1.203921692649601e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:05:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.004810656886547804 norm:1.1327218089718372e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:06:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.004808906931430101 norm:1.0940799256786704e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:07:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.004808854311704636 norm:1.0408683920104522e-05 max memory_allocated 29229.552734375 
[2025-02-18 10:07:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 10:08:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.006538071669638157 norm:0.0003481487510725856 max memory_allocated 29229.740234375 
[2025-02-18 10:09:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.005832366179674864 norm:0.00011073529458371922 max memory_allocated 29229.740234375 
[2025-02-18 10:09:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.005641919560730457 norm:6.310365279205143e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:10:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.005579711403697729 norm:5.249048990663141e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:11:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.005538613069802523 norm:4.306637856643647e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:12:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.005502298008650541 norm:3.4447875805199146e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:13:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.005474867299199104 norm:2.8105310775572434e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:14:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.005459970328956842 norm:2.3791741114109755e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:14:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0054473718628287315 norm:2.122140176652465e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:15:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.005436460487544537 norm:1.926287768583279e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:16:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.005427319090813398 norm:1.7527670934214257e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:17:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.005419792141765356 norm:1.631826307857409e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:18:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.005414845887571573 norm:1.5244295354932547e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:18:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.005409234203398228 norm:1.4416956219065469e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:19:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.005405288189649582 norm:1.361228532914538e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:20:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.005403262563049793 norm:1.2868079465988558e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:21:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.005400693975389004 norm:1.2281655472179409e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:22:13 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.005398303270339966 norm:1.1760768757085316e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:23:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.005396021064370871 norm:1.116372186515946e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:23:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.005394488573074341 norm:1.0959740393445827e-05 max memory_allocated 29229.740234375 
[2025-02-18 10:24:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 10:24:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.007595008704811335 norm:0.000521709444001317 max memory_allocated 29229.927734375 
[2025-02-18 10:25:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.006613460835069418 norm:0.00016882744967006147 max memory_allocated 29229.927734375 
[2025-02-18 10:26:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.006345568224787712 norm:9.531899559078738e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:27:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.006246574223041534 norm:7.274850213434547e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:28:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.006195740308612585 norm:6.132198905106634e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:29:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.006151889450848103 norm:5.028702798881568e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:29:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.00611469941213727 norm:3.967832890339196e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:30:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.00608788849785924 norm:3.3011318009812385e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:31:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.00606834702193737 norm:2.844779555744026e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:32:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.006055057514458895 norm:2.5041514163604006e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:33:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.006044607609510422 norm:2.241465335828252e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:34:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.006037200801074505 norm:2.033745295193512e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:34:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.006031163968145847 norm:1.865976992121432e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:35:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.006025590002536774 norm:1.7148682672996074e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:36:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.006021400447934866 norm:1.5817488019820303e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:37:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.0060188053175807 norm:1.4628121789428405e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:38:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.006014413665980101 norm:1.377067110297503e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:38:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0060110995545983315 norm:1.295644324272871e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:39:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.006009845528751612 norm:1.2132172741985414e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:40:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.006007108371704817 norm:1.159560360974865e-05 max memory_allocated 29229.927734375 
[2025-02-18 10:40:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 10:41:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.008614313788712025 norm:0.000777016335632652 max memory_allocated 29230.115234375 
[2025-02-18 10:42:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.007466226350516081 norm:0.00021214570733718574 max memory_allocated 29230.115234375 
[2025-02-18 10:43:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.007183818146586418 norm:0.00011013678886229172 max memory_allocated 29230.115234375 
[2025-02-18 10:44:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.00708827655762434 norm:8.652668475406244e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:45:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.007040885277092457 norm:7.659902621526271e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:45:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.006997596472501755 norm:6.311394099611789e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:46:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.006955733988434076 norm:4.902789441985078e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:47:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.006929098162800074 norm:4.124621773371473e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:48:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.0069090318866074085 norm:3.5592951462604105e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:49:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.00689424155279994 norm:3.1411502277478576e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:49:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.006881547626107931 norm:2.8239144739927724e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:50:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.006871764548122883 norm:2.5955809178412892e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:51:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.0068637533113360405 norm:2.395909723418299e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:52:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.006856368854641914 norm:2.1986539650242776e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:53:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.006850878242403269 norm:2.0189370843581855e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:54:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.006847994402050972 norm:1.8372997146798298e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:54:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.006843768060207367 norm:1.7162914446089417e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:55:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.0068401675671339035 norm:1.590860847500153e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:56:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.006837703753262758 norm:1.4942676898499485e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:57:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.0068363891914486885 norm:1.423053618054837e-05 max memory_allocated 29230.115234375 
[2025-02-18 10:57:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 10:58:28 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.009001137688755989 norm:0.00036797739448957145 max memory_allocated 29230.302734375 
[2025-02-18 10:59:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.008304609917104244 norm:0.0001303312019445002 max memory_allocated 29230.302734375 
[2025-02-18 11:00:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.008100847713649273 norm:7.937235932331532e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:00:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.00801891554147005 norm:6.239021604415029e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:01:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.007964382879436016 norm:5.035204958403483e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:02:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.007922064512968063 norm:3.999188629677519e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:03:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.007891288958489895 norm:3.329038372612558e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:04:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.007871605455875397 norm:2.801933078444563e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:05:03 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.007854312658309937 norm:2.4992339604068547e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:05:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.007841532118618488 norm:2.2467020244221203e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:06:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.00783027708530426 norm:2.0651870727306232e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:07:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.007823066785931587 norm:1.927942139445804e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:08:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.00781712494790554 norm:1.7863385437522084e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:09:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.007813890464603901 norm:1.662507747823838e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:09:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.007809195201843977 norm:1.583404082339257e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:10:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.0078064026311039925 norm:1.5055406038300134e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:11:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.007803915534168482 norm:1.4391175682249013e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:12:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.007802064996212721 norm:1.389227054460207e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:13:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.00780020747333765 norm:1.3660011973115616e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:14:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.007798728533089161 norm:1.342604446108453e-05 max memory_allocated 29230.302734375 
[2025-02-18 11:14:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 11:15:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.010442495346069336 norm:0.0005014105117879808 max memory_allocated 29230.490234375 
[2025-02-18 11:16:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.009640389122068882 norm:0.0001857680908869952 max memory_allocated 29230.490234375 
[2025-02-18 11:16:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.009408630430698395 norm:0.0001193496718769893 max memory_allocated 29230.490234375 
[2025-02-18 11:17:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.009302830323576927 norm:9.475422120885924e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:18:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.009226691909134388 norm:7.58807800593786e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:19:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.00916827842593193 norm:6.0819780628662556e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:20:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.009123450145125389 norm:4.9826790927909315e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:20:59 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.009086403995752335 norm:4.226068267598748e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:21:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.009060990065336227 norm:3.6626908695325255e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:22:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.009038261137902737 norm:3.2434465538244694e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:23:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.009024903178215027 norm:2.9116039513610303e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:24:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.009011580608785152 norm:2.613744800328277e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:25:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.009003464132547379 norm:2.3656892153667286e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:25:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.008995722979307175 norm:2.178378235839773e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:26:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.008991214446723461 norm:2.0050156308570877e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:27:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.008985892869532108 norm:1.86886918527307e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:28:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.008981258608400822 norm:1.7841553926700726e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:29:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.008978600613772869 norm:1.7064157873392105e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:30:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.008973847143352032 norm:1.641300150367897e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:30:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.008973593823611736 norm:1.5915164112811908e-05 max memory_allocated 29230.490234375 
[2025-02-18 11:31:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 11:31:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.011234971694648266 norm:0.00047665383317507803 max memory_allocated 29230.677734375 
[2025-02-18 11:32:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.010562708601355553 norm:0.00015791822806932032 max memory_allocated 29230.677734375 
[2025-02-18 11:33:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.010401124134659767 norm:9.737877553561702e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:34:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.010333343409001827 norm:7.518813072238117e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:35:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.010287287645041943 norm:5.7926459703594446e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:36:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.010254931636154652 norm:4.5359975047176704e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:36:54 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.010229013860225677 norm:3.632661173469387e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:37:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.010212674736976624 norm:2.9731525501119904e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:38:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.010199010372161865 norm:2.5728806576807983e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:39:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.010190058499574661 norm:2.282950299559161e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:40:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.010190099477767944 norm:2.0443942048586905e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:41:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.010186922736465931 norm:1.878901821328327e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:41:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.010180536657571793 norm:1.7894753909786232e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:42:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.01017358060926199 norm:1.722646084090229e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:43:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.010169945657253265 norm:1.6533986126887612e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:44:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.0101693756878376 norm:1.6050205886131153e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:45:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.010167194530367851 norm:1.572963265061844e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:45:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.010164504870772362 norm:1.547446481708903e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:46:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.010163613595068455 norm:1.5294695913325995e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:47:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.010161873884499073 norm:1.5106775208550971e-05 max memory_allocated 29230.677734375 
[2025-02-18 11:47:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 11:48:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.012635650113224983 norm:0.0004814903368242085 max memory_allocated 29230.865234375 
[2025-02-18 11:49:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.011903759092092514 norm:0.00016542253433726728 max memory_allocated 29230.865234375 
[2025-02-18 11:50:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.011733457446098328 norm:0.00011363312660250813 max memory_allocated 29230.865234375 
[2025-02-18 11:51:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.011654194444417953 norm:9.060540469363332e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:51:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.01159772090613842 norm:7.266578904818743e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:52:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.011548434384167194 norm:5.6063930969685316e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:53:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.011514337733387947 norm:4.482689837459475e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:54:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.011492040008306503 norm:3.8469835999421775e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:55:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.011472832411527634 norm:3.3688080293359235e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:56:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.011461036279797554 norm:2.934712938440498e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:56:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.011453057639300823 norm:2.579953616077546e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:57:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.011449011042714119 norm:2.251800469821319e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:58:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.011445334181189537 norm:2.021682848862838e-05 max memory_allocated 29230.865234375 
[2025-02-18 11:59:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.011439302936196327 norm:1.8619242837303318e-05 max memory_allocated 29230.865234375 
[2025-02-18 12:00:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.011434483341872692 norm:1.70774419530062e-05 max memory_allocated 29230.865234375 
[2025-02-18 12:01:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.011430483311414719 norm:1.5845196685404517e-05 max memory_allocated 29230.865234375 
[2025-02-18 12:01:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.0114286495372653 norm:1.4933624697732739e-05 max memory_allocated 29230.865234375 
[2025-02-18 12:02:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.011427771300077438 norm:1.4231627574190497e-05 max memory_allocated 29230.865234375 
[2025-02-18 12:03:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.011424902826547623 norm:1.3716866305912845e-05 max memory_allocated 29230.865234375 
[2025-02-18 12:04:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.011423682793974876 norm:1.3363109246711247e-05 max memory_allocated 29230.865234375 
[2025-02-18 12:04:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 12:05:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.013333232142031193 norm:0.00021284035756252706 max memory_allocated 29231.052734375 
[2025-02-18 12:06:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.013037781231105328 norm:0.00010424036008771509 max memory_allocated 29231.052734375 
[2025-02-18 12:07:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.01292876061052084 norm:7.199935498647392e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:07:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.012858257628977299 norm:5.271996997180395e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:08:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.012809599749743938 norm:4.095116310054436e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:09:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.01277774479240179 norm:3.327461308799684e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:10:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.012754352763295174 norm:2.8070699045201764e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:11:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.012736789882183075 norm:2.392072565271519e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:12:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.012722917832434177 norm:2.0690409655799158e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:12:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.012713944539427757 norm:1.8192920833826065e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:13:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.012708285823464394 norm:1.608410821063444e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:14:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.012705469503998756 norm:1.440687628928572e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:15:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.012701847590506077 norm:1.3302448678587098e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:16:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.012698827311396599 norm:1.2505988706834614e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:16:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.012697728350758553 norm:1.1953714420087636e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:17:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.012696526944637299 norm:1.1705975339282304e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:18:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.012695932760834694 norm:1.1442447430454195e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:19:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.012694109231233597 norm:1.1267652553215157e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:20:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.012692708522081375 norm:1.1181286026840098e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:21:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.012691473588347435 norm:1.1146707038278691e-05 max memory_allocated 29231.052734375 
[2025-02-18 12:21:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 12:22:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.014863969758152962 norm:0.00023961762781254947 max memory_allocated 29231.240234375 
[2025-02-18 12:22:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.014552447013556957 norm:0.00011349339911248535 max memory_allocated 29231.240234375 
[2025-02-18 12:23:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.014436333440244198 norm:7.932737935334444e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:24:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.014364930801093578 norm:5.973449515295215e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:25:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.014315130189061165 norm:4.699464625446126e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:26:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.014278383925557137 norm:3.8524762203451246e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:27:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.014253459870815277 norm:3.3037125831469893e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:27:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.014237450435757637 norm:2.869445779651869e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:28:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.014224085956811905 norm:2.5406989152543247e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:29:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.014215239323675632 norm:2.2526937755174004e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:30:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.014205686748027802 norm:2.026102265517693e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:31:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.014198435470461845 norm:1.8825798179022968e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:32:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.014190858229994774 norm:1.7662052414380014e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:32:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.014183628372848034 norm:1.6572292224736884e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:33:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.014175448566675186 norm:1.6062836948549375e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:34:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.014171539805829525 norm:1.5175439330050722e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:35:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.014167852699756622 norm:1.4794124581385404e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:36:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.014163866639137268 norm:1.4254317648010328e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:36:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.014158358797430992 norm:1.4046734577277675e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:37:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.014153863303363323 norm:1.3894761650590226e-05 max memory_allocated 29231.240234375 
[2025-02-18 12:38:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 12:38:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.017420034855604172 norm:0.0009856362594291568 max memory_allocated 29231.427734375 
[2025-02-18 12:39:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.01622939109802246 norm:0.0002706000814214349 max memory_allocated 29231.427734375 
[2025-02-18 12:40:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.015969259664416313 norm:0.00014944042777642608 max memory_allocated 29231.427734375 
[2025-02-18 12:41:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.015889223664999008 norm:0.0001214129151776433 max memory_allocated 29231.427734375 
[2025-02-18 12:42:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.01584932766854763 norm:0.00010603419650578871 max memory_allocated 29231.427734375 
[2025-02-18 12:43:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.01580921746790409 norm:8.66396221681498e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:43:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.01577097363770008 norm:6.918763392604887e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:44:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.015737324953079224 norm:5.707893433282152e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:45:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.015718163922429085 norm:4.8235320718958974e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:46:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.015704063698649406 norm:4.2520241549937055e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:47:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.01569056697189808 norm:3.834802919300273e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:47:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.01568187028169632 norm:3.525983265717514e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:48:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.015672896057367325 norm:3.253824252169579e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:49:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.015668779611587524 norm:2.981167745019775e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:50:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.01566130854189396 norm:2.758750088105444e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:51:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.01565813645720482 norm:2.5669505703262985e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:52:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.015654737129807472 norm:2.3455435439245775e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:52:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.01564980298280716 norm:2.1810910766362213e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:53:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.01564658433198929 norm:2.0530935216811486e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:54:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.01564423367381096 norm:1.935674845299218e-05 max memory_allocated 29231.427734375 
[2025-02-18 12:54:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 12:55:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.017795268446207047 norm:0.0002467425656504929 max memory_allocated 29231.615234375 
[2025-02-18 12:56:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.01749938353896141 norm:0.00011585192260099575 max memory_allocated 29231.615234375 
[2025-02-18 12:57:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.017404286190867424 norm:8.167896885424852e-05 max memory_allocated 29231.615234375 
[2025-02-18 12:58:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.01734124682843685 norm:6.0138841945445165e-05 max memory_allocated 29231.615234375 
[2025-02-18 12:58:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.01729625277221203 norm:4.5845434215152636e-05 max memory_allocated 29231.615234375 
[2025-02-18 12:59:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.017263807356357574 norm:3.728082447196357e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:00:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.017239976674318314 norm:3.1494251743424684e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:01:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.017223456874489784 norm:2.6731451725936495e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:02:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.017213614657521248 norm:2.272637175337877e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:03:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.017205802723765373 norm:1.929712379933335e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:03:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.01720418967306614 norm:1.6373749531339854e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:04:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.01720091700553894 norm:1.4322955394163728e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:05:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.017201248556375504 norm:1.2967491784365848e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:06:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.01719864457845688 norm:1.2168043213023338e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:07:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.017196375876665115 norm:1.1785925380536355e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:07:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.017193488776683807 norm:1.1488274139992427e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:08:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.017192166298627853 norm:1.1165981049998663e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:09:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.017191851511597633 norm:1.0994360309268814e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:10:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.017187833786010742 norm:1.094221261155326e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:11:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.01718645542860031 norm:1.0823016054928303e-05 max memory_allocated 29231.615234375 
[2025-02-18 13:11:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 13:12:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.019572414457798004 norm:0.00022719208209309727 max memory_allocated 29231.802734375 
[2025-02-18 13:13:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.019317427650094032 norm:0.0001263946178369224 max memory_allocated 29231.802734375 
[2025-02-18 13:14:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.019200166687369347 norm:8.667416113894433e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:14:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.019127588719129562 norm:6.533544365083799e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:15:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.019076788797974586 norm:5.134794628247619e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:16:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.01904691383242607 norm:4.210516999592073e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:17:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.01902158558368683 norm:3.550035762600601e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:18:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.019001945853233337 norm:3.052013198612258e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:18:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.01899014413356781 norm:2.6501120373723097e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:19:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.018977846950292587 norm:2.400832272542175e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:20:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.018966251984238625 norm:2.1658102923538536e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:21:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.01895756646990776 norm:2.001448410737794e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:22:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.018949417397379875 norm:1.880993295344524e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:23:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.018944870680570602 norm:1.750303454173263e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:23:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.018939506262540817 norm:1.6111242075567134e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:24:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.018935730680823326 norm:1.520455589343328e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:25:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.018929461017251015 norm:1.4835261026746593e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:26:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.01892801746726036 norm:1.4586374163627625e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:27:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.018926145508885384 norm:1.3727643818128854e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:27:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.018923480063676834 norm:1.3636547919304576e-05 max memory_allocated 29231.802734375 
[2025-02-18 13:28:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 13:29:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.021288525313138962 norm:0.0002261247136630118 max memory_allocated 29231.990234375 
[2025-02-18 13:29:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.021021675318479538 norm:0.00011328300752211362 max memory_allocated 29231.990234375 
[2025-02-18 13:30:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.020920302718877792 norm:7.771415403112769e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:31:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.020856717601418495 norm:5.671495455317199e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:32:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.020814264193177223 norm:4.437367897480726e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:33:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.020786035805940628 norm:3.5970824683317915e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:34:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.020766422152519226 norm:2.95561840175651e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:34:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.020750684663653374 norm:2.4668061087140813e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:35:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.0207461379468441 norm:1.968170727195684e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:36:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.020741697400808334 norm:1.5872396033955738e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:37:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.020740142092108727 norm:1.3467049939208664e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:38:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.02073669619858265 norm:1.2001492905255873e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:38:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.02073223888874054 norm:1.1290063412161544e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:39:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.020727746188640594 norm:1.083893766917754e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:40:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.020723633468151093 norm:1.0563607247604523e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:41:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.020722927525639534 norm:1.0326592928322498e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:42:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.020721446722745895 norm:1.0202639714407269e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:43:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.02072235196828842 norm:1.0087892405863386e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:43:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.02071969583630562 norm:1.0158699296880513e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:44:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.02071860432624817 norm:1.0031328201876022e-05 max memory_allocated 29231.990234375 
[2025-02-18 13:44:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-18 13:45:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.023138204589486122 norm:0.00022883998462930322 max memory_allocated 29232.177734375 
[2025-02-18 13:46:37 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.022904664278030396 norm:0.00012377290113363415 max memory_allocated 29232.177734375 
[2025-02-18 13:47:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.02278953604400158 norm:8.251206600107253e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:48:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.022721048444509506 norm:6.1017963162157685e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:49:04 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.022674908861517906 norm:4.8388479626737535e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:49:53 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.02263832464814186 norm:3.922973701264709e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:50:42 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.02261374145746231 norm:3.227201523259282e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:51:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.02259647101163864 norm:2.697473428270314e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:52:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.022582881152629852 norm:2.2663414711132646e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:53:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.022571422159671783 norm:1.9360486476216465e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:53:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.02256196364760399 norm:1.6659450920997187e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:54:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.022561268880963326 norm:1.378899014525814e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:55:37 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.02256223000586033 norm:1.2082669854862615e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:56:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.022560184821486473 norm:1.1169935532961972e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:57:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.02255670353770256 norm:1.071936458174605e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:58:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.022557370364665985 norm:1.0483634468982928e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:58:54 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.022554395720362663 norm:1.0293037121300586e-05 max memory_allocated 29232.177734375 
[2025-02-18 13:59:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.02255217544734478 norm:1.0143098734261002e-05 max memory_allocated 29232.177734375 
[2025-02-18 14:00:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.0225506778806448 norm:1.0079561434395146e-05 max memory_allocated 29232.177734375 
[2025-02-18 14:01:22 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.022549787536263466 norm:1.012378834275296e-05 max memory_allocated 29232.177734375 
[2025-02-18 14:01:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-18 14:02:30 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.025374561548233032 norm:0.00022253574570640922 max memory_allocated 29232.365234375 
[2025-02-18 14:03:19 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.025128444656729698 norm:0.00012127031368436292 max memory_allocated 29232.365234375 
[2025-02-18 14:04:08 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.025002840906381607 norm:8.028822776395828e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:04:57 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.02493101730942726 norm:5.922435593674891e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:05:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.02488541603088379 norm:4.653532960219309e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:06:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.024858977645635605 norm:3.760524850804359e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:07:25 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.02484055981040001 norm:3.1038958695717156e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:08:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.02482544630765915 norm:2.6758856620290317e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:09:03 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.024806687608361244 norm:2.38740994973341e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:09:53 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.024797970429062843 norm:2.1160389223950915e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:10:42 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.02478604204952717 norm:1.9323488231748343e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:11:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.024775495752692223 norm:1.787359178706538e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:12:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.02476721815764904 norm:1.6638283341308124e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:13:10 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.024762803688645363 norm:1.5711580999777652e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:13:59 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.02475789561867714 norm:1.5019133570604026e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:14:48 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.024757035076618195 norm:1.4142349755275063e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:15:37 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.024753008037805557 norm:1.354491450911155e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:16:27 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.02474956586956978 norm:1.3358713658817578e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:17:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.024747854098677635 norm:1.2951793905813247e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:18:05 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.024746637791395187 norm:1.2523948498710524e-05 max memory_allocated 29232.365234375 
[2025-02-18 14:18:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-18 14:19:14 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.028171544894576073 norm:0.00026516561047174037 max memory_allocated 29232.552734375 
[2025-02-18 14:20:03 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.02782246470451355 norm:0.00016200894606299698 max memory_allocated 29232.552734375 
[2025-02-18 14:20:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.02763606235384941 norm:0.00011307316890452057 max memory_allocated 29232.552734375 
[2025-02-18 14:21:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.02752416580915451 norm:8.522533607902005e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:22:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.02744881436228752 norm:6.727066647727042e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:23:21 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.027395276352763176 norm:5.464690548251383e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:24:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.027358636260032654 norm:4.478947084862739e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:24:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.027333252131938934 norm:3.755676152650267e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:25:49 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.02731211483478546 norm:3.234696123399772e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:26:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.02729519084095955 norm:2.82281544059515e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:27:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.027280500158667564 norm:2.518308065191377e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:28:16 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.02726546861231327 norm:2.2769972929381765e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:29:06 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.02725912258028984 norm:2.06732456717873e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:29:55 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.02724575251340866 norm:1.9237410015193745e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:30:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.02723812870681286 norm:1.807215448934585e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:31:34 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.027231864631175995 norm:1.7063604900613427e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:32:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.027226530015468597 norm:1.627323217689991e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:33:12 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.027223296463489532 norm:1.5533529222011566e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:34:02 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.027218125760555267 norm:1.5062255442899186e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:34:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.027216915041208267 norm:1.4574787201127037e-05 max memory_allocated 29232.552734375 
[2025-02-18 14:35:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-18 14:35:59 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.03107261098921299 norm:0.0002556132385507226 max memory_allocated 29232.740234375 
[2025-02-18 14:36:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.0306280255317688 norm:0.00015174203144852072 max memory_allocated 29232.740234375 
[2025-02-18 14:37:38 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.030394481495022774 norm:0.00010907994874287397 max memory_allocated 29232.740234375 
[2025-02-18 14:38:27 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.03024332784116268 norm:8.445743151241913e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:39:16 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.030137185007333755 norm:6.895595288369805e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:40:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.030062377452850342 norm:5.810643051518127e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:40:55 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.030003618448972702 norm:4.9195285100722685e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:41:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.029958022758364677 norm:4.2976720578735694e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:42:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.02991689182817936 norm:3.737012229976244e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:43:23 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.029887648299336433 norm:3.279934753663838e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:44:12 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.029868070036172867 norm:2.9401442589005455e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:45:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.029853960499167442 norm:2.6426972908666357e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:45:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.029841002076864243 norm:2.4219949409598485e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:46:40 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.02983112260699272 norm:2.2697990061715245e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:47:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.029820118099451065 norm:2.1322155589587055e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:48:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.02981407195329666 norm:2.0257855794625357e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:49:08 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.0298056248575449 norm:1.94357089640107e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:49:57 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.02980111911892891 norm:1.873365545179695e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:50:47 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.02980009652674198 norm:1.8003056538873352e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:51:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.029798313975334167 norm:1.750140290823765e-05 max memory_allocated 29232.740234375 
[2025-02-18 14:51:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-18 14:52:44 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.033952441066503525 norm:0.0001851495762821287 max memory_allocated 29232.927734375 
[2025-02-18 14:53:33 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.033538948744535446 norm:0.00011454000195953995 max memory_allocated 29232.927734375 
[2025-02-18 14:54:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.03329400345683098 norm:8.066555892582983e-05 max memory_allocated 29232.927734375 
[2025-02-18 14:55:12 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.03314550593495369 norm:6.179139018058777e-05 max memory_allocated 29232.927734375 
[2025-02-18 14:56:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.03303481265902519 norm:4.919800267089158e-05 max memory_allocated 29232.927734375 
[2025-02-18 14:56:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.03295648843050003 norm:4.043642184115015e-05 max memory_allocated 29232.927734375 
[2025-02-18 14:57:40 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.03290519490838051 norm:3.309757448732853e-05 max memory_allocated 29232.927734375 
[2025-02-18 14:58:29 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.03286684677004814 norm:2.7782380129792728e-05 max memory_allocated 29232.927734375 
[2025-02-18 14:59:18 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.03283781185746193 norm:2.4119086447171867e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:00:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.03281112760305405 norm:2.1219659174676053e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:00:56 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.03279421478509903 norm:1.9464245269773528e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:01:45 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.032782815396785736 norm:1.8013764929492027e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:02:34 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.03276985138654709 norm:1.7206168195116334e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:03:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.032759398221969604 norm:1.6967778719845228e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:04:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.03275366127490997 norm:1.586973485245835e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:05:02 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.032747555524110794 norm:1.545320264995098e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:05:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.03274139389395714 norm:1.54322824528208e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:06:40 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.03273290395736694 norm:1.492985484219389e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:07:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.03273112699389458 norm:1.486634664615849e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:08:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.03272358328104019 norm:1.4723449567100033e-05 max memory_allocated 29232.927734375 
[2025-02-18 15:08:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-18 15:09:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.03924998641014099 norm:0.0002749189152382314 max memory_allocated 29233.115234375 
[2025-02-18 15:10:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.03839334845542908 norm:0.00016629145829938352 max memory_allocated 29233.115234375 
[2025-02-18 15:11:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.03791089728474617 norm:0.0001207977402373217 max memory_allocated 29233.115234375 
[2025-02-18 15:11:54 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.037602946162223816 norm:8.834210893837735e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:12:43 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.037397272884845734 norm:7.13925837771967e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:13:32 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.03725779056549072 norm:5.876692375750281e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:14:21 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.03715190291404724 norm:4.9185277021024376e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:15:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.037071190774440765 norm:4.189707397017628e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:16:00 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.037012308835983276 norm:3.6967041523894295e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:16:49 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.03696557506918907 norm:3.278829899500124e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:17:38 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.036933548748493195 norm:3.053321415791288e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:18:27 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.03690139576792717 norm:2.8503836801974103e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:19:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.036878760904073715 norm:2.739556657616049e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:20:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.03685852885246277 norm:2.611435229482595e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:20:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.036844633519649506 norm:2.5897536033880897e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:21:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.036832306534051895 norm:2.5462679332122207e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:22:33 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.03682269528508186 norm:2.5153161914204247e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:23:22 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.036812569946050644 norm:2.4734623366384767e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:24:11 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.03680221736431122 norm:2.447546285111457e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:25:00 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.03680166229605675 norm:2.4429331460851245e-05 max memory_allocated 29233.115234375 
[2025-02-18 15:25:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-18 15:26:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.07409409433603287 norm:0.006586810573935509 max memory_allocated 29233.302734375 
[2025-02-18 15:26:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.05607180297374725 norm:0.0015775450738146901 max memory_allocated 29233.302734375 
[2025-02-18 15:27:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.05203204229474068 norm:0.0010217567905783653 max memory_allocated 29233.302734375 
[2025-02-18 15:28:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.05071333795785904 norm:0.0008034435450099409 max memory_allocated 29233.302734375 
[2025-02-18 15:29:26 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.04995270073413849 norm:0.0007257857359945774 max memory_allocated 29233.302734375 
[2025-02-18 15:30:14 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.04940224066376686 norm:0.0006474551628343761 max memory_allocated 29233.302734375 
[2025-02-18 15:31:04 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.048892877995967865 norm:0.0005423012771643698 max memory_allocated 29233.302734375 
[2025-02-18 15:31:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.04853995889425278 norm:0.0004560261149890721 max memory_allocated 29233.302734375 
[2025-02-18 15:32:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.04838162288069725 norm:0.00044476616312749684 max memory_allocated 29233.302734375 
[2025-02-18 15:33:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.04821810871362686 norm:0.0003980553010478616 max memory_allocated 29233.302734375 
[2025-02-18 15:34:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.04812834411859512 norm:0.00040650952723808587 max memory_allocated 29233.302734375 
[2025-02-18 15:35:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.04804646968841553 norm:0.00043391319923102856 max memory_allocated 29233.302734375 
[2025-02-18 15:35:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.04789223149418831 norm:0.0004111484158784151 max memory_allocated 29233.302734375 
[2025-02-18 15:36:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.04782549664378166 norm:0.0003866524784825742 max memory_allocated 29233.302734375 
[2025-02-18 15:37:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.04775693640112877 norm:0.00038171777850948274 max memory_allocated 29233.302734375 
[2025-02-18 15:38:26 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.0477069653570652 norm:0.00036020996049046516 max memory_allocated 29233.302734375 
[2025-02-18 15:39:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.04766751080751419 norm:0.000365706451702863 max memory_allocated 29233.302734375 
[2025-02-18 15:40:04 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.04756603017449379 norm:0.0003481766034383327 max memory_allocated 29233.302734375 
[2025-02-18 15:40:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.04746892675757408 norm:0.00033702675136737525 max memory_allocated 29233.302734375 
[2025-02-18 15:41:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.047464728355407715 norm:0.00034203779068775475 max memory_allocated 29233.302734375 
[2025-02-18 15:41:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-18 15:42:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.09842006862163544 norm:0.002865099348127842 max memory_allocated 29233.490234375 
[2025-02-18 15:43:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.09399941563606262 norm:0.0018681662622839212 max memory_allocated 29233.490234375 
[2025-02-18 15:44:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.0911240205168724 norm:0.001284456462599337 max memory_allocated 29233.490234375 
[2025-02-18 15:45:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.0889991968870163 norm:0.0009446603944525123 max memory_allocated 29233.490234375 
[2025-02-18 15:46:07 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.08748052269220352 norm:0.000714707188308239 max memory_allocated 29233.490234375 
[2025-02-18 15:46:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.08629751950502396 norm:0.0005560131976380944 max memory_allocated 29233.490234375 
[2025-02-18 15:47:46 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.08541902899742126 norm:0.0004505599499680102 max memory_allocated 29233.490234375 
[2025-02-18 15:48:35 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.0848027765750885 norm:0.0003788790199905634 max memory_allocated 29233.490234375 
[2025-02-18 15:49:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.08434946089982986 norm:0.0003343028365634382 max memory_allocated 29233.490234375 
[2025-02-18 15:50:13 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.08390476554632187 norm:0.00030673356377519667 max memory_allocated 29233.490234375 
[2025-02-18 15:51:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.08357278257608414 norm:0.0002891774638555944 max memory_allocated 29233.490234375 
[2025-02-18 15:51:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.08333200961351395 norm:0.00027778526418842375 max memory_allocated 29233.490234375 
[2025-02-18 15:52:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.0831083431839943 norm:0.0002734794979915023 max memory_allocated 29233.490234375 
[2025-02-18 15:53:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.08288442343473434 norm:0.00026414799503982067 max memory_allocated 29233.490234375 
[2025-02-18 15:54:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.08270450681447983 norm:0.00025948823895305395 max memory_allocated 29233.490234375 
[2025-02-18 15:55:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.08262770622968674 norm:0.00026683800388127565 max memory_allocated 29233.490234375 
[2025-02-18 15:55:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.08259320259094238 norm:0.00026931081083603203 max memory_allocated 29233.490234375 
[2025-02-18 15:56:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.08257020264863968 norm:0.0002575127291493118 max memory_allocated 29233.490234375 
[2025-02-18 15:57:36 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.0825805515050888 norm:0.0002544193703215569 max memory_allocated 29233.490234375 
[2025-02-18 15:58:25 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.08254934847354889 norm:0.0002512720529921353 max memory_allocated 29233.490234375 
[2025-02-18 15:58:40 root] (main_calibration.py 365): INFO 40163.98974990845
[2025-02-18 16:00:00 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-18 16:01:58 root] (main_calibration.py 158): INFO wikitext2 : 5.1070756912231445
[2025-02-18 16:01:58 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-18 16:04:59 root] (main_calibration.py 158): INFO c4 : 6.630303382873535
[2025-02-18 17:59:18 root] (main_calibration.py 169): INFO {'wikitext2': 5.1070756912231445, 'c4': 6.630303382873535, 'results': {'arc_challenge': {'acc': 0.4334470989761092, 'acc_stderr': 0.014481376224558898, 'acc_norm': 0.4402730375426621, 'acc_norm_stderr': 0.014506769524804243}, 'boolq': {'acc': 0.6825688073394496, 'acc_stderr': 0.00814124002260939}, 'arc_easy': {'acc': 0.7436868686868687, 'acc_stderr': 0.008958775997918337, 'acc_norm': 0.5951178451178452, 'acc_norm_stderr': 0.0100724239603957}, 'hellaswag': {'acc': 0.589523999203346, 'acc_stderr': 0.004909148239488276, 'acc_norm': 0.7618004381597291, 'acc_norm_stderr': 0.004251112563789765}, 'piqa': {'acc': 0.7861806311207835, 'acc_stderr': 0.009565994206915602, 'acc_norm': 0.7867247007616975, 'acc_norm_stderr': 0.009557121225861344}, 'winogrande': {'acc': 0.7111286503551697, 'acc_stderr': 0.01273824127101845}}, 'versions': {'arc_challenge': 0, 'boolq': 1, 'arc_easy': 0, 'hellaswag': 0, 'piqa': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
