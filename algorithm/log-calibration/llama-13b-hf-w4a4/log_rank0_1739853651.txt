[2025-02-18 04:40:51 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration/llama-13b-hf-w4a4', save_dir='./log-calibration/quant/llama-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-18 04:49:16 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 04:49:16 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-18 04:49:16 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 04:49:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 04:50:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.17120948433876038 norm:0.037139054387807846 max memory_allocated 29226.177734375 
[2025-02-18 04:50:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.13870495557785034 norm:0.03961611166596413 max memory_allocated 29226.177734375 
[2025-02-18 04:51:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.12885437905788422 norm:0.054760776460170746 max memory_allocated 29226.177734375 
[2025-02-18 04:52:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.11649623513221741 norm:0.04743838682770729 max memory_allocated 29226.177734375 
[2025-02-18 04:53:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.11896053701639175 norm:0.05304386466741562 max memory_allocated 29226.177734375 
[2025-02-18 04:54:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.09902387112379074 norm:0.023719722405076027 max memory_allocated 29226.177734375 
[2025-02-18 04:54:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.10371744632720947 norm:0.032009877264499664 max memory_allocated 29226.177734375 
[2025-02-18 04:55:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.1116870865225792 norm:0.04150323197245598 max memory_allocated 29226.177734375 
[2025-02-18 04:56:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.11575616896152496 norm:0.048836495727300644 max memory_allocated 29226.177734375 
[2025-02-18 04:57:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.10636875033378601 norm:0.039303697645664215 max memory_allocated 29226.177734375 
[2025-02-18 04:58:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.09498874843120575 norm:0.02639274299144745 max memory_allocated 29226.177734375 
[2025-02-18 04:58:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.09441269189119339 norm:0.02780676819384098 max memory_allocated 29226.177734375 
[2025-02-18 04:59:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.09286794066429138 norm:0.023846641182899475 max memory_allocated 29226.177734375 
[2025-02-18 05:00:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.09191309660673141 norm:0.024986546486616135 max memory_allocated 29226.177734375 
[2025-02-18 05:01:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.09583333134651184 norm:0.02699725516140461 max memory_allocated 29226.177734375 
[2025-02-18 05:02:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.10123597830533981 norm:0.03331295773386955 max memory_allocated 29226.177734375 
[2025-02-18 05:02:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.10271008312702179 norm:0.03365523740649223 max memory_allocated 29226.177734375 
[2025-02-18 05:03:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.1026526391506195 norm:0.033842116594314575 max memory_allocated 29226.177734375 
[2025-02-18 05:04:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.10115798562765121 norm:0.03191474825143814 max memory_allocated 29226.177734375 
[2025-02-18 05:05:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.10097742080688477 norm:0.032949626445770264 max memory_allocated 29226.177734375 
[2025-02-18 05:05:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 05:06:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.3448016941547394 norm:0.04723697900772095 max memory_allocated 29226.365234375 
[2025-02-18 05:07:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.3034960627555847 norm:0.035075996071100235 max memory_allocated 29226.365234375 
[2025-02-18 05:08:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.31372565031051636 norm:0.041758906096220016 max memory_allocated 29226.365234375 
[2025-02-18 05:08:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.29641780257225037 norm:0.04098820313811302 max memory_allocated 29226.365234375 
[2025-02-18 05:09:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.25976991653442383 norm:0.02797377109527588 max memory_allocated 29226.365234375 
[2025-02-18 05:10:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.25436338782310486 norm:0.024182777851819992 max memory_allocated 29226.365234375 
[2025-02-18 05:11:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.2448301613330841 norm:0.023365922272205353 max memory_allocated 29226.365234375 
[2025-02-18 05:12:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.24029874801635742 norm:0.022550484165549278 max memory_allocated 29226.365234375 
[2025-02-18 05:12:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.24736261367797852 norm:0.024399222806096077 max memory_allocated 29226.365234375 
[2025-02-18 05:13:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.24185413122177124 norm:0.023303434252738953 max memory_allocated 29226.365234375 
[2025-02-18 05:14:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.2319599688053131 norm:0.01983478106558323 max memory_allocated 29226.365234375 
[2025-02-18 05:15:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.22923599183559418 norm:0.019535481929779053 max memory_allocated 29226.365234375 
[2025-02-18 05:16:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.2298998385667801 norm:0.02001974545419216 max memory_allocated 29226.365234375 
[2025-02-18 05:16:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.22866353392601013 norm:0.020063668489456177 max memory_allocated 29226.365234375 
[2025-02-18 05:17:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.2229117602109909 norm:0.018080947920680046 max memory_allocated 29226.365234375 
[2025-02-18 05:18:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.22963593900203705 norm:0.020382875576615334 max memory_allocated 29226.365234375 
[2025-02-18 05:19:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.23337893187999725 norm:0.02081093192100525 max memory_allocated 29226.365234375 
[2025-02-18 05:20:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.2360798716545105 norm:0.021228190511465073 max memory_allocated 29226.365234375 
[2025-02-18 05:20:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.23561587929725647 norm:0.02115762233734131 max memory_allocated 29226.365234375 
[2025-02-18 05:21:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.22559888660907745 norm:0.018832899630069733 max memory_allocated 29226.365234375 
[2025-02-18 05:22:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 05:22:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.4810073971748352 norm:0.04158193990588188 max memory_allocated 29226.552734375 
[2025-02-18 05:23:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.4371650815010071 norm:0.023697610944509506 max memory_allocated 29226.552734375 
[2025-02-18 05:24:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.4131382405757904 norm:0.018758708611130714 max memory_allocated 29226.552734375 
[2025-02-18 05:25:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.40190351009368896 norm:0.016388557851314545 max memory_allocated 29226.552734375 
[2025-02-18 05:26:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.3889122009277344 norm:0.013553861528635025 max memory_allocated 29226.552734375 
[2025-02-18 05:26:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.3847055435180664 norm:0.013549166731536388 max memory_allocated 29226.552734375 
[2025-02-18 05:27:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.3816165328025818 norm:0.01381455734372139 max memory_allocated 29226.552734375 
[2025-02-18 05:28:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.378887414932251 norm:0.014632916077971458 max memory_allocated 29226.552734375 
[2025-02-18 05:29:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.3783430755138397 norm:0.013569511473178864 max memory_allocated 29226.552734375 
[2025-02-18 05:30:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.37735506892204285 norm:0.013618464581668377 max memory_allocated 29226.552734375 
[2025-02-18 05:30:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.3769569993019104 norm:0.014409139752388 max memory_allocated 29226.552734375 
[2025-02-18 05:31:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.376558393239975 norm:0.013227427378296852 max memory_allocated 29226.552734375 
[2025-02-18 05:32:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.3773362338542938 norm:0.014070920646190643 max memory_allocated 29226.552734375 
[2025-02-18 05:33:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.37854933738708496 norm:0.014398669824004173 max memory_allocated 29226.552734375 
[2025-02-18 05:34:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.376982182264328 norm:0.014198089018464088 max memory_allocated 29226.552734375 
[2025-02-18 05:35:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.37636321783065796 norm:0.013507384806871414 max memory_allocated 29226.552734375 
[2025-02-18 05:35:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.3754793405532837 norm:0.01492225006222725 max memory_allocated 29226.552734375 
[2025-02-18 05:36:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.37527310848236084 norm:0.013886535540223122 max memory_allocated 29226.552734375 
[2025-02-18 05:37:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.3743602931499481 norm:0.013834921643137932 max memory_allocated 29226.552734375 
[2025-02-18 05:38:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.3754725754261017 norm:0.014274870976805687 max memory_allocated 29226.552734375 
[2025-02-18 05:38:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 05:39:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.535527229309082 norm:0.05559030547738075 max memory_allocated 29226.740234375 
[2025-02-18 05:40:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.5110102891921997 norm:0.02856041118502617 max memory_allocated 29226.740234375 
[2025-02-18 05:40:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.501314640045166 norm:0.01913733221590519 max memory_allocated 29226.740234375 
[2025-02-18 05:41:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.4945628046989441 norm:0.013708651065826416 max memory_allocated 29226.740234375 
[2025-02-18 05:42:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.48764002323150635 norm:0.010636288672685623 max memory_allocated 29226.740234375 
[2025-02-18 05:43:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.4851321280002594 norm:0.009179436601698399 max memory_allocated 29226.740234375 
[2025-02-18 05:44:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.4836054742336273 norm:0.008076571859419346 max memory_allocated 29226.740234375 
[2025-02-18 05:45:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.47650429606437683 norm:0.006644099485129118 max memory_allocated 29226.740234375 
[2025-02-18 05:45:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.4694982171058655 norm:0.005936050321906805 max memory_allocated 29226.740234375 
[2025-02-18 05:46:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.46639305353164673 norm:0.005577596835792065 max memory_allocated 29226.740234375 
[2025-02-18 05:47:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.4639509618282318 norm:0.005235168617218733 max memory_allocated 29226.740234375 
[2025-02-18 05:48:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.4632771909236908 norm:0.00521835358813405 max memory_allocated 29226.740234375 
[2025-02-18 05:49:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.4621727168560028 norm:0.005098073277622461 max memory_allocated 29226.740234375 
[2025-02-18 05:49:51 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.4622546136379242 norm:0.004722269251942635 max memory_allocated 29226.740234375 
[2025-02-18 05:50:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.46254074573516846 norm:0.00473899208009243 max memory_allocated 29226.740234375 
[2025-02-18 05:51:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.46202343702316284 norm:0.004790318198502064 max memory_allocated 29226.740234375 
[2025-02-18 05:52:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.46177050471305847 norm:0.004662604071199894 max memory_allocated 29226.740234375 
[2025-02-18 05:53:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.46207860112190247 norm:0.00452549010515213 max memory_allocated 29226.740234375 
[2025-02-18 05:53:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.46137019991874695 norm:0.004577967803925276 max memory_allocated 29226.740234375 
[2025-02-18 05:54:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.46127793192863464 norm:0.004443011246621609 max memory_allocated 29226.740234375 
[2025-02-18 05:54:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 05:55:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.6140996813774109 norm:0.05029122531414032 max memory_allocated 29226.927734375 
[2025-02-18 05:56:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.5898730754852295 norm:0.018650326877832413 max memory_allocated 29226.927734375 
[2025-02-18 05:57:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.5806781649589539 norm:0.00969067681580782 max memory_allocated 29226.927734375 
[2025-02-18 05:58:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.577003002166748 norm:0.007233503274619579 max memory_allocated 29226.927734375 
[2025-02-18 05:59:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.5738487839698792 norm:0.005964571610093117 max memory_allocated 29226.927734375 
[2025-02-18 05:59:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.5715401768684387 norm:0.005376412533223629 max memory_allocated 29226.927734375 
[2025-02-18 06:00:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.5702194571495056 norm:0.0047567617148160934 max memory_allocated 29226.927734375 
[2025-02-18 06:01:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.5697551965713501 norm:0.004619930870831013 max memory_allocated 29226.927734375 
[2025-02-18 06:02:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.5690166354179382 norm:0.004404891282320023 max memory_allocated 29226.927734375 
[2025-02-18 06:03:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.5672463178634644 norm:0.004267075099050999 max memory_allocated 29226.927734375 
[2025-02-18 06:03:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.5662623643875122 norm:0.004131599795073271 max memory_allocated 29226.927734375 
[2025-02-18 06:04:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.5654541850090027 norm:0.004022381734102964 max memory_allocated 29226.927734375 
[2025-02-18 06:05:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.5645427703857422 norm:0.003998199477791786 max memory_allocated 29226.927734375 
[2025-02-18 06:06:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.5637159943580627 norm:0.003949130419641733 max memory_allocated 29226.927734375 
[2025-02-18 06:07:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.5632264614105225 norm:0.0038666732143610716 max memory_allocated 29226.927734375 
[2025-02-18 06:07:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.5629881620407104 norm:0.0039030194748193026 max memory_allocated 29226.927734375 
[2025-02-18 06:08:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.5629281997680664 norm:0.003809613874182105 max memory_allocated 29226.927734375 
[2025-02-18 06:09:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.5631281733512878 norm:0.0038327837828546762 max memory_allocated 29226.927734375 
[2025-02-18 06:10:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.5630465745925903 norm:0.0038781319744884968 max memory_allocated 29226.927734375 
[2025-02-18 06:11:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.5630841851234436 norm:0.0037882723845541477 max memory_allocated 29226.927734375 
[2025-02-18 06:11:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 06:12:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.7430976629257202 norm:0.0778728723526001 max memory_allocated 29227.115234375 
[2025-02-18 06:13:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.7085241079330444 norm:0.032878439873456955 max memory_allocated 29227.115234375 
[2025-02-18 06:13:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.6972454786300659 norm:0.01806146465241909 max memory_allocated 29227.115234375 
[2025-02-18 06:14:43 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.6975836753845215 norm:0.014839071780443192 max memory_allocated 29227.115234375 
[2025-02-18 06:15:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.6912477016448975 norm:0.011161468923091888 max memory_allocated 29227.115234375 
[2025-02-18 06:16:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.6856457591056824 norm:0.009144878946244717 max memory_allocated 29227.115234375 
[2025-02-18 06:17:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.6839759945869446 norm:0.00801634881645441 max memory_allocated 29227.115234375 
[2025-02-18 06:17:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.6859201788902283 norm:0.007973660714924335 max memory_allocated 29227.115234375 
[2025-02-18 06:18:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.6844442486763 norm:0.007940918207168579 max memory_allocated 29227.115234375 
[2025-02-18 06:19:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.6835264563560486 norm:0.007775392383337021 max memory_allocated 29227.115234375 
[2025-02-18 06:20:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.6771435737609863 norm:0.006510908715426922 max memory_allocated 29227.115234375 
[2025-02-18 06:21:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.6755731701850891 norm:0.006209774874150753 max memory_allocated 29227.115234375 
[2025-02-18 06:22:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.6752405166625977 norm:0.005968823097646236 max memory_allocated 29227.115234375 
[2025-02-18 06:22:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.6753854751586914 norm:0.006001731846481562 max memory_allocated 29227.115234375 
[2025-02-18 06:23:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.6756505966186523 norm:0.00596286915242672 max memory_allocated 29227.115234375 
[2025-02-18 06:24:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.6754209399223328 norm:0.005771588999778032 max memory_allocated 29227.115234375 
[2025-02-18 06:25:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.6746053099632263 norm:0.005679837428033352 max memory_allocated 29227.115234375 
[2025-02-18 06:26:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.6736544966697693 norm:0.00564115634188056 max memory_allocated 29227.115234375 
[2025-02-18 06:26:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.6733691692352295 norm:0.005646849051117897 max memory_allocated 29227.115234375 
[2025-02-18 06:27:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.6722668409347534 norm:0.005573306232690811 max memory_allocated 29227.115234375 
[2025-02-18 06:27:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 06:28:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:1.2075453996658325 norm:0.020590417087078094 max memory_allocated 29227.302734375 
[2025-02-18 06:29:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:1.1854020357131958 norm:0.014855621382594109 max memory_allocated 29227.302734375 
[2025-02-18 06:30:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:1.1744505167007446 norm:0.016857776790857315 max memory_allocated 29227.302734375 
[2025-02-18 06:31:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:1.1487293243408203 norm:0.15670864284038544 max memory_allocated 29227.302734375 
[2025-02-18 06:32:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:1.124127745628357 norm:0.2722190022468567 max memory_allocated 29227.302734375 
[2025-02-18 06:32:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:1.1124746799468994 norm:0.2791995108127594 max memory_allocated 29227.302734375 
[2025-02-18 06:33:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:1.1091759204864502 norm:0.21812525391578674 max memory_allocated 29227.302734375 
[2025-02-18 06:34:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:1.1057775020599365 norm:0.19261547923088074 max memory_allocated 29227.302734375 
[2025-02-18 06:35:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:1.103469729423523 norm:0.16298562288284302 max memory_allocated 29227.302734375 
[2025-02-18 06:36:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:1.102095365524292 norm:0.14190158247947693 max memory_allocated 29227.302734375 
[2025-02-18 06:36:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:1.1014317274093628 norm:0.1298293024301529 max memory_allocated 29227.302734375 
[2025-02-18 06:37:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:1.102080225944519 norm:0.14226005971431732 max memory_allocated 29227.302734375 
[2025-02-18 06:38:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:1.1005067825317383 norm:0.1336936354637146 max memory_allocated 29227.302734375 
[2025-02-18 06:39:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:1.0982284545898438 norm:0.12241338193416595 max memory_allocated 29227.302734375 
[2025-02-18 06:40:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:1.097420334815979 norm:0.12504902482032776 max memory_allocated 29227.302734375 
[2025-02-18 06:40:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:1.0962541103363037 norm:0.12594932317733765 max memory_allocated 29227.302734375 
[2025-02-18 06:41:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:1.09683358669281 norm:0.12373974919319153 max memory_allocated 29227.302734375 
[2025-02-18 06:42:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:1.096081256866455 norm:0.12203242629766464 max memory_allocated 29227.302734375 
[2025-02-18 06:43:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:1.0960986614227295 norm:0.12386103719472885 max memory_allocated 29227.302734375 
[2025-02-18 06:44:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:1.097695231437683 norm:0.12751853466033936 max memory_allocated 29227.302734375 
[2025-02-18 06:44:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 06:45:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:1.177956223487854 norm:0.025609154254198074 max memory_allocated 29227.490234375 
[2025-02-18 06:46:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:1.1595113277435303 norm:0.014780828729271889 max memory_allocated 29227.490234375 
[2025-02-18 06:46:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:1.1473170518875122 norm:0.008065625093877316 max memory_allocated 29227.490234375 
[2025-02-18 06:47:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:1.141486406326294 norm:0.0054216571152210236 max memory_allocated 29227.490234375 
[2025-02-18 06:48:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:1.1364320516586304 norm:0.004465154372155666 max memory_allocated 29227.490234375 
[2025-02-18 06:49:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:1.133493423461914 norm:0.0040540010668337345 max memory_allocated 29227.490234375 
[2025-02-18 06:50:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:1.1330714225769043 norm:0.003906916361302137 max memory_allocated 29227.490234375 
[2025-02-18 06:50:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:1.1317224502563477 norm:0.0037923192139714956 max memory_allocated 29227.490234375 
[2025-02-18 06:51:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:1.129833459854126 norm:0.00364820659160614 max memory_allocated 29227.490234375 
[2025-02-18 06:52:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:1.1297852993011475 norm:0.003647067118436098 max memory_allocated 29227.490234375 
[2025-02-18 06:53:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:1.1301100254058838 norm:0.00363302044570446 max memory_allocated 29227.490234375 
[2025-02-18 06:54:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:1.129392147064209 norm:0.00354968779720366 max memory_allocated 29227.490234375 
[2025-02-18 06:54:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:1.1279935836791992 norm:0.0034108676481992006 max memory_allocated 29227.490234375 
[2025-02-18 06:55:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:1.1275560855865479 norm:0.00337938847951591 max memory_allocated 29227.490234375 
[2025-02-18 06:56:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:1.1271378993988037 norm:0.0034007406793534756 max memory_allocated 29227.490234375 
[2025-02-18 06:57:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:1.1275205612182617 norm:0.0034130625426769257 max memory_allocated 29227.490234375 
[2025-02-18 06:58:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:1.1280876398086548 norm:0.003500940278172493 max memory_allocated 29227.490234375 
[2025-02-18 06:59:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:1.1274404525756836 norm:0.0034423305187374353 max memory_allocated 29227.490234375 
[2025-02-18 06:59:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:1.1292213201522827 norm:0.003567166393622756 max memory_allocated 29227.490234375 
[2025-02-18 07:00:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:1.1298109292984009 norm:0.0036273752339184284 max memory_allocated 29227.490234375 
[2025-02-18 07:00:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 07:01:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:1.2737267017364502 norm:0.025430338457226753 max memory_allocated 29227.677734375 
[2025-02-18 07:02:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:1.260227084159851 norm:0.01649153232574463 max memory_allocated 29227.677734375 
[2025-02-18 07:03:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:1.2519124746322632 norm:0.010568371042609215 max memory_allocated 29227.677734375 
[2025-02-18 07:04:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:1.2500537633895874 norm:0.007906458340585232 max memory_allocated 29227.677734375 
[2025-02-18 07:04:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:1.2455496788024902 norm:0.006875187158584595 max memory_allocated 29227.677734375 
[2025-02-18 07:05:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:1.2384618520736694 norm:0.006134542636573315 max memory_allocated 29227.677734375 
[2025-02-18 07:06:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:1.235902190208435 norm:0.0059129721485078335 max memory_allocated 29227.677734375 
[2025-02-18 07:07:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:1.2368146181106567 norm:0.006001011002808809 max memory_allocated 29227.677734375 
[2025-02-18 07:08:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:1.2341163158416748 norm:0.005866420920938253 max memory_allocated 29227.677734375 
[2025-02-18 07:09:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:1.2334216833114624 norm:0.0057913027703762054 max memory_allocated 29227.677734375 
[2025-02-18 07:09:50 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:1.2330777645111084 norm:0.005764467641711235 max memory_allocated 29227.677734375 
[2025-02-18 07:10:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:1.2310128211975098 norm:0.005654636770486832 max memory_allocated 29227.677734375 
[2025-02-18 07:11:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:1.228017807006836 norm:0.00553441047668457 max memory_allocated 29227.677734375 
[2025-02-18 07:12:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:1.2265368700027466 norm:0.005495600402355194 max memory_allocated 29227.677734375 
[2025-02-18 07:13:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:1.2255483865737915 norm:0.0054116323590278625 max memory_allocated 29227.677734375 
[2025-02-18 07:13:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:1.225377082824707 norm:0.005390124395489693 max memory_allocated 29227.677734375 
[2025-02-18 07:14:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:1.2240898609161377 norm:0.005307990126311779 max memory_allocated 29227.677734375 
[2025-02-18 07:15:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:1.223828673362732 norm:0.005330491345375776 max memory_allocated 29227.677734375 
[2025-02-18 07:16:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:1.223907232284546 norm:0.005357737652957439 max memory_allocated 29227.677734375 
[2025-02-18 07:17:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:1.2228879928588867 norm:0.005322990473359823 max memory_allocated 29227.677734375 
[2025-02-18 07:17:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 07:18:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:1.367817997932434 norm:0.03376170992851257 max memory_allocated 29227.865234375 
[2025-02-18 07:19:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:1.3544005155563354 norm:0.01906641758978367 max memory_allocated 29227.865234375 
[2025-02-18 07:19:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:1.3522999286651611 norm:0.011177930049598217 max memory_allocated 29227.865234375 
[2025-02-18 07:20:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:1.345637321472168 norm:0.006837888620793819 max memory_allocated 29227.865234375 
[2025-02-18 07:21:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:1.3406929969787598 norm:0.005549617577344179 max memory_allocated 29227.865234375 
[2025-02-18 07:22:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:1.3395646810531616 norm:0.0052542611956596375 max memory_allocated 29227.865234375 
[2025-02-18 07:23:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:1.3371959924697876 norm:0.004900315776467323 max memory_allocated 29227.865234375 
[2025-02-18 07:23:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:1.3375929594039917 norm:0.004869128577411175 max memory_allocated 29227.865234375 
[2025-02-18 07:24:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:1.337616205215454 norm:0.004870484117418528 max memory_allocated 29227.865234375 
[2025-02-18 07:25:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:1.3367730379104614 norm:0.004709948785603046 max memory_allocated 29227.865234375 
[2025-02-18 07:26:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:1.340317726135254 norm:0.004843499511480331 max memory_allocated 29227.865234375 
[2025-02-18 07:27:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:1.3388471603393555 norm:0.004745530895888805 max memory_allocated 29227.865234375 
[2025-02-18 07:27:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:1.3379919528961182 norm:0.004685990046709776 max memory_allocated 29227.865234375 
[2025-02-18 07:28:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:1.3360272645950317 norm:0.004563541151583195 max memory_allocated 29227.865234375 
[2025-02-18 07:29:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:1.334054708480835 norm:0.004581672139465809 max memory_allocated 29227.865234375 
[2025-02-18 07:30:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:1.332606315612793 norm:0.0046020252630114555 max memory_allocated 29227.865234375 
[2025-02-18 07:31:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:1.3336237668991089 norm:0.004518828820437193 max memory_allocated 29227.865234375 
[2025-02-18 07:31:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:1.3334821462631226 norm:0.004499970469623804 max memory_allocated 29227.865234375 
[2025-02-18 07:32:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:1.333336591720581 norm:0.004462468437850475 max memory_allocated 29227.865234375 
[2025-02-18 07:33:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:1.3332419395446777 norm:0.004436437971889973 max memory_allocated 29227.865234375 
[2025-02-18 07:33:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 07:34:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:1.467200756072998 norm:0.05411314219236374 max memory_allocated 29228.052734375 
[2025-02-18 07:35:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:1.450557827949524 norm:0.030186915770173073 max memory_allocated 29228.052734375 
[2025-02-18 07:36:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:1.447378158569336 norm:0.01932624541223049 max memory_allocated 29228.052734375 
[2025-02-18 07:37:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:1.4447355270385742 norm:0.012128139846026897 max memory_allocated 29228.052734375 
[2025-02-18 07:37:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:1.4410871267318726 norm:0.008472967892885208 max memory_allocated 29228.052734375 
[2025-02-18 07:38:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:1.4345108270645142 norm:0.0066486261785030365 max memory_allocated 29228.052734375 
[2025-02-18 07:39:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:1.4287264347076416 norm:0.00603718776255846 max memory_allocated 29228.052734375 
[2025-02-18 07:40:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:1.4272032976150513 norm:0.0055923620238900185 max memory_allocated 29228.052734375 
[2025-02-18 07:41:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:1.4236377477645874 norm:0.005063161253929138 max memory_allocated 29228.052734375 
[2025-02-18 07:41:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:1.4204856157302856 norm:0.004746433347463608 max memory_allocated 29228.052734375 
[2025-02-18 07:42:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:1.4216033220291138 norm:0.004470333456993103 max memory_allocated 29228.052734375 
[2025-02-18 07:43:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:1.4222441911697388 norm:0.004225402604788542 max memory_allocated 29228.052734375 
[2025-02-18 07:44:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:1.4224797487258911 norm:0.00412892596796155 max memory_allocated 29228.052734375 
[2025-02-18 07:45:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:1.4244633913040161 norm:0.0040271710604429245 max memory_allocated 29228.052734375 
[2025-02-18 07:46:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:1.4283547401428223 norm:0.0040931920520961285 max memory_allocated 29228.052734375 
[2025-02-18 07:46:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:1.4290804862976074 norm:0.00413120724260807 max memory_allocated 29228.052734375 
[2025-02-18 07:47:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:1.4302698373794556 norm:0.004179287236183882 max memory_allocated 29228.052734375 
[2025-02-18 07:48:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:1.4298304319381714 norm:0.003948861733078957 max memory_allocated 29228.052734375 
[2025-02-18 07:49:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:1.429325819015503 norm:0.003949454054236412 max memory_allocated 29228.052734375 
[2025-02-18 07:50:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:1.4275550842285156 norm:0.003918572794646025 max memory_allocated 29228.052734375 
[2025-02-18 07:50:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 07:51:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:1.559157133102417 norm:0.031828686594963074 max memory_allocated 29228.240234375 
[2025-02-18 07:52:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:1.5477681159973145 norm:0.018550194799900055 max memory_allocated 29228.240234375 
[2025-02-18 07:52:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:1.5441733598709106 norm:0.011956625618040562 max memory_allocated 29228.240234375 
[2025-02-18 07:53:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:1.5375463962554932 norm:0.007923544384539127 max memory_allocated 29228.240234375 
[2025-02-18 07:54:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:1.5366498231887817 norm:0.0062543610110878944 max memory_allocated 29228.240234375 
[2025-02-18 07:55:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:1.5358763933181763 norm:0.005317156668752432 max memory_allocated 29228.240234375 
[2025-02-18 07:56:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:1.5320744514465332 norm:0.004711287096142769 max memory_allocated 29228.240234375 
[2025-02-18 07:56:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:1.5319972038269043 norm:0.0043436698615550995 max memory_allocated 29228.240234375 
[2025-02-18 07:57:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:1.5326526165008545 norm:0.004082862287759781 max memory_allocated 29228.240234375 
[2025-02-18 07:58:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:1.5317553281784058 norm:0.0038862668443471193 max memory_allocated 29228.240234375 
[2025-02-18 07:59:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:1.530216932296753 norm:0.0037030293606221676 max memory_allocated 29228.240234375 
[2025-02-18 08:00:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:1.5280132293701172 norm:0.0036232660058885813 max memory_allocated 29228.240234375 
[2025-02-18 08:00:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:1.5273590087890625 norm:0.0036164214834570885 max memory_allocated 29228.240234375 
[2025-02-18 08:01:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:1.5267897844314575 norm:0.003530124668031931 max memory_allocated 29228.240234375 
[2025-02-18 08:02:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:1.5259727239608765 norm:0.0034419007133692503 max memory_allocated 29228.240234375 
[2025-02-18 08:03:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:1.5252646207809448 norm:0.0034734623041003942 max memory_allocated 29228.240234375 
[2025-02-18 08:04:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:1.5255682468414307 norm:0.0034806481562554836 max memory_allocated 29228.240234375 
[2025-02-18 08:04:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:1.5252556800842285 norm:0.0034853126853704453 max memory_allocated 29228.240234375 
[2025-02-18 08:05:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:1.5254076719284058 norm:0.0034942429047077894 max memory_allocated 29228.240234375 
[2025-02-18 08:06:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:1.52562415599823 norm:0.0034500255715101957 max memory_allocated 29228.240234375 
[2025-02-18 08:06:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 08:07:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:1.602712631225586 norm:0.023250408470630646 max memory_allocated 29228.427734375 
[2025-02-18 08:08:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:1.5975658893585205 norm:0.011831602081656456 max memory_allocated 29228.427734375 
[2025-02-18 08:09:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:1.5971482992172241 norm:0.007850158028304577 max memory_allocated 29228.427734375 
[2025-02-18 08:10:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:1.5922951698303223 norm:0.00534178176894784 max memory_allocated 29228.427734375 
[2025-02-18 08:10:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:1.5900923013687134 norm:0.004472407512366772 max memory_allocated 29228.427734375 
[2025-02-18 08:11:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:1.5862077474594116 norm:0.0037302395794540644 max memory_allocated 29228.427734375 
[2025-02-18 08:12:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:1.586883544921875 norm:0.0035485837142914534 max memory_allocated 29228.427734375 
[2025-02-18 08:13:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:1.5871683359146118 norm:0.003365228185430169 max memory_allocated 29228.427734375 
[2025-02-18 08:14:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:1.5857024192810059 norm:0.0032442209776490927 max memory_allocated 29228.427734375 
[2025-02-18 08:14:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:1.5877104997634888 norm:0.003372480859979987 max memory_allocated 29228.427734375 
[2025-02-18 08:15:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:1.5894801616668701 norm:0.003797871060669422 max memory_allocated 29228.427734375 
[2025-02-18 08:16:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:1.5901973247528076 norm:0.0040018898434937 max memory_allocated 29228.427734375 
[2025-02-18 08:17:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:1.5886064767837524 norm:0.0039274017326533794 max memory_allocated 29228.427734375 
[2025-02-18 08:18:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:1.5880067348480225 norm:0.003827884793281555 max memory_allocated 29228.427734375 
[2025-02-18 08:19:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:1.5887540578842163 norm:0.0038469063583761454 max memory_allocated 29228.427734375 
[2025-02-18 08:19:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:1.5886094570159912 norm:0.0038350282702594995 max memory_allocated 29228.427734375 
[2025-02-18 08:20:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:1.5874063968658447 norm:0.003926809877157211 max memory_allocated 29228.427734375 
[2025-02-18 08:21:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:1.5865631103515625 norm:0.0038792318664491177 max memory_allocated 29228.427734375 
[2025-02-18 08:22:16 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:1.5870747566223145 norm:0.003881276585161686 max memory_allocated 29228.427734375 
[2025-02-18 08:23:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:1.5879175662994385 norm:0.003893432207405567 max memory_allocated 29228.427734375 
[2025-02-18 08:23:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 08:24:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:1.6523205041885376 norm:0.011289834976196289 max memory_allocated 29228.615234375 
[2025-02-18 08:25:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:1.6408023834228516 norm:0.008032171055674553 max memory_allocated 29228.615234375 
[2025-02-18 08:25:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:1.6398159265518188 norm:0.005963715724647045 max memory_allocated 29228.615234375 
[2025-02-18 08:26:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:1.6354604959487915 norm:0.003935432061553001 max memory_allocated 29228.615234375 
[2025-02-18 08:27:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:1.6321064233779907 norm:0.0029937424696981907 max memory_allocated 29228.615234375 
[2025-02-18 08:28:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:1.6303452253341675 norm:0.0027065572794526815 max memory_allocated 29228.615234375 
[2025-02-18 08:29:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:1.6296625137329102 norm:0.002579733729362488 max memory_allocated 29228.615234375 
[2025-02-18 08:29:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:1.628711223602295 norm:0.0025889603421092033 max memory_allocated 29228.615234375 
[2025-02-18 08:30:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:1.628908395767212 norm:0.0026356049347668886 max memory_allocated 29228.615234375 
[2025-02-18 08:31:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:1.633001446723938 norm:0.0030604908242821693 max memory_allocated 29228.615234375 
[2025-02-18 08:32:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:1.632744550704956 norm:0.003219578880816698 max memory_allocated 29228.615234375 
[2025-02-18 08:33:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:1.6323343515396118 norm:0.003817123593762517 max memory_allocated 29228.615234375 
[2025-02-18 08:33:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:1.6308270692825317 norm:0.003783716121688485 max memory_allocated 29228.615234375 
[2025-02-18 08:34:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:1.6287345886230469 norm:0.003736861515790224 max memory_allocated 29228.615234375 
[2025-02-18 08:35:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:1.6268008947372437 norm:0.0036774668842554092 max memory_allocated 29228.615234375 
[2025-02-18 08:36:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:1.6254944801330566 norm:0.0036149872466921806 max memory_allocated 29228.615234375 
[2025-02-18 08:37:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:1.6254116296768188 norm:0.003680021967738867 max memory_allocated 29228.615234375 
[2025-02-18 08:37:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:1.6254791021347046 norm:0.003786046989262104 max memory_allocated 29228.615234375 
[2025-02-18 08:38:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:1.624537706375122 norm:0.003801427548751235 max memory_allocated 29228.615234375 
[2025-02-18 08:39:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:1.624099612236023 norm:0.0038396031595766544 max memory_allocated 29228.615234375 
[2025-02-18 08:39:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 08:40:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:1.7643462419509888 norm:0.0493839792907238 max memory_allocated 29228.802734375 
[2025-02-18 08:41:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:1.745354175567627 norm:0.03209419548511505 max memory_allocated 29228.802734375 
[2025-02-18 08:42:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:1.7406935691833496 norm:0.022959019988775253 max memory_allocated 29228.802734375 
[2025-02-18 08:43:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:1.731984257698059 norm:0.015128983184695244 max memory_allocated 29228.802734375 
[2025-02-18 08:43:55 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:1.7216744422912598 norm:0.01032185833901167 max memory_allocated 29228.802734375 
[2025-02-18 08:44:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:1.704842209815979 norm:0.009230941534042358 max memory_allocated 29228.802734375 
[2025-02-18 08:45:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:1.6858572959899902 norm:0.006496319081634283 max memory_allocated 29228.802734375 
[2025-02-18 08:46:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:1.6865726709365845 norm:0.005828308407217264 max memory_allocated 29228.802734375 
[2025-02-18 08:47:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:1.689133882522583 norm:0.005804035346955061 max memory_allocated 29228.802734375 
[2025-02-18 08:47:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:1.6905744075775146 norm:0.005809360183775425 max memory_allocated 29228.802734375 
[2025-02-18 08:48:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:1.6977490186691284 norm:0.006271827965974808 max memory_allocated 29228.802734375 
[2025-02-18 08:49:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:1.7070907354354858 norm:0.00781296007335186 max memory_allocated 29228.802734375 
[2025-02-18 08:50:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:1.7097249031066895 norm:0.009116018190979958 max memory_allocated 29228.802734375 
[2025-02-18 08:51:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:1.7115113735198975 norm:0.009006770327687263 max memory_allocated 29228.802734375 
[2025-02-18 08:52:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:1.710679292678833 norm:0.00903596542775631 max memory_allocated 29228.802734375 
[2025-02-18 08:52:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:1.7134044170379639 norm:0.009320174343883991 max memory_allocated 29228.802734375 
[2025-02-18 08:53:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:1.7167719602584839 norm:0.010220842435956001 max memory_allocated 29228.802734375 
[2025-02-18 08:54:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:1.7155195474624634 norm:0.00982614140957594 max memory_allocated 29228.802734375 
[2025-02-18 08:55:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:1.7146191596984863 norm:0.009724379517138004 max memory_allocated 29228.802734375 
[2025-02-18 08:56:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:1.7142670154571533 norm:0.009463317692279816 max memory_allocated 29228.802734375 
[2025-02-18 08:56:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 08:57:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:1.8521398305892944 norm:0.06331188976764679 max memory_allocated 29228.990234375 
[2025-02-18 08:57:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:1.835989236831665 norm:0.03854835405945778 max memory_allocated 29228.990234375 
[2025-02-18 08:58:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:1.8330903053283691 norm:0.02696147747337818 max memory_allocated 29228.990234375 
[2025-02-18 08:59:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:1.827537178993225 norm:0.018197162076830864 max memory_allocated 29228.990234375 
[2025-02-18 09:00:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:1.823043704032898 norm:0.012978117913007736 max memory_allocated 29228.990234375 
[2025-02-18 09:01:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:1.8201665878295898 norm:0.010552134364843369 max memory_allocated 29228.990234375 
[2025-02-18 09:02:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:1.8094611167907715 norm:0.008828828111290932 max memory_allocated 29228.990234375 
[2025-02-18 09:02:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:1.8064508438110352 norm:0.00827390979975462 max memory_allocated 29228.990234375 
[2025-02-18 09:03:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:1.8052759170532227 norm:0.007988743484020233 max memory_allocated 29228.990234375 
[2025-02-18 09:04:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:1.798815131187439 norm:0.0072304788045585155 max memory_allocated 29228.990234375 
[2025-02-18 09:05:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:1.7923939228057861 norm:0.007090409751981497 max memory_allocated 29228.990234375 
[2025-02-18 09:06:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:1.780733346939087 norm:0.008862488903105259 max memory_allocated 29228.990234375 
[2025-02-18 09:06:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:1.7732797861099243 norm:0.011065559461712837 max memory_allocated 29228.990234375 
[2025-02-18 09:07:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:1.7702661752700806 norm:0.014383869245648384 max memory_allocated 29228.990234375 
[2025-02-18 09:08:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:1.7701078653335571 norm:0.018310697749257088 max memory_allocated 29228.990234375 
[2025-02-18 09:09:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:1.7706031799316406 norm:0.02653958462178707 max memory_allocated 29228.990234375 
[2025-02-18 09:10:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:1.7702579498291016 norm:0.02971743606030941 max memory_allocated 29228.990234375 
[2025-02-18 09:10:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:1.7693861722946167 norm:0.030667703598737717 max memory_allocated 29228.990234375 
[2025-02-18 09:11:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:1.7684576511383057 norm:0.029976701363921165 max memory_allocated 29228.990234375 
[2025-02-18 09:12:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:1.7686116695404053 norm:0.02966996468603611 max memory_allocated 29228.990234375 
[2025-02-18 09:12:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 09:13:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:1.8782446384429932 norm:0.028675202280282974 max memory_allocated 29229.177734375 
[2025-02-18 09:14:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:1.8647031784057617 norm:0.019145473837852478 max memory_allocated 29229.177734375 
[2025-02-18 09:15:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:1.8532922267913818 norm:0.012519022449851036 max memory_allocated 29229.177734375 
[2025-02-18 09:16:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:1.8454821109771729 norm:0.007984593510627747 max memory_allocated 29229.177734375 
[2025-02-18 09:16:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:1.8435454368591309 norm:0.005325367208570242 max memory_allocated 29229.177734375 
[2025-02-18 09:17:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:1.8384182453155518 norm:0.004149319604039192 max memory_allocated 29229.177734375 
[2025-02-18 09:18:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:1.8360549211502075 norm:0.0036737776827067137 max memory_allocated 29229.177734375 
[2025-02-18 09:19:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:1.8368220329284668 norm:0.0036287051625549793 max memory_allocated 29229.177734375 
[2025-02-18 09:20:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:1.840015172958374 norm:0.00372919999063015 max memory_allocated 29229.177734375 
[2025-02-18 09:20:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:1.8394454717636108 norm:0.00348854111507535 max memory_allocated 29229.177734375 
[2025-02-18 09:21:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:1.8385593891143799 norm:0.003336181165650487 max memory_allocated 29229.177734375 
[2025-02-18 09:22:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:1.839397668838501 norm:0.0033384980633854866 max memory_allocated 29229.177734375 
[2025-02-18 09:23:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:1.8416060209274292 norm:0.0035289444494992495 max memory_allocated 29229.177734375 
[2025-02-18 09:24:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:1.8423640727996826 norm:0.0035585095174610615 max memory_allocated 29229.177734375 
[2025-02-18 09:24:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:1.8414512872695923 norm:0.0035167736932635307 max memory_allocated 29229.177734375 
[2025-02-18 09:25:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:1.8396179676055908 norm:0.0034285217989236116 max memory_allocated 29229.177734375 
[2025-02-18 09:26:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:1.836423397064209 norm:0.0032717918511480093 max memory_allocated 29229.177734375 
[2025-02-18 09:27:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:1.8352004289627075 norm:0.003212372539564967 max memory_allocated 29229.177734375 
[2025-02-18 09:28:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:1.8341432809829712 norm:0.003162962384521961 max memory_allocated 29229.177734375 
[2025-02-18 09:29:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:1.8329240083694458 norm:0.003088001860305667 max memory_allocated 29229.177734375 
[2025-02-18 09:29:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 09:30:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:1.9579906463623047 norm:0.021143527701497078 max memory_allocated 29229.365234375 
[2025-02-18 09:30:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:1.9425917863845825 norm:0.012854221276938915 max memory_allocated 29229.365234375 
[2025-02-18 09:31:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:1.9373735189437866 norm:0.009024686180055141 max memory_allocated 29229.365234375 
[2025-02-18 09:32:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:1.9340921640396118 norm:0.006667373701930046 max memory_allocated 29229.365234375 
[2025-02-18 09:33:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:1.9312824010849 norm:0.005228292662650347 max memory_allocated 29229.365234375 
[2025-02-18 09:34:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:1.9265426397323608 norm:0.004123998805880547 max memory_allocated 29229.365234375 
[2025-02-18 09:35:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:1.9226441383361816 norm:0.0035012199077755213 max memory_allocated 29229.365234375 
[2025-02-18 09:35:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:1.9203667640686035 norm:0.0031831192318350077 max memory_allocated 29229.365234375 
[2025-02-18 09:36:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:1.9171888828277588 norm:0.00338718481361866 max memory_allocated 29229.365234375 
[2025-02-18 09:37:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:1.9138952493667603 norm:0.003836791031062603 max memory_allocated 29229.365234375 
[2025-02-18 09:38:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:1.9126560688018799 norm:0.003740474581718445 max memory_allocated 29229.365234375 
[2025-02-18 09:39:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:1.9123256206512451 norm:0.0038690229412168264 max memory_allocated 29229.365234375 
[2025-02-18 09:39:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:1.9116560220718384 norm:0.004033613950014114 max memory_allocated 29229.365234375 
[2025-02-18 09:40:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:1.9106123447418213 norm:0.003992397338151932 max memory_allocated 29229.365234375 
[2025-02-18 09:41:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:1.9102267026901245 norm:0.003910234197974205 max memory_allocated 29229.365234375 
[2025-02-18 09:42:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:1.90952730178833 norm:0.0038896554615348577 max memory_allocated 29229.365234375 
[2025-02-18 09:43:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:1.9093176126480103 norm:0.003929391503334045 max memory_allocated 29229.365234375 
[2025-02-18 09:43:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:1.9087737798690796 norm:0.003906986676156521 max memory_allocated 29229.365234375 
[2025-02-18 09:44:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:1.9086154699325562 norm:0.0038597446400672197 max memory_allocated 29229.365234375 
[2025-02-18 09:45:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:1.907901406288147 norm:0.003988745156675577 max memory_allocated 29229.365234375 
[2025-02-18 09:45:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 09:46:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:2.146122455596924 norm:0.07211633026599884 max memory_allocated 29229.552734375 
[2025-02-18 09:47:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:2.1269872188568115 norm:0.04736287519335747 max memory_allocated 29229.552734375 
[2025-02-18 09:48:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:2.1207940578460693 norm:0.03398960828781128 max memory_allocated 29229.552734375 
[2025-02-18 09:49:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:2.116267204284668 norm:0.023763975128531456 max memory_allocated 29229.552734375 
[2025-02-18 09:49:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:2.1127266883850098 norm:0.01737571880221367 max memory_allocated 29229.552734375 
[2025-02-18 09:50:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:2.1091151237487793 norm:0.013675091788172722 max memory_allocated 29229.552734375 
[2025-02-18 09:51:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:2.1054017543792725 norm:0.011270804330706596 max memory_allocated 29229.552734375 
[2025-02-18 09:52:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:2.094322681427002 norm:0.010250044986605644 max memory_allocated 29229.552734375 
[2025-02-18 09:53:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:2.0829246044158936 norm:0.009683612734079361 max memory_allocated 29229.552734375 
[2025-02-18 09:53:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:2.0763330459594727 norm:0.008581550791859627 max memory_allocated 29229.552734375 
[2025-02-18 09:54:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:2.073725938796997 norm:0.00857301615178585 max memory_allocated 29229.552734375 
[2025-02-18 09:55:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:2.073118209838867 norm:0.009419482201337814 max memory_allocated 29229.552734375 
[2025-02-18 09:56:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:2.072676181793213 norm:0.009792555123567581 max memory_allocated 29229.552734375 
[2025-02-18 09:57:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:2.07127046585083 norm:0.010094433091580868 max memory_allocated 29229.552734375 
[2025-02-18 09:57:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:2.070888042449951 norm:0.010720116086304188 max memory_allocated 29229.552734375 
[2025-02-18 09:58:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:2.069610834121704 norm:0.010683955624699593 max memory_allocated 29229.552734375 
[2025-02-18 09:59:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:2.067540168762207 norm:0.009948475286364555 max memory_allocated 29229.552734375 
[2025-02-18 10:00:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:2.0670387744903564 norm:0.00988403707742691 max memory_allocated 29229.552734375 
[2025-02-18 10:01:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:2.068791627883911 norm:0.010181617923080921 max memory_allocated 29229.552734375 
[2025-02-18 10:02:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:2.070523262023926 norm:0.010411812923848629 max memory_allocated 29229.552734375 
[2025-02-18 10:02:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 10:03:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:2.2710447311401367 norm:0.020502641797065735 max memory_allocated 29229.740234375 
[2025-02-18 10:03:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:2.253981113433838 norm:0.011474179103970528 max memory_allocated 29229.740234375 
[2025-02-18 10:04:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:2.246034622192383 norm:0.007748149335384369 max memory_allocated 29229.740234375 
[2025-02-18 10:05:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:2.2421059608459473 norm:0.005924208555370569 max memory_allocated 29229.740234375 
[2025-02-18 10:06:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:2.23881196975708 norm:0.004587063565850258 max memory_allocated 29229.740234375 
[2025-02-18 10:07:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:2.235642910003662 norm:0.0037415572442114353 max memory_allocated 29229.740234375 
[2025-02-18 10:07:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:2.2328693866729736 norm:0.0032767814118415117 max memory_allocated 29229.740234375 
[2025-02-18 10:08:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:2.231229305267334 norm:0.002962612546980381 max memory_allocated 29229.740234375 
[2025-02-18 10:09:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:2.229670286178589 norm:0.00275417510420084 max memory_allocated 29229.740234375 
[2025-02-18 10:10:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:2.2283835411071777 norm:0.002612379379570484 max memory_allocated 29229.740234375 
[2025-02-18 10:11:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:2.2265851497650146 norm:0.002444580662995577 max memory_allocated 29229.740234375 
[2025-02-18 10:12:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:2.226163387298584 norm:0.0022869366221129894 max memory_allocated 29229.740234375 
[2025-02-18 10:12:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:2.2241406440734863 norm:0.002173923421651125 max memory_allocated 29229.740234375 
[2025-02-18 10:13:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:2.2223594188690186 norm:0.002066405490040779 max memory_allocated 29229.740234375 
[2025-02-18 10:14:26 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:2.2213315963745117 norm:0.0019909534603357315 max memory_allocated 29229.740234375 
[2025-02-18 10:15:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:2.2208001613616943 norm:0.0019209004240110517 max memory_allocated 29229.740234375 
[2025-02-18 10:16:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:2.2205071449279785 norm:0.00189060193952173 max memory_allocated 29229.740234375 
[2025-02-18 10:16:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:2.2201831340789795 norm:0.0018696488114073873 max memory_allocated 29229.740234375 
[2025-02-18 10:17:41 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:2.2197141647338867 norm:0.0018320941599085927 max memory_allocated 29229.740234375 
[2025-02-18 10:18:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:2.219374418258667 norm:0.0018032541265711188 max memory_allocated 29229.740234375 
[2025-02-18 10:18:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 10:19:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:2.4732706546783447 norm:0.04639718309044838 max memory_allocated 29229.927734375 
[2025-02-18 10:20:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:2.447242498397827 norm:0.025354014709591866 max memory_allocated 29229.927734375 
[2025-02-18 10:21:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:2.4356179237365723 norm:0.01559190172702074 max memory_allocated 29229.927734375 
[2025-02-18 10:22:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:2.4297192096710205 norm:0.01129140704870224 max memory_allocated 29229.927734375 
[2025-02-18 10:22:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:2.425844669342041 norm:0.008630897849798203 max memory_allocated 29229.927734375 
[2025-02-18 10:23:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:2.4243595600128174 norm:0.006910788826644421 max memory_allocated 29229.927734375 
[2025-02-18 10:24:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:2.4227826595306396 norm:0.006089615169912577 max memory_allocated 29229.927734375 
[2025-02-18 10:25:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:2.421692371368408 norm:0.005671995226293802 max memory_allocated 29229.927734375 
[2025-02-18 10:26:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:2.418825626373291 norm:0.005010324530303478 max memory_allocated 29229.927734375 
[2025-02-18 10:26:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:2.417961359024048 norm:0.004624905996024609 max memory_allocated 29229.927734375 
[2025-02-18 10:27:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:2.415440797805786 norm:0.004188599064946175 max memory_allocated 29229.927734375 
[2025-02-18 10:28:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:2.4120309352874756 norm:0.003743776585906744 max memory_allocated 29229.927734375 
[2025-02-18 10:29:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:2.411247491836548 norm:0.003552778158336878 max memory_allocated 29229.927734375 
[2025-02-18 10:30:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:2.4100723266601562 norm:0.0033661348279565573 max memory_allocated 29229.927734375 
[2025-02-18 10:30:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:2.4093732833862305 norm:0.0031641139648854733 max memory_allocated 29229.927734375 
[2025-02-18 10:31:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:2.4073240756988525 norm:0.0029480508528649807 max memory_allocated 29229.927734375 
[2025-02-18 10:32:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:2.408938407897949 norm:0.0029953178018331528 max memory_allocated 29229.927734375 
[2025-02-18 10:33:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:2.4097206592559814 norm:0.002985140774399042 max memory_allocated 29229.927734375 
[2025-02-18 10:34:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:2.4079794883728027 norm:0.002862378256395459 max memory_allocated 29229.927734375 
[2025-02-18 10:34:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:2.4066338539123535 norm:0.002810507081449032 max memory_allocated 29229.927734375 
[2025-02-18 10:35:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 10:36:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:2.699218273162842 norm:0.017926665022969246 max memory_allocated 29230.115234375 
[2025-02-18 10:36:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:2.6824545860290527 norm:0.010416021570563316 max memory_allocated 29230.115234375 
[2025-02-18 10:37:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:2.6766860485076904 norm:0.007524693384766579 max memory_allocated 29230.115234375 
[2025-02-18 10:38:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:2.6752400398254395 norm:0.005707279313355684 max memory_allocated 29230.115234375 
[2025-02-18 10:39:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:2.672553062438965 norm:0.0042280335910618305 max memory_allocated 29230.115234375 
[2025-02-18 10:40:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:2.6687419414520264 norm:0.0034217906650155783 max memory_allocated 29230.115234375 
[2025-02-18 10:40:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:2.6664254665374756 norm:0.0030508413910865784 max memory_allocated 29230.115234375 
[2025-02-18 10:41:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:2.6638588905334473 norm:0.0027417095843702555 max memory_allocated 29230.115234375 
[2025-02-18 10:42:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:2.6606316566467285 norm:0.002439857693389058 max memory_allocated 29230.115234375 
[2025-02-18 10:43:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:2.659283399581909 norm:0.0022945967502892017 max memory_allocated 29230.115234375 
[2025-02-18 10:44:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:2.6577184200286865 norm:0.0022179873194545507 max memory_allocated 29230.115234375 
[2025-02-18 10:44:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:2.657045841217041 norm:0.0021483779419213533 max memory_allocated 29230.115234375 
[2025-02-18 10:45:47 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:2.656325101852417 norm:0.002145194448530674 max memory_allocated 29230.115234375 
[2025-02-18 10:46:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:2.6557440757751465 norm:0.002146973507478833 max memory_allocated 29230.115234375 
[2025-02-18 10:47:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:2.654789686203003 norm:0.0021309545263648033 max memory_allocated 29230.115234375 
[2025-02-18 10:48:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:2.6543538570404053 norm:0.0021226098760962486 max memory_allocated 29230.115234375 
[2025-02-18 10:49:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:2.654787540435791 norm:0.002191616455093026 max memory_allocated 29230.115234375 
[2025-02-18 10:49:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:2.6545863151550293 norm:0.002202513860538602 max memory_allocated 29230.115234375 
[2025-02-18 10:50:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:2.6539950370788574 norm:0.00223864009603858 max memory_allocated 29230.115234375 
[2025-02-18 10:51:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:2.653137683868408 norm:0.002220293739810586 max memory_allocated 29230.115234375 
[2025-02-18 10:51:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 10:52:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:3.042365074157715 norm:0.02319825440645218 max memory_allocated 29230.302734375 
[2025-02-18 10:53:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:3.0280168056488037 norm:0.01460929587483406 max memory_allocated 29230.302734375 
[2025-02-18 10:54:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:3.0259571075439453 norm:0.011104842647910118 max memory_allocated 29230.302734375 
[2025-02-18 10:54:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:3.0252609252929688 norm:0.008550096303224564 max memory_allocated 29230.302734375 
[2025-02-18 10:55:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:3.023099899291992 norm:0.006761559750884771 max memory_allocated 29230.302734375 
[2025-02-18 10:56:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:3.0161900520324707 norm:0.005462653934955597 max memory_allocated 29230.302734375 
[2025-02-18 10:57:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:3.0115339756011963 norm:0.0046598054468631744 max memory_allocated 29230.302734375 
[2025-02-18 10:58:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:3.00943660736084 norm:0.0042222850024700165 max memory_allocated 29230.302734375 
[2025-02-18 10:59:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:3.0082688331604004 norm:0.0038793354760855436 max memory_allocated 29230.302734375 
[2025-02-18 10:59:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:3.0079972743988037 norm:0.0036946889013051987 max memory_allocated 29230.302734375 
[2025-02-18 11:00:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:3.0068857669830322 norm:0.003575011156499386 max memory_allocated 29230.302734375 
[2025-02-18 11:01:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:3.0050854682922363 norm:0.0034139917697757483 max memory_allocated 29230.302734375 
[2025-02-18 11:02:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:3.002969980239868 norm:0.0032761378679424524 max memory_allocated 29230.302734375 
[2025-02-18 11:03:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:3.002002239227295 norm:0.003172733820974827 max memory_allocated 29230.302734375 
[2025-02-18 11:03:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:3.0006182193756104 norm:0.00312403729185462 max memory_allocated 29230.302734375 
[2025-02-18 11:04:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:2.999999761581421 norm:0.0030862148851156235 max memory_allocated 29230.302734375 
[2025-02-18 11:05:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:2.9987387657165527 norm:0.003015254158526659 max memory_allocated 29230.302734375 
[2025-02-18 11:06:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:2.9983439445495605 norm:0.003002722281962633 max memory_allocated 29230.302734375 
[2025-02-18 11:07:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:2.9981307983398438 norm:0.0029743872582912445 max memory_allocated 29230.302734375 
[2025-02-18 11:07:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:2.997441530227661 norm:0.002970504341647029 max memory_allocated 29230.302734375 
[2025-02-18 11:08:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 11:09:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:3.3910202980041504 norm:0.021615279838442802 max memory_allocated 29230.490234375 
[2025-02-18 11:09:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:3.3808629512786865 norm:0.014791766181588173 max memory_allocated 29230.490234375 
[2025-02-18 11:10:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:3.3784217834472656 norm:0.010886406525969505 max memory_allocated 29230.490234375 
[2025-02-18 11:11:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:3.3756892681121826 norm:0.008333428762853146 max memory_allocated 29230.490234375 
[2025-02-18 11:12:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:3.3716607093811035 norm:0.00687228050082922 max memory_allocated 29230.490234375 
[2025-02-18 11:13:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:3.3693270683288574 norm:0.005699601955711842 max memory_allocated 29230.490234375 
[2025-02-18 11:13:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:3.3675835132598877 norm:0.004984527826309204 max memory_allocated 29230.490234375 
[2025-02-18 11:14:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:3.3653054237365723 norm:0.004410301800817251 max memory_allocated 29230.490234375 
[2025-02-18 11:15:30 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:3.3632473945617676 norm:0.0038852759171277285 max memory_allocated 29230.490234375 
[2025-02-18 11:16:18 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:3.360790729522705 norm:0.0035521346144378185 max memory_allocated 29230.490234375 
[2025-02-18 11:17:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:3.359029769897461 norm:0.00330656161531806 max memory_allocated 29230.490234375 
[2025-02-18 11:17:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:3.3566601276397705 norm:0.003066234290599823 max memory_allocated 29230.490234375 
[2025-02-18 11:18:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:3.355842351913452 norm:0.002873579040169716 max memory_allocated 29230.490234375 
[2025-02-18 11:19:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:3.3543038368225098 norm:0.0026893022004514933 max memory_allocated 29230.490234375 
[2025-02-18 11:20:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:3.352935791015625 norm:0.0025758908595889807 max memory_allocated 29230.490234375 
[2025-02-18 11:21:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:3.3526039123535156 norm:0.002466777106747031 max memory_allocated 29230.490234375 
[2025-02-18 11:21:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:3.3516952991485596 norm:0.0023839587811380625 max memory_allocated 29230.490234375 
[2025-02-18 11:22:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:3.350818634033203 norm:0.0023099426180124283 max memory_allocated 29230.490234375 
[2025-02-18 11:23:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:3.3501265048980713 norm:0.0022744559682905674 max memory_allocated 29230.490234375 
[2025-02-18 11:24:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:3.349519968032837 norm:0.0022324800956994295 max memory_allocated 29230.490234375 
[2025-02-18 11:24:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 11:25:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:3.755594491958618 norm:0.013800069689750671 max memory_allocated 29230.677734375 
[2025-02-18 11:26:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:3.7442049980163574 norm:0.0085908193141222 max memory_allocated 29230.677734375 
[2025-02-18 11:27:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:3.7422680854797363 norm:0.006328691728413105 max memory_allocated 29230.677734375 
[2025-02-18 11:27:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:3.740753173828125 norm:0.004873297642916441 max memory_allocated 29230.677734375 
[2025-02-18 11:28:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:3.73982572555542 norm:0.003966148942708969 max memory_allocated 29230.677734375 
[2025-02-18 11:29:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:3.738004446029663 norm:0.003394131548702717 max memory_allocated 29230.677734375 
[2025-02-18 11:30:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:3.7348694801330566 norm:0.0030920694116503 max memory_allocated 29230.677734375 
[2025-02-18 11:31:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:3.7308764457702637 norm:0.002847567666321993 max memory_allocated 29230.677734375 
[2025-02-18 11:31:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:3.726834535598755 norm:0.003006505314260721 max memory_allocated 29230.677734375 
[2025-02-18 11:32:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:3.7219762802124023 norm:0.008223656564950943 max memory_allocated 29230.677734375 
[2025-02-18 11:33:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:3.719992160797119 norm:0.008892079815268517 max memory_allocated 29230.677734375 
[2025-02-18 11:34:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:3.718736171722412 norm:0.008733566850423813 max memory_allocated 29230.677734375 
[2025-02-18 11:35:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:3.7175660133361816 norm:0.00848315842449665 max memory_allocated 29230.677734375 
[2025-02-18 11:36:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:3.7163772583007812 norm:0.008775845170021057 max memory_allocated 29230.677734375 
[2025-02-18 11:36:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:3.7153000831604004 norm:0.009035046212375164 max memory_allocated 29230.677734375 
[2025-02-18 11:37:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:3.7145094871520996 norm:0.009699148125946522 max memory_allocated 29230.677734375 
[2025-02-18 11:38:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:3.7137084007263184 norm:0.009960105642676353 max memory_allocated 29230.677734375 
[2025-02-18 11:39:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:3.7129909992218018 norm:0.010290742851793766 max memory_allocated 29230.677734375 
[2025-02-18 11:40:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:3.7121975421905518 norm:0.010556904599070549 max memory_allocated 29230.677734375 
[2025-02-18 11:40:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:3.7117807865142822 norm:0.01062903180718422 max memory_allocated 29230.677734375 
[2025-02-18 11:41:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 11:41:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:4.196502208709717 norm:0.018278956413269043 max memory_allocated 29230.865234375 
[2025-02-18 11:42:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:4.188998699188232 norm:0.012823187746107578 max memory_allocated 29230.865234375 
[2025-02-18 11:43:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:4.189908981323242 norm:0.00985778495669365 max memory_allocated 29230.865234375 
[2025-02-18 11:44:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:4.188348770141602 norm:0.007892611436545849 max memory_allocated 29230.865234375 
[2025-02-18 11:45:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:4.185758590698242 norm:0.006532752420753241 max memory_allocated 29230.865234375 
[2025-02-18 11:45:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:4.183368682861328 norm:0.005600719712674618 max memory_allocated 29230.865234375 
[2025-02-18 11:46:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:4.18261194229126 norm:0.004855463281273842 max memory_allocated 29230.865234375 
[2025-02-18 11:47:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:4.17988920211792 norm:0.004365995991975069 max memory_allocated 29230.865234375 
[2025-02-18 11:48:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:4.178014278411865 norm:0.003975014202296734 max memory_allocated 29230.865234375 
[2025-02-18 11:49:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:4.176748275756836 norm:0.0037543433718383312 max memory_allocated 29230.865234375 
[2025-02-18 11:50:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:4.175130844116211 norm:0.0036003831773996353 max memory_allocated 29230.865234375 
[2025-02-18 11:50:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:4.1736955642700195 norm:0.0034886959474533796 max memory_allocated 29230.865234375 
[2025-02-18 11:51:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:4.172802925109863 norm:0.0034042680636048317 max memory_allocated 29230.865234375 
[2025-02-18 11:52:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:4.171497821807861 norm:0.0033453889191150665 max memory_allocated 29230.865234375 
[2025-02-18 11:53:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:4.1695966720581055 norm:0.003253057599067688 max memory_allocated 29230.865234375 
[2025-02-18 11:54:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:4.16813850402832 norm:0.003228181041777134 max memory_allocated 29230.865234375 
[2025-02-18 11:54:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:4.167383193969727 norm:0.0032129152677953243 max memory_allocated 29230.865234375 
[2025-02-18 11:55:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:4.166732311248779 norm:0.003168933792039752 max memory_allocated 29230.865234375 
[2025-02-18 11:56:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:4.166032791137695 norm:0.003144332207739353 max memory_allocated 29230.865234375 
[2025-02-18 11:57:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:4.1649699211120605 norm:0.0031214081682264805 max memory_allocated 29230.865234375 
[2025-02-18 11:57:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 11:58:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:4.589618682861328 norm:0.015514723025262356 max memory_allocated 29231.052734375 
[2025-02-18 11:59:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:4.583430290222168 norm:0.010455416515469551 max memory_allocated 29231.052734375 
[2025-02-18 12:00:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:4.580451011657715 norm:0.007889867760241032 max memory_allocated 29231.052734375 
[2025-02-18 12:00:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:4.5786452293396 norm:0.0061071040108799934 max memory_allocated 29231.052734375 
[2025-02-18 12:01:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:4.576116561889648 norm:0.005048931576311588 max memory_allocated 29231.052734375 
[2025-02-18 12:02:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:4.572543621063232 norm:0.004302655346691608 max memory_allocated 29231.052734375 
[2025-02-18 12:03:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:4.569723129272461 norm:0.0037536434829235077 max memory_allocated 29231.052734375 
[2025-02-18 12:04:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:4.566486835479736 norm:0.0033851428888738155 max memory_allocated 29231.052734375 
[2025-02-18 12:04:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:4.5631232261657715 norm:0.0030748117715120316 max memory_allocated 29231.052734375 
[2025-02-18 12:05:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:4.560925483703613 norm:0.0029379818588495255 max memory_allocated 29231.052734375 
[2025-02-18 12:06:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:4.559277534484863 norm:0.0028447837103158236 max memory_allocated 29231.052734375 
[2025-02-18 12:07:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:4.5582098960876465 norm:0.0027165119536221027 max memory_allocated 29231.052734375 
[2025-02-18 12:08:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:4.556942939758301 norm:0.0027260356582701206 max memory_allocated 29231.052734375 
[2025-02-18 12:08:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:4.554829120635986 norm:0.003036974463611841 max memory_allocated 29231.052734375 
[2025-02-18 12:09:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:4.552816867828369 norm:0.005757593084126711 max memory_allocated 29231.052734375 
[2025-02-18 12:10:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:4.552068710327148 norm:0.005828240420669317 max memory_allocated 29231.052734375 
[2025-02-18 12:11:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:4.551906585693359 norm:0.006038118619471788 max memory_allocated 29231.052734375 
[2025-02-18 12:12:09 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:4.5516557693481445 norm:0.006021348759531975 max memory_allocated 29231.052734375 
[2025-02-18 12:12:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:4.551036834716797 norm:0.0059407539665699005 max memory_allocated 29231.052734375 
[2025-02-18 12:13:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:4.550362586975098 norm:0.0058420635759830475 max memory_allocated 29231.052734375 
[2025-02-18 12:14:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 12:14:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:5.0330119132995605 norm:0.01111020427197218 max memory_allocated 29231.240234375 
[2025-02-18 12:15:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:5.026465892791748 norm:0.007630581967532635 max memory_allocated 29231.240234375 
[2025-02-18 12:16:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:5.025442600250244 norm:0.005947523284703493 max memory_allocated 29231.240234375 
[2025-02-18 12:17:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:5.023911476135254 norm:0.004703748971223831 max memory_allocated 29231.240234375 
[2025-02-18 12:18:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:5.021906852722168 norm:0.003976469859480858 max memory_allocated 29231.240234375 
[2025-02-18 12:18:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:5.019835472106934 norm:0.0033780112862586975 max memory_allocated 29231.240234375 
[2025-02-18 12:19:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:5.01710319519043 norm:0.0030146087519824505 max memory_allocated 29231.240234375 
[2025-02-18 12:20:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:5.014547824859619 norm:0.002728493185713887 max memory_allocated 29231.240234375 
[2025-02-18 12:21:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:5.012748718261719 norm:0.0026046151760965586 max memory_allocated 29231.240234375 
[2025-02-18 12:22:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:5.010101318359375 norm:0.0024943288881331682 max memory_allocated 29231.240234375 
[2025-02-18 12:22:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:5.007746696472168 norm:0.0023710154928267 max memory_allocated 29231.240234375 
[2025-02-18 12:23:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:5.0052618980407715 norm:0.002279035048559308 max memory_allocated 29231.240234375 
[2025-02-18 12:24:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:5.003457546234131 norm:0.0022045685909688473 max memory_allocated 29231.240234375 
[2025-02-18 12:25:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:5.002790451049805 norm:0.002180163050070405 max memory_allocated 29231.240234375 
[2025-02-18 12:26:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:5.00221061706543 norm:0.0021705678664147854 max memory_allocated 29231.240234375 
[2025-02-18 12:26:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:5.001251697540283 norm:0.002157756360247731 max memory_allocated 29231.240234375 
[2025-02-18 12:27:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:5.00015926361084 norm:0.002144804922863841 max memory_allocated 29231.240234375 
[2025-02-18 12:28:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:4.998979091644287 norm:0.002148419851437211 max memory_allocated 29231.240234375 
[2025-02-18 12:29:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:4.998012542724609 norm:0.0021454410161823034 max memory_allocated 29231.240234375 
[2025-02-18 12:30:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:4.997272491455078 norm:0.002099327277392149 max memory_allocated 29231.240234375 
[2025-02-18 12:30:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 12:31:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:5.563539028167725 norm:0.015432557091116905 max memory_allocated 29231.427734375 
[2025-02-18 12:32:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:5.556385517120361 norm:0.011931434273719788 max memory_allocated 29231.427734375 
[2025-02-18 12:32:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:5.551518440246582 norm:0.01024832297116518 max memory_allocated 29231.427734375 
[2025-02-18 12:33:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:5.550219535827637 norm:0.009819895029067993 max memory_allocated 29231.427734375 
[2025-02-18 12:34:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:5.549615859985352 norm:0.009509813971817493 max memory_allocated 29231.427734375 
[2025-02-18 12:35:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:5.552756309509277 norm:0.009487861767411232 max memory_allocated 29231.427734375 
[2025-02-18 12:36:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:5.553377151489258 norm:0.00989234447479248 max memory_allocated 29231.427734375 
[2025-02-18 12:36:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:5.549191474914551 norm:0.009765321388840675 max memory_allocated 29231.427734375 
[2025-02-18 12:37:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:5.544895172119141 norm:0.009467808529734612 max memory_allocated 29231.427734375 
[2025-02-18 12:38:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:5.542481422424316 norm:0.009081839583814144 max memory_allocated 29231.427734375 
[2025-02-18 12:39:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:5.544441223144531 norm:0.009183544665575027 max memory_allocated 29231.427734375 
[2025-02-18 12:40:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:5.54633903503418 norm:0.009046261198818684 max memory_allocated 29231.427734375 
[2025-02-18 12:40:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:5.547313213348389 norm:0.009895418770611286 max memory_allocated 29231.427734375 
[2025-02-18 12:41:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:5.543426990509033 norm:0.010327614843845367 max memory_allocated 29231.427734375 
[2025-02-18 12:42:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:5.540637493133545 norm:0.010317598469555378 max memory_allocated 29231.427734375 
[2025-02-18 12:43:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:5.539797782897949 norm:0.010157182812690735 max memory_allocated 29231.427734375 
[2025-02-18 12:44:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:5.538015365600586 norm:0.010269138030707836 max memory_allocated 29231.427734375 
[2025-02-18 12:45:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:5.53642463684082 norm:0.010417691431939602 max memory_allocated 29231.427734375 
[2025-02-18 12:45:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:5.535140037536621 norm:0.010294247418642044 max memory_allocated 29231.427734375 
[2025-02-18 12:46:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:5.533286094665527 norm:0.010359350591897964 max memory_allocated 29231.427734375 
[2025-02-18 12:46:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 12:47:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:5.987844467163086 norm:0.013733149506151676 max memory_allocated 29231.615234375 
[2025-02-18 12:48:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:5.984549045562744 norm:0.008612960577011108 max memory_allocated 29231.615234375 
[2025-02-18 12:49:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:5.983107566833496 norm:0.006604390684515238 max memory_allocated 29231.615234375 
[2025-02-18 12:50:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:5.984213352203369 norm:0.005086828488856554 max memory_allocated 29231.615234375 
[2025-02-18 12:50:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:5.983941555023193 norm:0.004166122060269117 max memory_allocated 29231.615234375 
[2025-02-18 12:51:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:5.982091426849365 norm:0.003772271331399679 max memory_allocated 29231.615234375 
[2025-02-18 12:52:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:5.980953693389893 norm:0.003486023750156164 max memory_allocated 29231.615234375 
[2025-02-18 12:53:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:5.977992534637451 norm:0.0033058617264032364 max memory_allocated 29231.615234375 
[2025-02-18 12:54:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:5.975345611572266 norm:0.0032925265841186047 max memory_allocated 29231.615234375 
[2025-02-18 12:54:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:5.971673965454102 norm:0.003256760770455003 max memory_allocated 29231.615234375 
[2025-02-18 12:55:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:5.967667579650879 norm:0.004192760679870844 max memory_allocated 29231.615234375 
[2025-02-18 12:56:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:5.9631266593933105 norm:0.09014800190925598 max memory_allocated 29231.615234375 
[2025-02-18 12:57:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:5.963405609130859 norm:0.1282208263874054 max memory_allocated 29231.615234375 
[2025-02-18 12:58:12 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:5.951791763305664 norm:0.0726860836148262 max memory_allocated 29231.615234375 
[2025-02-18 12:59:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:5.947516918182373 norm:0.06903394311666489 max memory_allocated 29231.615234375 
[2025-02-18 12:59:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:5.94837760925293 norm:0.07817409187555313 max memory_allocated 29231.615234375 
[2025-02-18 13:00:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:5.944119453430176 norm:0.0768563374876976 max memory_allocated 29231.615234375 
[2025-02-18 13:01:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:5.93593692779541 norm:0.05566588416695595 max memory_allocated 29231.615234375 
[2025-02-18 13:02:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:5.931875705718994 norm:0.04469314217567444 max memory_allocated 29231.615234375 
[2025-02-18 13:03:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:5.930891036987305 norm:0.054731275886297226 max memory_allocated 29231.615234375 
[2025-02-18 13:03:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 13:04:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:6.393994331359863 norm:0.013132060877978802 max memory_allocated 29231.802734375 
[2025-02-18 13:04:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:6.3836750984191895 norm:0.009241743944585323 max memory_allocated 29231.802734375 
[2025-02-18 13:05:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:6.381309509277344 norm:0.007510045543313026 max memory_allocated 29231.802734375 
[2025-02-18 13:06:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:6.3807902336120605 norm:0.006161408498883247 max memory_allocated 29231.802734375 
[2025-02-18 13:07:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:6.379251003265381 norm:0.005414438433945179 max memory_allocated 29231.802734375 
[2025-02-18 13:08:11 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:6.377386569976807 norm:0.00473007233813405 max memory_allocated 29231.802734375 
[2025-02-18 13:08:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:6.375707149505615 norm:0.004215717315673828 max memory_allocated 29231.802734375 
[2025-02-18 13:09:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:6.374269962310791 norm:0.00383780081756413 max memory_allocated 29231.802734375 
[2025-02-18 13:10:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:6.372997760772705 norm:0.0036524657625705004 max memory_allocated 29231.802734375 
[2025-02-18 13:11:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:6.37126350402832 norm:0.003378380322828889 max memory_allocated 29231.802734375 
[2025-02-18 13:12:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:6.37012243270874 norm:0.0032792130950838327 max memory_allocated 29231.802734375 
[2025-02-18 13:13:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:6.369398593902588 norm:0.0031267660669982433 max memory_allocated 29231.802734375 
[2025-02-18 13:13:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:6.368566036224365 norm:0.0030195103026926517 max memory_allocated 29231.802734375 
[2025-02-18 13:14:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:6.368127822875977 norm:0.002950395690277219 max memory_allocated 29231.802734375 
[2025-02-18 13:15:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:6.367265701293945 norm:0.0028793374076485634 max memory_allocated 29231.802734375 
[2025-02-18 13:16:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:6.366185188293457 norm:0.002816393505781889 max memory_allocated 29231.802734375 
[2025-02-18 13:17:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:6.36523962020874 norm:0.0027517054695636034 max memory_allocated 29231.802734375 
[2025-02-18 13:17:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:6.364856243133545 norm:0.002719624899327755 max memory_allocated 29231.802734375 
[2025-02-18 13:18:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:6.364887237548828 norm:0.0026815184392035007 max memory_allocated 29231.802734375 
[2025-02-18 13:19:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:6.36500883102417 norm:0.0026628882624208927 max memory_allocated 29231.802734375 
[2025-02-18 13:19:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 13:20:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:6.848275661468506 norm:0.01065770536661148 max memory_allocated 29231.990234375 
[2025-02-18 13:21:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:6.8398942947387695 norm:0.0071130297146737576 max memory_allocated 29231.990234375 
[2025-02-18 13:22:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:6.835508346557617 norm:0.005403181072324514 max memory_allocated 29231.990234375 
[2025-02-18 13:23:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:6.832828521728516 norm:0.004221286159008741 max memory_allocated 29231.990234375 
[2025-02-18 13:23:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:6.831846237182617 norm:0.003463819157332182 max memory_allocated 29231.990234375 
[2025-02-18 13:24:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:6.831263065338135 norm:0.002917415928095579 max memory_allocated 29231.990234375 
[2025-02-18 13:25:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:6.830259323120117 norm:0.002666245214641094 max memory_allocated 29231.990234375 
[2025-02-18 13:26:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:6.8292765617370605 norm:0.0025802012532949448 max memory_allocated 29231.990234375 
[2025-02-18 13:27:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:6.827387809753418 norm:0.002585223177447915 max memory_allocated 29231.990234375 
[2025-02-18 13:27:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:6.826657295227051 norm:0.0025849887169897556 max memory_allocated 29231.990234375 
[2025-02-18 13:28:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:6.825249671936035 norm:0.002571272663772106 max memory_allocated 29231.990234375 
[2025-02-18 13:29:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:6.824562072753906 norm:0.0025529528502374887 max memory_allocated 29231.990234375 
[2025-02-18 13:30:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:6.823424816131592 norm:0.00256177200935781 max memory_allocated 29231.990234375 
[2025-02-18 13:31:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:6.8224101066589355 norm:0.002529237186536193 max memory_allocated 29231.990234375 
[2025-02-18 13:31:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:6.8214240074157715 norm:0.002546465490013361 max memory_allocated 29231.990234375 
[2025-02-18 13:32:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:6.820158004760742 norm:0.002532286336645484 max memory_allocated 29231.990234375 
[2025-02-18 13:33:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:6.820049285888672 norm:0.0025212515611201525 max memory_allocated 29231.990234375 
[2025-02-18 13:34:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:6.819714069366455 norm:0.0025416119024157524 max memory_allocated 29231.990234375 
[2025-02-18 13:35:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:6.819662094116211 norm:0.0025596097111701965 max memory_allocated 29231.990234375 
[2025-02-18 13:35:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:6.819493293762207 norm:0.0025711492635309696 max memory_allocated 29231.990234375 
[2025-02-18 13:36:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-18 13:37:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:7.274175643920898 norm:0.013191038742661476 max memory_allocated 29232.177734375 
[2025-02-18 13:37:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:7.2674336433410645 norm:0.009341055527329445 max memory_allocated 29232.177734375 
[2025-02-18 13:38:38 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:7.262560844421387 norm:0.007198695559054613 max memory_allocated 29232.177734375 
[2025-02-18 13:39:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:7.259333610534668 norm:0.006012423895299435 max memory_allocated 29232.177734375 
[2025-02-18 13:40:15 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:7.257242679595947 norm:0.005098999012261629 max memory_allocated 29232.177734375 
[2025-02-18 13:41:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:7.2549920082092285 norm:0.0043900227174162865 max memory_allocated 29232.177734375 
[2025-02-18 13:41:51 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:7.253340721130371 norm:0.003959274850785732 max memory_allocated 29232.177734375 
[2025-02-18 13:42:40 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:7.25143575668335 norm:0.003624032251536846 max memory_allocated 29232.177734375 
[2025-02-18 13:43:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:7.250004768371582 norm:0.0034103074576705694 max memory_allocated 29232.177734375 
[2025-02-18 13:44:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:7.248843193054199 norm:0.0032297989819198847 max memory_allocated 29232.177734375 
[2025-02-18 13:45:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:7.247801303863525 norm:0.0030976966954767704 max memory_allocated 29232.177734375 
[2025-02-18 13:45:53 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:7.247167587280273 norm:0.002982238307595253 max memory_allocated 29232.177734375 
[2025-02-18 13:46:41 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:7.2462477684021 norm:0.0028885381761938334 max memory_allocated 29232.177734375 
[2025-02-18 13:47:29 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:7.245800495147705 norm:0.0028234287165105343 max memory_allocated 29232.177734375 
[2025-02-18 13:48:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:7.24470329284668 norm:0.0027677163016051054 max memory_allocated 29232.177734375 
[2025-02-18 13:49:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:7.244358062744141 norm:0.0027492200024425983 max memory_allocated 29232.177734375 
[2025-02-18 13:49:54 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:7.24395751953125 norm:0.002726843347772956 max memory_allocated 29232.177734375 
[2025-02-18 13:50:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:7.243321418762207 norm:0.0027242167852818966 max memory_allocated 29232.177734375 
[2025-02-18 13:51:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:7.242674827575684 norm:0.002689564134925604 max memory_allocated 29232.177734375 
[2025-02-18 13:52:19 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:7.24249267578125 norm:0.0026960624381899834 max memory_allocated 29232.177734375 
[2025-02-18 13:52:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-18 13:53:26 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:7.7909650802612305 norm:0.011122975498437881 max memory_allocated 29232.365234375 
[2025-02-18 13:54:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:7.782282829284668 norm:0.007875955663621426 max memory_allocated 29232.365234375 
[2025-02-18 13:55:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:7.776691436767578 norm:0.006235492881387472 max memory_allocated 29232.365234375 
[2025-02-18 13:55:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:7.774474143981934 norm:0.005178411491215229 max memory_allocated 29232.365234375 
[2025-02-18 13:56:38 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:7.773050785064697 norm:0.00445896852761507 max memory_allocated 29232.365234375 
[2025-02-18 13:57:27 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:7.772434711456299 norm:0.003961872309446335 max memory_allocated 29232.365234375 
[2025-02-18 13:58:15 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:7.772127151489258 norm:0.0036333047319203615 max memory_allocated 29232.365234375 
[2025-02-18 13:59:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:7.771225929260254 norm:0.0032396831084042788 max memory_allocated 29232.365234375 
[2025-02-18 13:59:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:7.770750045776367 norm:0.0029749851673841476 max memory_allocated 29232.365234375 
[2025-02-18 14:00:41 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:7.7701873779296875 norm:0.0028057124000042677 max memory_allocated 29232.365234375 
[2025-02-18 14:01:29 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:7.7692484855651855 norm:0.0026817386969923973 max memory_allocated 29232.365234375 
[2025-02-18 14:02:17 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:7.768233299255371 norm:0.0025979136116802692 max memory_allocated 29232.365234375 
[2025-02-18 14:03:06 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:7.766474723815918 norm:0.0025057655293494463 max memory_allocated 29232.365234375 
[2025-02-18 14:03:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:7.765303611755371 norm:0.0024589705280959606 max memory_allocated 29232.365234375 
[2025-02-18 14:04:42 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:7.76506233215332 norm:0.0024490945506840944 max memory_allocated 29232.365234375 
[2025-02-18 14:05:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:7.764787673950195 norm:0.002405818784609437 max memory_allocated 29232.365234375 
[2025-02-18 14:06:19 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:7.764382362365723 norm:0.0023899113293737173 max memory_allocated 29232.365234375 
[2025-02-18 14:07:08 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:7.764089584350586 norm:0.0023866118863224983 max memory_allocated 29232.365234375 
[2025-02-18 14:07:56 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:7.764244079589844 norm:0.002392769558355212 max memory_allocated 29232.365234375 
[2025-02-18 14:08:44 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:7.76449728012085 norm:0.0023859073407948017 max memory_allocated 29232.365234375 
[2025-02-18 14:08:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-18 14:09:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:8.443185806274414 norm:0.025803493335843086 max memory_allocated 29232.552734375 
[2025-02-18 14:10:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:8.432924270629883 norm:0.019101930782198906 max memory_allocated 29232.552734375 
[2025-02-18 14:11:28 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:8.429893493652344 norm:0.015384208410978317 max memory_allocated 29232.552734375 
[2025-02-18 14:12:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:8.428437232971191 norm:0.01233545783907175 max memory_allocated 29232.552734375 
[2025-02-18 14:13:05 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:8.426726341247559 norm:0.010199964046478271 max memory_allocated 29232.552734375 
[2025-02-18 14:13:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:8.424344062805176 norm:0.008687710389494896 max memory_allocated 29232.552734375 
[2025-02-18 14:14:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:8.421527862548828 norm:0.007677930407226086 max memory_allocated 29232.552734375 
[2025-02-18 14:15:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:8.419427871704102 norm:0.006846826057881117 max memory_allocated 29232.552734375 
[2025-02-18 14:16:19 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:8.41701889038086 norm:0.0061941747553646564 max memory_allocated 29232.552734375 
[2025-02-18 14:17:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:8.41303825378418 norm:0.005593437235802412 max memory_allocated 29232.552734375 
[2025-02-18 14:17:57 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:8.409868240356445 norm:0.005125509575009346 max memory_allocated 29232.552734375 
[2025-02-18 14:18:45 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:8.406733512878418 norm:0.0047083087265491486 max memory_allocated 29232.552734375 
[2025-02-18 14:19:34 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:8.404425621032715 norm:0.004350040107965469 max memory_allocated 29232.552734375 
[2025-02-18 14:20:22 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:8.402766227722168 norm:0.004128331318497658 max memory_allocated 29232.552734375 
[2025-02-18 14:21:11 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:8.40176010131836 norm:0.003923817537724972 max memory_allocated 29232.552734375 
[2025-02-18 14:22:00 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:8.400277137756348 norm:0.0037429719232022762 max memory_allocated 29232.552734375 
[2025-02-18 14:22:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:8.399293899536133 norm:0.00360605726018548 max memory_allocated 29232.552734375 
[2025-02-18 14:23:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:8.39872932434082 norm:0.003517754375934601 max memory_allocated 29232.552734375 
[2025-02-18 14:24:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:8.397758483886719 norm:0.0034028920345008373 max memory_allocated 29232.552734375 
[2025-02-18 14:25:14 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:8.396547317504883 norm:0.0032671988010406494 max memory_allocated 29232.552734375 
[2025-02-18 14:25:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-18 14:26:21 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:9.096661567687988 norm:0.019573500379920006 max memory_allocated 29232.740234375 
[2025-02-18 14:27:10 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:9.085488319396973 norm:0.015130730345845222 max memory_allocated 29232.740234375 
[2025-02-18 14:27:58 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:9.080188751220703 norm:0.012274826876819134 max memory_allocated 29232.740234375 
[2025-02-18 14:28:47 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:9.076889991760254 norm:0.010388828814029694 max memory_allocated 29232.740234375 
[2025-02-18 14:29:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:9.075098991394043 norm:0.009242076426744461 max memory_allocated 29232.740234375 
[2025-02-18 14:30:24 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:9.068981170654297 norm:0.008096975274384022 max memory_allocated 29232.740234375 
[2025-02-18 14:31:12 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:9.064456939697266 norm:0.007294110953807831 max memory_allocated 29232.740234375 
[2025-02-18 14:32:01 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:9.061259269714355 norm:0.006578870117664337 max memory_allocated 29232.740234375 
[2025-02-18 14:32:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:9.055949211120605 norm:0.006082187872380018 max memory_allocated 29232.740234375 
[2025-02-18 14:33:38 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:9.052543640136719 norm:0.005692957900464535 max memory_allocated 29232.740234375 
[2025-02-18 14:34:27 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:9.048718452453613 norm:0.0052366009913384914 max memory_allocated 29232.740234375 
[2025-02-18 14:35:16 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:9.045753479003906 norm:0.004991666879504919 max memory_allocated 29232.740234375 
[2025-02-18 14:36:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:9.045743942260742 norm:0.004891643300652504 max memory_allocated 29232.740234375 
[2025-02-18 14:36:53 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:9.045909881591797 norm:0.004821183159947395 max memory_allocated 29232.740234375 
[2025-02-18 14:37:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:9.044076919555664 norm:0.004729044623672962 max memory_allocated 29232.740234375 
[2025-02-18 14:38:30 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:9.042316436767578 norm:0.004697400610893965 max memory_allocated 29232.740234375 
[2025-02-18 14:39:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:9.040276527404785 norm:0.004682991188019514 max memory_allocated 29232.740234375 
[2025-02-18 14:40:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:9.038442611694336 norm:0.004706459119915962 max memory_allocated 29232.740234375 
[2025-02-18 14:40:56 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:9.036258697509766 norm:0.004657966084778309 max memory_allocated 29232.740234375 
[2025-02-18 14:41:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:9.035238265991211 norm:0.0046075074933469296 max memory_allocated 29232.740234375 
[2025-02-18 14:41:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-18 14:42:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:9.820221900939941 norm:0.02120094746351242 max memory_allocated 29232.927734375 
[2025-02-18 14:43:40 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:9.80569839477539 norm:0.015894882380962372 max memory_allocated 29232.927734375 
[2025-02-18 14:44:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:9.798513412475586 norm:0.012916620820760727 max memory_allocated 29232.927734375 
[2025-02-18 14:45:17 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:9.791303634643555 norm:0.010540894232690334 max memory_allocated 29232.927734375 
[2025-02-18 14:46:05 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:9.78445053100586 norm:0.008821426890790462 max memory_allocated 29232.927734375 
[2025-02-18 14:46:54 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:9.779085159301758 norm:0.00760142644867301 max memory_allocated 29232.927734375 
[2025-02-18 14:47:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:9.774968147277832 norm:0.006597917061299086 max memory_allocated 29232.927734375 
[2025-02-18 14:48:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:9.770721435546875 norm:0.005914153065532446 max memory_allocated 29232.927734375 
[2025-02-18 14:49:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:9.767145156860352 norm:0.005399489775300026 max memory_allocated 29232.927734375 
[2025-02-18 14:50:08 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:9.764925003051758 norm:0.005034169182181358 max memory_allocated 29232.927734375 
[2025-02-18 14:50:57 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:9.76227855682373 norm:0.004742297809571028 max memory_allocated 29232.927734375 
[2025-02-18 14:51:45 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:9.76025390625 norm:0.0045580328442156315 max memory_allocated 29232.927734375 
[2025-02-18 14:52:34 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:9.758503913879395 norm:0.004379223566502333 max memory_allocated 29232.927734375 
[2025-02-18 14:53:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:9.756022453308105 norm:0.004262454807758331 max memory_allocated 29232.927734375 
[2025-02-18 14:54:11 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:9.75355052947998 norm:0.004113028757274151 max memory_allocated 29232.927734375 
[2025-02-18 14:54:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:9.75105094909668 norm:0.0039841970428824425 max memory_allocated 29232.927734375 
[2025-02-18 14:55:48 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:9.749189376831055 norm:0.003900847863405943 max memory_allocated 29232.927734375 
[2025-02-18 14:56:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:9.746543884277344 norm:0.0038051046431064606 max memory_allocated 29232.927734375 
[2025-02-18 14:57:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:9.74480152130127 norm:0.00373683194629848 max memory_allocated 29232.927734375 
[2025-02-18 14:58:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:9.743093490600586 norm:0.0036730796564370394 max memory_allocated 29232.927734375 
[2025-02-18 14:58:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-18 14:59:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:10.916704177856445 norm:0.04347657412290573 max memory_allocated 29233.115234375 
[2025-02-18 15:00:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:10.897560119628906 norm:0.030594000592827797 max memory_allocated 29233.115234375 
[2025-02-18 15:00:56 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:10.891288757324219 norm:0.023951927199959755 max memory_allocated 29233.115234375 
[2025-02-18 15:01:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:10.885655403137207 norm:0.019775399938225746 max memory_allocated 29233.115234375 
[2025-02-18 15:02:33 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:10.87890338897705 norm:0.016186833381652832 max memory_allocated 29233.115234375 
[2025-02-18 15:03:21 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:10.873689651489258 norm:0.013643682934343815 max memory_allocated 29233.115234375 
[2025-02-18 15:04:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:10.867305755615234 norm:0.011703883297741413 max memory_allocated 29233.115234375 
[2025-02-18 15:04:58 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:10.857728958129883 norm:0.010564465075731277 max memory_allocated 29233.115234375 
[2025-02-18 15:05:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:10.848097801208496 norm:0.009302584454417229 max memory_allocated 29233.115234375 
[2025-02-18 15:06:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:10.843755722045898 norm:0.008618584834039211 max memory_allocated 29233.115234375 
[2025-02-18 15:07:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:10.837867736816406 norm:0.008247485384345055 max memory_allocated 29233.115234375 
[2025-02-18 15:08:11 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:10.832897186279297 norm:0.007792509160935879 max memory_allocated 29233.115234375 
[2025-02-18 15:08:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:10.82888126373291 norm:0.007446350529789925 max memory_allocated 29233.115234375 
[2025-02-18 15:09:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:10.82583999633789 norm:0.007201709318906069 max memory_allocated 29233.115234375 
[2025-02-18 15:10:36 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:10.822071075439453 norm:0.0070413993671536446 max memory_allocated 29233.115234375 
[2025-02-18 15:11:25 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:10.818782806396484 norm:0.006830793339759111 max memory_allocated 29233.115234375 
[2025-02-18 15:12:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:10.815268516540527 norm:0.006694127805531025 max memory_allocated 29233.115234375 
[2025-02-18 15:13:01 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:10.813067436218262 norm:0.006642922293394804 max memory_allocated 29233.115234375 
[2025-02-18 15:13:49 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:10.81151008605957 norm:0.006608185358345509 max memory_allocated 29233.115234375 
[2025-02-18 15:14:38 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:10.810816764831543 norm:0.006571823265403509 max memory_allocated 29233.115234375 
[2025-02-18 15:14:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-18 15:15:44 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:15.922399520874023 norm:0.5899612307548523 max memory_allocated 29233.302734375 
[2025-02-18 15:16:32 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:15.912732124328613 norm:0.5423331260681152 max memory_allocated 29233.302734375 
[2025-02-18 15:17:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:15.889694213867188 norm:0.4854314923286438 max memory_allocated 29233.302734375 
[2025-02-18 15:18:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:15.910058975219727 norm:0.4606999158859253 max memory_allocated 29233.302734375 
[2025-02-18 15:18:57 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:14.287675857543945 norm:4.299648761749268 max memory_allocated 29233.302734375 
[2025-02-18 15:19:45 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:14.05721664428711 norm:3.187014579772949 max memory_allocated 29233.302734375 
[2025-02-18 15:20:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:13.982138633728027 norm:2.622377634048462 max memory_allocated 29233.302734375 
[2025-02-18 15:21:22 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:13.936513900756836 norm:1.7734137773513794 max memory_allocated 29233.302734375 
[2025-02-18 15:22:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:13.889201164245605 norm:1.7836962938308716 max memory_allocated 29233.302734375 
[2025-02-18 15:22:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:13.838750839233398 norm:1.2818520069122314 max memory_allocated 29233.302734375 
[2025-02-18 15:23:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:13.789270401000977 norm:0.6721867322921753 max memory_allocated 29233.302734375 
[2025-02-18 15:24:35 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:13.791786193847656 norm:0.6627490520477295 max memory_allocated 29233.302734375 
[2025-02-18 15:25:23 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:13.755966186523438 norm:0.607392430305481 max memory_allocated 29233.302734375 
[2025-02-18 15:26:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:13.784289360046387 norm:0.7339719533920288 max memory_allocated 29233.302734375 
[2025-02-18 15:26:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:13.817400932312012 norm:2.2247509956359863 max memory_allocated 29233.302734375 
[2025-02-18 15:27:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:13.828725814819336 norm:5.662530899047852 max memory_allocated 29233.302734375 
[2025-02-18 15:28:35 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:13.837409019470215 norm:12.456884384155273 max memory_allocated 29233.302734375 
[2025-02-18 15:29:23 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:13.854622840881348 norm:19.037368774414062 max memory_allocated 29233.302734375 
[2025-02-18 15:30:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:13.814912796020508 norm:16.385602951049805 max memory_allocated 29233.302734375 
[2025-02-18 15:30:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:13.80247688293457 norm:17.65879249572754 max memory_allocated 29233.302734375 
[2025-02-18 15:31:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-18 15:32:05 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:35.680999755859375 norm:0.24168168008327484 max memory_allocated 29233.490234375 
[2025-02-18 15:32:53 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:35.60757064819336 norm:0.6112450957298279 max memory_allocated 29233.490234375 
[2025-02-18 15:33:42 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:33.37885284423828 norm:30.945341110229492 max memory_allocated 29233.490234375 
[2025-02-18 15:34:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:33.27509689331055 norm:57.52579879760742 max memory_allocated 29233.490234375 
[2025-02-18 15:35:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:33.48764419555664 norm:91.080322265625 max memory_allocated 29233.490234375 
[2025-02-18 15:36:07 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:33.32170104980469 norm:127.52371215820312 max memory_allocated 29233.490234375 
[2025-02-18 15:36:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:33.34734344482422 norm:139.64410400390625 max memory_allocated 29233.490234375 
[2025-02-18 15:37:43 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:33.61821746826172 norm:139.82530212402344 max memory_allocated 29233.490234375 
[2025-02-18 15:38:31 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:34.017757415771484 norm:137.0243377685547 max memory_allocated 29233.490234375 
[2025-02-18 15:39:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:34.38706970214844 norm:210.82200622558594 max memory_allocated 29233.490234375 
[2025-02-18 15:40:07 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:34.48633575439453 norm:205.323486328125 max memory_allocated 29233.490234375 
[2025-02-18 15:40:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:34.32608413696289 norm:201.384521484375 max memory_allocated 29233.490234375 
[2025-02-18 15:41:42 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:34.09235382080078 norm:190.13365173339844 max memory_allocated 29233.490234375 
[2025-02-18 15:42:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:34.079830169677734 norm:190.8455047607422 max memory_allocated 29233.490234375 
[2025-02-18 15:43:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:34.079402923583984 norm:200.52149963378906 max memory_allocated 29233.490234375 
[2025-02-18 15:44:06 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:34.07012176513672 norm:200.798583984375 max memory_allocated 29233.490234375 
[2025-02-18 15:44:54 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:34.0463752746582 norm:217.93472290039062 max memory_allocated 29233.490234375 
[2025-02-18 15:45:42 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:34.00674819946289 norm:213.47560119628906 max memory_allocated 29233.490234375 
[2025-02-18 15:46:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:33.94623947143555 norm:224.16055297851562 max memory_allocated 29233.490234375 
[2025-02-18 15:47:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:33.880027770996094 norm:244.57723999023438 max memory_allocated 29233.490234375 
[2025-02-18 15:47:32 root] (main_calibration.py 365): INFO 39496.15210843086
[2025-02-18 15:48:53 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-18 15:50:45 root] (main_calibration.py 158): INFO wikitext2 : 22.668493270874023
[2025-02-18 15:50:45 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-18 15:53:40 root] (main_calibration.py 158): INFO c4 : 35.47365951538086
[2025-02-18 17:58:37 root] (main_calibration.py 169): INFO {'wikitext2': 22.668493270874023, 'c4': 35.47365951538086, 'results': {'hellaswag': {'acc': 0.3377813184624577, 'acc_stderr': 0.004719870074967229, 'acc_norm': 0.4253136825333599, 'acc_norm_stderr': 0.004933800927560527}, 'winogrande': {'acc': 0.5035516969218626, 'acc_stderr': 0.014052131146915857}, 'piqa': {'acc': 0.6142546245919478, 'acc_stderr': 0.011357166777524042, 'acc_norm': 0.6006528835690969, 'acc_norm_stderr': 0.011427006685027254}, 'boolq': {'acc': 0.6180428134556575, 'acc_stderr': 0.00849785199842719}, 'arc_easy': {'acc': 0.38425925925925924, 'acc_stderr': 0.009981120724601441, 'acc_norm': 0.3569023569023569, 'acc_norm_stderr': 0.009830630210347009}, 'arc_challenge': {'acc': 0.2363481228668942, 'acc_stderr': 0.012414960524301848, 'acc_norm': 0.2781569965870307, 'acc_norm_stderr': 0.013094469919538805}}, 'versions': {'hellaswag': 0, 'winogrande': 0, 'piqa': 0, 'boolq': 1, 'arc_easy': 0, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
