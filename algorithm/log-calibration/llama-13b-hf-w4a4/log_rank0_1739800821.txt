[2025-02-17 14:00:21 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration/llama-13b-hf-w4a4', save_dir='./log-calibration/quant/llama-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-17 14:07:02 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-17 14:07:02 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-17 14:07:03 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-17 14:07:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-17 14:07:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.17120948433876038 norm:0.037139054387807846 max memory_allocated 29226.177734375 
[2025-02-17 14:08:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.13870495557785034 norm:0.03961611166596413 max memory_allocated 29226.177734375 
[2025-02-17 14:09:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.12885437905788422 norm:0.054760776460170746 max memory_allocated 29226.177734375 
[2025-02-17 14:10:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.11649623513221741 norm:0.04743838682770729 max memory_allocated 29226.177734375 
[2025-02-17 14:10:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.11896053701639175 norm:0.05304386466741562 max memory_allocated 29226.177734375 
[2025-02-17 14:11:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.09902387112379074 norm:0.023719722405076027 max memory_allocated 29226.177734375 
[2025-02-17 14:12:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.10371744632720947 norm:0.032009877264499664 max memory_allocated 29226.177734375 
[2025-02-17 14:13:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.1116870865225792 norm:0.04150323197245598 max memory_allocated 29226.177734375 
[2025-02-17 14:13:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.11575616896152496 norm:0.048836495727300644 max memory_allocated 29226.177734375 
[2025-02-17 14:14:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.10636875033378601 norm:0.039303697645664215 max memory_allocated 29226.177734375 
[2025-02-17 14:15:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.09498874843120575 norm:0.02639274299144745 max memory_allocated 29226.177734375 
[2025-02-17 14:16:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.09441269189119339 norm:0.02780676819384098 max memory_allocated 29226.177734375 
[2025-02-17 14:16:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.09286794066429138 norm:0.023846641182899475 max memory_allocated 29226.177734375 
[2025-02-17 14:17:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.09191309660673141 norm:0.024986546486616135 max memory_allocated 29226.177734375 
[2025-02-17 14:18:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.09583333134651184 norm:0.02699725516140461 max memory_allocated 29226.177734375 
[2025-02-17 14:19:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.10123597830533981 norm:0.03331295773386955 max memory_allocated 29226.177734375 
[2025-02-17 14:19:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.10271008312702179 norm:0.03365523740649223 max memory_allocated 29226.177734375 
[2025-02-17 14:20:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.1026526391506195 norm:0.033842116594314575 max memory_allocated 29226.177734375 
[2025-02-17 14:21:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.10115798562765121 norm:0.03191474825143814 max memory_allocated 29226.177734375 
[2025-02-17 14:22:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.10097742080688477 norm:0.032949626445770264 max memory_allocated 29226.177734375 
[2025-02-17 14:22:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-17 14:23:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.3448016941547394 norm:0.04723697900772095 max memory_allocated 29226.365234375 
[2025-02-17 14:23:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.3034960627555847 norm:0.035075996071100235 max memory_allocated 29226.365234375 
[2025-02-17 14:24:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.31372565031051636 norm:0.041758906096220016 max memory_allocated 29226.365234375 
[2025-02-17 14:25:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.29641780257225037 norm:0.04098820313811302 max memory_allocated 29226.365234375 
[2025-02-17 14:26:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.25976991653442383 norm:0.02797377109527588 max memory_allocated 29226.365234375 
[2025-02-17 14:26:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.25436338782310486 norm:0.024182777851819992 max memory_allocated 29226.365234375 
[2025-02-17 14:27:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.2448301613330841 norm:0.023365922272205353 max memory_allocated 29226.365234375 
[2025-02-17 14:28:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.24029874801635742 norm:0.022550484165549278 max memory_allocated 29226.365234375 
[2025-02-17 14:28:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.24736261367797852 norm:0.024399222806096077 max memory_allocated 29226.365234375 
[2025-02-17 14:29:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.24185413122177124 norm:0.023303434252738953 max memory_allocated 29226.365234375 
[2025-02-17 14:30:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.2319599688053131 norm:0.01983478106558323 max memory_allocated 29226.365234375 
[2025-02-17 14:31:13 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.22923599183559418 norm:0.019535481929779053 max memory_allocated 29226.365234375 
[2025-02-17 14:31:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.2298998385667801 norm:0.02001974545419216 max memory_allocated 29226.365234375 
[2025-02-17 14:32:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.22866353392601013 norm:0.020063668489456177 max memory_allocated 29226.365234375 
[2025-02-17 14:33:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.2229117602109909 norm:0.018080947920680046 max memory_allocated 29226.365234375 
[2025-02-17 14:34:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.22963593900203705 norm:0.020382875576615334 max memory_allocated 29226.365234375 
[2025-02-17 14:34:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.23337893187999725 norm:0.02081093192100525 max memory_allocated 29226.365234375 
[2025-02-17 14:35:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.2360798716545105 norm:0.021228190511465073 max memory_allocated 29226.365234375 
[2025-02-17 14:36:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.23561587929725647 norm:0.02115762233734131 max memory_allocated 29226.365234375 
[2025-02-17 14:37:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.22559888660907745 norm:0.018832899630069733 max memory_allocated 29226.365234375 
[2025-02-17 14:37:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-17 14:38:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.4810073971748352 norm:0.04158193990588188 max memory_allocated 29226.552734375 
[2025-02-17 14:39:04 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.4371650815010071 norm:0.023697610944509506 max memory_allocated 29226.552734375 
[2025-02-17 14:39:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.4131382405757904 norm:0.018758708611130714 max memory_allocated 29226.552734375 
[2025-02-17 14:40:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.40190351009368896 norm:0.016388557851314545 max memory_allocated 29226.552734375 
[2025-02-17 14:41:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.3889122009277344 norm:0.013553861528635025 max memory_allocated 29226.552734375 
[2025-02-17 14:42:02 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.3847055435180664 norm:0.013549166731536388 max memory_allocated 29226.552734375 
[2025-02-17 14:42:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.3816165328025818 norm:0.01381455734372139 max memory_allocated 29226.552734375 
[2025-02-17 14:43:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.378887414932251 norm:0.014632916077971458 max memory_allocated 29226.552734375 
[2025-02-17 14:44:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.3783430755138397 norm:0.013569511473178864 max memory_allocated 29226.552734375 
[2025-02-17 14:45:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.37735506892204285 norm:0.013618464581668377 max memory_allocated 29226.552734375 
[2025-02-17 14:45:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.3769569993019104 norm:0.014409139752388 max memory_allocated 29226.552734375 
[2025-02-17 14:46:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.376558393239975 norm:0.013227427378296852 max memory_allocated 29226.552734375 
[2025-02-17 14:47:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.3773362338542938 norm:0.014070920646190643 max memory_allocated 29226.552734375 
[2025-02-17 14:47:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.37854933738708496 norm:0.014398669824004173 max memory_allocated 29226.552734375 
[2025-02-17 14:48:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.376982182264328 norm:0.014198089018464088 max memory_allocated 29226.552734375 
[2025-02-17 14:49:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.37636321783065796 norm:0.013507384806871414 max memory_allocated 29226.552734375 
[2025-02-17 14:50:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.3754793405532837 norm:0.01492225006222725 max memory_allocated 29226.552734375 
[2025-02-17 14:50:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.37527310848236084 norm:0.013886535540223122 max memory_allocated 29226.552734375 
[2025-02-17 14:51:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.3743602931499481 norm:0.013834921643137932 max memory_allocated 29226.552734375 
[2025-02-17 14:52:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.3754725754261017 norm:0.014274870976805687 max memory_allocated 29226.552734375 
[2025-02-17 14:52:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-17 14:53:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.535527229309082 norm:0.05559030547738075 max memory_allocated 29226.740234375 
[2025-02-17 14:54:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.5110102891921997 norm:0.02856041118502617 max memory_allocated 29226.740234375 
[2025-02-17 14:54:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.501314640045166 norm:0.01913733221590519 max memory_allocated 29226.740234375 
[2025-02-17 14:55:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.4945628046989441 norm:0.013708651065826416 max memory_allocated 29226.740234375 
[2025-02-17 14:56:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.48764002323150635 norm:0.010636288672685623 max memory_allocated 29226.740234375 
[2025-02-17 14:57:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.4851321280002594 norm:0.009179436601698399 max memory_allocated 29226.740234375 
[2025-02-17 14:57:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.4836054742336273 norm:0.008076571859419346 max memory_allocated 29226.740234375 
[2025-02-17 14:58:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.47650429606437683 norm:0.006644099485129118 max memory_allocated 29226.740234375 
[2025-02-17 14:59:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.4694982171058655 norm:0.005936050321906805 max memory_allocated 29226.740234375 
[2025-02-17 15:00:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.46639305353164673 norm:0.005577596835792065 max memory_allocated 29226.740234375 
[2025-02-17 15:00:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.4639509618282318 norm:0.005235168617218733 max memory_allocated 29226.740234375 
[2025-02-17 15:01:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.4632771909236908 norm:0.00521835358813405 max memory_allocated 29226.740234375 
[2025-02-17 15:02:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.4621727168560028 norm:0.005098073277622461 max memory_allocated 29226.740234375 
[2025-02-17 15:03:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.4622546136379242 norm:0.004722269251942635 max memory_allocated 29226.740234375 
[2025-02-17 15:03:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.46254074573516846 norm:0.00473899208009243 max memory_allocated 29226.740234375 
[2025-02-17 15:04:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.46202343702316284 norm:0.004790318198502064 max memory_allocated 29226.740234375 
[2025-02-17 15:05:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.46177050471305847 norm:0.004662604071199894 max memory_allocated 29226.740234375 
[2025-02-17 15:06:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.46207860112190247 norm:0.00452549010515213 max memory_allocated 29226.740234375 
[2025-02-17 15:06:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.46137019991874695 norm:0.004577967803925276 max memory_allocated 29226.740234375 
[2025-02-17 15:07:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.46127793192863464 norm:0.004443011246621609 max memory_allocated 29226.740234375 
[2025-02-17 15:07:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-17 15:08:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.6140996813774109 norm:0.05029122531414032 max memory_allocated 29226.927734375 
[2025-02-17 15:09:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.5898730754852295 norm:0.018650326877832413 max memory_allocated 29226.927734375 
[2025-02-17 15:10:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.5806781649589539 norm:0.00969067681580782 max memory_allocated 29226.927734375 
[2025-02-17 15:10:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.577003002166748 norm:0.007233503274619579 max memory_allocated 29226.927734375 
[2025-02-17 15:11:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.5738487839698792 norm:0.005964571610093117 max memory_allocated 29226.927734375 
[2025-02-17 15:12:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.5715401768684387 norm:0.005376412533223629 max memory_allocated 29226.927734375 
[2025-02-17 15:13:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.5702194571495056 norm:0.0047567617148160934 max memory_allocated 29226.927734375 
[2025-02-17 15:13:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.5697551965713501 norm:0.004619930870831013 max memory_allocated 29226.927734375 
[2025-02-17 15:14:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.5690166354179382 norm:0.004404891282320023 max memory_allocated 29226.927734375 
[2025-02-17 15:15:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.5672463178634644 norm:0.004267075099050999 max memory_allocated 29226.927734375 
[2025-02-17 15:15:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.5662623643875122 norm:0.004131599795073271 max memory_allocated 29226.927734375 
[2025-02-17 15:16:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.5654541850090027 norm:0.004022381734102964 max memory_allocated 29226.927734375 
[2025-02-17 15:17:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.5645427703857422 norm:0.003998199477791786 max memory_allocated 29226.927734375 
[2025-02-17 15:18:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.5637159943580627 norm:0.003949130419641733 max memory_allocated 29226.927734375 
[2025-02-17 15:18:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.5632264614105225 norm:0.0038666732143610716 max memory_allocated 29226.927734375 
[2025-02-17 15:19:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.5629881620407104 norm:0.0039030194748193026 max memory_allocated 29226.927734375 
[2025-02-17 15:20:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.5629281997680664 norm:0.003809613874182105 max memory_allocated 29226.927734375 
[2025-02-17 15:21:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.5631281733512878 norm:0.0038327837828546762 max memory_allocated 29226.927734375 
[2025-02-17 15:21:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.5630465745925903 norm:0.0038781319744884968 max memory_allocated 29226.927734375 
[2025-02-17 15:22:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.5630841851234436 norm:0.0037882723845541477 max memory_allocated 29226.927734375 
[2025-02-17 15:22:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-17 15:23:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.7430976629257202 norm:0.0778728723526001 max memory_allocated 29227.115234375 
[2025-02-17 15:24:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.7085241079330444 norm:0.032878439873456955 max memory_allocated 29227.115234375 
[2025-02-17 15:25:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.6972454786300659 norm:0.01806146465241909 max memory_allocated 29227.115234375 
[2025-02-17 15:25:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.6975836753845215 norm:0.014839071780443192 max memory_allocated 29227.115234375 
[2025-02-17 15:26:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.6912477016448975 norm:0.011161468923091888 max memory_allocated 29227.115234375 
[2025-02-17 15:27:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.6856457591056824 norm:0.009144878946244717 max memory_allocated 29227.115234375 
[2025-02-17 15:28:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.6839759945869446 norm:0.00801634881645441 max memory_allocated 29227.115234375 
[2025-02-17 15:28:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.6859201788902283 norm:0.007973660714924335 max memory_allocated 29227.115234375 
[2025-02-17 15:29:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.6844442486763 norm:0.007940918207168579 max memory_allocated 29227.115234375 
[2025-02-17 15:30:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.6835264563560486 norm:0.007775392383337021 max memory_allocated 29227.115234375 
[2025-02-17 15:31:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.6771435737609863 norm:0.006510908715426922 max memory_allocated 29227.115234375 
[2025-02-17 15:31:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.6755731701850891 norm:0.006209774874150753 max memory_allocated 29227.115234375 
[2025-02-17 15:32:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.6752405166625977 norm:0.005968823097646236 max memory_allocated 29227.115234375 
[2025-02-17 15:33:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.6753854751586914 norm:0.006001731846481562 max memory_allocated 29227.115234375 
[2025-02-17 15:34:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.6756505966186523 norm:0.00596286915242672 max memory_allocated 29227.115234375 
[2025-02-17 15:34:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.6754209399223328 norm:0.005771588999778032 max memory_allocated 29227.115234375 
[2025-02-17 15:35:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.6746053099632263 norm:0.005679837428033352 max memory_allocated 29227.115234375 
[2025-02-17 15:36:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.6736544966697693 norm:0.00564115634188056 max memory_allocated 29227.115234375 
[2025-02-17 15:37:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.6733691692352295 norm:0.005646849051117897 max memory_allocated 29227.115234375 
[2025-02-17 15:37:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.6722668409347534 norm:0.005573306232690811 max memory_allocated 29227.115234375 
[2025-02-17 15:37:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-17 15:38:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:1.2075453996658325 norm:0.020590417087078094 max memory_allocated 29227.302734375 
[2025-02-17 15:39:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:1.1854020357131958 norm:0.014855621382594109 max memory_allocated 29227.302734375 
[2025-02-17 15:40:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:1.1744505167007446 norm:0.016857776790857315 max memory_allocated 29227.302734375 
[2025-02-17 15:40:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:1.1487293243408203 norm:0.15670864284038544 max memory_allocated 29227.302734375 
[2025-02-17 15:41:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:1.124127745628357 norm:0.2722190022468567 max memory_allocated 29227.302734375 
[2025-02-17 15:42:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:1.1124746799468994 norm:0.2791995108127594 max memory_allocated 29227.302734375 
[2025-02-17 15:43:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:1.1091759204864502 norm:0.21812525391578674 max memory_allocated 29227.302734375 
[2025-02-17 15:43:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:1.1057775020599365 norm:0.19261547923088074 max memory_allocated 29227.302734375 
[2025-02-17 15:44:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:1.103469729423523 norm:0.16298562288284302 max memory_allocated 29227.302734375 
[2025-02-17 15:45:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:1.102095365524292 norm:0.14190158247947693 max memory_allocated 29227.302734375 
[2025-02-17 15:46:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:1.1014317274093628 norm:0.1298293024301529 max memory_allocated 29227.302734375 
[2025-02-17 15:46:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:1.102080225944519 norm:0.14226005971431732 max memory_allocated 29227.302734375 
[2025-02-17 15:47:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:1.1005067825317383 norm:0.1336936354637146 max memory_allocated 29227.302734375 
[2025-02-17 15:48:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:1.0982284545898438 norm:0.12241338193416595 max memory_allocated 29227.302734375 
[2025-02-17 15:49:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:1.097420334815979 norm:0.12504902482032776 max memory_allocated 29227.302734375 
[2025-02-17 15:49:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:1.0962541103363037 norm:0.12594932317733765 max memory_allocated 29227.302734375 
[2025-02-17 15:50:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:1.09683358669281 norm:0.12373974919319153 max memory_allocated 29227.302734375 
[2025-02-17 15:51:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:1.096081256866455 norm:0.12203242629766464 max memory_allocated 29227.302734375 
[2025-02-17 15:52:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:1.0960986614227295 norm:0.12386103719472885 max memory_allocated 29227.302734375 
[2025-02-17 15:52:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:1.097695231437683 norm:0.12751853466033936 max memory_allocated 29227.302734375 
[2025-02-17 15:53:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-17 15:53:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:1.177956223487854 norm:0.025609154254198074 max memory_allocated 29227.490234375 
[2025-02-17 15:54:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:1.1595113277435303 norm:0.014780828729271889 max memory_allocated 29227.490234375 
[2025-02-17 15:55:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:1.1473170518875122 norm:0.008065625093877316 max memory_allocated 29227.490234375 
[2025-02-17 15:56:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:1.141486406326294 norm:0.0054216571152210236 max memory_allocated 29227.490234375 
[2025-02-17 15:56:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:1.1364320516586304 norm:0.004465154372155666 max memory_allocated 29227.490234375 
[2025-02-17 15:57:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:1.133493423461914 norm:0.0040540010668337345 max memory_allocated 29227.490234375 
[2025-02-17 15:58:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:1.1330714225769043 norm:0.003906916361302137 max memory_allocated 29227.490234375 
[2025-02-17 15:59:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:1.1317224502563477 norm:0.0037923192139714956 max memory_allocated 29227.490234375 
[2025-02-17 15:59:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:1.129833459854126 norm:0.00364820659160614 max memory_allocated 29227.490234375 
[2025-02-17 16:00:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:1.1297852993011475 norm:0.003647067118436098 max memory_allocated 29227.490234375 
[2025-02-17 16:01:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:1.1301100254058838 norm:0.00363302044570446 max memory_allocated 29227.490234375 
[2025-02-17 16:02:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:1.129392147064209 norm:0.00354968779720366 max memory_allocated 29227.490234375 
[2025-02-17 16:02:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:1.1279935836791992 norm:0.0034108676481992006 max memory_allocated 29227.490234375 
[2025-02-17 16:03:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:1.1275560855865479 norm:0.00337938847951591 max memory_allocated 29227.490234375 
[2025-02-17 16:04:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:1.1271378993988037 norm:0.0034007406793534756 max memory_allocated 29227.490234375 
[2025-02-17 16:05:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:1.1275205612182617 norm:0.0034130625426769257 max memory_allocated 29227.490234375 
[2025-02-17 16:05:45 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:1.1280876398086548 norm:0.003500940278172493 max memory_allocated 29227.490234375 
[2025-02-17 16:06:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:1.1274404525756836 norm:0.0034423305187374353 max memory_allocated 29227.490234375 
[2025-02-17 16:07:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:1.1292213201522827 norm:0.003567166393622756 max memory_allocated 29227.490234375 
[2025-02-17 16:07:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:1.1298109292984009 norm:0.0036273752339184284 max memory_allocated 29227.490234375 
[2025-02-17 16:08:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-17 16:08:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:1.2737267017364502 norm:0.025430338457226753 max memory_allocated 29227.677734375 
[2025-02-17 16:09:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:1.260227084159851 norm:0.01649153232574463 max memory_allocated 29227.677734375 
[2025-02-17 16:10:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:1.2519124746322632 norm:0.010568371042609215 max memory_allocated 29227.677734375 
[2025-02-17 16:11:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:1.2500537633895874 norm:0.007906458340585232 max memory_allocated 29227.677734375 
[2025-02-17 16:11:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:1.2455496788024902 norm:0.006875187158584595 max memory_allocated 29227.677734375 
[2025-02-17 16:12:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:1.2384618520736694 norm:0.006134542636573315 max memory_allocated 29227.677734375 
[2025-02-17 16:13:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:1.235902190208435 norm:0.0059129721485078335 max memory_allocated 29227.677734375 
[2025-02-17 16:14:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:1.2368146181106567 norm:0.006001011002808809 max memory_allocated 29227.677734375 
[2025-02-17 16:14:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:1.2341163158416748 norm:0.005866420920938253 max memory_allocated 29227.677734375 
[2025-02-17 16:15:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:1.2334216833114624 norm:0.0057913027703762054 max memory_allocated 29227.677734375 
[2025-02-17 16:16:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:1.2330777645111084 norm:0.005764467641711235 max memory_allocated 29227.677734375 
[2025-02-17 16:17:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:1.2310128211975098 norm:0.005654636770486832 max memory_allocated 29227.677734375 
[2025-02-17 16:17:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:1.228017807006836 norm:0.00553441047668457 max memory_allocated 29227.677734375 
[2025-02-17 16:18:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:1.2265368700027466 norm:0.005495600402355194 max memory_allocated 29227.677734375 
[2025-02-17 16:19:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:1.2255483865737915 norm:0.0054116323590278625 max memory_allocated 29227.677734375 
[2025-02-17 16:20:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:1.225377082824707 norm:0.005390124395489693 max memory_allocated 29227.677734375 
[2025-02-17 16:20:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:1.2240898609161377 norm:0.005307990126311779 max memory_allocated 29227.677734375 
[2025-02-17 16:21:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:1.223828673362732 norm:0.005330491345375776 max memory_allocated 29227.677734375 
[2025-02-17 16:22:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:1.223907232284546 norm:0.005357737652957439 max memory_allocated 29227.677734375 
[2025-02-17 16:23:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:1.2228879928588867 norm:0.005322990473359823 max memory_allocated 29227.677734375 
[2025-02-17 16:23:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-17 16:24:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:1.367817997932434 norm:0.03376170992851257 max memory_allocated 29227.865234375 
[2025-02-17 16:24:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:1.3544005155563354 norm:0.01906641758978367 max memory_allocated 29227.865234375 
[2025-02-17 16:25:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:1.3522999286651611 norm:0.011177930049598217 max memory_allocated 29227.865234375 
[2025-02-17 16:26:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:1.345637321472168 norm:0.006837888620793819 max memory_allocated 29227.865234375 
[2025-02-17 16:27:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:1.3406929969787598 norm:0.005549617577344179 max memory_allocated 29227.865234375 
[2025-02-17 16:27:48 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:1.3395646810531616 norm:0.0052542611956596375 max memory_allocated 29227.865234375 
[2025-02-17 16:28:33 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:1.3371959924697876 norm:0.004900315776467323 max memory_allocated 29227.865234375 
[2025-02-17 16:29:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:1.3375929594039917 norm:0.004869128577411175 max memory_allocated 29227.865234375 
[2025-02-17 16:30:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:1.337616205215454 norm:0.004870484117418528 max memory_allocated 29227.865234375 
[2025-02-17 16:30:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:1.3367730379104614 norm:0.004709948785603046 max memory_allocated 29227.865234375 
[2025-02-17 16:31:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:1.340317726135254 norm:0.004843499511480331 max memory_allocated 29227.865234375 
[2025-02-17 16:32:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:1.3388471603393555 norm:0.004745530895888805 max memory_allocated 29227.865234375 
[2025-02-17 16:33:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:1.3379919528961182 norm:0.004685990046709776 max memory_allocated 29227.865234375 
[2025-02-17 16:33:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:1.3360272645950317 norm:0.004563541151583195 max memory_allocated 29227.865234375 
[2025-02-17 16:34:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:1.334054708480835 norm:0.004581672139465809 max memory_allocated 29227.865234375 
[2025-02-17 16:35:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:1.332606315612793 norm:0.0046020252630114555 max memory_allocated 29227.865234375 
[2025-02-17 16:35:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:1.3336237668991089 norm:0.004518828820437193 max memory_allocated 29227.865234375 
[2025-02-17 16:36:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:1.3334821462631226 norm:0.004499970469623804 max memory_allocated 29227.865234375 
[2025-02-17 16:37:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:1.333336591720581 norm:0.004462468437850475 max memory_allocated 29227.865234375 
[2025-02-17 16:38:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:1.3332419395446777 norm:0.004436437971889973 max memory_allocated 29227.865234375 
[2025-02-17 16:38:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-17 16:39:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:1.467200756072998 norm:0.05411314219236374 max memory_allocated 29228.052734375 
[2025-02-17 16:39:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:1.450557827949524 norm:0.030186915770173073 max memory_allocated 29228.052734375 
[2025-02-17 16:40:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:1.447378158569336 norm:0.01932624541223049 max memory_allocated 29228.052734375 
[2025-02-17 16:41:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:1.4447355270385742 norm:0.012128139846026897 max memory_allocated 29228.052734375 
[2025-02-17 16:42:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:1.4410871267318726 norm:0.008472967892885208 max memory_allocated 29228.052734375 
[2025-02-17 16:42:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:1.4345108270645142 norm:0.0066486261785030365 max memory_allocated 29228.052734375 
[2025-02-17 16:43:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:1.4287264347076416 norm:0.00603718776255846 max memory_allocated 29228.052734375 
[2025-02-17 16:44:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:1.4272032976150513 norm:0.0055923620238900185 max memory_allocated 29228.052734375 
[2025-02-17 16:45:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:1.4236377477645874 norm:0.005063161253929138 max memory_allocated 29228.052734375 
[2025-02-17 16:45:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:1.4204856157302856 norm:0.004746433347463608 max memory_allocated 29228.052734375 
[2025-02-17 16:46:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:1.4216033220291138 norm:0.004470333456993103 max memory_allocated 29228.052734375 
[2025-02-17 16:47:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:1.4222441911697388 norm:0.004225402604788542 max memory_allocated 29228.052734375 
[2025-02-17 16:48:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:1.4224797487258911 norm:0.00412892596796155 max memory_allocated 29228.052734375 
[2025-02-17 16:48:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:1.4244633913040161 norm:0.0040271710604429245 max memory_allocated 29228.052734375 
[2025-02-17 16:49:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:1.4283547401428223 norm:0.0040931920520961285 max memory_allocated 29228.052734375 
[2025-02-17 16:50:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:1.4290804862976074 norm:0.00413120724260807 max memory_allocated 29228.052734375 
[2025-02-17 16:51:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:1.4302698373794556 norm:0.004179287236183882 max memory_allocated 29228.052734375 
[2025-02-17 16:51:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:1.4298304319381714 norm:0.003948861733078957 max memory_allocated 29228.052734375 
[2025-02-17 16:52:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:1.429325819015503 norm:0.003949454054236412 max memory_allocated 29228.052734375 
[2025-02-17 16:53:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:1.4275550842285156 norm:0.003918572794646025 max memory_allocated 29228.052734375 
[2025-02-17 16:53:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-17 16:54:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:1.559157133102417 norm:0.031828686594963074 max memory_allocated 29228.240234375 
[2025-02-17 16:55:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:1.5477681159973145 norm:0.018550194799900055 max memory_allocated 29228.240234375 
[2025-02-17 16:55:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:1.5441733598709106 norm:0.011956625618040562 max memory_allocated 29228.240234375 
[2025-02-17 16:56:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:1.5375463962554932 norm:0.007923544384539127 max memory_allocated 29228.240234375 
[2025-02-17 16:57:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:1.5366498231887817 norm:0.0062543610110878944 max memory_allocated 29228.240234375 
[2025-02-17 16:58:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:1.5358763933181763 norm:0.005317156668752432 max memory_allocated 29228.240234375 
[2025-02-17 16:58:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:1.5320744514465332 norm:0.004711287096142769 max memory_allocated 29228.240234375 
[2025-02-17 16:59:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:1.5319972038269043 norm:0.0043436698615550995 max memory_allocated 29228.240234375 
[2025-02-17 17:00:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:1.5326526165008545 norm:0.004082862287759781 max memory_allocated 29228.240234375 
[2025-02-17 17:00:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:1.5317553281784058 norm:0.0038862668443471193 max memory_allocated 29228.240234375 
[2025-02-17 17:01:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:1.530216932296753 norm:0.0037030293606221676 max memory_allocated 29228.240234375 
[2025-02-17 17:02:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:1.5280132293701172 norm:0.0036232660058885813 max memory_allocated 29228.240234375 
[2025-02-17 17:03:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:1.5273590087890625 norm:0.0036164214834570885 max memory_allocated 29228.240234375 
[2025-02-17 17:03:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:1.5267897844314575 norm:0.003530124668031931 max memory_allocated 29228.240234375 
[2025-02-17 17:04:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:1.5259727239608765 norm:0.0034419007133692503 max memory_allocated 29228.240234375 
[2025-02-17 17:05:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:1.5252646207809448 norm:0.0034734623041003942 max memory_allocated 29228.240234375 
[2025-02-17 17:06:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:1.5255682468414307 norm:0.0034806481562554836 max memory_allocated 29228.240234375 
[2025-02-17 17:06:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:1.5252556800842285 norm:0.0034853126853704453 max memory_allocated 29228.240234375 
[2025-02-17 17:07:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:1.5254076719284058 norm:0.0034942429047077894 max memory_allocated 29228.240234375 
[2025-02-17 17:08:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:1.52562415599823 norm:0.0034500255715101957 max memory_allocated 29228.240234375 
[2025-02-17 17:08:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-17 17:09:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:1.602712631225586 norm:0.023250408470630646 max memory_allocated 29228.427734375 
[2025-02-17 17:10:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:1.5975658893585205 norm:0.011831602081656456 max memory_allocated 29228.427734375 
[2025-02-17 17:10:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:1.5971482992172241 norm:0.007850158028304577 max memory_allocated 29228.427734375 
[2025-02-17 17:11:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:1.5922951698303223 norm:0.00534178176894784 max memory_allocated 29228.427734375 
[2025-02-17 17:12:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:1.5900923013687134 norm:0.004472407512366772 max memory_allocated 29228.427734375 
[2025-02-17 17:13:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:1.5862077474594116 norm:0.0037302395794540644 max memory_allocated 29228.427734375 
[2025-02-17 17:13:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:1.586883544921875 norm:0.0035485837142914534 max memory_allocated 29228.427734375 
[2025-02-17 17:14:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:1.5871683359146118 norm:0.003365228185430169 max memory_allocated 29228.427734375 
[2025-02-17 17:15:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:1.5857024192810059 norm:0.0032442209776490927 max memory_allocated 29228.427734375 
[2025-02-17 17:16:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:1.5877104997634888 norm:0.003372480859979987 max memory_allocated 29228.427734375 
[2025-02-17 17:16:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:1.5894801616668701 norm:0.003797871060669422 max memory_allocated 29228.427734375 
[2025-02-17 17:17:34 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:1.5901973247528076 norm:0.0040018898434937 max memory_allocated 29228.427734375 
[2025-02-17 17:18:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:1.5886064767837524 norm:0.0039274017326533794 max memory_allocated 29228.427734375 
[2025-02-17 17:19:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:1.5880067348480225 norm:0.003827884793281555 max memory_allocated 29228.427734375 
[2025-02-17 17:19:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:1.5887540578842163 norm:0.0038469063583761454 max memory_allocated 29228.427734375 
[2025-02-17 17:20:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:1.5886094570159912 norm:0.0038350282702594995 max memory_allocated 29228.427734375 
[2025-02-17 17:21:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:1.5874063968658447 norm:0.003926809877157211 max memory_allocated 29228.427734375 
[2025-02-17 17:22:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:1.5865631103515625 norm:0.0038792318664491177 max memory_allocated 29228.427734375 
[2025-02-17 17:22:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:1.5870747566223145 norm:0.003881276585161686 max memory_allocated 29228.427734375 
[2025-02-17 17:23:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:1.5879175662994385 norm:0.003893432207405567 max memory_allocated 29228.427734375 
[2025-02-17 17:23:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-17 17:24:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:1.6523205041885376 norm:0.011289834976196289 max memory_allocated 29228.615234375 
[2025-02-17 17:25:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:1.6408023834228516 norm:0.008032171055674553 max memory_allocated 29228.615234375 
[2025-02-17 17:26:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:1.6398159265518188 norm:0.005963715724647045 max memory_allocated 29228.615234375 
[2025-02-17 17:26:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:1.6354604959487915 norm:0.003935432061553001 max memory_allocated 29228.615234375 
[2025-02-17 17:27:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:1.6321064233779907 norm:0.0029937424696981907 max memory_allocated 29228.615234375 
[2025-02-17 17:28:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:1.6303452253341675 norm:0.0027065572794526815 max memory_allocated 29228.615234375 
[2025-02-17 17:28:58 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:1.6296625137329102 norm:0.002579733729362488 max memory_allocated 29228.615234375 
[2025-02-17 17:29:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:1.628711223602295 norm:0.0025889603421092033 max memory_allocated 29228.615234375 
[2025-02-17 17:30:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:1.628908395767212 norm:0.0026356049347668886 max memory_allocated 29228.615234375 
[2025-02-17 17:31:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:1.633001446723938 norm:0.0030604908242821693 max memory_allocated 29228.615234375 
[2025-02-17 17:31:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:1.632744550704956 norm:0.003219578880816698 max memory_allocated 29228.615234375 
[2025-02-17 17:32:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:1.6323343515396118 norm:0.003817123593762517 max memory_allocated 29228.615234375 
[2025-02-17 17:33:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:1.6308270692825317 norm:0.003783716121688485 max memory_allocated 29228.615234375 
[2025-02-17 17:34:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:1.6287345886230469 norm:0.003736861515790224 max memory_allocated 29228.615234375 
[2025-02-17 17:34:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:1.6268008947372437 norm:0.0036774668842554092 max memory_allocated 29228.615234375 
[2025-02-17 17:35:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:1.6254944801330566 norm:0.0036149872466921806 max memory_allocated 29228.615234375 
[2025-02-17 17:36:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:1.6254116296768188 norm:0.003680021967738867 max memory_allocated 29228.615234375 
[2025-02-17 17:37:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:1.6254791021347046 norm:0.003786046989262104 max memory_allocated 29228.615234375 
[2025-02-17 17:37:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:1.624537706375122 norm:0.003801427548751235 max memory_allocated 29228.615234375 
[2025-02-17 17:38:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:1.624099612236023 norm:0.0038396031595766544 max memory_allocated 29228.615234375 
[2025-02-17 17:38:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-17 17:39:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:1.7643462419509888 norm:0.0493839792907238 max memory_allocated 29228.802734375 
[2025-02-17 17:40:22 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:1.745354175567627 norm:0.03209419548511505 max memory_allocated 29228.802734375 
[2025-02-17 17:41:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:1.7406935691833496 norm:0.022959019988775253 max memory_allocated 29228.802734375 
[2025-02-17 17:41:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:1.731984257698059 norm:0.015128983184695244 max memory_allocated 29228.802734375 
[2025-02-17 17:42:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:1.7216744422912598 norm:0.01032185833901167 max memory_allocated 29228.802734375 
[2025-02-17 17:43:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:1.704842209815979 norm:0.009230941534042358 max memory_allocated 29228.802734375 
[2025-02-17 17:44:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:1.6858572959899902 norm:0.006496319081634283 max memory_allocated 29228.802734375 
[2025-02-17 17:44:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:1.6865726709365845 norm:0.005828308407217264 max memory_allocated 29228.802734375 
[2025-02-17 17:45:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:1.689133882522583 norm:0.005804035346955061 max memory_allocated 29228.802734375 
[2025-02-17 17:46:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:1.6905744075775146 norm:0.005809360183775425 max memory_allocated 29228.802734375 
[2025-02-17 17:47:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:1.6977490186691284 norm:0.006271827965974808 max memory_allocated 29228.802734375 
[2025-02-17 17:47:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:1.7070907354354858 norm:0.00781296007335186 max memory_allocated 29228.802734375 
[2025-02-17 17:48:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:1.7097249031066895 norm:0.009116018190979958 max memory_allocated 29228.802734375 
[2025-02-17 17:49:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:1.7115113735198975 norm:0.009006770327687263 max memory_allocated 29228.802734375 
[2025-02-17 17:50:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:1.710679292678833 norm:0.00903596542775631 max memory_allocated 29228.802734375 
[2025-02-17 17:50:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:1.7134044170379639 norm:0.009320174343883991 max memory_allocated 29228.802734375 
[2025-02-17 17:51:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:1.7167719602584839 norm:0.010220842435956001 max memory_allocated 29228.802734375 
[2025-02-17 17:52:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:1.7155195474624634 norm:0.00982614140957594 max memory_allocated 29228.802734375 
[2025-02-17 17:52:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:1.7146191596984863 norm:0.009724379517138004 max memory_allocated 29228.802734375 
[2025-02-17 17:53:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:1.7142670154571533 norm:0.009463317692279816 max memory_allocated 29228.802734375 
[2025-02-17 17:53:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-17 17:54:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:1.8521398305892944 norm:0.06331188976764679 max memory_allocated 29228.990234375 
[2025-02-17 17:55:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:1.835989236831665 norm:0.03854835405945778 max memory_allocated 29228.990234375 
[2025-02-17 17:56:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:1.8330903053283691 norm:0.02696147747337818 max memory_allocated 29228.990234375 
[2025-02-17 17:56:57 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:1.827537178993225 norm:0.018197162076830864 max memory_allocated 29228.990234375 
[2025-02-17 17:57:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:1.823043704032898 norm:0.012978117913007736 max memory_allocated 29228.990234375 
[2025-02-17 17:58:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:1.8201665878295898 norm:0.010552134364843369 max memory_allocated 29228.990234375 
[2025-02-17 17:59:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:1.8094611167907715 norm:0.008828828111290932 max memory_allocated 29228.990234375 
[2025-02-17 17:59:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:1.8064508438110352 norm:0.00827390979975462 max memory_allocated 29228.990234375 
[2025-02-17 18:00:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:1.8052759170532227 norm:0.007988743484020233 max memory_allocated 29228.990234375 
[2025-02-17 18:01:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:1.798815131187439 norm:0.0072304788045585155 max memory_allocated 29228.990234375 
[2025-02-17 18:02:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:1.7923939228057861 norm:0.007090409751981497 max memory_allocated 29228.990234375 
[2025-02-17 18:02:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:1.780733346939087 norm:0.008862488903105259 max memory_allocated 29228.990234375 
[2025-02-17 18:03:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:1.7732797861099243 norm:0.011065559461712837 max memory_allocated 29228.990234375 
[2025-02-17 18:04:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:1.7702661752700806 norm:0.014383869245648384 max memory_allocated 29228.990234375 
[2025-02-17 18:05:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:1.7701078653335571 norm:0.018310697749257088 max memory_allocated 29228.990234375 
[2025-02-17 18:05:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:1.7706031799316406 norm:0.02653958462178707 max memory_allocated 29228.990234375 
[2025-02-17 18:06:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:1.7702579498291016 norm:0.02971743606030941 max memory_allocated 29228.990234375 
[2025-02-17 18:07:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:1.7693861722946167 norm:0.030667703598737717 max memory_allocated 29228.990234375 
[2025-02-17 18:08:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:1.7684576511383057 norm:0.029976701363921165 max memory_allocated 29228.990234375 
[2025-02-17 18:08:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:1.7686116695404053 norm:0.02966996468603611 max memory_allocated 29228.990234375 
[2025-02-17 18:09:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-17 18:09:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:1.8782446384429932 norm:0.028675202280282974 max memory_allocated 29229.177734375 
[2025-02-17 18:10:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:1.8647031784057617 norm:0.019145473837852478 max memory_allocated 29229.177734375 
[2025-02-17 18:11:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:1.8532922267913818 norm:0.012519022449851036 max memory_allocated 29229.177734375 
[2025-02-17 18:12:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:1.8454821109771729 norm:0.007984593510627747 max memory_allocated 29229.177734375 
[2025-02-17 18:12:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:1.8435454368591309 norm:0.005325367208570242 max memory_allocated 29229.177734375 
[2025-02-17 18:13:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:1.8384182453155518 norm:0.004149319604039192 max memory_allocated 29229.177734375 
[2025-02-17 18:14:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:1.8360549211502075 norm:0.0036737776827067137 max memory_allocated 29229.177734375 
[2025-02-17 18:15:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:1.8368220329284668 norm:0.0036287051625549793 max memory_allocated 29229.177734375 
[2025-02-17 18:15:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:1.840015172958374 norm:0.00372919999063015 max memory_allocated 29229.177734375 
[2025-02-17 18:16:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:1.8394454717636108 norm:0.00348854111507535 max memory_allocated 29229.177734375 
[2025-02-17 18:17:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:1.8385593891143799 norm:0.003336181165650487 max memory_allocated 29229.177734375 
[2025-02-17 18:18:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:1.839397668838501 norm:0.0033384980633854866 max memory_allocated 29229.177734375 
[2025-02-17 18:18:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:1.8416060209274292 norm:0.0035289444494992495 max memory_allocated 29229.177734375 
[2025-02-17 18:19:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:1.8423640727996826 norm:0.0035585095174610615 max memory_allocated 29229.177734375 
[2025-02-17 18:20:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:1.8414512872695923 norm:0.0035167736932635307 max memory_allocated 29229.177734375 
[2025-02-17 18:20:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:1.8396179676055908 norm:0.0034285217989236116 max memory_allocated 29229.177734375 
[2025-02-17 18:21:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:1.836423397064209 norm:0.0032717918511480093 max memory_allocated 29229.177734375 
[2025-02-17 18:22:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:1.8352004289627075 norm:0.003212372539564967 max memory_allocated 29229.177734375 
[2025-02-17 18:23:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:1.8341432809829712 norm:0.003162962384521961 max memory_allocated 29229.177734375 
[2025-02-17 18:23:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:1.8329240083694458 norm:0.003088001860305667 max memory_allocated 29229.177734375 
[2025-02-17 18:24:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-17 18:24:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:1.9579906463623047 norm:0.021143527701497078 max memory_allocated 29229.365234375 
[2025-02-17 18:25:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:1.9425917863845825 norm:0.012854221276938915 max memory_allocated 29229.365234375 
[2025-02-17 18:26:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:1.9373735189437866 norm:0.009024686180055141 max memory_allocated 29229.365234375 
[2025-02-17 18:27:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:1.9340921640396118 norm:0.006667373701930046 max memory_allocated 29229.365234375 
[2025-02-17 18:27:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:1.9312824010849 norm:0.005228292662650347 max memory_allocated 29229.365234375 
[2025-02-17 18:28:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:1.9265426397323608 norm:0.004123998805880547 max memory_allocated 29229.365234375 
[2025-02-17 18:29:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:1.9226441383361816 norm:0.0035012199077755213 max memory_allocated 29229.365234375 
[2025-02-17 18:30:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:1.9203667640686035 norm:0.0031831192318350077 max memory_allocated 29229.365234375 
[2025-02-17 18:30:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:1.9171888828277588 norm:0.00338718481361866 max memory_allocated 29229.365234375 
[2025-02-17 18:31:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:1.9138952493667603 norm:0.003836791031062603 max memory_allocated 29229.365234375 
[2025-02-17 18:32:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:1.9126560688018799 norm:0.003740474581718445 max memory_allocated 29229.365234375 
[2025-02-17 18:33:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:1.9123256206512451 norm:0.0038690229412168264 max memory_allocated 29229.365234375 
[2025-02-17 18:33:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:1.9116560220718384 norm:0.004033613950014114 max memory_allocated 29229.365234375 
[2025-02-17 18:34:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:1.9106123447418213 norm:0.003992397338151932 max memory_allocated 29229.365234375 
[2025-02-17 18:35:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:1.9102267026901245 norm:0.003910234197974205 max memory_allocated 29229.365234375 
[2025-02-17 18:36:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:1.90952730178833 norm:0.0038896554615348577 max memory_allocated 29229.365234375 
[2025-02-17 18:36:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:1.9093176126480103 norm:0.003929391503334045 max memory_allocated 29229.365234375 
[2025-02-17 18:37:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:1.9087737798690796 norm:0.003906986676156521 max memory_allocated 29229.365234375 
[2025-02-17 18:38:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:1.9086154699325562 norm:0.0038597446400672197 max memory_allocated 29229.365234375 
[2025-02-17 18:39:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:1.907901406288147 norm:0.003988745156675577 max memory_allocated 29229.365234375 
[2025-02-17 18:39:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-17 18:40:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:2.146122455596924 norm:0.07211633026599884 max memory_allocated 29229.552734375 
[2025-02-17 18:40:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:2.1269872188568115 norm:0.04736287519335747 max memory_allocated 29229.552734375 
[2025-02-17 18:41:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:2.1207940578460693 norm:0.03398960828781128 max memory_allocated 29229.552734375 
[2025-02-17 18:42:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:2.116267204284668 norm:0.023763975128531456 max memory_allocated 29229.552734375 
[2025-02-17 18:43:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:2.1127266883850098 norm:0.01737571880221367 max memory_allocated 29229.552734375 
[2025-02-17 18:43:49 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:2.1091151237487793 norm:0.013675091788172722 max memory_allocated 29229.552734375 
[2025-02-17 18:44:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:2.1054017543792725 norm:0.011270804330706596 max memory_allocated 29229.552734375 
[2025-02-17 18:45:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:2.094322681427002 norm:0.010250044986605644 max memory_allocated 29229.552734375 
[2025-02-17 18:46:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:2.0829246044158936 norm:0.009683612734079361 max memory_allocated 29229.552734375 
[2025-02-17 18:46:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:2.0763330459594727 norm:0.008581550791859627 max memory_allocated 29229.552734375 
[2025-02-17 18:47:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:2.073725938796997 norm:0.00857301615178585 max memory_allocated 29229.552734375 
[2025-02-17 18:48:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:2.073118209838867 norm:0.009419482201337814 max memory_allocated 29229.552734375 
[2025-02-17 18:49:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:2.072676181793213 norm:0.009792555123567581 max memory_allocated 29229.552734375 
[2025-02-17 18:49:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:2.07127046585083 norm:0.010094433091580868 max memory_allocated 29229.552734375 
[2025-02-17 18:50:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:2.070888042449951 norm:0.010720116086304188 max memory_allocated 29229.552734375 
[2025-02-17 18:51:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:2.069610834121704 norm:0.010683955624699593 max memory_allocated 29229.552734375 
[2025-02-17 18:51:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:2.067540168762207 norm:0.009948475286364555 max memory_allocated 29229.552734375 
[2025-02-17 18:52:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:2.0670387744903564 norm:0.00988403707742691 max memory_allocated 29229.552734375 
[2025-02-17 18:53:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:2.068791627883911 norm:0.010181617923080921 max memory_allocated 29229.552734375 
[2025-02-17 18:54:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:2.070523262023926 norm:0.010411812923848629 max memory_allocated 29229.552734375 
[2025-02-17 18:54:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-17 18:55:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:2.2710447311401367 norm:0.020502641797065735 max memory_allocated 29229.740234375 
[2025-02-17 18:55:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:2.253981113433838 norm:0.011474179103970528 max memory_allocated 29229.740234375 
[2025-02-17 18:56:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:2.246034622192383 norm:0.007748149335384369 max memory_allocated 29229.740234375 
[2025-02-17 18:57:26 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:2.2421059608459473 norm:0.005924208555370569 max memory_allocated 29229.740234375 
[2025-02-17 18:58:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:2.23881196975708 norm:0.004587063565850258 max memory_allocated 29229.740234375 
[2025-02-17 18:58:55 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:2.235642910003662 norm:0.0037415572442114353 max memory_allocated 29229.740234375 
[2025-02-17 18:59:40 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:2.2328693866729736 norm:0.0032767814118415117 max memory_allocated 29229.740234375 
[2025-02-17 19:00:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:2.231229305267334 norm:0.002962612546980381 max memory_allocated 29229.740234375 
[2025-02-17 19:01:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:2.229670286178589 norm:0.00275417510420084 max memory_allocated 29229.740234375 
[2025-02-17 19:01:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:2.2283835411071777 norm:0.002612379379570484 max memory_allocated 29229.740234375 
[2025-02-17 19:02:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:2.2265851497650146 norm:0.002444580662995577 max memory_allocated 29229.740234375 
[2025-02-17 19:03:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:2.226163387298584 norm:0.0022869366221129894 max memory_allocated 29229.740234375 
[2025-02-17 19:04:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:2.2241406440734863 norm:0.002173923421651125 max memory_allocated 29229.740234375 
[2025-02-17 19:04:52 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:2.2223594188690186 norm:0.002066405490040779 max memory_allocated 29229.740234375 
[2025-02-17 19:05:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:2.2213315963745117 norm:0.0019909534603357315 max memory_allocated 29229.740234375 
[2025-02-17 19:06:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:2.2208001613616943 norm:0.0019209004240110517 max memory_allocated 29229.740234375 
[2025-02-17 19:07:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:2.2205071449279785 norm:0.00189060193952173 max memory_allocated 29229.740234375 
[2025-02-17 19:07:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:2.2201831340789795 norm:0.0018696488114073873 max memory_allocated 29229.740234375 
[2025-02-17 19:08:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:2.2197141647338867 norm:0.0018320941599085927 max memory_allocated 29229.740234375 
[2025-02-17 19:09:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:2.219374418258667 norm:0.0018032541265711188 max memory_allocated 29229.740234375 
[2025-02-17 19:09:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-17 19:10:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:2.4732706546783447 norm:0.04639718309044838 max memory_allocated 29229.927734375 
[2025-02-17 19:11:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:2.447242498397827 norm:0.025354014709591866 max memory_allocated 29229.927734375 
[2025-02-17 19:11:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:2.4356179237365723 norm:0.01559190172702074 max memory_allocated 29229.927734375 
[2025-02-17 19:12:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:2.4297192096710205 norm:0.01129140704870224 max memory_allocated 29229.927734375 
[2025-02-17 19:13:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:2.425844669342041 norm:0.008630897849798203 max memory_allocated 29229.927734375 
[2025-02-17 19:14:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:2.4243595600128174 norm:0.006910788826644421 max memory_allocated 29229.927734375 
[2025-02-17 19:14:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:2.4227826595306396 norm:0.006089615169912577 max memory_allocated 29229.927734375 
[2025-02-17 19:15:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:2.421692371368408 norm:0.005671995226293802 max memory_allocated 29229.927734375 
[2025-02-17 19:16:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:2.418825626373291 norm:0.005010324530303478 max memory_allocated 29229.927734375 
[2025-02-17 19:17:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:2.417961359024048 norm:0.004624905996024609 max memory_allocated 29229.927734375 
[2025-02-17 19:17:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:2.415440797805786 norm:0.004188599064946175 max memory_allocated 29229.927734375 
[2025-02-17 19:18:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:2.4120309352874756 norm:0.003743776585906744 max memory_allocated 29229.927734375 
[2025-02-17 19:19:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:2.411247491836548 norm:0.003552778158336878 max memory_allocated 29229.927734375 
[2025-02-17 19:19:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:2.4100723266601562 norm:0.0033661348279565573 max memory_allocated 29229.927734375 
[2025-02-17 19:20:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:2.4093732833862305 norm:0.0031641139648854733 max memory_allocated 29229.927734375 
[2025-02-17 19:21:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:2.4073240756988525 norm:0.0029480508528649807 max memory_allocated 29229.927734375 
[2025-02-17 19:22:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:2.408938407897949 norm:0.0029953178018331528 max memory_allocated 29229.927734375 
[2025-02-17 19:22:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:2.4097206592559814 norm:0.002985140774399042 max memory_allocated 29229.927734375 
[2025-02-17 19:23:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:2.4079794883728027 norm:0.002862378256395459 max memory_allocated 29229.927734375 
[2025-02-17 19:24:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:2.4066338539123535 norm:0.002810507081449032 max memory_allocated 29229.927734375 
[2025-02-17 19:24:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-17 19:25:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:2.699218273162842 norm:0.017926665022969246 max memory_allocated 29230.115234375 
[2025-02-17 19:26:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:2.6824545860290527 norm:0.010416021570563316 max memory_allocated 29230.115234375 
[2025-02-17 19:26:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:2.6766860485076904 norm:0.007524693384766579 max memory_allocated 29230.115234375 
[2025-02-17 19:27:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:2.6752400398254395 norm:0.005707279313355684 max memory_allocated 29230.115234375 
[2025-02-17 19:28:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:2.672553062438965 norm:0.0042280335910618305 max memory_allocated 29230.115234375 
[2025-02-17 19:29:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:2.6687419414520264 norm:0.0034217906650155783 max memory_allocated 29230.115234375 
[2025-02-17 19:29:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:2.6664254665374756 norm:0.0030508413910865784 max memory_allocated 29230.115234375 
[2025-02-17 19:30:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:2.6638588905334473 norm:0.0027417095843702555 max memory_allocated 29230.115234375 
[2025-02-17 19:31:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:2.6606316566467285 norm:0.002439857693389058 max memory_allocated 29230.115234375 
[2025-02-17 19:32:07 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:2.659283399581909 norm:0.0022945967502892017 max memory_allocated 29230.115234375 
[2025-02-17 19:32:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:2.6577184200286865 norm:0.0022179873194545507 max memory_allocated 29230.115234375 
[2025-02-17 19:33:36 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:2.657045841217041 norm:0.0021483779419213533 max memory_allocated 29230.115234375 
[2025-02-17 19:34:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:2.656325101852417 norm:0.002145194448530674 max memory_allocated 29230.115234375 
[2025-02-17 19:35:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:2.6557440757751465 norm:0.002146973507478833 max memory_allocated 29230.115234375 
[2025-02-17 19:35:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:2.654789686203003 norm:0.0021309545263648033 max memory_allocated 29230.115234375 
[2025-02-17 19:36:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:2.6543538570404053 norm:0.0021226098760962486 max memory_allocated 29230.115234375 
[2025-02-17 19:37:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:2.654787540435791 norm:0.002191616455093026 max memory_allocated 29230.115234375 
[2025-02-17 19:38:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:2.6545863151550293 norm:0.002202513860538602 max memory_allocated 29230.115234375 
[2025-02-17 19:38:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:2.6539950370788574 norm:0.00223864009603858 max memory_allocated 29230.115234375 
[2025-02-17 19:39:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:2.653137683868408 norm:0.002220293739810586 max memory_allocated 29230.115234375 
[2025-02-17 19:39:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-17 19:40:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:3.042365074157715 norm:0.02319825440645218 max memory_allocated 29230.302734375 
[2025-02-17 19:41:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:3.0280168056488037 norm:0.01460929587483406 max memory_allocated 29230.302734375 
[2025-02-17 19:42:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:3.0259571075439453 norm:0.011104842647910118 max memory_allocated 29230.302734375 
[2025-02-17 19:42:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:3.0252609252929688 norm:0.008550096303224564 max memory_allocated 29230.302734375 
[2025-02-17 19:43:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:3.023099899291992 norm:0.006761559750884771 max memory_allocated 29230.302734375 
[2025-02-17 19:44:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:3.0161900520324707 norm:0.005462653934955597 max memory_allocated 29230.302734375 
[2025-02-17 19:45:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:3.0115339756011963 norm:0.0046598054468631744 max memory_allocated 29230.302734375 
[2025-02-17 19:45:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:3.00943660736084 norm:0.0042222850024700165 max memory_allocated 29230.302734375 
[2025-02-17 19:46:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:3.0082688331604004 norm:0.0038793354760855436 max memory_allocated 29230.302734375 
[2025-02-17 19:47:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:3.0079972743988037 norm:0.0036946889013051987 max memory_allocated 29230.302734375 
[2025-02-17 19:47:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:3.0068857669830322 norm:0.003575011156499386 max memory_allocated 29230.302734375 
[2025-02-17 19:48:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:3.0050854682922363 norm:0.0034139917697757483 max memory_allocated 29230.302734375 
[2025-02-17 19:49:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:3.002969980239868 norm:0.0032761378679424524 max memory_allocated 29230.302734375 
[2025-02-17 19:50:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:3.002002239227295 norm:0.003172733820974827 max memory_allocated 29230.302734375 
[2025-02-17 19:50:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:3.0006182193756104 norm:0.00312403729185462 max memory_allocated 29230.302734375 
[2025-02-17 19:51:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:2.999999761581421 norm:0.0030862148851156235 max memory_allocated 29230.302734375 
[2025-02-17 19:52:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:2.9987387657165527 norm:0.003015254158526659 max memory_allocated 29230.302734375 
[2025-02-17 19:53:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:2.9983439445495605 norm:0.003002722281962633 max memory_allocated 29230.302734375 
[2025-02-17 19:53:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:2.9981307983398438 norm:0.0029743872582912445 max memory_allocated 29230.302734375 
[2025-02-17 19:54:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:2.997441530227661 norm:0.002970504341647029 max memory_allocated 29230.302734375 
[2025-02-17 19:54:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-17 19:55:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:3.3910202980041504 norm:0.021615279838442802 max memory_allocated 29230.490234375 
[2025-02-17 19:56:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:3.3808629512786865 norm:0.014791766181588173 max memory_allocated 29230.490234375 
[2025-02-17 19:57:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:3.3784217834472656 norm:0.010886406525969505 max memory_allocated 29230.490234375 
[2025-02-17 19:57:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:3.3756892681121826 norm:0.008333428762853146 max memory_allocated 29230.490234375 
[2025-02-17 19:58:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:3.3716607093811035 norm:0.00687228050082922 max memory_allocated 29230.490234375 
[2025-02-17 19:59:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:3.3693270683288574 norm:0.005699601955711842 max memory_allocated 29230.490234375 
[2025-02-17 20:00:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:3.3675835132598877 norm:0.004984527826309204 max memory_allocated 29230.490234375 
[2025-02-17 20:00:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:3.3653054237365723 norm:0.004410301800817251 max memory_allocated 29230.490234375 
[2025-02-17 20:01:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:3.3632473945617676 norm:0.0038852759171277285 max memory_allocated 29230.490234375 
[2025-02-17 20:02:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:3.360790729522705 norm:0.0035521346144378185 max memory_allocated 29230.490234375 
[2025-02-17 20:03:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:3.359029769897461 norm:0.00330656161531806 max memory_allocated 29230.490234375 
[2025-02-17 20:03:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:3.3566601276397705 norm:0.003066234290599823 max memory_allocated 29230.490234375 
[2025-02-17 20:04:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:3.355842351913452 norm:0.002873579040169716 max memory_allocated 29230.490234375 
[2025-02-17 20:05:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:3.3543038368225098 norm:0.0026893022004514933 max memory_allocated 29230.490234375 
[2025-02-17 20:06:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:3.352935791015625 norm:0.0025758908595889807 max memory_allocated 29230.490234375 
[2025-02-17 20:06:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:3.3526039123535156 norm:0.002466777106747031 max memory_allocated 29230.490234375 
[2025-02-17 20:07:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:3.3516952991485596 norm:0.0023839587811380625 max memory_allocated 29230.490234375 
[2025-02-17 20:08:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:3.350818634033203 norm:0.0023099426180124283 max memory_allocated 29230.490234375 
[2025-02-17 20:09:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:3.3501265048980713 norm:0.0022744559682905674 max memory_allocated 29230.490234375 
[2025-02-17 20:09:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:3.349519968032837 norm:0.0022324800956994295 max memory_allocated 29230.490234375 
[2025-02-17 20:09:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-17 20:10:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:3.755594491958618 norm:0.013800069689750671 max memory_allocated 29230.677734375 
[2025-02-17 20:11:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:3.7442049980163574 norm:0.0085908193141222 max memory_allocated 29230.677734375 
[2025-02-17 20:12:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:3.7422680854797363 norm:0.006328691728413105 max memory_allocated 29230.677734375 
[2025-02-17 20:13:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:3.740753173828125 norm:0.004873297642916441 max memory_allocated 29230.677734375 
[2025-02-17 20:13:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:3.73982572555542 norm:0.003966148942708969 max memory_allocated 29230.677734375 
[2025-02-17 20:14:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:3.738004446029663 norm:0.003394131548702717 max memory_allocated 29230.677734375 
[2025-02-17 20:15:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:3.7348694801330566 norm:0.0030920694116503 max memory_allocated 29230.677734375 
[2025-02-17 20:15:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:3.7308764457702637 norm:0.002847567666321993 max memory_allocated 29230.677734375 
[2025-02-17 20:16:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:3.726834535598755 norm:0.003006505314260721 max memory_allocated 29230.677734375 
[2025-02-17 20:17:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:3.7219762802124023 norm:0.008223656564950943 max memory_allocated 29230.677734375 
[2025-02-17 20:18:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:3.719992160797119 norm:0.008892079815268517 max memory_allocated 29230.677734375 
[2025-02-17 20:18:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:3.718736171722412 norm:0.008733566850423813 max memory_allocated 29230.677734375 
[2025-02-17 20:19:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:3.7175660133361816 norm:0.00848315842449665 max memory_allocated 29230.677734375 
[2025-02-17 20:20:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:3.7163772583007812 norm:0.008775845170021057 max memory_allocated 29230.677734375 
[2025-02-17 20:21:10 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:3.7153000831604004 norm:0.009035046212375164 max memory_allocated 29230.677734375 
[2025-02-17 20:21:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:3.7145094871520996 norm:0.009699148125946522 max memory_allocated 29230.677734375 
[2025-02-17 20:22:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:3.7137084007263184 norm:0.009960105642676353 max memory_allocated 29230.677734375 
[2025-02-17 20:23:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:3.7129909992218018 norm:0.010290742851793766 max memory_allocated 29230.677734375 
[2025-02-17 20:24:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:3.7121975421905518 norm:0.010556904599070549 max memory_allocated 29230.677734375 
[2025-02-17 20:24:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:3.7117807865142822 norm:0.01062903180718422 max memory_allocated 29230.677734375 
[2025-02-17 20:25:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-17 20:25:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:4.196502208709717 norm:0.018278956413269043 max memory_allocated 29230.865234375 
[2025-02-17 20:26:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:4.188998699188232 norm:0.012823187746107578 max memory_allocated 29230.865234375 
[2025-02-17 20:27:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:4.189908981323242 norm:0.00985778495669365 max memory_allocated 29230.865234375 
[2025-02-17 20:28:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:4.188348770141602 norm:0.007892611436545849 max memory_allocated 29230.865234375 
[2025-02-17 20:28:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:4.185758590698242 norm:0.006532752420753241 max memory_allocated 29230.865234375 
[2025-02-17 20:29:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:4.183368682861328 norm:0.005600719712674618 max memory_allocated 29230.865234375 
[2025-02-17 20:30:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:4.18261194229126 norm:0.004855463281273842 max memory_allocated 29230.865234375 
[2025-02-17 20:31:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:4.17988920211792 norm:0.004365995991975069 max memory_allocated 29230.865234375 
[2025-02-17 20:31:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:4.178014278411865 norm:0.003975014202296734 max memory_allocated 29230.865234375 
[2025-02-17 20:32:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:4.176748275756836 norm:0.0037543433718383312 max memory_allocated 29230.865234375 
[2025-02-17 20:33:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:4.175130844116211 norm:0.0036003831773996353 max memory_allocated 29230.865234375 
[2025-02-17 20:34:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:4.1736955642700195 norm:0.0034886959474533796 max memory_allocated 29230.865234375 
[2025-02-17 20:34:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:4.172802925109863 norm:0.0034042680636048317 max memory_allocated 29230.865234375 
[2025-02-17 20:35:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:4.171497821807861 norm:0.0033453889191150665 max memory_allocated 29230.865234375 
[2025-02-17 20:36:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:4.1695966720581055 norm:0.003253057599067688 max memory_allocated 29230.865234375 
[2025-02-17 20:37:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:4.16813850402832 norm:0.003228181041777134 max memory_allocated 29230.865234375 
[2025-02-17 20:37:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:4.167383193969727 norm:0.0032129152677953243 max memory_allocated 29230.865234375 
[2025-02-17 20:38:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:4.166732311248779 norm:0.003168933792039752 max memory_allocated 29230.865234375 
[2025-02-17 20:39:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:4.166032791137695 norm:0.003144332207739353 max memory_allocated 29230.865234375 
[2025-02-17 20:39:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:4.1649699211120605 norm:0.0031214081682264805 max memory_allocated 29230.865234375 
[2025-02-17 20:40:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-17 20:41:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:4.589618682861328 norm:0.015514723025262356 max memory_allocated 29231.052734375 
[2025-02-17 20:41:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:4.583430290222168 norm:0.010455416515469551 max memory_allocated 29231.052734375 
[2025-02-17 20:42:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:4.580451011657715 norm:0.007889867760241032 max memory_allocated 29231.052734375 
[2025-02-17 20:43:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:4.5786452293396 norm:0.0061071040108799934 max memory_allocated 29231.052734375 
[2025-02-17 20:44:01 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:4.576116561889648 norm:0.005048931576311588 max memory_allocated 29231.052734375 
[2025-02-17 20:44:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:4.572543621063232 norm:0.004302655346691608 max memory_allocated 29231.052734375 
[2025-02-17 20:45:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:4.569723129272461 norm:0.0037536434829235077 max memory_allocated 29231.052734375 
[2025-02-17 20:46:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:4.566486835479736 norm:0.0033851428888738155 max memory_allocated 29231.052734375 
[2025-02-17 20:46:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:4.5631232261657715 norm:0.0030748117715120316 max memory_allocated 29231.052734375 
[2025-02-17 20:47:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:4.560925483703613 norm:0.0029379818588495255 max memory_allocated 29231.052734375 
[2025-02-17 20:48:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:4.559277534484863 norm:0.0028447837103158236 max memory_allocated 29231.052734375 
[2025-02-17 20:49:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:4.5582098960876465 norm:0.0027165119536221027 max memory_allocated 29231.052734375 
[2025-02-17 20:49:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:4.556942939758301 norm:0.0027260356582701206 max memory_allocated 29231.052734375 
[2025-02-17 20:50:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:4.554829120635986 norm:0.003036974463611841 max memory_allocated 29231.052734375 
[2025-02-17 20:51:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:4.552816867828369 norm:0.005757593084126711 max memory_allocated 29231.052734375 
[2025-02-17 20:52:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:4.552068710327148 norm:0.005828240420669317 max memory_allocated 29231.052734375 
[2025-02-17 20:52:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:4.551906585693359 norm:0.006038118619471788 max memory_allocated 29231.052734375 
[2025-02-17 20:53:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:4.5516557693481445 norm:0.006021348759531975 max memory_allocated 29231.052734375 
[2025-02-17 20:54:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:4.551036834716797 norm:0.0059407539665699005 max memory_allocated 29231.052734375 
[2025-02-17 20:55:09 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:4.550362586975098 norm:0.0058420635759830475 max memory_allocated 29231.052734375 
[2025-02-17 20:55:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-17 20:56:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:5.0330119132995605 norm:0.01111020427197218 max memory_allocated 29231.240234375 
[2025-02-17 20:56:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:5.026465892791748 norm:0.007630581967532635 max memory_allocated 29231.240234375 
[2025-02-17 20:57:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:5.025442600250244 norm:0.005947523284703493 max memory_allocated 29231.240234375 
[2025-02-17 20:58:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:5.023911476135254 norm:0.004703748971223831 max memory_allocated 29231.240234375 
[2025-02-17 20:59:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:5.021906852722168 norm:0.003976469859480858 max memory_allocated 29231.240234375 
[2025-02-17 20:59:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:5.019835472106934 norm:0.0033780112862586975 max memory_allocated 29231.240234375 
[2025-02-17 21:00:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:5.01710319519043 norm:0.0030146087519824505 max memory_allocated 29231.240234375 
[2025-02-17 21:01:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:5.014547824859619 norm:0.002728493185713887 max memory_allocated 29231.240234375 
[2025-02-17 21:02:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:5.012748718261719 norm:0.0026046151760965586 max memory_allocated 29231.240234375 
[2025-02-17 21:02:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:5.010101318359375 norm:0.0024943288881331682 max memory_allocated 29231.240234375 
[2025-02-17 21:03:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:5.007746696472168 norm:0.0023710154928267 max memory_allocated 29231.240234375 
[2025-02-17 21:04:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:5.0052618980407715 norm:0.002279035048559308 max memory_allocated 29231.240234375 
[2025-02-17 21:05:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:5.003457546234131 norm:0.0022045685909688473 max memory_allocated 29231.240234375 
[2025-02-17 21:05:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:5.002790451049805 norm:0.002180163050070405 max memory_allocated 29231.240234375 
[2025-02-17 21:06:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:5.00221061706543 norm:0.0021705678664147854 max memory_allocated 29231.240234375 
[2025-02-17 21:07:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:5.001251697540283 norm:0.002157756360247731 max memory_allocated 29231.240234375 
[2025-02-17 21:08:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:5.00015926361084 norm:0.002144804922863841 max memory_allocated 29231.240234375 
[2025-02-17 21:08:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:4.998979091644287 norm:0.002148419851437211 max memory_allocated 29231.240234375 
[2025-02-17 21:09:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:4.998012542724609 norm:0.0021454410161823034 max memory_allocated 29231.240234375 
[2025-02-17 21:10:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:4.997272491455078 norm:0.002099327277392149 max memory_allocated 29231.240234375 
[2025-02-17 21:10:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-17 21:11:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:5.563539028167725 norm:0.015432557091116905 max memory_allocated 29231.427734375 
[2025-02-17 21:12:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:5.556385517120361 norm:0.011931434273719788 max memory_allocated 29231.427734375 
[2025-02-17 21:12:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:5.551518440246582 norm:0.01024832297116518 max memory_allocated 29231.427734375 
[2025-02-17 21:13:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:5.550219535827637 norm:0.009819895029067993 max memory_allocated 29231.427734375 
[2025-02-17 21:14:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:5.549615859985352 norm:0.009509813971817493 max memory_allocated 29231.427734375 
[2025-02-17 21:14:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:5.552756309509277 norm:0.009487861767411232 max memory_allocated 29231.427734375 
[2025-02-17 21:15:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:5.553377151489258 norm:0.00989234447479248 max memory_allocated 29231.427734375 
[2025-02-17 21:16:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:5.549191474914551 norm:0.009765321388840675 max memory_allocated 29231.427734375 
[2025-02-17 21:17:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:5.544895172119141 norm:0.009467808529734612 max memory_allocated 29231.427734375 
[2025-02-17 21:17:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:5.542481422424316 norm:0.009081839583814144 max memory_allocated 29231.427734375 
[2025-02-17 21:18:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:5.544441223144531 norm:0.009183544665575027 max memory_allocated 29231.427734375 
[2025-02-17 21:19:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:5.54633903503418 norm:0.009046261198818684 max memory_allocated 29231.427734375 
[2025-02-17 21:20:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:5.547313213348389 norm:0.009895418770611286 max memory_allocated 29231.427734375 
[2025-02-17 21:20:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:5.543426990509033 norm:0.010327614843845367 max memory_allocated 29231.427734375 
[2025-02-17 21:21:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:5.540637493133545 norm:0.010317598469555378 max memory_allocated 29231.427734375 
[2025-02-17 21:22:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:5.539797782897949 norm:0.010157182812690735 max memory_allocated 29231.427734375 
[2025-02-17 21:23:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:5.538015365600586 norm:0.010269138030707836 max memory_allocated 29231.427734375 
[2025-02-17 21:23:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:5.53642463684082 norm:0.010417691431939602 max memory_allocated 29231.427734375 
[2025-02-17 21:24:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:5.535140037536621 norm:0.010294247418642044 max memory_allocated 29231.427734375 
[2025-02-17 21:25:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:5.533286094665527 norm:0.010359350591897964 max memory_allocated 29231.427734375 
[2025-02-17 21:25:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-17 21:26:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:5.987844467163086 norm:0.013733149506151676 max memory_allocated 29231.615234375 
[2025-02-17 21:27:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:5.984549045562744 norm:0.008612960577011108 max memory_allocated 29231.615234375 
[2025-02-17 21:27:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:5.983107566833496 norm:0.006604390684515238 max memory_allocated 29231.615234375 
[2025-02-17 21:28:37 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:5.984213352203369 norm:0.005086828488856554 max memory_allocated 29231.615234375 
[2025-02-17 21:29:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:5.983941555023193 norm:0.004166122060269117 max memory_allocated 29231.615234375 
[2025-02-17 21:30:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:5.982091426849365 norm:0.003772271331399679 max memory_allocated 29231.615234375 
[2025-02-17 21:30:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:5.980953693389893 norm:0.003486023750156164 max memory_allocated 29231.615234375 
[2025-02-17 21:31:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:5.977992534637451 norm:0.0033058617264032364 max memory_allocated 29231.615234375 
[2025-02-17 21:32:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:5.975345611572266 norm:0.0032925265841186047 max memory_allocated 29231.615234375 
[2025-02-17 21:33:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:5.971673965454102 norm:0.003256760770455003 max memory_allocated 29231.615234375 
[2025-02-17 21:33:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:5.967667579650879 norm:0.004192760679870844 max memory_allocated 29231.615234375 
[2025-02-17 21:34:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:5.9631266593933105 norm:0.09014800190925598 max memory_allocated 29231.615234375 
[2025-02-17 21:35:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:5.963405609130859 norm:0.1282208263874054 max memory_allocated 29231.615234375 
[2025-02-17 21:36:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:5.951791763305664 norm:0.0726860836148262 max memory_allocated 29231.615234375 
[2025-02-17 21:36:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:5.947516918182373 norm:0.06903394311666489 max memory_allocated 29231.615234375 
[2025-02-17 21:37:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:5.94837760925293 norm:0.07817409187555313 max memory_allocated 29231.615234375 
[2025-02-17 21:38:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:5.944119453430176 norm:0.0768563374876976 max memory_allocated 29231.615234375 
[2025-02-17 21:39:01 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:5.93593692779541 norm:0.05566588416695595 max memory_allocated 29231.615234375 
[2025-02-17 21:39:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:5.931875705718994 norm:0.04469314217567444 max memory_allocated 29231.615234375 
[2025-02-17 21:40:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:5.930891036987305 norm:0.054731275886297226 max memory_allocated 29231.615234375 
[2025-02-17 21:40:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-17 21:41:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:6.393994331359863 norm:0.013132060877978802 max memory_allocated 29231.802734375 
[2025-02-17 21:42:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:6.3836750984191895 norm:0.009241743944585323 max memory_allocated 29231.802734375 
[2025-02-17 21:43:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:6.381309509277344 norm:0.007510045543313026 max memory_allocated 29231.802734375 
[2025-02-17 21:43:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:6.3807902336120605 norm:0.006161408498883247 max memory_allocated 29231.802734375 
[2025-02-17 21:44:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:6.379251003265381 norm:0.005414438433945179 max memory_allocated 29231.802734375 
[2025-02-17 21:45:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:6.377386569976807 norm:0.00473007233813405 max memory_allocated 29231.802734375 
[2025-02-17 21:45:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:6.375707149505615 norm:0.004215717315673828 max memory_allocated 29231.802734375 
[2025-02-17 21:46:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:6.374269962310791 norm:0.00383780081756413 max memory_allocated 29231.802734375 
[2025-02-17 21:47:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:6.372997760772705 norm:0.0036524657625705004 max memory_allocated 29231.802734375 
[2025-02-17 21:48:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:6.37126350402832 norm:0.003378380322828889 max memory_allocated 29231.802734375 
[2025-02-17 21:48:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:6.37012243270874 norm:0.0032792130950838327 max memory_allocated 29231.802734375 
[2025-02-17 21:49:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:6.369398593902588 norm:0.0031267660669982433 max memory_allocated 29231.802734375 
[2025-02-17 21:50:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:6.368566036224365 norm:0.0030195103026926517 max memory_allocated 29231.802734375 
[2025-02-17 21:51:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:6.368127822875977 norm:0.002950395690277219 max memory_allocated 29231.802734375 
[2025-02-17 21:51:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:6.367265701293945 norm:0.0028793374076485634 max memory_allocated 29231.802734375 
[2025-02-17 21:52:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:6.366185188293457 norm:0.002816393505781889 max memory_allocated 29231.802734375 
[2025-02-17 21:53:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:6.36523962020874 norm:0.0027517054695636034 max memory_allocated 29231.802734375 
[2025-02-17 21:54:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:6.364856243133545 norm:0.002719624899327755 max memory_allocated 29231.802734375 
[2025-02-17 21:54:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:6.364887237548828 norm:0.0026815184392035007 max memory_allocated 29231.802734375 
[2025-02-17 21:55:38 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:6.36500883102417 norm:0.0026628882624208927 max memory_allocated 29231.802734375 
[2025-02-17 21:55:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-17 21:56:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:6.848275661468506 norm:0.01065770536661148 max memory_allocated 29231.990234375 
[2025-02-17 21:57:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:6.8398942947387695 norm:0.0071130297146737576 max memory_allocated 29231.990234375 
[2025-02-17 21:58:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:6.835508346557617 norm:0.005403181072324514 max memory_allocated 29231.990234375 
[2025-02-17 21:58:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:6.832828521728516 norm:0.004221286159008741 max memory_allocated 29231.990234375 
[2025-02-17 21:59:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:6.831846237182617 norm:0.003463819157332182 max memory_allocated 29231.990234375 
[2025-02-17 22:00:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:6.831263065338135 norm:0.002917415928095579 max memory_allocated 29231.990234375 
[2025-02-17 22:01:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:6.830259323120117 norm:0.002666245214641094 max memory_allocated 29231.990234375 
[2025-02-17 22:01:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:6.8292765617370605 norm:0.0025802012532949448 max memory_allocated 29231.990234375 
[2025-02-17 22:02:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:6.827387809753418 norm:0.002585223177447915 max memory_allocated 29231.990234375 
[2025-02-17 22:03:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:6.826657295227051 norm:0.0025849887169897556 max memory_allocated 29231.990234375 
[2025-02-17 22:04:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:6.825249671936035 norm:0.002571272663772106 max memory_allocated 29231.990234375 
[2025-02-17 22:04:48 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:6.824562072753906 norm:0.0025529528502374887 max memory_allocated 29231.990234375 
[2025-02-17 22:05:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:6.823424816131592 norm:0.00256177200935781 max memory_allocated 29231.990234375 
[2025-02-17 22:06:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:6.8224101066589355 norm:0.002529237186536193 max memory_allocated 29231.990234375 
[2025-02-17 22:07:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:6.8214240074157715 norm:0.002546465490013361 max memory_allocated 29231.990234375 
[2025-02-17 22:07:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:6.820158004760742 norm:0.002532286336645484 max memory_allocated 29231.990234375 
[2025-02-17 22:08:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:6.820049285888672 norm:0.0025212515611201525 max memory_allocated 29231.990234375 
[2025-02-17 22:09:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:6.819714069366455 norm:0.0025416119024157524 max memory_allocated 29231.990234375 
[2025-02-17 22:10:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:6.819662094116211 norm:0.0025596097111701965 max memory_allocated 29231.990234375 
[2025-02-17 22:10:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:6.819493293762207 norm:0.0025711492635309696 max memory_allocated 29231.990234375 
[2025-02-17 22:10:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-17 22:11:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:7.274175643920898 norm:0.013191038742661476 max memory_allocated 29232.177734375 
[2025-02-17 22:12:30 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:7.2674336433410645 norm:0.009341055527329445 max memory_allocated 29232.177734375 
[2025-02-17 22:13:14 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:7.262560844421387 norm:0.007198695559054613 max memory_allocated 29232.177734375 
[2025-02-17 22:13:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:7.259333610534668 norm:0.006012423895299435 max memory_allocated 29232.177734375 
[2025-02-17 22:14:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:7.257242679595947 norm:0.005098999012261629 max memory_allocated 29232.177734375 
[2025-02-17 22:15:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:7.2549920082092285 norm:0.0043900227174162865 max memory_allocated 29232.177734375 
[2025-02-17 22:16:12 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:7.253340721130371 norm:0.003959274850785732 max memory_allocated 29232.177734375 
[2025-02-17 22:16:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:7.25143575668335 norm:0.003624032251536846 max memory_allocated 29232.177734375 
[2025-02-17 22:17:42 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:7.250004768371582 norm:0.0034103074576705694 max memory_allocated 29232.177734375 
[2025-02-17 22:18:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:7.248843193054199 norm:0.0032297989819198847 max memory_allocated 29232.177734375 
[2025-02-17 22:19:11 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:7.247801303863525 norm:0.0030976966954767704 max memory_allocated 29232.177734375 
[2025-02-17 22:19:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:7.247167587280273 norm:0.002982238307595253 max memory_allocated 29232.177734375 
[2025-02-17 22:20:40 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:7.2462477684021 norm:0.0028885381761938334 max memory_allocated 29232.177734375 
[2025-02-17 22:21:25 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:7.245800495147705 norm:0.0028234287165105343 max memory_allocated 29232.177734375 
[2025-02-17 22:22:09 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:7.24470329284668 norm:0.0027677163016051054 max memory_allocated 29232.177734375 
[2025-02-17 22:22:54 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:7.244358062744141 norm:0.0027492200024425983 max memory_allocated 29232.177734375 
[2025-02-17 22:23:38 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:7.24395751953125 norm:0.002726843347772956 max memory_allocated 29232.177734375 
[2025-02-17 22:24:23 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:7.243321418762207 norm:0.0027242167852818966 max memory_allocated 29232.177734375 
[2025-02-17 22:25:07 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:7.242674827575684 norm:0.002689564134925604 max memory_allocated 29232.177734375 
[2025-02-17 22:25:52 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:7.24249267578125 norm:0.0026960624381899834 max memory_allocated 29232.177734375 
[2025-02-17 22:26:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-17 22:26:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:7.7909650802612305 norm:0.011122975498437881 max memory_allocated 29232.365234375 
[2025-02-17 22:27:37 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:7.782282829284668 norm:0.007875955663621426 max memory_allocated 29232.365234375 
[2025-02-17 22:28:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:7.776691436767578 norm:0.006235492881387472 max memory_allocated 29232.365234375 
[2025-02-17 22:29:06 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:7.774474143981934 norm:0.005178411491215229 max memory_allocated 29232.365234375 
[2025-02-17 22:29:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:7.773050785064697 norm:0.00445896852761507 max memory_allocated 29232.365234375 
[2025-02-17 22:30:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:7.772434711456299 norm:0.003961872309446335 max memory_allocated 29232.365234375 
[2025-02-17 22:31:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:7.772127151489258 norm:0.0036333047319203615 max memory_allocated 29232.365234375 
[2025-02-17 22:32:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:7.771225929260254 norm:0.0032396831084042788 max memory_allocated 29232.365234375 
[2025-02-17 22:32:49 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:7.770750045776367 norm:0.0029749851673841476 max memory_allocated 29232.365234375 
[2025-02-17 22:33:33 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:7.7701873779296875 norm:0.0028057124000042677 max memory_allocated 29232.365234375 
[2025-02-17 22:34:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:7.7692484855651855 norm:0.0026817386969923973 max memory_allocated 29232.365234375 
[2025-02-17 22:35:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:7.768233299255371 norm:0.0025979136116802692 max memory_allocated 29232.365234375 
[2025-02-17 22:35:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:7.766474723815918 norm:0.0025057655293494463 max memory_allocated 29232.365234375 
[2025-02-17 22:36:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:7.765303611755371 norm:0.0024589705280959606 max memory_allocated 29232.365234375 
[2025-02-17 22:37:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:7.76506233215332 norm:0.0024490945506840944 max memory_allocated 29232.365234375 
[2025-02-17 22:38:01 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:7.764787673950195 norm:0.002405818784609437 max memory_allocated 29232.365234375 
[2025-02-17 22:38:45 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:7.764382362365723 norm:0.0023899113293737173 max memory_allocated 29232.365234375 
[2025-02-17 22:39:30 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:7.764089584350586 norm:0.0023866118863224983 max memory_allocated 29232.365234375 
[2025-02-17 22:40:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:7.764244079589844 norm:0.002392769558355212 max memory_allocated 29232.365234375 
[2025-02-17 22:40:59 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:7.76449728012085 norm:0.0023859073407948017 max memory_allocated 29232.365234375 
[2025-02-17 22:41:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-17 22:41:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:8.443185806274414 norm:0.025803493335843086 max memory_allocated 29232.552734375 
[2025-02-17 22:42:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:8.432924270629883 norm:0.019101930782198906 max memory_allocated 29232.552734375 
[2025-02-17 22:43:28 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:8.429893493652344 norm:0.015384208410978317 max memory_allocated 29232.552734375 
[2025-02-17 22:44:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:8.428437232971191 norm:0.01233545783907175 max memory_allocated 29232.552734375 
[2025-02-17 22:44:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:8.426726341247559 norm:0.010199964046478271 max memory_allocated 29232.552734375 
[2025-02-17 22:45:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:8.424344062805176 norm:0.008687710389494896 max memory_allocated 29232.552734375 
[2025-02-17 22:46:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:8.421527862548828 norm:0.007677930407226086 max memory_allocated 29232.552734375 
[2025-02-17 22:47:11 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:8.419427871704102 norm:0.006846826057881117 max memory_allocated 29232.552734375 
[2025-02-17 22:47:56 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:8.41701889038086 norm:0.0061941747553646564 max memory_allocated 29232.552734375 
[2025-02-17 22:48:41 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:8.41303825378418 norm:0.005593437235802412 max memory_allocated 29232.552734375 
[2025-02-17 22:49:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:8.409868240356445 norm:0.005125509575009346 max memory_allocated 29232.552734375 
[2025-02-17 22:50:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:8.406733512878418 norm:0.0047083087265491486 max memory_allocated 29232.552734375 
[2025-02-17 22:50:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:8.404425621032715 norm:0.004350040107965469 max memory_allocated 29232.552734375 
[2025-02-17 22:51:39 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:8.402766227722168 norm:0.004128331318497658 max memory_allocated 29232.552734375 
[2025-02-17 22:52:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:8.40176010131836 norm:0.003923817537724972 max memory_allocated 29232.552734375 
[2025-02-17 22:53:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:8.400277137756348 norm:0.0037429719232022762 max memory_allocated 29232.552734375 
[2025-02-17 22:53:53 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:8.399293899536133 norm:0.00360605726018548 max memory_allocated 29232.552734375 
[2025-02-17 22:54:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:8.39872932434082 norm:0.003517754375934601 max memory_allocated 29232.552734375 
[2025-02-17 22:55:22 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:8.397758483886719 norm:0.0034028920345008373 max memory_allocated 29232.552734375 
[2025-02-17 22:56:06 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:8.396547317504883 norm:0.0032671988010406494 max memory_allocated 29232.552734375 
[2025-02-17 22:56:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-17 22:57:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:9.096661567687988 norm:0.019573500379920006 max memory_allocated 29232.740234375 
[2025-02-17 22:57:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:9.085488319396973 norm:0.015130730345845222 max memory_allocated 29232.740234375 
[2025-02-17 22:58:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:9.080188751220703 norm:0.012274826876819134 max memory_allocated 29232.740234375 
[2025-02-17 22:59:20 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:9.076889991760254 norm:0.010388828814029694 max memory_allocated 29232.740234375 
[2025-02-17 23:00:05 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:9.075098991394043 norm:0.009242076426744461 max memory_allocated 29232.740234375 
[2025-02-17 23:00:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:9.068981170654297 norm:0.008096975274384022 max memory_allocated 29232.740234375 
[2025-02-17 23:01:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:9.064456939697266 norm:0.007294110953807831 max memory_allocated 29232.740234375 
[2025-02-17 23:02:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:9.061259269714355 norm:0.006578870117664337 max memory_allocated 29232.740234375 
[2025-02-17 23:03:03 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:9.055949211120605 norm:0.006082187872380018 max memory_allocated 29232.740234375 
[2025-02-17 23:03:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:9.052543640136719 norm:0.005692957900464535 max memory_allocated 29232.740234375 
[2025-02-17 23:04:32 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:9.048718452453613 norm:0.0052366009913384914 max memory_allocated 29232.740234375 
[2025-02-17 23:05:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:9.045753479003906 norm:0.004991666879504919 max memory_allocated 29232.740234375 
[2025-02-17 23:06:01 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:9.045743942260742 norm:0.004891643300652504 max memory_allocated 29232.740234375 
[2025-02-17 23:06:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:9.045909881591797 norm:0.004821183159947395 max memory_allocated 29232.740234375 
[2025-02-17 23:07:31 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:9.044076919555664 norm:0.004729044623672962 max memory_allocated 29232.740234375 
[2025-02-17 23:08:15 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:9.042316436767578 norm:0.004697400610893965 max memory_allocated 29232.740234375 
[2025-02-17 23:09:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:9.040276527404785 norm:0.004682991188019514 max memory_allocated 29232.740234375 
[2025-02-17 23:09:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:9.038442611694336 norm:0.004706459119915962 max memory_allocated 29232.740234375 
[2025-02-17 23:10:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:9.036258697509766 norm:0.004657966084778309 max memory_allocated 29232.740234375 
[2025-02-17 23:11:14 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:9.035238265991211 norm:0.0046075074933469296 max memory_allocated 29232.740234375 
[2025-02-17 23:11:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-17 23:12:14 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:9.820221900939941 norm:0.02120094746351242 max memory_allocated 29232.927734375 
[2025-02-17 23:12:58 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:9.80569839477539 norm:0.015894882380962372 max memory_allocated 29232.927734375 
[2025-02-17 23:13:43 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:9.798513412475586 norm:0.012916620820760727 max memory_allocated 29232.927734375 
[2025-02-17 23:14:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:9.791303634643555 norm:0.010540894232690334 max memory_allocated 29232.927734375 
[2025-02-17 23:15:12 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:9.78445053100586 norm:0.008821426890790462 max memory_allocated 29232.927734375 
[2025-02-17 23:15:57 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:9.779085159301758 norm:0.00760142644867301 max memory_allocated 29232.927734375 
[2025-02-17 23:16:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:9.774968147277832 norm:0.006597917061299086 max memory_allocated 29232.927734375 
[2025-02-17 23:17:26 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:9.770721435546875 norm:0.005914153065532446 max memory_allocated 29232.927734375 
[2025-02-17 23:18:10 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:9.767145156860352 norm:0.005399489775300026 max memory_allocated 29232.927734375 
[2025-02-17 23:18:55 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:9.764925003051758 norm:0.005034169182181358 max memory_allocated 29232.927734375 
[2025-02-17 23:19:39 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:9.76227855682373 norm:0.004742297809571028 max memory_allocated 29232.927734375 
[2025-02-17 23:20:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:9.76025390625 norm:0.0045580328442156315 max memory_allocated 29232.927734375 
[2025-02-17 23:21:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:9.758503913879395 norm:0.004379223566502333 max memory_allocated 29232.927734375 
[2025-02-17 23:21:53 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:9.756022453308105 norm:0.004262454807758331 max memory_allocated 29232.927734375 
[2025-02-17 23:22:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:9.75355052947998 norm:0.004113028757274151 max memory_allocated 29232.927734375 
[2025-02-17 23:23:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:9.75105094909668 norm:0.0039841970428824425 max memory_allocated 29232.927734375 
[2025-02-17 23:24:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:9.749189376831055 norm:0.003900847863405943 max memory_allocated 29232.927734375 
[2025-02-17 23:24:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:9.746543884277344 norm:0.0038051046431064606 max memory_allocated 29232.927734375 
[2025-02-17 23:25:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:9.74480152130127 norm:0.00373683194629848 max memory_allocated 29232.927734375 
[2025-02-17 23:26:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:9.743093490600586 norm:0.0036730796564370394 max memory_allocated 29232.927734375 
[2025-02-17 23:26:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-17 23:27:21 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:10.916704177856445 norm:0.04347657412290573 max memory_allocated 29233.115234375 
[2025-02-17 23:28:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:10.897560119628906 norm:0.030594000592827797 max memory_allocated 29233.115234375 
[2025-02-17 23:28:50 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:10.891288757324219 norm:0.023951927199959755 max memory_allocated 29233.115234375 
[2025-02-17 23:29:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:10.885655403137207 norm:0.019775399938225746 max memory_allocated 29233.115234375 
[2025-02-17 23:30:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:10.87890338897705 norm:0.016186833381652832 max memory_allocated 29233.115234375 
[2025-02-17 23:31:04 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:10.873689651489258 norm:0.013643682934343815 max memory_allocated 29233.115234375 
[2025-02-17 23:31:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:10.867305755615234 norm:0.011703883297741413 max memory_allocated 29233.115234375 
[2025-02-17 23:32:33 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:10.857728958129883 norm:0.010564465075731277 max memory_allocated 29233.115234375 
[2025-02-17 23:33:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:10.848097801208496 norm:0.009302584454417229 max memory_allocated 29233.115234375 
[2025-02-17 23:34:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:10.843755722045898 norm:0.008618584834039211 max memory_allocated 29233.115234375 
[2025-02-17 23:34:47 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:10.837867736816406 norm:0.008247485384345055 max memory_allocated 29233.115234375 
[2025-02-17 23:35:31 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:10.832897186279297 norm:0.007792509160935879 max memory_allocated 29233.115234375 
[2025-02-17 23:36:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:10.82888126373291 norm:0.007446350529789925 max memory_allocated 29233.115234375 
[2025-02-17 23:37:00 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:10.82583999633789 norm:0.007201709318906069 max memory_allocated 29233.115234375 
[2025-02-17 23:37:45 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:10.822071075439453 norm:0.0070413993671536446 max memory_allocated 29233.115234375 
[2025-02-17 23:38:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:10.818782806396484 norm:0.006830793339759111 max memory_allocated 29233.115234375 
[2025-02-17 23:39:14 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:10.815268516540527 norm:0.006694127805531025 max memory_allocated 29233.115234375 
[2025-02-17 23:39:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:10.813067436218262 norm:0.006642922293394804 max memory_allocated 29233.115234375 
[2025-02-17 23:40:43 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:10.81151008605957 norm:0.006608185358345509 max memory_allocated 29233.115234375 
[2025-02-17 23:41:28 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:10.810816764831543 norm:0.006571823265403509 max memory_allocated 29233.115234375 
[2025-02-17 23:41:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-17 23:42:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:15.922399520874023 norm:0.5899612307548523 max memory_allocated 29233.302734375 
[2025-02-17 23:43:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:15.912732124328613 norm:0.5423331260681152 max memory_allocated 29233.302734375 
[2025-02-17 23:43:57 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:15.889694213867188 norm:0.4854314923286438 max memory_allocated 29233.302734375 
[2025-02-17 23:44:42 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:15.910058975219727 norm:0.4606999158859253 max memory_allocated 29233.302734375 
[2025-02-17 23:45:26 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:14.287675857543945 norm:4.299648761749268 max memory_allocated 29233.302734375 
[2025-02-17 23:46:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:14.05721664428711 norm:3.187014579772949 max memory_allocated 29233.302734375 
[2025-02-17 23:46:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:13.982138633728027 norm:2.622377634048462 max memory_allocated 29233.302734375 
[2025-02-17 23:47:40 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:13.936513900756836 norm:1.7734137773513794 max memory_allocated 29233.302734375 
[2025-02-17 23:48:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:13.889201164245605 norm:1.7836962938308716 max memory_allocated 29233.302734375 
[2025-02-17 23:49:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:13.838750839233398 norm:1.2818520069122314 max memory_allocated 29233.302734375 
[2025-02-17 23:49:54 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:13.789270401000977 norm:0.6721867322921753 max memory_allocated 29233.302734375 
[2025-02-17 23:50:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:13.791786193847656 norm:0.6627490520477295 max memory_allocated 29233.302734375 
[2025-02-17 23:51:23 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:13.755966186523438 norm:0.607392430305481 max memory_allocated 29233.302734375 
[2025-02-17 23:52:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:13.784289360046387 norm:0.7339719533920288 max memory_allocated 29233.302734375 
[2025-02-17 23:52:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:13.817400932312012 norm:2.2247509956359863 max memory_allocated 29233.302734375 
[2025-02-17 23:53:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:13.828725814819336 norm:5.662530899047852 max memory_allocated 29233.302734375 
[2025-02-17 23:54:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:13.837409019470215 norm:12.456884384155273 max memory_allocated 29233.302734375 
[2025-02-17 23:55:06 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:13.854622840881348 norm:19.037368774414062 max memory_allocated 29233.302734375 
[2025-02-17 23:55:51 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:13.814912796020508 norm:16.385602951049805 max memory_allocated 29233.302734375 
[2025-02-17 23:56:35 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:13.80247688293457 norm:17.65879249572754 max memory_allocated 29233.302734375 
[2025-02-17 23:56:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-17 23:57:35 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:35.680999755859375 norm:0.24168168008327484 max memory_allocated 29233.490234375 
[2025-02-17 23:58:20 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:35.60757064819336 norm:0.6112450957298279 max memory_allocated 29233.490234375 
[2025-02-17 23:59:04 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:33.37885284423828 norm:30.945341110229492 max memory_allocated 29233.490234375 
[2025-02-17 23:59:49 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:33.27509689331055 norm:57.52579879760742 max memory_allocated 29233.490234375 
[2025-02-18 00:00:34 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:33.48764419555664 norm:91.080322265625 max memory_allocated 29233.490234375 
[2025-02-18 00:01:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:33.32170104980469 norm:127.52371215820312 max memory_allocated 29233.490234375 
[2025-02-18 00:02:03 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:33.34734344482422 norm:139.64410400390625 max memory_allocated 29233.490234375 
[2025-02-18 00:02:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:33.61821746826172 norm:139.82530212402344 max memory_allocated 29233.490234375 
[2025-02-18 00:03:32 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:34.017757415771484 norm:137.0243377685547 max memory_allocated 29233.490234375 
[2025-02-18 00:04:16 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:34.38706970214844 norm:210.82200622558594 max memory_allocated 29233.490234375 
[2025-02-18 00:05:01 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:34.48633575439453 norm:205.323486328125 max memory_allocated 29233.490234375 
[2025-02-18 00:05:46 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:34.32608413696289 norm:201.384521484375 max memory_allocated 29233.490234375 
[2025-02-18 00:06:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:34.09235382080078 norm:190.13365173339844 max memory_allocated 29233.490234375 
[2025-02-18 00:07:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:34.079830169677734 norm:190.8455047607422 max memory_allocated 29233.490234375 
[2025-02-18 00:08:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:34.079402923583984 norm:200.52149963378906 max memory_allocated 29233.490234375 
[2025-02-18 00:08:44 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:34.07012176513672 norm:200.798583984375 max memory_allocated 29233.490234375 
[2025-02-18 00:09:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:34.0463752746582 norm:217.93472290039062 max memory_allocated 29233.490234375 
[2025-02-18 00:10:13 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:34.00674819946289 norm:213.47560119628906 max memory_allocated 29233.490234375 
[2025-02-18 00:10:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:33.94623947143555 norm:224.16055297851562 max memory_allocated 29233.490234375 
[2025-02-18 00:11:43 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:33.880027770996094 norm:244.57723999023438 max memory_allocated 29233.490234375 
[2025-02-18 00:11:55 root] (main_calibration.py 365): INFO 36292.876354932785
