[2025-02-17 14:00:11 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration/llama-7b-hf-w4a4', save_dir='./log-calibration/quant/llama-7b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-17 14:03:46 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-17 14:03:46 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-17 14:03:47 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-17 14:03:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-17 14:04:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.10526538640260696 norm:0.01656818576157093 max memory_allocated 22509.63671875 
[2025-02-17 14:04:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.10064010322093964 norm:0.019013838842511177 max memory_allocated 22509.63671875 
[2025-02-17 14:05:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.10110469162464142 norm:0.02558169513940811 max memory_allocated 22509.63671875 
[2025-02-17 14:05:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.08028621971607208 norm:0.014727269299328327 max memory_allocated 22509.63671875 
[2025-02-17 14:06:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.07152563333511353 norm:0.011845716275274754 max memory_allocated 22509.63671875 
[2025-02-17 14:06:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.06375192850828171 norm:0.009398152120411396 max memory_allocated 22509.63671875 
[2025-02-17 14:07:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.05646176636219025 norm:0.012598029337823391 max memory_allocated 22509.63671875 
[2025-02-17 14:07:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.06131550669670105 norm:0.01428636908531189 max memory_allocated 22509.63671875 
[2025-02-17 14:08:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.06876134872436523 norm:0.012049045413732529 max memory_allocated 22509.63671875 
[2025-02-17 14:08:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.06747892498970032 norm:0.00931435264647007 max memory_allocated 22509.63671875 
[2025-02-17 14:09:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.05957585945725441 norm:0.005417151842266321 max memory_allocated 22509.63671875 
[2025-02-17 14:09:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.05170382186770439 norm:0.007809664122760296 max memory_allocated 22509.63671875 
[2025-02-17 14:10:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.06664562225341797 norm:0.010337404906749725 max memory_allocated 22509.63671875 
[2025-02-17 14:10:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.06441976130008698 norm:0.012498446740210056 max memory_allocated 22509.63671875 
[2025-02-17 14:11:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.06215500086545944 norm:0.005867991130799055 max memory_allocated 22509.63671875 
[2025-02-17 14:11:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.06290893256664276 norm:0.006118119694292545 max memory_allocated 22509.63671875 
[2025-02-17 14:12:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.05065168812870979 norm:0.005291040521115065 max memory_allocated 22509.63671875 
[2025-02-17 14:12:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.06846868246793747 norm:0.009008482098579407 max memory_allocated 22509.63671875 
[2025-02-17 14:13:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.056102827191352844 norm:0.0064328452572226524 max memory_allocated 22509.63671875 
[2025-02-17 14:13:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.059970609843730927 norm:0.0067948768846690655 max memory_allocated 22509.63671875 
[2025-02-17 14:14:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-17 14:14:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.2740974724292755 norm:0.04545421525835991 max memory_allocated 22509.80859375 
[2025-02-17 14:15:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.23973101377487183 norm:0.021552857011556625 max memory_allocated 22509.80859375 
[2025-02-17 14:15:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.25904497504234314 norm:0.03297007456421852 max memory_allocated 22509.80859375 
[2025-02-17 14:16:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.2860427796840668 norm:0.07858262211084366 max memory_allocated 22509.80859375 
[2025-02-17 14:16:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.28189074993133545 norm:0.15173819661140442 max memory_allocated 22509.80859375 
[2025-02-17 14:17:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.1633571982383728 norm:0.017447086051106453 max memory_allocated 22509.80859375 
[2025-02-17 14:17:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.14725160598754883 norm:0.011200806125998497 max memory_allocated 22509.80859375 
[2025-02-17 14:18:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.14260536432266235 norm:0.010145308449864388 max memory_allocated 22509.80859375 
[2025-02-17 14:18:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.14746084809303284 norm:0.0120165403932333 max memory_allocated 22509.80859375 
[2025-02-17 14:19:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.16546417772769928 norm:0.020849358290433884 max memory_allocated 22509.80859375 
[2025-02-17 14:19:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.15492016077041626 norm:0.02622133493423462 max memory_allocated 22509.80859375 
[2025-02-17 14:20:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.13906095921993256 norm:0.00954706035554409 max memory_allocated 22509.80859375 
[2025-02-17 14:20:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.14470571279525757 norm:0.012927128002047539 max memory_allocated 22509.80859375 
[2025-02-17 14:21:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.13618022203445435 norm:0.004697869997471571 max memory_allocated 22509.80859375 
[2025-02-17 14:21:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.14416904747486115 norm:0.00522371893748641 max memory_allocated 22509.80859375 
[2025-02-17 14:22:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.20252425968647003 norm:0.022025812417268753 max memory_allocated 22509.80859375 
[2025-02-17 14:22:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.24903957545757294 norm:0.08036260306835175 max memory_allocated 22509.80859375 
[2025-02-17 14:23:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.25215399265289307 norm:0.055856119841337204 max memory_allocated 22509.80859375 
[2025-02-17 14:23:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.23280122876167297 norm:0.048141881823539734 max memory_allocated 22509.80859375 
[2025-02-17 14:24:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.1644180417060852 norm:0.0204238910228014 max memory_allocated 22509.80859375 
[2025-02-17 14:24:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-17 14:24:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.4728138744831085 norm:0.04837018623948097 max memory_allocated 22509.98046875 
[2025-02-17 14:25:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.362816721200943 norm:0.024670878425240517 max memory_allocated 22509.98046875 
[2025-02-17 14:25:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.3192121088504791 norm:0.016812507063150406 max memory_allocated 22509.98046875 
[2025-02-17 14:26:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.2980087697505951 norm:0.014066550880670547 max memory_allocated 22509.98046875 
[2025-02-17 14:26:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.28846126794815063 norm:0.013141720555722713 max memory_allocated 22509.98046875 
[2025-02-17 14:27:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.2860575318336487 norm:0.012982460670173168 max memory_allocated 22509.98046875 
[2025-02-17 14:27:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.2816542685031891 norm:0.013419301249086857 max memory_allocated 22509.98046875 
[2025-02-17 14:28:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.2777428925037384 norm:0.012574584223330021 max memory_allocated 22509.98046875 
[2025-02-17 14:28:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.27577298879623413 norm:0.011828110553324223 max memory_allocated 22509.98046875 
[2025-02-17 14:29:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.2726556360721588 norm:0.01173359900712967 max memory_allocated 22509.98046875 
[2025-02-17 14:29:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.2714100182056427 norm:0.011743444949388504 max memory_allocated 22509.98046875 
[2025-02-17 14:30:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.2709638774394989 norm:0.011815489269793034 max memory_allocated 22509.98046875 
[2025-02-17 14:30:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.27016741037368774 norm:0.011778228916227818 max memory_allocated 22509.98046875 
[2025-02-17 14:31:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.26827317476272583 norm:0.011348887346684933 max memory_allocated 22509.98046875 
[2025-02-17 14:31:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.2691327631473541 norm:0.012042837217450142 max memory_allocated 22509.98046875 
[2025-02-17 14:32:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.2697902023792267 norm:0.01226698886603117 max memory_allocated 22509.98046875 
[2025-02-17 14:32:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.26860949397087097 norm:0.01183232106268406 max memory_allocated 22509.98046875 
[2025-02-17 14:33:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.2704662084579468 norm:0.012157192453742027 max memory_allocated 22509.98046875 
[2025-02-17 14:33:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.27351483702659607 norm:0.013323264196515083 max memory_allocated 22509.98046875 
[2025-02-17 14:34:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.2715630531311035 norm:0.012174248695373535 max memory_allocated 22509.98046875 
[2025-02-17 14:34:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-17 14:35:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.3573610782623291 norm:0.05413932353258133 max memory_allocated 22510.15234375 
[2025-02-17 14:35:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.3321583569049835 norm:0.023797260597348213 max memory_allocated 22510.15234375 
[2025-02-17 14:36:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.32764795422554016 norm:0.014639843255281448 max memory_allocated 22510.15234375 
[2025-02-17 14:36:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.3197892904281616 norm:0.009125876240432262 max memory_allocated 22510.15234375 
[2025-02-17 14:37:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.31307804584503174 norm:0.006520578172057867 max memory_allocated 22510.15234375 
[2025-02-17 14:37:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.30828896164894104 norm:0.005044255405664444 max memory_allocated 22510.15234375 
[2025-02-17 14:38:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.30489566922187805 norm:0.00402678782120347 max memory_allocated 22510.15234375 
[2025-02-17 14:38:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.30272358655929565 norm:0.0034459743183106184 max memory_allocated 22510.15234375 
[2025-02-17 14:39:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.3018156588077545 norm:0.0030875394586473703 max memory_allocated 22510.15234375 
[2025-02-17 14:39:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.301103800535202 norm:0.0028427373617887497 max memory_allocated 22510.15234375 
[2025-02-17 14:40:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.30090367794036865 norm:0.0027289611753076315 max memory_allocated 22510.15234375 
[2025-02-17 14:40:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.30025291442871094 norm:0.002666142303496599 max memory_allocated 22510.15234375 
[2025-02-17 14:41:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.2997778654098511 norm:0.0025751132052391768 max memory_allocated 22510.15234375 
[2025-02-17 14:41:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.2992810606956482 norm:0.0024660967756062746 max memory_allocated 22510.15234375 
[2025-02-17 14:42:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.2986864745616913 norm:0.002405731473118067 max memory_allocated 22510.15234375 
[2025-02-17 14:42:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.2983054518699646 norm:0.002366351895034313 max memory_allocated 22510.15234375 
[2025-02-17 14:43:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.29770147800445557 norm:0.002343596890568733 max memory_allocated 22510.15234375 
[2025-02-17 14:43:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.2974781095981598 norm:0.0023194453679025173 max memory_allocated 22510.15234375 
[2025-02-17 14:44:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.29730141162872314 norm:0.0023029446601867676 max memory_allocated 22510.15234375 
[2025-02-17 14:44:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.29695358872413635 norm:0.002258867025375366 max memory_allocated 22510.15234375 
[2025-02-17 14:44:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-17 14:45:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.4289601147174835 norm:0.04913291335105896 max memory_allocated 22510.32421875 
[2025-02-17 14:45:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.4095543622970581 norm:0.026407908648252487 max memory_allocated 22510.32421875 
[2025-02-17 14:46:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.4036017954349518 norm:0.015204079449176788 max memory_allocated 22510.32421875 
[2025-02-17 14:46:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.3971441984176636 norm:0.009701332077383995 max memory_allocated 22510.32421875 
[2025-02-17 14:47:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.3918766379356384 norm:0.0066918497905135155 max memory_allocated 22510.32421875 
[2025-02-17 14:47:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.3850765824317932 norm:0.004808621481060982 max memory_allocated 22510.32421875 
[2025-02-17 14:48:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.3816361427307129 norm:0.003946109674870968 max memory_allocated 22510.32421875 
[2025-02-17 14:48:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.37789514660835266 norm:0.0033741584047675133 max memory_allocated 22510.32421875 
[2025-02-17 14:49:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.37475138902664185 norm:0.0029826941899955273 max memory_allocated 22510.32421875 
[2025-02-17 14:49:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.372355192899704 norm:0.0026877205818891525 max memory_allocated 22510.32421875 
[2025-02-17 14:50:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.3722146451473236 norm:0.002492588246241212 max memory_allocated 22510.32421875 
[2025-02-17 14:50:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.37299275398254395 norm:0.0024733608588576317 max memory_allocated 22510.32421875 
[2025-02-17 14:51:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.3725391626358032 norm:0.0024033391382545233 max memory_allocated 22510.32421875 
[2025-02-17 14:51:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.3717828392982483 norm:0.002349548041820526 max memory_allocated 22510.32421875 
[2025-02-17 14:52:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.37198060750961304 norm:0.002355723874643445 max memory_allocated 22510.32421875 
[2025-02-17 14:52:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.37141668796539307 norm:0.002328585833311081 max memory_allocated 22510.32421875 
[2025-02-17 14:53:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.37100258469581604 norm:0.0022986510302871466 max memory_allocated 22510.32421875 
[2025-02-17 14:53:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.3710389733314514 norm:0.0022834946867078543 max memory_allocated 22510.32421875 
[2025-02-17 14:54:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.36955344676971436 norm:0.0022239615209400654 max memory_allocated 22510.32421875 
[2025-02-17 14:54:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.3671914041042328 norm:0.002096037147566676 max memory_allocated 22510.32421875 
[2025-02-17 14:54:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-17 14:55:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.4866255521774292 norm:0.06156660616397858 max memory_allocated 22510.49609375 
[2025-02-17 14:56:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.4582942724227905 norm:0.028248662129044533 max memory_allocated 22510.49609375 
[2025-02-17 14:56:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.45192810893058777 norm:0.016289930790662766 max memory_allocated 22510.49609375 
[2025-02-17 14:57:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.44490912556648254 norm:0.008741531521081924 max memory_allocated 22510.49609375 
[2025-02-17 14:57:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.4413713812828064 norm:0.0063639753498137 max memory_allocated 22510.49609375 
[2025-02-17 14:58:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.4408532679080963 norm:0.005378627218306065 max memory_allocated 22510.49609375 
[2025-02-17 14:58:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.4381427764892578 norm:0.004664198495447636 max memory_allocated 22510.49609375 
[2025-02-17 14:59:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.43485915660858154 norm:0.004086670000106096 max memory_allocated 22510.49609375 
[2025-02-17 14:59:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.43431559205055237 norm:0.0038427719846367836 max memory_allocated 22510.49609375 
[2025-02-17 15:00:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.43312695622444153 norm:0.003615607973188162 max memory_allocated 22510.49609375 
[2025-02-17 15:00:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.43139442801475525 norm:0.003439544001594186 max memory_allocated 22510.49609375 
[2025-02-17 15:01:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.43067875504493713 norm:0.00338705163449049 max memory_allocated 22510.49609375 
[2025-02-17 15:01:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.429155170917511 norm:0.003260800614953041 max memory_allocated 22510.49609375 
[2025-02-17 15:02:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.42820942401885986 norm:0.0032140498515218496 max memory_allocated 22510.49609375 
[2025-02-17 15:02:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.42780014872550964 norm:0.0031825194600969553 max memory_allocated 22510.49609375 
[2025-02-17 15:03:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.4280851185321808 norm:0.003197707701474428 max memory_allocated 22510.49609375 
[2025-02-17 15:03:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.4287605881690979 norm:0.00322067947126925 max memory_allocated 22510.49609375 
[2025-02-17 15:04:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.42857953906059265 norm:0.0032004218082875013 max memory_allocated 22510.49609375 
[2025-02-17 15:04:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.42856884002685547 norm:0.0031785713508725166 max memory_allocated 22510.49609375 
[2025-02-17 15:05:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.4284065365791321 norm:0.0031769112683832645 max memory_allocated 22510.49609375 
[2025-02-17 15:05:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-17 15:05:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.5329837799072266 norm:0.052244946360588074 max memory_allocated 22510.66796875 
[2025-02-17 15:06:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.5096167325973511 norm:0.02559131383895874 max memory_allocated 22510.66796875 
[2025-02-17 15:06:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.5006042718887329 norm:0.013856510631740093 max memory_allocated 22510.66796875 
[2025-02-17 15:07:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.49824824929237366 norm:0.007694577798247337 max memory_allocated 22510.66796875 
[2025-02-17 15:07:48 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.49093353748321533 norm:0.004392118193209171 max memory_allocated 22510.66796875 
[2025-02-17 15:08:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.4867819547653198 norm:0.003430987475439906 max memory_allocated 22510.66796875 
[2025-02-17 15:08:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.4868568181991577 norm:0.0031223148107528687 max memory_allocated 22510.66796875 
[2025-02-17 15:09:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.48632675409317017 norm:0.002872135490179062 max memory_allocated 22510.66796875 
[2025-02-17 15:09:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.4861975312232971 norm:0.002902979962527752 max memory_allocated 22510.66796875 
[2025-02-17 15:10:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.4855785071849823 norm:0.00283042062073946 max memory_allocated 22510.66796875 
[2025-02-17 15:10:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.48357588052749634 norm:0.002543636132031679 max memory_allocated 22510.66796875 
[2025-02-17 15:11:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.4822629988193512 norm:0.002391927409917116 max memory_allocated 22510.66796875 
[2025-02-17 15:11:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.48223185539245605 norm:0.0024240808561444283 max memory_allocated 22510.66796875 
[2025-02-17 15:12:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.48276400566101074 norm:0.002430921420454979 max memory_allocated 22510.66796875 
[2025-02-17 15:12:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.4818154275417328 norm:0.002372730989009142 max memory_allocated 22510.66796875 
[2025-02-17 15:13:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.48195284605026245 norm:0.0023771082051098347 max memory_allocated 22510.66796875 
[2025-02-17 15:13:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.48104843497276306 norm:0.0023891297169029713 max memory_allocated 22510.66796875 
[2025-02-17 15:14:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.4810563027858734 norm:0.0024175946600735188 max memory_allocated 22510.66796875 
[2025-02-17 15:14:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.48087403178215027 norm:0.002390066394582391 max memory_allocated 22510.66796875 
[2025-02-17 15:15:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.48067766427993774 norm:0.002373424358665943 max memory_allocated 22510.66796875 
[2025-02-17 15:15:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-17 15:16:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.6024743318557739 norm:0.04282137006521225 max memory_allocated 22510.83984375 
[2025-02-17 15:16:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.57640141248703 norm:0.020824216306209564 max memory_allocated 22510.83984375 
[2025-02-17 15:17:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.568953275680542 norm:0.010992590337991714 max memory_allocated 22510.83984375 
[2025-02-17 15:17:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.5668411254882812 norm:0.006977478042244911 max memory_allocated 22510.83984375 
[2025-02-17 15:18:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.5614657998085022 norm:0.004348042421042919 max memory_allocated 22510.83984375 
[2025-02-17 15:18:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.5571963787078857 norm:0.003556087613105774 max memory_allocated 22510.83984375 
[2025-02-17 15:19:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.5537843108177185 norm:0.00321272574365139 max memory_allocated 22510.83984375 
[2025-02-17 15:19:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.5502678155899048 norm:0.0030827284790575504 max memory_allocated 22510.83984375 
[2025-02-17 15:20:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.5504443049430847 norm:0.0030427942983806133 max memory_allocated 22510.83984375 
[2025-02-17 15:20:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.5502840280532837 norm:0.003016059985384345 max memory_allocated 22510.83984375 
[2025-02-17 15:21:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.5494722723960876 norm:0.002992097521200776 max memory_allocated 22510.83984375 
[2025-02-17 15:21:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.54841548204422 norm:0.002934270305559039 max memory_allocated 22510.83984375 
[2025-02-17 15:22:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.5480937957763672 norm:0.0029410426504909992 max memory_allocated 22510.83984375 
[2025-02-17 15:22:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.5476419925689697 norm:0.0029063101392239332 max memory_allocated 22510.83984375 
[2025-02-17 15:23:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.5464141964912415 norm:0.0028645708225667477 max memory_allocated 22510.83984375 
[2025-02-17 15:23:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.5460201501846313 norm:0.002861279994249344 max memory_allocated 22510.83984375 
[2025-02-17 15:24:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.5457593202590942 norm:0.002826105337589979 max memory_allocated 22510.83984375 
[2025-02-17 15:24:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.5449827313423157 norm:0.00277735386043787 max memory_allocated 22510.83984375 
[2025-02-17 15:25:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.5447481870651245 norm:0.002784543205052614 max memory_allocated 22510.83984375 
[2025-02-17 15:25:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.5448675155639648 norm:0.00279375072568655 max memory_allocated 22510.83984375 
[2025-02-17 15:25:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-17 15:26:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.6460226774215698 norm:0.03139519691467285 max memory_allocated 22511.01171875 
[2025-02-17 15:26:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.6252959966659546 norm:0.016631945967674255 max memory_allocated 22511.01171875 
[2025-02-17 15:27:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.6171036958694458 norm:0.009466665796935558 max memory_allocated 22511.01171875 
[2025-02-17 15:27:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.616322934627533 norm:0.006772787310183048 max memory_allocated 22511.01171875 
[2025-02-17 15:28:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.6076646447181702 norm:0.004544391296803951 max memory_allocated 22511.01171875 
[2025-02-17 15:28:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.6014279723167419 norm:0.0037738841492682695 max memory_allocated 22511.01171875 
[2025-02-17 15:29:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.593069851398468 norm:0.0031362762674689293 max memory_allocated 22511.01171875 
[2025-02-17 15:29:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.5917954444885254 norm:0.002939586527645588 max memory_allocated 22511.01171875 
[2025-02-17 15:30:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.5909547209739685 norm:0.002885894849896431 max memory_allocated 22511.01171875 
[2025-02-17 15:30:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.5919945240020752 norm:0.002896987833082676 max memory_allocated 22511.01171875 
[2025-02-17 15:31:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.5920232534408569 norm:0.0028895584400743246 max memory_allocated 22511.01171875 
[2025-02-17 15:31:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.5908052325248718 norm:0.002852652221918106 max memory_allocated 22511.01171875 
[2025-02-17 15:32:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.591475784778595 norm:0.0028758475091308355 max memory_allocated 22511.01171875 
[2025-02-17 15:32:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.5921445488929749 norm:0.002903918270021677 max memory_allocated 22511.01171875 
[2025-02-17 15:33:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.5930562019348145 norm:0.002947511151432991 max memory_allocated 22511.01171875 
[2025-02-17 15:33:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.5926191210746765 norm:0.0029291908722370863 max memory_allocated 22511.01171875 
[2025-02-17 15:34:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.5923872590065002 norm:0.002900180174037814 max memory_allocated 22511.01171875 
[2025-02-17 15:34:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.5925631523132324 norm:0.0029279727023094893 max memory_allocated 22511.01171875 
[2025-02-17 15:35:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.5929649472236633 norm:0.0029357296880334616 max memory_allocated 22511.01171875 
[2025-02-17 15:35:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.5926127433776855 norm:0.002922132844105363 max memory_allocated 22511.01171875 
[2025-02-17 15:35:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-17 15:36:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.7133775949478149 norm:0.04009969159960747 max memory_allocated 22511.18359375 
[2025-02-17 15:36:59 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.7002326846122742 norm:0.026210496202111244 max memory_allocated 22511.18359375 
[2025-02-17 15:37:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.6869769096374512 norm:0.01368141733109951 max memory_allocated 22511.18359375 
[2025-02-17 15:38:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.6833493113517761 norm:0.009990955702960491 max memory_allocated 22511.18359375 
[2025-02-17 15:38:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.6792076826095581 norm:0.007351245731115341 max memory_allocated 22511.18359375 
[2025-02-17 15:39:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.6716582179069519 norm:0.004975995048880577 max memory_allocated 22511.18359375 
[2025-02-17 15:39:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.6640985012054443 norm:0.003944322466850281 max memory_allocated 22511.18359375 
[2025-02-17 15:40:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.6615749001502991 norm:0.003532822011038661 max memory_allocated 22511.18359375 
[2025-02-17 15:40:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.660990834236145 norm:0.0033787046559154987 max memory_allocated 22511.18359375 
[2025-02-17 15:41:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.6601683497428894 norm:0.003274502232670784 max memory_allocated 22511.18359375 
[2025-02-17 15:41:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.6605329513549805 norm:0.003267785534262657 max memory_allocated 22511.18359375 
[2025-02-17 15:42:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.6588420867919922 norm:0.0031810817308723927 max memory_allocated 22511.18359375 
[2025-02-17 15:42:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.6575517654418945 norm:0.003040785901248455 max memory_allocated 22511.18359375 
[2025-02-17 15:43:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.6575607657432556 norm:0.0030267154797911644 max memory_allocated 22511.18359375 
[2025-02-17 15:43:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.6583881378173828 norm:0.0030647260136902332 max memory_allocated 22511.18359375 
[2025-02-17 15:44:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.6588479280471802 norm:0.0030953488312661648 max memory_allocated 22511.18359375 
[2025-02-17 15:44:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.6584349870681763 norm:0.0030997053254395723 max memory_allocated 22511.18359375 
[2025-02-17 15:45:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.6585381031036377 norm:0.003076869063079357 max memory_allocated 22511.18359375 
[2025-02-17 15:45:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.6591054201126099 norm:0.003089477540925145 max memory_allocated 22511.18359375 
[2025-02-17 15:46:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.6591777801513672 norm:0.0030860379338264465 max memory_allocated 22511.18359375 
[2025-02-17 15:46:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-17 15:46:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.7201813459396362 norm:0.023249857127666473 max memory_allocated 22511.35546875 
[2025-02-17 15:47:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.7053993344306946 norm:0.01457514800131321 max memory_allocated 22511.35546875 
[2025-02-17 15:47:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.6992025971412659 norm:0.008790942840278149 max memory_allocated 22511.35546875 
[2025-02-17 15:48:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.6976944208145142 norm:0.006060080137103796 max memory_allocated 22511.35546875 
[2025-02-17 15:48:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.6945798993110657 norm:0.004151895642280579 max memory_allocated 22511.35546875 
[2025-02-17 15:49:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.6905855536460876 norm:0.0029156901873648167 max memory_allocated 22511.35546875 
[2025-02-17 15:49:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.6878796219825745 norm:0.0024632092099636793 max memory_allocated 22511.35546875 
[2025-02-17 15:50:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.6845856308937073 norm:0.0021857907995581627 max memory_allocated 22511.35546875 
[2025-02-17 15:50:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.6825830936431885 norm:0.0020212570670992136 max memory_allocated 22511.35546875 
[2025-02-17 15:51:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.6822436451911926 norm:0.001979369204491377 max memory_allocated 22511.35546875 
[2025-02-17 15:51:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.6816309690475464 norm:0.001884445664472878 max memory_allocated 22511.35546875 
[2025-02-17 15:52:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.6796955466270447 norm:0.001810951391234994 max memory_allocated 22511.35546875 
[2025-02-17 15:52:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.6779244542121887 norm:0.0016934091690927744 max memory_allocated 22511.35546875 
[2025-02-17 15:53:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.6777980327606201 norm:0.0016809420194476843 max memory_allocated 22511.35546875 
[2025-02-17 15:53:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.6768306493759155 norm:0.0016331755323335528 max memory_allocated 22511.35546875 
[2025-02-17 15:54:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.6765164732933044 norm:0.0016072524012997746 max memory_allocated 22511.35546875 
[2025-02-17 15:54:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.6763556003570557 norm:0.0015789297176524997 max memory_allocated 22511.35546875 
[2025-02-17 15:55:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.6756986975669861 norm:0.001558154821395874 max memory_allocated 22511.35546875 
[2025-02-17 15:55:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.6756302714347839 norm:0.0015800718683749437 max memory_allocated 22511.35546875 
[2025-02-17 15:56:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.6756251454353333 norm:0.0015727188438177109 max memory_allocated 22511.35546875 
[2025-02-17 15:56:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-17 15:56:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.7462943196296692 norm:0.02863745205104351 max memory_allocated 22511.52734375 
[2025-02-17 15:57:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.7288687229156494 norm:0.01737036183476448 max memory_allocated 22511.52734375 
[2025-02-17 15:57:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.7207859754562378 norm:0.01095559075474739 max memory_allocated 22511.52734375 
[2025-02-17 15:58:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.7155603766441345 norm:0.006757525727152824 max memory_allocated 22511.52734375 
[2025-02-17 15:58:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.7151196599006653 norm:0.00517007801681757 max memory_allocated 22511.52734375 
[2025-02-17 15:59:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.7123749256134033 norm:0.004102675709873438 max memory_allocated 22511.52734375 
[2025-02-17 15:59:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.706656277179718 norm:0.0029499728698283434 max memory_allocated 22511.52734375 
[2025-02-17 16:00:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.7039756774902344 norm:0.0023826011456549168 max memory_allocated 22511.52734375 
[2025-02-17 16:00:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.7027325630187988 norm:0.0021468335762619972 max memory_allocated 22511.52734375 
[2025-02-17 16:01:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.703129231929779 norm:0.0020194484386593103 max memory_allocated 22511.52734375 
[2025-02-17 16:01:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.7026352882385254 norm:0.001950554782524705 max memory_allocated 22511.52734375 
[2025-02-17 16:02:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.7005668878555298 norm:0.0018319900846108794 max memory_allocated 22511.52734375 
[2025-02-17 16:02:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.7002789974212646 norm:0.0017830324359238148 max memory_allocated 22511.52734375 
[2025-02-17 16:03:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.6990573406219482 norm:0.0017012024763971567 max memory_allocated 22511.52734375 
[2025-02-17 16:03:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.6974006295204163 norm:0.0016479039331898093 max memory_allocated 22511.52734375 
[2025-02-17 16:04:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.696802020072937 norm:0.001634176354855299 max memory_allocated 22511.52734375 
[2025-02-17 16:04:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.6972683072090149 norm:0.001630839193239808 max memory_allocated 22511.52734375 
[2025-02-17 16:05:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.6971158385276794 norm:0.001641124370507896 max memory_allocated 22511.52734375 
[2025-02-17 16:05:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.6965451240539551 norm:0.0016536156181246042 max memory_allocated 22511.52734375 
[2025-02-17 16:06:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.6968252062797546 norm:0.0017821269575506449 max memory_allocated 22511.52734375 
[2025-02-17 16:06:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-17 16:07:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.7705575227737427 norm:0.03428133577108383 max memory_allocated 22511.69921875 
[2025-02-17 16:07:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.7576647996902466 norm:0.020779110491275787 max memory_allocated 22511.69921875 
[2025-02-17 16:08:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.7496262192726135 norm:0.013109961524605751 max memory_allocated 22511.69921875 
[2025-02-17 16:08:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.7431164979934692 norm:0.008465646766126156 max memory_allocated 22511.69921875 
[2025-02-17 16:09:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.7372169494628906 norm:0.006247575860470533 max memory_allocated 22511.69921875 
[2025-02-17 16:09:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.7335895895957947 norm:0.0048720575869083405 max memory_allocated 22511.69921875 
[2025-02-17 16:10:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.7311949729919434 norm:0.003916366025805473 max memory_allocated 22511.69921875 
[2025-02-17 16:10:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.7279722094535828 norm:0.003231210634112358 max memory_allocated 22511.69921875 
[2025-02-17 16:11:11 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.7265586256980896 norm:0.0028196373023092747 max memory_allocated 22511.69921875 
[2025-02-17 16:11:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.7272900342941284 norm:0.0027035893872380257 max memory_allocated 22511.69921875 
[2025-02-17 16:12:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.728163480758667 norm:0.0025379706639796495 max memory_allocated 22511.69921875 
[2025-02-17 16:12:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.7280657887458801 norm:0.0024980613961815834 max memory_allocated 22511.69921875 
[2025-02-17 16:13:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.7276307344436646 norm:0.002371162408962846 max memory_allocated 22511.69921875 
[2025-02-17 16:13:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.7270222902297974 norm:0.0022824194747954607 max memory_allocated 22511.69921875 
[2025-02-17 16:14:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.7257336378097534 norm:0.0021435876842588186 max memory_allocated 22511.69921875 
[2025-02-17 16:14:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.7243550419807434 norm:0.0020727524533867836 max memory_allocated 22511.69921875 
[2025-02-17 16:15:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.7229184508323669 norm:0.0019855492282658815 max memory_allocated 22511.69921875 
[2025-02-17 16:15:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.7219032645225525 norm:0.0019453979330137372 max memory_allocated 22511.69921875 
[2025-02-17 16:16:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.7205963730812073 norm:0.0018906396580860019 max memory_allocated 22511.69921875 
[2025-02-17 16:16:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.719988226890564 norm:0.0018666117684915662 max memory_allocated 22511.69921875 
[2025-02-17 16:16:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-17 16:17:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.7936069369316101 norm:0.023391012102365494 max memory_allocated 22511.87109375 
[2025-02-17 16:17:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.7802268862724304 norm:0.01642300933599472 max memory_allocated 22511.87109375 
[2025-02-17 16:18:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.7721831798553467 norm:0.011697022244334221 max memory_allocated 22511.87109375 
[2025-02-17 16:18:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.7586034536361694 norm:0.006536561530083418 max memory_allocated 22511.87109375 
[2025-02-17 16:19:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.7542908787727356 norm:0.004630038049072027 max memory_allocated 22511.87109375 
[2025-02-17 16:19:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.7508867383003235 norm:0.003509888658300042 max memory_allocated 22511.87109375 
[2025-02-17 16:20:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.7482840418815613 norm:0.003072855994105339 max memory_allocated 22511.87109375 
[2025-02-17 16:20:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.7494926452636719 norm:0.002893701195716858 max memory_allocated 22511.87109375 
[2025-02-17 16:21:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.7500425577163696 norm:0.0028453173581510782 max memory_allocated 22511.87109375 
[2025-02-17 16:21:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.7511965036392212 norm:0.0027983966283500195 max memory_allocated 22511.87109375 
[2025-02-17 16:22:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.7531273365020752 norm:0.002902063075453043 max memory_allocated 22511.87109375 
[2025-02-17 16:22:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.7541228532791138 norm:0.002912591677159071 max memory_allocated 22511.87109375 
[2025-02-17 16:23:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.7509539127349854 norm:0.0027916752733290195 max memory_allocated 22511.87109375 
[2025-02-17 16:23:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.7517452239990234 norm:0.002749884966760874 max memory_allocated 22511.87109375 
[2025-02-17 16:24:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.7524154186248779 norm:0.002771347761154175 max memory_allocated 22511.87109375 
[2025-02-17 16:24:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.7521146535873413 norm:0.002777866553515196 max memory_allocated 22511.87109375 
[2025-02-17 16:25:26 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.7523235082626343 norm:0.002745844656601548 max memory_allocated 22511.87109375 
[2025-02-17 16:25:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.7521747946739197 norm:0.0027777382638305426 max memory_allocated 22511.87109375 
[2025-02-17 16:26:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.7520523071289062 norm:0.002781229093670845 max memory_allocated 22511.87109375 
[2025-02-17 16:26:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.7524372339248657 norm:0.0027769547887146473 max memory_allocated 22511.87109375 
[2025-02-17 16:27:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-17 16:27:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.8607826232910156 norm:0.04760991036891937 max memory_allocated 22512.04296875 
[2025-02-17 16:28:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.8481390476226807 norm:0.030755115672945976 max memory_allocated 22512.04296875 
[2025-02-17 16:28:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.8361753225326538 norm:0.018543731421232224 max memory_allocated 22512.04296875 
[2025-02-17 16:29:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.8311437964439392 norm:0.012485242448747158 max memory_allocated 22512.04296875 
[2025-02-17 16:29:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.8281521797180176 norm:0.009101780131459236 max memory_allocated 22512.04296875 
[2025-02-17 16:30:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.8257326483726501 norm:0.007094379048794508 max memory_allocated 22512.04296875 
[2025-02-17 16:30:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.8189753890037537 norm:0.005285264458507299 max memory_allocated 22512.04296875 
[2025-02-17 16:31:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.8137766718864441 norm:0.004355902783572674 max memory_allocated 22512.04296875 
[2025-02-17 16:31:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.8110056519508362 norm:0.003799088764935732 max memory_allocated 22512.04296875 
[2025-02-17 16:32:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.8086411952972412 norm:0.0032446179538965225 max memory_allocated 22512.04296875 
[2025-02-17 16:32:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.8067304491996765 norm:0.0029520466923713684 max memory_allocated 22512.04296875 
[2025-02-17 16:33:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.8060917258262634 norm:0.002870251890271902 max memory_allocated 22512.04296875 
[2025-02-17 16:33:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.8063292503356934 norm:0.002844211645424366 max memory_allocated 22512.04296875 
[2025-02-17 16:34:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.8057801127433777 norm:0.0027239476330578327 max memory_allocated 22512.04296875 
[2025-02-17 16:34:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.8042138814926147 norm:0.0025661075487732887 max memory_allocated 22512.04296875 
[2025-02-17 16:35:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.8032670617103577 norm:0.0024774058256298304 max memory_allocated 22512.04296875 
[2025-02-17 16:35:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.8034555315971375 norm:0.0024291048757731915 max memory_allocated 22512.04296875 
[2025-02-17 16:36:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.8040751814842224 norm:0.002437929855659604 max memory_allocated 22512.04296875 
[2025-02-17 16:36:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.804594874382019 norm:0.0024521907325834036 max memory_allocated 22512.04296875 
[2025-02-17 16:37:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.8046911358833313 norm:0.0024270866997539997 max memory_allocated 22512.04296875 
[2025-02-17 16:37:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-17 16:38:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.8765552043914795 norm:0.03603164106607437 max memory_allocated 22512.21484375 
[2025-02-17 16:38:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.8617087602615356 norm:0.020247306674718857 max memory_allocated 22512.21484375 
[2025-02-17 16:39:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.8550708293914795 norm:0.013154116459190845 max memory_allocated 22512.21484375 
[2025-02-17 16:39:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.8503246903419495 norm:0.008152785710990429 max memory_allocated 22512.21484375 
[2025-02-17 16:40:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.8487592339515686 norm:0.006026600953191519 max memory_allocated 22512.21484375 
[2025-02-17 16:40:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.8471195101737976 norm:0.004920282866805792 max memory_allocated 22512.21484375 
[2025-02-17 16:41:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.8456754088401794 norm:0.0042436919175088406 max memory_allocated 22512.21484375 
[2025-02-17 16:41:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.8434597849845886 norm:0.0035776528529822826 max memory_allocated 22512.21484375 
[2025-02-17 16:42:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.8424603939056396 norm:0.0030601087491959333 max memory_allocated 22512.21484375 
[2025-02-17 16:42:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.8410607576370239 norm:0.002787589095532894 max memory_allocated 22512.21484375 
[2025-02-17 16:43:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.8393303155899048 norm:0.0023959006648510695 max memory_allocated 22512.21484375 
[2025-02-17 16:43:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.8376977443695068 norm:0.00213037827052176 max memory_allocated 22512.21484375 
[2025-02-17 16:44:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.8374027609825134 norm:0.0020446341950446367 max memory_allocated 22512.21484375 
[2025-02-17 16:44:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.8370655179023743 norm:0.0019468053942546248 max memory_allocated 22512.21484375 
[2025-02-17 16:45:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.836341142654419 norm:0.00182810437399894 max memory_allocated 22512.21484375 
[2025-02-17 16:45:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.8353980779647827 norm:0.0017440748633816838 max memory_allocated 22512.21484375 
[2025-02-17 16:46:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.8352199196815491 norm:0.001724773203022778 max memory_allocated 22512.21484375 
[2025-02-17 16:46:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.834714949131012 norm:0.0016661575064063072 max memory_allocated 22512.21484375 
[2025-02-17 16:47:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.8335145711898804 norm:0.0016071932623162866 max memory_allocated 22512.21484375 
[2025-02-17 16:47:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.8326590061187744 norm:0.0015666827093809843 max memory_allocated 22512.21484375 
[2025-02-17 16:47:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-17 16:48:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.9632068276405334 norm:0.04049547389149666 max memory_allocated 22512.38671875 
[2025-02-17 16:48:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.9485213756561279 norm:0.02684605121612549 max memory_allocated 22512.38671875 
[2025-02-17 16:49:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.9366528391838074 norm:0.01646357774734497 max memory_allocated 22512.38671875 
[2025-02-17 16:49:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.9329572916030884 norm:0.011002159677445889 max memory_allocated 22512.38671875 
[2025-02-17 16:50:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.9322898387908936 norm:0.007420971989631653 max memory_allocated 22512.38671875 
[2025-02-17 16:50:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.9337947964668274 norm:0.005843231920152903 max memory_allocated 22512.38671875 
[2025-02-17 16:51:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.9330936670303345 norm:0.005160457454621792 max memory_allocated 22512.38671875 
[2025-02-17 16:51:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.9312880635261536 norm:0.004572072997689247 max memory_allocated 22512.38671875 
[2025-02-17 16:52:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.9290961623191833 norm:0.004203479737043381 max memory_allocated 22512.38671875 
[2025-02-17 16:52:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.928878903388977 norm:0.004020504653453827 max memory_allocated 22512.38671875 
[2025-02-17 16:53:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.9294703006744385 norm:0.004022589884698391 max memory_allocated 22512.38671875 
[2025-02-17 16:53:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.9295639991760254 norm:0.003949020989239216 max memory_allocated 22512.38671875 
[2025-02-17 16:54:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.9270831942558289 norm:0.003754996694624424 max memory_allocated 22512.38671875 
[2025-02-17 16:54:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.9240663051605225 norm:0.003570647444576025 max memory_allocated 22512.38671875 
[2025-02-17 16:55:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.9231924414634705 norm:0.0035241013392806053 max memory_allocated 22512.38671875 
[2025-02-17 16:55:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.9230732917785645 norm:0.0035324147902429104 max memory_allocated 22512.38671875 
[2025-02-17 16:56:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.9228414297103882 norm:0.003498128615319729 max memory_allocated 22512.38671875 
[2025-02-17 16:56:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.9230546951293945 norm:0.003497394733130932 max memory_allocated 22512.38671875 
[2025-02-17 16:57:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.9218858480453491 norm:0.003444950794801116 max memory_allocated 22512.38671875 
[2025-02-17 16:57:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.9223247766494751 norm:0.003378656692802906 max memory_allocated 22512.38671875 
[2025-02-17 16:57:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-17 16:58:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:1.0662257671356201 norm:0.04766271635890007 max memory_allocated 22512.55859375 
[2025-02-17 16:59:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:1.0554534196853638 norm:0.03327213227748871 max memory_allocated 22512.55859375 
[2025-02-17 16:59:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:1.0449353456497192 norm:0.021454650908708572 max memory_allocated 22512.55859375 
[2025-02-17 17:00:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:1.0416558980941772 norm:0.015522168949246407 max memory_allocated 22512.55859375 
[2025-02-17 17:00:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:1.0359703302383423 norm:0.010782130993902683 max memory_allocated 22512.55859375 
[2025-02-17 17:01:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:1.0327024459838867 norm:0.00819726288318634 max memory_allocated 22512.55859375 
[2025-02-17 17:01:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:1.0301072597503662 norm:0.006836630403995514 max memory_allocated 22512.55859375 
[2025-02-17 17:02:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:1.02685546875 norm:0.005785529967397451 max memory_allocated 22512.55859375 
[2025-02-17 17:02:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:1.0237947702407837 norm:0.004926852881908417 max memory_allocated 22512.55859375 
[2025-02-17 17:03:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:1.0214803218841553 norm:0.004502083174884319 max memory_allocated 22512.55859375 
[2025-02-17 17:03:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:1.0195960998535156 norm:0.004146683029830456 max memory_allocated 22512.55859375 
[2025-02-17 17:04:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:1.0180691480636597 norm:0.003909636754542589 max memory_allocated 22512.55859375 
[2025-02-17 17:04:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:1.0162323713302612 norm:0.003638858674094081 max memory_allocated 22512.55859375 
[2025-02-17 17:05:01 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:1.0152246952056885 norm:0.003462750231847167 max memory_allocated 22512.55859375 
[2025-02-17 17:05:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:1.014894723892212 norm:0.0033245738595724106 max memory_allocated 22512.55859375 
[2025-02-17 17:06:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:1.0135316848754883 norm:0.003171951975673437 max memory_allocated 22512.55859375 
[2025-02-17 17:06:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:1.0123850107192993 norm:0.0030759647488594055 max memory_allocated 22512.55859375 
[2025-02-17 17:07:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:1.0113856792449951 norm:0.002967396518215537 max memory_allocated 22512.55859375 
[2025-02-17 17:07:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:1.0108911991119385 norm:0.002903322223573923 max memory_allocated 22512.55859375 
[2025-02-17 17:08:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:1.0104107856750488 norm:0.0028241751715540886 max memory_allocated 22512.55859375 
[2025-02-17 17:08:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-17 17:08:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:1.1684192419052124 norm:0.035661812871694565 max memory_allocated 22512.73046875 
[2025-02-17 17:09:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:1.1579246520996094 norm:0.023584892973303795 max memory_allocated 22512.73046875 
[2025-02-17 17:09:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:1.1575696468353271 norm:0.01677028276026249 max memory_allocated 22512.73046875 
[2025-02-17 17:10:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:1.15467369556427 norm:0.010868913494050503 max memory_allocated 22512.73046875 
[2025-02-17 17:10:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:1.154026985168457 norm:0.008224459365010262 max memory_allocated 22512.73046875 
[2025-02-17 17:11:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:1.152637004852295 norm:0.006322195753455162 max memory_allocated 22512.73046875 
[2025-02-17 17:11:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:1.149552345275879 norm:0.004842548631131649 max memory_allocated 22512.73046875 
[2025-02-17 17:12:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:1.147667407989502 norm:0.0042259893380105495 max memory_allocated 22512.73046875 
[2025-02-17 17:12:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:1.1455411911010742 norm:0.0038434669841080904 max memory_allocated 22512.73046875 
[2025-02-17 17:13:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:1.1449997425079346 norm:0.003682676237076521 max memory_allocated 22512.73046875 
[2025-02-17 17:13:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:1.1436036825180054 norm:0.0034361928701400757 max memory_allocated 22512.73046875 
[2025-02-17 17:14:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:1.1431971788406372 norm:0.003266648855060339 max memory_allocated 22512.73046875 
[2025-02-17 17:14:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:1.1417367458343506 norm:0.0031378199346363544 max memory_allocated 22512.73046875 
[2025-02-17 17:15:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:1.1418299674987793 norm:0.0031170665752142668 max memory_allocated 22512.73046875 
[2025-02-17 17:15:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:1.1414035558700562 norm:0.003047061851248145 max memory_allocated 22512.73046875 
[2025-02-17 17:16:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:1.1411659717559814 norm:0.0030090261716395617 max memory_allocated 22512.73046875 
[2025-02-17 17:16:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:1.141005039215088 norm:0.0029762592166662216 max memory_allocated 22512.73046875 
[2025-02-17 17:17:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:1.1396398544311523 norm:0.0029209493659436703 max memory_allocated 22512.73046875 
[2025-02-17 17:17:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:1.1385279893875122 norm:0.0028787790797650814 max memory_allocated 22512.73046875 
[2025-02-17 17:18:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:1.137660026550293 norm:0.002846843795850873 max memory_allocated 22512.73046875 
[2025-02-17 17:18:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-17 17:18:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:1.3238860368728638 norm:0.022931044921278954 max memory_allocated 22512.90234375 
[2025-02-17 17:19:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:1.3162509202957153 norm:0.015592031180858612 max memory_allocated 22512.90234375 
[2025-02-17 17:19:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:1.3160409927368164 norm:0.012051868252456188 max memory_allocated 22512.90234375 
[2025-02-17 17:20:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:1.3136231899261475 norm:0.008329466916620731 max memory_allocated 22512.90234375 
[2025-02-17 17:20:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:1.3112119436264038 norm:0.0061944699846208096 max memory_allocated 22512.90234375 
[2025-02-17 17:21:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:1.3100627660751343 norm:0.005060321651399136 max memory_allocated 22512.90234375 
[2025-02-17 17:21:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:1.3091720342636108 norm:0.004487879574298859 max memory_allocated 22512.90234375 
[2025-02-17 17:22:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:1.3078370094299316 norm:0.004032385069876909 max memory_allocated 22512.90234375 
[2025-02-17 17:22:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:1.3061854839324951 norm:0.0036770543083548546 max memory_allocated 22512.90234375 
[2025-02-17 17:23:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:1.3044404983520508 norm:0.0033898046240210533 max memory_allocated 22512.90234375 
[2025-02-17 17:23:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:1.3039309978485107 norm:0.0032297626603394747 max memory_allocated 22512.90234375 
[2025-02-17 17:24:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:1.3027784824371338 norm:0.003122810972854495 max memory_allocated 22512.90234375 
[2025-02-17 17:24:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:1.3022291660308838 norm:0.0030283532105386257 max memory_allocated 22512.90234375 
[2025-02-17 17:25:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:1.3017562627792358 norm:0.002971647772938013 max memory_allocated 22512.90234375 
[2025-02-17 17:25:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:1.3010834455490112 norm:0.00291649391874671 max memory_allocated 22512.90234375 
[2025-02-17 17:26:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:1.3003009557724 norm:0.0028823702596127987 max memory_allocated 22512.90234375 
[2025-02-17 17:26:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:1.2997535467147827 norm:0.0028318152762949467 max memory_allocated 22512.90234375 
[2025-02-17 17:27:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:1.2991447448730469 norm:0.0028175008483231068 max memory_allocated 22512.90234375 
[2025-02-17 17:27:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:1.2985053062438965 norm:0.0028075617738068104 max memory_allocated 22512.90234375 
[2025-02-17 17:28:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:1.296194076538086 norm:0.0029441164806485176 max memory_allocated 22512.90234375 
[2025-02-17 17:28:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-17 17:29:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:1.5374101400375366 norm:0.03327981382608414 max memory_allocated 22513.07421875 
[2025-02-17 17:29:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:1.527127742767334 norm:0.02256748266518116 max memory_allocated 22513.07421875 
[2025-02-17 17:30:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:1.5231199264526367 norm:0.01695854961872101 max memory_allocated 22513.07421875 
[2025-02-17 17:30:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:1.5257608890533447 norm:0.012893647886812687 max memory_allocated 22513.07421875 
[2025-02-17 17:31:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:1.5238275527954102 norm:0.009446056559681892 max memory_allocated 22513.07421875 
[2025-02-17 17:31:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:1.521941900253296 norm:0.007394933141767979 max memory_allocated 22513.07421875 
[2025-02-17 17:32:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:1.5191559791564941 norm:0.006291156634688377 max memory_allocated 22513.07421875 
[2025-02-17 17:32:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:1.5182504653930664 norm:0.005604270379990339 max memory_allocated 22513.07421875 
[2025-02-17 17:33:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:1.5167794227600098 norm:0.005041324067860842 max memory_allocated 22513.07421875 
[2025-02-17 17:33:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:1.51545250415802 norm:0.004612413700670004 max memory_allocated 22513.07421875 
[2025-02-17 17:34:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:1.5142589807510376 norm:0.0043635074980556965 max memory_allocated 22513.07421875 
[2025-02-17 17:34:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:1.5140520334243774 norm:0.00417712889611721 max memory_allocated 22513.07421875 
[2025-02-17 17:35:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:1.5141137838363647 norm:0.004067470785230398 max memory_allocated 22513.07421875 
[2025-02-17 17:35:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:1.5140188932418823 norm:0.003992296755313873 max memory_allocated 22513.07421875 
[2025-02-17 17:36:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:1.5137587785720825 norm:0.00392573606222868 max memory_allocated 22513.07421875 
[2025-02-17 17:36:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:1.51416015625 norm:0.0038901506923139095 max memory_allocated 22513.07421875 
[2025-02-17 17:37:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:1.514270305633545 norm:0.0038479873910546303 max memory_allocated 22513.07421875 
[2025-02-17 17:37:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:1.514463186264038 norm:0.0038497920613735914 max memory_allocated 22513.07421875 
[2025-02-17 17:38:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:1.5147678852081299 norm:0.0038681314326822758 max memory_allocated 22513.07421875 
[2025-02-17 17:38:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:1.5145739316940308 norm:0.003841385245323181 max memory_allocated 22513.07421875 
[2025-02-17 17:38:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-17 17:39:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:1.7791917324066162 norm:0.021238841116428375 max memory_allocated 22513.24609375 
[2025-02-17 17:39:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:1.793874979019165 norm:0.017146583646535873 max memory_allocated 22513.24609375 
[2025-02-17 17:40:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:1.8166247606277466 norm:0.012012729421257973 max memory_allocated 22513.24609375 
[2025-02-17 17:40:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:1.8367533683776855 norm:0.009056574665009975 max memory_allocated 22513.24609375 
[2025-02-17 17:41:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:1.8365055322647095 norm:0.007504163309931755 max memory_allocated 22513.24609375 
[2025-02-17 17:41:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:1.8452132940292358 norm:0.007623544428497553 max memory_allocated 22513.24609375 
[2025-02-17 17:42:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:1.8444496393203735 norm:0.007495802827179432 max memory_allocated 22513.24609375 
[2025-02-17 17:42:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:1.844687581062317 norm:0.007500975392758846 max memory_allocated 22513.24609375 
[2025-02-17 17:43:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:1.8418266773223877 norm:0.00764826824888587 max memory_allocated 22513.24609375 
[2025-02-17 17:43:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:1.8412829637527466 norm:0.007522190921008587 max memory_allocated 22513.24609375 
[2025-02-17 17:44:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:1.8433250188827515 norm:0.007725898176431656 max memory_allocated 22513.24609375 
[2025-02-17 17:44:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:1.841361403465271 norm:0.007816208526492119 max memory_allocated 22513.24609375 
[2025-02-17 17:45:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:1.8412010669708252 norm:0.007867656648159027 max memory_allocated 22513.24609375 
[2025-02-17 17:45:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:1.8392627239227295 norm:0.007931817322969437 max memory_allocated 22513.24609375 
[2025-02-17 17:46:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:1.8380637168884277 norm:0.007852603681385517 max memory_allocated 22513.24609375 
[2025-02-17 17:46:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:1.8381065130233765 norm:0.007836400531232357 max memory_allocated 22513.24609375 
[2025-02-17 17:47:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:1.8383277654647827 norm:0.007983813062310219 max memory_allocated 22513.24609375 
[2025-02-17 17:47:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:1.8330847024917603 norm:0.0077688489109277725 max memory_allocated 22513.24609375 
[2025-02-17 17:48:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:1.8294249773025513 norm:0.007720531430095434 max memory_allocated 22513.24609375 
[2025-02-17 17:48:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:1.8275381326675415 norm:0.007805133704096079 max memory_allocated 22513.24609375 
[2025-02-17 17:49:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-17 17:49:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:2.0820252895355225 norm:0.017447756603360176 max memory_allocated 22513.41796875 
[2025-02-17 17:50:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:2.073729991912842 norm:0.011950730346143246 max memory_allocated 22513.41796875 
[2025-02-17 17:50:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:2.0755362510681152 norm:0.009174205362796783 max memory_allocated 22513.41796875 
[2025-02-17 17:51:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:2.0809364318847656 norm:0.006801076233386993 max memory_allocated 22513.41796875 
[2025-02-17 17:51:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:2.0806217193603516 norm:0.005200929474085569 max memory_allocated 22513.41796875 
[2025-02-17 17:52:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:2.080160140991211 norm:0.0043149287812411785 max memory_allocated 22513.41796875 
[2025-02-17 17:52:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:2.079134225845337 norm:0.0038968415465205908 max memory_allocated 22513.41796875 
[2025-02-17 17:53:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:2.0771262645721436 norm:0.003736971877515316 max memory_allocated 22513.41796875 
[2025-02-17 17:53:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:2.0760679244995117 norm:0.003663357114419341 max memory_allocated 22513.41796875 
[2025-02-17 17:54:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:2.0754261016845703 norm:0.003593559842556715 max memory_allocated 22513.41796875 
[2025-02-17 17:54:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:2.075089693069458 norm:0.0035950662568211555 max memory_allocated 22513.41796875 
[2025-02-17 17:55:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:2.075063467025757 norm:0.0035770083777606487 max memory_allocated 22513.41796875 
[2025-02-17 17:55:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:2.0744152069091797 norm:0.003550250083208084 max memory_allocated 22513.41796875 
[2025-02-17 17:56:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:2.0735349655151367 norm:0.003503126557916403 max memory_allocated 22513.41796875 
[2025-02-17 17:56:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:2.072277307510376 norm:0.003460702020674944 max memory_allocated 22513.41796875 
[2025-02-17 17:57:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:2.071378707885742 norm:0.003422203939408064 max memory_allocated 22513.41796875 
[2025-02-17 17:57:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:2.071260690689087 norm:0.0034392657689750195 max memory_allocated 22513.41796875 
[2025-02-17 17:58:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:2.07120680809021 norm:0.003440659260377288 max memory_allocated 22513.41796875 
[2025-02-17 17:58:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:2.0711426734924316 norm:0.0034507440868765116 max memory_allocated 22513.41796875 
[2025-02-17 17:59:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:2.0708460807800293 norm:0.0034305942244827747 max memory_allocated 22513.41796875 
[2025-02-17 17:59:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-17 17:59:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:2.432011365890503 norm:0.020945828408002853 max memory_allocated 22513.58984375 
[2025-02-17 18:00:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:2.42744779586792 norm:0.01408669538795948 max memory_allocated 22513.58984375 
[2025-02-17 18:00:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:2.428093194961548 norm:0.01062064990401268 max memory_allocated 22513.58984375 
[2025-02-17 18:01:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:2.4298901557922363 norm:0.008306426927447319 max memory_allocated 22513.58984375 
[2025-02-17 18:01:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:2.4289004802703857 norm:0.0069711413234472275 max memory_allocated 22513.58984375 
[2025-02-17 18:02:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:2.425034284591675 norm:0.005941945128142834 max memory_allocated 22513.58984375 
[2025-02-17 18:02:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:2.423522710800171 norm:0.005154706537723541 max memory_allocated 22513.58984375 
[2025-02-17 18:03:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:2.4205379486083984 norm:0.004569312557578087 max memory_allocated 22513.58984375 
[2025-02-17 18:03:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:2.4192018508911133 norm:0.004186859354376793 max memory_allocated 22513.58984375 
[2025-02-17 18:04:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:2.4168760776519775 norm:0.003895652014762163 max memory_allocated 22513.58984375 
[2025-02-17 18:04:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:2.4143872261047363 norm:0.0035843253135681152 max memory_allocated 22513.58984375 
[2025-02-17 18:05:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:2.4124703407287598 norm:0.0034223285038024187 max memory_allocated 22513.58984375 
[2025-02-17 18:05:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:2.411694049835205 norm:0.003318385686725378 max memory_allocated 22513.58984375 
[2025-02-17 18:06:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:2.411709785461426 norm:0.0032178645487874746 max memory_allocated 22513.58984375 
[2025-02-17 18:06:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:2.41117787361145 norm:0.003054550150409341 max memory_allocated 22513.58984375 
[2025-02-17 18:07:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:2.4109930992126465 norm:0.0030004940927028656 max memory_allocated 22513.58984375 
[2025-02-17 18:07:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:2.410472869873047 norm:0.0028952022548764944 max memory_allocated 22513.58984375 
[2025-02-17 18:08:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:2.41025710105896 norm:0.0029092158656567335 max memory_allocated 22513.58984375 
[2025-02-17 18:08:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:2.40986967086792 norm:0.0029699187725782394 max memory_allocated 22513.58984375 
[2025-02-17 18:09:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:2.4097039699554443 norm:0.0028640730306506157 max memory_allocated 22513.58984375 
[2025-02-17 18:09:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-17 18:10:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:2.7652602195739746 norm:0.03828439116477966 max memory_allocated 22513.76171875 
[2025-02-17 18:10:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:2.7560901641845703 norm:0.028794271871447563 max memory_allocated 22513.76171875 
[2025-02-17 18:11:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:2.7518422603607178 norm:0.02231881022453308 max memory_allocated 22513.76171875 
[2025-02-17 18:11:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:2.749070405960083 norm:0.018172547221183777 max memory_allocated 22513.76171875 
[2025-02-17 18:12:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:2.745826482772827 norm:0.015382464975118637 max memory_allocated 22513.76171875 
[2025-02-17 18:12:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:2.741628408432007 norm:0.013354533351957798 max memory_allocated 22513.76171875 
[2025-02-17 18:13:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:2.7403147220611572 norm:0.011803191155195236 max memory_allocated 22513.76171875 
[2025-02-17 18:13:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:2.73781681060791 norm:0.010522435419261456 max memory_allocated 22513.76171875 
[2025-02-17 18:14:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:2.733168601989746 norm:0.009461055509746075 max memory_allocated 22513.76171875 
[2025-02-17 18:14:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:2.73061466217041 norm:0.008567757904529572 max memory_allocated 22513.76171875 
[2025-02-17 18:15:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:2.7258176803588867 norm:0.007879935204982758 max memory_allocated 22513.76171875 
[2025-02-17 18:15:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:2.7254135608673096 norm:0.007050896529108286 max memory_allocated 22513.76171875 
[2025-02-17 18:16:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:2.7257678508758545 norm:0.006315026432275772 max memory_allocated 22513.76171875 
[2025-02-17 18:16:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:2.7254385948181152 norm:0.0055624679662287235 max memory_allocated 22513.76171875 
[2025-02-17 18:17:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:2.7246997356414795 norm:0.005092071834951639 max memory_allocated 22513.76171875 
[2025-02-17 18:17:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:2.7238399982452393 norm:0.0047260308638215065 max memory_allocated 22513.76171875 
[2025-02-17 18:18:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:2.722346305847168 norm:0.004472450353205204 max memory_allocated 22513.76171875 
[2025-02-17 18:18:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:2.7216169834136963 norm:0.004242642782628536 max memory_allocated 22513.76171875 
[2025-02-17 18:19:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:2.7213196754455566 norm:0.0039967685006558895 max memory_allocated 22513.76171875 
[2025-02-17 18:19:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:2.7205417156219482 norm:0.0037855764385312796 max memory_allocated 22513.76171875 
[2025-02-17 18:19:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-17 18:20:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:3.1165921688079834 norm:0.0189402736723423 max memory_allocated 22513.93359375 
[2025-02-17 18:20:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:3.111358642578125 norm:0.014269507490098476 max memory_allocated 22513.93359375 
[2025-02-17 18:21:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:3.1118648052215576 norm:0.010823077522218227 max memory_allocated 22513.93359375 
[2025-02-17 18:21:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:3.115164041519165 norm:0.007330200634896755 max memory_allocated 22513.93359375 
[2025-02-17 18:22:19 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:3.1143507957458496 norm:0.005299965851008892 max memory_allocated 22513.93359375 
[2025-02-17 18:22:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:3.114201307296753 norm:0.004356193356215954 max memory_allocated 22513.93359375 
[2025-02-17 18:23:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:3.112349033355713 norm:0.003793923417106271 max memory_allocated 22513.93359375 
[2025-02-17 18:23:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:3.111546039581299 norm:0.0035183257423341274 max memory_allocated 22513.93359375 
[2025-02-17 18:24:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:3.1108593940734863 norm:0.003471437143161893 max memory_allocated 22513.93359375 
[2025-02-17 18:24:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:3.1098666191101074 norm:0.0034196237102150917 max memory_allocated 22513.93359375 
[2025-02-17 18:25:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:3.1095595359802246 norm:0.0034164718817919493 max memory_allocated 22513.93359375 
[2025-02-17 18:25:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:3.1091818809509277 norm:0.0033914654050022364 max memory_allocated 22513.93359375 
[2025-02-17 18:26:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:3.1083455085754395 norm:0.003383504692465067 max memory_allocated 22513.93359375 
[2025-02-17 18:26:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:3.107830047607422 norm:0.00343145988881588 max memory_allocated 22513.93359375 
[2025-02-17 18:27:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:3.1069157123565674 norm:0.0034009311348199844 max memory_allocated 22513.93359375 
[2025-02-17 18:27:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:3.106071949005127 norm:0.0033931927755475044 max memory_allocated 22513.93359375 
[2025-02-17 18:28:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:3.105515241622925 norm:0.0034523552749305964 max memory_allocated 22513.93359375 
[2025-02-17 18:28:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:3.1051933765411377 norm:0.0033948130439966917 max memory_allocated 22513.93359375 
[2025-02-17 18:29:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:3.1049585342407227 norm:0.003384794108569622 max memory_allocated 22513.93359375 
[2025-02-17 18:29:52 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:3.105023145675659 norm:0.0033848630264401436 max memory_allocated 22513.93359375 
[2025-02-17 18:30:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-17 18:30:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:3.5552449226379395 norm:0.03224753588438034 max memory_allocated 22514.10546875 
[2025-02-17 18:31:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:3.5426201820373535 norm:0.021587498486042023 max memory_allocated 22514.10546875 
[2025-02-17 18:31:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:3.5364365577697754 norm:0.015220532193779945 max memory_allocated 22514.10546875 
[2025-02-17 18:32:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:3.5366673469543457 norm:0.010822215117514133 max memory_allocated 22514.10546875 
[2025-02-17 18:32:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:3.534061908721924 norm:0.007471587508916855 max memory_allocated 22514.10546875 
[2025-02-17 18:33:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:3.5331640243530273 norm:0.005992523860186338 max memory_allocated 22514.10546875 
[2025-02-17 18:33:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:3.5330042839050293 norm:0.004900308325886726 max memory_allocated 22514.10546875 
[2025-02-17 18:34:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:3.531839609146118 norm:0.004359588027000427 max memory_allocated 22514.10546875 
[2025-02-17 18:34:34 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:3.5308902263641357 norm:0.004019225016236305 max memory_allocated 22514.10546875 
[2025-02-17 18:35:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:3.5291588306427 norm:0.003751825774088502 max memory_allocated 22514.10546875 
[2025-02-17 18:35:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:3.526266098022461 norm:0.0035496351774781942 max memory_allocated 22514.10546875 
[2025-02-17 18:36:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:3.5221338272094727 norm:0.003327244659885764 max memory_allocated 22514.10546875 
[2025-02-17 18:36:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:3.519047737121582 norm:0.003239051438868046 max memory_allocated 22514.10546875 
[2025-02-17 18:37:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:3.5174918174743652 norm:0.0031732837669551373 max memory_allocated 22514.10546875 
[2025-02-17 18:37:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:3.518115997314453 norm:0.0031547972466796637 max memory_allocated 22514.10546875 
[2025-02-17 18:38:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:3.5174288749694824 norm:0.0031269590836018324 max memory_allocated 22514.10546875 
[2025-02-17 18:38:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:3.5174050331115723 norm:0.003133846679702401 max memory_allocated 22514.10546875 
[2025-02-17 18:39:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:3.5160751342773438 norm:0.0031199026852846146 max memory_allocated 22514.10546875 
[2025-02-17 18:39:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:3.5147905349731445 norm:0.0031007498037070036 max memory_allocated 22514.10546875 
[2025-02-17 18:40:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:3.51414155960083 norm:0.0030959402211010456 max memory_allocated 22514.10546875 
[2025-02-17 18:40:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-17 18:40:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:3.959813356399536 norm:0.03196534141898155 max memory_allocated 22514.27734375 
[2025-02-17 18:41:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:3.946183443069458 norm:0.020372925326228142 max memory_allocated 22514.27734375 
[2025-02-17 18:41:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:3.954451084136963 norm:0.01456189714372158 max memory_allocated 22514.27734375 
[2025-02-17 18:42:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:3.9658098220825195 norm:0.01022360660135746 max memory_allocated 22514.27734375 
[2025-02-17 18:42:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:3.9777815341949463 norm:0.008099805563688278 max memory_allocated 22514.27734375 
[2025-02-17 18:43:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:3.978220224380493 norm:0.0074845291674137115 max memory_allocated 22514.27734375 
[2025-02-17 18:43:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:3.9806463718414307 norm:0.007279712241142988 max memory_allocated 22514.27734375 
[2025-02-17 18:44:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:3.9805502891540527 norm:0.007187677547335625 max memory_allocated 22514.27734375 
[2025-02-17 18:44:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:3.979487895965576 norm:0.007139676716178656 max memory_allocated 22514.27734375 
[2025-02-17 18:45:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:3.976264476776123 norm:0.007033798843622208 max memory_allocated 22514.27734375 
[2025-02-17 18:45:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:3.9732024669647217 norm:0.006995873525738716 max memory_allocated 22514.27734375 
[2025-02-17 18:46:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:3.969773530960083 norm:0.006959559395909309 max memory_allocated 22514.27734375 
[2025-02-17 18:46:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:3.9664478302001953 norm:0.0068979766219854355 max memory_allocated 22514.27734375 
[2025-02-17 18:47:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:3.9665439128875732 norm:0.007000994868576527 max memory_allocated 22514.27734375 
[2025-02-17 18:47:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:3.966569423675537 norm:0.0070568290539085865 max memory_allocated 22514.27734375 
[2025-02-17 18:48:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:3.9661526679992676 norm:0.007005944382399321 max memory_allocated 22514.27734375 
[2025-02-17 18:48:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:3.9638679027557373 norm:0.0068988921120762825 max memory_allocated 22514.27734375 
[2025-02-17 18:49:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:3.963118076324463 norm:0.006924668792635202 max memory_allocated 22514.27734375 
[2025-02-17 18:49:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:3.9635660648345947 norm:0.006910358555614948 max memory_allocated 22514.27734375 
[2025-02-17 18:50:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:3.9621880054473877 norm:0.00698411138728261 max memory_allocated 22514.27734375 
[2025-02-17 18:50:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-17 18:51:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:4.57896614074707 norm:0.042267151176929474 max memory_allocated 22514.44921875 
[2025-02-17 18:51:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:4.564631938934326 norm:0.031283650547266006 max memory_allocated 22514.44921875 
[2025-02-17 18:52:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:4.56820011138916 norm:0.024448713287711143 max memory_allocated 22514.44921875 
[2025-02-17 18:52:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:4.564731597900391 norm:0.019359955564141273 max memory_allocated 22514.44921875 
[2025-02-17 18:53:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:4.567305088043213 norm:0.01597047783434391 max memory_allocated 22514.44921875 
[2025-02-17 18:53:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:4.564665794372559 norm:0.013708187267184258 max memory_allocated 22514.44921875 
[2025-02-17 18:54:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:4.566027641296387 norm:0.012070219963788986 max memory_allocated 22514.44921875 
[2025-02-17 18:54:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:4.566372871398926 norm:0.010868217796087265 max memory_allocated 22514.44921875 
[2025-02-17 18:55:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:4.562565803527832 norm:0.009455745108425617 max memory_allocated 22514.44921875 
[2025-02-17 18:55:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:4.55826997756958 norm:0.00822378322482109 max memory_allocated 22514.44921875 
[2025-02-17 18:56:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:4.556694030761719 norm:0.00761188380420208 max memory_allocated 22514.44921875 
[2025-02-17 18:56:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:4.55402946472168 norm:0.00697227381169796 max memory_allocated 22514.44921875 
[2025-02-17 18:57:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:4.5522027015686035 norm:0.006694687996059656 max memory_allocated 22514.44921875 
[2025-02-17 18:57:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:4.5513715744018555 norm:0.006451338529586792 max memory_allocated 22514.44921875 
[2025-02-17 18:58:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:4.550395488739014 norm:0.006315270438790321 max memory_allocated 22514.44921875 
[2025-02-17 18:58:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:4.545351028442383 norm:0.006083719432353973 max memory_allocated 22514.44921875 
[2025-02-17 18:59:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:4.543418884277344 norm:0.00593687454238534 max memory_allocated 22514.44921875 
[2025-02-17 18:59:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:4.5414276123046875 norm:0.005788952577859163 max memory_allocated 22514.44921875 
[2025-02-17 19:00:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:4.539668083190918 norm:0.005643878132104874 max memory_allocated 22514.44921875 
[2025-02-17 19:00:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:4.540867805480957 norm:0.0055429209023714066 max memory_allocated 22514.44921875 
[2025-02-17 19:00:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-17 19:01:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:5.377562999725342 norm:0.04550239071249962 max memory_allocated 22514.62109375 
[2025-02-17 19:01:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:5.366647243499756 norm:0.034870751202106476 max memory_allocated 22514.62109375 
[2025-02-17 19:02:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:5.359101295471191 norm:0.02735617198050022 max memory_allocated 22514.62109375 
[2025-02-17 19:02:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:5.360820293426514 norm:0.02036752924323082 max memory_allocated 22514.62109375 
[2025-02-17 19:03:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:5.3640899658203125 norm:0.01538328267633915 max memory_allocated 22514.62109375 
[2025-02-17 19:03:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:5.364973068237305 norm:0.012487647123634815 max memory_allocated 22514.62109375 
[2025-02-17 19:04:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:5.362259387969971 norm:0.010609351098537445 max memory_allocated 22514.62109375 
[2025-02-17 19:04:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:5.359273910522461 norm:0.009275675751268864 max memory_allocated 22514.62109375 
[2025-02-17 19:05:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:5.351812362670898 norm:0.008701901882886887 max memory_allocated 22514.62109375 
[2025-02-17 19:05:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:5.349487781524658 norm:0.008237932808697224 max memory_allocated 22514.62109375 
[2025-02-17 19:06:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:5.346615791320801 norm:0.00800522044301033 max memory_allocated 22514.62109375 
[2025-02-17 19:06:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:5.3428449630737305 norm:0.007831444032490253 max memory_allocated 22514.62109375 
[2025-02-17 19:07:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:5.339372634887695 norm:0.007681923918426037 max memory_allocated 22514.62109375 
[2025-02-17 19:07:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:5.33695125579834 norm:0.007603752426803112 max memory_allocated 22514.62109375 
[2025-02-17 19:08:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:5.335964202880859 norm:0.007483753841370344 max memory_allocated 22514.62109375 
[2025-02-17 19:08:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:5.332708358764648 norm:0.007413108367472887 max memory_allocated 22514.62109375 
[2025-02-17 19:09:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:5.332010269165039 norm:0.007403460796922445 max memory_allocated 22514.62109375 
[2025-02-17 19:09:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:5.3307108879089355 norm:0.007412641309201717 max memory_allocated 22514.62109375 
[2025-02-17 19:10:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:5.328536510467529 norm:0.007361753843724728 max memory_allocated 22514.62109375 
[2025-02-17 19:10:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:5.326590538024902 norm:0.00735091557726264 max memory_allocated 22514.62109375 
[2025-02-17 19:10:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-17 19:11:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:6.832322120666504 norm:0.04429582878947258 max memory_allocated 22514.79296875 
[2025-02-17 19:12:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:6.7986321449279785 norm:0.03204319626092911 max memory_allocated 22514.79296875 
[2025-02-17 19:12:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:6.8039350509643555 norm:0.026253951713442802 max memory_allocated 22514.79296875 
[2025-02-17 19:13:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:6.806554794311523 norm:0.019818823784589767 max memory_allocated 22514.79296875 
[2025-02-17 19:13:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:6.79268741607666 norm:0.015389835461974144 max memory_allocated 22514.79296875 
[2025-02-17 19:14:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:6.782308578491211 norm:0.013212055899202824 max memory_allocated 22514.79296875 
[2025-02-17 19:14:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:6.766892910003662 norm:0.01232832483947277 max memory_allocated 22514.79296875 
[2025-02-17 19:15:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:6.758939743041992 norm:0.011489907279610634 max memory_allocated 22514.79296875 
[2025-02-17 19:15:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:6.746946334838867 norm:0.011018244549632072 max memory_allocated 22514.79296875 
[2025-02-17 19:16:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:6.73841667175293 norm:0.010719050653278828 max memory_allocated 22514.79296875 
[2025-02-17 19:16:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:6.731966972351074 norm:0.010648047551512718 max memory_allocated 22514.79296875 
[2025-02-17 19:17:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:6.722593307495117 norm:0.010553117841482162 max memory_allocated 22514.79296875 
[2025-02-17 19:17:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:6.712196350097656 norm:0.010508536361157894 max memory_allocated 22514.79296875 
[2025-02-17 19:18:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:6.701018333435059 norm:0.010385192930698395 max memory_allocated 22514.79296875 
[2025-02-17 19:18:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:6.695915699005127 norm:0.010352757759392262 max memory_allocated 22514.79296875 
[2025-02-17 19:19:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:6.69278621673584 norm:0.010260412469506264 max memory_allocated 22514.79296875 
[2025-02-17 19:19:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:6.68961238861084 norm:0.010150899179279804 max memory_allocated 22514.79296875 
[2025-02-17 19:20:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:6.683498382568359 norm:0.010056604631245136 max memory_allocated 22514.79296875 
[2025-02-17 19:20:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:6.678345680236816 norm:0.010002225637435913 max memory_allocated 22514.79296875 
[2025-02-17 19:21:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:6.6739501953125 norm:0.010159485973417759 max memory_allocated 22514.79296875 
[2025-02-17 19:21:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-17 19:21:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:15.137908935546875 norm:0.23876333236694336 max memory_allocated 22514.96484375 
[2025-02-17 19:22:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:15.05794906616211 norm:0.18989485502243042 max memory_allocated 22514.96484375 
[2025-02-17 19:22:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:15.059415817260742 norm:0.20191209018230438 max memory_allocated 22514.96484375 
[2025-02-17 19:23:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:14.699417114257812 norm:2.7430379390716553 max memory_allocated 22514.96484375 
[2025-02-17 19:23:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:14.324495315551758 norm:4.24290132522583 max memory_allocated 22514.96484375 
[2025-02-17 19:24:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:14.073541641235352 norm:6.22443151473999 max memory_allocated 22514.96484375 
[2025-02-17 19:24:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:13.960596084594727 norm:7.95882511138916 max memory_allocated 22514.96484375 
[2025-02-17 19:25:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:13.961503982543945 norm:9.51832103729248 max memory_allocated 22514.96484375 
[2025-02-17 19:25:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:14.017460823059082 norm:10.790088653564453 max memory_allocated 22514.96484375 
[2025-02-17 19:26:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:14.068341255187988 norm:12.92302131652832 max memory_allocated 22514.96484375 
[2025-02-17 19:26:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:14.069701194763184 norm:14.612051010131836 max memory_allocated 22514.96484375 
[2025-02-17 19:27:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:14.148674011230469 norm:15.294378280639648 max memory_allocated 22514.96484375 
[2025-02-17 19:27:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:14.302160263061523 norm:17.762144088745117 max memory_allocated 22514.96484375 
[2025-02-17 19:28:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:14.559887886047363 norm:22.58161163330078 max memory_allocated 22514.96484375 
[2025-02-17 19:28:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:14.88015079498291 norm:31.215299606323242 max memory_allocated 22514.96484375 
[2025-02-17 19:29:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:15.052265167236328 norm:34.582252502441406 max memory_allocated 22514.96484375 
[2025-02-17 19:29:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:15.119970321655273 norm:41.60205078125 max memory_allocated 22514.96484375 
[2025-02-17 19:30:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:15.276261329650879 norm:43.610450744628906 max memory_allocated 22514.96484375 
[2025-02-17 19:30:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:15.371068954467773 norm:28.717926025390625 max memory_allocated 22514.96484375 
[2025-02-17 19:31:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:15.152510643005371 norm:25.754314422607422 max memory_allocated 22514.96484375 
[2025-02-17 19:31:26 root] (main_calibration.py 365): INFO 19659.871267080307
