[2025-02-17 13:59:55 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration/Llama-2-13b-hf-w4a4', save_dir='./log-calibration/quant/Llama-2-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-17 13:59:56 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-17 13:59:56 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-17 13:59:56 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-17 13:59:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-17 14:00:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.07881267368793488 norm:0.020949382334947586 max memory_allocated 29229.177734375 
[2025-02-17 14:01:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.060357533395290375 norm:0.012186387553811073 max memory_allocated 29229.177734375 
[2025-02-17 14:02:15 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.05895356088876724 norm:0.05691366270184517 max memory_allocated 29229.177734375 
[2025-02-17 14:02:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.06471293419599533 norm:0.09777435660362244 max memory_allocated 29229.177734375 
[2025-02-17 14:03:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.06193966045975685 norm:0.0709129348397255 max memory_allocated 29229.177734375 
[2025-02-17 14:04:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.050896838307380676 norm:0.018670251592993736 max memory_allocated 29229.177734375 
[2025-02-17 14:05:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.05178691819310188 norm:0.034685567021369934 max memory_allocated 29229.177734375 
[2025-02-17 14:05:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.05347684770822525 norm:0.0978676825761795 max memory_allocated 29229.177734375 
[2025-02-17 14:06:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0450109988451004 norm:0.024406857788562775 max memory_allocated 29229.177734375 
[2025-02-17 14:07:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.05082596093416214 norm:0.0559951588511467 max memory_allocated 29229.177734375 
[2025-02-17 14:08:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.053707633167505264 norm:0.04657493531703949 max memory_allocated 29229.177734375 
[2025-02-17 14:08:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.05393606051802635 norm:0.05089198052883148 max memory_allocated 29229.177734375 
[2025-02-17 14:09:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.05335717275738716 norm:0.07567954808473587 max memory_allocated 29229.177734375 
[2025-02-17 14:10:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.060599420219659805 norm:0.07241524010896683 max memory_allocated 29229.177734375 
[2025-02-17 14:11:09 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.05265786498785019 norm:0.032686375081539154 max memory_allocated 29229.177734375 
[2025-02-17 14:11:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.05127270519733429 norm:0.05137810856103897 max memory_allocated 29229.177734375 
[2025-02-17 14:12:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.046485841274261475 norm:0.029704168438911438 max memory_allocated 29229.177734375 
[2025-02-17 14:13:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.051109448075294495 norm:0.04711943119764328 max memory_allocated 29229.177734375 
[2025-02-17 14:14:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.04613644257187843 norm:0.012366286478936672 max memory_allocated 29229.177734375 
[2025-02-17 14:14:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.04837900772690773 norm:0.019324125722050667 max memory_allocated 29229.177734375 
[2025-02-17 14:15:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-17 14:15:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.19834865629673004 norm:0.019456772133708 max memory_allocated 29229.365234375 
[2025-02-17 14:16:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.169278085231781 norm:0.008025731891393661 max memory_allocated 29229.365234375 
[2025-02-17 14:17:20 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.16202637553215027 norm:0.007484317757189274 max memory_allocated 29229.365234375 
[2025-02-17 14:18:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.15875163674354553 norm:0.00446453969925642 max memory_allocated 29229.365234375 
[2025-02-17 14:18:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.15691514313220978 norm:0.003839752869680524 max memory_allocated 29229.365234375 
[2025-02-17 14:19:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.15772387385368347 norm:0.003666921751573682 max memory_allocated 29229.365234375 
[2025-02-17 14:20:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.15831322968006134 norm:0.0035956548526883125 max memory_allocated 29229.365234375 
[2025-02-17 14:21:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.15767036378383636 norm:0.0035045770928263664 max memory_allocated 29229.365234375 
[2025-02-17 14:21:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.1568651795387268 norm:0.003439656225964427 max memory_allocated 29229.365234375 
[2025-02-17 14:22:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.15551356971263885 norm:0.003502452280372381 max memory_allocated 29229.365234375 
[2025-02-17 14:23:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.15494488179683685 norm:0.0033132496755570173 max memory_allocated 29229.365234375 
[2025-02-17 14:24:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.15444627404212952 norm:0.0033646656665951014 max memory_allocated 29229.365234375 
[2025-02-17 14:24:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.15263451635837555 norm:0.003323117271065712 max memory_allocated 29229.365234375 
[2025-02-17 14:25:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.1522947996854782 norm:0.0032505509443581104 max memory_allocated 29229.365234375 
[2025-02-17 14:26:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.15273089706897736 norm:0.0032196498941630125 max memory_allocated 29229.365234375 
[2025-02-17 14:26:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.15300190448760986 norm:0.0032732049003243446 max memory_allocated 29229.365234375 
[2025-02-17 14:27:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.15286852419376373 norm:0.003224650165066123 max memory_allocated 29229.365234375 
[2025-02-17 14:28:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.15245327353477478 norm:0.0031791646033525467 max memory_allocated 29229.365234375 
[2025-02-17 14:29:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.1520693004131317 norm:0.0031531350687146187 max memory_allocated 29229.365234375 
[2025-02-17 14:29:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.1521352082490921 norm:0.0031614243052899837 max memory_allocated 29229.365234375 
[2025-02-17 14:30:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-17 14:30:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.2064957320690155 norm:0.012527347542345524 max memory_allocated 29229.552734375 
[2025-02-17 14:31:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.18871331214904785 norm:0.006166848819702864 max memory_allocated 29229.552734375 
[2025-02-17 14:32:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.18221083283424377 norm:0.0033553570974618196 max memory_allocated 29229.552734375 
[2025-02-17 14:33:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.1791515052318573 norm:0.0024932590313255787 max memory_allocated 29229.552734375 
[2025-02-17 14:33:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.17786026000976562 norm:0.002108794404193759 max memory_allocated 29229.552734375 
[2025-02-17 14:34:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.176022008061409 norm:0.0018738998332992196 max memory_allocated 29229.552734375 
[2025-02-17 14:35:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.17568309605121613 norm:0.0017778108594939113 max memory_allocated 29229.552734375 
[2025-02-17 14:36:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.17527806758880615 norm:0.0017053581541404128 max memory_allocated 29229.552734375 
[2025-02-17 14:36:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.17524689435958862 norm:0.0016853914130479097 max memory_allocated 29229.552734375 
[2025-02-17 14:37:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.1751815676689148 norm:0.001665838179178536 max memory_allocated 29229.552734375 
[2025-02-17 14:38:20 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.17520923912525177 norm:0.0016664552967995405 max memory_allocated 29229.552734375 
[2025-02-17 14:39:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.1752522736787796 norm:0.0016569060971960425 max memory_allocated 29229.552734375 
[2025-02-17 14:39:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.1754414588212967 norm:0.0016578417271375656 max memory_allocated 29229.552734375 
[2025-02-17 14:40:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.17547523975372314 norm:0.0016692624194547534 max memory_allocated 29229.552734375 
[2025-02-17 14:41:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.17554771900177002 norm:0.0016760205617174506 max memory_allocated 29229.552734375 
[2025-02-17 14:42:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.17532411217689514 norm:0.001650749472901225 max memory_allocated 29229.552734375 
[2025-02-17 14:42:47 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.17533782124519348 norm:0.0016549755819141865 max memory_allocated 29229.552734375 
[2025-02-17 14:43:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.1753520667552948 norm:0.001655331696383655 max memory_allocated 29229.552734375 
[2025-02-17 14:44:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.17536386847496033 norm:0.0016717904945835471 max memory_allocated 29229.552734375 
[2025-02-17 14:45:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.17531481385231018 norm:0.0016575143672525883 max memory_allocated 29229.552734375 
[2025-02-17 14:45:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-17 14:46:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.41829270124435425 norm:0.12632893025875092 max memory_allocated 29229.740234375 
[2025-02-17 14:46:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.3273768723011017 norm:0.04317237436771393 max memory_allocated 29229.740234375 
[2025-02-17 14:47:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.30518168210983276 norm:0.027069661766290665 max memory_allocated 29229.740234375 
[2025-02-17 14:48:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.28562068939208984 norm:0.01926463283598423 max memory_allocated 29229.740234375 
[2025-02-17 14:49:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.28127720952033997 norm:0.01579236425459385 max memory_allocated 29229.740234375 
[2025-02-17 14:49:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.27776795625686646 norm:0.015310865826904774 max memory_allocated 29229.740234375 
[2025-02-17 14:50:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.2748714089393616 norm:0.014039806090295315 max memory_allocated 29229.740234375 
[2025-02-17 14:51:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.27418434619903564 norm:0.013639525510370731 max memory_allocated 29229.740234375 
[2025-02-17 14:51:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.2766576111316681 norm:0.0123148113489151 max memory_allocated 29229.740234375 
[2025-02-17 14:52:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.276213139295578 norm:0.01230858638882637 max memory_allocated 29229.740234375 
[2025-02-17 14:53:27 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.2733774185180664 norm:0.011451967060565948 max memory_allocated 29229.740234375 
[2025-02-17 14:54:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.27272510528564453 norm:0.011272585019469261 max memory_allocated 29229.740234375 
[2025-02-17 14:54:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.2739540636539459 norm:0.011896634474396706 max memory_allocated 29229.740234375 
[2025-02-17 14:55:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.27376848459243774 norm:0.011961380951106548 max memory_allocated 29229.740234375 
[2025-02-17 14:56:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.27445104718208313 norm:0.012806345708668232 max memory_allocated 29229.740234375 
[2025-02-17 14:57:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.27294379472732544 norm:0.011416182853281498 max memory_allocated 29229.740234375 
[2025-02-17 14:57:54 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.27376651763916016 norm:0.011398048140108585 max memory_allocated 29229.740234375 
[2025-02-17 14:58:38 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.2760254740715027 norm:0.011654546484351158 max memory_allocated 29229.740234375 
[2025-02-17 14:59:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.27458131313323975 norm:0.011458160355687141 max memory_allocated 29229.740234375 
[2025-02-17 15:00:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.276616632938385 norm:0.011130411177873611 max memory_allocated 29229.740234375 
[2025-02-17 15:00:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-17 15:01:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.3356429636478424 norm:0.017591331154108047 max memory_allocated 29229.927734375 
[2025-02-17 15:01:51 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.3188926577568054 norm:0.008865839801728725 max memory_allocated 29229.927734375 
[2025-02-17 15:02:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.31140419840812683 norm:0.005747216288000345 max memory_allocated 29229.927734375 
[2025-02-17 15:03:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.306788831949234 norm:0.004097673110663891 max memory_allocated 29229.927734375 
[2025-02-17 15:04:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.30481699109077454 norm:0.003331555053591728 max memory_allocated 29229.927734375 
[2025-02-17 15:04:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.30335384607315063 norm:0.0027525369077920914 max memory_allocated 29229.927734375 
[2025-02-17 15:05:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.3024631440639496 norm:0.002410639077425003 max memory_allocated 29229.927734375 
[2025-02-17 15:06:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.30161166191101074 norm:0.002218075795099139 max memory_allocated 29229.927734375 
[2025-02-17 15:07:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.3013024926185608 norm:0.002067137509584427 max memory_allocated 29229.927734375 
[2025-02-17 15:07:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.30095750093460083 norm:0.0019137341296300292 max memory_allocated 29229.927734375 
[2025-02-17 15:08:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.3009420931339264 norm:0.001797338598407805 max memory_allocated 29229.927734375 
[2025-02-17 15:09:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.30061179399490356 norm:0.0017656635027378798 max memory_allocated 29229.927734375 
[2025-02-17 15:10:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.30040642619132996 norm:0.0017214007675647736 max memory_allocated 29229.927734375 
[2025-02-17 15:10:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.30039164423942566 norm:0.001761136227287352 max memory_allocated 29229.927734375 
[2025-02-17 15:11:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.3002726137638092 norm:0.0017078128876164556 max memory_allocated 29229.927734375 
[2025-02-17 15:12:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.3002683222293854 norm:0.0016876942245289683 max memory_allocated 29229.927734375 
[2025-02-17 15:12:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.30051785707473755 norm:0.0017124980222433805 max memory_allocated 29229.927734375 
[2025-02-17 15:13:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.30037829279899597 norm:0.0017016036435961723 max memory_allocated 29229.927734375 
[2025-02-17 15:14:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.3002021908760071 norm:0.0017245184862986207 max memory_allocated 29229.927734375 
[2025-02-17 15:15:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.2999555468559265 norm:0.0017299320315942168 max memory_allocated 29229.927734375 
[2025-02-17 15:15:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-17 15:16:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.38239866495132446 norm:0.017768755555152893 max memory_allocated 29230.115234375 
[2025-02-17 15:16:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.3683928847312927 norm:0.009971152991056442 max memory_allocated 29230.115234375 
[2025-02-17 15:17:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.3619908392429352 norm:0.007361996453255415 max memory_allocated 29230.115234375 
[2025-02-17 15:18:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.35816720128059387 norm:0.00568563723936677 max memory_allocated 29230.115234375 
[2025-02-17 15:19:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.35658687353134155 norm:0.004927098751068115 max memory_allocated 29230.115234375 
[2025-02-17 15:19:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.3548634946346283 norm:0.004519946873188019 max memory_allocated 29230.115234375 
[2025-02-17 15:20:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.35305193066596985 norm:0.0037948251701891422 max memory_allocated 29230.115234375 
[2025-02-17 15:21:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.352095365524292 norm:0.0035529034212231636 max memory_allocated 29230.115234375 
[2025-02-17 15:22:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.35163620114326477 norm:0.003344241762533784 max memory_allocated 29230.115234375 
[2025-02-17 15:22:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.35104185342788696 norm:0.002936396049335599 max memory_allocated 29230.115234375 
[2025-02-17 15:23:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.3503088653087616 norm:0.003099564928561449 max memory_allocated 29230.115234375 
[2025-02-17 15:24:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.3501945734024048 norm:0.0028252904303371906 max memory_allocated 29230.115234375 
[2025-02-17 15:25:05 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.35007718205451965 norm:0.0029904155526310205 max memory_allocated 29230.115234375 
[2025-02-17 15:25:50 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.35006779432296753 norm:0.002945592626929283 max memory_allocated 29230.115234375 
[2025-02-17 15:26:34 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.34978416562080383 norm:0.0027077803388237953 max memory_allocated 29230.115234375 
[2025-02-17 15:27:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.34954333305358887 norm:0.0026780893094837666 max memory_allocated 29230.115234375 
[2025-02-17 15:28:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.34956255555152893 norm:0.0025963543448597193 max memory_allocated 29230.115234375 
[2025-02-17 15:28:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.34938254952430725 norm:0.0025879049208015203 max memory_allocated 29230.115234375 
[2025-02-17 15:29:32 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.3488692343235016 norm:0.0022787251509726048 max memory_allocated 29230.115234375 
[2025-02-17 15:30:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.34897634387016296 norm:0.0024201306514441967 max memory_allocated 29230.115234375 
[2025-02-17 15:30:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-17 15:31:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.44596603512763977 norm:0.027982566505670547 max memory_allocated 29230.302734375 
[2025-02-17 15:32:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.42654359340667725 norm:0.016442378982901573 max memory_allocated 29230.302734375 
[2025-02-17 15:32:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.41777440905570984 norm:0.01109167467802763 max memory_allocated 29230.302734375 
[2025-02-17 15:33:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.4126601219177246 norm:0.008140964433550835 max memory_allocated 29230.302734375 
[2025-02-17 15:34:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.4088296592235565 norm:0.006023240275681019 max memory_allocated 29230.302734375 
[2025-02-17 15:34:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.40617576241493225 norm:0.004836032632738352 max memory_allocated 29230.302734375 
[2025-02-17 15:35:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.4048703908920288 norm:0.0038822689093649387 max memory_allocated 29230.302734375 
[2025-02-17 15:36:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.4042360484600067 norm:0.003475223435088992 max memory_allocated 29230.302734375 
[2025-02-17 15:37:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.40359431505203247 norm:0.003111418802291155 max memory_allocated 29230.302734375 
[2025-02-17 15:37:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.40274739265441895 norm:0.00283021479845047 max memory_allocated 29230.302734375 
[2025-02-17 15:38:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.4025929868221283 norm:0.0027724106330424547 max memory_allocated 29230.302734375 
[2025-02-17 15:39:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.40258651971817017 norm:0.0025482294149696827 max memory_allocated 29230.302734375 
[2025-02-17 15:40:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.4021872282028198 norm:0.0024528936482965946 max memory_allocated 29230.302734375 
[2025-02-17 15:40:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.4021553695201874 norm:0.002356798853725195 max memory_allocated 29230.302734375 
[2025-02-17 15:41:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.40164580941200256 norm:0.0022912751883268356 max memory_allocated 29230.302734375 
[2025-02-17 15:42:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.4012426733970642 norm:0.002185238292440772 max memory_allocated 29230.302734375 
[2025-02-17 15:43:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.40205034613609314 norm:0.002327479887753725 max memory_allocated 29230.302734375 
[2025-02-17 15:43:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.4008484482765198 norm:0.002127734012901783 max memory_allocated 29230.302734375 
[2025-02-17 15:44:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.40140676498413086 norm:0.002265081275254488 max memory_allocated 29230.302734375 
[2025-02-17 15:45:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.400983989238739 norm:0.0021601375192403793 max memory_allocated 29230.302734375 
[2025-02-17 15:45:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-17 15:46:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.5165658593177795 norm:0.012073573656380177 max memory_allocated 29230.490234375 
[2025-02-17 15:47:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.48709025979042053 norm:0.005918518640100956 max memory_allocated 29230.490234375 
[2025-02-17 15:47:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.47704237699508667 norm:0.0041269464418292046 max memory_allocated 29230.490234375 
[2025-02-17 15:48:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.4728075861930847 norm:0.003294560359790921 max memory_allocated 29230.490234375 
[2025-02-17 15:49:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.4706912934780121 norm:0.00285890931263566 max memory_allocated 29230.490234375 
[2025-02-17 15:50:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.46968260407447815 norm:0.0025896162260323763 max memory_allocated 29230.490234375 
[2025-02-17 15:50:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.46773916482925415 norm:0.002462590578943491 max memory_allocated 29230.490234375 
[2025-02-17 15:51:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.46663782000541687 norm:0.002232012338936329 max memory_allocated 29230.490234375 
[2025-02-17 15:52:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.4664309322834015 norm:0.0022092419676482677 max memory_allocated 29230.490234375 
[2025-02-17 15:53:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.4658201336860657 norm:0.002135897520929575 max memory_allocated 29230.490234375 
[2025-02-17 15:53:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.4655854403972626 norm:0.0022468960378319025 max memory_allocated 29230.490234375 
[2025-02-17 15:54:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.46485546231269836 norm:0.0021680046338588 max memory_allocated 29230.490234375 
[2025-02-17 15:55:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.46474242210388184 norm:0.002168992767110467 max memory_allocated 29230.490234375 
[2025-02-17 15:55:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.4642590284347534 norm:0.002169670071452856 max memory_allocated 29230.490234375 
[2025-02-17 15:56:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.46410247683525085 norm:0.0021417737007141113 max memory_allocated 29230.490234375 
[2025-02-17 15:57:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.46405887603759766 norm:0.0021313095930963755 max memory_allocated 29230.490234375 
[2025-02-17 15:58:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.46398791670799255 norm:0.0021484268363565207 max memory_allocated 29230.490234375 
[2025-02-17 15:58:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.46399277448654175 norm:0.0021486193872988224 max memory_allocated 29230.490234375 
[2025-02-17 15:59:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.46387314796447754 norm:0.0021729040890932083 max memory_allocated 29230.490234375 
[2025-02-17 16:00:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.46407240629196167 norm:0.002230849815532565 max memory_allocated 29230.490234375 
[2025-02-17 16:00:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-17 16:01:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.5535645484924316 norm:0.03486894816160202 max memory_allocated 29230.677734375 
[2025-02-17 16:02:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.5319253206253052 norm:0.021456709131598473 max memory_allocated 29230.677734375 
[2025-02-17 16:02:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.5247231721878052 norm:0.01503127720206976 max memory_allocated 29230.677734375 
[2025-02-17 16:03:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.5174771547317505 norm:0.011242784559726715 max memory_allocated 29230.677734375 
[2025-02-17 16:04:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.5127744674682617 norm:0.009059746749699116 max memory_allocated 29230.677734375 
[2025-02-17 16:05:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.5090640783309937 norm:0.007161751855164766 max memory_allocated 29230.677734375 
[2025-02-17 16:05:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.5077351927757263 norm:0.005744569469243288 max memory_allocated 29230.677734375 
[2025-02-17 16:06:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.505800724029541 norm:0.004915524274110794 max memory_allocated 29230.677734375 
[2025-02-17 16:07:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.502739667892456 norm:0.0040872166864573956 max memory_allocated 29230.677734375 
[2025-02-17 16:08:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.5009819269180298 norm:0.003443493042141199 max memory_allocated 29230.677734375 
[2025-02-17 16:08:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.5003226399421692 norm:0.0031094320584088564 max memory_allocated 29230.677734375 
[2025-02-17 16:09:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.500822901725769 norm:0.0028643098194152117 max memory_allocated 29230.677734375 
[2025-02-17 16:10:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.5001848340034485 norm:0.0027244938537478447 max memory_allocated 29230.677734375 
[2025-02-17 16:11:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.4985067844390869 norm:0.002403798047453165 max memory_allocated 29230.677734375 
[2025-02-17 16:11:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.4986599385738373 norm:0.0024597779847681522 max memory_allocated 29230.677734375 
[2025-02-17 16:12:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.49863728880882263 norm:0.002244917443022132 max memory_allocated 29230.677734375 
[2025-02-17 16:13:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.4974558651447296 norm:0.0022006880026310682 max memory_allocated 29230.677734375 
[2025-02-17 16:14:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.49711889028549194 norm:0.002148748841136694 max memory_allocated 29230.677734375 
[2025-02-17 16:14:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.4973720908164978 norm:0.0021781595423817635 max memory_allocated 29230.677734375 
[2025-02-17 16:15:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.49717211723327637 norm:0.0021336167119443417 max memory_allocated 29230.677734375 
[2025-02-17 16:15:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-17 16:16:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.6447458267211914 norm:0.027237365022301674 max memory_allocated 29230.865234375 
[2025-02-17 16:17:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.6060880422592163 norm:0.013538993895053864 max memory_allocated 29230.865234375 
[2025-02-17 16:18:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.5912801027297974 norm:0.00915801152586937 max memory_allocated 29230.865234375 
[2025-02-17 16:18:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.581782341003418 norm:0.0066645266488194466 max memory_allocated 29230.865234375 
[2025-02-17 16:19:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.5773823261260986 norm:0.005564503371715546 max memory_allocated 29230.865234375 
[2025-02-17 16:20:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.5739353895187378 norm:0.004790036007761955 max memory_allocated 29230.865234375 
[2025-02-17 16:20:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.5707871913909912 norm:0.0037843724712729454 max memory_allocated 29230.865234375 
[2025-02-17 16:21:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.5695459246635437 norm:0.0033799863886088133 max memory_allocated 29230.865234375 
[2025-02-17 16:22:27 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.5697377324104309 norm:0.0033556383568793535 max memory_allocated 29230.865234375 
[2025-02-17 16:23:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.5687507390975952 norm:0.003189723938703537 max memory_allocated 29230.865234375 
[2025-02-17 16:23:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.5674394369125366 norm:0.003035305766388774 max memory_allocated 29230.865234375 
[2025-02-17 16:24:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.5665534138679504 norm:0.003043131437152624 max memory_allocated 29230.865234375 
[2025-02-17 16:25:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.5674713850021362 norm:0.002963003935292363 max memory_allocated 29230.865234375 
[2025-02-17 16:26:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.5667033791542053 norm:0.002916589379310608 max memory_allocated 29230.865234375 
[2025-02-17 16:26:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.5669748783111572 norm:0.002903458895161748 max memory_allocated 29230.865234375 
[2025-02-17 16:27:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.5677120089530945 norm:0.003093155799433589 max memory_allocated 29230.865234375 
[2025-02-17 16:28:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.5662438869476318 norm:0.0030114443507045507 max memory_allocated 29230.865234375 
[2025-02-17 16:29:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.5666028261184692 norm:0.002837671199813485 max memory_allocated 29230.865234375 
[2025-02-17 16:29:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.5657500624656677 norm:0.002821641508489847 max memory_allocated 29230.865234375 
[2025-02-17 16:30:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.5663198232650757 norm:0.002822707872837782 max memory_allocated 29230.865234375 
[2025-02-17 16:30:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-17 16:31:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.6561943888664246 norm:0.022069212049245834 max memory_allocated 29231.052734375 
[2025-02-17 16:32:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.6340789794921875 norm:0.012217812240123749 max memory_allocated 29231.052734375 
[2025-02-17 16:33:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.6213963031768799 norm:0.008299395442008972 max memory_allocated 29231.052734375 
[2025-02-17 16:33:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.6161487102508545 norm:0.006386227905750275 max memory_allocated 29231.052734375 
[2025-02-17 16:34:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.6132478713989258 norm:0.005021441727876663 max memory_allocated 29231.052734375 
[2025-02-17 16:35:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.6123974323272705 norm:0.004360302351415157 max memory_allocated 29231.052734375 
[2025-02-17 16:36:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.611535906791687 norm:0.0038312533870339394 max memory_allocated 29231.052734375 
[2025-02-17 16:36:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.6106732487678528 norm:0.0034193398896604776 max memory_allocated 29231.052734375 
[2025-02-17 16:37:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.610273540019989 norm:0.0031641290988773108 max memory_allocated 29231.052734375 
[2025-02-17 16:38:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.6080455780029297 norm:0.0028378923889249563 max memory_allocated 29231.052734375 
[2025-02-17 16:39:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.6061756014823914 norm:0.0025463635101914406 max memory_allocated 29231.052734375 
[2025-02-17 16:39:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.6048871278762817 norm:0.002461303723976016 max memory_allocated 29231.052734375 
[2025-02-17 16:40:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.6038151383399963 norm:0.002401607111096382 max memory_allocated 29231.052734375 
[2025-02-17 16:41:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.6039966344833374 norm:0.002358511323109269 max memory_allocated 29231.052734375 
[2025-02-17 16:41:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.6038280129432678 norm:0.0023465678095817566 max memory_allocated 29231.052734375 
[2025-02-17 16:42:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.6041422486305237 norm:0.002348222304135561 max memory_allocated 29231.052734375 
[2025-02-17 16:43:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.6035977005958557 norm:0.0023482399992644787 max memory_allocated 29231.052734375 
[2025-02-17 16:44:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.6027222275733948 norm:0.0023760716430842876 max memory_allocated 29231.052734375 
[2025-02-17 16:44:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.6026108264923096 norm:0.00227973866276443 max memory_allocated 29231.052734375 
[2025-02-17 16:45:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.6035640239715576 norm:0.0023469473235309124 max memory_allocated 29231.052734375 
[2025-02-17 16:45:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-17 16:46:41 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.6809465885162354 norm:0.021147046238183975 max memory_allocated 29231.240234375 
[2025-02-17 16:47:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.6604965925216675 norm:0.010575886815786362 max memory_allocated 29231.240234375 
[2025-02-17 16:48:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.6525246500968933 norm:0.007564250845462084 max memory_allocated 29231.240234375 
[2025-02-17 16:48:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.6492068767547607 norm:0.005751245189458132 max memory_allocated 29231.240234375 
[2025-02-17 16:49:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.6452447772026062 norm:0.004456565249711275 max memory_allocated 29231.240234375 
[2025-02-17 16:50:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.6453655362129211 norm:0.0040740519762039185 max memory_allocated 29231.240234375 
[2025-02-17 16:51:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.6445726156234741 norm:0.0034928834065794945 max memory_allocated 29231.240234375 
[2025-02-17 16:51:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.6447160840034485 norm:0.0031558407936245203 max memory_allocated 29231.240234375 
[2025-02-17 16:52:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.6441329717636108 norm:0.002819815883412957 max memory_allocated 29231.240234375 
[2025-02-17 16:53:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.6443359851837158 norm:0.0026220879517495632 max memory_allocated 29231.240234375 
[2025-02-17 16:54:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.6436760425567627 norm:0.002524693263694644 max memory_allocated 29231.240234375 
[2025-02-17 16:54:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.6425403356552124 norm:0.002445794176310301 max memory_allocated 29231.240234375 
[2025-02-17 16:55:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.6422309279441833 norm:0.0023495072964578867 max memory_allocated 29231.240234375 
[2025-02-17 16:56:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.6422753930091858 norm:0.0023022829554975033 max memory_allocated 29231.240234375 
[2025-02-17 16:57:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.6413149833679199 norm:0.002204533666372299 max memory_allocated 29231.240234375 
[2025-02-17 16:57:48 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.6407498717308044 norm:0.0021977024152874947 max memory_allocated 29231.240234375 
[2025-02-17 16:58:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.6405019164085388 norm:0.0021245130337774754 max memory_allocated 29231.240234375 
[2025-02-17 16:59:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.6406305432319641 norm:0.0022009634412825108 max memory_allocated 29231.240234375 
[2025-02-17 17:00:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.6398231387138367 norm:0.002078323857858777 max memory_allocated 29231.240234375 
[2025-02-17 17:00:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.639549195766449 norm:0.002061753999441862 max memory_allocated 29231.240234375 
[2025-02-17 17:00:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-17 17:01:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.7150260210037231 norm:0.017093129456043243 max memory_allocated 29231.427734375 
[2025-02-17 17:02:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.6888361573219299 norm:0.008599241264164448 max memory_allocated 29231.427734375 
[2025-02-17 17:03:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.6797727942466736 norm:0.005915991496294737 max memory_allocated 29231.427734375 
[2025-02-17 17:03:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.6763314604759216 norm:0.004717440810054541 max memory_allocated 29231.427734375 
[2025-02-17 17:04:44 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.6722210645675659 norm:0.0037727192975580692 max memory_allocated 29231.427734375 
[2025-02-17 17:05:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.6671922206878662 norm:0.003020373173058033 max memory_allocated 29231.427734375 
[2025-02-17 17:06:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.6663334965705872 norm:0.00268903118558228 max memory_allocated 29231.427734375 
[2025-02-17 17:06:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.6653920412063599 norm:0.002418499207124114 max memory_allocated 29231.427734375 
[2025-02-17 17:07:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.6642690896987915 norm:0.0022323557641357183 max memory_allocated 29231.427734375 
[2025-02-17 17:08:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.664626955986023 norm:0.002215519780293107 max memory_allocated 29231.427734375 
[2025-02-17 17:09:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.6637293100357056 norm:0.0021430165506899357 max memory_allocated 29231.427734375 
[2025-02-17 17:09:55 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.6622251868247986 norm:0.0021122898906469345 max memory_allocated 29231.427734375 
[2025-02-17 17:10:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.6620236039161682 norm:0.0020690876990556717 max memory_allocated 29231.427734375 
[2025-02-17 17:11:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.6611931920051575 norm:0.0019314781529828906 max memory_allocated 29231.427734375 
[2025-02-17 17:12:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.6610530018806458 norm:0.001879582880064845 max memory_allocated 29231.427734375 
[2025-02-17 17:12:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.6609412431716919 norm:0.0018486116314306855 max memory_allocated 29231.427734375 
[2025-02-17 17:13:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.6609218716621399 norm:0.001883080112747848 max memory_allocated 29231.427734375 
[2025-02-17 17:14:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.6605602502822876 norm:0.001806312007829547 max memory_allocated 29231.427734375 
[2025-02-17 17:15:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.6602306962013245 norm:0.0017648902721703053 max memory_allocated 29231.427734375 
[2025-02-17 17:15:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.6595186591148376 norm:0.0016886809607967734 max memory_allocated 29231.427734375 
[2025-02-17 17:16:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-17 17:16:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.7486751675605774 norm:0.02121645025908947 max memory_allocated 29231.615234375 
[2025-02-17 17:17:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.7240043878555298 norm:0.011629894375801086 max memory_allocated 29231.615234375 
[2025-02-17 17:18:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.7153152227401733 norm:0.007865546271204948 max memory_allocated 29231.615234375 
[2025-02-17 17:19:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.710676908493042 norm:0.006266254931688309 max memory_allocated 29231.615234375 
[2025-02-17 17:19:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.7102592587471008 norm:0.005444716662168503 max memory_allocated 29231.615234375 
[2025-02-17 17:20:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.7078254222869873 norm:0.004667164757847786 max memory_allocated 29231.615234375 
[2025-02-17 17:21:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.7053033709526062 norm:0.004055443685501814 max memory_allocated 29231.615234375 
[2025-02-17 17:22:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.7044827938079834 norm:0.003726236056536436 max memory_allocated 29231.615234375 
[2025-02-17 17:22:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.704571545124054 norm:0.003526462707668543 max memory_allocated 29231.615234375 
[2025-02-17 17:23:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.703472912311554 norm:0.0033035469241440296 max memory_allocated 29231.615234375 
[2025-02-17 17:24:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.7029582262039185 norm:0.0029890493024140596 max memory_allocated 29231.615234375 
[2025-02-17 17:25:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.7026505470275879 norm:0.002738672774285078 max memory_allocated 29231.615234375 
[2025-02-17 17:25:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.7008069157600403 norm:0.0026264172047376633 max memory_allocated 29231.615234375 
[2025-02-17 17:26:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.699937105178833 norm:0.0024913358502089977 max memory_allocated 29231.615234375 
[2025-02-17 17:27:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.6992023587226868 norm:0.002511582337319851 max memory_allocated 29231.615234375 
[2025-02-17 17:27:58 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.6993436813354492 norm:0.002833459060639143 max memory_allocated 29231.615234375 
[2025-02-17 17:28:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.6982910633087158 norm:0.0031003705225884914 max memory_allocated 29231.615234375 
[2025-02-17 17:29:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.6972071528434753 norm:0.0031560687348246574 max memory_allocated 29231.615234375 
[2025-02-17 17:30:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.6963181495666504 norm:0.003032331820577383 max memory_allocated 29231.615234375 
[2025-02-17 17:30:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.6970597505569458 norm:0.0033548669889569283 max memory_allocated 29231.615234375 
[2025-02-17 17:31:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-17 17:31:56 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.7691177725791931 norm:0.02053830772638321 max memory_allocated 29231.802734375 
[2025-02-17 17:32:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.7448837757110596 norm:0.012133412063121796 max memory_allocated 29231.802734375 
[2025-02-17 17:33:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.7352436780929565 norm:0.008566636592149734 max memory_allocated 29231.802734375 
[2025-02-17 17:34:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.7298682928085327 norm:0.006777899339795113 max memory_allocated 29231.802734375 
[2025-02-17 17:34:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.7264794111251831 norm:0.005646831821650267 max memory_allocated 29231.802734375 
[2025-02-17 17:35:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.7240389585494995 norm:0.004902789369225502 max memory_allocated 29231.802734375 
[2025-02-17 17:36:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.7238017320632935 norm:0.004330521449446678 max memory_allocated 29231.802734375 
[2025-02-17 17:37:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.7195398211479187 norm:0.003642295952886343 max memory_allocated 29231.802734375 
[2025-02-17 17:37:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.7184770107269287 norm:0.0033205829095095396 max memory_allocated 29231.802734375 
[2025-02-17 17:38:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.7193934321403503 norm:0.0031239413656294346 max memory_allocated 29231.802734375 
[2025-02-17 17:39:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.71970134973526 norm:0.0029740380123257637 max memory_allocated 29231.802734375 
[2025-02-17 17:40:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.7181981801986694 norm:0.002782401628792286 max memory_allocated 29231.802734375 
[2025-02-17 17:40:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.7184286117553711 norm:0.0026894863694906235 max memory_allocated 29231.802734375 
[2025-02-17 17:41:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.7182028293609619 norm:0.002501494251191616 max memory_allocated 29231.802734375 
[2025-02-17 17:42:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.7178882360458374 norm:0.00235280254855752 max memory_allocated 29231.802734375 
[2025-02-17 17:43:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.7180758118629456 norm:0.0023879711516201496 max memory_allocated 29231.802734375 
[2025-02-17 17:43:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.7179378271102905 norm:0.0021405117586255074 max memory_allocated 29231.802734375 
[2025-02-17 17:44:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.719110906124115 norm:0.002180106472223997 max memory_allocated 29231.802734375 
[2025-02-17 17:45:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.7192018032073975 norm:0.002105048857629299 max memory_allocated 29231.802734375 
[2025-02-17 17:46:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.7187466025352478 norm:0.001990503864362836 max memory_allocated 29231.802734375 
[2025-02-17 17:46:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-17 17:47:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.7700851559638977 norm:0.01276802271604538 max memory_allocated 29231.990234375 
[2025-02-17 17:47:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.7531791925430298 norm:0.006992573384195566 max memory_allocated 29231.990234375 
[2025-02-17 17:48:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.7474912405014038 norm:0.005466105416417122 max memory_allocated 29231.990234375 
[2025-02-17 17:49:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.7451289892196655 norm:0.004702181555330753 max memory_allocated 29231.990234375 
[2025-02-17 17:50:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.742519736289978 norm:0.003918383736163378 max memory_allocated 29231.990234375 
[2025-02-17 17:50:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.7398684024810791 norm:0.0034569473937153816 max memory_allocated 29231.990234375 
[2025-02-17 17:51:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.7377713918685913 norm:0.003068281337618828 max memory_allocated 29231.990234375 
[2025-02-17 17:52:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.7365443706512451 norm:0.0028481187764555216 max memory_allocated 29231.990234375 
[2025-02-17 17:52:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.736967146396637 norm:0.0028308923356235027 max memory_allocated 29231.990234375 
[2025-02-17 17:53:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.7378456592559814 norm:0.0028111448045819998 max memory_allocated 29231.990234375 
[2025-02-17 17:54:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.7370678186416626 norm:0.0026015867479145527 max memory_allocated 29231.990234375 
[2025-02-17 17:55:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.7372848987579346 norm:0.0025753637310117483 max memory_allocated 29231.990234375 
[2025-02-17 17:55:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.7382264137268066 norm:0.0025089895352721214 max memory_allocated 29231.990234375 
[2025-02-17 17:56:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.7374618053436279 norm:0.0024667601101100445 max memory_allocated 29231.990234375 
[2025-02-17 17:57:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.7378730177879333 norm:0.0024659226182848215 max memory_allocated 29231.990234375 
[2025-02-17 17:58:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.7381797432899475 norm:0.0023382212966680527 max memory_allocated 29231.990234375 
[2025-02-17 17:58:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.7372483611106873 norm:0.0022367367055267096 max memory_allocated 29231.990234375 
[2025-02-17 17:59:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.7375273704528809 norm:0.0022618905641138554 max memory_allocated 29231.990234375 
[2025-02-17 18:00:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.7362418174743652 norm:0.0021381787955760956 max memory_allocated 29231.990234375 
[2025-02-17 18:01:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.7353672981262207 norm:0.002130480483174324 max memory_allocated 29231.990234375 
[2025-02-17 18:01:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-17 18:02:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.8114121556282043 norm:0.023425010964274406 max memory_allocated 29232.177734375 
[2025-02-17 18:02:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.7925080060958862 norm:0.013576346449553967 max memory_allocated 29232.177734375 
[2025-02-17 18:03:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.7823586463928223 norm:0.008913554251194 max memory_allocated 29232.177734375 
[2025-02-17 18:04:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.7786304950714111 norm:0.007059783674776554 max memory_allocated 29232.177734375 
[2025-02-17 18:05:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.7760006785392761 norm:0.005870684515684843 max memory_allocated 29232.177734375 
[2025-02-17 18:05:50 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.7717169523239136 norm:0.004771203268319368 max memory_allocated 29232.177734375 
[2025-02-17 18:06:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.769540011882782 norm:0.004380255937576294 max memory_allocated 29232.177734375 
[2025-02-17 18:07:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.7680124044418335 norm:0.0042439112439751625 max memory_allocated 29232.177734375 
[2025-02-17 18:08:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.7648755311965942 norm:0.0036304069217294455 max memory_allocated 29232.177734375 
[2025-02-17 18:08:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.7642605900764465 norm:0.0035535828210413456 max memory_allocated 29232.177734375 
[2025-02-17 18:09:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.7632142305374146 norm:0.0031871171668171883 max memory_allocated 29232.177734375 
[2025-02-17 18:10:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.761420488357544 norm:0.002769648563116789 max memory_allocated 29232.177734375 
[2025-02-17 18:11:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.7633146047592163 norm:0.002809364115819335 max memory_allocated 29232.177734375 
[2025-02-17 18:11:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.7648094296455383 norm:0.0028476680163294077 max memory_allocated 29232.177734375 
[2025-02-17 18:12:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.7655278444290161 norm:0.0028839088045060635 max memory_allocated 29232.177734375 
[2025-02-17 18:13:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.7667891383171082 norm:0.0028356497641652822 max memory_allocated 29232.177734375 
[2025-02-17 18:13:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.7657535672187805 norm:0.0027686767280101776 max memory_allocated 29232.177734375 
[2025-02-17 18:14:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.76595538854599 norm:0.002693190472200513 max memory_allocated 29232.177734375 
[2025-02-17 18:15:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.7645667791366577 norm:0.0027205816004425287 max memory_allocated 29232.177734375 
[2025-02-17 18:16:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.765586793422699 norm:0.002708177315071225 max memory_allocated 29232.177734375 
[2025-02-17 18:16:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-17 18:17:13 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.8076215982437134 norm:0.028822822496294975 max memory_allocated 29232.365234375 
[2025-02-17 18:17:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.789728581905365 norm:0.01658070832490921 max memory_allocated 29232.365234375 
[2025-02-17 18:18:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.7831252217292786 norm:0.01242003869265318 max memory_allocated 29232.365234375 
[2025-02-17 18:19:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.779887318611145 norm:0.010030481964349747 max memory_allocated 29232.365234375 
[2025-02-17 18:20:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.7759079337120056 norm:0.008265713229775429 max memory_allocated 29232.365234375 
[2025-02-17 18:20:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.7731041312217712 norm:0.007091193925589323 max memory_allocated 29232.365234375 
[2025-02-17 18:21:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.7711287140846252 norm:0.006153370253741741 max memory_allocated 29232.365234375 
[2025-02-17 18:22:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.7698792219161987 norm:0.00541288498789072 max memory_allocated 29232.365234375 
[2025-02-17 18:23:08 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.7693589925765991 norm:0.005196787882596254 max memory_allocated 29232.365234375 
[2025-02-17 18:23:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.7696529626846313 norm:0.004826026502996683 max memory_allocated 29232.365234375 
[2025-02-17 18:24:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.7685043215751648 norm:0.0047037736512720585 max memory_allocated 29232.365234375 
[2025-02-17 18:25:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.7670629620552063 norm:0.004115821328014135 max memory_allocated 29232.365234375 
[2025-02-17 18:26:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.7659127116203308 norm:0.003887641942128539 max memory_allocated 29232.365234375 
[2025-02-17 18:26:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.7647812366485596 norm:0.0038283998146653175 max memory_allocated 29232.365234375 
[2025-02-17 18:27:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.7651848793029785 norm:0.0037597548216581345 max memory_allocated 29232.365234375 
[2025-02-17 18:28:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.7652044892311096 norm:0.003504301654174924 max memory_allocated 29232.365234375 
[2025-02-17 18:29:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.7641410827636719 norm:0.003663277020677924 max memory_allocated 29232.365234375 
[2025-02-17 18:29:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.763689398765564 norm:0.0032869731076061726 max memory_allocated 29232.365234375 
[2025-02-17 18:30:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.7633580565452576 norm:0.0031716814264655113 max memory_allocated 29232.365234375 
[2025-02-17 18:31:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.763453483581543 norm:0.003080628579482436 max memory_allocated 29232.365234375 
[2025-02-17 18:31:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-17 18:32:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.845366358757019 norm:0.03907369077205658 max memory_allocated 29232.552734375 
[2025-02-17 18:33:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.8230038285255432 norm:0.0216013602912426 max memory_allocated 29232.552734375 
[2025-02-17 18:33:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.8159699440002441 norm:0.015020061284303665 max memory_allocated 29232.552734375 
[2025-02-17 18:34:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.8127368092536926 norm:0.012272768653929234 max memory_allocated 29232.552734375 
[2025-02-17 18:35:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.8088849186897278 norm:0.00923903938382864 max memory_allocated 29232.552734375 
[2025-02-17 18:36:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.8054547905921936 norm:0.007318344432860613 max memory_allocated 29232.552734375 
[2025-02-17 18:36:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.8053632974624634 norm:0.006906506605446339 max memory_allocated 29232.552734375 
[2025-02-17 18:37:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.804304301738739 norm:0.006120427045971155 max memory_allocated 29232.552734375 
[2025-02-17 18:38:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.8045620322227478 norm:0.0056828889064490795 max memory_allocated 29232.552734375 
[2025-02-17 18:38:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.8038380146026611 norm:0.005318501498550177 max memory_allocated 29232.552734375 
[2025-02-17 18:39:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.8026466965675354 norm:0.005020832177251577 max memory_allocated 29232.552734375 
[2025-02-17 18:40:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.8023499250411987 norm:0.004484033677726984 max memory_allocated 29232.552734375 
[2025-02-17 18:41:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.8014282584190369 norm:0.0041156187653541565 max memory_allocated 29232.552734375 
[2025-02-17 18:41:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.8013383746147156 norm:0.003959035966545343 max memory_allocated 29232.552734375 
[2025-02-17 18:42:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.8006499409675598 norm:0.003918701317161322 max memory_allocated 29232.552734375 
[2025-02-17 18:43:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.7975269556045532 norm:0.0037171535659581423 max memory_allocated 29232.552734375 
[2025-02-17 18:44:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.7976239919662476 norm:0.003262097714468837 max memory_allocated 29232.552734375 
[2025-02-17 18:44:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.797080934047699 norm:0.0032149872276932 max memory_allocated 29232.552734375 
[2025-02-17 18:45:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.7969321012496948 norm:0.0030790946912020445 max memory_allocated 29232.552734375 
[2025-02-17 18:46:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.7966532707214355 norm:0.0027345309499651194 max memory_allocated 29232.552734375 
[2025-02-17 18:46:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-17 18:47:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.8757623434066772 norm:0.014566801488399506 max memory_allocated 29232.740234375 
[2025-02-17 18:48:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.8640502095222473 norm:0.008953025564551353 max memory_allocated 29232.740234375 
[2025-02-17 18:48:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.8613417744636536 norm:0.007403206080198288 max memory_allocated 29232.740234375 
[2025-02-17 18:49:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.8598891496658325 norm:0.006401748396456242 max memory_allocated 29232.740234375 
[2025-02-17 18:50:20 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.8577089309692383 norm:0.005357340909540653 max memory_allocated 29232.740234375 
[2025-02-17 18:51:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.8552947044372559 norm:0.004680823069065809 max memory_allocated 29232.740234375 
[2025-02-17 18:51:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.8536456227302551 norm:0.004153257701545954 max memory_allocated 29232.740234375 
[2025-02-17 18:52:34 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.8532841801643372 norm:0.0037500665057450533 max memory_allocated 29232.740234375 
[2025-02-17 18:53:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.8528388738632202 norm:0.003541921963915229 max memory_allocated 29232.740234375 
[2025-02-17 18:54:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.8532343506813049 norm:0.0037650922313332558 max memory_allocated 29232.740234375 
[2025-02-17 18:54:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.852471649646759 norm:0.0034423081669956446 max memory_allocated 29232.740234375 
[2025-02-17 18:55:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.8527052402496338 norm:0.0032295233104377985 max memory_allocated 29232.740234375 
[2025-02-17 18:56:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.8524417281150818 norm:0.002927144756540656 max memory_allocated 29232.740234375 
[2025-02-17 18:57:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.8518097996711731 norm:0.003036251524463296 max memory_allocated 29232.740234375 
[2025-02-17 18:57:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.8512897491455078 norm:0.0029266600031405687 max memory_allocated 29232.740234375 
[2025-02-17 18:58:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.8509425520896912 norm:0.002497173612937331 max memory_allocated 29232.740234375 
[2025-02-17 18:59:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.8519866466522217 norm:0.0027720248326659203 max memory_allocated 29232.740234375 
[2025-02-17 18:59:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.8521116375923157 norm:0.002963949926197529 max memory_allocated 29232.740234375 
[2025-02-17 19:00:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.8517805933952332 norm:0.002975506242364645 max memory_allocated 29232.740234375 
[2025-02-17 19:01:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.8531200885772705 norm:0.0025599589571356773 max memory_allocated 29232.740234375 
[2025-02-17 19:01:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-17 19:02:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.9390252828598022 norm:0.016589011996984482 max memory_allocated 29232.927734375 
[2025-02-17 19:03:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.9249026775360107 norm:0.009794581681489944 max memory_allocated 29232.927734375 
[2025-02-17 19:03:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.9195560216903687 norm:0.007570582441985607 max memory_allocated 29232.927734375 
[2025-02-17 19:04:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.9195373058319092 norm:0.006989837624132633 max memory_allocated 29232.927734375 
[2025-02-17 19:05:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.919960081577301 norm:0.006122604012489319 max memory_allocated 29232.927734375 
[2025-02-17 19:06:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.9189728498458862 norm:0.005987333599478006 max memory_allocated 29232.927734375 
[2025-02-17 19:06:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.9199991226196289 norm:0.0059792413376271725 max memory_allocated 29232.927734375 
[2025-02-17 19:07:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.9200218915939331 norm:0.004273322876542807 max memory_allocated 29232.927734375 
[2025-02-17 19:08:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.919273853302002 norm:0.005372796207666397 max memory_allocated 29232.927734375 
[2025-02-17 19:09:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.9193422198295593 norm:0.004113603848963976 max memory_allocated 29232.927734375 
[2025-02-17 19:09:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.9199034571647644 norm:0.004960570018738508 max memory_allocated 29232.927734375 
[2025-02-17 19:10:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.9213721752166748 norm:0.0043262033723294735 max memory_allocated 29232.927734375 
[2025-02-17 19:11:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.9191054105758667 norm:0.005387197248637676 max memory_allocated 29232.927734375 
[2025-02-17 19:12:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.9207555055618286 norm:0.004868106916546822 max memory_allocated 29232.927734375 
[2025-02-17 19:12:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.9207463264465332 norm:0.004737359471619129 max memory_allocated 29232.927734375 
[2025-02-17 19:13:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.9198461174964905 norm:0.005102787632495165 max memory_allocated 29232.927734375 
[2025-02-17 19:14:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.9190423488616943 norm:0.004827509168535471 max memory_allocated 29232.927734375 
[2025-02-17 19:15:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.9207482933998108 norm:0.0035878336057066917 max memory_allocated 29232.927734375 
[2025-02-17 19:15:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.9186437726020813 norm:0.0041506304405629635 max memory_allocated 29232.927734375 
[2025-02-17 19:16:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.9196392297744751 norm:0.004668490029871464 max memory_allocated 29232.927734375 
[2025-02-17 19:16:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-17 19:17:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:1.0019015073776245 norm:0.016593467444181442 max memory_allocated 29233.115234375 
[2025-02-17 19:18:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.9903799295425415 norm:0.01107244472950697 max memory_allocated 29233.115234375 
[2025-02-17 19:19:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.9848966002464294 norm:0.00831206701695919 max memory_allocated 29233.115234375 
[2025-02-17 19:19:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.9812167286872864 norm:0.006811066530644894 max memory_allocated 29233.115234375 
[2025-02-17 19:20:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.9801006317138672 norm:0.005912717431783676 max memory_allocated 29233.115234375 
[2025-02-17 19:21:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.9802396893501282 norm:0.005506871733814478 max memory_allocated 29233.115234375 
[2025-02-17 19:22:00 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.9779517650604248 norm:0.004703512880951166 max memory_allocated 29233.115234375 
[2025-02-17 19:22:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.9767944812774658 norm:0.004228578880429268 max memory_allocated 29233.115234375 
[2025-02-17 19:23:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.9750458598136902 norm:0.0035920431837439537 max memory_allocated 29233.115234375 
[2025-02-17 19:24:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.9749371409416199 norm:0.00366065907292068 max memory_allocated 29233.115234375 
[2025-02-17 19:24:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.9733763933181763 norm:0.003137956140562892 max memory_allocated 29233.115234375 
[2025-02-17 19:25:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.9741499423980713 norm:0.0031845304183661938 max memory_allocated 29233.115234375 
[2025-02-17 19:26:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.9757425785064697 norm:0.003392343409359455 max memory_allocated 29233.115234375 
[2025-02-17 19:27:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.9758957624435425 norm:0.0030742117669433355 max memory_allocated 29233.115234375 
[2025-02-17 19:27:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.9743539094924927 norm:0.0029447374399751425 max memory_allocated 29233.115234375 
[2025-02-17 19:28:40 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.9736824035644531 norm:0.0028567772824317217 max memory_allocated 29233.115234375 
[2025-02-17 19:29:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.9740440845489502 norm:0.002913935109972954 max memory_allocated 29233.115234375 
[2025-02-17 19:30:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.9728710651397705 norm:0.0026953029446303844 max memory_allocated 29233.115234375 
[2025-02-17 19:30:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.9730775952339172 norm:0.0029327217489480972 max memory_allocated 29233.115234375 
[2025-02-17 19:31:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.9714966416358948 norm:0.002477207686752081 max memory_allocated 29233.115234375 
[2025-02-17 19:31:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-17 19:32:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:1.0799039602279663 norm:0.014130419120192528 max memory_allocated 29233.302734375 
[2025-02-17 19:33:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:1.0691399574279785 norm:0.009555243887007236 max memory_allocated 29233.302734375 
[2025-02-17 19:34:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:1.0639678239822388 norm:0.007312927860766649 max memory_allocated 29233.302734375 
[2025-02-17 19:34:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:1.0617079734802246 norm:0.005893559195101261 max memory_allocated 29233.302734375 
[2025-02-17 19:35:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:1.0619112253189087 norm:0.005168928764760494 max memory_allocated 29233.302734375 
[2025-02-17 19:36:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:1.0614310503005981 norm:0.0046342299319803715 max memory_allocated 29233.302734375 
[2025-02-17 19:37:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:1.0610407590866089 norm:0.004120037890970707 max memory_allocated 29233.302734375 
[2025-02-17 19:37:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:1.060187816619873 norm:0.00335743953473866 max memory_allocated 29233.302734375 
[2025-02-17 19:38:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:1.0603363513946533 norm:0.00346836494281888 max memory_allocated 29233.302734375 
[2025-02-17 19:39:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:1.0594557523727417 norm:0.002948483219370246 max memory_allocated 29233.302734375 
[2025-02-17 19:40:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:1.060066819190979 norm:0.003190141636878252 max memory_allocated 29233.302734375 
[2025-02-17 19:40:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:1.060049295425415 norm:0.00280135590583086 max memory_allocated 29233.302734375 
[2025-02-17 19:41:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:1.0606878995895386 norm:0.002829502569511533 max memory_allocated 29233.302734375 
[2025-02-17 19:42:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:1.0599077939987183 norm:0.0023328771349042654 max memory_allocated 29233.302734375 
[2025-02-17 19:43:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:1.0603034496307373 norm:0.0027381302788853645 max memory_allocated 29233.302734375 
[2025-02-17 19:43:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:1.060402512550354 norm:0.0029951483011245728 max memory_allocated 29233.302734375 
[2025-02-17 19:44:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:1.0608453750610352 norm:0.0026572204660624266 max memory_allocated 29233.302734375 
[2025-02-17 19:45:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:1.0612324476242065 norm:0.0027379884850233793 max memory_allocated 29233.302734375 
[2025-02-17 19:45:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:1.0607836246490479 norm:0.0025723460130393505 max memory_allocated 29233.302734375 
[2025-02-17 19:46:43 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:1.0603126287460327 norm:0.0026669083163142204 max memory_allocated 29233.302734375 
[2025-02-17 19:46:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-17 19:47:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:1.1711472272872925 norm:0.02435886301100254 max memory_allocated 29233.490234375 
[2025-02-17 19:48:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:1.1639188528060913 norm:0.015764327719807625 max memory_allocated 29233.490234375 
[2025-02-17 19:49:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:1.1602578163146973 norm:0.011771056801080704 max memory_allocated 29233.490234375 
[2025-02-17 19:49:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:1.1587494611740112 norm:0.00940505787730217 max memory_allocated 29233.490234375 
[2025-02-17 19:50:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:1.1579484939575195 norm:0.007799659390002489 max memory_allocated 29233.490234375 
[2025-02-17 19:51:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:1.1567556858062744 norm:0.006510993465781212 max memory_allocated 29233.490234375 
[2025-02-17 19:52:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:1.1560771465301514 norm:0.005905982106924057 max memory_allocated 29233.490234375 
[2025-02-17 19:52:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:1.1554200649261475 norm:0.005224438849836588 max memory_allocated 29233.490234375 
[2025-02-17 19:53:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:1.1552581787109375 norm:0.004683175589889288 max memory_allocated 29233.490234375 
[2025-02-17 19:54:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:1.1551122665405273 norm:0.00419173389673233 max memory_allocated 29233.490234375 
[2025-02-17 19:55:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:1.1540790796279907 norm:0.003808927722275257 max memory_allocated 29233.490234375 
[2025-02-17 19:55:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:1.153908610343933 norm:0.0035668639466166496 max memory_allocated 29233.490234375 
[2025-02-17 19:56:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:1.1540741920471191 norm:0.0033945036120712757 max memory_allocated 29233.490234375 
[2025-02-17 19:57:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:1.1538569927215576 norm:0.003190943505614996 max memory_allocated 29233.490234375 
[2025-02-17 19:58:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:1.1538599729537964 norm:0.003073552157729864 max memory_allocated 29233.490234375 
[2025-02-17 19:58:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:1.1533465385437012 norm:0.0029211570508778095 max memory_allocated 29233.490234375 
[2025-02-17 19:59:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:1.1536810398101807 norm:0.002600481966510415 max memory_allocated 29233.490234375 
[2025-02-17 20:00:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:1.154818058013916 norm:0.002681673737242818 max memory_allocated 29233.490234375 
[2025-02-17 20:01:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:1.1537933349609375 norm:0.0026449942961335182 max memory_allocated 29233.490234375 
[2025-02-17 20:01:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:1.1531881093978882 norm:0.002433606656268239 max memory_allocated 29233.490234375 
[2025-02-17 20:02:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-17 20:02:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:1.249190330505371 norm:0.012532254680991173 max memory_allocated 29233.677734375 
[2025-02-17 20:03:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:1.2447478771209717 norm:0.009741578251123428 max memory_allocated 29233.677734375 
[2025-02-17 20:04:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:1.2409956455230713 norm:0.007738686166703701 max memory_allocated 29233.677734375 
[2025-02-17 20:05:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:1.237833023071289 norm:0.006588794756680727 max memory_allocated 29233.677734375 
[2025-02-17 20:05:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:1.2361091375350952 norm:0.006170613691210747 max memory_allocated 29233.677734375 
[2025-02-17 20:06:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:1.233837366104126 norm:0.005757521837949753 max memory_allocated 29233.677734375 
[2025-02-17 20:07:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:1.2345556020736694 norm:0.005984270945191383 max memory_allocated 29233.677734375 
[2025-02-17 20:07:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:1.2334208488464355 norm:0.006304808426648378 max memory_allocated 29233.677734375 
[2025-02-17 20:08:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:1.2327297925949097 norm:0.005984616000205278 max memory_allocated 29233.677734375 
[2025-02-17 20:09:28 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:1.2323554754257202 norm:0.006227440666407347 max memory_allocated 29233.677734375 
[2025-02-17 20:10:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:1.2329998016357422 norm:0.005910028237849474 max memory_allocated 29233.677734375 
[2025-02-17 20:10:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:1.2331732511520386 norm:0.006389348302036524 max memory_allocated 29233.677734375 
[2025-02-17 20:11:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:1.2315471172332764 norm:0.006650123745203018 max memory_allocated 29233.677734375 
[2025-02-17 20:12:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:1.2307270765304565 norm:0.007028622552752495 max memory_allocated 29233.677734375 
[2025-02-17 20:13:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:1.2309925556182861 norm:0.007725034840404987 max memory_allocated 29233.677734375 
[2025-02-17 20:13:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:1.2301918268203735 norm:0.007769579999148846 max memory_allocated 29233.677734375 
[2025-02-17 20:14:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:1.230966567993164 norm:0.008118359372019768 max memory_allocated 29233.677734375 
[2025-02-17 20:15:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:1.2306504249572754 norm:0.008637307211756706 max memory_allocated 29233.677734375 
[2025-02-17 20:16:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:1.2305485010147095 norm:0.009835425764322281 max memory_allocated 29233.677734375 
[2025-02-17 20:16:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:1.2301857471466064 norm:0.010879013687372208 max memory_allocated 29233.677734375 
[2025-02-17 20:17:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-17 20:18:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:1.350303292274475 norm:0.010888399556279182 max memory_allocated 29233.865234375 
[2025-02-17 20:18:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:1.34442138671875 norm:0.007553883828222752 max memory_allocated 29233.865234375 
[2025-02-17 20:19:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:1.3417526483535767 norm:0.005724563729017973 max memory_allocated 29233.865234375 
[2025-02-17 20:20:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:1.3400524854660034 norm:0.004673904739320278 max memory_allocated 29233.865234375 
[2025-02-17 20:21:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:1.3398438692092896 norm:0.004318338353186846 max memory_allocated 29233.865234375 
[2025-02-17 20:21:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:1.3389853239059448 norm:0.0037339485716074705 max memory_allocated 29233.865234375 
[2025-02-17 20:22:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:1.3380340337753296 norm:0.00334772071801126 max memory_allocated 29233.865234375 
[2025-02-17 20:23:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:1.3374496698379517 norm:0.00304671679623425 max memory_allocated 29233.865234375 
[2025-02-17 20:23:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:1.33720064163208 norm:0.002858174964785576 max memory_allocated 29233.865234375 
[2025-02-17 20:24:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:1.3368656635284424 norm:0.002607807284221053 max memory_allocated 29233.865234375 
[2025-02-17 20:25:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:1.3369934558868408 norm:0.002580460160970688 max memory_allocated 29233.865234375 
[2025-02-17 20:26:12 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:1.3361773490905762 norm:0.0023465738631784916 max memory_allocated 29233.865234375 
[2025-02-17 20:26:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:1.3358213901519775 norm:0.002288347575813532 max memory_allocated 29233.865234375 
[2025-02-17 20:27:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:1.3349710702896118 norm:0.0019952834118157625 max memory_allocated 29233.865234375 
[2025-02-17 20:28:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:1.335770845413208 norm:0.0019534393213689327 max memory_allocated 29233.865234375 
[2025-02-17 20:29:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:1.3347632884979248 norm:0.002110876841470599 max memory_allocated 29233.865234375 
[2025-02-17 20:29:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:1.3341373205184937 norm:0.0018278720090165734 max memory_allocated 29233.865234375 
[2025-02-17 20:30:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:1.334527611732483 norm:0.0019318045815452933 max memory_allocated 29233.865234375 
[2025-02-17 20:31:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:1.333634614944458 norm:0.002023211680352688 max memory_allocated 29233.865234375 
[2025-02-17 20:32:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:1.3336960077285767 norm:0.0018460068386048079 max memory_allocated 29233.865234375 
[2025-02-17 20:32:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-17 20:33:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:1.490000605583191 norm:0.01356154028326273 max memory_allocated 29234.052734375 
[2025-02-17 20:33:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:1.4833248853683472 norm:0.009608806110918522 max memory_allocated 29234.052734375 
[2025-02-17 20:34:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:1.4808980226516724 norm:0.007501330226659775 max memory_allocated 29234.052734375 
[2025-02-17 20:35:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:1.4796433448791504 norm:0.006156339310109615 max memory_allocated 29234.052734375 
[2025-02-17 20:36:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:1.4795500040054321 norm:0.005043372977524996 max memory_allocated 29234.052734375 
[2025-02-17 20:36:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:1.4791951179504395 norm:0.004164583049714565 max memory_allocated 29234.052734375 
[2025-02-17 20:37:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:1.479844093322754 norm:0.003648415207862854 max memory_allocated 29234.052734375 
[2025-02-17 20:38:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:1.480442762374878 norm:0.0033446031156927347 max memory_allocated 29234.052734375 
[2025-02-17 20:39:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:1.4801256656646729 norm:0.0030319998040795326 max memory_allocated 29234.052734375 
[2025-02-17 20:39:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:1.4805400371551514 norm:0.002816835418343544 max memory_allocated 29234.052734375 
[2025-02-17 20:40:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:1.4807926416397095 norm:0.002785023534670472 max memory_allocated 29234.052734375 
[2025-02-17 20:41:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:1.4803590774536133 norm:0.0024146451614797115 max memory_allocated 29234.052734375 
[2025-02-17 20:42:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:1.4810798168182373 norm:0.002482365118339658 max memory_allocated 29234.052734375 
[2025-02-17 20:42:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:1.4810229539871216 norm:0.0023766064550727606 max memory_allocated 29234.052734375 
[2025-02-17 20:43:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:1.4818127155303955 norm:0.0024698846973478794 max memory_allocated 29234.052734375 
[2025-02-17 20:44:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:1.4823673963546753 norm:0.00277687911875546 max memory_allocated 29234.052734375 
[2025-02-17 20:45:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:1.4820318222045898 norm:0.003138734493404627 max memory_allocated 29234.052734375 
[2025-02-17 20:45:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:1.481137752532959 norm:0.00270509603433311 max memory_allocated 29234.052734375 
[2025-02-17 20:46:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:1.4829890727996826 norm:0.0022685604635626078 max memory_allocated 29234.052734375 
[2025-02-17 20:47:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:1.4830588102340698 norm:0.0034957691095769405 max memory_allocated 29234.052734375 
[2025-02-17 20:47:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-17 20:48:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:1.6169517040252686 norm:0.006233398802578449 max memory_allocated 29234.240234375 
[2025-02-17 20:49:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:1.6129167079925537 norm:0.0046584028750658035 max memory_allocated 29234.240234375 
[2025-02-17 20:49:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:1.6116063594818115 norm:0.0037863487377762794 max memory_allocated 29234.240234375 
[2025-02-17 20:50:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:1.610793113708496 norm:0.0032014783937484026 max memory_allocated 29234.240234375 
[2025-02-17 20:51:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:1.6102367639541626 norm:0.0029738526791334152 max memory_allocated 29234.240234375 
[2025-02-17 20:52:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:1.6092920303344727 norm:0.0024111184757202864 max memory_allocated 29234.240234375 
[2025-02-17 20:52:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:1.6085431575775146 norm:0.0022391786333173513 max memory_allocated 29234.240234375 
[2025-02-17 20:53:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:1.6084542274475098 norm:0.002284427173435688 max memory_allocated 29234.240234375 
[2025-02-17 20:54:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:1.607459306716919 norm:0.001964608207345009 max memory_allocated 29234.240234375 
[2025-02-17 20:55:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:1.6081024408340454 norm:0.0017037497600540519 max memory_allocated 29234.240234375 
[2025-02-17 20:55:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:1.6078428030014038 norm:0.0017461874522268772 max memory_allocated 29234.240234375 
[2025-02-17 20:56:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:1.6084891557693481 norm:0.0016556376358494163 max memory_allocated 29234.240234375 
[2025-02-17 20:57:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:1.6083171367645264 norm:0.0016892682760953903 max memory_allocated 29234.240234375 
[2025-02-17 20:58:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:1.6087557077407837 norm:0.0014621720183640718 max memory_allocated 29234.240234375 
[2025-02-17 20:58:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:1.6097406148910522 norm:0.0018768004374578595 max memory_allocated 29234.240234375 
[2025-02-17 20:59:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:1.6097620725631714 norm:0.0014437846839427948 max memory_allocated 29234.240234375 
[2025-02-17 21:00:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:1.6095699071884155 norm:0.0017771499697118998 max memory_allocated 29234.240234375 
[2025-02-17 21:00:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:1.6100527048110962 norm:0.001540633151307702 max memory_allocated 29234.240234375 
[2025-02-17 21:01:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:1.6102393865585327 norm:0.0014869451988488436 max memory_allocated 29234.240234375 
[2025-02-17 21:02:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:1.6100513935089111 norm:0.00158980512060225 max memory_allocated 29234.240234375 
[2025-02-17 21:02:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-17 21:03:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:1.7334953546524048 norm:0.01979941874742508 max memory_allocated 29234.427734375 
[2025-02-17 21:04:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:1.726332187652588 norm:0.01383273396641016 max memory_allocated 29234.427734375 
[2025-02-17 21:04:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:1.7226136922836304 norm:0.010614452883601189 max memory_allocated 29234.427734375 
[2025-02-17 21:05:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:1.7216287851333618 norm:0.008744181133806705 max memory_allocated 29234.427734375 
[2025-02-17 21:06:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:1.7203941345214844 norm:0.007409868761897087 max memory_allocated 29234.427734375 
[2025-02-17 21:07:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:1.7198907136917114 norm:0.006459991447627544 max memory_allocated 29234.427734375 
[2025-02-17 21:07:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:1.7204738855361938 norm:0.005776752717792988 max memory_allocated 29234.427734375 
[2025-02-17 21:08:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:1.72135591506958 norm:0.0053559537045657635 max memory_allocated 29234.427734375 
[2025-02-17 21:09:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:1.720442771911621 norm:0.0049981484189629555 max memory_allocated 29234.427734375 
[2025-02-17 21:10:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:1.7202129364013672 norm:0.004460279364138842 max memory_allocated 29234.427734375 
[2025-02-17 21:10:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:1.719327449798584 norm:0.004480500239878893 max memory_allocated 29234.427734375 
[2025-02-17 21:11:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:1.7199995517730713 norm:0.004241642076522112 max memory_allocated 29234.427734375 
[2025-02-17 21:12:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:1.720096230506897 norm:0.0041652279905974865 max memory_allocated 29234.427734375 
[2025-02-17 21:13:06 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:1.7197328805923462 norm:0.00406260509043932 max memory_allocated 29234.427734375 
[2025-02-17 21:13:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:1.7189693450927734 norm:0.0036380947567522526 max memory_allocated 29234.427734375 
[2025-02-17 21:14:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:1.7201255559921265 norm:0.0038314340636134148 max memory_allocated 29234.427734375 
[2025-02-17 21:15:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:1.7196747064590454 norm:0.0036658954340964556 max memory_allocated 29234.427734375 
[2025-02-17 21:16:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:1.7198885679244995 norm:0.004253868479281664 max memory_allocated 29234.427734375 
[2025-02-17 21:16:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:1.7181750535964966 norm:0.003441657405346632 max memory_allocated 29234.427734375 
[2025-02-17 21:17:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:1.718462347984314 norm:0.003447551280260086 max memory_allocated 29234.427734375 
[2025-02-17 21:17:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-17 21:18:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:1.8978766202926636 norm:0.0094029251486063 max memory_allocated 29234.615234375 
[2025-02-17 21:19:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:1.8947802782058716 norm:0.0066244942136108875 max memory_allocated 29234.615234375 
[2025-02-17 21:20:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:1.8922004699707031 norm:0.005208214279264212 max memory_allocated 29234.615234375 
[2025-02-17 21:20:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:1.891485571861267 norm:0.004284145310521126 max memory_allocated 29234.615234375 
[2025-02-17 21:21:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:1.890449047088623 norm:0.0037038184236735106 max memory_allocated 29234.615234375 
[2025-02-17 21:22:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:1.8911235332489014 norm:0.0033249822445213795 max memory_allocated 29234.615234375 
[2025-02-17 21:23:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:1.8906327486038208 norm:0.002937664743512869 max memory_allocated 29234.615234375 
[2025-02-17 21:23:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:1.8902349472045898 norm:0.002733794739469886 max memory_allocated 29234.615234375 
[2025-02-17 21:24:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:1.8909562826156616 norm:0.0025700426194816828 max memory_allocated 29234.615234375 
[2025-02-17 21:25:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:1.889595866203308 norm:0.002288194838911295 max memory_allocated 29234.615234375 
[2025-02-17 21:25:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:1.8892844915390015 norm:0.0024879807606339455 max memory_allocated 29234.615234375 
[2025-02-17 21:26:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:1.8898824453353882 norm:0.002104152226820588 max memory_allocated 29234.615234375 
[2025-02-17 21:27:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:1.8901230096817017 norm:0.0021094009280204773 max memory_allocated 29234.615234375 
[2025-02-17 21:28:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:1.889653205871582 norm:0.0024858335964381695 max memory_allocated 29234.615234375 
[2025-02-17 21:28:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:1.889387845993042 norm:0.0018948331708088517 max memory_allocated 29234.615234375 
[2025-02-17 21:29:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:1.8893225193023682 norm:0.0019305511377751827 max memory_allocated 29234.615234375 
[2025-02-17 21:30:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:1.8883650302886963 norm:0.0019191609462723136 max memory_allocated 29234.615234375 
[2025-02-17 21:31:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:1.8899874687194824 norm:0.002302902052178979 max memory_allocated 29234.615234375 
[2025-02-17 21:31:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:1.8902325630187988 norm:0.0022475826554000378 max memory_allocated 29234.615234375 
[2025-02-17 21:32:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:1.8888603448867798 norm:0.0021906960755586624 max memory_allocated 29234.615234375 
[2025-02-17 21:32:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-17 21:33:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:2.1033966541290283 norm:0.017275799065828323 max memory_allocated 29234.802734375 
[2025-02-17 21:34:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:2.0955283641815186 norm:0.01105818897485733 max memory_allocated 29234.802734375 
[2025-02-17 21:35:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:2.09196400642395 norm:0.008230212144553661 max memory_allocated 29234.802734375 
[2025-02-17 21:35:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:2.089099645614624 norm:0.006455328315496445 max memory_allocated 29234.802734375 
[2025-02-17 21:36:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:2.089167833328247 norm:0.00544672179967165 max memory_allocated 29234.802734375 
[2025-02-17 21:37:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:2.0881006717681885 norm:0.004729417152702808 max memory_allocated 29234.802734375 
[2025-02-17 21:38:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:2.087385654449463 norm:0.004220994655042887 max memory_allocated 29234.802734375 
[2025-02-17 21:38:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:2.086731195449829 norm:0.003688471857458353 max memory_allocated 29234.802734375 
[2025-02-17 21:39:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:2.0886807441711426 norm:0.003830245230346918 max memory_allocated 29234.802734375 
[2025-02-17 21:40:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:2.0864105224609375 norm:0.0033177677541971207 max memory_allocated 29234.802734375 
[2025-02-17 21:41:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:2.087315797805786 norm:0.0030719537753611803 max memory_allocated 29234.802734375 
[2025-02-17 21:41:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:2.086090087890625 norm:0.003235684707760811 max memory_allocated 29234.802734375 
[2025-02-17 21:42:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:2.085505962371826 norm:0.002689114771783352 max memory_allocated 29234.802734375 
[2025-02-17 21:43:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:2.0860695838928223 norm:0.00252553797326982 max memory_allocated 29234.802734375 
[2025-02-17 21:44:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:2.085827350616455 norm:0.002467374550178647 max memory_allocated 29234.802734375 
[2025-02-17 21:44:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:2.0864152908325195 norm:0.0030082915909588337 max memory_allocated 29234.802734375 
[2025-02-17 21:45:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:2.0851430892944336 norm:0.002484231488779187 max memory_allocated 29234.802734375 
[2025-02-17 21:46:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:2.0851473808288574 norm:0.002295084297657013 max memory_allocated 29234.802734375 
[2025-02-17 21:46:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:2.084226369857788 norm:0.0029591163620352745 max memory_allocated 29234.802734375 
[2025-02-17 21:47:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:2.084113836288452 norm:0.002068777335807681 max memory_allocated 29234.802734375 
[2025-02-17 21:47:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-17 21:48:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:2.2789947986602783 norm:0.010383952409029007 max memory_allocated 29234.990234375 
[2025-02-17 21:49:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:2.275853157043457 norm:0.008219683542847633 max memory_allocated 29234.990234375 
[2025-02-17 21:50:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:2.2733757495880127 norm:0.0067753903567790985 max memory_allocated 29234.990234375 
[2025-02-17 21:50:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:2.2732136249542236 norm:0.005709678865969181 max memory_allocated 29234.990234375 
[2025-02-17 21:51:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:2.2730727195739746 norm:0.00490821897983551 max memory_allocated 29234.990234375 
[2025-02-17 21:52:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:2.273853063583374 norm:0.004463766701519489 max memory_allocated 29234.990234375 
[2025-02-17 21:53:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:2.2751691341400146 norm:0.00428764009848237 max memory_allocated 29234.990234375 
[2025-02-17 21:53:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:2.276561975479126 norm:0.0040071685798466206 max memory_allocated 29234.990234375 
[2025-02-17 21:54:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:2.275139093399048 norm:0.0035849446430802345 max memory_allocated 29234.990234375 
[2025-02-17 21:55:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:2.2764854431152344 norm:0.003697969950735569 max memory_allocated 29234.990234375 
[2025-02-17 21:56:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:2.2769641876220703 norm:0.0032624471932649612 max memory_allocated 29234.990234375 
[2025-02-17 21:56:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:2.276303768157959 norm:0.003349939826875925 max memory_allocated 29234.990234375 
[2025-02-17 21:57:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:2.27653169631958 norm:0.0031386143527925014 max memory_allocated 29234.990234375 
[2025-02-17 21:58:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:2.2769346237182617 norm:0.003286834806203842 max memory_allocated 29234.990234375 
[2025-02-17 21:59:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:2.2761049270629883 norm:0.003416449762880802 max memory_allocated 29234.990234375 
[2025-02-17 21:59:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:2.2770724296569824 norm:0.0034132427535951138 max memory_allocated 29234.990234375 
[2025-02-17 22:00:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:2.2770254611968994 norm:0.0030738685745745897 max memory_allocated 29234.990234375 
[2025-02-17 22:01:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:2.2769618034362793 norm:0.002712551038712263 max memory_allocated 29234.990234375 
[2025-02-17 22:02:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:2.2769975662231445 norm:0.0028043861966580153 max memory_allocated 29234.990234375 
[2025-02-17 22:02:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:2.2768168449401855 norm:0.0034931369591504335 max memory_allocated 29234.990234375 
[2025-02-17 22:03:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-17 22:03:49 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:2.5309550762176514 norm:0.024492334574460983 max memory_allocated 29235.177734375 
[2025-02-17 22:04:34 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:2.5357303619384766 norm:0.026342272758483887 max memory_allocated 29235.177734375 
[2025-02-17 22:05:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:2.5211141109466553 norm:0.015490876510739326 max memory_allocated 29235.177734375 
[2025-02-17 22:06:03 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:2.51495099067688 norm:0.012727629393339157 max memory_allocated 29235.177734375 
[2025-02-17 22:06:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:2.515137195587158 norm:0.011785918846726418 max memory_allocated 29235.177734375 
[2025-02-17 22:07:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:2.5059361457824707 norm:0.009125562384724617 max memory_allocated 29235.177734375 
[2025-02-17 22:08:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:2.5149710178375244 norm:0.011505347676575184 max memory_allocated 29235.177734375 
[2025-02-17 22:09:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:2.500791549682617 norm:0.007544919848442078 max memory_allocated 29235.177734375 
[2025-02-17 22:09:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:2.5057003498077393 norm:0.007683077827095985 max memory_allocated 29235.177734375 
[2025-02-17 22:10:30 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:2.5112884044647217 norm:0.010675476863980293 max memory_allocated 29235.177734375 
[2025-02-17 22:11:14 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:2.4963338375091553 norm:0.005905691999942064 max memory_allocated 29235.177734375 
[2025-02-17 22:11:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:2.501383066177368 norm:0.006537226028740406 max memory_allocated 29235.177734375 
[2025-02-17 22:12:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:2.5168097019195557 norm:0.02280394360423088 max memory_allocated 29235.177734375 
[2025-02-17 22:13:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:2.5030465126037598 norm:0.00891450047492981 max memory_allocated 29235.177734375 
[2025-02-17 22:14:12 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:2.508303642272949 norm:0.012288886122405529 max memory_allocated 29235.177734375 
[2025-02-17 22:14:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:2.505138874053955 norm:0.007992345839738846 max memory_allocated 29235.177734375 
[2025-02-17 22:15:41 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:2.500464916229248 norm:0.005866778548806906 max memory_allocated 29235.177734375 
[2025-02-17 22:16:26 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:2.4987294673919678 norm:0.005397627130150795 max memory_allocated 29235.177734375 
[2025-02-17 22:17:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:2.5023417472839355 norm:0.0056832111440598965 max memory_allocated 29235.177734375 
[2025-02-17 22:17:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:2.5067880153656006 norm:0.0077146971598267555 max memory_allocated 29235.177734375 
[2025-02-17 22:18:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-17 22:18:54 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:2.7139408588409424 norm:0.01747366040945053 max memory_allocated 29235.365234375 
[2025-02-17 22:19:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:2.7055957317352295 norm:0.012903708964586258 max memory_allocated 29235.365234375 
[2025-02-17 22:20:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:2.703328847885132 norm:0.010129086673259735 max memory_allocated 29235.365234375 
[2025-02-17 22:21:08 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:2.7035791873931885 norm:0.008626801893115044 max memory_allocated 29235.365234375 
[2025-02-17 22:21:53 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:2.705169677734375 norm:0.007690563332289457 max memory_allocated 29235.365234375 
[2025-02-17 22:22:37 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:2.7055070400238037 norm:0.006363549269735813 max memory_allocated 29235.365234375 
[2025-02-17 22:23:22 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:2.706049919128418 norm:0.0055997311137616634 max memory_allocated 29235.365234375 
[2025-02-17 22:24:06 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:2.706732988357544 norm:0.004959062673151493 max memory_allocated 29235.365234375 
[2025-02-17 22:24:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:2.7081873416900635 norm:0.004902590997517109 max memory_allocated 29235.365234375 
[2025-02-17 22:25:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:2.7084295749664307 norm:0.004535863175988197 max memory_allocated 29235.365234375 
[2025-02-17 22:26:20 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:2.7091593742370605 norm:0.004311618395149708 max memory_allocated 29235.365234375 
[2025-02-17 22:27:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:2.7069284915924072 norm:0.004142696503549814 max memory_allocated 29235.365234375 
[2025-02-17 22:27:49 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:2.708739757537842 norm:0.004708208609372377 max memory_allocated 29235.365234375 
[2025-02-17 22:28:33 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:2.7066490650177 norm:0.0039299651980400085 max memory_allocated 29235.365234375 
[2025-02-17 22:29:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:2.7113940715789795 norm:0.0060299537144601345 max memory_allocated 29235.365234375 
[2025-02-17 22:30:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:2.701137065887451 norm:0.003351785708218813 max memory_allocated 29235.365234375 
[2025-02-17 22:30:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:2.7061705589294434 norm:0.00393723277375102 max memory_allocated 29235.365234375 
[2025-02-17 22:31:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:2.7063326835632324 norm:0.003930504433810711 max memory_allocated 29235.365234375 
[2025-02-17 22:32:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:2.704529285430908 norm:0.004031050950288773 max memory_allocated 29235.365234375 
[2025-02-17 22:33:00 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:2.7093005180358887 norm:0.006267709657549858 max memory_allocated 29235.365234375 
[2025-02-17 22:33:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-17 22:34:02 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:2.9690937995910645 norm:0.016732223331928253 max memory_allocated 29235.552734375 
[2025-02-17 22:34:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:2.9608471393585205 norm:0.010713418945670128 max memory_allocated 29235.552734375 
[2025-02-17 22:35:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:2.9561400413513184 norm:0.007710031699389219 max memory_allocated 29235.552734375 
[2025-02-17 22:36:16 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:2.9540352821350098 norm:0.006013086065649986 max memory_allocated 29235.552734375 
[2025-02-17 22:37:00 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:2.952535629272461 norm:0.004947040230035782 max memory_allocated 29235.552734375 
[2025-02-17 22:37:45 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:2.9517810344696045 norm:0.004263921175152063 max memory_allocated 29235.552734375 
[2025-02-17 22:38:29 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:2.950350761413574 norm:0.0037053204141557217 max memory_allocated 29235.552734375 
[2025-02-17 22:39:14 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:2.949728012084961 norm:0.003244841704145074 max memory_allocated 29235.552734375 
[2025-02-17 22:39:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:2.9489126205444336 norm:0.0029782773926854134 max memory_allocated 29235.552734375 
[2025-02-17 22:40:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:2.947925090789795 norm:0.002882544184103608 max memory_allocated 29235.552734375 
[2025-02-17 22:41:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:2.9476680755615234 norm:0.0024661710485816 max memory_allocated 29235.552734375 
[2025-02-17 22:42:12 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:2.948131799697876 norm:0.00253760302439332 max memory_allocated 29235.552734375 
[2025-02-17 22:42:56 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:2.948054313659668 norm:0.0024504547473043203 max memory_allocated 29235.552734375 
[2025-02-17 22:43:41 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:2.948460340499878 norm:0.002267422154545784 max memory_allocated 29235.552734375 
[2025-02-17 22:44:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:2.9475526809692383 norm:0.0021827947348356247 max memory_allocated 29235.552734375 
[2025-02-17 22:45:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:2.94647479057312 norm:0.002382154343649745 max memory_allocated 29235.552734375 
[2025-02-17 22:45:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:2.947267770767212 norm:0.002260505221784115 max memory_allocated 29235.552734375 
[2025-02-17 22:46:39 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:2.945634603500366 norm:0.002118344185873866 max memory_allocated 29235.552734375 
[2025-02-17 22:47:23 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:2.94751238822937 norm:0.002451119711622596 max memory_allocated 29235.552734375 
[2025-02-17 22:48:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:2.9461991786956787 norm:0.001808736938983202 max memory_allocated 29235.552734375 
[2025-02-17 22:48:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-17 22:49:08 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:3.249293327331543 norm:0.012081578373908997 max memory_allocated 29235.740234375 
[2025-02-17 22:49:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:3.2437970638275146 norm:0.010228173807263374 max memory_allocated 29235.740234375 
[2025-02-17 22:50:37 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:3.240454912185669 norm:0.008777806535363197 max memory_allocated 29235.740234375 
[2025-02-17 22:51:21 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:3.2392117977142334 norm:0.007762509863823652 max memory_allocated 29235.740234375 
[2025-02-17 22:52:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:3.2389678955078125 norm:0.006877682637423277 max memory_allocated 29235.740234375 
[2025-02-17 22:52:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:3.237755298614502 norm:0.005996972322463989 max memory_allocated 29235.740234375 
[2025-02-17 22:53:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:3.237438917160034 norm:0.005245727486908436 max memory_allocated 29235.740234375 
[2025-02-17 22:54:19 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:3.2367162704467773 norm:0.005237188655883074 max memory_allocated 29235.740234375 
[2025-02-17 22:55:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:3.2348060607910156 norm:0.004991583060473204 max memory_allocated 29235.740234375 
[2025-02-17 22:55:48 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:3.236199140548706 norm:0.004779069218784571 max memory_allocated 29235.740234375 
[2025-02-17 22:56:33 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:3.2363362312316895 norm:0.004485889803618193 max memory_allocated 29235.740234375 
[2025-02-17 22:57:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:3.232569694519043 norm:0.004451575223356485 max memory_allocated 29235.740234375 
[2025-02-17 22:58:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:3.234593152999878 norm:0.004341848660260439 max memory_allocated 29235.740234375 
[2025-02-17 22:58:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:3.23659086227417 norm:0.0038309472147375345 max memory_allocated 29235.740234375 
[2025-02-17 22:59:31 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:3.235626697540283 norm:0.0040978300385177135 max memory_allocated 29235.740234375 
[2025-02-17 23:00:15 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:3.236978054046631 norm:0.003767953719943762 max memory_allocated 29235.740234375 
[2025-02-17 23:01:00 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:3.241424798965454 norm:0.009085140191018581 max memory_allocated 29235.740234375 
[2025-02-17 23:01:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:3.2249844074249268 norm:0.0034645115956664085 max memory_allocated 29235.740234375 
[2025-02-17 23:02:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:3.2336044311523438 norm:0.0030996366403996944 max memory_allocated 29235.740234375 
[2025-02-17 23:03:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:3.232804775238037 norm:0.0032654355745762587 max memory_allocated 29235.740234375 
[2025-02-17 23:03:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-17 23:04:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:3.637396812438965 norm:0.027953894808888435 max memory_allocated 29235.927734375 
[2025-02-17 23:04:58 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:3.6263813972473145 norm:0.021049564704298973 max memory_allocated 29235.927734375 
[2025-02-17 23:05:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:3.6234307289123535 norm:0.01834431104362011 max memory_allocated 29235.927734375 
[2025-02-17 23:06:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:3.623255729675293 norm:0.017366649582982063 max memory_allocated 29235.927734375 
[2025-02-17 23:07:11 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:3.6169800758361816 norm:0.01435657124966383 max memory_allocated 29235.927734375 
[2025-02-17 23:07:56 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:3.6337625980377197 norm:0.020022079348564148 max memory_allocated 29235.927734375 
[2025-02-17 23:08:40 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:3.6314938068389893 norm:0.017304087057709694 max memory_allocated 29235.927734375 
[2025-02-17 23:09:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:3.631096363067627 norm:0.01718808338046074 max memory_allocated 29235.927734375 
[2025-02-17 23:10:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:3.6317150592803955 norm:0.016040269285440445 max memory_allocated 29235.927734375 
[2025-02-17 23:10:54 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:3.6326236724853516 norm:0.01991049014031887 max memory_allocated 29235.927734375 
[2025-02-17 23:11:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:3.6135964393615723 norm:0.009112006984651089 max memory_allocated 29235.927734375 
[2025-02-17 23:12:23 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:3.6049373149871826 norm:0.00761997327208519 max memory_allocated 29235.927734375 
[2025-02-17 23:13:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:3.602726697921753 norm:0.007092271465808153 max memory_allocated 29235.927734375 
[2025-02-17 23:13:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:3.6030900478363037 norm:0.006774640642106533 max memory_allocated 29235.927734375 
[2025-02-17 23:14:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:3.6048476696014404 norm:0.006590998265892267 max memory_allocated 29235.927734375 
[2025-02-17 23:15:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:3.607414722442627 norm:0.006102591287344694 max memory_allocated 29235.927734375 
[2025-02-17 23:16:05 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:3.6117537021636963 norm:0.006434142589569092 max memory_allocated 29235.927734375 
[2025-02-17 23:16:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:3.6116504669189453 norm:0.006228134501725435 max memory_allocated 29235.927734375 
[2025-02-17 23:17:34 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:3.6168596744537354 norm:0.010546841658651829 max memory_allocated 29235.927734375 
[2025-02-17 23:18:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:3.615269899368286 norm:0.007928600534796715 max memory_allocated 29235.927734375 
[2025-02-17 23:18:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-17 23:19:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:4.049158096313477 norm:0.06192149966955185 max memory_allocated 29236.115234375 
[2025-02-17 23:20:03 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:4.036186218261719 norm:0.04960136488080025 max memory_allocated 29236.115234375 
[2025-02-17 23:20:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:4.034559726715088 norm:0.041308216750621796 max memory_allocated 29236.115234375 
[2025-02-17 23:21:32 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:4.047197341918945 norm:0.041346095502376556 max memory_allocated 29236.115234375 
[2025-02-17 23:22:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:4.043157577514648 norm:0.03193838894367218 max memory_allocated 29236.115234375 
[2025-02-17 23:23:01 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:4.052757263183594 norm:0.02927558496594429 max memory_allocated 29236.115234375 
[2025-02-17 23:23:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:4.058581829071045 norm:0.026323527097702026 max memory_allocated 29236.115234375 
[2025-02-17 23:24:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:4.062057971954346 norm:0.024345099925994873 max memory_allocated 29236.115234375 
[2025-02-17 23:25:15 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:4.063803672790527 norm:0.029183462262153625 max memory_allocated 29236.115234375 
[2025-02-17 23:25:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:4.0568671226501465 norm:0.025865353643894196 max memory_allocated 29236.115234375 
[2025-02-17 23:26:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:4.056940078735352 norm:0.01989780180156231 max memory_allocated 29236.115234375 
[2025-02-17 23:27:28 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:4.0573248863220215 norm:0.01859907992184162 max memory_allocated 29236.115234375 
[2025-02-17 23:28:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:4.076642990112305 norm:0.0360596664249897 max memory_allocated 29236.115234375 
[2025-02-17 23:28:57 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:4.055922985076904 norm:0.02110336534678936 max memory_allocated 29236.115234375 
[2025-02-17 23:29:42 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:4.048702716827393 norm:0.018706876784563065 max memory_allocated 29236.115234375 
[2025-02-17 23:30:26 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:4.0474019050598145 norm:0.016672177240252495 max memory_allocated 29236.115234375 
[2025-02-17 23:31:11 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:4.077193260192871 norm:0.03980429843068123 max memory_allocated 29236.115234375 
[2025-02-17 23:31:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:4.053643226623535 norm:0.0151806166395545 max memory_allocated 29236.115234375 
[2025-02-17 23:32:40 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:4.047677516937256 norm:0.013715263456106186 max memory_allocated 29236.115234375 
[2025-02-17 23:33:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:4.0479207038879395 norm:0.01413759309798479 max memory_allocated 29236.115234375 
[2025-02-17 23:33:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-17 23:34:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:4.996979236602783 norm:0.11216457933187485 max memory_allocated 29236.302734375 
[2025-02-17 23:35:09 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:4.951606750488281 norm:0.08022815734148026 max memory_allocated 29236.302734375 
[2025-02-17 23:35:53 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:4.9225568771362305 norm:0.06385453790426254 max memory_allocated 29236.302734375 
[2025-02-17 23:36:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:4.896126747131348 norm:0.05072589963674545 max memory_allocated 29236.302734375 
[2025-02-17 23:37:22 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:4.888706207275391 norm:0.04268573597073555 max memory_allocated 29236.302734375 
[2025-02-17 23:38:07 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:4.877209663391113 norm:0.038573045283555984 max memory_allocated 29236.302734375 
[2025-02-17 23:38:51 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:4.864439010620117 norm:0.035088445991277695 max memory_allocated 29236.302734375 
[2025-02-17 23:39:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:4.860646724700928 norm:0.03583648055791855 max memory_allocated 29236.302734375 
[2025-02-17 23:40:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:4.850528717041016 norm:0.02662895992398262 max memory_allocated 29236.302734375 
[2025-02-17 23:41:05 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:4.862544059753418 norm:0.02966861054301262 max memory_allocated 29236.302734375 
[2025-02-17 23:41:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:4.8535542488098145 norm:0.02494571916759014 max memory_allocated 29236.302734375 
[2025-02-17 23:42:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:4.852465629577637 norm:0.026494432240724564 max memory_allocated 29236.302734375 
[2025-02-17 23:43:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:4.845961093902588 norm:0.029151394963264465 max memory_allocated 29236.302734375 
[2025-02-17 23:44:03 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:4.841500282287598 norm:0.046426255255937576 max memory_allocated 29236.302734375 
[2025-02-17 23:44:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:4.81238317489624 norm:0.016982732340693474 max memory_allocated 29236.302734375 
[2025-02-17 23:45:32 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:4.817827224731445 norm:0.0167204812169075 max memory_allocated 29236.302734375 
[2025-02-17 23:46:17 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:4.819483280181885 norm:0.017662012949585915 max memory_allocated 29236.302734375 
[2025-02-17 23:47:01 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:4.81998872756958 norm:0.016538724303245544 max memory_allocated 29236.302734375 
[2025-02-17 23:47:46 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:4.821625232696533 norm:0.020999712869524956 max memory_allocated 29236.302734375 
[2025-02-17 23:48:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:4.815749645233154 norm:0.016071626916527748 max memory_allocated 29236.302734375 
[2025-02-17 23:48:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-17 23:49:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:9.270415306091309 norm:0.14317137002944946 max memory_allocated 29236.490234375 
[2025-02-17 23:50:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:9.074786186218262 norm:0.12721088528633118 max memory_allocated 29236.490234375 
[2025-02-17 23:50:59 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:8.964329719543457 norm:0.11950142681598663 max memory_allocated 29236.490234375 
[2025-02-17 23:51:44 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:8.949440002441406 norm:0.11768579483032227 max memory_allocated 29236.490234375 
[2025-02-17 23:52:28 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:8.927858352661133 norm:0.12054824829101562 max memory_allocated 29236.490234375 
[2025-02-17 23:53:13 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:8.95309066772461 norm:0.13584159314632416 max memory_allocated 29236.490234375 
[2025-02-17 23:53:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:8.942044258117676 norm:0.16875746846199036 max memory_allocated 29236.490234375 
[2025-02-17 23:54:42 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:8.826184272766113 norm:0.29440197348594666 max memory_allocated 29236.490234375 
[2025-02-17 23:55:26 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:8.724806785583496 norm:2.3074162006378174 max memory_allocated 29236.490234375 
[2025-02-17 23:56:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:8.739590644836426 norm:22.081806182861328 max memory_allocated 29236.490234375 
[2025-02-17 23:56:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:8.572254180908203 norm:24.693552017211914 max memory_allocated 29236.490234375 
[2025-02-17 23:57:40 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:8.487288475036621 norm:12.92042350769043 max memory_allocated 29236.490234375 
[2025-02-17 23:58:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:8.529420852661133 norm:29.42901039123535 max memory_allocated 29236.490234375 
[2025-02-17 23:59:09 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:8.577184677124023 norm:58.48142623901367 max memory_allocated 29236.490234375 
[2025-02-17 23:59:53 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:8.511702537536621 norm:58.719573974609375 max memory_allocated 29236.490234375 
[2025-02-18 00:00:38 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:8.481698989868164 norm:47.2500114440918 max memory_allocated 29236.490234375 
[2025-02-18 00:01:22 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:8.460494995117188 norm:52.48345947265625 max memory_allocated 29236.490234375 
[2025-02-18 00:02:07 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:8.454119682312012 norm:62.36151885986328 max memory_allocated 29236.490234375 
[2025-02-18 00:02:51 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:8.431451797485352 norm:49.91075134277344 max memory_allocated 29236.490234375 
[2025-02-18 00:03:36 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:8.423628807067871 norm:55.61982727050781 max memory_allocated 29236.490234375 
[2025-02-18 00:03:48 root] (main_calibration.py 365): INFO 36232.018696546555
