[2025-02-18 04:40:17 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration/Llama-2-13b-hf-w4a4', save_dir='./log-calibration/quant/Llama-2-13b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-18 04:40:20 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 04:40:21 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-18 04:40:21 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 04:40:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 04:41:27 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.07881267368793488 norm:0.020949382334947586 max memory_allocated 29229.177734375 
[2025-02-18 04:42:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.060357533395290375 norm:0.012186387553811073 max memory_allocated 29229.177734375 
[2025-02-18 04:43:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.05895356088876724 norm:0.05691366270184517 max memory_allocated 29229.177734375 
[2025-02-18 04:43:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.06471293419599533 norm:0.09777435660362244 max memory_allocated 29229.177734375 
[2025-02-18 04:44:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.06193966045975685 norm:0.0709129348397255 max memory_allocated 29229.177734375 
[2025-02-18 04:45:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.050896838307380676 norm:0.018670251592993736 max memory_allocated 29229.177734375 
[2025-02-18 04:46:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.05178691819310188 norm:0.034685567021369934 max memory_allocated 29229.177734375 
[2025-02-18 04:47:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.05347684770822525 norm:0.0978676825761795 max memory_allocated 29229.177734375 
[2025-02-18 04:47:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0450109988451004 norm:0.024406857788562775 max memory_allocated 29229.177734375 
[2025-02-18 04:48:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.05082596093416214 norm:0.0559951588511467 max memory_allocated 29229.177734375 
[2025-02-18 04:49:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.053707633167505264 norm:0.04657493531703949 max memory_allocated 29229.177734375 
[2025-02-18 04:50:19 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.05393606051802635 norm:0.05089198052883148 max memory_allocated 29229.177734375 
[2025-02-18 04:51:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.05335717275738716 norm:0.07567954808473587 max memory_allocated 29229.177734375 
[2025-02-18 04:51:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.060599420219659805 norm:0.07241524010896683 max memory_allocated 29229.177734375 
[2025-02-18 04:52:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.05265786498785019 norm:0.032686375081539154 max memory_allocated 29229.177734375 
[2025-02-18 04:53:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.05127270519733429 norm:0.05137810856103897 max memory_allocated 29229.177734375 
[2025-02-18 04:54:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.046485841274261475 norm:0.029704168438911438 max memory_allocated 29229.177734375 
[2025-02-18 04:55:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.051109448075294495 norm:0.04711943119764328 max memory_allocated 29229.177734375 
[2025-02-18 04:55:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.04613644257187843 norm:0.012366286478936672 max memory_allocated 29229.177734375 
[2025-02-18 04:56:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.04837900772690773 norm:0.019324125722050667 max memory_allocated 29229.177734375 
[2025-02-18 04:57:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 04:58:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.19834865629673004 norm:0.019456772133708 max memory_allocated 29229.365234375 
[2025-02-18 04:58:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.169278085231781 norm:0.008025731891393661 max memory_allocated 29229.365234375 
[2025-02-18 04:59:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.16202637553215027 norm:0.007484317757189274 max memory_allocated 29229.365234375 
[2025-02-18 05:00:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.15875163674354553 norm:0.00446453969925642 max memory_allocated 29229.365234375 
[2025-02-18 05:01:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.15691514313220978 norm:0.003839752869680524 max memory_allocated 29229.365234375 
[2025-02-18 05:02:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.15772387385368347 norm:0.003666921751573682 max memory_allocated 29229.365234375 
[2025-02-18 05:02:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.15831322968006134 norm:0.0035956548526883125 max memory_allocated 29229.365234375 
[2025-02-18 05:03:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.15767036378383636 norm:0.0035045770928263664 max memory_allocated 29229.365234375 
[2025-02-18 05:04:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.1568651795387268 norm:0.003439656225964427 max memory_allocated 29229.365234375 
[2025-02-18 05:05:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.15551356971263885 norm:0.003502452280372381 max memory_allocated 29229.365234375 
[2025-02-18 05:06:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.15494488179683685 norm:0.0033132496755570173 max memory_allocated 29229.365234375 
[2025-02-18 05:07:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.15444627404212952 norm:0.0033646656665951014 max memory_allocated 29229.365234375 
[2025-02-18 05:07:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.15263451635837555 norm:0.003323117271065712 max memory_allocated 29229.365234375 
[2025-02-18 05:08:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.1522947996854782 norm:0.0032505509443581104 max memory_allocated 29229.365234375 
[2025-02-18 05:09:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.15273089706897736 norm:0.0032196498941630125 max memory_allocated 29229.365234375 
[2025-02-18 05:10:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.15300190448760986 norm:0.0032732049003243446 max memory_allocated 29229.365234375 
[2025-02-18 05:11:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.15286852419376373 norm:0.003224650165066123 max memory_allocated 29229.365234375 
[2025-02-18 05:11:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.15245327353477478 norm:0.0031791646033525467 max memory_allocated 29229.365234375 
[2025-02-18 05:12:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.1520693004131317 norm:0.0031531350687146187 max memory_allocated 29229.365234375 
[2025-02-18 05:13:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.1521352082490921 norm:0.0031614243052899837 max memory_allocated 29229.365234375 
[2025-02-18 05:13:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 05:14:37 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.2064957320690155 norm:0.012527347542345524 max memory_allocated 29229.552734375 
[2025-02-18 05:15:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.18871331214904785 norm:0.006166848819702864 max memory_allocated 29229.552734375 
[2025-02-18 05:16:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.18221083283424377 norm:0.0033553570974618196 max memory_allocated 29229.552734375 
[2025-02-18 05:17:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.1791515052318573 norm:0.0024932590313255787 max memory_allocated 29229.552734375 
[2025-02-18 05:17:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.17786026000976562 norm:0.002108794404193759 max memory_allocated 29229.552734375 
[2025-02-18 05:18:41 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.176022008061409 norm:0.0018738998332992196 max memory_allocated 29229.552734375 
[2025-02-18 05:19:30 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.17568309605121613 norm:0.0017778108594939113 max memory_allocated 29229.552734375 
[2025-02-18 05:20:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.17527806758880615 norm:0.0017053581541404128 max memory_allocated 29229.552734375 
[2025-02-18 05:21:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.17524689435958862 norm:0.0016853914130479097 max memory_allocated 29229.552734375 
[2025-02-18 05:21:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.1751815676689148 norm:0.001665838179178536 max memory_allocated 29229.552734375 
[2025-02-18 05:22:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.17520923912525177 norm:0.0016664552967995405 max memory_allocated 29229.552734375 
[2025-02-18 05:23:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.1752522736787796 norm:0.0016569060971960425 max memory_allocated 29229.552734375 
[2025-02-18 05:24:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.1754414588212967 norm:0.0016578417271375656 max memory_allocated 29229.552734375 
[2025-02-18 05:25:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.17547523975372314 norm:0.0016692624194547534 max memory_allocated 29229.552734375 
[2025-02-18 05:26:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.17554771900177002 norm:0.0016760205617174506 max memory_allocated 29229.552734375 
[2025-02-18 05:26:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.17532411217689514 norm:0.001650749472901225 max memory_allocated 29229.552734375 
[2025-02-18 05:27:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.17533782124519348 norm:0.0016549755819141865 max memory_allocated 29229.552734375 
[2025-02-18 05:28:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.1753520667552948 norm:0.001655331696383655 max memory_allocated 29229.552734375 
[2025-02-18 05:29:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.17536386847496033 norm:0.0016717904945835471 max memory_allocated 29229.552734375 
[2025-02-18 05:30:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.17531481385231018 norm:0.0016575143672525883 max memory_allocated 29229.552734375 
[2025-02-18 05:30:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 05:31:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.41829270124435425 norm:0.12632893025875092 max memory_allocated 29229.740234375 
[2025-02-18 05:32:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.3273768723011017 norm:0.04317237436771393 max memory_allocated 29229.740234375 
[2025-02-18 05:32:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.30518168210983276 norm:0.027069661766290665 max memory_allocated 29229.740234375 
[2025-02-18 05:33:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.28562068939208984 norm:0.01926463283598423 max memory_allocated 29229.740234375 
[2025-02-18 05:34:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.28127720952033997 norm:0.01579236425459385 max memory_allocated 29229.740234375 
[2025-02-18 05:35:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.27776795625686646 norm:0.015310865826904774 max memory_allocated 29229.740234375 
[2025-02-18 05:36:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.2748714089393616 norm:0.014039806090295315 max memory_allocated 29229.740234375 
[2025-02-18 05:36:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.27418434619903564 norm:0.013639525510370731 max memory_allocated 29229.740234375 
[2025-02-18 05:37:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.2766576111316681 norm:0.0123148113489151 max memory_allocated 29229.740234375 
[2025-02-18 05:38:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.276213139295578 norm:0.01230858638882637 max memory_allocated 29229.740234375 
[2025-02-18 05:39:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.2733774185180664 norm:0.011451967060565948 max memory_allocated 29229.740234375 
[2025-02-18 05:40:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.27272510528564453 norm:0.011272585019469261 max memory_allocated 29229.740234375 
[2025-02-18 05:40:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.2739540636539459 norm:0.011896634474396706 max memory_allocated 29229.740234375 
[2025-02-18 05:41:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.27376848459243774 norm:0.011961380951106548 max memory_allocated 29229.740234375 
[2025-02-18 05:42:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.27445104718208313 norm:0.012806345708668232 max memory_allocated 29229.740234375 
[2025-02-18 05:43:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.27294379472732544 norm:0.011416182853281498 max memory_allocated 29229.740234375 
[2025-02-18 05:44:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.27376651763916016 norm:0.011398048140108585 max memory_allocated 29229.740234375 
[2025-02-18 05:45:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.2760254740715027 norm:0.011654546484351158 max memory_allocated 29229.740234375 
[2025-02-18 05:45:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.27458131313323975 norm:0.011458160355687141 max memory_allocated 29229.740234375 
[2025-02-18 05:46:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.276616632938385 norm:0.011130411177873611 max memory_allocated 29229.740234375 
[2025-02-18 05:46:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 05:47:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.3356429636478424 norm:0.017591331154108047 max memory_allocated 29229.927734375 
[2025-02-18 05:48:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.3188926577568054 norm:0.008865839801728725 max memory_allocated 29229.927734375 
[2025-02-18 05:49:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.31140419840812683 norm:0.005747216288000345 max memory_allocated 29229.927734375 
[2025-02-18 05:50:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.306788831949234 norm:0.004097673110663891 max memory_allocated 29229.927734375 
[2025-02-18 05:51:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.30481699109077454 norm:0.003331555053591728 max memory_allocated 29229.927734375 
[2025-02-18 05:51:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.30335384607315063 norm:0.0027525369077920914 max memory_allocated 29229.927734375 
[2025-02-18 05:52:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.3024631440639496 norm:0.002410639077425003 max memory_allocated 29229.927734375 
[2025-02-18 05:53:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.30161166191101074 norm:0.002218075795099139 max memory_allocated 29229.927734375 
[2025-02-18 05:54:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.3013024926185608 norm:0.002067137509584427 max memory_allocated 29229.927734375 
[2025-02-18 05:55:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.30095750093460083 norm:0.0019137341296300292 max memory_allocated 29229.927734375 
[2025-02-18 05:55:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.3009420931339264 norm:0.001797338598407805 max memory_allocated 29229.927734375 
[2025-02-18 05:56:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.30061179399490356 norm:0.0017656635027378798 max memory_allocated 29229.927734375 
[2025-02-18 05:57:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.30040642619132996 norm:0.0017214007675647736 max memory_allocated 29229.927734375 
[2025-02-18 05:58:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.30039164423942566 norm:0.001761136227287352 max memory_allocated 29229.927734375 
[2025-02-18 05:59:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.3002726137638092 norm:0.0017078128876164556 max memory_allocated 29229.927734375 
[2025-02-18 06:00:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.3002683222293854 norm:0.0016876942245289683 max memory_allocated 29229.927734375 
[2025-02-18 06:00:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.30051785707473755 norm:0.0017124980222433805 max memory_allocated 29229.927734375 
[2025-02-18 06:01:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.30037829279899597 norm:0.0017016036435961723 max memory_allocated 29229.927734375 
[2025-02-18 06:02:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.3002021908760071 norm:0.0017245184862986207 max memory_allocated 29229.927734375 
[2025-02-18 06:03:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.2999555468559265 norm:0.0017299320315942168 max memory_allocated 29229.927734375 
[2025-02-18 06:03:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 06:04:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.38239866495132446 norm:0.017768755555152893 max memory_allocated 29230.115234375 
[2025-02-18 06:05:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.3683928847312927 norm:0.009971152991056442 max memory_allocated 29230.115234375 
[2025-02-18 06:06:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.3619908392429352 norm:0.007361996453255415 max memory_allocated 29230.115234375 
[2025-02-18 06:06:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.35816720128059387 norm:0.00568563723936677 max memory_allocated 29230.115234375 
[2025-02-18 06:07:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.35658687353134155 norm:0.004927098751068115 max memory_allocated 29230.115234375 
[2025-02-18 06:08:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.3548634946346283 norm:0.004519946873188019 max memory_allocated 29230.115234375 
[2025-02-18 06:09:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.35305193066596985 norm:0.0037948251701891422 max memory_allocated 29230.115234375 
[2025-02-18 06:10:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.352095365524292 norm:0.0035529034212231636 max memory_allocated 29230.115234375 
[2025-02-18 06:10:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.35163620114326477 norm:0.003344241762533784 max memory_allocated 29230.115234375 
[2025-02-18 06:11:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.35104185342788696 norm:0.002936396049335599 max memory_allocated 29230.115234375 
[2025-02-18 06:12:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.3503088653087616 norm:0.003099564928561449 max memory_allocated 29230.115234375 
[2025-02-18 06:13:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.3501945734024048 norm:0.0028252904303371906 max memory_allocated 29230.115234375 
[2025-02-18 06:14:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.35007718205451965 norm:0.0029904155526310205 max memory_allocated 29230.115234375 
[2025-02-18 06:15:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.35006779432296753 norm:0.002945592626929283 max memory_allocated 29230.115234375 
[2025-02-18 06:15:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.34978416562080383 norm:0.0027077803388237953 max memory_allocated 29230.115234375 
[2025-02-18 06:16:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.34954333305358887 norm:0.0026780893094837666 max memory_allocated 29230.115234375 
[2025-02-18 06:17:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.34956255555152893 norm:0.0025963543448597193 max memory_allocated 29230.115234375 
[2025-02-18 06:18:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.34938254952430725 norm:0.0025879049208015203 max memory_allocated 29230.115234375 
[2025-02-18 06:19:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.3488692343235016 norm:0.0022787251509726048 max memory_allocated 29230.115234375 
[2025-02-18 06:19:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.34897634387016296 norm:0.0024201306514441967 max memory_allocated 29230.115234375 
[2025-02-18 06:20:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 06:21:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.44596603512763977 norm:0.027982566505670547 max memory_allocated 29230.302734375 
[2025-02-18 06:21:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.42654359340667725 norm:0.016442378982901573 max memory_allocated 29230.302734375 
[2025-02-18 06:22:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.41777440905570984 norm:0.01109167467802763 max memory_allocated 29230.302734375 
[2025-02-18 06:23:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.4126601219177246 norm:0.008140964433550835 max memory_allocated 29230.302734375 
[2025-02-18 06:24:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.4088296592235565 norm:0.006023240275681019 max memory_allocated 29230.302734375 
[2025-02-18 06:25:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.40617576241493225 norm:0.004836032632738352 max memory_allocated 29230.302734375 
[2025-02-18 06:25:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.4048703908920288 norm:0.0038822689093649387 max memory_allocated 29230.302734375 
[2025-02-18 06:26:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.4042360484600067 norm:0.003475223435088992 max memory_allocated 29230.302734375 
[2025-02-18 06:27:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.40359431505203247 norm:0.003111418802291155 max memory_allocated 29230.302734375 
[2025-02-18 06:28:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.40274739265441895 norm:0.00283021479845047 max memory_allocated 29230.302734375 
[2025-02-18 06:29:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.4025929868221283 norm:0.0027724106330424547 max memory_allocated 29230.302734375 
[2025-02-18 06:30:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.40258651971817017 norm:0.0025482294149696827 max memory_allocated 29230.302734375 
[2025-02-18 06:30:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.4021872282028198 norm:0.0024528936482965946 max memory_allocated 29230.302734375 
[2025-02-18 06:31:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.4021553695201874 norm:0.002356798853725195 max memory_allocated 29230.302734375 
[2025-02-18 06:32:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.40164580941200256 norm:0.0022912751883268356 max memory_allocated 29230.302734375 
[2025-02-18 06:33:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.4012426733970642 norm:0.002185238292440772 max memory_allocated 29230.302734375 
[2025-02-18 06:34:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.40205034613609314 norm:0.002327479887753725 max memory_allocated 29230.302734375 
[2025-02-18 06:34:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.4008484482765198 norm:0.002127734012901783 max memory_allocated 29230.302734375 
[2025-02-18 06:35:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.40140676498413086 norm:0.002265081275254488 max memory_allocated 29230.302734375 
[2025-02-18 06:36:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.400983989238739 norm:0.0021601375192403793 max memory_allocated 29230.302734375 
[2025-02-18 06:36:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 06:37:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.5165658593177795 norm:0.012073573656380177 max memory_allocated 29230.490234375 
[2025-02-18 06:38:31 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.48709025979042053 norm:0.005918518640100956 max memory_allocated 29230.490234375 
[2025-02-18 06:39:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.47704237699508667 norm:0.0041269464418292046 max memory_allocated 29230.490234375 
[2025-02-18 06:40:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.4728075861930847 norm:0.003294560359790921 max memory_allocated 29230.490234375 
[2025-02-18 06:40:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.4706912934780121 norm:0.00285890931263566 max memory_allocated 29230.490234375 
[2025-02-18 06:41:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.46968260407447815 norm:0.0025896162260323763 max memory_allocated 29230.490234375 
[2025-02-18 06:42:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.46773916482925415 norm:0.002462590578943491 max memory_allocated 29230.490234375 
[2025-02-18 06:43:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.46663782000541687 norm:0.002232012338936329 max memory_allocated 29230.490234375 
[2025-02-18 06:44:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.4664309322834015 norm:0.0022092419676482677 max memory_allocated 29230.490234375 
[2025-02-18 06:45:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.4658201336860657 norm:0.002135897520929575 max memory_allocated 29230.490234375 
[2025-02-18 06:45:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.4655854403972626 norm:0.0022468960378319025 max memory_allocated 29230.490234375 
[2025-02-18 06:46:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.46485546231269836 norm:0.0021680046338588 max memory_allocated 29230.490234375 
[2025-02-18 06:47:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.46474242210388184 norm:0.002168992767110467 max memory_allocated 29230.490234375 
[2025-02-18 06:48:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.4642590284347534 norm:0.002169670071452856 max memory_allocated 29230.490234375 
[2025-02-18 06:49:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.46410247683525085 norm:0.0021417737007141113 max memory_allocated 29230.490234375 
[2025-02-18 06:49:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.46405887603759766 norm:0.0021313095930963755 max memory_allocated 29230.490234375 
[2025-02-18 06:50:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.46398791670799255 norm:0.0021484268363565207 max memory_allocated 29230.490234375 
[2025-02-18 06:51:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.46399277448654175 norm:0.0021486193872988224 max memory_allocated 29230.490234375 
[2025-02-18 06:52:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.46387314796447754 norm:0.0021729040890932083 max memory_allocated 29230.490234375 
[2025-02-18 06:53:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.46407240629196167 norm:0.002230849815532565 max memory_allocated 29230.490234375 
[2025-02-18 06:53:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 06:54:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.5535645484924316 norm:0.03486894816160202 max memory_allocated 29230.677734375 
[2025-02-18 06:55:09 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.5319253206253052 norm:0.021456709131598473 max memory_allocated 29230.677734375 
[2025-02-18 06:55:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.5247231721878052 norm:0.01503127720206976 max memory_allocated 29230.677734375 
[2025-02-18 06:56:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.5174771547317505 norm:0.011242784559726715 max memory_allocated 29230.677734375 
[2025-02-18 06:57:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.5127744674682617 norm:0.009059746749699116 max memory_allocated 29230.677734375 
[2025-02-18 06:58:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.5090640783309937 norm:0.007161751855164766 max memory_allocated 29230.677734375 
[2025-02-18 06:59:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.5077351927757263 norm:0.005744569469243288 max memory_allocated 29230.677734375 
[2025-02-18 07:00:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.505800724029541 norm:0.004915524274110794 max memory_allocated 29230.677734375 
[2025-02-18 07:00:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.502739667892456 norm:0.0040872166864573956 max memory_allocated 29230.677734375 
[2025-02-18 07:01:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.5009819269180298 norm:0.003443493042141199 max memory_allocated 29230.677734375 
[2025-02-18 07:02:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.5003226399421692 norm:0.0031094320584088564 max memory_allocated 29230.677734375 
[2025-02-18 07:03:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.500822901725769 norm:0.0028643098194152117 max memory_allocated 29230.677734375 
[2025-02-18 07:04:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.5001848340034485 norm:0.0027244938537478447 max memory_allocated 29230.677734375 
[2025-02-18 07:04:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.4985067844390869 norm:0.002403798047453165 max memory_allocated 29230.677734375 
[2025-02-18 07:05:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.4986599385738373 norm:0.0024597779847681522 max memory_allocated 29230.677734375 
[2025-02-18 07:06:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.49863728880882263 norm:0.002244917443022132 max memory_allocated 29230.677734375 
[2025-02-18 07:07:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.4974558651447296 norm:0.0022006880026310682 max memory_allocated 29230.677734375 
[2025-02-18 07:08:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.49711889028549194 norm:0.002148748841136694 max memory_allocated 29230.677734375 
[2025-02-18 07:09:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.4973720908164978 norm:0.0021781595423817635 max memory_allocated 29230.677734375 
[2025-02-18 07:09:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.49717211723327637 norm:0.0021336167119443417 max memory_allocated 29230.677734375 
[2025-02-18 07:10:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 07:10:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.6447458267211914 norm:0.027237365022301674 max memory_allocated 29230.865234375 
[2025-02-18 07:11:47 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.6060880422592163 norm:0.013538993895053864 max memory_allocated 29230.865234375 
[2025-02-18 07:12:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.5912801027297974 norm:0.00915801152586937 max memory_allocated 29230.865234375 
[2025-02-18 07:13:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.581782341003418 norm:0.0066645266488194466 max memory_allocated 29230.865234375 
[2025-02-18 07:14:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.5773823261260986 norm:0.005564503371715546 max memory_allocated 29230.865234375 
[2025-02-18 07:15:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.5739353895187378 norm:0.004790036007761955 max memory_allocated 29230.865234375 
[2025-02-18 07:15:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.5707871913909912 norm:0.0037843724712729454 max memory_allocated 29230.865234375 
[2025-02-18 07:16:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.5695459246635437 norm:0.0033799863886088133 max memory_allocated 29230.865234375 
[2025-02-18 07:17:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.5697377324104309 norm:0.0033556383568793535 max memory_allocated 29230.865234375 
[2025-02-18 07:18:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.5687507390975952 norm:0.003189723938703537 max memory_allocated 29230.865234375 
[2025-02-18 07:19:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.5674394369125366 norm:0.003035305766388774 max memory_allocated 29230.865234375 
[2025-02-18 07:19:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.5665534138679504 norm:0.003043131437152624 max memory_allocated 29230.865234375 
[2025-02-18 07:20:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.5674713850021362 norm:0.002963003935292363 max memory_allocated 29230.865234375 
[2025-02-18 07:21:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.5667033791542053 norm:0.002916589379310608 max memory_allocated 29230.865234375 
[2025-02-18 07:22:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.5669748783111572 norm:0.002903458895161748 max memory_allocated 29230.865234375 
[2025-02-18 07:23:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.5677120089530945 norm:0.003093155799433589 max memory_allocated 29230.865234375 
[2025-02-18 07:24:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.5662438869476318 norm:0.0030114443507045507 max memory_allocated 29230.865234375 
[2025-02-18 07:24:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.5666028261184692 norm:0.002837671199813485 max memory_allocated 29230.865234375 
[2025-02-18 07:25:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.5657500624656677 norm:0.002821641508489847 max memory_allocated 29230.865234375 
[2025-02-18 07:26:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.5663198232650757 norm:0.002822707872837782 max memory_allocated 29230.865234375 
[2025-02-18 07:26:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 07:27:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.6561943888664246 norm:0.022069212049245834 max memory_allocated 29231.052734375 
[2025-02-18 07:28:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.6340789794921875 norm:0.012217812240123749 max memory_allocated 29231.052734375 
[2025-02-18 07:29:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.6213963031768799 norm:0.008299395442008972 max memory_allocated 29231.052734375 
[2025-02-18 07:30:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.6161487102508545 norm:0.006386227905750275 max memory_allocated 29231.052734375 
[2025-02-18 07:30:51 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.6132478713989258 norm:0.005021441727876663 max memory_allocated 29231.052734375 
[2025-02-18 07:31:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.6123974323272705 norm:0.004360302351415157 max memory_allocated 29231.052734375 
[2025-02-18 07:32:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.611535906791687 norm:0.0038312533870339394 max memory_allocated 29231.052734375 
[2025-02-18 07:33:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.6106732487678528 norm:0.0034193398896604776 max memory_allocated 29231.052734375 
[2025-02-18 07:34:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.610273540019989 norm:0.0031641290988773108 max memory_allocated 29231.052734375 
[2025-02-18 07:34:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.6080455780029297 norm:0.0028378923889249563 max memory_allocated 29231.052734375 
[2025-02-18 07:35:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.6061756014823914 norm:0.0025463635101914406 max memory_allocated 29231.052734375 
[2025-02-18 07:36:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.6048871278762817 norm:0.002461303723976016 max memory_allocated 29231.052734375 
[2025-02-18 07:37:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.6038151383399963 norm:0.002401607111096382 max memory_allocated 29231.052734375 
[2025-02-18 07:38:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.6039966344833374 norm:0.002358511323109269 max memory_allocated 29231.052734375 
[2025-02-18 07:39:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.6038280129432678 norm:0.0023465678095817566 max memory_allocated 29231.052734375 
[2025-02-18 07:39:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.6041422486305237 norm:0.002348222304135561 max memory_allocated 29231.052734375 
[2025-02-18 07:40:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.6035977005958557 norm:0.0023482399992644787 max memory_allocated 29231.052734375 
[2025-02-18 07:41:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.6027222275733948 norm:0.0023760716430842876 max memory_allocated 29231.052734375 
[2025-02-18 07:42:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.6026108264923096 norm:0.00227973866276443 max memory_allocated 29231.052734375 
[2025-02-18 07:43:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.6035640239715576 norm:0.0023469473235309124 max memory_allocated 29231.052734375 
[2025-02-18 07:43:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 07:44:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.6809465885162354 norm:0.021147046238183975 max memory_allocated 29231.240234375 
[2025-02-18 07:45:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.6604965925216675 norm:0.010575886815786362 max memory_allocated 29231.240234375 
[2025-02-18 07:45:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.6525246500968933 norm:0.007564250845462084 max memory_allocated 29231.240234375 
[2025-02-18 07:46:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.6492068767547607 norm:0.005751245189458132 max memory_allocated 29231.240234375 
[2025-02-18 07:47:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.6452447772026062 norm:0.004456565249711275 max memory_allocated 29231.240234375 
[2025-02-18 07:48:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.6453655362129211 norm:0.0040740519762039185 max memory_allocated 29231.240234375 
[2025-02-18 07:49:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.6445726156234741 norm:0.0034928834065794945 max memory_allocated 29231.240234375 
[2025-02-18 07:49:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.6447160840034485 norm:0.0031558407936245203 max memory_allocated 29231.240234375 
[2025-02-18 07:50:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.6441329717636108 norm:0.002819815883412957 max memory_allocated 29231.240234375 
[2025-02-18 07:51:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.6443359851837158 norm:0.0026220879517495632 max memory_allocated 29231.240234375 
[2025-02-18 07:52:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.6436760425567627 norm:0.002524693263694644 max memory_allocated 29231.240234375 
[2025-02-18 07:53:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.6425403356552124 norm:0.002445794176310301 max memory_allocated 29231.240234375 
[2025-02-18 07:54:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.6422309279441833 norm:0.0023495072964578867 max memory_allocated 29231.240234375 
[2025-02-18 07:54:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.6422753930091858 norm:0.0023022829554975033 max memory_allocated 29231.240234375 
[2025-02-18 07:55:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.6413149833679199 norm:0.002204533666372299 max memory_allocated 29231.240234375 
[2025-02-18 07:56:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.6407498717308044 norm:0.0021977024152874947 max memory_allocated 29231.240234375 
[2025-02-18 07:57:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.6405019164085388 norm:0.0021245130337774754 max memory_allocated 29231.240234375 
[2025-02-18 07:58:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.6406305432319641 norm:0.0022009634412825108 max memory_allocated 29231.240234375 
[2025-02-18 07:58:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.6398231387138367 norm:0.002078323857858777 max memory_allocated 29231.240234375 
[2025-02-18 07:59:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.639549195766449 norm:0.002061753999441862 max memory_allocated 29231.240234375 
[2025-02-18 07:59:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 08:00:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.7150260210037231 norm:0.017093129456043243 max memory_allocated 29231.427734375 
[2025-02-18 08:01:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.6888361573219299 norm:0.008599241264164448 max memory_allocated 29231.427734375 
[2025-02-18 08:02:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.6797727942466736 norm:0.005915991496294737 max memory_allocated 29231.427734375 
[2025-02-18 08:03:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.6763314604759216 norm:0.004717440810054541 max memory_allocated 29231.427734375 
[2025-02-18 08:04:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.6722210645675659 norm:0.0037727192975580692 max memory_allocated 29231.427734375 
[2025-02-18 08:04:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.6671922206878662 norm:0.003020373173058033 max memory_allocated 29231.427734375 
[2025-02-18 08:05:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.6663334965705872 norm:0.00268903118558228 max memory_allocated 29231.427734375 
[2025-02-18 08:06:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.6653920412063599 norm:0.002418499207124114 max memory_allocated 29231.427734375 
[2025-02-18 08:07:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.6642690896987915 norm:0.0022323557641357183 max memory_allocated 29231.427734375 
[2025-02-18 08:08:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.664626955986023 norm:0.002215519780293107 max memory_allocated 29231.427734375 
[2025-02-18 08:09:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.6637293100357056 norm:0.0021430165506899357 max memory_allocated 29231.427734375 
[2025-02-18 08:09:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.6622251868247986 norm:0.0021122898906469345 max memory_allocated 29231.427734375 
[2025-02-18 08:10:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.6620236039161682 norm:0.0020690876990556717 max memory_allocated 29231.427734375 
[2025-02-18 08:11:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.6611931920051575 norm:0.0019314781529828906 max memory_allocated 29231.427734375 
[2025-02-18 08:12:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.6610530018806458 norm:0.001879582880064845 max memory_allocated 29231.427734375 
[2025-02-18 08:13:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.6609412431716919 norm:0.0018486116314306855 max memory_allocated 29231.427734375 
[2025-02-18 08:13:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.6609218716621399 norm:0.001883080112747848 max memory_allocated 29231.427734375 
[2025-02-18 08:14:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.6605602502822876 norm:0.001806312007829547 max memory_allocated 29231.427734375 
[2025-02-18 08:15:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.6602306962013245 norm:0.0017648902721703053 max memory_allocated 29231.427734375 
[2025-02-18 08:16:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.6595186591148376 norm:0.0016886809607967734 max memory_allocated 29231.427734375 
[2025-02-18 08:16:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 08:17:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.7486751675605774 norm:0.02121645025908947 max memory_allocated 29231.615234375 
[2025-02-18 08:18:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.7240043878555298 norm:0.011629894375801086 max memory_allocated 29231.615234375 
[2025-02-18 08:19:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.7153152227401733 norm:0.007865546271204948 max memory_allocated 29231.615234375 
[2025-02-18 08:19:58 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.710676908493042 norm:0.006266254931688309 max memory_allocated 29231.615234375 
[2025-02-18 08:20:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.7102592587471008 norm:0.005444716662168503 max memory_allocated 29231.615234375 
[2025-02-18 08:21:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.7078254222869873 norm:0.004667164757847786 max memory_allocated 29231.615234375 
[2025-02-18 08:22:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.7053033709526062 norm:0.004055443685501814 max memory_allocated 29231.615234375 
[2025-02-18 08:23:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.7044827938079834 norm:0.003726236056536436 max memory_allocated 29231.615234375 
[2025-02-18 08:24:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.704571545124054 norm:0.003526462707668543 max memory_allocated 29231.615234375 
[2025-02-18 08:24:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.703472912311554 norm:0.0033035469241440296 max memory_allocated 29231.615234375 
[2025-02-18 08:25:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.7029582262039185 norm:0.0029890493024140596 max memory_allocated 29231.615234375 
[2025-02-18 08:26:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.7026505470275879 norm:0.002738672774285078 max memory_allocated 29231.615234375 
[2025-02-18 08:27:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.7008069157600403 norm:0.0026264172047376633 max memory_allocated 29231.615234375 
[2025-02-18 08:28:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.699937105178833 norm:0.0024913358502089977 max memory_allocated 29231.615234375 
[2025-02-18 08:28:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.6992023587226868 norm:0.002511582337319851 max memory_allocated 29231.615234375 
[2025-02-18 08:29:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.6993436813354492 norm:0.002833459060639143 max memory_allocated 29231.615234375 
[2025-02-18 08:30:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.6982910633087158 norm:0.0031003705225884914 max memory_allocated 29231.615234375 
[2025-02-18 08:31:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.6972071528434753 norm:0.0031560687348246574 max memory_allocated 29231.615234375 
[2025-02-18 08:32:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.6963181495666504 norm:0.003032331820577383 max memory_allocated 29231.615234375 
[2025-02-18 08:33:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.6970597505569458 norm:0.0033548669889569283 max memory_allocated 29231.615234375 
[2025-02-18 08:33:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 08:34:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.7691177725791931 norm:0.02053830772638321 max memory_allocated 29231.802734375 
[2025-02-18 08:34:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.7448837757110596 norm:0.012133412063121796 max memory_allocated 29231.802734375 
[2025-02-18 08:35:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.7352436780929565 norm:0.008566636592149734 max memory_allocated 29231.802734375 
[2025-02-18 08:36:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.7298682928085327 norm:0.006777899339795113 max memory_allocated 29231.802734375 
[2025-02-18 08:37:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.7264794111251831 norm:0.005646831821650267 max memory_allocated 29231.802734375 
[2025-02-18 08:38:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.7240389585494995 norm:0.004902789369225502 max memory_allocated 29231.802734375 
[2025-02-18 08:39:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.7238017320632935 norm:0.004330521449446678 max memory_allocated 29231.802734375 
[2025-02-18 08:39:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.7195398211479187 norm:0.003642295952886343 max memory_allocated 29231.802734375 
[2025-02-18 08:40:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.7184770107269287 norm:0.0033205829095095396 max memory_allocated 29231.802734375 
[2025-02-18 08:41:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.7193934321403503 norm:0.0031239413656294346 max memory_allocated 29231.802734375 
[2025-02-18 08:42:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.71970134973526 norm:0.0029740380123257637 max memory_allocated 29231.802734375 
[2025-02-18 08:43:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.7181981801986694 norm:0.002782401628792286 max memory_allocated 29231.802734375 
[2025-02-18 08:43:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.7184286117553711 norm:0.0026894863694906235 max memory_allocated 29231.802734375 
[2025-02-18 08:44:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.7182028293609619 norm:0.002501494251191616 max memory_allocated 29231.802734375 
[2025-02-18 08:45:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.7178882360458374 norm:0.00235280254855752 max memory_allocated 29231.802734375 
[2025-02-18 08:46:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.7180758118629456 norm:0.0023879711516201496 max memory_allocated 29231.802734375 
[2025-02-18 08:47:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.7179378271102905 norm:0.0021405117586255074 max memory_allocated 29231.802734375 
[2025-02-18 08:48:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.719110906124115 norm:0.002180106472223997 max memory_allocated 29231.802734375 
[2025-02-18 08:48:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.7192018032073975 norm:0.002105048857629299 max memory_allocated 29231.802734375 
[2025-02-18 08:49:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.7187466025352478 norm:0.001990503864362836 max memory_allocated 29231.802734375 
[2025-02-18 08:49:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 08:50:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.7700851559638977 norm:0.01276802271604538 max memory_allocated 29231.990234375 
[2025-02-18 08:51:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.7531791925430298 norm:0.006992573384195566 max memory_allocated 29231.990234375 
[2025-02-18 08:52:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.7474912405014038 norm:0.005466105416417122 max memory_allocated 29231.990234375 
[2025-02-18 08:53:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.7451289892196655 norm:0.004702181555330753 max memory_allocated 29231.990234375 
[2025-02-18 08:54:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.742519736289978 norm:0.003918383736163378 max memory_allocated 29231.990234375 
[2025-02-18 08:54:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.7398684024810791 norm:0.0034569473937153816 max memory_allocated 29231.990234375 
[2025-02-18 08:55:42 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.7377713918685913 norm:0.003068281337618828 max memory_allocated 29231.990234375 
[2025-02-18 08:56:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.7365443706512451 norm:0.0028481187764555216 max memory_allocated 29231.990234375 
[2025-02-18 08:57:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.736967146396637 norm:0.0028308923356235027 max memory_allocated 29231.990234375 
[2025-02-18 08:58:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.7378456592559814 norm:0.0028111448045819998 max memory_allocated 29231.990234375 
[2025-02-18 08:58:58 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.7370678186416626 norm:0.0026015867479145527 max memory_allocated 29231.990234375 
[2025-02-18 08:59:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.7372848987579346 norm:0.0025753637310117483 max memory_allocated 29231.990234375 
[2025-02-18 09:00:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.7382264137268066 norm:0.0025089895352721214 max memory_allocated 29231.990234375 
[2025-02-18 09:01:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.7374618053436279 norm:0.0024667601101100445 max memory_allocated 29231.990234375 
[2025-02-18 09:02:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.7378730177879333 norm:0.0024659226182848215 max memory_allocated 29231.990234375 
[2025-02-18 09:03:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.7381797432899475 norm:0.0023382212966680527 max memory_allocated 29231.990234375 
[2025-02-18 09:03:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.7372483611106873 norm:0.0022367367055267096 max memory_allocated 29231.990234375 
[2025-02-18 09:04:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.7375273704528809 norm:0.0022618905641138554 max memory_allocated 29231.990234375 
[2025-02-18 09:05:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.7362418174743652 norm:0.0021381787955760956 max memory_allocated 29231.990234375 
[2025-02-18 09:06:19 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.7353672981262207 norm:0.002130480483174324 max memory_allocated 29231.990234375 
[2025-02-18 09:06:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 09:07:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.8114121556282043 norm:0.023425010964274406 max memory_allocated 29232.177734375 
[2025-02-18 09:08:15 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.7925080060958862 norm:0.013576346449553967 max memory_allocated 29232.177734375 
[2025-02-18 09:09:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.7823586463928223 norm:0.008913554251194 max memory_allocated 29232.177734375 
[2025-02-18 09:09:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.7786304950714111 norm:0.007059783674776554 max memory_allocated 29232.177734375 
[2025-02-18 09:10:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.7760006785392761 norm:0.005870684515684843 max memory_allocated 29232.177734375 
[2025-02-18 09:11:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.7717169523239136 norm:0.004771203268319368 max memory_allocated 29232.177734375 
[2025-02-18 09:12:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.769540011882782 norm:0.004380255937576294 max memory_allocated 29232.177734375 
[2025-02-18 09:13:09 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.7680124044418335 norm:0.0042439112439751625 max memory_allocated 29232.177734375 
[2025-02-18 09:13:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.7648755311965942 norm:0.0036304069217294455 max memory_allocated 29232.177734375 
[2025-02-18 09:14:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.7642605900764465 norm:0.0035535828210413456 max memory_allocated 29232.177734375 
[2025-02-18 09:15:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.7632142305374146 norm:0.0031871171668171883 max memory_allocated 29232.177734375 
[2025-02-18 09:16:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.761420488357544 norm:0.002769648563116789 max memory_allocated 29232.177734375 
[2025-02-18 09:17:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.7633146047592163 norm:0.002809364115819335 max memory_allocated 29232.177734375 
[2025-02-18 09:18:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.7648094296455383 norm:0.0028476680163294077 max memory_allocated 29232.177734375 
[2025-02-18 09:18:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.7655278444290161 norm:0.0028839088045060635 max memory_allocated 29232.177734375 
[2025-02-18 09:19:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.7667891383171082 norm:0.0028356497641652822 max memory_allocated 29232.177734375 
[2025-02-18 09:20:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.7657535672187805 norm:0.0027686767280101776 max memory_allocated 29232.177734375 
[2025-02-18 09:21:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.76595538854599 norm:0.002693190472200513 max memory_allocated 29232.177734375 
[2025-02-18 09:22:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.7645667791366577 norm:0.0027205816004425287 max memory_allocated 29232.177734375 
[2025-02-18 09:22:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.765586793422699 norm:0.002708177315071225 max memory_allocated 29232.177734375 
[2025-02-18 09:23:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 09:24:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.8076215982437134 norm:0.028822822496294975 max memory_allocated 29232.365234375 
[2025-02-18 09:24:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.789728581905365 norm:0.01658070832490921 max memory_allocated 29232.365234375 
[2025-02-18 09:25:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.7831252217292786 norm:0.01242003869265318 max memory_allocated 29232.365234375 
[2025-02-18 09:26:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.779887318611145 norm:0.010030481964349747 max memory_allocated 29232.365234375 
[2025-02-18 09:27:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.7759079337120056 norm:0.008265713229775429 max memory_allocated 29232.365234375 
[2025-02-18 09:28:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.7731041312217712 norm:0.007091193925589323 max memory_allocated 29232.365234375 
[2025-02-18 09:28:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.7711287140846252 norm:0.006153370253741741 max memory_allocated 29232.365234375 
[2025-02-18 09:29:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.7698792219161987 norm:0.00541288498789072 max memory_allocated 29232.365234375 
[2025-02-18 09:30:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.7693589925765991 norm:0.005196787882596254 max memory_allocated 29232.365234375 
[2025-02-18 09:31:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.7696529626846313 norm:0.004826026502996683 max memory_allocated 29232.365234375 
[2025-02-18 09:32:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.7685043215751648 norm:0.0047037736512720585 max memory_allocated 29232.365234375 
[2025-02-18 09:33:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.7670629620552063 norm:0.004115821328014135 max memory_allocated 29232.365234375 
[2025-02-18 09:33:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.7659127116203308 norm:0.003887641942128539 max memory_allocated 29232.365234375 
[2025-02-18 09:34:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.7647812366485596 norm:0.0038283998146653175 max memory_allocated 29232.365234375 
[2025-02-18 09:35:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.7651848793029785 norm:0.0037597548216581345 max memory_allocated 29232.365234375 
[2025-02-18 09:36:20 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.7652044892311096 norm:0.003504301654174924 max memory_allocated 29232.365234375 
[2025-02-18 09:37:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.7641410827636719 norm:0.003663277020677924 max memory_allocated 29232.365234375 
[2025-02-18 09:37:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.763689398765564 norm:0.0032869731076061726 max memory_allocated 29232.365234375 
[2025-02-18 09:38:47 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.7633580565452576 norm:0.0031716814264655113 max memory_allocated 29232.365234375 
[2025-02-18 09:39:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.763453483581543 norm:0.003080628579482436 max memory_allocated 29232.365234375 
[2025-02-18 09:39:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 09:40:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.845366358757019 norm:0.03907369077205658 max memory_allocated 29232.552734375 
[2025-02-18 09:41:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.8230038285255432 norm:0.0216013602912426 max memory_allocated 29232.552734375 
[2025-02-18 09:42:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.8159699440002441 norm:0.015020061284303665 max memory_allocated 29232.552734375 
[2025-02-18 09:43:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.8127368092536926 norm:0.012272768653929234 max memory_allocated 29232.552734375 
[2025-02-18 09:43:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.8088849186897278 norm:0.00923903938382864 max memory_allocated 29232.552734375 
[2025-02-18 09:44:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.8054547905921936 norm:0.007318344432860613 max memory_allocated 29232.552734375 
[2025-02-18 09:45:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.8053632974624634 norm:0.006906506605446339 max memory_allocated 29232.552734375 
[2025-02-18 09:46:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.804304301738739 norm:0.006120427045971155 max memory_allocated 29232.552734375 
[2025-02-18 09:47:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.8045620322227478 norm:0.0056828889064490795 max memory_allocated 29232.552734375 
[2025-02-18 09:48:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.8038380146026611 norm:0.005318501498550177 max memory_allocated 29232.552734375 
[2025-02-18 09:48:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.8026466965675354 norm:0.005020832177251577 max memory_allocated 29232.552734375 
[2025-02-18 09:49:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.8023499250411987 norm:0.004484033677726984 max memory_allocated 29232.552734375 
[2025-02-18 09:50:31 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.8014282584190369 norm:0.0041156187653541565 max memory_allocated 29232.552734375 
[2025-02-18 09:51:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.8013383746147156 norm:0.003959035966545343 max memory_allocated 29232.552734375 
[2025-02-18 09:52:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.8006499409675598 norm:0.003918701317161322 max memory_allocated 29232.552734375 
[2025-02-18 09:52:58 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.7975269556045532 norm:0.0037171535659581423 max memory_allocated 29232.552734375 
[2025-02-18 09:53:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.7976239919662476 norm:0.003262097714468837 max memory_allocated 29232.552734375 
[2025-02-18 09:54:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.797080934047699 norm:0.0032149872276932 max memory_allocated 29232.552734375 
[2025-02-18 09:55:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.7969321012496948 norm:0.0030790946912020445 max memory_allocated 29232.552734375 
[2025-02-18 09:56:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.7966532707214355 norm:0.0027345309499651194 max memory_allocated 29232.552734375 
[2025-02-18 09:56:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 09:57:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.8757623434066772 norm:0.014566801488399506 max memory_allocated 29232.740234375 
[2025-02-18 09:58:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.8640502095222473 norm:0.008953025564551353 max memory_allocated 29232.740234375 
[2025-02-18 09:59:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.8613417744636536 norm:0.007403206080198288 max memory_allocated 29232.740234375 
[2025-02-18 09:59:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.8598891496658325 norm:0.006401748396456242 max memory_allocated 29232.740234375 
[2025-02-18 10:00:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.8577089309692383 norm:0.005357340909540653 max memory_allocated 29232.740234375 
[2025-02-18 10:01:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.8552947044372559 norm:0.004680823069065809 max memory_allocated 29232.740234375 
[2025-02-18 10:02:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.8536456227302551 norm:0.004153257701545954 max memory_allocated 29232.740234375 
[2025-02-18 10:03:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.8532841801643372 norm:0.0037500665057450533 max memory_allocated 29232.740234375 
[2025-02-18 10:03:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.8528388738632202 norm:0.003541921963915229 max memory_allocated 29232.740234375 
[2025-02-18 10:04:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.8532343506813049 norm:0.0037650922313332558 max memory_allocated 29232.740234375 
[2025-02-18 10:05:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.852471649646759 norm:0.0034423081669956446 max memory_allocated 29232.740234375 
[2025-02-18 10:06:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.8527052402496338 norm:0.0032295233104377985 max memory_allocated 29232.740234375 
[2025-02-18 10:07:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.8524417281150818 norm:0.002927144756540656 max memory_allocated 29232.740234375 
[2025-02-18 10:07:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.8518097996711731 norm:0.003036251524463296 max memory_allocated 29232.740234375 
[2025-02-18 10:08:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.8512897491455078 norm:0.0029266600031405687 max memory_allocated 29232.740234375 
[2025-02-18 10:09:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.8509425520896912 norm:0.002497173612937331 max memory_allocated 29232.740234375 
[2025-02-18 10:10:26 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.8519866466522217 norm:0.0027720248326659203 max memory_allocated 29232.740234375 
[2025-02-18 10:11:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.8521116375923157 norm:0.002963949926197529 max memory_allocated 29232.740234375 
[2025-02-18 10:12:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.8517805933952332 norm:0.002975506242364645 max memory_allocated 29232.740234375 
[2025-02-18 10:12:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.8531200885772705 norm:0.0025599589571356773 max memory_allocated 29232.740234375 
[2025-02-18 10:13:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 10:14:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.9390252828598022 norm:0.016589011996984482 max memory_allocated 29232.927734375 
[2025-02-18 10:14:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.9249026775360107 norm:0.009794581681489944 max memory_allocated 29232.927734375 
[2025-02-18 10:15:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.9195560216903687 norm:0.007570582441985607 max memory_allocated 29232.927734375 
[2025-02-18 10:16:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.9195373058319092 norm:0.006989837624132633 max memory_allocated 29232.927734375 
[2025-02-18 10:17:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.919960081577301 norm:0.006122604012489319 max memory_allocated 29232.927734375 
[2025-02-18 10:18:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.9189728498458862 norm:0.005987333599478006 max memory_allocated 29232.927734375 
[2025-02-18 10:18:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.9199991226196289 norm:0.0059792413376271725 max memory_allocated 29232.927734375 
[2025-02-18 10:19:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.9200218915939331 norm:0.004273322876542807 max memory_allocated 29232.927734375 
[2025-02-18 10:20:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.919273853302002 norm:0.005372796207666397 max memory_allocated 29232.927734375 
[2025-02-18 10:21:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.9193422198295593 norm:0.004113603848963976 max memory_allocated 29232.927734375 
[2025-02-18 10:22:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.9199034571647644 norm:0.004960570018738508 max memory_allocated 29232.927734375 
[2025-02-18 10:23:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.9213721752166748 norm:0.0043262033723294735 max memory_allocated 29232.927734375 
[2025-02-18 10:23:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.9191054105758667 norm:0.005387197248637676 max memory_allocated 29232.927734375 
[2025-02-18 10:24:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.9207555055618286 norm:0.004868106916546822 max memory_allocated 29232.927734375 
[2025-02-18 10:25:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.9207463264465332 norm:0.004737359471619129 max memory_allocated 29232.927734375 
[2025-02-18 10:26:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.9198461174964905 norm:0.005102787632495165 max memory_allocated 29232.927734375 
[2025-02-18 10:27:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.9190423488616943 norm:0.004827509168535471 max memory_allocated 29232.927734375 
[2025-02-18 10:27:54 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.9207482933998108 norm:0.0035878336057066917 max memory_allocated 29232.927734375 
[2025-02-18 10:28:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.9186437726020813 norm:0.0041506304405629635 max memory_allocated 29232.927734375 
[2025-02-18 10:29:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.9196392297744751 norm:0.004668490029871464 max memory_allocated 29232.927734375 
[2025-02-18 10:29:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 10:30:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:1.0019015073776245 norm:0.016593467444181442 max memory_allocated 29233.115234375 
[2025-02-18 10:31:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.9903799295425415 norm:0.01107244472950697 max memory_allocated 29233.115234375 
[2025-02-18 10:32:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.9848966002464294 norm:0.00831206701695919 max memory_allocated 29233.115234375 
[2025-02-18 10:33:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.9812167286872864 norm:0.006811066530644894 max memory_allocated 29233.115234375 
[2025-02-18 10:33:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.9801006317138672 norm:0.005912717431783676 max memory_allocated 29233.115234375 
[2025-02-18 10:34:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.9802396893501282 norm:0.005506871733814478 max memory_allocated 29233.115234375 
[2025-02-18 10:35:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.9779517650604248 norm:0.004703512880951166 max memory_allocated 29233.115234375 
[2025-02-18 10:36:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.9767944812774658 norm:0.004228578880429268 max memory_allocated 29233.115234375 
[2025-02-18 10:37:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.9750458598136902 norm:0.0035920431837439537 max memory_allocated 29233.115234375 
[2025-02-18 10:38:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.9749371409416199 norm:0.00366065907292068 max memory_allocated 29233.115234375 
[2025-02-18 10:38:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.9733763933181763 norm:0.003137956140562892 max memory_allocated 29233.115234375 
[2025-02-18 10:39:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.9741499423980713 norm:0.0031845304183661938 max memory_allocated 29233.115234375 
[2025-02-18 10:40:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.9757425785064697 norm:0.003392343409359455 max memory_allocated 29233.115234375 
[2025-02-18 10:41:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.9758957624435425 norm:0.0030742117669433355 max memory_allocated 29233.115234375 
[2025-02-18 10:42:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.9743539094924927 norm:0.0029447374399751425 max memory_allocated 29233.115234375 
[2025-02-18 10:42:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.9736824035644531 norm:0.0028567772824317217 max memory_allocated 29233.115234375 
[2025-02-18 10:43:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.9740440845489502 norm:0.002913935109972954 max memory_allocated 29233.115234375 
[2025-02-18 10:44:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.9728710651397705 norm:0.0026953029446303844 max memory_allocated 29233.115234375 
[2025-02-18 10:45:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.9730775952339172 norm:0.0029327217489480972 max memory_allocated 29233.115234375 
[2025-02-18 10:46:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.9714966416358948 norm:0.002477207686752081 max memory_allocated 29233.115234375 
[2025-02-18 10:46:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 10:47:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:1.0799039602279663 norm:0.014130419120192528 max memory_allocated 29233.302734375 
[2025-02-18 10:48:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:1.0691399574279785 norm:0.009555243887007236 max memory_allocated 29233.302734375 
[2025-02-18 10:48:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:1.0639678239822388 norm:0.007312927860766649 max memory_allocated 29233.302734375 
[2025-02-18 10:49:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:1.0617079734802246 norm:0.005893559195101261 max memory_allocated 29233.302734375 
[2025-02-18 10:50:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:1.0619112253189087 norm:0.005168928764760494 max memory_allocated 29233.302734375 
[2025-02-18 10:51:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:1.0614310503005981 norm:0.0046342299319803715 max memory_allocated 29233.302734375 
[2025-02-18 10:52:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:1.0610407590866089 norm:0.004120037890970707 max memory_allocated 29233.302734375 
[2025-02-18 10:53:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:1.060187816619873 norm:0.00335743953473866 max memory_allocated 29233.302734375 
[2025-02-18 10:53:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:1.0603363513946533 norm:0.00346836494281888 max memory_allocated 29233.302734375 
[2025-02-18 10:54:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:1.0594557523727417 norm:0.002948483219370246 max memory_allocated 29233.302734375 
[2025-02-18 10:55:28 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:1.060066819190979 norm:0.003190141636878252 max memory_allocated 29233.302734375 
[2025-02-18 10:56:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:1.060049295425415 norm:0.00280135590583086 max memory_allocated 29233.302734375 
[2025-02-18 10:57:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:1.0606878995895386 norm:0.002829502569511533 max memory_allocated 29233.302734375 
[2025-02-18 10:57:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:1.0599077939987183 norm:0.0023328771349042654 max memory_allocated 29233.302734375 
[2025-02-18 10:58:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:1.0603034496307373 norm:0.0027381302788853645 max memory_allocated 29233.302734375 
[2025-02-18 10:59:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:1.060402512550354 norm:0.0029951483011245728 max memory_allocated 29233.302734375 
[2025-02-18 11:00:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:1.0608453750610352 norm:0.0026572204660624266 max memory_allocated 29233.302734375 
[2025-02-18 11:01:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:1.0612324476242065 norm:0.0027379884850233793 max memory_allocated 29233.302734375 
[2025-02-18 11:02:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:1.0607836246490479 norm:0.0025723460130393505 max memory_allocated 29233.302734375 
[2025-02-18 11:02:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:1.0603126287460327 norm:0.0026669083163142204 max memory_allocated 29233.302734375 
[2025-02-18 11:03:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 11:03:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:1.1711472272872925 norm:0.02435886301100254 max memory_allocated 29233.490234375 
[2025-02-18 11:04:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:1.1639188528060913 norm:0.015764327719807625 max memory_allocated 29233.490234375 
[2025-02-18 11:05:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:1.1602578163146973 norm:0.011771056801080704 max memory_allocated 29233.490234375 
[2025-02-18 11:06:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:1.1587494611740112 norm:0.00940505787730217 max memory_allocated 29233.490234375 
[2025-02-18 11:07:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:1.1579484939575195 norm:0.007799659390002489 max memory_allocated 29233.490234375 
[2025-02-18 11:08:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:1.1567556858062744 norm:0.006510993465781212 max memory_allocated 29233.490234375 
[2025-02-18 11:08:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:1.1560771465301514 norm:0.005905982106924057 max memory_allocated 29233.490234375 
[2025-02-18 11:09:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:1.1554200649261475 norm:0.005224438849836588 max memory_allocated 29233.490234375 
[2025-02-18 11:10:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:1.1552581787109375 norm:0.004683175589889288 max memory_allocated 29233.490234375 
[2025-02-18 11:11:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:1.1551122665405273 norm:0.00419173389673233 max memory_allocated 29233.490234375 
[2025-02-18 11:12:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:1.1540790796279907 norm:0.003808927722275257 max memory_allocated 29233.490234375 
[2025-02-18 11:12:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:1.153908610343933 norm:0.0035668639466166496 max memory_allocated 29233.490234375 
[2025-02-18 11:13:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:1.1540741920471191 norm:0.0033945036120712757 max memory_allocated 29233.490234375 
[2025-02-18 11:14:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:1.1538569927215576 norm:0.003190943505614996 max memory_allocated 29233.490234375 
[2025-02-18 11:15:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:1.1538599729537964 norm:0.003073552157729864 max memory_allocated 29233.490234375 
[2025-02-18 11:16:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:1.1533465385437012 norm:0.0029211570508778095 max memory_allocated 29233.490234375 
[2025-02-18 11:17:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:1.1536810398101807 norm:0.002600481966510415 max memory_allocated 29233.490234375 
[2025-02-18 11:17:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:1.154818058013916 norm:0.002681673737242818 max memory_allocated 29233.490234375 
[2025-02-18 11:18:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:1.1537933349609375 norm:0.0026449942961335182 max memory_allocated 29233.490234375 
[2025-02-18 11:19:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:1.1531881093978882 norm:0.002433606656268239 max memory_allocated 29233.490234375 
[2025-02-18 11:19:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 11:20:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:1.249190330505371 norm:0.012532254680991173 max memory_allocated 29233.677734375 
[2025-02-18 11:21:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:1.2447478771209717 norm:0.009741578251123428 max memory_allocated 29233.677734375 
[2025-02-18 11:22:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:1.2409956455230713 norm:0.007738686166703701 max memory_allocated 29233.677734375 
[2025-02-18 11:23:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:1.237833023071289 norm:0.006588794756680727 max memory_allocated 29233.677734375 
[2025-02-18 11:23:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:1.2361091375350952 norm:0.006170613691210747 max memory_allocated 29233.677734375 
[2025-02-18 11:24:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:1.233837366104126 norm:0.005757521837949753 max memory_allocated 29233.677734375 
[2025-02-18 11:25:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:1.2345556020736694 norm:0.005984270945191383 max memory_allocated 29233.677734375 
[2025-02-18 11:26:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:1.2334208488464355 norm:0.006304808426648378 max memory_allocated 29233.677734375 
[2025-02-18 11:27:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:1.2327297925949097 norm:0.005984616000205278 max memory_allocated 29233.677734375 
[2025-02-18 11:27:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:1.2323554754257202 norm:0.006227440666407347 max memory_allocated 29233.677734375 
[2025-02-18 11:28:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:1.2329998016357422 norm:0.005910028237849474 max memory_allocated 29233.677734375 
[2025-02-18 11:29:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:1.2331732511520386 norm:0.006389348302036524 max memory_allocated 29233.677734375 
[2025-02-18 11:30:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:1.2315471172332764 norm:0.006650123745203018 max memory_allocated 29233.677734375 
[2025-02-18 11:31:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:1.2307270765304565 norm:0.007028622552752495 max memory_allocated 29233.677734375 
[2025-02-18 11:32:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:1.2309925556182861 norm:0.007725034840404987 max memory_allocated 29233.677734375 
[2025-02-18 11:32:49 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:1.2301918268203735 norm:0.007769579999148846 max memory_allocated 29233.677734375 
[2025-02-18 11:33:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:1.230966567993164 norm:0.008118359372019768 max memory_allocated 29233.677734375 
[2025-02-18 11:34:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:1.2306504249572754 norm:0.008637307211756706 max memory_allocated 29233.677734375 
[2025-02-18 11:35:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:1.2305485010147095 norm:0.009835425764322281 max memory_allocated 29233.677734375 
[2025-02-18 11:36:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:1.2301857471466064 norm:0.010879013687372208 max memory_allocated 29233.677734375 
[2025-02-18 11:36:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 11:37:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:1.350303292274475 norm:0.010888399556279182 max memory_allocated 29233.865234375 
[2025-02-18 11:38:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:1.34442138671875 norm:0.007553883828222752 max memory_allocated 29233.865234375 
[2025-02-18 11:38:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:1.3417526483535767 norm:0.005724563729017973 max memory_allocated 29233.865234375 
[2025-02-18 11:39:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:1.3400524854660034 norm:0.004673904739320278 max memory_allocated 29233.865234375 
[2025-02-18 11:40:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:1.3398438692092896 norm:0.004318338353186846 max memory_allocated 29233.865234375 
[2025-02-18 11:41:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:1.3389853239059448 norm:0.0037339485716074705 max memory_allocated 29233.865234375 
[2025-02-18 11:42:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:1.3380340337753296 norm:0.00334772071801126 max memory_allocated 29233.865234375 
[2025-02-18 11:42:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:1.3374496698379517 norm:0.00304671679623425 max memory_allocated 29233.865234375 
[2025-02-18 11:43:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:1.33720064163208 norm:0.002858174964785576 max memory_allocated 29233.865234375 
[2025-02-18 11:44:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:1.3368656635284424 norm:0.002607807284221053 max memory_allocated 29233.865234375 
[2025-02-18 11:45:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:1.3369934558868408 norm:0.002580460160970688 max memory_allocated 29233.865234375 
[2025-02-18 11:46:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:1.3361773490905762 norm:0.0023465738631784916 max memory_allocated 29233.865234375 
[2025-02-18 11:47:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:1.3358213901519775 norm:0.002288347575813532 max memory_allocated 29233.865234375 
[2025-02-18 11:47:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:1.3349710702896118 norm:0.0019952834118157625 max memory_allocated 29233.865234375 
[2025-02-18 11:48:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:1.335770845413208 norm:0.0019534393213689327 max memory_allocated 29233.865234375 
[2025-02-18 11:49:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:1.3347632884979248 norm:0.002110876841470599 max memory_allocated 29233.865234375 
[2025-02-18 11:50:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:1.3341373205184937 norm:0.0018278720090165734 max memory_allocated 29233.865234375 
[2025-02-18 11:51:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:1.334527611732483 norm:0.0019318045815452933 max memory_allocated 29233.865234375 
[2025-02-18 11:51:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:1.333634614944458 norm:0.002023211680352688 max memory_allocated 29233.865234375 
[2025-02-18 11:52:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:1.3336960077285767 norm:0.0018460068386048079 max memory_allocated 29233.865234375 
[2025-02-18 11:52:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 11:53:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:1.490000605583191 norm:0.01356154028326273 max memory_allocated 29234.052734375 
[2025-02-18 11:54:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:1.4833248853683472 norm:0.009608806110918522 max memory_allocated 29234.052734375 
[2025-02-18 11:55:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:1.4808980226516724 norm:0.007501330226659775 max memory_allocated 29234.052734375 
[2025-02-18 11:56:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:1.4796433448791504 norm:0.006156339310109615 max memory_allocated 29234.052734375 
[2025-02-18 11:57:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:1.4795500040054321 norm:0.005043372977524996 max memory_allocated 29234.052734375 
[2025-02-18 11:57:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:1.4791951179504395 norm:0.004164583049714565 max memory_allocated 29234.052734375 
[2025-02-18 11:58:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:1.479844093322754 norm:0.003648415207862854 max memory_allocated 29234.052734375 
[2025-02-18 11:59:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:1.480442762374878 norm:0.0033446031156927347 max memory_allocated 29234.052734375 
[2025-02-18 12:00:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:1.4801256656646729 norm:0.0030319998040795326 max memory_allocated 29234.052734375 
[2025-02-18 12:01:11 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:1.4805400371551514 norm:0.002816835418343544 max memory_allocated 29234.052734375 
[2025-02-18 12:02:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:1.4807926416397095 norm:0.002785023534670472 max memory_allocated 29234.052734375 
[2025-02-18 12:02:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:1.4803590774536133 norm:0.0024146451614797115 max memory_allocated 29234.052734375 
[2025-02-18 12:03:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:1.4810798168182373 norm:0.002482365118339658 max memory_allocated 29234.052734375 
[2025-02-18 12:04:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:1.4810229539871216 norm:0.0023766064550727606 max memory_allocated 29234.052734375 
[2025-02-18 12:05:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:1.4818127155303955 norm:0.0024698846973478794 max memory_allocated 29234.052734375 
[2025-02-18 12:06:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:1.4823673963546753 norm:0.00277687911875546 max memory_allocated 29234.052734375 
[2025-02-18 12:06:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:1.4820318222045898 norm:0.003138734493404627 max memory_allocated 29234.052734375 
[2025-02-18 12:07:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:1.481137752532959 norm:0.00270509603433311 max memory_allocated 29234.052734375 
[2025-02-18 12:08:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:1.4829890727996826 norm:0.0022685604635626078 max memory_allocated 29234.052734375 
[2025-02-18 12:09:20 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:1.4830588102340698 norm:0.0034957691095769405 max memory_allocated 29234.052734375 
[2025-02-18 12:09:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 12:10:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:1.6169517040252686 norm:0.006233398802578449 max memory_allocated 29234.240234375 
[2025-02-18 12:11:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:1.6129167079925537 norm:0.0046584028750658035 max memory_allocated 29234.240234375 
[2025-02-18 12:12:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:1.6116063594818115 norm:0.0037863487377762794 max memory_allocated 29234.240234375 
[2025-02-18 12:12:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:1.610793113708496 norm:0.0032014783937484026 max memory_allocated 29234.240234375 
[2025-02-18 12:13:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:1.6102367639541626 norm:0.0029738526791334152 max memory_allocated 29234.240234375 
[2025-02-18 12:14:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:1.6092920303344727 norm:0.0024111184757202864 max memory_allocated 29234.240234375 
[2025-02-18 12:15:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:1.6085431575775146 norm:0.0022391786333173513 max memory_allocated 29234.240234375 
[2025-02-18 12:16:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:1.6084542274475098 norm:0.002284427173435688 max memory_allocated 29234.240234375 
[2025-02-18 12:16:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:1.607459306716919 norm:0.001964608207345009 max memory_allocated 29234.240234375 
[2025-02-18 12:17:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:1.6081024408340454 norm:0.0017037497600540519 max memory_allocated 29234.240234375 
[2025-02-18 12:18:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:1.6078428030014038 norm:0.0017461874522268772 max memory_allocated 29234.240234375 
[2025-02-18 12:19:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:1.6084891557693481 norm:0.0016556376358494163 max memory_allocated 29234.240234375 
[2025-02-18 12:20:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:1.6083171367645264 norm:0.0016892682760953903 max memory_allocated 29234.240234375 
[2025-02-18 12:21:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:1.6087557077407837 norm:0.0014621720183640718 max memory_allocated 29234.240234375 
[2025-02-18 12:21:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:1.6097406148910522 norm:0.0018768004374578595 max memory_allocated 29234.240234375 
[2025-02-18 12:22:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:1.6097620725631714 norm:0.0014437846839427948 max memory_allocated 29234.240234375 
[2025-02-18 12:23:30 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:1.6095699071884155 norm:0.0017771499697118998 max memory_allocated 29234.240234375 
[2025-02-18 12:24:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:1.6100527048110962 norm:0.001540633151307702 max memory_allocated 29234.240234375 
[2025-02-18 12:25:08 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:1.6102393865585327 norm:0.0014869451988488436 max memory_allocated 29234.240234375 
[2025-02-18 12:25:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:1.6100513935089111 norm:0.00158980512060225 max memory_allocated 29234.240234375 
[2025-02-18 12:26:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 12:27:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:1.7334953546524048 norm:0.01979941874742508 max memory_allocated 29234.427734375 
[2025-02-18 12:27:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:1.726332187652588 norm:0.01383273396641016 max memory_allocated 29234.427734375 
[2025-02-18 12:28:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:1.7226136922836304 norm:0.010614452883601189 max memory_allocated 29234.427734375 
[2025-02-18 12:29:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:1.7216287851333618 norm:0.008744181133806705 max memory_allocated 29234.427734375 
[2025-02-18 12:30:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:1.7203941345214844 norm:0.007409868761897087 max memory_allocated 29234.427734375 
[2025-02-18 12:31:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:1.7198907136917114 norm:0.006459991447627544 max memory_allocated 29234.427734375 
[2025-02-18 12:31:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:1.7204738855361938 norm:0.005776752717792988 max memory_allocated 29234.427734375 
[2025-02-18 12:32:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:1.72135591506958 norm:0.0053559537045657635 max memory_allocated 29234.427734375 
[2025-02-18 12:33:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:1.720442771911621 norm:0.0049981484189629555 max memory_allocated 29234.427734375 
[2025-02-18 12:34:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:1.7202129364013672 norm:0.004460279364138842 max memory_allocated 29234.427734375 
[2025-02-18 12:35:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:1.719327449798584 norm:0.004480500239878893 max memory_allocated 29234.427734375 
[2025-02-18 12:36:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:1.7199995517730713 norm:0.004241642076522112 max memory_allocated 29234.427734375 
[2025-02-18 12:36:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:1.720096230506897 norm:0.0041652279905974865 max memory_allocated 29234.427734375 
[2025-02-18 12:37:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:1.7197328805923462 norm:0.00406260509043932 max memory_allocated 29234.427734375 
[2025-02-18 12:38:27 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:1.7189693450927734 norm:0.0036380947567522526 max memory_allocated 29234.427734375 
[2025-02-18 12:39:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:1.7201255559921265 norm:0.0038314340636134148 max memory_allocated 29234.427734375 
[2025-02-18 12:40:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:1.7196747064590454 norm:0.0036658954340964556 max memory_allocated 29234.427734375 
[2025-02-18 12:40:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:1.7198885679244995 norm:0.004253868479281664 max memory_allocated 29234.427734375 
[2025-02-18 12:41:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:1.7181750535964966 norm:0.003441657405346632 max memory_allocated 29234.427734375 
[2025-02-18 12:42:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:1.718462347984314 norm:0.003447551280260086 max memory_allocated 29234.427734375 
[2025-02-18 12:42:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 12:43:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:1.8978766202926636 norm:0.0094029251486063 max memory_allocated 29234.615234375 
[2025-02-18 12:44:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:1.8947802782058716 norm:0.0066244942136108875 max memory_allocated 29234.615234375 
[2025-02-18 12:45:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:1.8922004699707031 norm:0.005208214279264212 max memory_allocated 29234.615234375 
[2025-02-18 12:46:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:1.891485571861267 norm:0.004284145310521126 max memory_allocated 29234.615234375 
[2025-02-18 12:46:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:1.890449047088623 norm:0.0037038184236735106 max memory_allocated 29234.615234375 
[2025-02-18 12:47:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:1.8911235332489014 norm:0.0033249822445213795 max memory_allocated 29234.615234375 
[2025-02-18 12:48:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:1.8906327486038208 norm:0.002937664743512869 max memory_allocated 29234.615234375 
[2025-02-18 12:49:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:1.8902349472045898 norm:0.002733794739469886 max memory_allocated 29234.615234375 
[2025-02-18 12:50:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:1.8909562826156616 norm:0.0025700426194816828 max memory_allocated 29234.615234375 
[2025-02-18 12:50:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:1.889595866203308 norm:0.002288194838911295 max memory_allocated 29234.615234375 
[2025-02-18 12:51:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:1.8892844915390015 norm:0.0024879807606339455 max memory_allocated 29234.615234375 
[2025-02-18 12:52:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:1.8898824453353882 norm:0.002104152226820588 max memory_allocated 29234.615234375 
[2025-02-18 12:53:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:1.8901230096817017 norm:0.0021094009280204773 max memory_allocated 29234.615234375 
[2025-02-18 12:54:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:1.889653205871582 norm:0.0024858335964381695 max memory_allocated 29234.615234375 
[2025-02-18 12:55:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:1.889387845993042 norm:0.0018948331708088517 max memory_allocated 29234.615234375 
[2025-02-18 12:55:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:1.8893225193023682 norm:0.0019305511377751827 max memory_allocated 29234.615234375 
[2025-02-18 12:56:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:1.8883650302886963 norm:0.0019191609462723136 max memory_allocated 29234.615234375 
[2025-02-18 12:57:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:1.8899874687194824 norm:0.002302902052178979 max memory_allocated 29234.615234375 
[2025-02-18 12:58:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:1.8902325630187988 norm:0.0022475826554000378 max memory_allocated 29234.615234375 
[2025-02-18 12:59:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:1.8888603448867798 norm:0.0021906960755586624 max memory_allocated 29234.615234375 
[2025-02-18 12:59:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 13:00:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:2.1033966541290283 norm:0.017275799065828323 max memory_allocated 29234.802734375 
[2025-02-18 13:01:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:2.0955283641815186 norm:0.01105818897485733 max memory_allocated 29234.802734375 
[2025-02-18 13:01:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:2.09196400642395 norm:0.008230212144553661 max memory_allocated 29234.802734375 
[2025-02-18 13:02:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:2.089099645614624 norm:0.006455328315496445 max memory_allocated 29234.802734375 
[2025-02-18 13:03:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:2.089167833328247 norm:0.00544672179967165 max memory_allocated 29234.802734375 
[2025-02-18 13:04:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:2.0881006717681885 norm:0.004729417152702808 max memory_allocated 29234.802734375 
[2025-02-18 13:05:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:2.087385654449463 norm:0.004220994655042887 max memory_allocated 29234.802734375 
[2025-02-18 13:05:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:2.086731195449829 norm:0.003688471857458353 max memory_allocated 29234.802734375 
[2025-02-18 13:06:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:2.0886807441711426 norm:0.003830245230346918 max memory_allocated 29234.802734375 
[2025-02-18 13:07:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:2.0864105224609375 norm:0.0033177677541971207 max memory_allocated 29234.802734375 
[2025-02-18 13:08:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:2.087315797805786 norm:0.0030719537753611803 max memory_allocated 29234.802734375 
[2025-02-18 13:09:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:2.086090087890625 norm:0.003235684707760811 max memory_allocated 29234.802734375 
[2025-02-18 13:10:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:2.085505962371826 norm:0.002689114771783352 max memory_allocated 29234.802734375 
[2025-02-18 13:10:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:2.0860695838928223 norm:0.00252553797326982 max memory_allocated 29234.802734375 
[2025-02-18 13:11:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:2.085827350616455 norm:0.002467374550178647 max memory_allocated 29234.802734375 
[2025-02-18 13:12:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:2.0864152908325195 norm:0.0030082915909588337 max memory_allocated 29234.802734375 
[2025-02-18 13:13:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:2.0851430892944336 norm:0.002484231488779187 max memory_allocated 29234.802734375 
[2025-02-18 13:14:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:2.0851473808288574 norm:0.002295084297657013 max memory_allocated 29234.802734375 
[2025-02-18 13:14:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:2.084226369857788 norm:0.0029591163620352745 max memory_allocated 29234.802734375 
[2025-02-18 13:15:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:2.084113836288452 norm:0.002068777335807681 max memory_allocated 29234.802734375 
[2025-02-18 13:15:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 13:16:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:2.2789947986602783 norm:0.010383952409029007 max memory_allocated 29234.990234375 
[2025-02-18 13:17:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:2.275853157043457 norm:0.008219683542847633 max memory_allocated 29234.990234375 
[2025-02-18 13:18:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:2.2733757495880127 norm:0.0067753903567790985 max memory_allocated 29234.990234375 
[2025-02-18 13:19:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:2.2732136249542236 norm:0.005709678865969181 max memory_allocated 29234.990234375 
[2025-02-18 13:20:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:2.2730727195739746 norm:0.00490821897983551 max memory_allocated 29234.990234375 
[2025-02-18 13:20:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:2.273853063583374 norm:0.004463766701519489 max memory_allocated 29234.990234375 
[2025-02-18 13:21:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:2.2751691341400146 norm:0.00428764009848237 max memory_allocated 29234.990234375 
[2025-02-18 13:22:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:2.276561975479126 norm:0.0040071685798466206 max memory_allocated 29234.990234375 
[2025-02-18 13:23:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:2.275139093399048 norm:0.0035849446430802345 max memory_allocated 29234.990234375 
[2025-02-18 13:24:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:2.2764854431152344 norm:0.003697969950735569 max memory_allocated 29234.990234375 
[2025-02-18 13:25:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:2.2769641876220703 norm:0.0032624471932649612 max memory_allocated 29234.990234375 
[2025-02-18 13:25:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:2.276303768157959 norm:0.003349939826875925 max memory_allocated 29234.990234375 
[2025-02-18 13:26:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:2.27653169631958 norm:0.0031386143527925014 max memory_allocated 29234.990234375 
[2025-02-18 13:27:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:2.2769346237182617 norm:0.003286834806203842 max memory_allocated 29234.990234375 
[2025-02-18 13:28:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:2.2761049270629883 norm:0.003416449762880802 max memory_allocated 29234.990234375 
[2025-02-18 13:29:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:2.2770724296569824 norm:0.0034132427535951138 max memory_allocated 29234.990234375 
[2025-02-18 13:29:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:2.2770254611968994 norm:0.0030738685745745897 max memory_allocated 29234.990234375 
[2025-02-18 13:30:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:2.2769618034362793 norm:0.002712551038712263 max memory_allocated 29234.990234375 
[2025-02-18 13:31:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:2.2769975662231445 norm:0.0028043861966580153 max memory_allocated 29234.990234375 
[2025-02-18 13:32:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:2.2768168449401855 norm:0.0034931369591504335 max memory_allocated 29234.990234375 
[2025-02-18 13:32:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-18 13:33:27 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:2.5309550762176514 norm:0.024492334574460983 max memory_allocated 29235.177734375 
[2025-02-18 13:34:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:2.5357303619384766 norm:0.026342272758483887 max memory_allocated 29235.177734375 
[2025-02-18 13:35:05 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:2.5211141109466553 norm:0.015490876510739326 max memory_allocated 29235.177734375 
[2025-02-18 13:35:54 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:2.51495099067688 norm:0.012727629393339157 max memory_allocated 29235.177734375 
[2025-02-18 13:36:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:2.515137195587158 norm:0.011785918846726418 max memory_allocated 29235.177734375 
[2025-02-18 13:37:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:2.5059361457824707 norm:0.009125562384724617 max memory_allocated 29235.177734375 
[2025-02-18 13:38:20 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:2.5149710178375244 norm:0.011505347676575184 max memory_allocated 29235.177734375 
[2025-02-18 13:39:09 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:2.500791549682617 norm:0.007544919848442078 max memory_allocated 29235.177734375 
[2025-02-18 13:39:58 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:2.5057003498077393 norm:0.007683077827095985 max memory_allocated 29235.177734375 
[2025-02-18 13:40:47 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:2.5112884044647217 norm:0.010675476863980293 max memory_allocated 29235.177734375 
[2025-02-18 13:41:36 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:2.4963338375091553 norm:0.005905691999942064 max memory_allocated 29235.177734375 
[2025-02-18 13:42:25 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:2.501383066177368 norm:0.006537226028740406 max memory_allocated 29235.177734375 
[2025-02-18 13:43:14 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:2.5168097019195557 norm:0.02280394360423088 max memory_allocated 29235.177734375 
[2025-02-18 13:44:02 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:2.5030465126037598 norm:0.00891450047492981 max memory_allocated 29235.177734375 
[2025-02-18 13:44:51 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:2.508303642272949 norm:0.012288886122405529 max memory_allocated 29235.177734375 
[2025-02-18 13:45:40 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:2.505138874053955 norm:0.007992345839738846 max memory_allocated 29235.177734375 
[2025-02-18 13:46:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:2.500464916229248 norm:0.005866778548806906 max memory_allocated 29235.177734375 
[2025-02-18 13:47:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:2.4987294673919678 norm:0.005397627130150795 max memory_allocated 29235.177734375 
[2025-02-18 13:48:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:2.5023417472839355 norm:0.0056832111440598965 max memory_allocated 29235.177734375 
[2025-02-18 13:48:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:2.5067880153656006 norm:0.0077146971598267555 max memory_allocated 29235.177734375 
[2025-02-18 13:49:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-18 13:50:03 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:2.7139408588409424 norm:0.01747366040945053 max memory_allocated 29235.365234375 
[2025-02-18 13:50:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:2.7055957317352295 norm:0.012903708964586258 max memory_allocated 29235.365234375 
[2025-02-18 13:51:40 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:2.703328847885132 norm:0.010129086673259735 max memory_allocated 29235.365234375 
[2025-02-18 13:52:29 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:2.7035791873931885 norm:0.008626801893115044 max memory_allocated 29235.365234375 
[2025-02-18 13:53:18 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:2.705169677734375 norm:0.007690563332289457 max memory_allocated 29235.365234375 
[2025-02-18 13:54:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:2.7055070400238037 norm:0.006363549269735813 max memory_allocated 29235.365234375 
[2025-02-18 13:54:56 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:2.706049919128418 norm:0.0055997311137616634 max memory_allocated 29235.365234375 
[2025-02-18 13:55:44 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:2.706732988357544 norm:0.004959062673151493 max memory_allocated 29235.365234375 
[2025-02-18 13:56:33 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:2.7081873416900635 norm:0.004902590997517109 max memory_allocated 29235.365234375 
[2025-02-18 13:57:22 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:2.7084295749664307 norm:0.004535863175988197 max memory_allocated 29235.365234375 
[2025-02-18 13:58:11 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:2.7091593742370605 norm:0.004311618395149708 max memory_allocated 29235.365234375 
[2025-02-18 13:59:00 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:2.7069284915924072 norm:0.004142696503549814 max memory_allocated 29235.365234375 
[2025-02-18 13:59:48 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:2.708739757537842 norm:0.004708208609372377 max memory_allocated 29235.365234375 
[2025-02-18 14:00:38 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:2.7066490650177 norm:0.0039299651980400085 max memory_allocated 29235.365234375 
[2025-02-18 14:01:26 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:2.7113940715789795 norm:0.0060299537144601345 max memory_allocated 29235.365234375 
[2025-02-18 14:02:15 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:2.701137065887451 norm:0.003351785708218813 max memory_allocated 29235.365234375 
[2025-02-18 14:03:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:2.7061705589294434 norm:0.00393723277375102 max memory_allocated 29235.365234375 
[2025-02-18 14:03:53 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:2.7063326835632324 norm:0.003930504433810711 max memory_allocated 29235.365234375 
[2025-02-18 14:04:42 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:2.704529285430908 norm:0.004031050950288773 max memory_allocated 29235.365234375 
[2025-02-18 14:05:31 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:2.7093005180358887 norm:0.006267709657549858 max memory_allocated 29235.365234375 
[2025-02-18 14:05:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-18 14:06:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:2.9690937995910645 norm:0.016732223331928253 max memory_allocated 29235.552734375 
[2025-02-18 14:07:27 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:2.9608471393585205 norm:0.010713418945670128 max memory_allocated 29235.552734375 
[2025-02-18 14:08:16 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:2.9561400413513184 norm:0.007710031699389219 max memory_allocated 29235.552734375 
[2025-02-18 14:09:05 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:2.9540352821350098 norm:0.006013086065649986 max memory_allocated 29235.552734375 
[2025-02-18 14:09:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:2.952535629272461 norm:0.004947040230035782 max memory_allocated 29235.552734375 
[2025-02-18 14:10:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:2.9517810344696045 norm:0.004263921175152063 max memory_allocated 29235.552734375 
[2025-02-18 14:11:32 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:2.950350761413574 norm:0.0037053204141557217 max memory_allocated 29235.552734375 
[2025-02-18 14:12:21 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:2.949728012084961 norm:0.003244841704145074 max memory_allocated 29235.552734375 
[2025-02-18 14:13:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:2.9489126205444336 norm:0.0029782773926854134 max memory_allocated 29235.552734375 
[2025-02-18 14:13:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:2.947925090789795 norm:0.002882544184103608 max memory_allocated 29235.552734375 
[2025-02-18 14:14:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:2.9476680755615234 norm:0.0024661710485816 max memory_allocated 29235.552734375 
[2025-02-18 14:15:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:2.948131799697876 norm:0.00253760302439332 max memory_allocated 29235.552734375 
[2025-02-18 14:16:26 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:2.948054313659668 norm:0.0024504547473043203 max memory_allocated 29235.552734375 
[2025-02-18 14:17:15 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:2.948460340499878 norm:0.002267422154545784 max memory_allocated 29235.552734375 
[2025-02-18 14:18:05 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:2.9475526809692383 norm:0.0021827947348356247 max memory_allocated 29235.552734375 
[2025-02-18 14:18:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:2.94647479057312 norm:0.002382154343649745 max memory_allocated 29235.552734375 
[2025-02-18 14:19:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:2.947267770767212 norm:0.002260505221784115 max memory_allocated 29235.552734375 
[2025-02-18 14:20:32 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:2.945634603500366 norm:0.002118344185873866 max memory_allocated 29235.552734375 
[2025-02-18 14:21:21 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:2.94751238822937 norm:0.002451119711622596 max memory_allocated 29235.552734375 
[2025-02-18 14:22:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:2.9461991786956787 norm:0.001808736938983202 max memory_allocated 29235.552734375 
[2025-02-18 14:22:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-18 14:23:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:3.249293327331543 norm:0.012081578373908997 max memory_allocated 29235.740234375 
[2025-02-18 14:24:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:3.2437970638275146 norm:0.010228173807263374 max memory_allocated 29235.740234375 
[2025-02-18 14:24:56 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:3.240454912185669 norm:0.008777806535363197 max memory_allocated 29235.740234375 
[2025-02-18 14:25:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:3.2392117977142334 norm:0.007762509863823652 max memory_allocated 29235.740234375 
[2025-02-18 14:26:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:3.2389678955078125 norm:0.006877682637423277 max memory_allocated 29235.740234375 
[2025-02-18 14:27:23 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:3.237755298614502 norm:0.005996972322463989 max memory_allocated 29235.740234375 
[2025-02-18 14:28:12 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:3.237438917160034 norm:0.005245727486908436 max memory_allocated 29235.740234375 
[2025-02-18 14:29:01 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:3.2367162704467773 norm:0.005237188655883074 max memory_allocated 29235.740234375 
[2025-02-18 14:29:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:3.2348060607910156 norm:0.004991583060473204 max memory_allocated 29235.740234375 
[2025-02-18 14:30:39 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:3.236199140548706 norm:0.004779069218784571 max memory_allocated 29235.740234375 
[2025-02-18 14:31:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:3.2363362312316895 norm:0.004485889803618193 max memory_allocated 29235.740234375 
[2025-02-18 14:32:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:3.232569694519043 norm:0.004451575223356485 max memory_allocated 29235.740234375 
[2025-02-18 14:33:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:3.234593152999878 norm:0.004341848660260439 max memory_allocated 29235.740234375 
[2025-02-18 14:33:56 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:3.23659086227417 norm:0.0038309472147375345 max memory_allocated 29235.740234375 
[2025-02-18 14:34:45 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:3.235626697540283 norm:0.0040978300385177135 max memory_allocated 29235.740234375 
[2025-02-18 14:35:34 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:3.236978054046631 norm:0.003767953719943762 max memory_allocated 29235.740234375 
[2025-02-18 14:36:23 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:3.241424798965454 norm:0.009085140191018581 max memory_allocated 29235.740234375 
[2025-02-18 14:37:12 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:3.2249844074249268 norm:0.0034645115956664085 max memory_allocated 29235.740234375 
[2025-02-18 14:38:01 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:3.2336044311523438 norm:0.0030996366403996944 max memory_allocated 29235.740234375 
[2025-02-18 14:38:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:3.232804775238037 norm:0.0032654355745762587 max memory_allocated 29235.740234375 
[2025-02-18 14:39:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-18 14:39:58 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:3.637396812438965 norm:0.027953894808888435 max memory_allocated 29235.927734375 
[2025-02-18 14:40:47 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:3.6263813972473145 norm:0.021049564704298973 max memory_allocated 29235.927734375 
[2025-02-18 14:41:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:3.6234307289123535 norm:0.01834431104362011 max memory_allocated 29235.927734375 
[2025-02-18 14:42:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:3.623255729675293 norm:0.017366649582982063 max memory_allocated 29235.927734375 
[2025-02-18 14:43:14 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:3.6169800758361816 norm:0.01435657124966383 max memory_allocated 29235.927734375 
[2025-02-18 14:44:03 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:3.6337625980377197 norm:0.020022079348564148 max memory_allocated 29235.927734375 
[2025-02-18 14:44:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:3.6314938068389893 norm:0.017304087057709694 max memory_allocated 29235.927734375 
[2025-02-18 14:45:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:3.631096363067627 norm:0.01718808338046074 max memory_allocated 29235.927734375 
[2025-02-18 14:46:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:3.6317150592803955 norm:0.016040269285440445 max memory_allocated 29235.927734375 
[2025-02-18 14:47:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:3.6326236724853516 norm:0.01991049014031887 max memory_allocated 29235.927734375 
[2025-02-18 14:48:08 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:3.6135964393615723 norm:0.009112006984651089 max memory_allocated 29235.927734375 
[2025-02-18 14:48:58 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:3.6049373149871826 norm:0.00761997327208519 max memory_allocated 29235.927734375 
[2025-02-18 14:49:47 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:3.602726697921753 norm:0.007092271465808153 max memory_allocated 29235.927734375 
[2025-02-18 14:50:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:3.6030900478363037 norm:0.006774640642106533 max memory_allocated 29235.927734375 
[2025-02-18 14:51:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:3.6048476696014404 norm:0.006590998265892267 max memory_allocated 29235.927734375 
[2025-02-18 14:52:14 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:3.607414722442627 norm:0.006102591287344694 max memory_allocated 29235.927734375 
[2025-02-18 14:53:03 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:3.6117537021636963 norm:0.006434142589569092 max memory_allocated 29235.927734375 
[2025-02-18 14:53:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:3.6116504669189453 norm:0.006228134501725435 max memory_allocated 29235.927734375 
[2025-02-18 14:54:41 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:3.6168596744537354 norm:0.010546841658651829 max memory_allocated 29235.927734375 
[2025-02-18 14:55:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:3.615269899368286 norm:0.007928600534796715 max memory_allocated 29235.927734375 
[2025-02-18 14:55:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-18 14:56:37 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:4.049158096313477 norm:0.06192149966955185 max memory_allocated 29236.115234375 
[2025-02-18 14:57:26 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:4.036186218261719 norm:0.04960136488080025 max memory_allocated 29236.115234375 
[2025-02-18 14:58:15 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:4.034559726715088 norm:0.041308216750621796 max memory_allocated 29236.115234375 
[2025-02-18 14:59:04 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:4.047197341918945 norm:0.041346095502376556 max memory_allocated 29236.115234375 
[2025-02-18 14:59:53 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:4.043157577514648 norm:0.03193838894367218 max memory_allocated 29236.115234375 
[2025-02-18 15:00:42 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:4.052757263183594 norm:0.02927558496594429 max memory_allocated 29236.115234375 
[2025-02-18 15:01:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:4.058581829071045 norm:0.026323527097702026 max memory_allocated 29236.115234375 
[2025-02-18 15:02:19 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:4.062057971954346 norm:0.024345099925994873 max memory_allocated 29236.115234375 
[2025-02-18 15:03:08 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:4.063803672790527 norm:0.029183462262153625 max memory_allocated 29236.115234375 
[2025-02-18 15:03:57 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:4.0568671226501465 norm:0.025865353643894196 max memory_allocated 29236.115234375 
[2025-02-18 15:04:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:4.056940078735352 norm:0.01989780180156231 max memory_allocated 29236.115234375 
[2025-02-18 15:05:35 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:4.0573248863220215 norm:0.01859907992184162 max memory_allocated 29236.115234375 
[2025-02-18 15:06:24 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:4.076642990112305 norm:0.0360596664249897 max memory_allocated 29236.115234375 
[2025-02-18 15:07:12 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:4.055922985076904 norm:0.02110336534678936 max memory_allocated 29236.115234375 
[2025-02-18 15:08:01 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:4.048702716827393 norm:0.018706876784563065 max memory_allocated 29236.115234375 
[2025-02-18 15:08:50 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:4.0474019050598145 norm:0.016672177240252495 max memory_allocated 29236.115234375 
[2025-02-18 15:09:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:4.077193260192871 norm:0.03980429843068123 max memory_allocated 29236.115234375 
[2025-02-18 15:10:28 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:4.053643226623535 norm:0.0151806166395545 max memory_allocated 29236.115234375 
[2025-02-18 15:11:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:4.047677516937256 norm:0.013715263456106186 max memory_allocated 29236.115234375 
[2025-02-18 15:12:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:4.0479207038879395 norm:0.01413759309798479 max memory_allocated 29236.115234375 
[2025-02-18 15:12:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-18 15:13:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:4.996979236602783 norm:0.11216457933187485 max memory_allocated 29236.302734375 
[2025-02-18 15:14:01 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:4.951606750488281 norm:0.08022815734148026 max memory_allocated 29236.302734375 
[2025-02-18 15:14:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:4.9225568771362305 norm:0.06385453790426254 max memory_allocated 29236.302734375 
[2025-02-18 15:15:39 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:4.896126747131348 norm:0.05072589963674545 max memory_allocated 29236.302734375 
[2025-02-18 15:16:28 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:4.888706207275391 norm:0.04268573597073555 max memory_allocated 29236.302734375 
[2025-02-18 15:17:17 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:4.877209663391113 norm:0.038573045283555984 max memory_allocated 29236.302734375 
[2025-02-18 15:18:06 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:4.864439010620117 norm:0.035088445991277695 max memory_allocated 29236.302734375 
[2025-02-18 15:18:55 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:4.860646724700928 norm:0.03583648055791855 max memory_allocated 29236.302734375 
[2025-02-18 15:19:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:4.850528717041016 norm:0.02662895992398262 max memory_allocated 29236.302734375 
[2025-02-18 15:20:32 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:4.862544059753418 norm:0.02966861054301262 max memory_allocated 29236.302734375 
[2025-02-18 15:21:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:4.8535542488098145 norm:0.02494571916759014 max memory_allocated 29236.302734375 
[2025-02-18 15:22:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:4.852465629577637 norm:0.026494432240724564 max memory_allocated 29236.302734375 
[2025-02-18 15:22:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:4.845961093902588 norm:0.029151394963264465 max memory_allocated 29236.302734375 
[2025-02-18 15:23:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:4.841500282287598 norm:0.046426255255937576 max memory_allocated 29236.302734375 
[2025-02-18 15:24:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:4.81238317489624 norm:0.016982732340693474 max memory_allocated 29236.302734375 
[2025-02-18 15:25:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:4.817827224731445 norm:0.0167204812169075 max memory_allocated 29236.302734375 
[2025-02-18 15:26:14 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:4.819483280181885 norm:0.017662012949585915 max memory_allocated 29236.302734375 
[2025-02-18 15:27:03 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:4.81998872756958 norm:0.016538724303245544 max memory_allocated 29236.302734375 
[2025-02-18 15:27:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:4.821625232696533 norm:0.020999712869524956 max memory_allocated 29236.302734375 
[2025-02-18 15:28:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:4.815749645233154 norm:0.016071626916527748 max memory_allocated 29236.302734375 
[2025-02-18 15:28:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-18 15:29:48 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:9.270415306091309 norm:0.14317137002944946 max memory_allocated 29236.490234375 
[2025-02-18 15:30:36 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:9.074786186218262 norm:0.12721088528633118 max memory_allocated 29236.490234375 
[2025-02-18 15:31:25 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:8.964329719543457 norm:0.11950142681598663 max memory_allocated 29236.490234375 
[2025-02-18 15:32:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:8.949440002441406 norm:0.11768579483032227 max memory_allocated 29236.490234375 
[2025-02-18 15:33:03 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:8.927858352661133 norm:0.12054824829101562 max memory_allocated 29236.490234375 
[2025-02-18 15:33:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:8.95309066772461 norm:0.13584159314632416 max memory_allocated 29236.490234375 
[2025-02-18 15:34:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:8.942044258117676 norm:0.16875746846199036 max memory_allocated 29236.490234375 
[2025-02-18 15:35:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:8.826184272766113 norm:0.29440197348594666 max memory_allocated 29236.490234375 
[2025-02-18 15:36:18 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:8.724806785583496 norm:2.3074162006378174 max memory_allocated 29236.490234375 
[2025-02-18 15:37:06 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:8.739590644836426 norm:22.081806182861328 max memory_allocated 29236.490234375 
[2025-02-18 15:37:55 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:8.572254180908203 norm:24.693552017211914 max memory_allocated 29236.490234375 
[2025-02-18 15:38:44 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:8.487288475036621 norm:12.92042350769043 max memory_allocated 29236.490234375 
[2025-02-18 15:39:32 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:8.529420852661133 norm:29.42901039123535 max memory_allocated 29236.490234375 
[2025-02-18 15:40:20 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:8.577184677124023 norm:58.48142623901367 max memory_allocated 29236.490234375 
[2025-02-18 15:41:09 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:8.511702537536621 norm:58.719573974609375 max memory_allocated 29236.490234375 
[2025-02-18 15:41:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:8.481698989868164 norm:47.2500114440918 max memory_allocated 29236.490234375 
[2025-02-18 15:42:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:8.460494995117188 norm:52.48345947265625 max memory_allocated 29236.490234375 
[2025-02-18 15:43:33 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:8.454119682312012 norm:62.36151885986328 max memory_allocated 29236.490234375 
[2025-02-18 15:44:22 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:8.431451797485352 norm:49.91075134277344 max memory_allocated 29236.490234375 
[2025-02-18 15:45:10 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:8.423628807067871 norm:55.61982727050781 max memory_allocated 29236.490234375 
[2025-02-18 15:45:24 root] (main_calibration.py 365): INFO 39904.196917295456
[2025-02-18 15:46:29 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-18 15:48:22 root] (main_calibration.py 158): INFO wikitext2 : 12.512764930725098
[2025-02-18 15:48:22 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-18 15:51:20 root] (main_calibration.py 158): INFO c4 : 19.70403480529785
[2025-02-18 17:53:24 root] (main_calibration.py 169): INFO {'wikitext2': 12.512764930725098, 'c4': 19.70403480529785, 'results': {'piqa': {'acc': 0.6621327529923831, 'acc_stderr': 0.011035474307853841, 'acc_norm': 0.6637649619151251, 'acc_norm_stderr': 0.011022346708970239}, 'arc_easy': {'acc': 0.4751683501683502, 'acc_stderr': 0.010247123122159262, 'acc_norm': 0.406986531986532, 'acc_norm_stderr': 0.010080695355466589}, 'arc_challenge': {'acc': 0.28754266211604096, 'acc_stderr': 0.013226719056266134, 'acc_norm': 0.3242320819112628, 'acc_norm_stderr': 0.013678810399518815}, 'hellaswag': {'acc': 0.37223660625373434, 'acc_stderr': 0.004824130528590594, 'acc_norm': 0.4614618601872137, 'acc_norm_stderr': 0.004974937803907463}, 'winogrande': {'acc': 0.526440410418311, 'acc_stderr': 0.014032823874407224}, 'boolq': {'acc': 0.6140672782874618, 'acc_stderr': 0.00851444449586334}}, 'versions': {'piqa': 0, 'arc_easy': 0, 'arc_challenge': 0, 'hellaswag': 0, 'winogrande': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
