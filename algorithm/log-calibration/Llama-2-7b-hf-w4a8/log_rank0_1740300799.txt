[2025-02-23 08:53:19 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration/Llama-2-7b-hf-w4a8', save_dir='./log-calibration/quant/Llama-2-7b-hf-w4a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-23 08:53:26 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-23 08:53:26 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-23 08:53:26 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-23 08:53:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-23 08:54:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.008779672905802727 norm:0.0011356662726029754 max memory_allocated 22512.63671875 
[2025-02-23 08:54:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.005772148258984089 norm:0.0008873039623722434 max memory_allocated 22512.63671875 
[2025-02-23 08:55:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.00838320143520832 norm:0.006847381126135588 max memory_allocated 22512.63671875 
[2025-02-23 08:55:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.009081688709557056 norm:0.011650027707219124 max memory_allocated 22512.63671875 
[2025-02-23 08:56:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.009142212569713593 norm:0.010054616257548332 max memory_allocated 22512.63671875 
[2025-02-23 08:56:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.009388638660311699 norm:0.01070717815309763 max memory_allocated 22512.63671875 
[2025-02-23 08:57:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.008800150826573372 norm:0.009166779927909374 max memory_allocated 22512.63671875 
[2025-02-23 08:57:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.008868295699357986 norm:0.009162196889519691 max memory_allocated 22512.63671875 
[2025-02-23 08:58:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.008833409287035465 norm:0.009350177831947803 max memory_allocated 22512.63671875 
[2025-02-23 08:58:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00907097663730383 norm:0.010241823270916939 max memory_allocated 22512.63671875 
[2025-02-23 08:59:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.009252608753740788 norm:0.010899217799305916 max memory_allocated 22512.63671875 
[2025-02-23 09:00:01 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.009852368384599686 norm:0.011031629517674446 max memory_allocated 22512.63671875 
[2025-02-23 09:00:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.009279297664761543 norm:0.010071296244859695 max memory_allocated 22512.63671875 
[2025-02-23 09:01:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.009587579406797886 norm:0.010503800585865974 max memory_allocated 22512.63671875 
[2025-02-23 09:01:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.009279917925596237 norm:0.009905763901770115 max memory_allocated 22512.63671875 
[2025-02-23 09:02:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.009235229343175888 norm:0.010359151288866997 max memory_allocated 22512.63671875 
[2025-02-23 09:02:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.009033970534801483 norm:0.009525475092232227 max memory_allocated 22512.63671875 
[2025-02-23 09:03:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00919939111918211 norm:0.009947666898369789 max memory_allocated 22512.63671875 
[2025-02-23 09:03:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00911200325936079 norm:0.009799391031265259 max memory_allocated 22512.63671875 
[2025-02-23 09:04:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.00918678380548954 norm:0.010022382251918316 max memory_allocated 22512.63671875 
[2025-02-23 09:04:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-23 09:05:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.04597548395395279 norm:0.003780592931434512 max memory_allocated 22512.80859375 
[2025-02-23 09:05:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.03534628450870514 norm:0.005114443600177765 max memory_allocated 22512.80859375 
[2025-02-23 09:06:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0318475142121315 norm:0.011401372030377388 max memory_allocated 22512.80859375 
[2025-02-23 09:06:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.030654270201921463 norm:0.007265105843544006 max memory_allocated 22512.80859375 
[2025-02-23 09:07:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.028627239167690277 norm:0.00832888949662447 max memory_allocated 22512.80859375 
[2025-02-23 09:07:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.02830670028924942 norm:0.006966012064367533 max memory_allocated 22512.80859375 
[2025-02-23 09:08:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.027807503938674927 norm:0.004514171276241541 max memory_allocated 22512.80859375 
[2025-02-23 09:08:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.027089746668934822 norm:0.00956437923014164 max memory_allocated 22512.80859375 
[2025-02-23 09:09:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.026368921622633934 norm:0.004880395717918873 max memory_allocated 22512.80859375 
[2025-02-23 09:10:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.025879304856061935 norm:0.005682847928255796 max memory_allocated 22512.80859375 
[2025-02-23 09:10:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.025365639477968216 norm:0.005038558505475521 max memory_allocated 22512.80859375 
[2025-02-23 09:11:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.025623589754104614 norm:0.009439115412533283 max memory_allocated 22512.80859375 
[2025-02-23 09:11:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.025189749896526337 norm:0.0052469754591584206 max memory_allocated 22512.80859375 
[2025-02-23 09:12:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.02498788945376873 norm:0.004167428705841303 max memory_allocated 22512.80859375 
[2025-02-23 09:12:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.025334514677524567 norm:0.0076510668732225895 max memory_allocated 22512.80859375 
[2025-02-23 09:13:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.025809455662965775 norm:0.00676322728395462 max memory_allocated 22512.80859375 
[2025-02-23 09:13:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.024973182007670403 norm:0.005673028528690338 max memory_allocated 22512.80859375 
[2025-02-23 09:14:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.024951323866844177 norm:0.005816787946969271 max memory_allocated 22512.80859375 
[2025-02-23 09:14:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.02449587732553482 norm:0.0051893689669668674 max memory_allocated 22512.80859375 
[2025-02-23 09:15:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.02446633018553257 norm:0.00730915367603302 max memory_allocated 22512.80859375 
[2025-02-23 09:15:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-23 09:16:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.03817048296332359 norm:0.006797821260988712 max memory_allocated 22512.98046875 
[2025-02-23 09:16:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.031241856515407562 norm:0.002005006419494748 max memory_allocated 22512.98046875 
[2025-02-23 09:17:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.029870295897126198 norm:0.00123729114420712 max memory_allocated 22512.98046875 
[2025-02-23 09:17:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.02943066880106926 norm:0.0007838861201889813 max memory_allocated 22512.98046875 
[2025-02-23 09:18:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.02906298264861107 norm:0.000594451033975929 max memory_allocated 22512.98046875 
[2025-02-23 09:18:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.02895236574113369 norm:0.0003534845309332013 max memory_allocated 22512.98046875 
[2025-02-23 09:19:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.02875100076198578 norm:0.00035509432200342417 max memory_allocated 22512.98046875 
[2025-02-23 09:20:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.028745310381054878 norm:0.0002875693317037076 max memory_allocated 22512.98046875 
[2025-02-23 09:20:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.028695758432149887 norm:0.0002623168984428048 max memory_allocated 22512.98046875 
[2025-02-23 09:21:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.028608452528715134 norm:0.0002346623659832403 max memory_allocated 22512.98046875 
[2025-02-23 09:21:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.028175659477710724 norm:0.00023975054500624537 max memory_allocated 22512.98046875 
[2025-02-23 09:22:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.028265610337257385 norm:0.0002625077613629401 max memory_allocated 22512.98046875 
[2025-02-23 09:22:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.02808566391468048 norm:0.00020213348034303635 max memory_allocated 22512.98046875 
[2025-02-23 09:23:16 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.028088225051760674 norm:0.00018618225294630975 max memory_allocated 22512.98046875 
[2025-02-23 09:23:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.02802847884595394 norm:0.00018302751414012164 max memory_allocated 22512.98046875 
[2025-02-23 09:24:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.02813929133117199 norm:0.00018703915702644736 max memory_allocated 22512.98046875 
[2025-02-23 09:24:54 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.027936726808547974 norm:0.00017403218953404576 max memory_allocated 22512.98046875 
[2025-02-23 09:25:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.028047660365700722 norm:0.00018672570877242833 max memory_allocated 22512.98046875 
[2025-02-23 09:26:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.027839092537760735 norm:0.00017356936587020755 max memory_allocated 22512.98046875 
[2025-02-23 09:26:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.02805180847644806 norm:0.00018343261035624892 max memory_allocated 22512.98046875 
[2025-02-23 09:26:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-23 09:27:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.04464840516448021 norm:0.003921055234968662 max memory_allocated 22513.15234375 
[2025-02-23 09:27:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.03843897581100464 norm:0.0007539960206486285 max memory_allocated 22513.15234375 
[2025-02-23 09:28:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.03675107657909393 norm:0.00040374798118136823 max memory_allocated 22513.15234375 
[2025-02-23 09:28:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0359518937766552 norm:0.0002963356382679194 max memory_allocated 22513.15234375 
[2025-02-23 09:29:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.035947442054748535 norm:0.00026093493215739727 max memory_allocated 22513.15234375 
[2025-02-23 09:30:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.03588331490755081 norm:0.00022999875363893807 max memory_allocated 22513.15234375 
[2025-02-23 09:30:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.03590238094329834 norm:0.0002036189252976328 max memory_allocated 22513.15234375 
[2025-02-23 09:31:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.036229077726602554 norm:0.00022864181664772332 max memory_allocated 22513.15234375 
[2025-02-23 09:31:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.03629923611879349 norm:0.00025169781292788684 max memory_allocated 22513.15234375 
[2025-02-23 09:32:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0360451340675354 norm:0.00023980368860065937 max memory_allocated 22513.15234375 
[2025-02-23 09:32:44 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.03616834431886673 norm:0.00023981691629160196 max memory_allocated 22513.15234375 
[2025-02-23 09:33:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.03605097904801369 norm:0.00023839762434363365 max memory_allocated 22513.15234375 
[2025-02-23 09:33:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0362306572496891 norm:0.0002285812224727124 max memory_allocated 22513.15234375 
[2025-02-23 09:34:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.03608958423137665 norm:0.0002382941747782752 max memory_allocated 22513.15234375 
[2025-02-23 09:34:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.035745859146118164 norm:0.00024174836289603263 max memory_allocated 22513.15234375 
[2025-02-23 09:35:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.03586383908987045 norm:0.0002314829034730792 max memory_allocated 22513.15234375 
[2025-02-23 09:36:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.035583846271038055 norm:0.00023776950547471642 max memory_allocated 22513.15234375 
[2025-02-23 09:36:33 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.03555997461080551 norm:0.00022868516680318862 max memory_allocated 22513.15234375 
[2025-02-23 09:37:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.03571319207549095 norm:0.00023662310559302568 max memory_allocated 22513.15234375 
[2025-02-23 09:37:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.035948608070611954 norm:0.00024204381043091416 max memory_allocated 22513.15234375 
[2025-02-23 09:37:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-23 09:38:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.05269014090299606 norm:0.001894808723591268 max memory_allocated 22513.32421875 
[2025-02-23 09:38:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.046599384397268295 norm:0.0006853829836472869 max memory_allocated 22513.32421875 
[2025-02-23 09:39:29 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0443502776324749 norm:0.00038354043499566615 max memory_allocated 22513.32421875 
[2025-02-23 09:40:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.04304981976747513 norm:0.0002612119715195149 max memory_allocated 22513.32421875 
[2025-02-23 09:40:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.04241972044110298 norm:0.0002060849074041471 max memory_allocated 22513.32421875 
[2025-02-23 09:41:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0420951321721077 norm:0.0001815840369090438 max memory_allocated 22513.32421875 
[2025-02-23 09:41:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.04206114262342453 norm:0.0001609581522643566 max memory_allocated 22513.32421875 
[2025-02-23 09:42:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.04216882213950157 norm:0.00016702085849829018 max memory_allocated 22513.32421875 
[2025-02-23 09:42:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.04216499254107475 norm:0.00016870046965777874 max memory_allocated 22513.32421875 
[2025-02-23 09:43:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.042231716215610504 norm:0.0001724444591673091 max memory_allocated 22513.32421875 
[2025-02-23 09:43:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.04233740270137787 norm:0.00017009374278131872 max memory_allocated 22513.32421875 
[2025-02-23 09:44:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.04223696142435074 norm:0.00016905141819734126 max memory_allocated 22513.32421875 
[2025-02-23 09:44:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.042314428836107254 norm:0.0001630019978620112 max memory_allocated 22513.32421875 
[2025-02-23 09:45:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.04239359870553017 norm:0.0001767986686900258 max memory_allocated 22513.32421875 
[2025-02-23 09:46:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.04227961599826813 norm:0.00016692641656845808 max memory_allocated 22513.32421875 
[2025-02-23 09:46:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.04211774095892906 norm:0.00016993778990581632 max memory_allocated 22513.32421875 
[2025-02-23 09:47:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.04212401807308197 norm:0.00015950249508023262 max memory_allocated 22513.32421875 
[2025-02-23 09:47:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.04206245765089989 norm:0.0001662001886870712 max memory_allocated 22513.32421875 
[2025-02-23 09:48:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.04198826849460602 norm:0.0001668956538196653 max memory_allocated 22513.32421875 
[2025-02-23 09:48:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.04202625900506973 norm:0.00016525744285900146 max memory_allocated 22513.32421875 
[2025-02-23 09:48:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-23 09:49:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.054742734879255295 norm:0.0015031505608931184 max memory_allocated 22513.49609375 
[2025-02-23 09:50:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.04970994591712952 norm:0.0004959018551744521 max memory_allocated 22513.49609375 
[2025-02-23 09:50:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.047956496477127075 norm:0.0002559863787610084 max memory_allocated 22513.49609375 
[2025-02-23 09:51:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.04689965024590492 norm:0.00018540778546594083 max memory_allocated 22513.49609375 
[2025-02-23 09:51:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.04617500677704811 norm:0.00016617179790046066 max memory_allocated 22513.49609375 
[2025-02-23 09:52:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0459873266518116 norm:0.00015131509280763566 max memory_allocated 22513.49609375 
[2025-02-23 09:52:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.046251993626356125 norm:0.00015751294267829508 max memory_allocated 22513.49609375 
[2025-02-23 09:53:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.04623272269964218 norm:0.00013923048391006887 max memory_allocated 22513.49609375 
[2025-02-23 09:53:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0459991917014122 norm:0.0001431434793630615 max memory_allocated 22513.49609375 
[2025-02-23 09:54:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.04565364867448807 norm:0.00013245712034404278 max memory_allocated 22513.49609375 
[2025-02-23 09:54:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.045603930950164795 norm:0.0001294954854529351 max memory_allocated 22513.49609375 
[2025-02-23 09:55:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.04565523564815521 norm:0.00014223316975403577 max memory_allocated 22513.49609375 
[2025-02-23 09:56:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.04570179432630539 norm:0.00013032338756602257 max memory_allocated 22513.49609375 
[2025-02-23 09:56:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.04575479030609131 norm:0.00013824168127030134 max memory_allocated 22513.49609375 
[2025-02-23 09:57:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.04576714709401131 norm:0.00012747083383146673 max memory_allocated 22513.49609375 
[2025-02-23 09:57:40 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.045737870037555695 norm:0.0001414786238456145 max memory_allocated 22513.49609375 
[2025-02-23 09:58:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.045761946588754654 norm:0.00013570352166425437 max memory_allocated 22513.49609375 
[2025-02-23 09:58:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.04578585550189018 norm:0.0001388210366712883 max memory_allocated 22513.49609375 
[2025-02-23 09:59:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.04574526846408844 norm:0.00014126885798759758 max memory_allocated 22513.49609375 
[2025-02-23 09:59:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.04570803791284561 norm:0.0001328477228526026 max memory_allocated 22513.49609375 
[2025-02-23 10:00:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-23 10:00:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.063722625374794 norm:0.0008600670844316483 max memory_allocated 22513.66796875 
[2025-02-23 10:01:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.05704450607299805 norm:0.0004221643612254411 max memory_allocated 22513.66796875 
[2025-02-23 10:01:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.0543133020401001 norm:0.00030707899713888764 max memory_allocated 22513.66796875 
[2025-02-23 10:02:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.052878379821777344 norm:0.00023141669225879014 max memory_allocated 22513.66796875 
[2025-02-23 10:02:47 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.05226755142211914 norm:0.0002116430114256218 max memory_allocated 22513.66796875 
[2025-02-23 10:03:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.05159759521484375 norm:0.00017895283235702664 max memory_allocated 22513.66796875 
[2025-02-23 10:03:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.051262594759464264 norm:0.00015997461741790175 max memory_allocated 22513.66796875 
[2025-02-23 10:04:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.051672834903001785 norm:0.00015649398847017437 max memory_allocated 22513.66796875 
[2025-02-23 10:04:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.05175141990184784 norm:0.000157641465193592 max memory_allocated 22513.66796875 
[2025-02-23 10:05:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.051575884222984314 norm:0.00014349716366268694 max memory_allocated 22513.66796875 
[2025-02-23 10:06:03 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.051495395600795746 norm:0.00014579838898498565 max memory_allocated 22513.66796875 
[2025-02-23 10:06:36 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.05166652426123619 norm:0.00014555272355210036 max memory_allocated 22513.66796875 
[2025-02-23 10:07:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.05183436721563339 norm:0.0001769986847648397 max memory_allocated 22513.66796875 
[2025-02-23 10:07:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.05182632803916931 norm:0.00014869438018649817 max memory_allocated 22513.66796875 
[2025-02-23 10:08:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.051983848214149475 norm:0.00015576061559841037 max memory_allocated 22513.66796875 
[2025-02-23 10:08:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.05206211656332016 norm:0.00016014355060178787 max memory_allocated 22513.66796875 
[2025-02-23 10:09:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.052099067717790604 norm:0.00016604330448899418 max memory_allocated 22513.66796875 
[2025-02-23 10:09:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0520748533308506 norm:0.00017229828517884016 max memory_allocated 22513.66796875 
[2025-02-23 10:10:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.052073486149311066 norm:0.00016250304179266095 max memory_allocated 22513.66796875 
[2025-02-23 10:10:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.05217858403921127 norm:0.00017586592002771795 max memory_allocated 22513.66796875 
[2025-02-23 10:11:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-23 10:11:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.07107838988304138 norm:0.0020105927251279354 max memory_allocated 22513.83984375 
[2025-02-23 10:12:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.06363830715417862 norm:0.002036368940025568 max memory_allocated 22513.83984375 
[2025-02-23 10:12:47 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.06074630469083786 norm:0.0006076730787754059 max memory_allocated 22513.83984375 
[2025-02-23 10:13:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.05896858870983124 norm:0.0004055883036926389 max memory_allocated 22513.83984375 
[2025-02-23 10:13:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0580410435795784 norm:0.00034473027335479856 max memory_allocated 22513.83984375 
[2025-02-23 10:14:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.05725735053420067 norm:0.00027377071091905236 max memory_allocated 22513.83984375 
[2025-02-23 10:14:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.056957535445690155 norm:0.00023622567823622376 max memory_allocated 22513.83984375 
[2025-02-23 10:15:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.05654742941260338 norm:0.00019989212159998715 max memory_allocated 22513.83984375 
[2025-02-23 10:16:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.056571412831544876 norm:0.00019694237562362105 max memory_allocated 22513.83984375 
[2025-02-23 10:16:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.05642373487353325 norm:0.00018068638746626675 max memory_allocated 22513.83984375 
[2025-02-23 10:17:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.056584760546684265 norm:0.0001708958443487063 max memory_allocated 22513.83984375 
[2025-02-23 10:17:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.056698426604270935 norm:0.00015809877368155867 max memory_allocated 22513.83984375 
[2025-02-23 10:18:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0563807487487793 norm:0.00014131373609416187 max memory_allocated 22513.83984375 
[2025-02-23 10:18:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.05638532340526581 norm:0.00013341227895580232 max memory_allocated 22513.83984375 
[2025-02-23 10:19:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.05608564615249634 norm:0.00011526407615747303 max memory_allocated 22513.83984375 
[2025-02-23 10:19:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.056127239018678665 norm:0.00010856612789211795 max memory_allocated 22513.83984375 
[2025-02-23 10:20:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.05631884187459946 norm:0.00011060469842050225 max memory_allocated 22513.83984375 
[2025-02-23 10:20:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.056610703468322754 norm:0.00011626474588410929 max memory_allocated 22513.83984375 
[2025-02-23 10:21:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.056815750896930695 norm:0.00011885464482475072 max memory_allocated 22513.83984375 
[2025-02-23 10:22:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.05709271878004074 norm:0.0001230606867466122 max memory_allocated 22513.83984375 
[2025-02-23 10:22:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-23 10:22:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.07248669117689133 norm:0.0007753445534035563 max memory_allocated 22514.01171875 
[2025-02-23 10:23:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.06689396500587463 norm:0.0007990756421349943 max memory_allocated 22514.01171875 
[2025-02-23 10:23:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06451448798179626 norm:0.00036373050534166396 max memory_allocated 22514.01171875 
[2025-02-23 10:24:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.06303570419549942 norm:0.00026083827833645046 max memory_allocated 22514.01171875 
[2025-02-23 10:24:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.062418289482593536 norm:0.0002184537152061239 max memory_allocated 22514.01171875 
[2025-02-23 10:25:30 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.062022045254707336 norm:0.00019649464229587466 max memory_allocated 22514.01171875 
[2025-02-23 10:26:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.06193504482507706 norm:0.00019091449212282896 max memory_allocated 22514.01171875 
[2025-02-23 10:26:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0618414506316185 norm:0.0001748286304064095 max memory_allocated 22514.01171875 
[2025-02-23 10:27:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.06160055100917816 norm:0.00015973210975062102 max memory_allocated 22514.01171875 
[2025-02-23 10:27:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0615122988820076 norm:0.00015720004739705473 max memory_allocated 22514.01171875 
[2025-02-23 10:28:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.06145007535815239 norm:0.000146063175634481 max memory_allocated 22514.01171875 
[2025-02-23 10:28:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.06130407005548477 norm:0.00013847187801729888 max memory_allocated 22514.01171875 
[2025-02-23 10:29:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.061160773038864136 norm:0.00011412370804464445 max memory_allocated 22514.01171875 
[2025-02-23 10:29:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.06124085932970047 norm:0.00010935481986962259 max memory_allocated 22514.01171875 
[2025-02-23 10:30:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.061217375099658966 norm:0.00010458503675181419 max memory_allocated 22514.01171875 
[2025-02-23 10:30:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.061214737594127655 norm:0.00010410680260974914 max memory_allocated 22514.01171875 
[2025-02-23 10:31:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.061251476407051086 norm:0.00010507021215744317 max memory_allocated 22514.01171875 
[2025-02-23 10:32:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.06126420572400093 norm:0.00010675120574887842 max memory_allocated 22514.01171875 
[2025-02-23 10:32:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.06136573478579521 norm:0.00011840010847663507 max memory_allocated 22514.01171875 
[2025-02-23 10:33:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0613967590034008 norm:0.0001045793978846632 max memory_allocated 22514.01171875 
[2025-02-23 10:33:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-23 10:33:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.08091428130865097 norm:0.002422010526061058 max memory_allocated 22514.18359375 
[2025-02-23 10:34:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.07306107133626938 norm:0.0011126867029815912 max memory_allocated 22514.18359375 
[2025-02-23 10:34:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0705869048833847 norm:0.000969357555732131 max memory_allocated 22514.18359375 
[2025-02-23 10:35:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.06889595836400986 norm:0.000719703733921051 max memory_allocated 22514.18359375 
[2025-02-23 10:36:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.06774865090847015 norm:0.0005822267848998308 max memory_allocated 22514.18359375 
[2025-02-23 10:36:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.06709281355142593 norm:0.00048344142851419747 max memory_allocated 22514.18359375 
[2025-02-23 10:37:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.06659151613712311 norm:0.00043605529936030507 max memory_allocated 22514.18359375 
[2025-02-23 10:37:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.06604673713445663 norm:0.00035657433909364045 max memory_allocated 22514.18359375 
[2025-02-23 10:38:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0657513365149498 norm:0.00031537128961645067 max memory_allocated 22514.18359375 
[2025-02-23 10:38:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.06566325575113297 norm:0.0002966562460642308 max memory_allocated 22514.18359375 
[2025-02-23 10:39:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0651707798242569 norm:0.00025768307386897504 max memory_allocated 22514.18359375 
[2025-02-23 10:39:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.06497878581285477 norm:0.0002340545179322362 max memory_allocated 22514.18359375 
[2025-02-23 10:40:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.06528835743665695 norm:0.00022889673709869385 max memory_allocated 22514.18359375 
[2025-02-23 10:40:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.06526851654052734 norm:0.00020880215743090957 max memory_allocated 22514.18359375 
[2025-02-23 10:41:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.06539016962051392 norm:0.0001931730075739324 max memory_allocated 22514.18359375 
[2025-02-23 10:42:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.06544658541679382 norm:0.000178116126335226 max memory_allocated 22514.18359375 
[2025-02-23 10:42:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.06528806686401367 norm:0.00016081910871434957 max memory_allocated 22514.18359375 
[2025-02-23 10:43:06 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06528240442276001 norm:0.00015920989972073585 max memory_allocated 22514.18359375 
[2025-02-23 10:43:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.06521907448768616 norm:0.00014646546333096921 max memory_allocated 22514.18359375 
[2025-02-23 10:44:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.06487749516963959 norm:0.00011619612632784992 max memory_allocated 22514.18359375 
[2025-02-23 10:44:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-23 10:44:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.08420854806900024 norm:0.0011990922503173351 max memory_allocated 22514.35546875 
[2025-02-23 10:45:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.07800866663455963 norm:0.0008226633071899414 max memory_allocated 22514.35546875 
[2025-02-23 10:46:02 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.07565897703170776 norm:0.0007001092890277505 max memory_allocated 22514.35546875 
[2025-02-23 10:46:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.07429051399230957 norm:0.0005937456153333187 max memory_allocated 22514.35546875 
[2025-02-23 10:47:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.07280491292476654 norm:0.0005260963225737214 max memory_allocated 22514.35546875 
[2025-02-23 10:47:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0723496526479721 norm:0.0004973451723344624 max memory_allocated 22514.35546875 
[2025-02-23 10:48:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.07157351821660995 norm:0.000456280104117468 max memory_allocated 22514.35546875 
[2025-02-23 10:48:45 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.07126067578792572 norm:0.0004259916895534843 max memory_allocated 22514.35546875 
[2025-02-23 10:49:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.07111731916666031 norm:0.0003875000693369657 max memory_allocated 22514.35546875 
[2025-02-23 10:49:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.07106263935565948 norm:0.0004229801706969738 max memory_allocated 22514.35546875 
[2025-02-23 10:50:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.07082156836986542 norm:0.00028313061920925975 max memory_allocated 22514.35546875 
[2025-02-23 10:50:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.07170210778713226 norm:0.00026296262512914836 max memory_allocated 22514.35546875 
[2025-02-23 10:51:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.07191469520330429 norm:0.0002685667132027447 max memory_allocated 22514.35546875 
[2025-02-23 10:52:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0710090920329094 norm:0.00024821353144943714 max memory_allocated 22514.35546875 
[2025-02-23 10:52:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.06997432559728622 norm:0.00022874513524584472 max memory_allocated 22514.35546875 
[2025-02-23 10:53:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.06950297951698303 norm:0.00021172301785554737 max memory_allocated 22514.35546875 
[2025-02-23 10:53:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.06931295245885849 norm:0.00019510563288349658 max memory_allocated 22514.35546875 
[2025-02-23 10:54:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.06912852823734283 norm:0.00016665893781464547 max memory_allocated 22514.35546875 
[2025-02-23 10:54:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.06908724457025528 norm:0.000144950085086748 max memory_allocated 22514.35546875 
[2025-02-23 10:55:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.069265216588974 norm:0.0001129981828853488 max memory_allocated 22514.35546875 
[2025-02-23 10:55:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-23 10:56:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.08672051131725311 norm:0.0015458560083061457 max memory_allocated 22514.52734375 
[2025-02-23 10:56:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.08009664714336395 norm:0.0009646319667808712 max memory_allocated 22514.52734375 
[2025-02-23 10:57:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.07707469165325165 norm:0.0007331130909733474 max memory_allocated 22514.52734375 
[2025-02-23 10:57:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.07512906938791275 norm:0.0006037874845787883 max memory_allocated 22514.52734375 
[2025-02-23 10:58:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.07404769957065582 norm:0.0005277374875731766 max memory_allocated 22514.52734375 
[2025-02-23 10:58:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.07349991053342819 norm:0.0004634995711967349 max memory_allocated 22514.52734375 
[2025-02-23 10:59:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.07317056506872177 norm:0.0004406390944495797 max memory_allocated 22514.52734375 
[2025-02-23 10:59:48 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.07260112464427948 norm:0.0003820376005023718 max memory_allocated 22514.52734375 
[2025-02-23 11:00:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.07234998792409897 norm:0.00034883033367805183 max memory_allocated 22514.52734375 
[2025-02-23 11:00:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.07226112484931946 norm:0.000319849030347541 max memory_allocated 22514.52734375 
[2025-02-23 11:01:26 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.07230765372514725 norm:0.000282452383544296 max memory_allocated 22514.52734375 
[2025-02-23 11:01:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0721844807267189 norm:0.00026428417186252773 max memory_allocated 22514.52734375 
[2025-02-23 11:02:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.07209702581167221 norm:0.00024208921240642667 max memory_allocated 22514.52734375 
[2025-02-23 11:03:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.07191462814807892 norm:0.000232814927585423 max memory_allocated 22514.52734375 
[2025-02-23 11:03:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.07180403172969818 norm:0.00022821890888735652 max memory_allocated 22514.52734375 
[2025-02-23 11:04:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.07176730036735535 norm:0.0002068378234980628 max memory_allocated 22514.52734375 
[2025-02-23 11:04:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.07183200120925903 norm:0.0001965871633728966 max memory_allocated 22514.52734375 
[2025-02-23 11:05:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.07188021391630173 norm:0.00016278431576211005 max memory_allocated 22514.52734375 
[2025-02-23 11:05:47 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.07172466069459915 norm:0.00013060869241598994 max memory_allocated 22514.52734375 
[2025-02-23 11:06:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.0716034546494484 norm:0.00011219599400646985 max memory_allocated 22514.52734375 
[2025-02-23 11:06:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-23 11:07:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.08489722013473511 norm:0.0009080469026230276 max memory_allocated 22514.69921875 
[2025-02-23 11:07:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.07953754812479019 norm:0.0006266726413741708 max memory_allocated 22514.69921875 
[2025-02-23 11:08:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.07704490423202515 norm:0.0004992217291146517 max memory_allocated 22514.69921875 
[2025-02-23 11:08:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.07583136111497879 norm:0.0004216735833324492 max memory_allocated 22514.69921875 
[2025-02-23 11:09:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.07495326548814774 norm:0.00036691187415272 max memory_allocated 22514.69921875 
[2025-02-23 11:09:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.07436227053403854 norm:0.00032391585409641266 max memory_allocated 22514.69921875 
[2025-02-23 11:10:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.07407701760530472 norm:0.00030014896765351295 max memory_allocated 22514.69921875 
[2025-02-23 11:10:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.07395845651626587 norm:0.00028568063862621784 max memory_allocated 22514.69921875 
[2025-02-23 11:11:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.07377196848392487 norm:0.0002619558072183281 max memory_allocated 22514.69921875 
[2025-02-23 11:11:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.07340344786643982 norm:0.0002209414087701589 max memory_allocated 22514.69921875 
[2025-02-23 11:12:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0733683854341507 norm:0.00018710097356233746 max memory_allocated 22514.69921875 
[2025-02-23 11:13:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.07335200905799866 norm:0.00015467505727428943 max memory_allocated 22514.69921875 
[2025-02-23 11:13:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.07317189872264862 norm:0.00014578821719624102 max memory_allocated 22514.69921875 
[2025-02-23 11:14:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.07325390726327896 norm:0.0001316509151365608 max memory_allocated 22514.69921875 
[2025-02-23 11:14:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.07318805903196335 norm:0.00011050713510485366 max memory_allocated 22514.69921875 
[2025-02-23 11:15:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.07304346561431885 norm:9.951574611477554e-05 max memory_allocated 22514.69921875 
[2025-02-23 11:15:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.07297743111848831 norm:8.399537182413042e-05 max memory_allocated 22514.69921875 
[2025-02-23 11:16:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.07299564778804779 norm:7.939994247863069e-05 max memory_allocated 22514.69921875 
[2025-02-23 11:16:50 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0729573667049408 norm:8.080952829914168e-05 max memory_allocated 22514.69921875 
[2025-02-23 11:17:23 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0729435384273529 norm:7.678512338316068e-05 max memory_allocated 22514.69921875 
[2025-02-23 11:17:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-23 11:18:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.08337965607643127 norm:0.001628449885174632 max memory_allocated 22514.87109375 
[2025-02-23 11:18:40 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.07804786413908005 norm:0.0008042279514484107 max memory_allocated 22514.87109375 
[2025-02-23 11:19:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.0756041407585144 norm:0.0005760823842138052 max memory_allocated 22514.87109375 
[2025-02-23 11:19:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.07438431680202484 norm:0.0004677525721490383 max memory_allocated 22514.87109375 
[2025-02-23 11:20:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.0735512301325798 norm:0.0003645826654974371 max memory_allocated 22514.87109375 
[2025-02-23 11:20:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.07292057573795319 norm:0.0002932919596787542 max memory_allocated 22514.87109375 
[2025-02-23 11:21:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.07248632609844208 norm:0.0002580388099886477 max memory_allocated 22514.87109375 
[2025-02-23 11:21:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.07217356562614441 norm:0.00022262736456468701 max memory_allocated 22514.87109375 
[2025-02-23 11:22:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.07195606827735901 norm:0.00020449141447898 max memory_allocated 22514.87109375 
[2025-02-23 11:23:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.07177734375 norm:0.00017687010404188186 max memory_allocated 22514.87109375 
[2025-02-23 11:23:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.07177108526229858 norm:0.000164209573995322 max memory_allocated 22514.87109375 
[2025-02-23 11:24:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.07164456695318222 norm:0.0001490748254582286 max memory_allocated 22514.87109375 
[2025-02-23 11:24:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.07159595191478729 norm:0.00013335935364011675 max memory_allocated 22514.87109375 
[2025-02-23 11:25:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.0716467797756195 norm:0.00012741121463477612 max memory_allocated 22514.87109375 
[2025-02-23 11:25:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.07163634896278381 norm:0.00010910562559729442 max memory_allocated 22514.87109375 
[2025-02-23 11:26:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.07169865816831589 norm:9.829920600168407e-05 max memory_allocated 22514.87109375 
[2025-02-23 11:26:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0716630220413208 norm:9.133985440712422e-05 max memory_allocated 22514.87109375 
[2025-02-23 11:27:21 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.07165710628032684 norm:8.559679554309696e-05 max memory_allocated 22514.87109375 
[2025-02-23 11:27:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.07158862054347992 norm:8.187713683582842e-05 max memory_allocated 22514.87109375 
[2025-02-23 11:28:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.07154384255409241 norm:7.383256161119789e-05 max memory_allocated 22514.87109375 
[2025-02-23 11:28:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-23 11:29:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.08241862058639526 norm:0.0006874167593196034 max memory_allocated 22515.04296875 
[2025-02-23 11:29:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.07801519334316254 norm:0.00041271207737736404 max memory_allocated 22515.04296875 
[2025-02-23 11:30:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0759282261133194 norm:0.00032786361407488585 max memory_allocated 22515.04296875 
[2025-02-23 11:30:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.07482651621103287 norm:0.0002807363634929061 max memory_allocated 22515.04296875 
[2025-02-23 11:31:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.07414322346448898 norm:0.00022966833785176277 max memory_allocated 22515.04296875 
[2025-02-23 11:31:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.07377302646636963 norm:0.0002078362158499658 max memory_allocated 22515.04296875 
[2025-02-23 11:32:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0733424499630928 norm:0.00018972871475853026 max memory_allocated 22515.04296875 
[2025-02-23 11:32:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0731067955493927 norm:0.00015928212087601423 max memory_allocated 22515.04296875 
[2025-02-23 11:33:32 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.07312225550413132 norm:0.0001476370234740898 max memory_allocated 22515.04296875 
[2025-02-23 11:34:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0730644166469574 norm:0.00014001874660607427 max memory_allocated 22515.04296875 
[2025-02-23 11:34:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.07298186421394348 norm:0.00012641675129998475 max memory_allocated 22515.04296875 
[2025-02-23 11:35:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.07272420823574066 norm:0.00011646741040749475 max memory_allocated 22515.04296875 
[2025-02-23 11:35:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.07265979796648026 norm:9.718882211018354e-05 max memory_allocated 22515.04296875 
[2025-02-23 11:36:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.07264673709869385 norm:8.285217336378992e-05 max memory_allocated 22515.04296875 
[2025-02-23 11:36:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.07272408157587051 norm:7.521595398429781e-05 max memory_allocated 22515.04296875 
[2025-02-23 11:37:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.07271058857440948 norm:7.303308666450903e-05 max memory_allocated 22515.04296875 
[2025-02-23 11:37:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.07270386070013046 norm:7.066996477078646e-05 max memory_allocated 22515.04296875 
[2025-02-23 11:38:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.07276738435029984 norm:7.239333353936672e-05 max memory_allocated 22515.04296875 
[2025-02-23 11:38:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.07278349995613098 norm:7.125573029043153e-05 max memory_allocated 22515.04296875 
[2025-02-23 11:39:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.07283371686935425 norm:7.209721661638469e-05 max memory_allocated 22515.04296875 
[2025-02-23 11:39:39 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-23 11:40:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.08567173779010773 norm:0.0014677213039249182 max memory_allocated 22515.21484375 
[2025-02-23 11:40:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.07967309653759003 norm:0.0006482827011495829 max memory_allocated 22515.21484375 
[2025-02-23 11:41:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.07720153778791428 norm:0.000473207765026018 max memory_allocated 22515.21484375 
[2025-02-23 11:41:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.07568343728780746 norm:0.0003293444460723549 max memory_allocated 22515.21484375 
[2025-02-23 11:42:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.07445879280567169 norm:0.0002458448288962245 max memory_allocated 22515.21484375 
[2025-02-23 11:42:57 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.07369302213191986 norm:0.0002213930565631017 max memory_allocated 22515.21484375 
[2025-02-23 11:43:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.07319236546754837 norm:0.0002084398438455537 max memory_allocated 22515.21484375 
[2025-02-23 11:44:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.07283981889486313 norm:0.00018953804101329297 max memory_allocated 22515.21484375 
[2025-02-23 11:44:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.07255109399557114 norm:0.00017167274199891835 max memory_allocated 22515.21484375 
[2025-02-23 11:45:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.07257054001092911 norm:0.00015518517466261983 max memory_allocated 22515.21484375 
[2025-02-23 11:45:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.07235239446163177 norm:0.00014387638657353818 max memory_allocated 22515.21484375 
[2025-02-23 11:46:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.07238695025444031 norm:0.0001363602641504258 max memory_allocated 22515.21484375 
[2025-02-23 11:46:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.07240674644708633 norm:0.00012965893256478012 max memory_allocated 22515.21484375 
[2025-02-23 11:47:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.07246774435043335 norm:0.00011928490130230784 max memory_allocated 22515.21484375 
[2025-02-23 11:47:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.07249225676059723 norm:0.00011179017019458115 max memory_allocated 22515.21484375 
[2025-02-23 11:48:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.07236730307340622 norm:0.0001023211152642034 max memory_allocated 22515.21484375 
[2025-02-23 11:48:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.07219594717025757 norm:9.571221744408831e-05 max memory_allocated 22515.21484375 
[2025-02-23 11:49:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.07202167809009552 norm:8.946668822318316e-05 max memory_allocated 22515.21484375 
[2025-02-23 11:50:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0721026211977005 norm:8.206389611586928e-05 max memory_allocated 22515.21484375 
[2025-02-23 11:50:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.07192429155111313 norm:7.940876093925908e-05 max memory_allocated 22515.21484375 
[2025-02-23 11:50:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-23 11:51:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.0866936594247818 norm:0.0017876852070912719 max memory_allocated 22515.38671875 
[2025-02-23 11:51:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.08070018142461777 norm:0.000737043796107173 max memory_allocated 22515.38671875 
[2025-02-23 11:52:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.07764428108930588 norm:0.00043353723594918847 max memory_allocated 22515.38671875 
[2025-02-23 11:52:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.07625677436590195 norm:0.00034422081080265343 max memory_allocated 22515.38671875 
[2025-02-23 11:53:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.07524542510509491 norm:0.0002657232980709523 max memory_allocated 22515.38671875 
[2025-02-23 11:54:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.07459461688995361 norm:0.00022430733952205628 max memory_allocated 22515.38671875 
[2025-02-23 11:54:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.07422193884849548 norm:0.00022168055875226855 max memory_allocated 22515.38671875 
[2025-02-23 11:55:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0739564374089241 norm:0.0001882610667962581 max memory_allocated 22515.38671875 
[2025-02-23 11:55:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.07360143214464188 norm:0.00016140341176651418 max memory_allocated 22515.38671875 
[2025-02-23 11:56:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.07346373051404953 norm:0.00014916539657860994 max memory_allocated 22515.38671875 
[2025-02-23 11:56:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.07351472973823547 norm:0.00013873663556296378 max memory_allocated 22515.38671875 
[2025-02-23 11:57:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.07345180213451385 norm:0.00012572144623845816 max memory_allocated 22515.38671875 
[2025-02-23 11:57:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.07328307628631592 norm:0.000114233567728661 max memory_allocated 22515.38671875 
[2025-02-23 11:58:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0731792151927948 norm:0.00011132207146147266 max memory_allocated 22515.38671875 
[2025-02-23 11:58:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.07316307723522186 norm:0.00010187421867158264 max memory_allocated 22515.38671875 
[2025-02-23 11:59:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.07309973984956741 norm:9.391360072186217e-05 max memory_allocated 22515.38671875 
[2025-02-23 11:59:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.07319413125514984 norm:9.374687215313315e-05 max memory_allocated 22515.38671875 
[2025-02-23 12:00:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.07319319993257523 norm:8.468201122013852e-05 max memory_allocated 22515.38671875 
[2025-02-23 12:01:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.07304652035236359 norm:8.368388807866722e-05 max memory_allocated 22515.38671875 
[2025-02-23 12:01:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.07303136587142944 norm:8.009924931684509e-05 max memory_allocated 22515.38671875 
[2025-02-23 12:01:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-23 12:02:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.08416904509067535 norm:0.00154811330139637 max memory_allocated 22515.55859375 
[2025-02-23 12:02:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0796893835067749 norm:0.0005796451005153358 max memory_allocated 22515.55859375 
[2025-02-23 12:03:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.07796537131071091 norm:0.00040224468102678657 max memory_allocated 22515.55859375 
[2025-02-23 12:03:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.07668071985244751 norm:0.000265291309915483 max memory_allocated 22515.55859375 
[2025-02-23 12:04:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.07597818970680237 norm:0.0002153862442355603 max memory_allocated 22515.55859375 
[2025-02-23 12:05:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.07540982216596603 norm:0.0001570525928400457 max memory_allocated 22515.55859375 
[2025-02-23 12:05:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.07499637454748154 norm:0.00014306837692856789 max memory_allocated 22515.55859375 
[2025-02-23 12:06:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.07491053640842438 norm:0.00012739549856632948 max memory_allocated 22515.55859375 
[2025-02-23 12:06:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.07492335885763168 norm:0.0001111971796490252 max memory_allocated 22515.55859375 
[2025-02-23 12:07:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.07481059432029724 norm:9.624594531487674e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:07:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.07472783327102661 norm:8.654098201077431e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:08:19 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.07494258135557175 norm:8.625039481557906e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:08:52 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.07504193484783173 norm:8.562285802327096e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:09:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.07494650036096573 norm:8.512011845596135e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:09:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.07498729228973389 norm:8.634335245005786e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:10:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.07471964508295059 norm:8.11491918284446e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:11:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.07489760220050812 norm:8.531149796908721e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:11:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.07488735020160675 norm:8.404326217714697e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:12:07 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.07488393038511276 norm:8.388897549593821e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:12:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.07490324974060059 norm:8.420336962444708e-05 max memory_allocated 22515.55859375 
[2025-02-23 12:12:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-23 12:13:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.09218144416809082 norm:0.00131139624863863 max memory_allocated 22515.73046875 
[2025-02-23 12:13:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.08792060613632202 norm:0.0005442668916657567 max memory_allocated 22515.73046875 
[2025-02-23 12:14:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.0860757902264595 norm:0.0003927911166101694 max memory_allocated 22515.73046875 
[2025-02-23 12:15:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.08427019417285919 norm:0.00030786817660555243 max memory_allocated 22515.73046875 
[2025-02-23 12:15:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.08271616697311401 norm:0.00024421216221526265 max memory_allocated 22515.73046875 
[2025-02-23 12:16:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.08249583095312119 norm:0.00023711685207672417 max memory_allocated 22515.73046875 
[2025-02-23 12:16:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.08163426071405411 norm:0.0001926143595483154 max memory_allocated 22515.73046875 
[2025-02-23 12:17:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.08137539029121399 norm:0.00018064746109303087 max memory_allocated 22515.73046875 
[2025-02-23 12:17:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.08119721710681915 norm:0.00016258331015706062 max memory_allocated 22515.73046875 
[2025-02-23 12:18:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.08054140210151672 norm:0.00014146207831799984 max memory_allocated 22515.73046875 
[2025-02-23 12:18:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.08018949627876282 norm:0.00012552279804367572 max memory_allocated 22515.73046875 
[2025-02-23 12:19:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.08031798899173737 norm:0.00012430523929651827 max memory_allocated 22515.73046875 
[2025-02-23 12:19:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.08030493557453156 norm:0.00011376627662684768 max memory_allocated 22515.73046875 
[2025-02-23 12:20:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.0799691453576088 norm:0.00010051616845885292 max memory_allocated 22515.73046875 
[2025-02-23 12:21:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.08008449524641037 norm:0.0001021231000777334 max memory_allocated 22515.73046875 
[2025-02-23 12:21:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.07976590096950531 norm:8.540071576135233e-05 max memory_allocated 22515.73046875 
[2025-02-23 12:22:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.08008541166782379 norm:0.00010571922757662833 max memory_allocated 22515.73046875 
[2025-02-23 12:22:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.08038437366485596 norm:0.00010982473031617701 max memory_allocated 22515.73046875 
[2025-02-23 12:23:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.08055679500102997 norm:0.000116478004201781 max memory_allocated 22515.73046875 
[2025-02-23 12:23:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.08071655035018921 norm:0.00012102995242457837 max memory_allocated 22515.73046875 
[2025-02-23 12:23:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-23 12:24:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.10052605718374252 norm:0.0016291660722345114 max memory_allocated 22515.90234375 
[2025-02-23 12:25:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.09679391980171204 norm:0.000844960508402437 max memory_allocated 22515.90234375 
[2025-02-23 12:25:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.0940050333738327 norm:0.0005143127054907382 max memory_allocated 22515.90234375 
[2025-02-23 12:26:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.09237873554229736 norm:0.00034813550882972777 max memory_allocated 22515.90234375 
[2025-02-23 12:26:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.09145055711269379 norm:0.00027494801906868815 max memory_allocated 22515.90234375 
[2025-02-23 12:27:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.09087533503770828 norm:0.00021350466704461724 max memory_allocated 22515.90234375 
[2025-02-23 12:27:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.09047654271125793 norm:0.0002090165507979691 max memory_allocated 22515.90234375 
[2025-02-23 12:28:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.09024413675069809 norm:0.00019852054538205266 max memory_allocated 22515.90234375 
[2025-02-23 12:28:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.08985356986522675 norm:0.0001847613020800054 max memory_allocated 22515.90234375 
[2025-02-23 12:29:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.08984599262475967 norm:0.000167575417435728 max memory_allocated 22515.90234375 
[2025-02-23 12:29:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.09002026915550232 norm:0.00016771908849477768 max memory_allocated 22515.90234375 
[2025-02-23 12:30:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.08980164676904678 norm:0.00016477338795084506 max memory_allocated 22515.90234375 
[2025-02-23 12:30:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.09000732004642487 norm:0.00016552224406041205 max memory_allocated 22515.90234375 
[2025-02-23 12:31:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.08999601006507874 norm:0.00016628879529889673 max memory_allocated 22515.90234375 
[2025-02-23 12:32:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.0898054912686348 norm:0.00016401660104747862 max memory_allocated 22515.90234375 
[2025-02-23 12:32:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.08962327986955643 norm:0.00016457792662549764 max memory_allocated 22515.90234375 
[2025-02-23 12:33:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.08921213448047638 norm:0.00014864298282191157 max memory_allocated 22515.90234375 
[2025-02-23 12:33:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.08916737884283066 norm:0.0001517407945357263 max memory_allocated 22515.90234375 
[2025-02-23 12:34:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.08920124173164368 norm:0.0001512220478616655 max memory_allocated 22515.90234375 
[2025-02-23 12:34:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.08916087448596954 norm:0.00014961733540985733 max memory_allocated 22515.90234375 
[2025-02-23 12:34:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-23 12:35:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.11178488284349442 norm:0.002362289000302553 max memory_allocated 22516.07421875 
[2025-02-23 12:36:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.1066594272851944 norm:0.001165241003036499 max memory_allocated 22516.07421875 
[2025-02-23 12:36:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.10382549464702606 norm:0.0006841866998001933 max memory_allocated 22516.07421875 
[2025-02-23 12:37:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.10279116779565811 norm:0.0004636512021534145 max memory_allocated 22516.07421875 
[2025-02-23 12:37:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.10139989852905273 norm:0.00031686169677414 max memory_allocated 22516.07421875 
[2025-02-23 12:38:15 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.10047123581171036 norm:0.000265750742983073 max memory_allocated 22516.07421875 
[2025-02-23 12:38:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.09964847564697266 norm:0.00021569343516603112 max memory_allocated 22516.07421875 
[2025-02-23 12:39:20 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.09997028112411499 norm:0.00019979034550487995 max memory_allocated 22516.07421875 
[2025-02-23 12:39:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.09978340566158295 norm:0.00020152285287622362 max memory_allocated 22516.07421875 
[2025-02-23 12:40:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.09881400316953659 norm:0.0001731301745167002 max memory_allocated 22516.07421875 
[2025-02-23 12:40:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.0985429659485817 norm:0.00015423132572323084 max memory_allocated 22516.07421875 
[2025-02-23 12:41:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.09866895526647568 norm:0.00014198545250110328 max memory_allocated 22516.07421875 
[2025-02-23 12:42:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.09873618930578232 norm:0.0001372286642435938 max memory_allocated 22516.07421875 
[2025-02-23 12:42:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.09831025451421738 norm:0.0001228383043780923 max memory_allocated 22516.07421875 
[2025-02-23 12:43:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.09778839349746704 norm:0.00011054756760131568 max memory_allocated 22516.07421875 
[2025-02-23 12:43:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.09759676456451416 norm:0.00011492003977764398 max memory_allocated 22516.07421875 
[2025-02-23 12:44:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.09802161157131195 norm:0.00012199103366583586 max memory_allocated 22516.07421875 
[2025-02-23 12:44:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0979335755109787 norm:0.0001196892699226737 max memory_allocated 22516.07421875 
[2025-02-23 12:45:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.09745366871356964 norm:0.0001044814707711339 max memory_allocated 22516.07421875 
[2025-02-23 12:45:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.09709307551383972 norm:9.895767288981006e-05 max memory_allocated 22516.07421875 
[2025-02-23 12:46:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-23 12:46:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.11993151903152466 norm:0.0021178205497562885 max memory_allocated 22516.24609375 
[2025-02-23 12:47:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.11535510420799255 norm:0.0008999231504276395 max memory_allocated 22516.24609375 
[2025-02-23 12:47:40 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.11304505914449692 norm:0.0004982313839718699 max memory_allocated 22516.24609375 
[2025-02-23 12:48:13 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.11135639250278473 norm:0.00033123212051577866 max memory_allocated 22516.24609375 
[2025-02-23 12:48:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.11023648828268051 norm:0.00024686817778274417 max memory_allocated 22516.24609375 
[2025-02-23 12:49:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.10939507931470871 norm:0.00020141412096563727 max memory_allocated 22516.24609375 
[2025-02-23 12:49:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.10875076055526733 norm:0.00016938643238972872 max memory_allocated 22516.24609375 
[2025-02-23 12:50:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.108218252658844 norm:0.00014416774502024055 max memory_allocated 22516.24609375 
[2025-02-23 12:50:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.10791611671447754 norm:0.00012928052456118166 max memory_allocated 22516.24609375 
[2025-02-23 12:51:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.10776018351316452 norm:0.00011158805864397436 max memory_allocated 22516.24609375 
[2025-02-23 12:52:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.10756972432136536 norm:9.723547555040568e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:52:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.10744698345661163 norm:8.977468678494915e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:53:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.10738790780305862 norm:8.616840204922482e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:53:39 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.10728612542152405 norm:8.145123138092458e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:54:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.10712017863988876 norm:7.710145291639492e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:54:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.10706155747175217 norm:7.50909312046133e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:55:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.10714659839868546 norm:7.966379780555144e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:55:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.10712213814258575 norm:7.912216824479401e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:56:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.10720156133174896 norm:7.827705849194899e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:56:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.10727553814649582 norm:8.003983384696767e-05 max memory_allocated 22516.24609375 
[2025-02-23 12:57:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-23 12:57:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.1370081603527069 norm:0.0018006369937211275 max memory_allocated 22516.41796875 
[2025-02-23 12:58:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.1311827152967453 norm:0.0006661889492534101 max memory_allocated 22516.41796875 
[2025-02-23 12:58:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.12910501658916473 norm:0.0004798422451131046 max memory_allocated 22516.41796875 
[2025-02-23 12:59:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.1272556185722351 norm:0.0003309982130303979 max memory_allocated 22516.41796875 
[2025-02-23 12:59:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.12591193616390228 norm:0.000277577230008319 max memory_allocated 22516.41796875 
[2025-02-23 13:00:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.12468734383583069 norm:0.0002331313444301486 max memory_allocated 22516.41796875 
[2025-02-23 13:00:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.12414060533046722 norm:0.00019451887055765837 max memory_allocated 22516.41796875 
[2025-02-23 13:01:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.12347119301557541 norm:0.00016885140212252736 max memory_allocated 22516.41796875 
[2025-02-23 13:02:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.12280210852622986 norm:0.0001530871377326548 max memory_allocated 22516.41796875 
[2025-02-23 13:02:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.1225278377532959 norm:0.00014015374472364783 max memory_allocated 22516.41796875 
[2025-02-23 13:03:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.12275348603725433 norm:0.00013042827777098864 max memory_allocated 22516.41796875 
[2025-02-23 13:03:37 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.12300437688827515 norm:0.00013603911793325096 max memory_allocated 22516.41796875 
[2025-02-23 13:04:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.12302816659212112 norm:0.00014103394642006606 max memory_allocated 22516.41796875 
[2025-02-23 13:04:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.1228858083486557 norm:0.00014492619084194303 max memory_allocated 22516.41796875 
[2025-02-23 13:05:15 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.12326560914516449 norm:0.00015987020742613822 max memory_allocated 22516.41796875 
[2025-02-23 13:05:48 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.1238444522023201 norm:0.00018958852160722017 max memory_allocated 22516.41796875 
[2025-02-23 13:06:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.12364277988672256 norm:0.0001864223158918321 max memory_allocated 22516.41796875 
[2025-02-23 13:06:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.12387468665838242 norm:0.00019205539138056338 max memory_allocated 22516.41796875 
[2025-02-23 13:07:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.12298601865768433 norm:0.00015989865642040968 max memory_allocated 22516.41796875 
[2025-02-23 13:07:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.12347538024187088 norm:0.0001901515934150666 max memory_allocated 22516.41796875 
[2025-02-23 13:08:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-23 13:08:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.15513397753238678 norm:0.002063937019556761 max memory_allocated 22516.58984375 
[2025-02-23 13:09:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.15111401677131653 norm:0.0011120925191789865 max memory_allocated 22516.58984375 
[2025-02-23 13:09:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.1486985683441162 norm:0.0006859457353129983 max memory_allocated 22516.58984375 
[2025-02-23 13:10:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.14743107557296753 norm:0.00043915322748944163 max memory_allocated 22516.58984375 
[2025-02-23 13:10:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.1460854709148407 norm:0.00035659875720739365 max memory_allocated 22516.58984375 
[2025-02-23 13:11:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.14541196823120117 norm:0.0003194496384821832 max memory_allocated 22516.58984375 
[2025-02-23 13:11:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.1450108140707016 norm:0.0002690870314836502 max memory_allocated 22516.58984375 
[2025-02-23 13:12:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.14471259713172913 norm:0.00023716466967016459 max memory_allocated 22516.58984375 
[2025-02-23 13:13:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.14402103424072266 norm:0.00023689931549597532 max memory_allocated 22516.58984375 
[2025-02-23 13:13:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.1435871422290802 norm:0.000219164474401623 max memory_allocated 22516.58984375 
[2025-02-23 13:14:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.14349719882011414 norm:0.0002117068797815591 max memory_allocated 22516.58984375 
[2025-02-23 13:14:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.14319901168346405 norm:0.00021831507910974324 max memory_allocated 22516.58984375 
[2025-02-23 13:15:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.14353980123996735 norm:0.00021135175484232605 max memory_allocated 22516.58984375 
[2025-02-23 13:15:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.14341093599796295 norm:0.00021080885198898613 max memory_allocated 22516.58984375 
[2025-02-23 13:16:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.14343875646591187 norm:0.00021576948347501457 max memory_allocated 22516.58984375 
[2025-02-23 13:16:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.14345814287662506 norm:0.00022149751021061093 max memory_allocated 22516.58984375 
[2025-02-23 13:17:24 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.14347252249717712 norm:0.00021700603247154504 max memory_allocated 22516.58984375 
[2025-02-23 13:17:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.14349263906478882 norm:0.00021853337239008397 max memory_allocated 22516.58984375 
[2025-02-23 13:18:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.14344173669815063 norm:0.0002242463087895885 max memory_allocated 22516.58984375 
[2025-02-23 13:19:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.14344145357608795 norm:0.00021975900745019317 max memory_allocated 22516.58984375 
[2025-02-23 13:19:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-23 13:19:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.17941798269748688 norm:0.0030756150372326374 max memory_allocated 22516.76171875 
[2025-02-23 13:20:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.17275398969650269 norm:0.001025297213345766 max memory_allocated 22516.76171875 
[2025-02-23 13:20:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.16898444294929504 norm:0.0005585734616033733 max memory_allocated 22516.76171875 
[2025-02-23 13:21:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.16658321022987366 norm:0.0003655654436443001 max memory_allocated 22516.76171875 
[2025-02-23 13:21:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.16564291715621948 norm:0.00029325694777071476 max memory_allocated 22516.76171875 
[2025-02-23 13:22:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.16583630442619324 norm:0.0002690537367016077 max memory_allocated 22516.76171875 
[2025-02-23 13:23:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.16556769609451294 norm:0.00025833011022768915 max memory_allocated 22516.76171875 
[2025-02-23 13:23:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.16433414816856384 norm:0.00023666213382966816 max memory_allocated 22516.76171875 
[2025-02-23 13:24:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.16303470730781555 norm:0.0001930881990119815 max memory_allocated 22516.76171875 
[2025-02-23 13:24:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.16271521151065826 norm:0.00017894436314236373 max memory_allocated 22516.76171875 
[2025-02-23 13:25:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.16128110885620117 norm:0.00017521916015539318 max memory_allocated 22516.76171875 
[2025-02-23 13:25:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.16143570840358734 norm:0.00015580380568280816 max memory_allocated 22516.76171875 
[2025-02-23 13:26:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.1625249683856964 norm:0.0001986347051570192 max memory_allocated 22516.76171875 
[2025-02-23 13:26:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.16335126757621765 norm:0.00024586624931544065 max memory_allocated 22516.76171875 
[2025-02-23 13:27:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.1633981317281723 norm:0.00023251728271134198 max memory_allocated 22516.76171875 
[2025-02-23 13:27:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.16335564851760864 norm:0.0002508125617168844 max memory_allocated 22516.76171875 
[2025-02-23 13:28:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.16251938045024872 norm:0.00021457804541569203 max memory_allocated 22516.76171875 
[2025-02-23 13:29:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.16249030828475952 norm:0.0002313262812094763 max memory_allocated 22516.76171875 
[2025-02-23 13:29:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.1628950536251068 norm:0.00025334832025691867 max memory_allocated 22516.76171875 
[2025-02-23 13:30:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.1629462093114853 norm:0.00024210863921325654 max memory_allocated 22516.76171875 
[2025-02-23 13:30:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-23 13:30:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.2048092931509018 norm:0.001571738044731319 max memory_allocated 22516.93359375 
[2025-02-23 13:31:24 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.19977501034736633 norm:0.0009371449705213308 max memory_allocated 22516.93359375 
[2025-02-23 13:31:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.1969684660434723 norm:0.0006891534430906177 max memory_allocated 22516.93359375 
[2025-02-23 13:32:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.19458065927028656 norm:0.0005106984172016382 max memory_allocated 22516.93359375 
[2025-02-23 13:33:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.1934676170349121 norm:0.0004716131661552936 max memory_allocated 22516.93359375 
[2025-02-23 13:33:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.19284509122371674 norm:0.00041413880535401404 max memory_allocated 22516.93359375 
[2025-02-23 13:34:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.19202089309692383 norm:0.00035243446473032236 max memory_allocated 22516.93359375 
[2025-02-23 13:34:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.19141395390033722 norm:0.000333661213517189 max memory_allocated 22516.93359375 
[2025-02-23 13:35:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.1909809410572052 norm:0.0002819924266077578 max memory_allocated 22516.93359375 
[2025-02-23 13:35:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.190696120262146 norm:0.00025470892433077097 max memory_allocated 22516.93359375 
[2025-02-23 13:36:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.19003888964653015 norm:0.0002366404514759779 max memory_allocated 22516.93359375 
[2025-02-23 13:36:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.18976503610610962 norm:0.00022513559088110924 max memory_allocated 22516.93359375 
[2025-02-23 13:37:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.1895270198583603 norm:0.00023657387646380812 max memory_allocated 22516.93359375 
[2025-02-23 13:37:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.1894720196723938 norm:0.00021776361973024905 max memory_allocated 22516.93359375 
[2025-02-23 13:38:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.18941274285316467 norm:0.00020991935161873698 max memory_allocated 22516.93359375 
[2025-02-23 13:38:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.1891534924507141 norm:0.000205350574105978 max memory_allocated 22516.93359375 
[2025-02-23 13:39:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.18912500143051147 norm:0.00019996557966805995 max memory_allocated 22516.93359375 
[2025-02-23 13:40:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.1889992356300354 norm:0.00019802857423201203 max memory_allocated 22516.93359375 
[2025-02-23 13:40:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.18879565596580505 norm:0.00020026485435664654 max memory_allocated 22516.93359375 
[2025-02-23 13:41:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.18870288133621216 norm:0.00019299349514767528 max memory_allocated 22516.93359375 
[2025-02-23 13:41:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-23 13:41:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.23499555885791779 norm:0.0029742014594376087 max memory_allocated 22517.10546875 
[2025-02-23 13:42:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.23028074204921722 norm:0.0019129944266751409 max memory_allocated 22517.10546875 
[2025-02-23 13:43:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.2270357459783554 norm:0.001261955825611949 max memory_allocated 22517.10546875 
[2025-02-23 13:43:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.22521249949932098 norm:0.000818329572211951 max memory_allocated 22517.10546875 
[2025-02-23 13:44:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.2232523113489151 norm:0.0005719471955671906 max memory_allocated 22517.10546875 
[2025-02-23 13:44:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.22198888659477234 norm:0.0004908395931124687 max memory_allocated 22517.10546875 
[2025-02-23 13:45:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.2210858017206192 norm:0.0004092992458026856 max memory_allocated 22517.10546875 
[2025-02-23 13:45:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.22028309106826782 norm:0.00036132638342678547 max memory_allocated 22517.10546875 
[2025-02-23 13:46:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.21950675547122955 norm:0.00031685270369052887 max memory_allocated 22517.10546875 
[2025-02-23 13:46:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.21865105628967285 norm:0.0002971833455376327 max memory_allocated 22517.10546875 
[2025-02-23 13:47:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.21826918423175812 norm:0.00026186826289631426 max memory_allocated 22517.10546875 
[2025-02-23 13:47:53 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.2174549251794815 norm:0.00023047391732688993 max memory_allocated 22517.10546875 
[2025-02-23 13:48:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.21718159317970276 norm:0.00021607120288535953 max memory_allocated 22517.10546875 
[2025-02-23 13:48:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.21668483316898346 norm:0.0001935963227879256 max memory_allocated 22517.10546875 
[2025-02-23 13:49:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.21651053428649902 norm:0.00019458917086012661 max memory_allocated 22517.10546875 
[2025-02-23 13:50:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.21632423996925354 norm:0.0001891736319521442 max memory_allocated 22517.10546875 
[2025-02-23 13:50:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.21637171506881714 norm:0.00018954600091092288 max memory_allocated 22517.10546875 
[2025-02-23 13:51:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.2165173888206482 norm:0.00018970917153637856 max memory_allocated 22517.10546875 
[2025-02-23 13:51:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.21677356958389282 norm:0.0001983192196348682 max memory_allocated 22517.10546875 
[2025-02-23 13:52:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.216898113489151 norm:0.00019715563394129276 max memory_allocated 22517.10546875 
[2025-02-23 13:52:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-23 13:52:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.2681982219219208 norm:0.0034405395854264498 max memory_allocated 22517.27734375 
[2025-02-23 13:53:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.2616173028945923 norm:0.0017573705408722162 max memory_allocated 22517.27734375 
[2025-02-23 13:54:04 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.2584897577762604 norm:0.0011494153877720237 max memory_allocated 22517.27734375 
[2025-02-23 13:54:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.2561604678630829 norm:0.0008319834596477449 max memory_allocated 22517.27734375 
[2025-02-23 13:55:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.2541884183883667 norm:0.0006145620136521757 max memory_allocated 22517.27734375 
[2025-02-23 13:55:41 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.2531861662864685 norm:0.00048385627451352775 max memory_allocated 22517.27734375 
[2025-02-23 13:56:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.25232309103012085 norm:0.00038828307879157364 max memory_allocated 22517.27734375 
[2025-02-23 13:56:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.25160548090934753 norm:0.00033441814593970776 max memory_allocated 22517.27734375 
[2025-02-23 13:57:19 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.25067993998527527 norm:0.00029303072369657457 max memory_allocated 22517.27734375 
[2025-02-23 13:57:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.24991220235824585 norm:0.00026773326680995524 max memory_allocated 22517.27734375 
[2025-02-23 13:58:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.24949544668197632 norm:0.00023233506362885237 max memory_allocated 22517.27734375 
[2025-02-23 13:58:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.24961209297180176 norm:0.000216178348637186 max memory_allocated 22517.27734375 
[2025-02-23 13:59:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.2491997480392456 norm:0.00021407900203485042 max memory_allocated 22517.27734375 
[2025-02-23 14:00:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.24895700812339783 norm:0.0002075954689644277 max memory_allocated 22517.27734375 
[2025-02-23 14:00:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.24848267436027527 norm:0.00019782062736339867 max memory_allocated 22517.27734375 
[2025-02-23 14:01:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.24807772040367126 norm:0.0001904313248815015 max memory_allocated 22517.27734375 
[2025-02-23 14:01:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.2482786476612091 norm:0.00018769442976918072 max memory_allocated 22517.27734375 
[2025-02-23 14:02:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.24812263250350952 norm:0.000188132980838418 max memory_allocated 22517.27734375 
[2025-02-23 14:02:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.2480449229478836 norm:0.00018682674271985888 max memory_allocated 22517.27734375 
[2025-02-23 14:03:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.24791790544986725 norm:0.00019308298942632973 max memory_allocated 22517.27734375 
[2025-02-23 14:03:27 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-23 14:04:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.31024599075317383 norm:0.0037167302798479795 max memory_allocated 22517.44921875 
[2025-02-23 14:04:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.30289527773857117 norm:0.001774808275513351 max memory_allocated 22517.44921875 
[2025-02-23 14:05:07 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.29979458451271057 norm:0.0012185575906187296 max memory_allocated 22517.44921875 
[2025-02-23 14:05:40 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.2974339425563812 norm:0.0008299594046548009 max memory_allocated 22517.44921875 
[2025-02-23 14:06:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.29583486914634705 norm:0.0005840943194925785 max memory_allocated 22517.44921875 
[2025-02-23 14:06:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.2952883839607239 norm:0.0004420791519805789 max memory_allocated 22517.44921875 
[2025-02-23 14:07:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.2950136065483093 norm:0.00037331372732296586 max memory_allocated 22517.44921875 
[2025-02-23 14:07:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.2942751348018646 norm:0.00036508130142465234 max memory_allocated 22517.44921875 
[2025-02-23 14:08:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.29382988810539246 norm:0.0003521409526001662 max memory_allocated 22517.44921875 
[2025-02-23 14:08:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.2935883402824402 norm:0.0002986575127579272 max memory_allocated 22517.44921875 
[2025-02-23 14:09:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.293146550655365 norm:0.0002797396155074239 max memory_allocated 22517.44921875 
[2025-02-23 14:10:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.2929708659648895 norm:0.0002696049341466278 max memory_allocated 22517.44921875 
[2025-02-23 14:10:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.293203204870224 norm:0.00028325917082838714 max memory_allocated 22517.44921875 
[2025-02-23 14:11:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.293562114238739 norm:0.00030105310725048184 max memory_allocated 22517.44921875 
[2025-02-23 14:11:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.2931865453720093 norm:0.0002871893811970949 max memory_allocated 22517.44921875 
[2025-02-23 14:12:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.29297128319740295 norm:0.0002964660234283656 max memory_allocated 22517.44921875 
[2025-02-23 14:12:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.2927473187446594 norm:0.0002753524458967149 max memory_allocated 22517.44921875 
[2025-02-23 14:13:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.2926223874092102 norm:0.00027202413184568286 max memory_allocated 22517.44921875 
[2025-02-23 14:13:48 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.2926180064678192 norm:0.0002913033531513065 max memory_allocated 22517.44921875 
[2025-02-23 14:14:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.2926804721355438 norm:0.00028005155036225915 max memory_allocated 22517.44921875 
[2025-02-23 14:14:30 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-23 14:15:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.3630169630050659 norm:0.004134634975343943 max memory_allocated 22517.62109375 
[2025-02-23 14:15:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.3534204959869385 norm:0.002013398567214608 max memory_allocated 22517.62109375 
[2025-02-23 14:16:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.3484581708908081 norm:0.001173011725768447 max memory_allocated 22517.62109375 
[2025-02-23 14:16:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.345807284116745 norm:0.0006928507355041802 max memory_allocated 22517.62109375 
[2025-02-23 14:17:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.344409704208374 norm:0.0005514640361070633 max memory_allocated 22517.62109375 
[2025-02-23 14:17:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.3430326282978058 norm:0.0004646361048799008 max memory_allocated 22517.62109375 
[2025-02-23 14:18:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.34226223826408386 norm:0.0004255161911714822 max memory_allocated 22517.62109375 
[2025-02-23 14:18:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.3415582776069641 norm:0.0003694201004691422 max memory_allocated 22517.62109375 
[2025-02-23 14:19:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.34058865904808044 norm:0.000367421074770391 max memory_allocated 22517.62109375 
[2025-02-23 14:19:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.34010422229766846 norm:0.0003299823438283056 max memory_allocated 22517.62109375 
[2025-02-23 14:20:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.33979952335357666 norm:0.0003178551560267806 max memory_allocated 22517.62109375 
[2025-02-23 14:21:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.3395891785621643 norm:0.0003312573826406151 max memory_allocated 22517.62109375 
[2025-02-23 14:21:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.33956143260002136 norm:0.0003399099223315716 max memory_allocated 22517.62109375 
[2025-02-23 14:22:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.3395601212978363 norm:0.0003146195667795837 max memory_allocated 22517.62109375 
[2025-02-23 14:22:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.3394930958747864 norm:0.0003344251017551869 max memory_allocated 22517.62109375 
[2025-02-23 14:23:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.3392829895019531 norm:0.0003305764985270798 max memory_allocated 22517.62109375 
[2025-02-23 14:23:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.3390950560569763 norm:0.0003224415413569659 max memory_allocated 22517.62109375 
[2025-02-23 14:24:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.33924734592437744 norm:0.00031606797710992396 max memory_allocated 22517.62109375 
[2025-02-23 14:24:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.3393994867801666 norm:0.0003391339269001037 max memory_allocated 22517.62109375 
[2025-02-23 14:25:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.3395467698574066 norm:0.00033980331500060856 max memory_allocated 22517.62109375 
[2025-02-23 14:25:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-23 14:26:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.6291185617446899 norm:0.006301658693701029 max memory_allocated 22517.79296875 
[2025-02-23 14:26:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.6025770902633667 norm:0.003727974835783243 max memory_allocated 22517.79296875 
[2025-02-23 14:27:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.596881628036499 norm:0.003382264170795679 max memory_allocated 22517.79296875 
[2025-02-23 14:27:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.5980599522590637 norm:0.0037003965117037296 max memory_allocated 22517.79296875 
[2025-02-23 14:28:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.6018816828727722 norm:0.0039146337658166885 max memory_allocated 22517.79296875 
[2025-02-23 14:28:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.6023814678192139 norm:0.003928084392100573 max memory_allocated 22517.79296875 
[2025-02-23 14:29:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.6025557518005371 norm:0.004693010821938515 max memory_allocated 22517.79296875 
[2025-02-23 14:29:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.6045846939086914 norm:0.0053390623070299625 max memory_allocated 22517.79296875 
[2025-02-23 14:30:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.6050384044647217 norm:0.006679695099592209 max memory_allocated 22517.79296875 
[2025-02-23 14:31:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.605492115020752 norm:0.0069799525663256645 max memory_allocated 22517.79296875 
[2025-02-23 14:31:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.6035000085830688 norm:0.0068002501502633095 max memory_allocated 22517.79296875 
[2025-02-23 14:32:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.6032074689865112 norm:0.007073543034493923 max memory_allocated 22517.79296875 
[2025-02-23 14:32:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.6038327217102051 norm:0.007079557981342077 max memory_allocated 22517.79296875 
[2025-02-23 14:33:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.6012087464332581 norm:0.007098993752151728 max memory_allocated 22517.79296875 
[2025-02-23 14:33:44 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.6029616594314575 norm:0.007894689217209816 max memory_allocated 22517.79296875 
[2025-02-23 14:34:17 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.6012290716171265 norm:0.00935225747525692 max memory_allocated 22517.79296875 
[2025-02-23 14:34:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.6035219430923462 norm:0.00817912258207798 max memory_allocated 22517.79296875 
[2025-02-23 14:35:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.6024852991104126 norm:0.00912292581051588 max memory_allocated 22517.79296875 
[2025-02-23 14:35:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.6038022041320801 norm:0.007602933328598738 max memory_allocated 22517.79296875 
[2025-02-23 14:36:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.6036481857299805 norm:0.008745359256863594 max memory_allocated 22517.79296875 
[2025-02-23 14:36:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-23 14:37:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.9690228700637817 norm:0.0042523727752268314 max memory_allocated 22517.96484375 
[2025-02-23 14:37:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.9304272532463074 norm:0.0030477133113890886 max memory_allocated 22517.96484375 
[2025-02-23 14:38:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.9133045077323914 norm:0.0025961382780224085 max memory_allocated 22517.96484375 
[2025-02-23 14:38:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.9002906084060669 norm:0.002205362543463707 max memory_allocated 22517.96484375 
[2025-02-23 14:39:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.8915833830833435 norm:0.0019873122218996286 max memory_allocated 22517.96484375 
[2025-02-23 14:39:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.8857747912406921 norm:0.001813928596675396 max memory_allocated 22517.96484375 
[2025-02-23 14:40:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.8857860565185547 norm:0.0017232366371899843 max memory_allocated 22517.96484375 
[2025-02-23 14:40:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.8921093344688416 norm:0.0016936355968937278 max memory_allocated 22517.96484375 
[2025-02-23 14:41:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.8938117027282715 norm:0.0016854554414749146 max memory_allocated 22517.96484375 
[2025-02-23 14:42:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.8927665948867798 norm:0.001634949236176908 max memory_allocated 22517.96484375 
[2025-02-23 14:42:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.8876344561576843 norm:0.0015052903909236193 max memory_allocated 22517.96484375 
[2025-02-23 14:43:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.8789535760879517 norm:0.001472689094953239 max memory_allocated 22517.96484375 
[2025-02-23 14:43:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.8666152954101562 norm:0.0012808741303160787 max memory_allocated 22517.96484375 
[2025-02-23 14:44:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.8684911131858826 norm:0.0013217309024184942 max memory_allocated 22517.96484375 
[2025-02-23 14:44:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.8711943030357361 norm:0.001333765685558319 max memory_allocated 22517.96484375 
[2025-02-23 14:45:20 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.8728314638137817 norm:0.0013730302453041077 max memory_allocated 22517.96484375 
[2025-02-23 14:45:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.8743062615394592 norm:0.0013773759128525853 max memory_allocated 22517.96484375 
[2025-02-23 14:46:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.8733222484588623 norm:0.0013964171521365643 max memory_allocated 22517.96484375 
[2025-02-23 14:46:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.8735415935516357 norm:0.0014124384615570307 max memory_allocated 22517.96484375 
[2025-02-23 14:47:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.8741814494132996 norm:0.0014128242619335651 max memory_allocated 22517.96484375 
[2025-02-23 14:47:39 root] (main_calibration.py 365): INFO 21252.927696228027
[2025-02-23 14:48:11 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-23 14:49:20 root] (main_calibration.py 158): INFO wikitext2 : 5.964860439300537
[2025-02-23 14:49:20 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-23 14:51:08 root] (main_calibration.py 158): INFO c4 : 7.546379566192627
[2025-02-23 16:22:47 root] (main_calibration.py 169): INFO {'wikitext2': 5.964860439300537, 'c4': 7.546379566192627, 'results': {'arc_easy': {'acc': 0.6893939393939394, 'acc_stderr': 0.009495260551195608, 'acc_norm': 0.5315656565656566, 'acc_norm_stderr': 0.010239317603199509}, 'hellaswag': {'acc': 0.5585540728938458, 'acc_stderr': 0.004955447564694052, 'acc_norm': 0.7176857199761004, 'acc_norm_stderr': 0.004492055279407122}, 'piqa': {'acc': 0.7627856365614799, 'acc_stderr': 0.009924694933586359, 'acc_norm': 0.7584330794341676, 'acc_norm_stderr': 0.009986718001804456}, 'arc_challenge': {'acc': 0.39590443686006827, 'acc_stderr': 0.014291228393536585, 'acc_norm': 0.4061433447098976, 'acc_norm_stderr': 0.014351656690097862}, 'winogrande': {'acc': 0.6661404893449092, 'acc_stderr': 0.013254029695143355}, 'boolq': {'acc': 0.6810397553516819, 'acc_stderr': 0.008151678629528387}}, 'versions': {'arc_easy': 0, 'hellaswag': 0, 'piqa': 0, 'arc_challenge': 0, 'winogrande': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
