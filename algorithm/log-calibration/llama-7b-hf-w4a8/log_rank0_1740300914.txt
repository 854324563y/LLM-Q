[2025-02-23 08:55:14 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration/llama-7b-hf-w4a8', save_dir='./log-calibration/quant/llama-7b-hf-w4a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-23 08:55:22 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-23 08:55:22 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-23 08:55:22 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-23 08:55:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-23 08:55:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.015004647895693779 norm:0.0028205744456499815 max memory_allocated 22509.63671875 
[2025-02-23 08:56:32 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.009452611207962036 norm:0.0011108294129371643 max memory_allocated 22509.63671875 
[2025-02-23 08:57:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.012388460338115692 norm:0.0035397480241954327 max memory_allocated 22509.63671875 
[2025-02-23 08:57:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0133697260171175 norm:0.004842586815357208 max memory_allocated 22509.63671875 
[2025-02-23 08:58:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.013815606012940407 norm:0.004863457754254341 max memory_allocated 22509.63671875 
[2025-02-23 08:58:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.013813141733407974 norm:0.005131424404680729 max memory_allocated 22509.63671875 
[2025-02-23 08:59:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.01422598585486412 norm:0.005699561908841133 max memory_allocated 22509.63671875 
[2025-02-23 08:59:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.014193032868206501 norm:0.005768575705587864 max memory_allocated 22509.63671875 
[2025-02-23 09:00:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.013218835927546024 norm:0.005056759808212519 max memory_allocated 22509.63671875 
[2025-02-23 09:00:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.009881540201604366 norm:0.0031817485578358173 max memory_allocated 22509.63671875 
[2025-02-23 09:01:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.006027248688042164 norm:0.0004655655357055366 max memory_allocated 22509.63671875 
[2025-02-23 09:02:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.008435273543000221 norm:0.0013603628613054752 max memory_allocated 22509.63671875 
[2025-02-23 09:02:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.007441249676048756 norm:0.0010338168358430266 max memory_allocated 22509.63671875 
[2025-02-23 09:03:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.005595932248979807 norm:0.0003801606653723866 max memory_allocated 22509.63671875 
[2025-02-23 09:03:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.007370294071733952 norm:0.0008729546098038554 max memory_allocated 22509.63671875 
[2025-02-23 09:04:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.008156949654221535 norm:0.0013529068091884255 max memory_allocated 22509.63671875 
[2025-02-23 09:04:47 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00568231800571084 norm:0.00041498662903904915 max memory_allocated 22509.63671875 
[2025-02-23 09:05:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.007659416180104017 norm:0.0010762776946648955 max memory_allocated 22509.63671875 
[2025-02-23 09:05:53 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.005707794334739447 norm:0.0004575248749461025 max memory_allocated 22509.63671875 
[2025-02-23 09:06:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.007476978003978729 norm:0.0008551693172194064 max memory_allocated 22509.63671875 
[2025-02-23 09:06:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-23 09:07:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.03641154244542122 norm:0.0058016590774059296 max memory_allocated 22509.80859375 
[2025-02-23 09:07:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.022322343662381172 norm:0.0017789889825507998 max memory_allocated 22509.80859375 
[2025-02-23 09:08:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.01951555907726288 norm:0.0012727284338325262 max memory_allocated 22509.80859375 
[2025-02-23 09:08:51 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.01883011870086193 norm:0.0012686527334153652 max memory_allocated 22509.80859375 
[2025-02-23 09:09:24 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.016338253393769264 norm:0.0009871813235804439 max memory_allocated 22509.80859375 
[2025-02-23 09:09:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.014408194459974766 norm:0.00043113736319355667 max memory_allocated 22509.80859375 
[2025-02-23 09:10:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.017055107280611992 norm:0.001024930621497333 max memory_allocated 22509.80859375 
[2025-02-23 09:11:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.017600461840629578 norm:0.0010876437881961465 max memory_allocated 22509.80859375 
[2025-02-23 09:11:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.017968550324440002 norm:0.0012068290961906314 max memory_allocated 22509.80859375 
[2025-02-23 09:12:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.018332980573177338 norm:0.001378749031573534 max memory_allocated 22509.80859375 
[2025-02-23 09:12:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.018159382045269012 norm:0.001296701724641025 max memory_allocated 22509.80859375 
[2025-02-23 09:13:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.018016869202256203 norm:0.0012306580320000648 max memory_allocated 22509.80859375 
[2025-02-23 09:13:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.018147077411413193 norm:0.001267927116714418 max memory_allocated 22509.80859375 
[2025-02-23 09:14:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.018275011330842972 norm:0.0012862663716077805 max memory_allocated 22509.80859375 
[2025-02-23 09:14:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.01828024722635746 norm:0.0013040798949077725 max memory_allocated 22509.80859375 
[2025-02-23 09:15:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.018319690600037575 norm:0.0013271645875647664 max memory_allocated 22509.80859375 
[2025-02-23 09:16:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.018117021769285202 norm:0.001243713079020381 max memory_allocated 22509.80859375 
[2025-02-23 09:16:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.017799269407987595 norm:0.0011342257494106889 max memory_allocated 22509.80859375 
[2025-02-23 09:17:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.017836950719356537 norm:0.0011806542752310634 max memory_allocated 22509.80859375 
[2025-02-23 09:17:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.017762143164873123 norm:0.0011467868462204933 max memory_allocated 22509.80859375 
[2025-02-23 09:17:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-23 09:18:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.026973089203238487 norm:0.000783553346991539 max memory_allocated 22509.98046875 
[2025-02-23 09:19:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.023336388170719147 norm:0.0003901119634974748 max memory_allocated 22509.98046875 
[2025-02-23 09:19:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.023036029189825058 norm:0.0003566365339793265 max memory_allocated 22509.98046875 
[2025-02-23 09:20:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.022270698100328445 norm:0.00023749998945277184 max memory_allocated 22509.98046875 
[2025-02-23 09:20:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0218648761510849 norm:0.00020593931549228728 max memory_allocated 22509.98046875 
[2025-02-23 09:21:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.02182909846305847 norm:0.0001963056856766343 max memory_allocated 22509.98046875 
[2025-02-23 09:21:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.021857263520359993 norm:0.00021873207879252732 max memory_allocated 22509.98046875 
[2025-02-23 09:22:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.021801885217428207 norm:0.00018301475211046636 max memory_allocated 22509.98046875 
[2025-02-23 09:22:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.021795053035020828 norm:0.00019318319391459227 max memory_allocated 22509.98046875 
[2025-02-23 09:23:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.021758247166872025 norm:0.00019692946807481349 max memory_allocated 22509.98046875 
[2025-02-23 09:23:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.021934114396572113 norm:0.0002025858557317406 max memory_allocated 22509.98046875 
[2025-02-23 09:24:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.02190309762954712 norm:0.00020498508820310235 max memory_allocated 22509.98046875 
[2025-02-23 09:25:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.021766304969787598 norm:0.0001995377679122612 max memory_allocated 22509.98046875 
[2025-02-23 09:25:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.021711895242333412 norm:0.00020047335419803858 max memory_allocated 22509.98046875 
[2025-02-23 09:26:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.0218107458204031 norm:0.00022375828120857477 max memory_allocated 22509.98046875 
[2025-02-23 09:26:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.021796032786369324 norm:0.0001992922043427825 max memory_allocated 22509.98046875 
[2025-02-23 09:27:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.021812986582517624 norm:0.00021938788995612413 max memory_allocated 22509.98046875 
[2025-02-23 09:27:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.021748241037130356 norm:0.0002083079016301781 max memory_allocated 22509.98046875 
[2025-02-23 09:28:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.021799473091959953 norm:0.00021452164219226688 max memory_allocated 22509.98046875 
[2025-02-23 09:28:57 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.021768903359770775 norm:0.00020867986313533038 max memory_allocated 22509.98046875 
[2025-02-23 09:29:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-23 09:29:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.034609440714120865 norm:0.0034275646321475506 max memory_allocated 22510.15234375 
[2025-02-23 09:30:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.02984267845749855 norm:0.0009803795255720615 max memory_allocated 22510.15234375 
[2025-02-23 09:30:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.02854074537754059 norm:0.00047019057092256844 max memory_allocated 22510.15234375 
[2025-02-23 09:31:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.02799852192401886 norm:0.00023623448214493692 max memory_allocated 22510.15234375 
[2025-02-23 09:31:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.027967242524027824 norm:0.0001915216853376478 max memory_allocated 22510.15234375 
[2025-02-23 09:32:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.02784644067287445 norm:0.00017069409659598023 max memory_allocated 22510.15234375 
[2025-02-23 09:33:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.027307799085974693 norm:0.0001475234021199867 max memory_allocated 22510.15234375 
[2025-02-23 09:33:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.02724427916109562 norm:0.00014206659398041666 max memory_allocated 22510.15234375 
[2025-02-23 09:34:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.027159016579389572 norm:0.00013907361426390707 max memory_allocated 22510.15234375 
[2025-02-23 09:34:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.02712574042379856 norm:0.0001409298274666071 max memory_allocated 22510.15234375 
[2025-02-23 09:35:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.027287239208817482 norm:0.00014157754776533693 max memory_allocated 22510.15234375 
[2025-02-23 09:35:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.02735912799835205 norm:0.00015386079030577093 max memory_allocated 22510.15234375 
[2025-02-23 09:36:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.027301091700792313 norm:0.00015622959472239017 max memory_allocated 22510.15234375 
[2025-02-23 09:36:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.027297722175717354 norm:0.00015114547568373382 max memory_allocated 22510.15234375 
[2025-02-23 09:37:26 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.027281787246465683 norm:0.00015678800991736352 max memory_allocated 22510.15234375 
[2025-02-23 09:37:59 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.027192896232008934 norm:0.00015224389790091664 max memory_allocated 22510.15234375 
[2025-02-23 09:38:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.027381742373108864 norm:0.00015112229448277503 max memory_allocated 22510.15234375 
[2025-02-23 09:39:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.02737937867641449 norm:0.00015243249072227627 max memory_allocated 22510.15234375 
[2025-02-23 09:39:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.02741561457514763 norm:0.00015825529408175498 max memory_allocated 22510.15234375 
[2025-02-23 09:40:12 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.027492132037878036 norm:0.0001584885030752048 max memory_allocated 22510.15234375 
[2025-02-23 09:40:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-23 09:40:57 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.043990395963191986 norm:0.0022332901135087013 max memory_allocated 22510.32421875 
[2025-02-23 09:41:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.039214517921209335 norm:0.0006211209110915661 max memory_allocated 22510.32421875 
[2025-02-23 09:42:03 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.037569038569927216 norm:0.00037603938835673034 max memory_allocated 22510.32421875 
[2025-02-23 09:42:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.036343127489089966 norm:0.00028175266925245523 max memory_allocated 22510.32421875 
[2025-02-23 09:43:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.035525303333997726 norm:0.0002169874351238832 max memory_allocated 22510.32421875 
[2025-02-23 09:43:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.03559734672307968 norm:0.00018690217984840274 max memory_allocated 22510.32421875 
[2025-02-23 09:44:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.03516918048262596 norm:0.00016553701425436884 max memory_allocated 22510.32421875 
[2025-02-23 09:44:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.03519304841756821 norm:0.00016641279216855764 max memory_allocated 22510.32421875 
[2025-02-23 09:45:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0350983552634716 norm:0.00017079628014471382 max memory_allocated 22510.32421875 
[2025-02-23 09:45:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.03535178303718567 norm:0.00018690249999053776 max memory_allocated 22510.32421875 
[2025-02-23 09:46:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0354427769780159 norm:0.00018936859851237386 max memory_allocated 22510.32421875 
[2025-02-23 09:47:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.03530896082520485 norm:0.0001778904115781188 max memory_allocated 22510.32421875 
[2025-02-23 09:47:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.03547828271985054 norm:0.00019351589435245842 max memory_allocated 22510.32421875 
[2025-02-23 09:48:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.03545854985713959 norm:0.00018863510922528803 max memory_allocated 22510.32421875 
[2025-02-23 09:48:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.035534124821424484 norm:0.00019995843467768282 max memory_allocated 22510.32421875 
[2025-02-23 09:49:14 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.03541996330022812 norm:0.00019630155293270946 max memory_allocated 22510.32421875 
[2025-02-23 09:49:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.03533980995416641 norm:0.00018643557268660516 max memory_allocated 22510.32421875 
[2025-02-23 09:50:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.03530851751565933 norm:0.00019501769565977156 max memory_allocated 22510.32421875 
[2025-02-23 09:50:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.03540603443980217 norm:0.00019782199524343014 max memory_allocated 22510.32421875 
[2025-02-23 09:51:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0352819561958313 norm:0.00018417902174405754 max memory_allocated 22510.32421875 
[2025-02-23 09:51:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-23 09:52:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.049784280359745026 norm:0.002158581046387553 max memory_allocated 22510.49609375 
[2025-02-23 09:52:45 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.04430552199482918 norm:0.0006652541342191398 max memory_allocated 22510.49609375 
[2025-02-23 09:53:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.042646657675504684 norm:0.0004137399373576045 max memory_allocated 22510.49609375 
[2025-02-23 09:53:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.041277263313531876 norm:0.0002488084719516337 max memory_allocated 22510.49609375 
[2025-02-23 09:54:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.04166516661643982 norm:0.00022638894733972847 max memory_allocated 22510.49609375 
[2025-02-23 09:54:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.041522443294525146 norm:0.00021611771080642939 max memory_allocated 22510.49609375 
[2025-02-23 09:55:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.04155752435326576 norm:0.00020415434846654534 max memory_allocated 22510.49609375 
[2025-02-23 09:56:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.04137681424617767 norm:0.000200181981199421 max memory_allocated 22510.49609375 
[2025-02-23 09:56:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.04016401618719101 norm:0.00014922044647391886 max memory_allocated 22510.49609375 
[2025-02-23 09:57:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.04092341661453247 norm:0.0001617470697965473 max memory_allocated 22510.49609375 
[2025-02-23 09:57:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.04119521379470825 norm:0.00017390504945069551 max memory_allocated 22510.49609375 
[2025-02-23 09:58:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.04130256175994873 norm:0.00017018578364513814 max memory_allocated 22510.49609375 
[2025-02-23 09:58:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.04119323566555977 norm:0.00016927873366512358 max memory_allocated 22510.49609375 
[2025-02-23 09:59:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.04032193869352341 norm:0.0001435095618944615 max memory_allocated 22510.49609375 
[2025-02-23 09:59:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.041337184607982635 norm:0.0001691771758487448 max memory_allocated 22510.49609375 
[2025-02-23 10:00:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.040593471378088 norm:0.00015366658044513315 max memory_allocated 22510.49609375 
[2025-02-23 10:01:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.040825460106134415 norm:0.0001559188385726884 max memory_allocated 22510.49609375 
[2025-02-23 10:01:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.040826793760061264 norm:0.00015749517478980124 max memory_allocated 22510.49609375 
[2025-02-23 10:02:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.040739648044109344 norm:0.00015158284804783762 max memory_allocated 22510.49609375 
[2025-02-23 10:02:39 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0406002514064312 norm:0.00015528161020483822 max memory_allocated 22510.49609375 
[2025-02-23 10:02:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-23 10:03:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.05736587569117546 norm:0.004357147496193647 max memory_allocated 22510.66796875 
[2025-02-23 10:03:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.050297632813453674 norm:0.0012912245001643896 max memory_allocated 22510.66796875 
[2025-02-23 10:04:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.048183392733335495 norm:0.0006766406586393714 max memory_allocated 22510.66796875 
[2025-02-23 10:05:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0472862645983696 norm:0.000449839310022071 max memory_allocated 22510.66796875 
[2025-02-23 10:05:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.04684416949748993 norm:0.00035040362854488194 max memory_allocated 22510.66796875 
[2025-02-23 10:06:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.04652133956551552 norm:0.00029584960429929197 max memory_allocated 22510.66796875 
[2025-02-23 10:06:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.046641696244478226 norm:0.00024294738250318915 max memory_allocated 22510.66796875 
[2025-02-23 10:07:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.04678966850042343 norm:0.00023026784765534103 max memory_allocated 22510.66796875 
[2025-02-23 10:07:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.04656963422894478 norm:0.000226334115723148 max memory_allocated 22510.66796875 
[2025-02-23 10:08:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.04664439707994461 norm:0.00020482570107560605 max memory_allocated 22510.66796875 
[2025-02-23 10:08:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.04713524878025055 norm:0.00022412830730900168 max memory_allocated 22510.66796875 
[2025-02-23 10:09:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.04699745401740074 norm:0.00020677220891229808 max memory_allocated 22510.66796875 
[2025-02-23 10:10:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.046921223402023315 norm:0.00020984638831578195 max memory_allocated 22510.66796875 
[2025-02-23 10:10:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.047130707651376724 norm:0.0002081798593280837 max memory_allocated 22510.66796875 
[2025-02-23 10:11:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.046868693083524704 norm:0.0002025946305366233 max memory_allocated 22510.66796875 
[2025-02-23 10:11:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.04686858877539635 norm:0.00020488403970375657 max memory_allocated 22510.66796875 
[2025-02-23 10:12:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0467599518597126 norm:0.0001989907177630812 max memory_allocated 22510.66796875 
[2025-02-23 10:12:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.04688984900712967 norm:0.00020575537928380072 max memory_allocated 22510.66796875 
[2025-02-23 10:13:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.04706230387091637 norm:0.00021308120631147176 max memory_allocated 22510.66796875 
[2025-02-23 10:13:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.046942103654146194 norm:0.00022079725749790668 max memory_allocated 22510.66796875 
[2025-02-23 10:14:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-23 10:14:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.06317756325006485 norm:0.0014866084093227983 max memory_allocated 22510.83984375 
[2025-02-23 10:15:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.056748099625110626 norm:0.0004955738550052047 max memory_allocated 22510.83984375 
[2025-02-23 10:15:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.05479380115866661 norm:0.00031484244391322136 max memory_allocated 22510.83984375 
[2025-02-23 10:16:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.05384417995810509 norm:0.00022308465850073844 max memory_allocated 22510.83984375 
[2025-02-23 10:16:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.053104620426893234 norm:0.00018140175961889327 max memory_allocated 22510.83984375 
[2025-02-23 10:17:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.05272305756807327 norm:0.0001490575377829373 max memory_allocated 22510.83984375 
[2025-02-23 10:17:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.052694741636514664 norm:0.00012762275582645088 max memory_allocated 22510.83984375 
[2025-02-23 10:18:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.05272478982806206 norm:0.0001272314548259601 max memory_allocated 22510.83984375 
[2025-02-23 10:19:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.05287225916981697 norm:0.0001330105005763471 max memory_allocated 22510.83984375 
[2025-02-23 10:19:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.05268571153283119 norm:0.00012106364738428965 max memory_allocated 22510.83984375 
[2025-02-23 10:20:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.05266445130109787 norm:0.0001193541829707101 max memory_allocated 22510.83984375 
[2025-02-23 10:20:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.05260767787694931 norm:0.00012207296094857156 max memory_allocated 22510.83984375 
[2025-02-23 10:21:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.05244019627571106 norm:0.00011697793524945155 max memory_allocated 22510.83984375 
[2025-02-23 10:21:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0522778183221817 norm:0.00011085253936471418 max memory_allocated 22510.83984375 
[2025-02-23 10:22:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.05223695561289787 norm:0.00010951485455734655 max memory_allocated 22510.83984375 
[2025-02-23 10:22:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.05244506150484085 norm:0.00012008439807686955 max memory_allocated 22510.83984375 
[2025-02-23 10:23:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.05273181200027466 norm:0.00013163936091586947 max memory_allocated 22510.83984375 
[2025-02-23 10:24:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.05292854085564613 norm:0.0001286851183976978 max memory_allocated 22510.83984375 
[2025-02-23 10:24:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.05286246910691261 norm:0.0001438824983779341 max memory_allocated 22510.83984375 
[2025-02-23 10:25:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.05295812338590622 norm:0.00013811542885378003 max memory_allocated 22510.83984375 
[2025-02-23 10:25:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-23 10:25:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.07184257358312607 norm:0.0012725064298138022 max memory_allocated 22511.01171875 
[2025-02-23 10:26:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.06425183266401291 norm:0.0005565563915297389 max memory_allocated 22511.01171875 
[2025-02-23 10:26:57 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06161339953541756 norm:0.00043405970791354775 max memory_allocated 22511.01171875 
[2025-02-23 10:27:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.06038348004221916 norm:0.00033938162960112095 max memory_allocated 22511.01171875 
[2025-02-23 10:28:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.059302907437086105 norm:0.0002735857851803303 max memory_allocated 22511.01171875 
[2025-02-23 10:28:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.058506034314632416 norm:0.00022007760708220303 max memory_allocated 22511.01171875 
[2025-02-23 10:29:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.058285318315029144 norm:0.00019156809139531106 max memory_allocated 22511.01171875 
[2025-02-23 10:29:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.05826076492667198 norm:0.00017986551392823458 max memory_allocated 22511.01171875 
[2025-02-23 10:30:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.05831849202513695 norm:0.000167743768543005 max memory_allocated 22511.01171875 
[2025-02-23 10:30:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.058431267738342285 norm:0.00015810300828889012 max memory_allocated 22511.01171875 
[2025-02-23 10:31:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.058290526270866394 norm:0.000147159124026075 max memory_allocated 22511.01171875 
[2025-02-23 10:31:55 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.058301039040088654 norm:0.00014801451470702887 max memory_allocated 22511.01171875 
[2025-02-23 10:32:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.05823637917637825 norm:0.0001414650905644521 max memory_allocated 22511.01171875 
[2025-02-23 10:33:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.05817601829767227 norm:0.00013797005522064865 max memory_allocated 22511.01171875 
[2025-02-23 10:33:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.058633338660001755 norm:0.00014770533016417176 max memory_allocated 22511.01171875 
[2025-02-23 10:34:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.05915772169828415 norm:0.00015265820547938347 max memory_allocated 22511.01171875 
[2025-02-23 10:34:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.059408195316791534 norm:0.00016115469043143094 max memory_allocated 22511.01171875 
[2025-02-23 10:35:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.05937289074063301 norm:0.00015511589299421757 max memory_allocated 22511.01171875 
[2025-02-23 10:35:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.05927363038063049 norm:0.0001471844152547419 max memory_allocated 22511.01171875 
[2025-02-23 10:36:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.059302572160959244 norm:0.00014603236922994256 max memory_allocated 22511.01171875 
[2025-02-23 10:36:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-23 10:37:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.07810021936893463 norm:0.0014735448639839888 max memory_allocated 22511.18359375 
[2025-02-23 10:37:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.06993750482797623 norm:0.0006037602433934808 max memory_allocated 22511.18359375 
[2025-02-23 10:38:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.06699094921350479 norm:0.0004378081939648837 max memory_allocated 22511.18359375 
[2025-02-23 10:38:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.06584416329860687 norm:0.0003456563572399318 max memory_allocated 22511.18359375 
[2025-02-23 10:39:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.06520877778530121 norm:0.000281045853625983 max memory_allocated 22511.18359375 
[2025-02-23 10:39:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.06468674540519714 norm:0.0002271701378049329 max memory_allocated 22511.18359375 
[2025-02-23 10:40:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.06444618850946426 norm:0.00019464279466774315 max memory_allocated 22511.18359375 
[2025-02-23 10:40:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.06422276049852371 norm:0.00018831381748896092 max memory_allocated 22511.18359375 
[2025-02-23 10:41:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.06378190964460373 norm:0.0001549140433780849 max memory_allocated 22511.18359375 
[2025-02-23 10:42:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.06386854499578476 norm:0.0001397866290062666 max memory_allocated 22511.18359375 
[2025-02-23 10:42:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0640181452035904 norm:0.00013648325693793595 max memory_allocated 22511.18359375 
[2025-02-23 10:43:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0639197826385498 norm:0.00012676147161982954 max memory_allocated 22511.18359375 
[2025-02-23 10:43:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.06384295225143433 norm:0.00012203781807329506 max memory_allocated 22511.18359375 
[2025-02-23 10:44:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.06403815746307373 norm:0.00011459059169283137 max memory_allocated 22511.18359375 
[2025-02-23 10:44:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.06411194801330566 norm:0.00011244875349802896 max memory_allocated 22511.18359375 
[2025-02-23 10:45:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.06445290893316269 norm:0.00011641178571153432 max memory_allocated 22511.18359375 
[2025-02-23 10:45:52 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0642998069524765 norm:0.00011081734555773437 max memory_allocated 22511.18359375 
[2025-02-23 10:46:25 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06401936709880829 norm:0.00010405729699414223 max memory_allocated 22511.18359375 
[2025-02-23 10:46:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.06405982375144958 norm:0.0001054865206242539 max memory_allocated 22511.18359375 
[2025-02-23 10:47:31 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.06387923657894135 norm:0.00010087719419971108 max memory_allocated 22511.18359375 
[2025-02-23 10:47:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-23 10:48:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0786600261926651 norm:0.0010565188713371754 max memory_allocated 22511.35546875 
[2025-02-23 10:48:49 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.07362817972898483 norm:0.0006168407271616161 max memory_allocated 22511.35546875 
[2025-02-23 10:49:22 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0715036690235138 norm:0.00041404011426493526 max memory_allocated 22511.35546875 
[2025-02-23 10:49:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.07037509232759476 norm:0.0003462103777565062 max memory_allocated 22511.35546875 
[2025-02-23 10:50:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.069757379591465 norm:0.0002765317913144827 max memory_allocated 22511.35546875 
[2025-02-23 10:51:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.06928000599145889 norm:0.0002272100537084043 max memory_allocated 22511.35546875 
[2025-02-23 10:51:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.06902528554201126 norm:0.0001936673215823248 max memory_allocated 22511.35546875 
[2025-02-23 10:52:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.06883799284696579 norm:0.00016880591283552349 max memory_allocated 22511.35546875 
[2025-02-23 10:52:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.06906420737504959 norm:0.00015599369362462312 max memory_allocated 22511.35546875 
[2025-02-23 10:53:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.06898944824934006 norm:0.0001424490037607029 max memory_allocated 22511.35546875 
[2025-02-23 10:53:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.06884387135505676 norm:0.00012888986384496093 max memory_allocated 22511.35546875 
[2025-02-23 10:54:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.06904424726963043 norm:0.00011744944640668109 max memory_allocated 22511.35546875 
[2025-02-23 10:54:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.06910327076911926 norm:0.00010573976032901555 max memory_allocated 22511.35546875 
[2025-02-23 10:55:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0689493864774704 norm:0.00010163230763282627 max memory_allocated 22511.35546875 
[2025-02-23 10:55:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.06861993670463562 norm:0.00010108129936270416 max memory_allocated 22511.35546875 
[2025-02-23 10:56:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0686265155673027 norm:9.602618956705555e-05 max memory_allocated 22511.35546875 
[2025-02-23 10:57:03 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.06871725618839264 norm:9.642579243518412e-05 max memory_allocated 22511.35546875 
[2025-02-23 10:57:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.06868353486061096 norm:9.758050873642787e-05 max memory_allocated 22511.35546875 
[2025-02-23 10:58:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.06853104382753372 norm:9.542009502183646e-05 max memory_allocated 22511.35546875 
[2025-02-23 10:58:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.06849261373281479 norm:9.490572119830176e-05 max memory_allocated 22511.35546875 
[2025-02-23 10:58:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-23 10:59:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.08253555744886398 norm:0.002092924667522311 max memory_allocated 22511.52734375 
[2025-02-23 11:00:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.07672765105962753 norm:0.0009936192072927952 max memory_allocated 22511.52734375 
[2025-02-23 11:00:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.07495783269405365 norm:0.0007173364283517003 max memory_allocated 22511.52734375 
[2025-02-23 11:01:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.07369521260261536 norm:0.00046793429646641016 max memory_allocated 22511.52734375 
[2025-02-23 11:01:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.07317470759153366 norm:0.00037222274113446474 max memory_allocated 22511.52734375 
[2025-02-23 11:02:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.07276656478643417 norm:0.0003149308904539794 max memory_allocated 22511.52734375 
[2025-02-23 11:02:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.07205577194690704 norm:0.00022782321320846677 max memory_allocated 22511.52734375 
[2025-02-23 11:03:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.07181119173765182 norm:0.0001938309578690678 max memory_allocated 22511.52734375 
[2025-02-23 11:03:52 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.07147736847400665 norm:0.00016794088878668845 max memory_allocated 22511.52734375 
[2025-02-23 11:04:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.07144611328840256 norm:0.00014653627295047045 max memory_allocated 22511.52734375 
[2025-02-23 11:04:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.07139553129673004 norm:0.00013155296619515866 max memory_allocated 22511.52734375 
[2025-02-23 11:05:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.0716465413570404 norm:0.00012589391553774476 max memory_allocated 22511.52734375 
[2025-02-23 11:06:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.07183606922626495 norm:0.00011007707507815212 max memory_allocated 22511.52734375 
[2025-02-23 11:06:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.07191561162471771 norm:0.0001124705740949139 max memory_allocated 22511.52734375 
[2025-02-23 11:07:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.07179424166679382 norm:0.00010437074524816126 max memory_allocated 22511.52734375 
[2025-02-23 11:07:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.07178016752004623 norm:9.72895577433519e-05 max memory_allocated 22511.52734375 
[2025-02-23 11:08:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.07170818746089935 norm:9.226762631442398e-05 max memory_allocated 22511.52734375 
[2025-02-23 11:08:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.07141297310590744 norm:8.641822205390781e-05 max memory_allocated 22511.52734375 
[2025-02-23 11:09:21 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.07138654589653015 norm:8.271270780824125e-05 max memory_allocated 22511.52734375 
[2025-02-23 11:09:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.07149466872215271 norm:8.356608304893598e-05 max memory_allocated 22511.52734375 
[2025-02-23 11:10:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-23 11:10:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.08688969910144806 norm:0.0009518440347164869 max memory_allocated 22511.69921875 
[2025-02-23 11:11:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0812801867723465 norm:0.0005125424358993769 max memory_allocated 22511.69921875 
[2025-02-23 11:11:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.079036645591259 norm:0.0003697309293784201 max memory_allocated 22511.69921875 
[2025-02-23 11:12:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.07783788442611694 norm:0.00028453252161853015 max memory_allocated 22511.69921875 
[2025-02-23 11:12:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0770854651927948 norm:0.0002255680738016963 max memory_allocated 22511.69921875 
[2025-02-23 11:13:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.07662346959114075 norm:0.0001985787384910509 max memory_allocated 22511.69921875 
[2025-02-23 11:13:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.07644405215978622 norm:0.00017880689119920135 max memory_allocated 22511.69921875 
[2025-02-23 11:14:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.07615870982408524 norm:0.00016275944653898478 max memory_allocated 22511.69921875 
[2025-02-23 11:15:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.07597418874502182 norm:0.00014897625078447163 max memory_allocated 22511.69921875 
[2025-02-23 11:15:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.07602516561746597 norm:0.00013413069245871156 max memory_allocated 22511.69921875 
[2025-02-23 11:16:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.07621858268976212 norm:0.00012703012907877564 max memory_allocated 22511.69921875 
[2025-02-23 11:16:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.07614032924175262 norm:0.00011790838470915332 max memory_allocated 22511.69921875 
[2025-02-23 11:17:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.07558740675449371 norm:0.0001062820156221278 max memory_allocated 22511.69921875 
[2025-02-23 11:17:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.07547885179519653 norm:9.778343519428745e-05 max memory_allocated 22511.69921875 
[2025-02-23 11:18:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.075576052069664 norm:9.510773816145957e-05 max memory_allocated 22511.69921875 
[2025-02-23 11:18:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.07565569877624512 norm:9.50523026403971e-05 max memory_allocated 22511.69921875 
[2025-02-23 11:19:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.07556944340467453 norm:9.558238525642082e-05 max memory_allocated 22511.69921875 
[2025-02-23 11:20:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.07556209713220596 norm:9.459026477998123e-05 max memory_allocated 22511.69921875 
[2025-02-23 11:20:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0756017416715622 norm:9.026549378177151e-05 max memory_allocated 22511.69921875 
[2025-02-23 11:21:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.07566392421722412 norm:8.917501691030338e-05 max memory_allocated 22511.69921875 
[2025-02-23 11:21:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-23 11:21:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.08977369219064713 norm:0.0009362671407870948 max memory_allocated 22511.87109375 
[2025-02-23 11:22:24 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.08421587944030762 norm:0.00044175205403007567 max memory_allocated 22511.87109375 
[2025-02-23 11:22:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.08208344876766205 norm:0.000321620813338086 max memory_allocated 22511.87109375 
[2025-02-23 11:23:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0803731307387352 norm:0.00023234264517668635 max memory_allocated 22511.87109375 
[2025-02-23 11:24:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.07965006679296494 norm:0.0001851898559834808 max memory_allocated 22511.87109375 
[2025-02-23 11:24:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0794094130396843 norm:0.000148257939144969 max memory_allocated 22511.87109375 
[2025-02-23 11:25:09 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.07916144281625748 norm:0.00013070124259684235 max memory_allocated 22511.87109375 
[2025-02-23 11:25:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.07884176820516586 norm:0.00012240553041920066 max memory_allocated 22511.87109375 
[2025-02-23 11:26:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.07881906628608704 norm:0.00011059075768571347 max memory_allocated 22511.87109375 
[2025-02-23 11:26:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.07891480624675751 norm:0.00010247847239952534 max memory_allocated 22511.87109375 
[2025-02-23 11:27:21 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.07915214449167252 norm:0.00010863421630347148 max memory_allocated 22511.87109375 
[2025-02-23 11:27:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.07919690757989883 norm:0.00010545690020080656 max memory_allocated 22511.87109375 
[2025-02-23 11:28:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0791085809469223 norm:0.00010046697570942342 max memory_allocated 22511.87109375 
[2025-02-23 11:29:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.07893148064613342 norm:9.511787357041612e-05 max memory_allocated 22511.87109375 
[2025-02-23 11:29:32 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.07911509275436401 norm:0.00010049049888039008 max memory_allocated 22511.87109375 
[2025-02-23 11:30:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.07920844852924347 norm:0.00010223413119092584 max memory_allocated 22511.87109375 
[2025-02-23 11:30:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.07920259982347488 norm:0.00010249855404254049 max memory_allocated 22511.87109375 
[2025-02-23 11:31:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.07926186174154282 norm:0.00010446535452501848 max memory_allocated 22511.87109375 
[2025-02-23 11:31:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.07929105311632156 norm:0.00010677313548512757 max memory_allocated 22511.87109375 
[2025-02-23 11:32:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0792560800909996 norm:0.00010470971028553322 max memory_allocated 22511.87109375 
[2025-02-23 11:32:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-23 11:33:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.096328005194664 norm:0.0009268678841181099 max memory_allocated 22512.04296875 
[2025-02-23 11:33:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.09071803092956543 norm:0.0004595172358676791 max memory_allocated 22512.04296875 
[2025-02-23 11:34:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.08833326399326324 norm:0.00032069452572613955 max memory_allocated 22512.04296875 
[2025-02-23 11:34:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.08722204715013504 norm:0.00024921286967583 max memory_allocated 22512.04296875 
[2025-02-23 11:35:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.08602990955114365 norm:0.0002173472457798198 max memory_allocated 22512.04296875 
[2025-02-23 11:35:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.08538192510604858 norm:0.00018098474538419396 max memory_allocated 22512.04296875 
[2025-02-23 11:36:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.08510133624076843 norm:0.00016487500397488475 max memory_allocated 22512.04296875 
[2025-02-23 11:36:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.08479137718677521 norm:0.00014814554015174508 max memory_allocated 22512.04296875 
[2025-02-23 11:37:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0847092792391777 norm:0.00013570665032602847 max memory_allocated 22512.04296875 
[2025-02-23 11:37:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.08457895368337631 norm:0.00011863044346682727 max memory_allocated 22512.04296875 
[2025-02-23 11:38:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.08447591960430145 norm:0.0001069181744242087 max memory_allocated 22512.04296875 
[2025-02-23 11:39:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.08470764756202698 norm:0.00010527642734814435 max memory_allocated 22512.04296875 
[2025-02-23 11:39:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.08467654883861542 norm:0.00010010538971982896 max memory_allocated 22512.04296875 
[2025-02-23 11:40:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.08444453775882721 norm:9.293038601754233e-05 max memory_allocated 22512.04296875 
[2025-02-23 11:40:44 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.08437959104776382 norm:9.050394874066114e-05 max memory_allocated 22512.04296875 
[2025-02-23 11:41:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.08437657356262207 norm:8.943981083575636e-05 max memory_allocated 22512.04296875 
[2025-02-23 11:41:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.08431950211524963 norm:8.839344809530303e-05 max memory_allocated 22512.04296875 
[2025-02-23 11:42:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.08437054604291916 norm:8.785707177594304e-05 max memory_allocated 22512.04296875 
[2025-02-23 11:42:56 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.08433011174201965 norm:8.781012002145872e-05 max memory_allocated 22512.04296875 
[2025-02-23 11:43:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.08433493971824646 norm:8.791477011982352e-05 max memory_allocated 22512.04296875 
[2025-02-23 11:43:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-23 11:44:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.09995078295469284 norm:0.0006824855227023363 max memory_allocated 22512.21484375 
[2025-02-23 11:44:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.09546860307455063 norm:0.0003403937444090843 max memory_allocated 22512.21484375 
[2025-02-23 11:45:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.09305872023105621 norm:0.00023930530005600303 max memory_allocated 22512.21484375 
[2025-02-23 11:45:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0918082520365715 norm:0.00018334839842282236 max memory_allocated 22512.21484375 
[2025-02-23 11:46:26 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.09094449877738953 norm:0.00016531525761820376 max memory_allocated 22512.21484375 
[2025-02-23 11:46:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.09048494696617126 norm:0.00014425348490476608 max memory_allocated 22512.21484375 
[2025-02-23 11:47:31 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0901266559958458 norm:0.00011197960702702403 max memory_allocated 22512.21484375 
[2025-02-23 11:48:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0897003635764122 norm:9.972405678126961e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:48:37 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.08956661075353622 norm:9.495353151578456e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:49:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.08954416960477829 norm:9.049277286976576e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:49:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.08953768014907837 norm:8.931796764954925e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:50:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.08957422524690628 norm:8.840141526889056e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:50:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.08957504481077194 norm:8.72959935804829e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:51:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.08946322649717331 norm:8.504266588715836e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:51:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.08951201289892197 norm:8.555749809602275e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:52:28 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.08929494023323059 norm:8.189221989596263e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:53:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.08920550346374512 norm:8.022061956580728e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:53:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.08920975774526596 norm:8.027265721466392e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:54:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.08926429599523544 norm:8.056729711825028e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:54:40 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.08925673365592957 norm:8.181779412552714e-05 max memory_allocated 22512.21484375 
[2025-02-23 11:54:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-23 11:55:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.11016984283924103 norm:0.0014127022586762905 max memory_allocated 22512.38671875 
[2025-02-23 11:55:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.10405443608760834 norm:0.0005564417224377394 max memory_allocated 22512.38671875 
[2025-02-23 11:56:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.10164181143045425 norm:0.00037797880941070616 max memory_allocated 22512.38671875 
[2025-02-23 11:57:04 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.10022405534982681 norm:0.000292845768854022 max memory_allocated 22512.38671875 
[2025-02-23 11:57:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0990862101316452 norm:0.00022520014317706227 max memory_allocated 22512.38671875 
[2025-02-23 11:58:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.09842058271169662 norm:0.00018919397552963346 max memory_allocated 22512.38671875 
[2025-02-23 11:58:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.09788816422224045 norm:0.00015745023847557604 max memory_allocated 22512.38671875 
[2025-02-23 11:59:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0973665863275528 norm:0.00012822405551560223 max memory_allocated 22512.38671875 
[2025-02-23 11:59:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.09726031124591827 norm:0.00011819404608104378 max memory_allocated 22512.38671875 
[2025-02-23 12:00:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.09722645580768585 norm:0.0001111762976506725 max memory_allocated 22512.38671875 
[2025-02-23 12:00:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.0973527580499649 norm:0.00011246754002058879 max memory_allocated 22512.38671875 
[2025-02-23 12:01:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.09756128489971161 norm:0.00011135650856886059 max memory_allocated 22512.38671875 
[2025-02-23 12:02:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.09747839719057083 norm:0.00010642177221598104 max memory_allocated 22512.38671875 
[2025-02-23 12:02:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.09750604629516602 norm:0.00010592448961688206 max memory_allocated 22512.38671875 
[2025-02-23 12:03:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.0973675400018692 norm:0.00010226437007077038 max memory_allocated 22512.38671875 
[2025-02-23 12:03:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.09746558964252472 norm:0.00010515371832298115 max memory_allocated 22512.38671875 
[2025-02-23 12:04:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.09748873859643936 norm:0.00010636728256940842 max memory_allocated 22512.38671875 
[2025-02-23 12:04:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.09756700694561005 norm:0.00011037900549126789 max memory_allocated 22512.38671875 
[2025-02-23 12:05:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.09752850234508514 norm:0.00010681405547074974 max memory_allocated 22512.38671875 
[2025-02-23 12:05:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.09758584201335907 norm:0.00011000013910233974 max memory_allocated 22512.38671875 
[2025-02-23 12:06:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-23 12:06:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.12340207397937775 norm:0.0016325591132044792 max memory_allocated 22512.55859375 
[2025-02-23 12:07:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.11750482022762299 norm:0.0006890331278555095 max memory_allocated 22512.55859375 
[2025-02-23 12:07:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.11484828591346741 norm:0.0004508935962803662 max memory_allocated 22512.55859375 
[2025-02-23 12:08:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.11308250576257706 norm:0.0003318263916298747 max memory_allocated 22512.55859375 
[2025-02-23 12:08:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.1119692400097847 norm:0.0002594326506368816 max memory_allocated 22512.55859375 
[2025-02-23 12:09:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.11108429729938507 norm:0.00022027244267519563 max memory_allocated 22512.55859375 
[2025-02-23 12:09:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.1105029508471489 norm:0.0001929731370182708 max memory_allocated 22512.55859375 
[2025-02-23 12:10:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.11023395508527756 norm:0.00017995249072555453 max memory_allocated 22512.55859375 
[2025-02-23 12:11:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.11016251146793365 norm:0.00016099044296424836 max memory_allocated 22512.55859375 
[2025-02-23 12:11:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.11017529666423798 norm:0.00015825475566089153 max memory_allocated 22512.55859375 
[2025-02-23 12:12:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.10985621809959412 norm:0.00015437525871675462 max memory_allocated 22512.55859375 
[2025-02-23 12:12:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.10974729061126709 norm:0.00015303301915992051 max memory_allocated 22512.55859375 
[2025-02-23 12:13:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.10968698561191559 norm:0.0001561738463351503 max memory_allocated 22512.55859375 
[2025-02-23 12:13:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.10949184745550156 norm:0.00015301055100280792 max memory_allocated 22512.55859375 
[2025-02-23 12:14:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.10955765843391418 norm:0.0001527973508927971 max memory_allocated 22512.55859375 
[2025-02-23 12:14:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.10965035855770111 norm:0.00015424024604726583 max memory_allocated 22512.55859375 
[2025-02-23 12:15:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.1096387505531311 norm:0.00015432690270245075 max memory_allocated 22512.55859375 
[2025-02-23 12:15:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.10964114218950272 norm:0.00015390230691991746 max memory_allocated 22512.55859375 
[2025-02-23 12:16:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.10961881279945374 norm:0.00015456482651643455 max memory_allocated 22512.55859375 
[2025-02-23 12:17:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.10961975902318954 norm:0.00015502051974181086 max memory_allocated 22512.55859375 
[2025-02-23 12:17:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-23 12:17:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.13888899981975555 norm:0.0014056640211492777 max memory_allocated 22512.73046875 
[2025-02-23 12:18:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.13345924019813538 norm:0.0006493944092653692 max memory_allocated 22512.73046875 
[2025-02-23 12:18:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.13074073195457458 norm:0.0004106805135961622 max memory_allocated 22512.73046875 
[2025-02-23 12:19:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.12876662611961365 norm:0.00029883740353398025 max memory_allocated 22512.73046875 
[2025-02-23 12:20:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.12748725712299347 norm:0.0002450350730214268 max memory_allocated 22512.73046875 
[2025-02-23 12:20:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.12673316895961761 norm:0.00020616265828721225 max memory_allocated 22512.73046875 
[2025-02-23 12:21:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.12618666887283325 norm:0.0001710479991743341 max memory_allocated 22512.73046875 
[2025-02-23 12:21:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.1256740242242813 norm:0.00015394877118524164 max memory_allocated 22512.73046875 
[2025-02-23 12:22:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.12529312074184418 norm:0.00013711460633203387 max memory_allocated 22512.73046875 
[2025-02-23 12:22:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.12506350874900818 norm:0.00012797635281458497 max memory_allocated 22512.73046875 
[2025-02-23 12:23:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.1249297484755516 norm:0.00012342879199422896 max memory_allocated 22512.73046875 
[2025-02-23 12:23:51 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.12469165027141571 norm:0.00011871260358020663 max memory_allocated 22512.73046875 
[2025-02-23 12:24:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.12464958429336548 norm:0.0001175414799945429 max memory_allocated 22512.73046875 
[2025-02-23 12:24:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.12473791837692261 norm:0.0001196254015667364 max memory_allocated 22512.73046875 
[2025-02-23 12:25:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.12482665479183197 norm:0.00012000026617897674 max memory_allocated 22512.73046875 
[2025-02-23 12:26:03 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.12486723065376282 norm:0.00012071718811057508 max memory_allocated 22512.73046875 
[2025-02-23 12:26:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.12484519183635712 norm:0.00011987928883172572 max memory_allocated 22512.73046875 
[2025-02-23 12:27:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.12483974546194077 norm:0.00011771506979130208 max memory_allocated 22512.73046875 
[2025-02-23 12:27:42 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.12488923966884613 norm:0.0001210267873830162 max memory_allocated 22512.73046875 
[2025-02-23 12:28:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.12486279010772705 norm:0.00012090009840903804 max memory_allocated 22512.73046875 
[2025-02-23 12:28:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-23 12:29:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.15955860912799835 norm:0.0014364533126354218 max memory_allocated 22512.90234375 
[2025-02-23 12:29:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.15395767986774445 norm:0.0006481567397713661 max memory_allocated 22512.90234375 
[2025-02-23 12:30:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.1510894000530243 norm:0.00041029631393030286 max memory_allocated 22512.90234375 
[2025-02-23 12:30:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.14914166927337646 norm:0.00029820494819432497 max memory_allocated 22512.90234375 
[2025-02-23 12:31:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.14789344370365143 norm:0.00025159912183880806 max memory_allocated 22512.90234375 
[2025-02-23 12:31:45 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.14693868160247803 norm:0.00020087762095499784 max memory_allocated 22512.90234375 
[2025-02-23 12:32:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.14636115729808807 norm:0.00018032851221505553 max memory_allocated 22512.90234375 
[2025-02-23 12:32:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.14607006311416626 norm:0.0001665037707425654 max memory_allocated 22512.90234375 
[2025-02-23 12:33:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.14568065106868744 norm:0.0001535262999823317 max memory_allocated 22512.90234375 
[2025-02-23 12:33:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.14536452293395996 norm:0.0001457340840715915 max memory_allocated 22512.90234375 
[2025-02-23 12:34:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.14498016238212585 norm:0.0001343907497357577 max memory_allocated 22512.90234375 
[2025-02-23 12:35:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.1447400152683258 norm:0.0001262227160623297 max memory_allocated 22512.90234375 
[2025-02-23 12:35:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.1446044147014618 norm:0.00012239976786077023 max memory_allocated 22512.90234375 
[2025-02-23 12:36:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.14444434642791748 norm:0.00011772504512919113 max memory_allocated 22512.90234375 
[2025-02-23 12:36:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.14443661272525787 norm:0.00011584103776840493 max memory_allocated 22512.90234375 
[2025-02-23 12:37:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.144400492310524 norm:0.00011514473590068519 max memory_allocated 22512.90234375 
[2025-02-23 12:37:47 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.14442503452301025 norm:0.00011644612823147327 max memory_allocated 22512.90234375 
[2025-02-23 12:38:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.14443419873714447 norm:0.00011579304555198178 max memory_allocated 22512.90234375 
[2025-02-23 12:38:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.1443684697151184 norm:0.00011458361404947937 max memory_allocated 22512.90234375 
[2025-02-23 12:39:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.14444079995155334 norm:0.00011533971701283008 max memory_allocated 22512.90234375 
[2025-02-23 12:39:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-23 12:40:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.1932857781648636 norm:0.004159948322921991 max memory_allocated 22513.07421875 
[2025-02-23 12:40:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.18530403077602386 norm:0.0017207819037139416 max memory_allocated 22513.07421875 
[2025-02-23 12:41:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.18115901947021484 norm:0.0009355915244668722 max memory_allocated 22513.07421875 
[2025-02-23 12:41:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.17822520434856415 norm:0.000654222967568785 max memory_allocated 22513.07421875 
[2025-02-23 12:42:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.17668965458869934 norm:0.0005314060254022479 max memory_allocated 22513.07421875 
[2025-02-23 12:42:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.17562858760356903 norm:0.0004177379887551069 max memory_allocated 22513.07421875 
[2025-02-23 12:43:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.17484423518180847 norm:0.0003272288595326245 max memory_allocated 22513.07421875 
[2025-02-23 12:44:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.17435282468795776 norm:0.00025704188738018274 max memory_allocated 22513.07421875 
[2025-02-23 12:44:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.1737767904996872 norm:0.00023087693261913955 max memory_allocated 22513.07421875 
[2025-02-23 12:45:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.17318964004516602 norm:0.00020160619169473648 max memory_allocated 22513.07421875 
[2025-02-23 12:45:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.17319196462631226 norm:0.00018439165432937443 max memory_allocated 22513.07421875 
[2025-02-23 12:46:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.17278680205345154 norm:0.00017162655421998352 max memory_allocated 22513.07421875 
[2025-02-23 12:46:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.17271369695663452 norm:0.00016917649190872908 max memory_allocated 22513.07421875 
[2025-02-23 12:47:20 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.17257031798362732 norm:0.00016825582133606076 max memory_allocated 22513.07421875 
[2025-02-23 12:47:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.17222291231155396 norm:0.00016159149527084082 max memory_allocated 22513.07421875 
[2025-02-23 12:48:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.17219431698322296 norm:0.00015623532817699015 max memory_allocated 22513.07421875 
[2025-02-23 12:48:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.17212603986263275 norm:0.0001568561710882932 max memory_allocated 22513.07421875 
[2025-02-23 12:49:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.17205919325351715 norm:0.00015950252418406308 max memory_allocated 22513.07421875 
[2025-02-23 12:50:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.1720816045999527 norm:0.00016184414562303573 max memory_allocated 22513.07421875 
[2025-02-23 12:50:38 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.1720527857542038 norm:0.00016099732602015138 max memory_allocated 22513.07421875 
[2025-02-23 12:50:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-23 12:51:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.22336865961551666 norm:0.0011509368196129799 max memory_allocated 22513.24609375 
[2025-02-23 12:51:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.21754512190818787 norm:0.0005699881003238261 max memory_allocated 22513.24609375 
[2025-02-23 12:52:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.2132970541715622 norm:0.00037121053901501 max memory_allocated 22513.24609375 
[2025-02-23 12:53:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.2108066827058792 norm:0.00031167545239441097 max memory_allocated 22513.24609375 
[2025-02-23 12:53:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.2090403139591217 norm:0.0002848216099664569 max memory_allocated 22513.24609375 
[2025-02-23 12:54:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.20757043361663818 norm:0.00024719422799535096 max memory_allocated 22513.24609375 
[2025-02-23 12:54:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.20662739872932434 norm:0.00023003562819212675 max memory_allocated 22513.24609375 
[2025-02-23 12:55:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.20622453093528748 norm:0.00021390953043010086 max memory_allocated 22513.24609375 
[2025-02-23 12:55:47 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.2053903043270111 norm:0.00019289820920675993 max memory_allocated 22513.24609375 
[2025-02-23 12:56:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.20478211343288422 norm:0.00018100290617439896 max memory_allocated 22513.24609375 
[2025-02-23 12:56:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.20420092344284058 norm:0.00016503695223946124 max memory_allocated 22513.24609375 
[2025-02-23 12:57:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.20391592383384705 norm:0.0001604998396942392 max memory_allocated 22513.24609375 
[2025-02-23 12:57:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.2036553919315338 norm:0.00015876899124123156 max memory_allocated 22513.24609375 
[2025-02-23 12:58:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.20349113643169403 norm:0.00015747523866593838 max memory_allocated 22513.24609375 
[2025-02-23 12:59:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.2033243328332901 norm:0.00015388266183435917 max memory_allocated 22513.24609375 
[2025-02-23 12:59:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.20322862267494202 norm:0.00015544115740340203 max memory_allocated 22513.24609375 
[2025-02-23 13:00:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.2031916379928589 norm:0.0001564723497722298 max memory_allocated 22513.24609375 
[2025-02-23 13:00:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.2031928151845932 norm:0.00015424509183503687 max memory_allocated 22513.24609375 
[2025-02-23 13:01:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.20322129130363464 norm:0.00015608158719260246 max memory_allocated 22513.24609375 
[2025-02-23 13:01:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.2032485008239746 norm:0.00015622645150870085 max memory_allocated 22513.24609375 
[2025-02-23 13:01:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-23 13:02:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.2574353814125061 norm:0.0019944608211517334 max memory_allocated 22513.41796875 
[2025-02-23 13:03:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.2513248324394226 norm:0.0009534077253192663 max memory_allocated 22513.41796875 
[2025-02-23 13:03:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.24786211550235748 norm:0.0006119132158346474 max memory_allocated 22513.41796875 
[2025-02-23 13:04:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.24526265263557434 norm:0.00041797812446020544 max memory_allocated 22513.41796875 
[2025-02-23 13:04:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.24350066483020782 norm:0.00033313652966171503 max memory_allocated 22513.41796875 
[2025-02-23 13:05:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.24211712181568146 norm:0.0002848187286872417 max memory_allocated 22513.41796875 
[2025-02-23 13:05:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.2413305640220642 norm:0.0002646753564476967 max memory_allocated 22513.41796875 
[2025-02-23 13:06:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.2402503788471222 norm:0.0002467021986376494 max memory_allocated 22513.41796875 
[2025-02-23 13:06:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.2397080957889557 norm:0.0002351772563997656 max memory_allocated 22513.41796875 
[2025-02-23 13:07:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.23958763480186462 norm:0.00023941328981891274 max memory_allocated 22513.41796875 
[2025-02-23 13:08:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.23968054354190826 norm:0.0002508228353690356 max memory_allocated 22513.41796875 
[2025-02-23 13:08:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.23941951990127563 norm:0.00025262232520617545 max memory_allocated 22513.41796875 
[2025-02-23 13:09:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.2389231026172638 norm:0.00024711337755434215 max memory_allocated 22513.41796875 
[2025-02-23 13:09:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.2388705015182495 norm:0.00024782834225334227 max memory_allocated 22513.41796875 
[2025-02-23 13:10:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.23868955671787262 norm:0.0002485050936229527 max memory_allocated 22513.41796875 
[2025-02-23 13:10:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.2384677678346634 norm:0.0002447081496939063 max memory_allocated 22513.41796875 
[2025-02-23 13:11:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.23833096027374268 norm:0.00024284180835820735 max memory_allocated 22513.41796875 
[2025-02-23 13:11:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.23830513656139374 norm:0.00024404535361099988 max memory_allocated 22513.41796875 
[2025-02-23 13:12:28 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.238051638007164 norm:0.0002318665210623294 max memory_allocated 22513.41796875 
[2025-02-23 13:13:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.23792026937007904 norm:0.0002315977617399767 max memory_allocated 22513.41796875 
[2025-02-23 13:13:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-23 13:13:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.3014112114906311 norm:0.0012065423652529716 max memory_allocated 22513.58984375 
[2025-02-23 13:14:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.2951764464378357 norm:0.0006565127405337989 max memory_allocated 22513.58984375 
[2025-02-23 13:14:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.2910723090171814 norm:0.00046124454820528626 max memory_allocated 22513.58984375 
[2025-02-23 13:15:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.2884819507598877 norm:0.0003789993061218411 max memory_allocated 22513.58984375 
[2025-02-23 13:15:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.2866901457309723 norm:0.0003033495740965009 max memory_allocated 22513.58984375 
[2025-02-23 13:16:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.2853800058364868 norm:0.0002618101716507226 max memory_allocated 22513.58984375 
[2025-02-23 13:17:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.2843629717826843 norm:0.00024172698613256216 max memory_allocated 22513.58984375 
[2025-02-23 13:17:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.2836746871471405 norm:0.00021785672288388014 max memory_allocated 22513.58984375 
[2025-02-23 13:18:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.28241556882858276 norm:0.00019338258425705135 max memory_allocated 22513.58984375 
[2025-02-23 13:18:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.2817872166633606 norm:0.00018909096252173185 max memory_allocated 22513.58984375 
[2025-02-23 13:19:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.2816912531852722 norm:0.00018511974485591054 max memory_allocated 22513.58984375 
[2025-02-23 13:19:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.2811422646045685 norm:0.00018704324611462653 max memory_allocated 22513.58984375 
[2025-02-23 13:20:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.2809901535511017 norm:0.0001842181954998523 max memory_allocated 22513.58984375 
[2025-02-23 13:20:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.28177356719970703 norm:0.0001864598598331213 max memory_allocated 22513.58984375 
[2025-02-23 13:21:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.28206008672714233 norm:0.0001971144083654508 max memory_allocated 22513.58984375 
[2025-02-23 13:22:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.28216254711151123 norm:0.0002106505708070472 max memory_allocated 22513.58984375 
[2025-02-23 13:22:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.28095850348472595 norm:0.0002583827299531549 max memory_allocated 22513.58984375 
[2025-02-23 13:23:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.28065481781959534 norm:0.0001825734943849966 max memory_allocated 22513.58984375 
[2025-02-23 13:23:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.28121936321258545 norm:0.00019147169950883836 max memory_allocated 22513.58984375 
[2025-02-23 13:24:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.2814745306968689 norm:0.0001925796241266653 max memory_allocated 22513.58984375 
[2025-02-23 13:24:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-23 13:25:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.3481161892414093 norm:0.0023622168228030205 max memory_allocated 22513.76171875 
[2025-02-23 13:25:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.3420591950416565 norm:0.0014926238218322396 max memory_allocated 22513.76171875 
[2025-02-23 13:26:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.3381704092025757 norm:0.001055450295098126 max memory_allocated 22513.76171875 
[2025-02-23 13:26:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.3353009819984436 norm:0.0007823349442332983 max memory_allocated 22513.76171875 
[2025-02-23 13:27:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.33304333686828613 norm:0.0006444882019422948 max memory_allocated 22513.76171875 
[2025-02-23 13:27:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.3312201201915741 norm:0.0005163771566003561 max memory_allocated 22513.76171875 
[2025-02-23 13:28:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.3300577700138092 norm:0.000415718270232901 max memory_allocated 22513.76171875 
[2025-02-23 13:28:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.3295139670372009 norm:0.00036255051963962615 max memory_allocated 22513.76171875 
[2025-02-23 13:29:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.32882198691368103 norm:0.00033467987668700516 max memory_allocated 22513.76171875 
[2025-02-23 13:29:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.32778307795524597 norm:0.0003078039735555649 max memory_allocated 22513.76171875 
[2025-02-23 13:30:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.32717031240463257 norm:0.0002925350272562355 max memory_allocated 22513.76171875 
[2025-02-23 13:31:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.32691365480422974 norm:0.0002843967522494495 max memory_allocated 22513.76171875 
[2025-02-23 13:31:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.32678061723709106 norm:0.00027723057428374887 max memory_allocated 22513.76171875 
[2025-02-23 13:32:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.3266443610191345 norm:0.0002758106857072562 max memory_allocated 22513.76171875 
[2025-02-23 13:32:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.326681911945343 norm:0.0002740893105510622 max memory_allocated 22513.76171875 
[2025-02-23 13:33:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.32671746611595154 norm:0.00026065847487188876 max memory_allocated 22513.76171875 
[2025-02-23 13:33:47 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.3266671597957611 norm:0.0002592408563941717 max memory_allocated 22513.76171875 
[2025-02-23 13:34:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.3265244662761688 norm:0.0002575863909441978 max memory_allocated 22513.76171875 
[2025-02-23 13:34:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.3264603614807129 norm:0.0002554009261075407 max memory_allocated 22513.76171875 
[2025-02-23 13:35:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.3264074921607971 norm:0.0002553981030359864 max memory_allocated 22513.76171875 
[2025-02-23 13:35:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-23 13:36:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.4026072919368744 norm:0.0021273591555655003 max memory_allocated 22513.93359375 
[2025-02-23 13:36:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.39536404609680176 norm:0.0010454527800902724 max memory_allocated 22513.93359375 
[2025-02-23 13:37:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.3916289210319519 norm:0.0007255967357195914 max memory_allocated 22513.93359375 
[2025-02-23 13:37:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.3884468078613281 norm:0.0005568524356931448 max memory_allocated 22513.93359375 
[2025-02-23 13:38:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.3863486647605896 norm:0.0004533462051767856 max memory_allocated 22513.93359375 
[2025-02-23 13:38:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.38522863388061523 norm:0.0003920210583601147 max memory_allocated 22513.93359375 
[2025-02-23 13:39:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.38387972116470337 norm:0.00035607750760391355 max memory_allocated 22513.93359375 
[2025-02-23 13:40:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.3832768499851227 norm:0.0003324291901662946 max memory_allocated 22513.93359375 
[2025-02-23 13:40:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.3825152516365051 norm:0.0003247088461648673 max memory_allocated 22513.93359375 
[2025-02-23 13:41:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.381992906332016 norm:0.00031047032098285854 max memory_allocated 22513.93359375 
[2025-02-23 13:41:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.3816230297088623 norm:0.00030635984148830175 max memory_allocated 22513.93359375 
[2025-02-23 13:42:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.3810790181159973 norm:0.00031354170641861856 max memory_allocated 22513.93359375 
[2025-02-23 13:42:45 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.38048744201660156 norm:0.0003064615011680871 max memory_allocated 22513.93359375 
[2025-02-23 13:43:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.3803677558898926 norm:0.0002998918353114277 max memory_allocated 22513.93359375 
[2025-02-23 13:43:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.3802068829536438 norm:0.00030779477674514055 max memory_allocated 22513.93359375 
[2025-02-23 13:44:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.3801998198032379 norm:0.0003131093690171838 max memory_allocated 22513.93359375 
[2025-02-23 13:44:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.3800870478153229 norm:0.0003094234853051603 max memory_allocated 22513.93359375 
[2025-02-23 13:45:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.379930317401886 norm:0.0003029405779670924 max memory_allocated 22513.93359375 
[2025-02-23 13:46:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.3798997700214386 norm:0.00031362142181023955 max memory_allocated 22513.93359375 
[2025-02-23 13:46:36 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.3799494504928589 norm:0.00029783399077132344 max memory_allocated 22513.93359375 
[2025-02-23 13:46:46 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-23 13:47:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.46558916568756104 norm:0.003275318071246147 max memory_allocated 22514.10546875 
[2025-02-23 13:47:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.45774853229522705 norm:0.0017613377422094345 max memory_allocated 22514.10546875 
[2025-02-23 13:48:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.4534913897514343 norm:0.0012330294121056795 max memory_allocated 22514.10546875 
[2025-02-23 13:49:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.45080119371414185 norm:0.000921444792766124 max memory_allocated 22514.10546875 
[2025-02-23 13:49:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.4485129117965698 norm:0.0007193117053247988 max memory_allocated 22514.10546875 
[2025-02-23 13:50:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.4466092884540558 norm:0.0005648222286254168 max memory_allocated 22514.10546875 
[2025-02-23 13:50:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.4457753896713257 norm:0.000463217991637066 max memory_allocated 22514.10546875 
[2025-02-23 13:51:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.44462913274765015 norm:0.0004085339605808258 max memory_allocated 22514.10546875 
[2025-02-23 13:51:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.44321393966674805 norm:0.00039418379310518503 max memory_allocated 22514.10546875 
[2025-02-23 13:52:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.442425936460495 norm:0.0003710435703396797 max memory_allocated 22514.10546875 
[2025-02-23 13:52:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.4421246647834778 norm:0.0003629392303992063 max memory_allocated 22514.10546875 
[2025-02-23 13:53:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.4418334662914276 norm:0.0003538847086019814 max memory_allocated 22514.10546875 
[2025-02-23 13:53:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.44185590744018555 norm:0.00035326776560395956 max memory_allocated 22514.10546875 
[2025-02-23 13:54:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.4416925013065338 norm:0.0003519835590850562 max memory_allocated 22514.10546875 
[2025-02-23 13:55:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.4411904811859131 norm:0.00035621196730062366 max memory_allocated 22514.10546875 
[2025-02-23 13:55:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.4410579800605774 norm:0.00035606432356871665 max memory_allocated 22514.10546875 
[2025-02-23 13:56:09 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.4409114420413971 norm:0.0003546816296875477 max memory_allocated 22514.10546875 
[2025-02-23 13:56:42 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.44080227613449097 norm:0.00035494117764756083 max memory_allocated 22514.10546875 
[2025-02-23 13:57:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.4407272934913635 norm:0.0003565582155715674 max memory_allocated 22514.10546875 
[2025-02-23 13:57:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.4406050741672516 norm:0.0003569814725778997 max memory_allocated 22514.10546875 
[2025-02-23 13:57:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-23 13:58:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.5206003189086914 norm:0.0010575868654996157 max memory_allocated 22514.27734375 
[2025-02-23 13:59:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.5107696652412415 norm:0.0005972463404759765 max memory_allocated 22514.27734375 
[2025-02-23 13:59:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.5051782131195068 norm:0.00046804946032352746 max memory_allocated 22514.27734375 
[2025-02-23 14:00:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.501991868019104 norm:0.0003981850459240377 max memory_allocated 22514.27734375 
[2025-02-23 14:00:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.5000389218330383 norm:0.00034517724998295307 max memory_allocated 22514.27734375 
[2025-02-23 14:01:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.49820274114608765 norm:0.0003355083172209561 max memory_allocated 22514.27734375 
[2025-02-23 14:01:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.49736469984054565 norm:0.0003255130141042173 max memory_allocated 22514.27734375 
[2025-02-23 14:02:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.49705278873443604 norm:0.0003229653520975262 max memory_allocated 22514.27734375 
[2025-02-23 14:02:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.4961320757865906 norm:0.00034167818375863135 max memory_allocated 22514.27734375 
[2025-02-23 14:03:30 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.4946783483028412 norm:0.000309099443256855 max memory_allocated 22514.27734375 
[2025-02-23 14:04:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.49373483657836914 norm:0.00028042387566529214 max memory_allocated 22514.27734375 
[2025-02-23 14:04:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.49307796359062195 norm:0.00025391404051333666 max memory_allocated 22514.27734375 
[2025-02-23 14:05:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.49283310770988464 norm:0.0002433893532725051 max memory_allocated 22514.27734375 
[2025-02-23 14:05:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.4931567907333374 norm:0.0002707006351556629 max memory_allocated 22514.27734375 
[2025-02-23 14:06:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.4936392903327942 norm:0.0002699989126995206 max memory_allocated 22514.27734375 
[2025-02-23 14:06:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.49214738607406616 norm:0.0002445307618472725 max memory_allocated 22514.27734375 
[2025-02-23 14:07:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.4923969507217407 norm:0.00023154592781793326 max memory_allocated 22514.27734375 
[2025-02-23 14:07:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.4926745891571045 norm:0.00026437066844664514 max memory_allocated 22514.27734375 
[2025-02-23 14:08:26 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.49268704652786255 norm:0.0002459216630086303 max memory_allocated 22514.27734375 
[2025-02-23 14:08:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.49278202652931213 norm:0.00026760707260109484 max memory_allocated 22514.27734375 
[2025-02-23 14:09:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-23 14:09:45 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.6141011714935303 norm:0.005001329351216555 max memory_allocated 22514.44921875 
[2025-02-23 14:10:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.6004039645195007 norm:0.0023246395867317915 max memory_allocated 22514.44921875 
[2025-02-23 14:10:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.5942343473434448 norm:0.0015109921805560589 max memory_allocated 22514.44921875 
[2025-02-23 14:11:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.5890050530433655 norm:0.0011133786756545305 max memory_allocated 22514.44921875 
[2025-02-23 14:11:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.5859130620956421 norm:0.0008514492074027658 max memory_allocated 22514.44921875 
[2025-02-23 14:12:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.5853812098503113 norm:0.0006970459362491965 max memory_allocated 22514.44921875 
[2025-02-23 14:13:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.5842714309692383 norm:0.0006107629742473364 max memory_allocated 22514.44921875 
[2025-02-23 14:13:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.5840754508972168 norm:0.0005786697147414088 max memory_allocated 22514.44921875 
[2025-02-23 14:14:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.5831868648529053 norm:0.0005058889510110021 max memory_allocated 22514.44921875 
[2025-02-23 14:14:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.5819318294525146 norm:0.0005013689515180886 max memory_allocated 22514.44921875 
[2025-02-23 14:15:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.5811513662338257 norm:0.00047077477211132646 max memory_allocated 22514.44921875 
[2025-02-23 14:15:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.5817462801933289 norm:0.0004454287118278444 max memory_allocated 22514.44921875 
[2025-02-23 14:16:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.5816188454627991 norm:0.00044749805238097906 max memory_allocated 22514.44921875 
[2025-02-23 14:16:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.5817856788635254 norm:0.0004688822664320469 max memory_allocated 22514.44921875 
[2025-02-23 14:17:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.5819408297538757 norm:0.00048545608296990395 max memory_allocated 22514.44921875 
[2025-02-23 14:17:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.5814849138259888 norm:0.0004891796852461994 max memory_allocated 22514.44921875 
[2025-02-23 14:18:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.5811153650283813 norm:0.00047970778541639447 max memory_allocated 22514.44921875 
[2025-02-23 14:19:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.5810635089874268 norm:0.0004603008273988962 max memory_allocated 22514.44921875 
[2025-02-23 14:19:38 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.5812404751777649 norm:0.00046447699423879385 max memory_allocated 22514.44921875 
[2025-02-23 14:20:11 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.5815916657447815 norm:0.00048052065540105104 max memory_allocated 22514.44921875 
[2025-02-23 14:20:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-23 14:20:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.7257570028305054 norm:0.006143688689917326 max memory_allocated 22514.62109375 
[2025-02-23 14:21:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.7107570171356201 norm:0.003135590348392725 max memory_allocated 22514.62109375 
[2025-02-23 14:22:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.7030668258666992 norm:0.00199637352488935 max memory_allocated 22514.62109375 
[2025-02-23 14:22:35 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.6977453231811523 norm:0.0014461626997217536 max memory_allocated 22514.62109375 
[2025-02-23 14:23:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.6950870752334595 norm:0.0010607385775074363 max memory_allocated 22514.62109375 
[2025-02-23 14:23:41 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.6924991607666016 norm:0.0008089864859357476 max memory_allocated 22514.62109375 
[2025-02-23 14:24:14 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.6904186606407166 norm:0.0006835630047135055 max memory_allocated 22514.62109375 
[2025-02-23 14:24:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.6887410879135132 norm:0.0006292728940024972 max memory_allocated 22514.62109375 
[2025-02-23 14:25:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.6876692771911621 norm:0.0005553279770538211 max memory_allocated 22514.62109375 
[2025-02-23 14:25:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.6869959831237793 norm:0.0005149622447788715 max memory_allocated 22514.62109375 
[2025-02-23 14:26:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.6870356798171997 norm:0.00048627954674884677 max memory_allocated 22514.62109375 
[2025-02-23 14:26:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.6869868040084839 norm:0.00048192322719842196 max memory_allocated 22514.62109375 
[2025-02-23 14:27:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.6869408488273621 norm:0.0004791260580532253 max memory_allocated 22514.62109375 
[2025-02-23 14:28:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.6868327260017395 norm:0.00047101915697567165 max memory_allocated 22514.62109375 
[2025-02-23 14:28:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.6869122982025146 norm:0.0004704544844571501 max memory_allocated 22514.62109375 
[2025-02-23 14:29:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.6866443753242493 norm:0.0004761800810229033 max memory_allocated 22514.62109375 
[2025-02-23 14:29:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.6865718364715576 norm:0.0004838343011215329 max memory_allocated 22514.62109375 
[2025-02-23 14:30:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.6865342855453491 norm:0.00048754559247754514 max memory_allocated 22514.62109375 
[2025-02-23 14:30:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.6861648559570312 norm:0.000473115302156657 max memory_allocated 22514.62109375 
[2025-02-23 14:31:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.6860191822052002 norm:0.0004765692865476012 max memory_allocated 22514.62109375 
[2025-02-23 14:31:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-23 14:32:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.9332002997398376 norm:0.0040889522060751915 max memory_allocated 22514.79296875 
[2025-02-23 14:32:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.9080740213394165 norm:0.0022892775014042854 max memory_allocated 22514.79296875 
[2025-02-23 14:33:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.8946901559829712 norm:0.0015950590604916215 max memory_allocated 22514.79296875 
[2025-02-23 14:33:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.886487603187561 norm:0.0013169747544452548 max memory_allocated 22514.79296875 
[2025-02-23 14:34:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.8810203671455383 norm:0.0010926713002845645 max memory_allocated 22514.79296875 
[2025-02-23 14:34:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.8766696453094482 norm:0.0009618060430511832 max memory_allocated 22514.79296875 
[2025-02-23 14:35:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.8733856081962585 norm:0.0009374869405291975 max memory_allocated 22514.79296875 
[2025-02-23 14:35:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.8712413907051086 norm:0.0010566990822553635 max memory_allocated 22514.79296875 
[2025-02-23 14:36:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.8694949150085449 norm:0.0009224819950759411 max memory_allocated 22514.79296875 
[2025-02-23 14:37:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.8684333562850952 norm:0.0008316009189002216 max memory_allocated 22514.79296875 
[2025-02-23 14:37:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.8672432899475098 norm:0.0008548569749109447 max memory_allocated 22514.79296875 
[2025-02-23 14:38:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.8664348125457764 norm:0.00082542491145432 max memory_allocated 22514.79296875 
[2025-02-23 14:38:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.866706907749176 norm:0.0007947928970679641 max memory_allocated 22514.79296875 
[2025-02-23 14:39:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.865652322769165 norm:0.0008360676001757383 max memory_allocated 22514.79296875 
[2025-02-23 14:39:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.8647662997245789 norm:0.0008226809441111982 max memory_allocated 22514.79296875 
[2025-02-23 14:40:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.8642180562019348 norm:0.000791868835221976 max memory_allocated 22514.79296875 
[2025-02-23 14:40:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.863962709903717 norm:0.0008171435911208391 max memory_allocated 22514.79296875 
[2025-02-23 14:41:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.8637567758560181 norm:0.0008170856162905693 max memory_allocated 22514.79296875 
[2025-02-23 14:42:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.8634142875671387 norm:0.0008421486127190292 max memory_allocated 22514.79296875 
[2025-02-23 14:42:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.8631955981254578 norm:0.0007914583547972143 max memory_allocated 22514.79296875 
[2025-02-23 14:42:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-23 14:43:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:1.5893434286117554 norm:0.00879213958978653 max memory_allocated 22514.96484375 
[2025-02-23 14:43:52 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:1.5155508518218994 norm:0.005988291464745998 max memory_allocated 22514.96484375 
[2025-02-23 14:44:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:1.4789543151855469 norm:0.004801396746188402 max memory_allocated 22514.96484375 
[2025-02-23 14:44:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:1.4587832689285278 norm:0.003900084877386689 max memory_allocated 22514.96484375 
[2025-02-23 14:45:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:1.449892520904541 norm:0.0035807443782687187 max memory_allocated 22514.96484375 
[2025-02-23 14:46:03 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:1.4527390003204346 norm:0.0034438488073647022 max memory_allocated 22514.96484375 
[2025-02-23 14:46:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:1.4533742666244507 norm:0.0033174531999975443 max memory_allocated 22514.96484375 
[2025-02-23 14:47:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:1.4420456886291504 norm:0.0031220035161823034 max memory_allocated 22514.96484375 
[2025-02-23 14:47:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:1.4465034008026123 norm:0.0032671806402504444 max memory_allocated 22514.96484375 
[2025-02-23 14:48:15 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:1.4421039819717407 norm:0.003149973228573799 max memory_allocated 22514.96484375 
[2025-02-23 14:48:48 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:1.4237220287322998 norm:0.002841084497049451 max memory_allocated 22514.96484375 
[2025-02-23 14:49:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:1.398263931274414 norm:0.003194545628502965 max memory_allocated 22514.96484375 
[2025-02-23 14:49:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:1.3866862058639526 norm:0.0022625955753028393 max memory_allocated 22514.96484375 
[2025-02-23 14:50:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:1.385745644569397 norm:0.0022607215214520693 max memory_allocated 22514.96484375 
[2025-02-23 14:51:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:1.384093165397644 norm:0.002201969502493739 max memory_allocated 22514.96484375 
[2025-02-23 14:51:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:1.3867888450622559 norm:0.0021968521177768707 max memory_allocated 22514.96484375 
[2025-02-23 14:52:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:1.3898811340332031 norm:0.0021727541461586952 max memory_allocated 22514.96484375 
[2025-02-23 14:52:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:1.4011015892028809 norm:0.002441896591335535 max memory_allocated 22514.96484375 
[2025-02-23 14:53:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:1.4158053398132324 norm:0.0033995904959738255 max memory_allocated 22514.96484375 
[2025-02-23 14:53:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:1.4315485954284668 norm:0.004995888564735651 max memory_allocated 22514.96484375 
[2025-02-23 14:53:54 root] (main_calibration.py 365): INFO 21512.56724667549
[2025-02-23 14:54:27 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-23 14:55:38 root] (main_calibration.py 158): INFO wikitext2 : 6.098235607147217
[2025-02-23 14:55:38 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-23 14:57:27 root] (main_calibration.py 158): INFO c4 : 7.532521724700928
[2025-02-23 16:31:39 root] (main_calibration.py 169): INFO {'wikitext2': 6.098235607147217, 'c4': 7.532521724700928, 'results': {'piqa': {'acc': 0.7747551686615887, 'acc_stderr': 0.009746643471032155, 'acc_norm': 0.7693144722524483, 'acc_norm_stderr': 0.0098289595509831}, 'boolq': {'acc': 0.6960244648318042, 'acc_stderr': 0.008044964056917368}, 'arc_easy': {'acc': 0.6519360269360269, 'acc_stderr': 0.009774627600259014, 'acc_norm': 0.5143097643097643, 'acc_norm_stderr': 0.010255580881603618}, 'arc_challenge': {'acc': 0.3703071672354949, 'acc_stderr': 0.01411129875167495, 'acc_norm': 0.40187713310580203, 'acc_norm_stderr': 0.014327268614578273}, 'winogrande': {'acc': 0.6574585635359116, 'acc_stderr': 0.013337483579075925}, 'hellaswag': {'acc': 0.5500896235809599, 'acc_stderr': 0.004964679845918435, 'acc_norm': 0.7179844652459669, 'acc_norm_stderr': 0.004490612245335232}}, 'versions': {'piqa': 0, 'boolq': 1, 'arc_easy': 0, 'arc_challenge': 0, 'winogrande': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
