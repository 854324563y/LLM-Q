[2025-02-18 04:40:13 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration/Llama-2-13b-hf-w8a8', save_dir='./log-calibration/quant/Llama-2-13b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-18 04:40:20 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 04:40:21 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-18 04:40:21 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 04:40:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 04:41:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0010102269006893039 norm:0.00020210984803270549 max memory_allocated 29229.177734375 
[2025-02-18 04:42:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0006669461727142334 norm:7.220748375402763e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:43:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.000554323778487742 norm:4.4137694203527644e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:43:52 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0004983580438420177 norm:3.4593933378346264e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:44:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.00046899684821255505 norm:2.7812455300590955e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:45:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.0004497629124671221 norm:2.639665581227746e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:46:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0004366420325823128 norm:2.2912015992915258e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:47:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.00042931531788781285 norm:2.2707168682245538e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:47:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.00042338372441008687 norm:2.7108982976642437e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:48:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00041920068906620145 norm:2.5596846171538346e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:49:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0004151153261773288 norm:3.495922283036634e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:50:19 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0004124215920455754 norm:2.407030297035817e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:51:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0004119490331504494 norm:2.4625898731756024e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:51:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.00040903224726207554 norm:2.3178370611276478e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:52:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0004084808751940727 norm:2.5091167117352597e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:53:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0004092760500498116 norm:2.4399987523793243e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:54:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0004069911956321448 norm:2.267459967697505e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:55:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00040814420208334923 norm:2.537810905778315e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:55:58 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00040693391929380596 norm:2.5286026357207447e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:56:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.000405535422032699 norm:2.8001726604998112e-05 max memory_allocated 29229.177734375 
[2025-02-18 04:57:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 04:58:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0017526693409308791 norm:0.0002770541759673506 max memory_allocated 29229.365234375 
[2025-02-18 04:58:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0012885445030406117 norm:9.650508582126349e-05 max memory_allocated 29229.365234375 
[2025-02-18 04:59:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0011432877508923411 norm:5.696466541849077e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:00:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.001078047789633274 norm:3.904417098965496e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:01:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0010366674978286028 norm:3.030389234481845e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:02:05 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.001012575812637806 norm:2.4973875042633153e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:02:54 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0009984476491808891 norm:2.2680425900034606e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:03:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.00098871486261487 norm:2.16295629797969e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:04:30 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.000977465882897377 norm:2.119301825587172e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:05:19 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0009705307311378419 norm:2.1226194803602993e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:06:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.000966438208706677 norm:2.0614515960915014e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:06:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0009604654042050242 norm:2.0308769308030605e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:07:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0009611085988581181 norm:2.0311093976488337e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:08:32 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0009568138048052788 norm:2.043555286945775e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:09:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0009555334690958261 norm:2.099162338708993e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:10:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.000953249167650938 norm:2.0698256776086055e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:10:57 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0009518557344563305 norm:2.070160917355679e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:11:46 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0009508889634162188 norm:2.0626745026675053e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:12:34 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0009488629875704646 norm:2.018037048401311e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:13:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.000947610184084624 norm:2.0121384295634925e-05 max memory_allocated 29229.365234375 
[2025-02-18 05:13:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 05:14:35 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0020994923543184996 norm:0.0005641515599563718 max memory_allocated 29229.552734375 
[2025-02-18 05:15:24 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.001635446329601109 norm:0.0002681407204363495 max memory_allocated 29229.552734375 
[2025-02-18 05:16:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0014355400344356894 norm:0.00016158673679456115 max memory_allocated 29229.552734375 
[2025-02-18 05:17:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0013297486584633589 norm:0.00010459336044732481 max memory_allocated 29229.552734375 
[2025-02-18 05:17:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0012819517869502306 norm:6.789874169044197e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:18:38 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.001259680837392807 norm:4.3802152504213154e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:19:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0012492667883634567 norm:2.9795568480039947e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:20:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0012407194590196013 norm:2.23713977902662e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:21:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0012368394527584314 norm:1.8024085875367746e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:21:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0012353349011391401 norm:1.607082595000975e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:22:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0012272611493244767 norm:1.533361864858307e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:23:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.001232427777722478 norm:1.4816688235441688e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:24:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.001228427398018539 norm:1.4573212865798268e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:25:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0012240736978128552 norm:1.4568671758752316e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:25:54 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.001220571924932301 norm:1.4553666005667765e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:26:43 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.001219063764438033 norm:1.4531517081195489e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:27:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0012183341896161437 norm:1.4540470147039741e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:28:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0012184556107968092 norm:1.4610950529458933e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:29:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.0012160803889855742 norm:1.4505173567158636e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:29:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.001218023826368153 norm:1.4641733287135139e-05 max memory_allocated 29229.552734375 
[2025-02-18 05:30:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 05:31:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.002281379885971546 norm:0.000557992490939796 max memory_allocated 29229.740234375 
[2025-02-18 05:31:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0018546257633715868 norm:0.0002691331028472632 max memory_allocated 29229.740234375 
[2025-02-18 05:32:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.001675001229159534 norm:0.00017082534031942487 max memory_allocated 29229.740234375 
[2025-02-18 05:33:34 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0015690079890191555 norm:0.00011510583863127977 max memory_allocated 29229.740234375 
[2025-02-18 05:34:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.0015041453298181295 norm:8.42888475744985e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:35:11 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0014666763599961996 norm:6.286964344326407e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:36:00 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.001447562943212688 norm:4.835412619286217e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:36:48 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.001434710226021707 norm:3.805521555477753e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:37:37 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0014240533346310258 norm:3.0477152904495597e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:38:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0014170395443215966 norm:2.5427420041523874e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:39:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0014135483652353287 norm:2.1520050722756423e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:40:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0014064810238778591 norm:1.9065952074015513e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:40:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.001403807196766138 norm:1.7352438590023667e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:41:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0014029531739652157 norm:1.647901808610186e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:42:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.0014046308351680636 norm:1.6149779185070656e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:43:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.001400592620484531 norm:1.5188199540716596e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:44:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0014007097342982888 norm:1.5337720469688065e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:44:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0013976950431242585 norm:1.5147691556194331e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:45:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0013932199217379093 norm:1.4620990441471804e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:46:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0013942085206508636 norm:1.4586167708330322e-05 max memory_allocated 29229.740234375 
[2025-02-18 05:46:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 05:47:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0017939815297722816 norm:9.827395842876285e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:48:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0016541460063308477 norm:4.8700079787522554e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:49:21 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.001604541321285069 norm:3.180325438734144e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:50:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0015804682625457644 norm:2.328907976334449e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:50:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0015669912099838257 norm:1.83044121513376e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:51:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0015608438989147544 norm:1.530623558210209e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:52:35 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.001555608119815588 norm:1.3319155186763965e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:53:23 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0015499732689931989 norm:1.1940210242755711e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:54:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0015470198122784495 norm:1.1034882845706306e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:55:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0015462704468518496 norm:1.048483136401046e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:55:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0015453107189387083 norm:1.0019356523116585e-05 max memory_allocated 29229.927734375 
[2025-02-18 05:56:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.001545596867799759 norm:9.820058949117083e-06 max memory_allocated 29229.927734375 
[2025-02-18 05:57:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0015448479680344462 norm:9.552711162541527e-06 max memory_allocated 29229.927734375 
[2025-02-18 05:58:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0015426824102178216 norm:9.630773092794698e-06 max memory_allocated 29229.927734375 
[2025-02-18 05:59:02 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.001540817436762154 norm:9.362750461150426e-06 max memory_allocated 29229.927734375 
[2025-02-18 05:59:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0015411421190947294 norm:9.284887710236944e-06 max memory_allocated 29229.927734375 
[2025-02-18 06:00:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0015402327990159392 norm:9.37886034080293e-06 max memory_allocated 29229.927734375 
[2025-02-18 06:01:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0015400568954646587 norm:9.197705367114395e-06 max memory_allocated 29229.927734375 
[2025-02-18 06:02:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.001538727548904717 norm:9.147007403953467e-06 max memory_allocated 29229.927734375 
[2025-02-18 06:03:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0015387819148600101 norm:9.019682693178765e-06 max memory_allocated 29229.927734375 
[2025-02-18 06:03:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 06:04:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0019287473987787962 norm:9.656463225837797e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:05:06 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0017962411511689425 norm:4.502256342675537e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:05:54 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.001744028297252953 norm:2.895376928790938e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:06:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0017141769640147686 norm:2.1876921891816892e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:07:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0016987374983727932 norm:1.742182212183252e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:08:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0016864200588315725 norm:1.4664585251011886e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:09:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.001679071574471891 norm:1.2873335435870104e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:09:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0016730122733861208 norm:1.1799538697232492e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:10:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.001665509189479053 norm:1.0857354936888441e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:11:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0016617593355476856 norm:1.0172321708523668e-05 max memory_allocated 29230.115234375 
[2025-02-18 06:12:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0016607176512479782 norm:9.68534095591167e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:13:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.0016572971362620592 norm:9.299791599914897e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:13:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0016564677935093641 norm:8.9714794739848e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:14:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.001656032633036375 norm:8.792345397523604e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:15:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.001654117600992322 norm:8.642940883873962e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:16:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0016531356377527118 norm:8.547543984605e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:17:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0016526072286069393 norm:8.420154699706472e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:18:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0016534689348191023 norm:8.338304724020418e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:18:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0016523548401892185 norm:8.289045581477694e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:19:37 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0016522589139640331 norm:8.24609105620766e-06 max memory_allocated 29230.115234375 
[2025-02-18 06:19:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 06:20:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0020675058476626873 norm:0.0001412735873600468 max memory_allocated 29230.302734375 
[2025-02-18 06:21:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0019070933340117335 norm:6.522819603560492e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:22:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.001841809367761016 norm:3.993554491898976e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:23:14 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0018102858448401093 norm:2.860045242414344e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:24:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0017893273616209626 norm:2.201490497100167e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:24:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0017775795422494411 norm:1.784032428986393e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:25:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.001768665504641831 norm:1.5078656360856257e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:26:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.001763401785865426 norm:1.3045373634668067e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:27:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0017596186371520162 norm:1.1542426364030689e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:28:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.001755193923600018 norm:1.0587318683974445e-05 max memory_allocated 29230.302734375 
[2025-02-18 06:28:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0017528908792883158 norm:9.979193237086292e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:29:41 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0017502487171441317 norm:9.492545359535143e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:30:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.0017487079603597522 norm:9.206953109242022e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:31:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.001746920170262456 norm:8.905120012059342e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:32:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.001745502115227282 norm:8.717512173461728e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:32:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0017450114246457815 norm:8.549295671400614e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:33:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0017441711388528347 norm:8.385507499042433e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:34:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0017440140945836902 norm:8.288013304991182e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:35:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0017436796333640814 norm:8.236605026468169e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:36:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.001742816180922091 norm:8.157910997397266e-06 max memory_allocated 29230.302734375 
[2025-02-18 06:36:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 06:37:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0021090225782245398 norm:7.999935769475996e-05 max memory_allocated 29230.490234375 
[2025-02-18 06:38:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0019818926230072975 norm:3.92821675632149e-05 max memory_allocated 29230.490234375 
[2025-02-18 06:38:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.001927593257278204 norm:2.565235990914516e-05 max memory_allocated 29230.490234375 
[2025-02-18 06:39:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0018992653349414468 norm:1.940347283380106e-05 max memory_allocated 29230.490234375 
[2025-02-18 06:40:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.001881766365841031 norm:1.5897778212092817e-05 max memory_allocated 29230.490234375 
[2025-02-18 06:41:20 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.0018695963080972433 norm:1.30219978018431e-05 max memory_allocated 29230.490234375 
[2025-02-18 06:42:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0018612208077684045 norm:1.1450973033788614e-05 max memory_allocated 29230.490234375 
[2025-02-18 06:42:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0018544093472883105 norm:1.0590619240247179e-05 max memory_allocated 29230.490234375 
[2025-02-18 06:43:45 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.001849001389928162 norm:9.80269305728143e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:44:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0018440790008753538 norm:9.138749192061368e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:45:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.001841803197748959 norm:8.598426575190388e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:46:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0018398204119876027 norm:8.279926987597719e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:46:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.001838037627749145 norm:8.133611117955297e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:47:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.001837339485064149 norm:7.94127208791906e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:48:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0018357629887759686 norm:7.873309186834376e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:49:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0018351860344409943 norm:7.7436889114324e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:50:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0018344912678003311 norm:7.69154303270625e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:51:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.001834442256949842 norm:7.556413947895635e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:51:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0018345324788242579 norm:7.488683877454605e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:52:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.001834016409702599 norm:7.486147751478711e-06 max memory_allocated 29230.490234375 
[2025-02-18 06:52:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 06:53:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0022987802512943745 norm:0.00020965632575098425 max memory_allocated 29230.677734375 
[2025-02-18 06:54:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0021045119501650333 norm:0.0001011591957649216 max memory_allocated 29230.677734375 
[2025-02-18 06:55:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.0020258405711501837 norm:6.230000144569203e-05 max memory_allocated 29230.677734375 
[2025-02-18 06:56:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.00198561605066061 norm:4.348572838352993e-05 max memory_allocated 29230.677734375 
[2025-02-18 06:57:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0019623374100774527 norm:3.2649542845319957e-05 max memory_allocated 29230.677734375 
[2025-02-18 06:57:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0019469193648546934 norm:2.5491943233646452e-05 max memory_allocated 29230.677734375 
[2025-02-18 06:58:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.001936093671247363 norm:2.0885878257104196e-05 max memory_allocated 29230.677734375 
[2025-02-18 06:59:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0019286081660538912 norm:1.7561262211529538e-05 max memory_allocated 29230.677734375 
[2025-02-18 07:00:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0019229380413889885 norm:1.5111678294488229e-05 max memory_allocated 29230.677734375 
[2025-02-18 07:01:04 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0019180466188117862 norm:1.335772958555026e-05 max memory_allocated 29230.677734375 
[2025-02-18 07:01:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0019146294798702002 norm:1.1989033737336285e-05 max memory_allocated 29230.677734375 
[2025-02-18 07:02:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.001911669853143394 norm:1.0982605090248398e-05 max memory_allocated 29230.677734375 
[2025-02-18 07:03:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0019108809065073729 norm:1.0164608283957932e-05 max memory_allocated 29230.677734375 
[2025-02-18 07:04:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.001908525824546814 norm:9.56645544647472e-06 max memory_allocated 29230.677734375 
[2025-02-18 07:05:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0019063260406255722 norm:9.04745866137091e-06 max memory_allocated 29230.677734375 
[2025-02-18 07:05:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0019050982082262635 norm:8.665730092616286e-06 max memory_allocated 29230.677734375 
[2025-02-18 07:06:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0019035977311432362 norm:8.30027238407638e-06 max memory_allocated 29230.677734375 
[2025-02-18 07:07:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0019027587259188294 norm:8.063349923759233e-06 max memory_allocated 29230.677734375 
[2025-02-18 07:08:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.001901906100101769 norm:7.87130011303816e-06 max memory_allocated 29230.677734375 
[2025-02-18 07:09:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0019012680277228355 norm:7.700450623815414e-06 max memory_allocated 29230.677734375 
[2025-02-18 07:09:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 07:10:21 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.0022770671639591455 norm:6.43693856545724e-05 max memory_allocated 29230.865234375 
[2025-02-18 07:11:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0021633519791066647 norm:3.180262865498662e-05 max memory_allocated 29230.865234375 
[2025-02-18 07:11:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.002110809553414583 norm:2.1281855879351497e-05 max memory_allocated 29230.865234375 
[2025-02-18 07:12:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.002080274047330022 norm:1.6281846910715103e-05 max memory_allocated 29230.865234375 
[2025-02-18 07:13:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.002060225699096918 norm:1.2626162060769275e-05 max memory_allocated 29230.865234375 
[2025-02-18 07:14:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.002048220718279481 norm:1.070592497853795e-05 max memory_allocated 29230.865234375 
[2025-02-18 07:15:12 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0020390625577419996 norm:9.311163012171164e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:16:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0020326885860413313 norm:8.333292498718947e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:16:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0020280343014746904 norm:7.540706064901315e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:17:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0020251902751624584 norm:7.129764526325744e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:18:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.002022211439907551 norm:6.710831257805694e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:19:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0020192726515233517 norm:6.329676580207888e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:20:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0020169969648122787 norm:6.078290425648447e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:20:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0020167154725641012 norm:5.894586593058193e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:21:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0020149603951722383 norm:5.756968675996177e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:22:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0020153382793068886 norm:5.627046448353212e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:23:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.00201518926769495 norm:5.5508166951767635e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:24:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.002013907302170992 norm:5.535694526770385e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:24:54 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0020160714630037546 norm:5.469910320243798e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:25:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.002014484256505966 norm:5.448991032608319e-06 max memory_allocated 29230.865234375 
[2025-02-18 07:25:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 07:26:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0023397202603518963 norm:5.9248199249850586e-05 max memory_allocated 29231.052734375 
[2025-02-18 07:27:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.0022388743236660957 norm:2.9352406272664666e-05 max memory_allocated 29231.052734375 
[2025-02-18 07:28:36 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0021945866756141186 norm:1.9223722119932063e-05 max memory_allocated 29231.052734375 
[2025-02-18 07:29:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.002173755317926407 norm:1.4338239452627022e-05 max memory_allocated 29231.052734375 
[2025-02-18 07:30:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.002160285133868456 norm:1.146996146417223e-05 max memory_allocated 29231.052734375 
[2025-02-18 07:31:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0021530732046812773 norm:9.818322723731399e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:31:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0021475059911608696 norm:8.531423191016074e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:32:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.0021432966459542513 norm:7.747511517663952e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:33:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.002141109434887767 norm:7.21652986612753e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:34:15 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0021372835617512465 norm:6.745870450686198e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:35:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.0021355266217142344 norm:6.498414222733118e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:35:52 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.002133968286216259 norm:6.244330506888218e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:36:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0021327692084014416 norm:6.0473876146716066e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:37:29 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.002131920773535967 norm:5.928458449488971e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:38:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0021308891009539366 norm:5.799138762085931e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:39:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.002130210632458329 norm:5.69896155866445e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:39:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.0021305689588189125 norm:5.62768127565505e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:40:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.002130368025973439 norm:5.570231223828159e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:41:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.002130679087713361 norm:5.563923423324013e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:42:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0021303186658769846 norm:5.542794951907126e-06 max memory_allocated 29231.052734375 
[2025-02-18 07:42:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 07:43:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.002483255695551634 norm:7.019347685854882e-05 max memory_allocated 29231.240234375 
[2025-02-18 07:44:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.002395904390141368 norm:3.192350413883105e-05 max memory_allocated 29231.240234375 
[2025-02-18 07:45:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.002356479410082102 norm:2.052716263278853e-05 max memory_allocated 29231.240234375 
[2025-02-18 07:46:00 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.002333841286599636 norm:1.477897421864327e-05 max memory_allocated 29231.240234375 
[2025-02-18 07:46:48 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.00231937225908041 norm:1.1576056749618147e-05 max memory_allocated 29231.240234375 
[2025-02-18 07:47:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.002310729818418622 norm:9.457684427616186e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:48:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0023044978734105825 norm:8.08960794529412e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:49:14 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.002300121821463108 norm:7.217329766717739e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:50:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.0022970130667090416 norm:6.482721346401377e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:50:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.002294893842190504 norm:6.0130541896796785e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:51:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0022934365551918745 norm:5.745962880610023e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:52:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.002291293116286397 norm:5.455800419440493e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:53:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0022908421233296394 norm:5.275468083709711e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:54:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0022908004466444254 norm:5.1656929827004205e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:54:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.002289283089339733 norm:5.133448667038465e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:55:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0022891657426953316 norm:5.054015673522372e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:56:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0022888139355927706 norm:4.99744237458799e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:57:19 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.002288303105160594 norm:4.990634351997869e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:58:08 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.002287166891619563 norm:4.9301042963634245e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:58:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.0022878695745021105 norm:4.922677362628747e-06 max memory_allocated 29231.240234375 
[2025-02-18 07:59:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 08:00:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.002574122278019786 norm:4.325289773987606e-05 max memory_allocated 29231.427734375 
[2025-02-18 08:01:01 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0024825341533869505 norm:2.181918898713775e-05 max memory_allocated 29231.427734375 
[2025-02-18 08:01:49 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.002441731747239828 norm:1.5086370694916695e-05 max memory_allocated 29231.427734375 
[2025-02-18 08:02:38 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.00242157862521708 norm:1.1226094102312345e-05 max memory_allocated 29231.427734375 
[2025-02-18 08:03:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.002408691681921482 norm:9.234468052454758e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:04:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.002399214543402195 norm:7.766408089082688e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:05:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.002393546514213085 norm:6.977395969443023e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:05:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0023884372785687447 norm:6.2961939875094686e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:06:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0023862968664616346 norm:5.95666688241181e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:07:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0023833680897951126 norm:5.590867658611387e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:08:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.002380700083449483 norm:5.384509677242022e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:09:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.002378246281296015 norm:5.074835371488007e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:09:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0023777822498232126 norm:4.929738224745961e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:10:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0023769589606672525 norm:4.780491508427076e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:11:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002375351032242179 norm:4.655840712075587e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:12:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.0023749989923089743 norm:4.577453637466533e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:13:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.0023749906104058027 norm:4.469794021133566e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:13:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.0023747852537781 norm:4.4001367314194795e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:14:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0023741007316857576 norm:4.386154614621773e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:15:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0023746653459966183 norm:4.366342182038352e-06 max memory_allocated 29231.427734375 
[2025-02-18 08:15:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 08:16:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.0027267851401120424 norm:8.170949149644002e-05 max memory_allocated 29231.615234375 
[2025-02-18 08:17:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0026282770559191704 norm:3.826296597253531e-05 max memory_allocated 29231.615234375 
[2025-02-18 08:18:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.0025876055005937815 norm:2.405278974038083e-05 max memory_allocated 29231.615234375 
[2025-02-18 08:19:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.0025650830939412117 norm:1.7276863218285143e-05 max memory_allocated 29231.615234375 
[2025-02-18 08:20:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.002550075063481927 norm:1.3487247088050935e-05 max memory_allocated 29231.615234375 
[2025-02-18 08:20:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.002539199311286211 norm:1.0913476216956042e-05 max memory_allocated 29231.615234375 
[2025-02-18 08:21:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.002531900303438306 norm:9.402592695550993e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:22:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.002525672549381852 norm:8.235330824390985e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:23:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.002520838053897023 norm:7.452586487488588e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:24:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.002517720917239785 norm:6.8500417000905145e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:24:51 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.002516032662242651 norm:6.324316018435638e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:25:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.002514812396839261 norm:5.9633121054503135e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:26:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0025138994678854942 norm:5.677042736351723e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:27:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.0025131592992693186 norm:5.444966063805623e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:28:05 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0025133262388408184 norm:5.285161932988558e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:28:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.0025121294893324375 norm:5.156752195034642e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:29:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.0025117453187704086 norm:5.05725665789214e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:30:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0025099958293139935 norm:5.011782832298195e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:31:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0025096710305660963 norm:4.954561973136151e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:32:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0025092067662626505 norm:4.915074441669276e-06 max memory_allocated 29231.615234375 
[2025-02-18 08:32:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 08:33:23 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.002906640525907278 norm:0.00011376342445146292 max memory_allocated 29231.802734375 
[2025-02-18 08:34:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.002795289969071746 norm:5.275521834846586e-05 max memory_allocated 29231.802734375 
[2025-02-18 08:35:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0027458721306174994 norm:3.235218900954351e-05 max memory_allocated 29231.802734375 
[2025-02-18 08:35:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.0027194179128855467 norm:2.2416281353798695e-05 max memory_allocated 29231.802734375 
[2025-02-18 08:36:37 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.002703059231862426 norm:1.6981544831651263e-05 max memory_allocated 29231.802734375 
[2025-02-18 08:37:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.0026911587920039892 norm:1.3702228898182511e-05 max memory_allocated 29231.802734375 
[2025-02-18 08:38:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0026828080881386995 norm:1.1329424523864873e-05 max memory_allocated 29231.802734375 
[2025-02-18 08:39:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0026762469206005335 norm:9.74535578279756e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:39:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.002670963993296027 norm:8.530952982255258e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:40:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.0026668671052902937 norm:7.80034861236345e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:41:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.002664445899426937 norm:7.176738108682912e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:42:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.002661924110725522 norm:6.652098363701953e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:43:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.0026601748540997505 norm:6.239100912353024e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:43:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0026592945214360952 norm:5.972467533865711e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:44:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.002657640725374222 norm:5.725078608520562e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:45:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.002657084260135889 norm:5.516744295164244e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:46:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0026566777378320694 norm:5.3873236538493074e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:47:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.0026555487420409918 norm:5.236267497821245e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:47:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0026544458232820034 norm:5.173410499992315e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:48:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.002654571318998933 norm:5.085164502816042e-06 max memory_allocated 29231.802734375 
[2025-02-18 08:48:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 08:49:59 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.002977403113618493 norm:6.468831998063251e-05 max memory_allocated 29231.990234375 
[2025-02-18 08:50:47 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.002887444570660591 norm:3.61846323357895e-05 max memory_allocated 29231.990234375 
[2025-02-18 08:51:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.002843111287802458 norm:2.468995444360189e-05 max memory_allocated 29231.990234375 
[2025-02-18 08:52:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.0028177034109830856 norm:1.867667742772028e-05 max memory_allocated 29231.990234375 
[2025-02-18 08:53:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0028018520679324865 norm:1.4748822650290094e-05 max memory_allocated 29231.990234375 
[2025-02-18 08:54:01 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.002789780031889677 norm:1.2168468856543768e-05 max memory_allocated 29231.990234375 
[2025-02-18 08:54:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0027821569237858057 norm:1.0441493031976279e-05 max memory_allocated 29231.990234375 
[2025-02-18 08:55:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.002775961998850107 norm:9.20365346246399e-06 max memory_allocated 29231.990234375 
[2025-02-18 08:56:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0027714460156857967 norm:8.22866422822699e-06 max memory_allocated 29231.990234375 
[2025-02-18 08:57:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.002767627825960517 norm:7.498345439671539e-06 max memory_allocated 29231.990234375 
[2025-02-18 08:58:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0027645123191177845 norm:6.934883913345402e-06 max memory_allocated 29231.990234375 
[2025-02-18 08:58:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.0027627183590084314 norm:6.497609319922049e-06 max memory_allocated 29231.990234375 
[2025-02-18 08:59:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0027611968107521534 norm:6.070139988878509e-06 max memory_allocated 29231.990234375 
[2025-02-18 09:00:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0027607327792793512 norm:5.799280188512057e-06 max memory_allocated 29231.990234375 
[2025-02-18 09:01:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.002759832888841629 norm:5.612149379885523e-06 max memory_allocated 29231.990234375 
[2025-02-18 09:02:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0027596289291977882 norm:5.515072189155035e-06 max memory_allocated 29231.990234375 
[2025-02-18 09:02:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.002758762566372752 norm:5.433362730400404e-06 max memory_allocated 29231.990234375 
[2025-02-18 09:03:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.002758905990049243 norm:5.35708932147827e-06 max memory_allocated 29231.990234375 
[2025-02-18 09:04:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.00275813159532845 norm:5.3028634283691645e-06 max memory_allocated 29231.990234375 
[2025-02-18 09:05:20 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.002757333219051361 norm:5.2547511586453766e-06 max memory_allocated 29231.990234375 
[2025-02-18 09:05:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 09:06:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.0032045356929302216 norm:0.00010259360715281218 max memory_allocated 29232.177734375 
[2025-02-18 09:07:23 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.003099035006016493 norm:5.326744940248318e-05 max memory_allocated 29232.177734375 
[2025-02-18 09:08:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.003048689104616642 norm:3.4967462852364406e-05 max memory_allocated 29232.177734375 
[2025-02-18 09:09:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.0030185282230377197 norm:2.530928213673178e-05 max memory_allocated 29232.177734375 
[2025-02-18 09:09:48 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.002998812822625041 norm:1.9877450540661812e-05 max memory_allocated 29232.177734375 
[2025-02-18 09:10:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0029835097957402468 norm:1.6114598111016676e-05 max memory_allocated 29232.177734375 
[2025-02-18 09:11:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.0029725481290370226 norm:1.3730669707001653e-05 max memory_allocated 29232.177734375 
[2025-02-18 09:12:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0029643152374774218 norm:1.2086223250662442e-05 max memory_allocated 29232.177734375 
[2025-02-18 09:13:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.0029574199579656124 norm:1.0708205081755295e-05 max memory_allocated 29232.177734375 
[2025-02-18 09:13:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.0029525007121264935 norm:9.727896213007625e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:14:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.0029486108105629683 norm:8.931874617701396e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:15:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.0029458259232342243 norm:8.284661817015149e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:16:17 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.002944157924503088 norm:7.86144028097624e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:17:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.00294135301373899 norm:7.4315139499958605e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:17:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.002939795609563589 norm:7.110374099283945e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:18:42 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.00293905520811677 norm:6.852002115920186e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:19:31 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.0029381373897194862 norm:6.731939265591791e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:20:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.0029374384321272373 norm:6.585700248251669e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:21:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0029362491331994534 norm:6.482566277554724e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:21:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.0029358086176216602 norm:6.373360065481393e-06 max memory_allocated 29232.177734375 
[2025-02-18 09:22:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 09:23:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.003283029654994607 norm:6.447453051805496e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:23:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0032116270158439875 norm:3.392637154320255e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:24:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.003176329657435417 norm:2.3266766220331192e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:25:35 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.003153483849018812 norm:1.8131540855392814e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:26:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0031385363545268774 norm:1.5018562407931313e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:27:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.0031302087008953094 norm:1.301675001741387e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:28:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.003125095274299383 norm:1.1657170944090467e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:28:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0031188218854367733 norm:1.0910865057667252e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:29:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0031136744655668736 norm:1.0251495041302405e-05 max memory_allocated 29232.365234375 
[2025-02-18 09:30:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.003110421122983098 norm:9.763219168235082e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:31:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.00310787046328187 norm:9.533367119729519e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:32:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.0031050830148160458 norm:9.303001206717454e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:32:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.00310319266282022 norm:9.162869901047088e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:33:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.0031016352586448193 norm:8.965232154878322e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:34:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.0031008576042950153 norm:8.82918084244011e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:35:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.003099772147834301 norm:8.923374480218627e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:36:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.003099269699305296 norm:8.734507900953759e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:36:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.003099103458225727 norm:8.625858754385263e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:37:42 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.0030983604956418276 norm:8.588005584897473e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:38:31 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0030973325483500957 norm:8.566815267840866e-06 max memory_allocated 29232.365234375 
[2025-02-18 09:38:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 09:39:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.0035186875611543655 norm:7.190584437921643e-05 max memory_allocated 29232.552734375 
[2025-02-18 09:40:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0034276056103408337 norm:3.722815017681569e-05 max memory_allocated 29232.552734375 
[2025-02-18 09:41:22 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.003383840201422572 norm:2.499095535313245e-05 max memory_allocated 29232.552734375 
[2025-02-18 09:42:10 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.0033562323078513145 norm:1.853058347478509e-05 max memory_allocated 29232.552734375 
[2025-02-18 09:42:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.003338148118928075 norm:1.4799117707298137e-05 max memory_allocated 29232.552734375 
[2025-02-18 09:43:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.003324326127767563 norm:1.2538015653262846e-05 max memory_allocated 29232.552734375 
[2025-02-18 09:44:36 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.00331374304369092 norm:1.0985056178469677e-05 max memory_allocated 29232.552734375 
[2025-02-18 09:45:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.003306156489998102 norm:9.840766324487049e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:46:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.003299583913758397 norm:8.967703251983039e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:47:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.003293506568297744 norm:8.402751518588047e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:47:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.0032891773153096437 norm:7.908151019364595e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:48:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.0032865703105926514 norm:7.487009952455992e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:49:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.003283359808847308 norm:7.240198556246469e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:50:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.003280745819211006 norm:6.969290552660823e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:51:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.0032791695557534695 norm:6.774193479941459e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:51:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.0032780843321233988 norm:6.63495802655234e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:52:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.0032768237870186567 norm:6.515590939670801e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:53:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.0032753676641732454 norm:6.412475158867892e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:54:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.003274459857493639 norm:6.319318799796747e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:55:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.003273571841418743 norm:6.241203664103523e-06 max memory_allocated 29232.552734375 
[2025-02-18 09:55:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 09:56:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.0037563424557447433 norm:6.296429637586698e-05 max memory_allocated 29232.740234375 
[2025-02-18 09:57:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.003674292704090476 norm:3.6253688449505717e-05 max memory_allocated 29232.740234375 
[2025-02-18 09:58:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.0036309531424194574 norm:2.527667129470501e-05 max memory_allocated 29232.740234375 
[2025-02-18 09:58:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.003602539421990514 norm:1.9543926100595854e-05 max memory_allocated 29232.740234375 
[2025-02-18 09:59:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.0035826091188937426 norm:1.6051208149292506e-05 max memory_allocated 29232.740234375 
[2025-02-18 10:00:25 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.003568401327356696 norm:1.3659151591127738e-05 max memory_allocated 29232.740234375 
[2025-02-18 10:01:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0035574205685406923 norm:1.1952315617236309e-05 max memory_allocated 29232.740234375 
[2025-02-18 10:02:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.003548713866621256 norm:1.0744813152996358e-05 max memory_allocated 29232.740234375 
[2025-02-18 10:02:51 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.003541275393217802 norm:9.848145055002533e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:03:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.0035362604539841413 norm:9.15992586669745e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:04:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.003530460875481367 norm:8.550532584195025e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:05:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.003526272950693965 norm:8.050905307754874e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:06:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.0035226880572736263 norm:7.702532457187772e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:06:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.0035200833808630705 norm:7.420533165714005e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:07:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.0035180398263037205 norm:7.180022294051014e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:08:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.0035159301478415728 norm:6.968809884710936e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:09:19 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.003514711745083332 norm:6.763767942175036e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:10:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.003512871917337179 norm:6.654629487456987e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:10:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.003511751303449273 norm:6.549749741679989e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:11:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0035110723692923784 norm:6.432790996768745e-06 max memory_allocated 29232.740234375 
[2025-02-18 10:11:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 10:12:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.004014265723526478 norm:6.874847167637199e-05 max memory_allocated 29232.927734375 
[2025-02-18 10:13:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.0039250049740076065 norm:3.701005698530935e-05 max memory_allocated 29232.927734375 
[2025-02-18 10:14:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.0038806162774562836 norm:2.5142746380879544e-05 max memory_allocated 29232.927734375 
[2025-02-18 10:15:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.003853119444102049 norm:1.924196840263903e-05 max memory_allocated 29232.927734375 
[2025-02-18 10:16:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.003832831745967269 norm:1.5673458619858138e-05 max memory_allocated 29232.927734375 
[2025-02-18 10:16:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.003819072851911187 norm:1.3102171578793786e-05 max memory_allocated 29232.927734375 
[2025-02-18 10:17:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.0038071710150688887 norm:1.1433034160290845e-05 max memory_allocated 29232.927734375 
[2025-02-18 10:18:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.00379781611263752 norm:1.0310548532288522e-05 max memory_allocated 29232.927734375 
[2025-02-18 10:19:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.0037903455086052418 norm:9.442525879421737e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:20:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.0037848716601729393 norm:8.818253263598308e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:21:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.003780166618525982 norm:8.24428025225643e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:21:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.0037767805624753237 norm:7.755172191536985e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:22:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.003773026866838336 norm:7.426762749673799e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:23:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.003770320676267147 norm:7.200820618891157e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:24:17 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.0037678605876863003 norm:6.957958248676732e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:25:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.00376570550724864 norm:6.768194452888565e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:25:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.0037643066607415676 norm:6.595821560040349e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:26:42 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.003762714797630906 norm:6.454779850173509e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:27:31 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.0037618232890963554 norm:6.346935151668731e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:28:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.0037607590202242136 norm:6.249831585591892e-06 max memory_allocated 29232.927734375 
[2025-02-18 10:28:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 10:29:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.004412161186337471 norm:6.688082794426009e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:30:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.004327649716287851 norm:3.8176989619387314e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:31:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.00428309291601181 norm:2.72693250735756e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:31:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.004255102016031742 norm:2.137880255759228e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:32:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.004234564956277609 norm:1.7958027456188574e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:33:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.00421965541318059 norm:1.56492260430241e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:34:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.00420747883617878 norm:1.404885551892221e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:35:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.004199197515845299 norm:1.2675161087827291e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:36:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.004191963002085686 norm:1.1625006663962267e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:36:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.004185306373983622 norm:1.0844702046597376e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:37:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.0041798376478254795 norm:1.0242727512377314e-05 max memory_allocated 29233.115234375 
[2025-02-18 10:38:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.004175727255642414 norm:9.756223334989045e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:39:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.0041717966087162495 norm:9.3938751888345e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:40:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.004168564919382334 norm:9.034068170876708e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:40:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.004165983758866787 norm:8.723004611965735e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:41:41 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.004164145793765783 norm:8.499571777065285e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:42:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.004162985365837812 norm:8.314298611367121e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:43:18 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.004162278957664967 norm:8.13625956652686e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:44:06 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.004161011893302202 norm:7.978942448971793e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:44:55 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.004160304553806782 norm:7.871200978115667e-06 max memory_allocated 29233.115234375 
[2025-02-18 10:45:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 10:46:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.004875782877206802 norm:7.045205711619928e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:46:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.004774475935846567 norm:4.3060568714281544e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:47:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.004719565622508526 norm:3.1105482776183635e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:48:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.004684139043092728 norm:2.43185022554826e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:49:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.0046588280238211155 norm:2.003229019464925e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:50:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.004639912396669388 norm:1.7089818356907926e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:50:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.004625578410923481 norm:1.493222953286022e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:51:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.004615394398570061 norm:1.3250614756543655e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:52:36 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.004605459980666637 norm:1.2023825547657907e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:53:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.004597533494234085 norm:1.1087141501775477e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:54:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.004589747171849012 norm:1.033836724673165e-05 max memory_allocated 29233.302734375 
[2025-02-18 10:55:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.00458445493131876 norm:9.749805030878633e-06 max memory_allocated 29233.302734375 
[2025-02-18 10:55:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.004579879809170961 norm:9.225585017702542e-06 max memory_allocated 29233.302734375 
[2025-02-18 10:56:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.004575771279633045 norm:8.85539520822931e-06 max memory_allocated 29233.302734375 
[2025-02-18 10:57:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.004573063924908638 norm:8.500104740960523e-06 max memory_allocated 29233.302734375 
[2025-02-18 10:58:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.004569008946418762 norm:8.229850209318101e-06 max memory_allocated 29233.302734375 
[2025-02-18 10:59:04 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.004566902294754982 norm:7.98251130618155e-06 max memory_allocated 29233.302734375 
[2025-02-18 10:59:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.004565266892313957 norm:7.770473530399613e-06 max memory_allocated 29233.302734375 
[2025-02-18 11:00:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.00456381868571043 norm:7.6042674663767684e-06 max memory_allocated 29233.302734375 
[2025-02-18 11:01:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.004562648478895426 norm:7.4625468187150545e-06 max memory_allocated 29233.302734375 
[2025-02-18 11:01:44 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 11:02:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.005375642329454422 norm:0.00016914053412619978 max memory_allocated 29233.490234375 
[2025-02-18 11:03:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.005252666771411896 norm:9.268216672353446e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:04:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.005193300545215607 norm:6.137055606814101e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:05:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.005155866499990225 norm:4.4849344703834504e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:05:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.00513076689094305 norm:3.470025694696233e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:06:45 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.005112944636493921 norm:2.8017311706207693e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:07:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.005098674446344376 norm:2.3313717974815518e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:08:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.005086720455437899 norm:1.9788381905527785e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:09:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.005078925285488367 norm:1.6982501620077528e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:10:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.005071943625807762 norm:1.4995390301919542e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:10:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.005066053010523319 norm:1.3363820471568033e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:11:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.005061390809714794 norm:1.2016247637802735e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:12:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.005057491362094879 norm:1.100344888982363e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:13:14 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.005054060369729996 norm:1.0220226613455452e-05 max memory_allocated 29233.490234375 
[2025-02-18 11:14:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.005050987936556339 norm:9.505771231488325e-06 max memory_allocated 29233.490234375 
[2025-02-18 11:14:51 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.005048280116170645 norm:8.889699529390782e-06 max memory_allocated 29233.490234375 
[2025-02-18 11:15:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.005046647507697344 norm:8.3729737525573e-06 max memory_allocated 29233.490234375 
[2025-02-18 11:16:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.005045493133366108 norm:7.973088941071182e-06 max memory_allocated 29233.490234375 
[2025-02-18 11:17:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.005044834688305855 norm:7.62634226703085e-06 max memory_allocated 29233.490234375 
[2025-02-18 11:18:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.005043805111199617 norm:7.357049071288202e-06 max memory_allocated 29233.490234375 
[2025-02-18 11:18:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 11:19:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.005827685352414846 norm:0.00014363812806550413 max memory_allocated 29233.677734375 
[2025-02-18 11:20:06 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.005717125255614519 norm:6.032758392393589e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:20:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.005671720951795578 norm:4.258324042893946e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:21:43 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.0056457482278347015 norm:3.22419436997734e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:22:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.005626569502055645 norm:2.4909935746109113e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:23:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.0056117200292646885 norm:2.0675613995990716e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:24:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.0055996207520365715 norm:1.7439071598346345e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:24:58 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.00559175806120038 norm:3.841313082375564e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:25:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.005584924481809139 norm:1.3361412129597738e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:26:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.005578122101724148 norm:1.2022047485515941e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:27:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.005573080852627754 norm:1.1056654329877347e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:28:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.005568386055529118 norm:1.0247820682707243e-05 max memory_allocated 29233.677734375 
[2025-02-18 11:29:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.005563707090914249 norm:9.44726252782857e-06 max memory_allocated 29233.677734375 
[2025-02-18 11:29:49 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.005559964571148157 norm:8.967595022113528e-06 max memory_allocated 29233.677734375 
[2025-02-18 11:30:37 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.005557822063565254 norm:8.45271824800875e-06 max memory_allocated 29233.677734375 
[2025-02-18 11:31:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.005555911920964718 norm:8.079568033281248e-06 max memory_allocated 29233.677734375 
[2025-02-18 11:32:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.005553161259740591 norm:7.843617822800297e-06 max memory_allocated 29233.677734375 
[2025-02-18 11:33:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.005549994762986898 norm:7.570078196295071e-06 max memory_allocated 29233.677734375 
[2025-02-18 11:33:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.005548606161028147 norm:7.341864147747401e-06 max memory_allocated 29233.677734375 
[2025-02-18 11:34:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.0055487086065113544 norm:7.193019882834051e-06 max memory_allocated 29233.677734375 
[2025-02-18 11:34:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 11:35:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.006284250877797604 norm:8.932808850659057e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:36:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.006212321110069752 norm:4.5200384192867205e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:37:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.006177786272019148 norm:3.020109943463467e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:38:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.0061561367474496365 norm:2.2247988454182632e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:39:09 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.006142427213490009 norm:1.7630622096476145e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:39:57 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.006132853217422962 norm:1.4562031537934672e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:40:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.006124008446931839 norm:1.2543479897431098e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:41:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.006117999088019133 norm:1.1194808394066058e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:42:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.006112986244261265 norm:1.0151514288736507e-05 max memory_allocated 29233.865234375 
[2025-02-18 11:43:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.006108792498707771 norm:9.248277820006479e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:44:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.006105260923504829 norm:8.589247954660095e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:44:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.006101964507251978 norm:8.10931760497624e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:45:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.006099648308008909 norm:7.689166523050517e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:46:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.006097825709730387 norm:7.312569323403295e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:47:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.006095736753195524 norm:6.978028068260755e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:48:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.006094010546803474 norm:6.691688213322777e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:48:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.0060914065688848495 norm:6.43057228444377e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:49:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.006090326234698296 norm:6.261253474804107e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:50:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.006088955793529749 norm:6.116169515735237e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:51:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.006088112480938435 norm:5.9395888456492685e-06 max memory_allocated 29233.865234375 
[2025-02-18 11:51:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 11:52:32 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.006990901660174131 norm:8.043537673074752e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:53:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.006901383865624666 norm:4.6585919335484505e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:54:09 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.006854398176074028 norm:3.2204159651882946e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:54:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.00682545593008399 norm:2.4452729121549055e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:55:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.006805764511227608 norm:1.9700797565747052e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:56:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.0067901164293289185 norm:1.6263271390926093e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:57:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.006778507959097624 norm:1.3871168448531535e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:58:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.006768885534256697 norm:1.2131697985751089e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:59:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.0067617520689964294 norm:1.0835283319465816e-05 max memory_allocated 29234.052734375 
[2025-02-18 11:59:49 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.0067554134875535965 norm:9.710160156828351e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:00:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.006750323809683323 norm:8.962489118857775e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:01:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.006745178252458572 norm:8.195370355679188e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:02:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.006741958670318127 norm:7.632088454556651e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:03:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.0067392149940133095 norm:7.217542133730603e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:03:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.006739040836691856 norm:6.780174771847669e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:04:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.006737681105732918 norm:6.410592504835222e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:05:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.006737212650477886 norm:6.080103958083782e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:06:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.006736167706549168 norm:5.996875643177191e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:07:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.006733914837241173 norm:5.892735316592734e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:07:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.006733984686434269 norm:5.8608002291293815e-06 max memory_allocated 29234.052734375 
[2025-02-18 12:08:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 12:09:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.007475964725017548 norm:2.340479113627225e-05 max memory_allocated 29234.240234375 
[2025-02-18 12:09:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.00744379498064518 norm:1.4524055586662143e-05 max memory_allocated 29234.240234375 
[2025-02-18 12:10:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.007426230236887932 norm:1.074856936611468e-05 max memory_allocated 29234.240234375 
[2025-02-18 12:11:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.007413377054035664 norm:8.836825145408511e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:12:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.0074043720960617065 norm:7.59694285079604e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:13:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.007397398352622986 norm:6.844834388175514e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:14:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.007390508893877268 norm:6.297394065768458e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:14:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.007385114207863808 norm:5.887477982469136e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:15:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.007381064351648092 norm:5.568260348809417e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:16:26 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.007376968860626221 norm:5.327938652044395e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:17:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.007373425178229809 norm:5.173468707653228e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:18:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.007370823062956333 norm:5.0220501179865096e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:18:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.007368981372565031 norm:4.884453119302634e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:19:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.007367382757365704 norm:4.7902785809128545e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:20:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.007365004159510136 norm:4.7060707402124535e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:21:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.007363553158938885 norm:4.641683062800439e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:22:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.007362876087427139 norm:4.5748206503049005e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:22:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.007362415548413992 norm:4.530095793597866e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:23:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.00736110657453537 norm:4.492969765124144e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:24:31 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.0073609184473752975 norm:4.458196599443909e-06 max memory_allocated 29234.240234375 
[2025-02-18 12:24:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 12:25:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.008378720842301846 norm:0.00011611730587901548 max memory_allocated 29234.427734375 
[2025-02-18 12:26:33 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.008286869153380394 norm:6.656612094957381e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:27:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.00824024435132742 norm:4.520440779742785e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:28:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.008208904415369034 norm:3.376332460902631e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:28:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.008191702887415886 norm:2.638035402924288e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:29:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.008181803859770298 norm:2.1266225303406827e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:30:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.00816939864307642 norm:1.7890584786073305e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:31:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.008161008358001709 norm:1.5264849935192615e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:32:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.008152857422828674 norm:1.3391065294854343e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:33:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.008146334439516068 norm:1.1951558917644434e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:33:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.008142167702317238 norm:1.0824737728398759e-05 max memory_allocated 29234.427734375 
[2025-02-18 12:34:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.008138700388371944 norm:9.794088327907957e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:35:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.008135933429002762 norm:9.046212653629482e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:36:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.008131980895996094 norm:8.389533832087182e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:37:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.008129536174237728 norm:7.761622327961959e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:37:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.008126998320221901 norm:7.2547991294413805e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:38:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.008127317763864994 norm:6.898166247992776e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:39:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.00812609028071165 norm:6.635295903834049e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:40:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.008124898187816143 norm:6.364983164530713e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:41:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.008125939406454563 norm:6.0652678257611115e-06 max memory_allocated 29234.427734375 
[2025-02-18 12:41:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 12:42:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.0090401042252779 norm:4.4434207666199654e-05 max memory_allocated 29234.615234375 
[2025-02-18 12:43:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.008990089409053326 norm:2.545778261264786e-05 max memory_allocated 29234.615234375 
[2025-02-18 12:43:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.008965217508375645 norm:1.7801410649553873e-05 max memory_allocated 29234.615234375 
[2025-02-18 12:44:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.0089495200663805 norm:1.366024480375927e-05 max memory_allocated 29234.615234375 
[2025-02-18 12:45:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.008937078528106213 norm:1.107451862480957e-05 max memory_allocated 29234.615234375 
[2025-02-18 12:46:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.00892932154238224 norm:9.26020584302023e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:47:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.008921908214688301 norm:8.021148460102268e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:47:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.008915755897760391 norm:7.125065167201683e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:48:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.008909844793379307 norm:6.505561032099649e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:49:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.008904941380023956 norm:6.11451923759887e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:50:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.008901674300432205 norm:5.710377536161104e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:51:07 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.00889721978455782 norm:5.379970389185473e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:51:56 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.008894809521734715 norm:5.060881449026056e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:52:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.008892158046364784 norm:4.886893293587491e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:53:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.008891100063920021 norm:4.704562797996914e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:54:21 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.008889148011803627 norm:4.559467470244272e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:55:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.0088875787332654 norm:4.4832268031314015e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:55:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.00888528861105442 norm:4.365109361970099e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:56:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.00888427160680294 norm:4.30097088610637e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:57:34 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.008883349597454071 norm:4.2500814743107185e-06 max memory_allocated 29234.615234375 
[2025-02-18 12:57:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 12:58:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.0099447937682271 norm:6.87960273353383e-05 max memory_allocated 29234.802734375 
[2025-02-18 12:59:33 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.009864703752100468 norm:3.846395338769071e-05 max memory_allocated 29234.802734375 
[2025-02-18 13:00:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.009826813824474812 norm:2.5947234462364577e-05 max memory_allocated 29234.802734375 
[2025-02-18 13:01:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.009804111905395985 norm:1.9419971067691222e-05 max memory_allocated 29234.802734375 
[2025-02-18 13:01:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.00978772807866335 norm:1.5469060599571094e-05 max memory_allocated 29234.802734375 
[2025-02-18 13:02:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.009774759411811829 norm:1.281953154830262e-05 max memory_allocated 29234.802734375 
[2025-02-18 13:03:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.009764139540493488 norm:1.0895870218519121e-05 max memory_allocated 29234.802734375 
[2025-02-18 13:04:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.009756079874932766 norm:9.52798109210562e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:05:12 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.009750314056873322 norm:8.572267688577995e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:06:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.009743685834109783 norm:7.672771971556358e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:06:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.009739253669977188 norm:6.954208402021322e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:07:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.00973497238010168 norm:6.418581506295595e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:08:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.009731418453156948 norm:6.0110278354841284e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:09:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.009728015400469303 norm:5.677375611412572e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:10:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.009726649150252342 norm:5.341167707229033e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:10:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.009724363684654236 norm:5.1064666877209675e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:11:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.009721435606479645 norm:4.929858732793946e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:12:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.009719526395201683 norm:4.810820428247098e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:13:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.00971832126379013 norm:4.61681383967516e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:14:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.009716506116092205 norm:4.500884642766323e-06 max memory_allocated 29234.802734375 
[2025-02-18 13:14:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 13:15:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.01098443940281868 norm:0.0001322677417192608 max memory_allocated 29234.990234375 
[2025-02-18 13:16:10 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.01088192779570818 norm:8.360571519006044e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:16:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.010825212113559246 norm:6.011086588841863e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:17:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.010787862353026867 norm:4.630982220987789e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:18:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.0107621680945158 norm:3.722660767380148e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:19:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.01074210088700056 norm:3.077707515330985e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:20:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.010726087726652622 norm:2.58183827099856e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:21:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.010713953524827957 norm:2.2292537323664874e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:21:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.010702917352318764 norm:1.9578963474486955e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:22:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.010695348493754864 norm:1.7401791410520673e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:23:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.010688286274671555 norm:1.5614205040037632e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:24:14 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.010682478547096252 norm:1.405527837050613e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:25:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.010677136480808258 norm:1.2909034012409393e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:25:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.010673124343156815 norm:1.1822701708297245e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:26:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.010668335482478142 norm:1.102313763112761e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:27:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.010666418820619583 norm:1.011496533465106e-05 max memory_allocated 29234.990234375 
[2025-02-18 13:28:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.010663365945219994 norm:9.461093213758431e-06 max memory_allocated 29234.990234375 
[2025-02-18 13:29:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.010661030188202858 norm:8.939217877923511e-06 max memory_allocated 29234.990234375 
[2025-02-18 13:29:53 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.010658210143446922 norm:8.545402124582324e-06 max memory_allocated 29234.990234375 
[2025-02-18 13:30:41 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.010657135397195816 norm:8.204880941775627e-06 max memory_allocated 29234.990234375 
[2025-02-18 13:30:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-18 13:31:51 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.011882101185619831 norm:9.775995567906648e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:32:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.011799782514572144 norm:5.7759367336984724e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:33:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.011756946332752705 norm:3.967113298131153e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:34:16 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.011731033213436604 norm:2.9349035685299896e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:35:04 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.011713087558746338 norm:2.277930616401136e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:35:53 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.011697554029524326 norm:1.852640343713574e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:36:41 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.011686117388308048 norm:1.5536692444584332e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:37:30 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.011676180176436901 norm:1.3194575330999214e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:38:18 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.011668829247355461 norm:1.1411982086428907e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:39:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.011662895791232586 norm:1.0017499334935565e-05 max memory_allocated 29235.177734375 
[2025-02-18 13:39:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.011657171882689 norm:8.983712177723646e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:40:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.011651959270238876 norm:8.219893061323091e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:41:31 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.011647808365523815 norm:7.503870165237458e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:42:20 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.011645002290606499 norm:6.889968972245697e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:43:08 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.011641441844403744 norm:6.422073056455702e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:43:56 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.011637359857559204 norm:5.988802513456903e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:44:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.011635597795248032 norm:5.6562003010185435e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:45:33 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.01163464691489935 norm:5.317320756148547e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:46:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.011634154245257378 norm:5.047256763646146e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:47:09 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.011632862500846386 norm:4.8357187552028336e-06 max memory_allocated 29235.177734375 
[2025-02-18 13:47:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-18 13:48:19 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.013019265606999397 norm:9.408994810655713e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:49:07 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.012916735373437405 norm:5.874421913176775e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:49:56 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.012859640643000603 norm:4.200776675133966e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:50:44 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.012823156081140041 norm:3.188457776559517e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:51:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.012799314223229885 norm:2.4977403882076032e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:52:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.012780625373125076 norm:2.0762920030392706e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:53:09 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.012766960076987743 norm:1.7522172129247338e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:53:57 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.012755726464092731 norm:1.5195300875348039e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:54:46 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.01274769939482212 norm:1.3304470485309139e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:55:34 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.012739183381199837 norm:1.1828638889710419e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:56:22 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.012733128853142262 norm:1.0532675332797226e-05 max memory_allocated 29235.365234375 
[2025-02-18 13:57:11 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.012726671993732452 norm:9.750117897056043e-06 max memory_allocated 29235.365234375 
[2025-02-18 13:57:59 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.01272305566817522 norm:8.925395377445966e-06 max memory_allocated 29235.365234375 
[2025-02-18 13:58:48 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.012720776721835136 norm:8.03708826424554e-06 max memory_allocated 29235.365234375 
[2025-02-18 13:59:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.012718452140688896 norm:7.394705335173057e-06 max memory_allocated 29235.365234375 
[2025-02-18 14:00:25 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.012715782970190048 norm:6.96049119142117e-06 max memory_allocated 29235.365234375 
[2025-02-18 14:01:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.012714087031781673 norm:6.6107359089073725e-06 max memory_allocated 29235.365234375 
[2025-02-18 14:02:01 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.01271276455372572 norm:6.304483122221427e-06 max memory_allocated 29235.365234375 
[2025-02-18 14:02:50 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.012712826952338219 norm:5.958295787422685e-06 max memory_allocated 29235.365234375 
[2025-02-18 14:03:38 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.012710205279290676 norm:5.820439128001453e-06 max memory_allocated 29235.365234375 
[2025-02-18 14:03:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-18 14:04:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.01424275804311037 norm:6.35271571809426e-05 max memory_allocated 29235.552734375 
[2025-02-18 14:05:35 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.014172766357660294 norm:3.677307904581539e-05 max memory_allocated 29235.552734375 
[2025-02-18 14:06:24 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.014136667363345623 norm:2.5090335839195177e-05 max memory_allocated 29235.552734375 
[2025-02-18 14:07:12 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.01411318127065897 norm:1.869425250333734e-05 max memory_allocated 29235.552734375 
[2025-02-18 14:08:00 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.01410364918410778 norm:1.513947790954262e-05 max memory_allocated 29235.552734375 
[2025-02-18 14:08:49 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.014079239219427109 norm:1.2208886801090557e-05 max memory_allocated 29235.552734375 
[2025-02-18 14:09:37 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.014073358848690987 norm:1.0334810212953016e-05 max memory_allocated 29235.552734375 
[2025-02-18 14:10:26 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.01406399067491293 norm:8.989072739495896e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:11:14 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.014071975834667683 norm:1.0556826055108104e-05 max memory_allocated 29235.552734375 
[2025-02-18 14:12:03 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.014048436656594276 norm:7.106876637408277e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:12:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.014043796807527542 norm:6.500495146610774e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:13:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.014040756970643997 norm:5.941001745668473e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:14:29 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.014039558358490467 norm:5.526789209397975e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:15:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.014038782566785812 norm:5.342654731066432e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:16:06 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.01403649989515543 norm:5.080337814433733e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:16:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.01403399370610714 norm:4.8793131099955644e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:17:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.014032823033630848 norm:4.83478333990206e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:18:31 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.014031851664185524 norm:4.693797563959379e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:19:20 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.014031427912414074 norm:4.594520760292653e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:20:08 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.01403055340051651 norm:4.598457962856628e-06 max memory_allocated 29235.552734375 
[2025-02-18 14:20:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-18 14:21:15 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.015603916719555855 norm:2.494759246474132e-05 max memory_allocated 29235.740234375 
[2025-02-18 14:22:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.01555259432643652 norm:1.5978028386598453e-05 max memory_allocated 29235.740234375 
[2025-02-18 14:22:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.015524530783295631 norm:1.2054631042701658e-05 max memory_allocated 29235.740234375 
[2025-02-18 14:23:41 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.015505063347518444 norm:9.868202141660731e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:24:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.015489351004362106 norm:8.42015833768528e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:25:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.015478795394301414 norm:7.388768608507235e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:26:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.015471719205379486 norm:6.6181378315377515e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:26:55 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.015463734045624733 norm:6.078707883716561e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:27:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.015459096990525723 norm:5.6434637372149155e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:28:32 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.015454457141458988 norm:5.3507028496824205e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:29:21 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.015450147911906242 norm:5.098335350339767e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:30:09 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.015446994453668594 norm:4.83358780911658e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:30:58 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.015443414449691772 norm:4.707349944510497e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:31:46 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.01543966494500637 norm:4.636663106794003e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:32:35 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.01543841976672411 norm:4.51128153144964e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:33:24 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.015436464920639992 norm:4.404116225487087e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:34:12 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.015435734763741493 norm:4.343643922766205e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:35:01 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.015433833003044128 norm:4.2825922719202936e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:35:49 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.015431673265993595 norm:4.237562279740814e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:36:38 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.015429778024554253 norm:4.205052846373292e-06 max memory_allocated 29235.740234375 
[2025-02-18 14:36:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-18 14:37:44 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.01763048768043518 norm:0.0001753136166371405 max memory_allocated 29235.927734375 
[2025-02-18 14:38:33 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.017447754740715027 norm:0.00010825155914062634 max memory_allocated 29235.927734375 
[2025-02-18 14:39:22 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.01734965667128563 norm:7.608487794641405e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:40:10 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.017289387062191963 norm:5.761756983702071e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:40:59 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.01724722608923912 norm:4.5581833546748385e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:41:47 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.017219169065356255 norm:3.691684833029285e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:42:36 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.01719770021736622 norm:3.0704595701536164e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:43:25 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.017178978770971298 norm:2.6077552320202813e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:44:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.01716350018978119 norm:2.2575994080398232e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:45:02 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.017153481021523476 norm:1.9828246877295896e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:45:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.017147384583950043 norm:1.7195749023812823e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:46:39 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.017141155898571014 norm:1.521343529020669e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:47:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.017137661576271057 norm:1.3554490578826517e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:48:16 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.017133258283138275 norm:1.2234634596097749e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:49:04 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.017129041254520416 norm:1.1221078239032067e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:49:53 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.01712591014802456 norm:1.0356892744312063e-05 max memory_allocated 29235.927734375 
[2025-02-18 14:50:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.017126770690083504 norm:9.534268428978976e-06 max memory_allocated 29235.927734375 
[2025-02-18 14:51:30 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.017125951126217842 norm:8.801902367849834e-06 max memory_allocated 29235.927734375 
[2025-02-18 14:52:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.017124047502875328 norm:8.357477781828493e-06 max memory_allocated 29235.927734375 
[2025-02-18 14:53:07 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.017123011872172356 norm:7.931562322482932e-06 max memory_allocated 29235.927734375 
[2025-02-18 14:53:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-18 14:54:14 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.01957070641219616 norm:6.287291034823284e-05 max memory_allocated 29236.115234375 
[2025-02-18 14:55:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.019403845071792603 norm:4.0214519685832784e-05 max memory_allocated 29236.115234375 
[2025-02-18 14:55:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.01932227984070778 norm:2.71229164354736e-05 max memory_allocated 29236.115234375 
[2025-02-18 14:56:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.019276749342679977 norm:1.913331652758643e-05 max memory_allocated 29236.115234375 
[2025-02-18 14:57:28 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.019244946539402008 norm:1.5449955753865652e-05 max memory_allocated 29236.115234375 
[2025-02-18 14:58:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.019220221787691116 norm:1.3567663700086996e-05 max memory_allocated 29236.115234375 
[2025-02-18 14:59:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.019203485921025276 norm:1.2573659660120029e-05 max memory_allocated 29236.115234375 
[2025-02-18 14:59:53 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.019185390323400497 norm:1.1417955647630151e-05 max memory_allocated 29236.115234375 
[2025-02-18 15:00:42 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.01917497254908085 norm:1.051374783855863e-05 max memory_allocated 29236.115234375 
[2025-02-18 15:01:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.01917014643549919 norm:1.0151851711270865e-05 max memory_allocated 29236.115234375 
[2025-02-18 15:02:18 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.01916373334825039 norm:9.775851140148006e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:03:07 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.019156523048877716 norm:9.682647032605018e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:03:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.019150102511048317 norm:9.490252523391973e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:04:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.019150294363498688 norm:9.295857125835028e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:05:32 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.019149107858538628 norm:9.282129212806467e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:06:20 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.019152499735355377 norm:9.273133400711231e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:07:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.019152019172906876 norm:9.238091479346622e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:07:57 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.019155384972691536 norm:9.126698387262877e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:08:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.019169852137565613 norm:9.116699402511586e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:09:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.01917538233101368 norm:9.167594726022799e-06 max memory_allocated 29236.115234375 
[2025-02-18 15:09:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-18 15:10:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.0235152468085289 norm:0.00010199304233537987 max memory_allocated 29236.302734375 
[2025-02-18 15:11:29 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.023245397955179214 norm:5.4786862165201455e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:12:17 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.02313782274723053 norm:4.009160693385638e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:13:06 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.023074451833963394 norm:3.646889308583923e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:13:54 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.023031990975141525 norm:3.531738184392452e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:14:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.023001763969659805 norm:3.6610010283766314e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:15:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.02297530695796013 norm:3.7401729059638456e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:16:19 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.02294873632490635 norm:3.806834138231352e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:17:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.02292202040553093 norm:3.928444129996933e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:17:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.022899575531482697 norm:4.044143497594632e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:18:45 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.02286379784345627 norm:4.057621117681265e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:19:33 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.022850673645734787 norm:3.8631515053566545e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:20:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.022839361801743507 norm:3.668012504931539e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:21:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.022834839299321175 norm:3.560996992746368e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:21:58 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.02282808907330036 norm:3.328788443468511e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:22:47 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.022823192179203033 norm:3.282564284745604e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:23:35 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.02282017469406128 norm:3.21348306897562e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:24:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.02281705103814602 norm:3.0176441214280203e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:25:12 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.022818593308329582 norm:3.0023826184333302e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:26:01 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.02281458117067814 norm:2.8541102437884547e-05 max memory_allocated 29236.302734375 
[2025-02-18 15:26:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-18 15:27:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:0.0782143622636795 norm:0.0009723039693199098 max memory_allocated 29236.490234375 
[2025-02-18 15:27:56 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:0.07580376416444778 norm:0.0007463606307283044 max memory_allocated 29236.490234375 
[2025-02-18 15:28:44 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.07512947171926498 norm:0.0006678351783193648 max memory_allocated 29236.490234375 
[2025-02-18 15:29:33 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.07466824352741241 norm:0.0006632913136854768 max memory_allocated 29236.490234375 
[2025-02-18 15:30:21 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.07412797957658768 norm:0.0007122375536710024 max memory_allocated 29236.490234375 
[2025-02-18 15:31:09 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.07375417649745941 norm:0.0007266103057190776 max memory_allocated 29236.490234375 
[2025-02-18 15:31:58 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.07365168631076813 norm:0.000782144081313163 max memory_allocated 29236.490234375 
[2025-02-18 15:32:46 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.07347088307142258 norm:0.0008354088058695197 max memory_allocated 29236.490234375 
[2025-02-18 15:33:35 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.07351619750261307 norm:0.0008603506721556187 max memory_allocated 29236.490234375 
[2025-02-18 15:34:23 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.07332973927259445 norm:0.0009710344020277262 max memory_allocated 29236.490234375 
[2025-02-18 15:35:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.07368013262748718 norm:0.001027979888021946 max memory_allocated 29236.490234375 
[2025-02-18 15:36:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.07362309843301773 norm:0.001147114671766758 max memory_allocated 29236.490234375 
[2025-02-18 15:36:48 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.07391484081745148 norm:0.0012710365699604154 max memory_allocated 29236.490234375 
[2025-02-18 15:37:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.0736955851316452 norm:0.0013669936452060938 max memory_allocated 29236.490234375 
[2025-02-18 15:38:25 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.07394861429929733 norm:0.0014792969450354576 max memory_allocated 29236.490234375 
[2025-02-18 15:39:13 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.07402699440717697 norm:0.0015020378632470965 max memory_allocated 29236.490234375 
[2025-02-18 15:40:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.07405924052000046 norm:0.0016407535877078772 max memory_allocated 29236.490234375 
[2025-02-18 15:40:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.07411003112792969 norm:0.0017642751336097717 max memory_allocated 29236.490234375 
[2025-02-18 15:41:39 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.07446341216564178 norm:0.0018707659328356385 max memory_allocated 29236.490234375 
[2025-02-18 15:42:27 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.07468842715024948 norm:0.0020330457482486963 max memory_allocated 29236.490234375 
[2025-02-18 15:42:41 root] (main_calibration.py 365): INFO 39741.30063843727
[2025-02-18 15:43:45 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-18 15:45:40 root] (main_calibration.py 158): INFO wikitext2 : 4.9061279296875
[2025-02-18 15:45:40 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-18 15:48:39 root] (main_calibration.py 158): INFO c4 : 6.492581844329834
