[2025-02-18 04:40:22 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration/llama-7b-hf-w8a8', save_dir='./log-calibration/quant/llama-7b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-18 04:44:58 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 04:44:58 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-18 04:44:59 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 04:45:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 04:45:36 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0008208120707422495 norm:0.00020595885871443897 max memory_allocated 22509.63671875 
[2025-02-18 04:46:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0005469414172694087 norm:6.814244989072904e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:46:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.00045877424417994916 norm:3.9669837860856205e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:47:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.00041791232069954276 norm:2.8210475647938438e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:47:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0003964302013628185 norm:2.273081372550223e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:48:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.00038500019581988454 norm:2.0000912627438083e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:48:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.000376950396457687 norm:1.8545037164585665e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:49:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.00037180795334279537 norm:1.7714110072120093e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:49:56 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.00036850577453151345 norm:1.7236729036085308e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:50:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00036507099866867065 norm:1.6935417079366744e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:51:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.00036289356648921967 norm:1.6737649275455624e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:51:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.0003612326108850539 norm:1.6661258996464312e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:52:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0003602851938921958 norm:1.6685364244040102e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:52:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0003604904923122376 norm:1.6912596038309857e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:53:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0003585135855246335 norm:1.6800340745248832e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:53:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.00035807909443974495 norm:1.679305023571942e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:54:18 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.00035751110408455133 norm:1.702686677163001e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:54:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00035778331221081316 norm:1.7081361875170842e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:55:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00035843567457050085 norm:1.7317937818006612e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:55:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.000357231852831319 norm:1.704633359622676e-05 max memory_allocated 22509.63671875 
[2025-02-18 04:56:06 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 04:56:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.001180279185064137 norm:0.00015716931375209242 max memory_allocated 22509.80859375 
[2025-02-18 04:57:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0009327256120741367 norm:6.0234822740312666e-05 max memory_allocated 22509.80859375 
[2025-02-18 04:57:47 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.000840343302115798 norm:3.656723492895253e-05 max memory_allocated 22509.80859375 
[2025-02-18 04:58:20 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.000794969208072871 norm:2.6474714104551822e-05 max memory_allocated 22509.80859375 
[2025-02-18 04:58:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0007695730892010033 norm:2.1401192498160526e-05 max memory_allocated 22509.80859375 
[2025-02-18 04:59:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0007527995621785522 norm:1.8603252101456746e-05 max memory_allocated 22509.80859375 
[2025-02-18 04:59:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0007414526771754026 norm:1.6935844541876577e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:00:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0007324913749471307 norm:1.590790634509176e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:01:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0007271093782037497 norm:1.5263194654835388e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:01:36 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0007227043388411403 norm:1.4930677025404293e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:02:09 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0007194480276666582 norm:1.4783943697693758e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:02:42 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0007181064574979246 norm:1.4782491234655026e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:03:15 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0007185116992332041 norm:1.4843500139249954e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:03:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0007140709785744548 norm:1.4689396266476251e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:04:20 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0007129281293600798 norm:1.4865456250845455e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:04:53 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0007117065833881497 norm:1.4905555872246623e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:05:26 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0007113130996003747 norm:1.4835314686933998e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:05:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.000710172695107758 norm:1.4724078027938958e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:06:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0007101169321686029 norm:1.467540096200537e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:07:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0007096276385709643 norm:1.4919714885763824e-05 max memory_allocated 22509.80859375 
[2025-02-18 05:07:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 05:07:49 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.0011587657500058413 norm:7.905623351689428e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:08:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.0010080869542434812 norm:4.4429816625779495e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:08:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.000939263729378581 norm:3.296752038295381e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:09:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0009051344823092222 norm:2.8316764655755833e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:10:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0008801167132332921 norm:2.524404408177361e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:10:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.0008634548867121339 norm:2.334810415050015e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:11:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0008578973356634378 norm:2.256493826280348e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:11:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.0008555003441870213 norm:2.3669099391554482e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:12:11 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.0008423482067883015 norm:2.0722045519505627e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:12:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0008475276990793645 norm:2.18204968405189e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:13:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0008376865298487246 norm:2.0795141608687118e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:13:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0008387886919081211 norm:2.1247875338303857e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:14:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0008311071433126926 norm:2.0000943550257944e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:14:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0008289944380521774 norm:1.9515029634931125e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:15:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.000828904623631388 norm:1.9665596482809633e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:16:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0008329169941134751 norm:2.0854984541074373e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:16:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0008286964148283005 norm:2.0114874132559635e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:17:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0008320945780724287 norm:2.0506839064182714e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:17:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.000828611955512315 norm:2.015029895119369e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:18:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0008295707521028817 norm:2.0581499484251253e-05 max memory_allocated 22509.98046875 
[2025-02-18 05:18:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 05:18:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0015906546032056212 norm:0.0003965198702644557 max memory_allocated 22510.15234375 
[2025-02-18 05:19:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0012391605414450169 norm:0.00016506770043633878 max memory_allocated 22510.15234375 
[2025-02-18 05:20:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.0011110614286735654 norm:9.837189281824976e-05 max memory_allocated 22510.15234375 
[2025-02-18 05:20:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0010436682496219873 norm:6.740682147210464e-05 max memory_allocated 22510.15234375 
[2025-02-18 05:21:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.0010031949495896697 norm:4.7479348722845316e-05 max memory_allocated 22510.15234375 
[2025-02-18 05:21:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0009832079522311687 norm:3.3709944545989856e-05 max memory_allocated 22510.15234375 
[2025-02-18 05:22:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0009768358431756496 norm:2.3363580112345517e-05 max memory_allocated 22510.15234375 
[2025-02-18 05:22:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0009736820938996971 norm:1.6079222405096516e-05 max memory_allocated 22510.15234375 
[2025-02-18 05:23:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.0009702186798676848 norm:1.2771625733876135e-05 max memory_allocated 22510.15234375 
[2025-02-18 05:23:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0009674052707850933 norm:1.1099120456492528e-05 max memory_allocated 22510.15234375 
[2025-02-18 05:24:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0009648517589084804 norm:9.824054359341972e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:24:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0009631430730223656 norm:9.165021765511483e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:25:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0009609691915102303 norm:8.753227120905649e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:26:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0009608150576241314 norm:8.500212061335333e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:26:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.000959776749368757 norm:8.405811968259513e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:27:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0009588561952114105 norm:8.36532035464188e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:27:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.0009591334965080023 norm:8.420648555329535e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:28:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0009578666649758816 norm:8.331294338859152e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:28:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0009582527563907206 norm:8.259924470621627e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:29:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0009578611934557557 norm:8.336993232660461e-06 max memory_allocated 22510.15234375 
[2025-02-18 05:29:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 05:30:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0017971550114452839 norm:0.00038905744440853596 max memory_allocated 22510.32421875 
[2025-02-18 05:30:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0014305681688711047 norm:0.00015952404646668583 max memory_allocated 22510.32421875 
[2025-02-18 05:31:10 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.001300466014072299 norm:9.380872506881133e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:31:43 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0012454885290935636 norm:6.002868394716643e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:32:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0012288144789636135 norm:3.8107464206404984e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:32:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.001222042366862297 norm:2.5123879822785966e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:33:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0012151394039392471 norm:1.9059432815993205e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:33:54 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0012091960525140166 norm:1.5554724086541682e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:34:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.0012062082532793283 norm:1.386734948027879e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:35:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.0012019327841699123 norm:1.2805661754100583e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:35:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0012005474418401718 norm:1.2022840564895887e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:36:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.0011990919010713696 norm:1.1516542144818231e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:36:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.0011971581261605024 norm:1.1242180335102603e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:37:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0011967923492193222 norm:1.1007738066837192e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:37:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.0011959667317569256 norm:1.0812278560479172e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:38:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.001196081517264247 norm:1.068191158992704e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:38:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0011962020071223378 norm:1.0540878975007217e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:39:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0011970525374636054 norm:1.0614164239086676e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:39:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0011964014265686274 norm:1.0593664228508715e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:40:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0011951003689318895 norm:1.0516976544749923e-05 max memory_allocated 22510.32421875 
[2025-02-18 05:40:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 05:41:13 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0018932195380330086 norm:0.00028612310416065156 max memory_allocated 22510.49609375 
[2025-02-18 05:41:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.0015808865427970886 norm:0.00012991313997190446 max memory_allocated 22510.49609375 
[2025-02-18 05:42:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0014730863040313125 norm:7.91686397860758e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:42:51 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.001424409681931138 norm:5.4719988838769495e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:43:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0013989245053380728 norm:3.905887933797203e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:43:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0013863168423995376 norm:2.853290061466396e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:44:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0013785188784822822 norm:2.084701191051863e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:45:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0013721247669309378 norm:1.6202135157072917e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:45:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0013689558254554868 norm:1.3045180821791291e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:46:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.001366621581837535 norm:1.1488206837384496e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:46:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0013647284358739853 norm:1.0468765140103642e-05 max memory_allocated 22510.49609375 
[2025-02-18 05:47:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.001364239607937634 norm:9.963892807718366e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:47:46 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.0013627170119434595 norm:9.803209650272038e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:48:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.0013637595111504197 norm:9.704724106995855e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:48:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0013636723160743713 norm:9.566227163304575e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:49:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0013624762650579214 norm:9.445875548408367e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:49:57 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0013620787067338824 norm:9.41653433983447e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:50:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0013603913830593228 norm:9.451819096284453e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:51:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0013594564516097307 norm:9.402530849911273e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:51:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.001358726411126554 norm:9.476060768065508e-06 max memory_allocated 22510.49609375 
[2025-02-18 05:51:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 05:52:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0022600798401981592 norm:0.0005397471832111478 max memory_allocated 22510.66796875 
[2025-02-18 05:52:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.001748778042383492 norm:0.00017786011449061334 max memory_allocated 22510.66796875 
[2025-02-18 05:53:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.001637370907701552 norm:0.00011221912427572533 max memory_allocated 22510.66796875 
[2025-02-18 05:53:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.001586916740052402 norm:8.081146370386705e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:54:32 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0015551019459962845 norm:6.159613985801116e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:55:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0015403360594063997 norm:4.982711107004434e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:55:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0015265109250321984 norm:4.027593968203291e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:56:10 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.0015172469429671764 norm:3.3783348044380546e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:56:43 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0015126258367672563 norm:2.8052498237229884e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:57:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0015077474527060986 norm:2.3041806343826465e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:57:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0015040176222100854 norm:1.9042827261728235e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:58:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0015043605817481875 norm:1.618491660337895e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:58:54 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.0015048003988340497 norm:1.276305101782782e-05 max memory_allocated 22510.66796875 
[2025-02-18 05:59:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0015033807139843702 norm:1.1204195288883056e-05 max memory_allocated 22510.66796875 
[2025-02-18 06:00:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0015035459073260427 norm:9.914090696838684e-06 max memory_allocated 22510.66796875 
[2025-02-18 06:00:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.001500805839896202 norm:9.166895324597135e-06 max memory_allocated 22510.66796875 
[2025-02-18 06:01:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0015026378678157926 norm:8.274888386949897e-06 max memory_allocated 22510.66796875 
[2025-02-18 06:01:38 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0015018023550510406 norm:8.232083018810954e-06 max memory_allocated 22510.66796875 
[2025-02-18 06:02:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.001500817947089672 norm:8.075272489804775e-06 max memory_allocated 22510.66796875 
[2025-02-18 06:02:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.0015053898096084595 norm:7.876729796407744e-06 max memory_allocated 22510.66796875 
[2025-02-18 06:02:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 06:03:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.0022619394585490227 norm:0.0003492841497063637 max memory_allocated 22510.83984375 
[2025-02-18 06:04:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0018393596401438117 norm:0.00012135962606407702 max memory_allocated 22510.83984375 
[2025-02-18 06:04:34 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.0017460824456065893 norm:7.828718662494794e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:05:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0016996570629999042 norm:5.654841515934095e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:05:40 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0016737118130549788 norm:4.318605351727456e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:06:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.0016586306737735868 norm:3.465871122898534e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:06:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0016454930882900953 norm:2.7331752789905295e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:07:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.0016378747532144189 norm:2.2887965315021574e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:07:51 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.0016351102385669947 norm:1.8929287762148306e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:08:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0016277236863970757 norm:1.4811106666456908e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:08:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0016254308866336942 norm:1.2232013432367239e-05 max memory_allocated 22510.83984375 
[2025-02-18 06:09:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.0016283881850540638 norm:9.992143532144837e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:10:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0016293219523504376 norm:8.537233952665702e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:10:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.0016305004246532917 norm:7.816624020051677e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:11:08 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0016263547586277127 norm:7.76748674979899e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:11:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0016249642940238118 norm:7.113520496204728e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:12:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0016237448435276747 norm:6.851204489066731e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:12:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0016242826823145151 norm:6.5376070779166184e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:13:19 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.0016247755847871304 norm:6.42031773168128e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:13:52 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.001624736818484962 norm:6.286356438067742e-06 max memory_allocated 22510.83984375 
[2025-02-18 06:14:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 06:14:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0021757972426712513 norm:0.0001953321334440261 max memory_allocated 22511.01171875 
[2025-02-18 06:15:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0018991531105712056 norm:7.623205601703376e-05 max memory_allocated 22511.01171875 
[2025-02-18 06:15:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.0018229042179882526 norm:4.861754860030487e-05 max memory_allocated 22511.01171875 
[2025-02-18 06:16:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.001787936664186418 norm:3.2573316275374964e-05 max memory_allocated 22511.01171875 
[2025-02-18 06:16:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0017720595933496952 norm:2.6385398086858913e-05 max memory_allocated 22511.01171875 
[2025-02-18 06:17:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.0017620952567085624 norm:2.074815347441472e-05 max memory_allocated 22511.01171875 
[2025-02-18 06:17:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.0017555970698595047 norm:1.6469108231831342e-05 max memory_allocated 22511.01171875 
[2025-02-18 06:18:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.0017504113493487239 norm:1.4081104382057674e-05 max memory_allocated 22511.01171875 
[2025-02-18 06:18:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0017472724430263042 norm:1.1501981134642847e-05 max memory_allocated 22511.01171875 
[2025-02-18 06:19:32 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0017451837193220854 norm:9.157847671303898e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:20:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.0017422896344214678 norm:7.980343980307225e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:20:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0017413903260603547 norm:7.197280865511857e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:21:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0017405461985617876 norm:6.564740488101961e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:21:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0017390617867931724 norm:6.212078005773947e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:22:16 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.001740997307933867 norm:5.847240572620649e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:22:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.001741733169183135 norm:5.781141680927249e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:23:22 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.0017397855408489704 norm:5.5119207900133915e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:23:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0017387950792908669 norm:5.549772595259128e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:24:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0017379040364176035 norm:5.603266799880657e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:25:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0017396191833540797 norm:5.462191893457202e-06 max memory_allocated 22511.01171875 
[2025-02-18 06:25:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 06:25:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002497714478522539 norm:0.00027280068024992943 max memory_allocated 22511.18359375 
[2025-02-18 06:26:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0020611414220184088 norm:7.177048973971978e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:26:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0020130989141762257 norm:5.9143963881069794e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:27:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0019654955249279737 norm:3.961235415772535e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:27:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0019439267925918102 norm:3.0148346922942437e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:28:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0019348053028807044 norm:2.5025989089044742e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:29:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.001924113486893475 norm:2.1508971258299425e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:29:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0019115599570795894 norm:1.7169608327094465e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:30:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0019097770564258099 norm:1.4697351616632659e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:30:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0019092988222837448 norm:1.3090852917230222e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:31:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0019060103222727776 norm:1.1726373486453667e-05 max memory_allocated 22511.18359375 
[2025-02-18 06:31:46 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0019049617694690824 norm:9.7978036137647e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:32:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0019034030847251415 norm:8.55891721585067e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:32:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.0019041730556637049 norm:7.858379831304774e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:33:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0019036062294617295 norm:7.059517429297557e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:33:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.001899163005873561 norm:6.543858489749255e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:34:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.0019010805990546942 norm:6.016838597133756e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:35:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0019007575465366244 norm:5.626315214612987e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:35:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.001902159070596099 norm:5.456937287817709e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:36:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0019012745469808578 norm:5.200925443205051e-06 max memory_allocated 22511.18359375 
[2025-02-18 06:36:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 06:36:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0024306774139404297 norm:0.00016769430658314377 max memory_allocated 22511.35546875 
[2025-02-18 06:37:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.002158871153369546 norm:5.887238512514159e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:37:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0021061324514448643 norm:4.324788096710108e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:38:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0020704788621515036 norm:2.9801709388266318e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:39:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.002053176984190941 norm:2.3543301722384058e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:39:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.0020410779397934675 norm:1.9318766135256737e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:40:10 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.00203527370467782 norm:1.659391273278743e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:40:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.0020287828519940376 norm:1.4148617992759682e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:41:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0020248303189873695 norm:1.1922137673536781e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:41:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0020222365856170654 norm:1.0255861525365617e-05 max memory_allocated 22511.35546875 
[2025-02-18 06:42:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.002019867766648531 norm:8.931571755965706e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:42:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0020201080478727818 norm:8.208588951674756e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:43:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0020203811582177877 norm:7.445687970175641e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:43:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.002018716186285019 norm:6.748872237949399e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:44:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0020180530846118927 norm:6.2146536947693676e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:45:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.002016807906329632 norm:5.772824806626886e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:45:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.002017584163695574 norm:5.358447197068017e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:46:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0020186034962534904 norm:5.074571163277142e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:46:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0020172432996332645 norm:4.9058644435717724e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:47:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.0020170228090137243 norm:4.87097986479057e-06 max memory_allocated 22511.35546875 
[2025-02-18 06:47:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 06:48:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.0030137330759316683 norm:0.0004304737667553127 max memory_allocated 22511.52734375 
[2025-02-18 06:48:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0023249259684234858 norm:9.022741869557649e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:49:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0022245957516133785 norm:5.817521014250815e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:49:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0021941796876490116 norm:5.0055667088599876e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:50:12 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0021608220413327217 norm:3.671225567813963e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:50:45 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.0021400474943220615 norm:2.7376763682696037e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:51:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.0021313244942575693 norm:2.251960540888831e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:51:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0021273959428071976 norm:1.9926123059121892e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:52:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.002120352815836668 norm:1.7515121726319194e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:52:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0021173295099288225 norm:1.583737503096927e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:53:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0021160903852432966 norm:1.4745028238394298e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:54:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.002112773945555091 norm:1.362860348308459e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:54:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.002110893838107586 norm:1.2512587090895977e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:55:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0021094814874231815 norm:1.1539958904904779e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:55:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0021077946294099092 norm:1.078087279893225e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:56:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.002106546424329281 norm:1.0107622983923648e-05 max memory_allocated 22511.52734375 
[2025-02-18 06:56:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0021042600274086 norm:8.924923349695746e-06 max memory_allocated 22511.52734375 
[2025-02-18 06:57:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.0021021550055593252 norm:8.401862942264415e-06 max memory_allocated 22511.52734375 
[2025-02-18 06:57:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.002103168051689863 norm:7.815568096702918e-06 max memory_allocated 22511.52734375 
[2025-02-18 06:58:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.002103890525177121 norm:7.259993253683206e-06 max memory_allocated 22511.52734375 
[2025-02-18 06:58:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 06:59:09 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0027595949359238148 norm:0.0002051775372819975 max memory_allocated 22511.69921875 
[2025-02-18 06:59:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.0024288068525493145 norm:7.664646545890719e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:00:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0023663875181227922 norm:5.4803698731120676e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:00:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.002322644228115678 norm:3.979543180321343e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:01:20 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.002293006982654333 norm:2.9200489734648727e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:01:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.002280702581629157 norm:2.3766568119754083e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:02:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.0022714121732860804 norm:2.003819463425316e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:02:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0022621084935963154 norm:1.6842761397128925e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:03:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0022573573514819145 norm:1.4957103303459007e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:04:04 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0022524730302393436 norm:1.3374115951592103e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:04:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.002249297918751836 norm:1.1780543900385965e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:05:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0022467942908406258 norm:1.0576302884146571e-05 max memory_allocated 22511.69921875 
[2025-02-18 07:05:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0022456080187112093 norm:9.488425348536111e-06 max memory_allocated 22511.69921875 
[2025-02-18 07:06:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.0022453090641647577 norm:8.271866136055905e-06 max memory_allocated 22511.69921875 
[2025-02-18 07:06:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.002245876006782055 norm:7.718184861005284e-06 max memory_allocated 22511.69921875 
[2025-02-18 07:07:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.002244330942630768 norm:7.110297701728996e-06 max memory_allocated 22511.69921875 
[2025-02-18 07:07:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.00224484084174037 norm:6.740773642377462e-06 max memory_allocated 22511.69921875 
[2025-02-18 07:08:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.002245493931695819 norm:6.61210424368619e-06 max memory_allocated 22511.69921875 
[2025-02-18 07:08:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0022464925423264503 norm:6.371095878421329e-06 max memory_allocated 22511.69921875 
[2025-02-18 07:09:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.002245476935058832 norm:6.115472842793679e-06 max memory_allocated 22511.69921875 
[2025-02-18 07:09:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 07:10:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.0028438526205718517 norm:0.00016932636208366603 max memory_allocated 22511.87109375 
[2025-02-18 07:10:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0025654849596321583 norm:6.303040572674945e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:11:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.002511758590117097 norm:4.834458013647236e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:11:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.002470763400197029 norm:3.346866651554592e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:12:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.0024502864107489586 norm:2.6223011445836164e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:13:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.002437219023704529 norm:2.1480802388396114e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:13:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.002425841288641095 norm:1.8003813238465227e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:14:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.002419510390609503 norm:1.544329097669106e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:14:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.002414839109405875 norm:1.3374048648984171e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:15:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.002410170389339328 norm:1.1529738003446255e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:15:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0024092928506433964 norm:1.0128279427590314e-05 max memory_allocated 22511.87109375 
[2025-02-18 07:16:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0024076260160654783 norm:9.077405593416188e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:16:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0024048329796642065 norm:7.94356037658872e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:17:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.002403947990387678 norm:7.22687673260225e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:17:55 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.002402654616162181 norm:6.738418505847221e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:18:28 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.002402971498668194 norm:6.333482815534808e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:19:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.002402165438979864 norm:5.98144470131956e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:19:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0024013076908886433 norm:5.765720288763987e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:20:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.002401408739387989 norm:5.727945790567901e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:20:39 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.002402252983301878 norm:5.5841805988166016e-06 max memory_allocated 22511.87109375 
[2025-02-18 07:20:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 07:21:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.0032281833700835705 norm:0.0002600701409392059 max memory_allocated 22512.04296875 
[2025-02-18 07:21:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.0028420432936400175 norm:0.00010086241672979668 max memory_allocated 22512.04296875 
[2025-02-18 07:22:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.002768245991319418 norm:7.104301039362326e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:23:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.00271764793433249 norm:5.163567402632907e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:23:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.0026856015902012587 norm:3.890778316417709e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:24:08 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.0026676913257688284 norm:3.129603646812029e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:24:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.0026543820276856422 norm:2.5994635507231578e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:25:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.002646028995513916 norm:2.1698780983570032e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:25:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.002641472266986966 norm:1.9001228793058544e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:26:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.002635731128975749 norm:1.5878507838351652e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:26:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.002630974631756544 norm:1.388785403833026e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:27:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0026283084880560637 norm:1.2149167559982743e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:27:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.0026262961328029633 norm:1.0840984032256529e-05 max memory_allocated 22512.04296875 
[2025-02-18 07:28:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.00262314360588789 norm:9.689929356682114e-06 max memory_allocated 22512.04296875 
[2025-02-18 07:29:03 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0026210204232484102 norm:8.872535545378923e-06 max memory_allocated 22512.04296875 
[2025-02-18 07:29:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.002621167339384556 norm:8.187942512449808e-06 max memory_allocated 22512.04296875 
[2025-02-18 07:30:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0026201866567134857 norm:7.516784080507932e-06 max memory_allocated 22512.04296875 
[2025-02-18 07:30:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.002620602957904339 norm:6.974335519771557e-06 max memory_allocated 22512.04296875 
[2025-02-18 07:31:14 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0026202418375760317 norm:6.643916549364803e-06 max memory_allocated 22512.04296875 
[2025-02-18 07:31:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.0026203482411801815 norm:6.3514676185150165e-06 max memory_allocated 22512.04296875 
[2025-02-18 07:31:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 07:32:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.003236634423956275 norm:0.00014926485891919583 max memory_allocated 22512.21484375 
[2025-02-18 07:33:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.0030085393227636814 norm:6.008097989251837e-05 max memory_allocated 22512.21484375 
[2025-02-18 07:33:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0029556523077189922 norm:4.207367965136655e-05 max memory_allocated 22512.21484375 
[2025-02-18 07:34:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.002918516518548131 norm:2.9585729862446897e-05 max memory_allocated 22512.21484375 
[2025-02-18 07:34:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.00289784069173038 norm:2.2289337721304037e-05 max memory_allocated 22512.21484375 
[2025-02-18 07:35:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0028843311592936516 norm:1.7876200217870064e-05 max memory_allocated 22512.21484375 
[2025-02-18 07:35:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0028757816180586815 norm:1.490887370891869e-05 max memory_allocated 22512.21484375 
[2025-02-18 07:36:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.0028687049634754658 norm:1.273599627893418e-05 max memory_allocated 22512.21484375 
[2025-02-18 07:36:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0028645906131714582 norm:1.0849293175851926e-05 max memory_allocated 22512.21484375 
[2025-02-18 07:37:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.002861595479771495 norm:9.375869922223501e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:38:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.0028593523893505335 norm:8.408958819927648e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:38:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.002856364008039236 norm:7.819294296496082e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:39:05 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0028542012441903353 norm:7.401443781418493e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:39:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.0028535763267427683 norm:6.999264314799802e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:40:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.002853773534297943 norm:6.725701496179681e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:40:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.002853141166269779 norm:6.541713901242474e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:41:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.0028516012243926525 norm:6.495105481008068e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:41:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.002851638710126281 norm:6.366210527630756e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:42:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0028504501096904278 norm:6.372659299813677e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:42:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.002851498778909445 norm:6.303036116150906e-06 max memory_allocated 22512.21484375 
[2025-02-18 07:43:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 07:43:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.004533880390226841 norm:0.0005702802445739508 max memory_allocated 22512.38671875 
[2025-02-18 07:44:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.003595057874917984 norm:0.0001422856585122645 max memory_allocated 22512.38671875 
[2025-02-18 07:44:45 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0034433489199727774 norm:9.422584844287485e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:45:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.0033882800489664078 norm:8.055411308305338e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:45:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0033321790397167206 norm:6.098022640799172e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:46:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.0032854187302291393 norm:4.545702540781349e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:46:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.003263054881244898 norm:3.737018414540216e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:47:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.003247172338888049 norm:3.210834620404057e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:48:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.0032355657313019037 norm:2.8723890864057466e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:48:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.003224440384656191 norm:2.4994296836666763e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:49:08 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.0032159239053726196 norm:2.236214277218096e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:49:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.003210086142644286 norm:2.02749597519869e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:50:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0032054840121418238 norm:1.8101305613527074e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:50:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0032012364827096462 norm:1.664423143665772e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:51:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.003197816666215658 norm:1.4878023648634553e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:51:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.003194845747202635 norm:1.348822934232885e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:52:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.003192813601344824 norm:1.2413713193382137e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:52:57 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.003191475523635745 norm:1.1637635907391086e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:53:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0031900745816528797 norm:1.0403882697573863e-05 max memory_allocated 22512.38671875 
[2025-02-18 07:54:03 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.003188802395015955 norm:9.913911526382435e-06 max memory_allocated 22512.38671875 
[2025-02-18 07:54:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 07:54:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.005046105943620205 norm:0.0006135353469289839 max memory_allocated 22512.55859375 
[2025-02-18 07:55:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0041174995712935925 norm:0.00015107686340343207 max memory_allocated 22512.55859375 
[2025-02-18 07:55:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.0039534298703074455 norm:9.611176938051358e-05 max memory_allocated 22512.55859375 
[2025-02-18 07:56:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.00390073424205184 norm:8.577250991947949e-05 max memory_allocated 22512.55859375 
[2025-02-18 07:56:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.0038364713545888662 norm:6.584868970094249e-05 max memory_allocated 22512.55859375 
[2025-02-18 07:57:32 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.0037877061404287815 norm:4.8479218094144017e-05 max memory_allocated 22512.55859375 
[2025-02-18 07:58:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.0037636456545442343 norm:4.0144215745385736e-05 max memory_allocated 22512.55859375 
[2025-02-18 07:58:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0037442846223711967 norm:3.499062586342916e-05 max memory_allocated 22512.55859375 
[2025-02-18 07:59:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0037280318792909384 norm:3.0918723496142775e-05 max memory_allocated 22512.55859375 
[2025-02-18 07:59:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.003716514678671956 norm:2.7731230147765018e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:00:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.003709915792569518 norm:2.4809864044073038e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:00:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.003704370930790901 norm:2.2424781491281465e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:01:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.0036984900943934917 norm:2.041731022472959e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:01:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.003693445585668087 norm:1.8602288037072867e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:02:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.003690047189593315 norm:1.6733003576518968e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:03:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.003686850192025304 norm:1.5891973816906102e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:03:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.0036850229371339083 norm:1.457744110666681e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:04:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.0036834590137004852 norm:1.3407580809143838e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:04:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.0036808322183787823 norm:1.2938594409206416e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:05:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0036811078898608685 norm:1.2407391295710113e-05 max memory_allocated 22512.55859375 
[2025-02-18 08:05:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 08:05:56 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.005277610849589109 norm:0.00039956672117114067 max memory_allocated 22512.73046875 
[2025-02-18 08:06:29 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0046377032995224 norm:0.0001309117942582816 max memory_allocated 22512.73046875 
[2025-02-18 08:07:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.004517844878137112 norm:9.692453750176355e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:07:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.004448906984180212 norm:7.705033931415528e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:08:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.004383191000670195 norm:5.8214067394146696e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:08:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.004342478699982166 norm:4.638926111510955e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:09:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.004315798636525869 norm:3.8251590012805536e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:09:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.004299790132790804 norm:3.2578394893789664e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:10:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.004283766262233257 norm:2.8093205401091836e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:10:51 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.004269606899470091 norm:2.4499195205862634e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:11:24 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.004260966554284096 norm:2.1630021365126595e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:11:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.004255373030900955 norm:1.885415258584544e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:12:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.004249860066920519 norm:1.709434945951216e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:13:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.004245305433869362 norm:1.553772744955495e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:13:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.004243005998432636 norm:1.420111129846191e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:14:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.004239571280777454 norm:1.3168142686481588e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:14:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.0042388769797980785 norm:1.2244110621395521e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:15:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.004238260444253683 norm:1.1373916095180903e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:15:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.004238282795995474 norm:1.0855354048544541e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:16:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.004235050641000271 norm:1.0521096555748954e-05 max memory_allocated 22512.73046875 
[2025-02-18 08:16:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 08:17:04 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.0067514702677726746 norm:0.0008660612511448562 max memory_allocated 22512.90234375 
[2025-02-18 08:17:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.005523711442947388 norm:0.00022797012934461236 max memory_allocated 22512.90234375 
[2025-02-18 08:18:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.005260650534182787 norm:0.0001246102328877896 max memory_allocated 22512.90234375 
[2025-02-18 08:18:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.005183009430766106 norm:0.00010416140139568597 max memory_allocated 22512.90234375 
[2025-02-18 08:19:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.005140083376318216 norm:9.35348798520863e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:19:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.005084706004709005 norm:7.326916966121644e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:20:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.005039374344050884 norm:5.676694127032533e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:20:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0050113387405872345 norm:4.6667020797031e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:21:26 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.004993529990315437 norm:4.057393380207941e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:21:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.004979047924280167 norm:3.688236029120162e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:22:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.004966431763023138 norm:3.3359345252392814e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:23:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.00495606753975153 norm:3.0269273338490166e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:23:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.004949647001922131 norm:2.741653224802576e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:24:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.004943264648318291 norm:2.4919258066802286e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:24:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.00493637565523386 norm:2.2384167095879093e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:25:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.00493245804682374 norm:2.025602589128539e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:25:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.004930789582431316 norm:1.851605520641897e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:26:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.004928303882479668 norm:1.677215550444089e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:26:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.004924715030938387 norm:1.5072977475938387e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:27:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.004922482650727034 norm:1.3829925592290238e-05 max memory_allocated 22512.90234375 
[2025-02-18 08:27:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 08:28:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.009847667999565601 norm:0.002345879329368472 max memory_allocated 22513.07421875 
[2025-02-18 08:28:45 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.0070355720818042755 norm:0.0005624798941425979 max memory_allocated 22513.07421875 
[2025-02-18 08:29:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.006387573201209307 norm:0.00023699375742580742 max memory_allocated 22513.07421875 
[2025-02-18 08:29:51 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.006193333771079779 norm:0.00016099028289318085 max memory_allocated 22513.07421875 
[2025-02-18 08:30:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.006143569014966488 norm:0.00015182801871560514 max memory_allocated 22513.07421875 
[2025-02-18 08:30:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.00611486192792654 norm:0.00014540583651978523 max memory_allocated 22513.07421875 
[2025-02-18 08:31:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.006053582299500704 norm:0.00012057050480507314 max memory_allocated 22513.07421875 
[2025-02-18 08:32:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.005995988845825195 norm:9.617931209504604e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:32:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.005952607374638319 norm:7.833840936655179e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:33:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.005923511926084757 norm:6.590050179511309e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:33:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.005905884318053722 norm:5.763813169323839e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:34:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.005892267916351557 norm:5.2140669140499085e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:34:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.005883523728698492 norm:4.715825343737379e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:35:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.005872798152267933 norm:4.301439184928313e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:35:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.005863579455763102 norm:3.944836134905927e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:36:24 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.005857646465301514 norm:3.618951086536981e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:36:57 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.005856401287019253 norm:3.363904761499725e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:37:30 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.005850953981280327 norm:3.124224167549983e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:38:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.005846232175827026 norm:2.7730386136681773e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:38:35 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.005843921564519405 norm:2.5238938178517856e-05 max memory_allocated 22513.07421875 
[2025-02-18 08:38:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 08:39:20 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.009378914721310139 norm:0.0013343248283490539 max memory_allocated 22513.24609375 
[2025-02-18 08:39:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.0076553598046302795 norm:0.0003182673826813698 max memory_allocated 22513.24609375 
[2025-02-18 08:40:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.007302646990865469 norm:0.00015581191109959036 max memory_allocated 22513.24609375 
[2025-02-18 08:40:59 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.007213190663605928 norm:0.0001396406878484413 max memory_allocated 22513.24609375 
[2025-02-18 08:41:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.007204556837677956 norm:0.00014481823018286377 max memory_allocated 22513.24609375 
[2025-02-18 08:42:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.007131747901439667 norm:0.00011013491894118488 max memory_allocated 22513.24609375 
[2025-02-18 08:42:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.007070614490658045 norm:7.879188342485577e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:43:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.007036195136606693 norm:6.336256046779454e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:43:43 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.00702445674687624 norm:5.4198302677832544e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:44:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.0070169903337955475 norm:4.826105941901915e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:44:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.007007021456956863 norm:4.2075043893419206e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:45:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.006996412295848131 norm:3.6099379940424114e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:45:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.0069874366745352745 norm:3.1262534321285784e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:46:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.0069840396754443645 norm:2.723611396504566e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:47:00 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.006986029911786318 norm:2.404550468781963e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:47:32 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.006984746549278498 norm:2.2217271180124953e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:48:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.006986267864704132 norm:2.0204406609991565e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:48:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.006982515566051006 norm:1.9190347302355804e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:49:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.0069861141964793205 norm:1.7594968085177243e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:49:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.006979638710618019 norm:1.6804249753477052e-05 max memory_allocated 22513.24609375 
[2025-02-18 08:49:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 08:50:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.010228377766907215 norm:0.0010505580576136708 max memory_allocated 22513.41796875 
[2025-02-18 08:51:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.008800963871181011 norm:0.00027742108795791864 max memory_allocated 22513.41796875 
[2025-02-18 08:51:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.008471244014799595 norm:0.00014138180995360017 max memory_allocated 22513.41796875 
[2025-02-18 08:52:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.008378714323043823 norm:0.00011816279584309086 max memory_allocated 22513.41796875 
[2025-02-18 08:52:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.008336316794157028 norm:0.00011181730951648206 max memory_allocated 22513.41796875 
[2025-02-18 08:53:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.008286764845252037 norm:9.468634380027652e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:53:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.008235391229391098 norm:7.528047717642039e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:54:18 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.008199866861104965 norm:6.174969166750088e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:54:51 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.008172756060957909 norm:5.2936517022317275e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:55:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.008153614588081837 norm:4.6946715883677825e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:55:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.008137868717312813 norm:4.18848758272361e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:56:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.008123347535729408 norm:3.80157507606782e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:57:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.008113573305308819 norm:3.4361015423201025e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:57:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.008107081986963749 norm:3.1015781132737175e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:58:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.008098535239696503 norm:2.8401043891790323e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:58:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.00809234194457531 norm:2.6033801987068728e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:59:13 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.008090583607554436 norm:2.393951581325382e-05 max memory_allocated 22513.41796875 
[2025-02-18 08:59:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.008087236434221268 norm:2.172658969357144e-05 max memory_allocated 22513.41796875 
[2025-02-18 09:00:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.008086028508841991 norm:1.986957067856565e-05 max memory_allocated 22513.41796875 
[2025-02-18 09:00:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.008080379106104374 norm:1.8694154277909547e-05 max memory_allocated 22513.41796875 
[2025-02-18 09:01:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 09:01:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.011017807759344578 norm:0.0006736238719895482 max memory_allocated 22513.58984375 
[2025-02-18 09:02:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.010077050887048244 norm:0.0002350041613681242 max memory_allocated 22513.58984375 
[2025-02-18 09:02:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.009823751635849476 norm:0.00014771783025935292 max memory_allocated 22513.58984375 
[2025-02-18 09:03:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.00973306130617857 norm:0.00012330058962106705 max memory_allocated 22513.58984375 
[2025-02-18 09:03:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.009661237709224224 norm:0.00010033193393610418 max memory_allocated 22513.58984375 
[2025-02-18 09:04:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.009596463292837143 norm:7.793930126354098e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:04:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.009553302079439163 norm:6.254944310057908e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:05:26 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.009526997804641724 norm:5.123543814988807e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:05:59 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.009507639333605766 norm:4.368013105704449e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:06:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.009496593847870827 norm:3.7840844015590847e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:07:05 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.009482079185545444 norm:3.407646363484673e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:07:37 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.009470384567975998 norm:3.0498666092171334e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:08:10 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.009462984278798103 norm:2.753818807832431e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:08:43 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.00945245660841465 norm:2.474009306752123e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:09:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.009448075667023659 norm:2.2495185476145707e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:09:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.0094432532787323 norm:2.0194043827359565e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:10:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.009439477697014809 norm:1.8082058886648156e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:10:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.009440017864108086 norm:1.648585566726979e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:11:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.00943908654153347 norm:1.5364799764938653e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:12:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.009436743333935738 norm:1.4291680599853862e-05 max memory_allocated 22513.58984375 
[2025-02-18 09:12:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 09:12:45 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.012647075578570366 norm:0.0007471840362995863 max memory_allocated 22513.76171875 
[2025-02-18 09:13:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.01172721665352583 norm:0.0003222587110940367 max memory_allocated 22513.76171875 
[2025-02-18 09:13:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.011448852717876434 norm:0.00022643017291557044 max memory_allocated 22513.76171875 
[2025-02-18 09:14:23 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.011304939165711403 norm:0.00017715358990244567 max memory_allocated 22513.76171875 
[2025-02-18 09:14:56 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.011196646839380264 norm:0.00014147294859867543 max memory_allocated 22513.76171875 
[2025-02-18 09:15:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.011114578694105148 norm:0.00011398454080335796 max memory_allocated 22513.76171875 
[2025-02-18 09:16:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.011050974950194359 norm:9.415421664016321e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:16:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.0110113974660635 norm:7.929537241579965e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:17:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.01097741536796093 norm:6.791906344005838e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:17:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.01094913948327303 norm:5.926025187363848e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:18:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.010929261334240437 norm:5.165115726413205e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:18:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.010918395593762398 norm:4.5301330828806385e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:19:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.010906189680099487 norm:4.0131671994458884e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:19:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.010891866870224476 norm:3.6193945561535656e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:20:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.010885952971875668 norm:3.2370480766985565e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:20:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.010879668407142162 norm:2.939545811386779e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:21:29 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.010874644853174686 norm:2.656537435541395e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:22:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.010869950987398624 norm:2.4397571905865334e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:22:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.010865962132811546 norm:2.2354568500304595e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:23:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.010860523208975792 norm:2.0616500478354283e-05 max memory_allocated 22513.76171875 
[2025-02-18 09:23:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 09:23:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.013917539268732071 norm:0.0007299716817215085 max memory_allocated 22513.93359375 
[2025-02-18 09:24:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.012980167753994465 norm:0.00024329045845661312 max memory_allocated 22513.93359375 
[2025-02-18 09:24:59 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.012772560119628906 norm:0.00016062159556895494 max memory_allocated 22513.93359375 
[2025-02-18 09:25:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.012696010991930962 norm:0.00013532298908103257 max memory_allocated 22513.93359375 
[2025-02-18 09:26:05 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.012634177692234516 norm:0.00011039818491553888 max memory_allocated 22513.93359375 
[2025-02-18 09:26:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.012576356530189514 norm:8.553527732146904e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:27:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.012534687295556068 norm:6.880816363263875e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:27:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.012510900385677814 norm:5.646596764563583e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:28:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.012499034404754639 norm:4.688252374762669e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:28:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.012485074810683727 norm:4.004302900284529e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:29:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.012477442622184753 norm:3.358998947078362e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:29:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.012470842339098454 norm:2.915777440648526e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:30:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.012467899359762669 norm:2.45323681156151e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:31:00 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.01246635988354683 norm:2.1073299649287947e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:31:33 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.01246197335422039 norm:1.8821872799890116e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:32:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.01245876681059599 norm:1.675762905506417e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:32:38 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.012457483448088169 norm:1.5258376151905395e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:33:11 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.012455860152840614 norm:1.3827173461322673e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:33:44 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.012451914139091969 norm:1.3246526577859186e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:34:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.012450829148292542 norm:1.244892155227717e-05 max memory_allocated 22513.93359375 
[2025-02-18 09:34:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 09:35:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.015776386484503746 norm:0.0007371297688223422 max memory_allocated 22514.10546875 
[2025-02-18 09:35:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.014820552431046963 norm:0.00027810357278212905 max memory_allocated 22514.10546875 
[2025-02-18 09:36:07 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.014572917483747005 norm:0.0001858568866737187 max memory_allocated 22514.10546875 
[2025-02-18 09:36:40 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.014480497688055038 norm:0.00015128831728361547 max memory_allocated 22514.10546875 
[2025-02-18 09:37:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.014406624250113964 norm:0.000121464574476704 max memory_allocated 22514.10546875 
[2025-02-18 09:37:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.014333111234009266 norm:9.434382081963122e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:38:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.014284552074968815 norm:7.573399489047006e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:38:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.014253894798457623 norm:6.37618504697457e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:39:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.014234581030905247 norm:5.4066618758952245e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:39:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.014221125282347202 norm:4.656539385905489e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:40:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.014210201799869537 norm:4.046670801471919e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:41:03 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.014202425256371498 norm:3.540991747286171e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:41:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.014198848977684975 norm:3.064873453695327e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:42:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.014194706454873085 norm:2.6607329346006736e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:42:41 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.01418604701757431 norm:2.3306980438064784e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:43:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.01418494712561369 norm:2.0002617020509206e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:43:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.014185133390128613 norm:1.7166037650895305e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:44:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.014181519858539104 norm:1.5401781638502143e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:44:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.014180831611156464 norm:1.3501270586857572e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:45:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.014177775010466576 norm:1.254685685125878e-05 max memory_allocated 22514.10546875 
[2025-02-18 09:45:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 09:46:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.01816749759018421 norm:0.0009491201490163803 max memory_allocated 22514.27734375 
[2025-02-18 09:46:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.01685732789337635 norm:0.00032652384834364057 max memory_allocated 22514.27734375 
[2025-02-18 09:47:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.016576718538999557 norm:0.00023142961435951293 max memory_allocated 22514.27734375 
[2025-02-18 09:47:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.016453681513667107 norm:0.0001951313461177051 max memory_allocated 22514.27734375 
[2025-02-18 09:48:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.016340941190719604 norm:0.0001533527538413182 max memory_allocated 22514.27734375 
[2025-02-18 09:48:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.01624753698706627 norm:0.0001181381885544397 max memory_allocated 22514.27734375 
[2025-02-18 09:49:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.016188697889447212 norm:9.63452912401408e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:49:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.016160640865564346 norm:8.024559792829677e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:50:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.016137350350618362 norm:6.710492743877694e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:51:05 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.01611984893679619 norm:5.551823051064275e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:51:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.016107924282550812 norm:4.7052941226866096e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:52:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.016103073954582214 norm:4.0132210415322334e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:52:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.016098573803901672 norm:3.488213405944407e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:53:16 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.01609840989112854 norm:3.049344195460435e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:53:49 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.016091037541627884 norm:2.7020463676308282e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:54:22 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.0160836111754179 norm:2.3167174731497653e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:54:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.016078229993581772 norm:2.0399256754899397e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:55:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.016075067222118378 norm:1.833353417168837e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:56:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.016073046252131462 norm:1.699142376310192e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:56:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.016069576144218445 norm:1.6003243217710406e-05 max memory_allocated 22514.27734375 
[2025-02-18 09:56:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 09:57:18 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.020515907555818558 norm:0.0006735058268532157 max memory_allocated 22514.44921875 
[2025-02-18 09:57:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.019608287140727043 norm:0.00033005428849719465 max memory_allocated 22514.44921875 
[2025-02-18 09:58:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.019275715574622154 norm:0.00024306561681441963 max memory_allocated 22514.44921875 
[2025-02-18 09:58:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.019078446552157402 norm:0.00019237030937802047 max memory_allocated 22514.44921875 
[2025-02-18 09:59:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.01892523281276226 norm:0.00015163638454396278 max memory_allocated 22514.44921875 
[2025-02-18 10:00:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.01881694421172142 norm:0.00012450954818632454 max memory_allocated 22514.44921875 
[2025-02-18 10:00:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.018733816221356392 norm:0.00010538046626606956 max memory_allocated 22514.44921875 
[2025-02-18 10:01:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.01867387443780899 norm:9.054636757355183e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:01:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.018630700185894966 norm:7.86798627814278e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:02:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.018593408167362213 norm:6.802369898650795e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:02:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.018564904108643532 norm:5.8724461268866435e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:03:19 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.01854812726378441 norm:5.01536087540444e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:03:52 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.018536871299147606 norm:4.38460883742664e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:04:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.01853056065738201 norm:3.835242023342289e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:04:57 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.018514882773160934 norm:3.448722418397665e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:05:30 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.01850760541856289 norm:3.118444510619156e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:06:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.01850276067852974 norm:2.8474134524003603e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:06:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.018491866067051888 norm:2.6290059395250864e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:07:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.018484028056263924 norm:2.439908348605968e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:07:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.018477359786629677 norm:2.3014054022496566e-05 max memory_allocated 22514.44921875 
[2025-02-18 10:07:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 10:08:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.02321581356227398 norm:0.0007135404739528894 max memory_allocated 22514.62109375 
[2025-02-18 10:08:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.02231503836810589 norm:0.0003122896596323699 max memory_allocated 22514.62109375 
[2025-02-18 10:09:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.022037746384739876 norm:0.00022912748681847006 max memory_allocated 22514.62109375 
[2025-02-18 10:10:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.0218435637652874 norm:0.0001718738640192896 max memory_allocated 22514.62109375 
[2025-02-18 10:10:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.021706964820623398 norm:0.00013267918257042766 max memory_allocated 22514.62109375 
[2025-02-18 10:11:10 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.02161535993218422 norm:0.00010538323112996295 max memory_allocated 22514.62109375 
[2025-02-18 10:11:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.021552085876464844 norm:8.512940257787704e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:12:16 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.021508801728487015 norm:7.046002428978682e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:12:49 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.021481137722730637 norm:5.916126974625513e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:13:22 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.021467771381139755 norm:4.8147619963856414e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:13:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.02145017497241497 norm:4.0605813410365954e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:14:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.02143307961523533 norm:3.4200485970359296e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:15:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.02141791768372059 norm:3.060212839045562e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:15:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.021410951390862465 norm:2.667767330422066e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:16:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.021397385746240616 norm:2.4735689294175245e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:16:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.0213901586830616 norm:2.2467533199233003e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:17:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.021388931199908257 norm:2.1572945115622133e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:17:44 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.021385518833994865 norm:1.9894083379767835e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:18:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.02138356864452362 norm:1.9337325284141116e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:18:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.021381309255957603 norm:1.892830550787039e-05 max memory_allocated 22514.62109375 
[2025-02-18 10:18:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 10:19:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.03171534463763237 norm:0.0005334724555723369 max memory_allocated 22514.79296875 
[2025-02-18 10:20:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.03029610589146614 norm:0.00030106326448731124 max memory_allocated 22514.79296875 
[2025-02-18 10:20:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.02953702211380005 norm:0.00021585177455563098 max memory_allocated 22514.79296875 
[2025-02-18 10:21:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.02903902903199196 norm:0.00015822587010916322 max memory_allocated 22514.79296875 
[2025-02-18 10:21:46 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.028715133666992188 norm:0.00012782670091837645 max memory_allocated 22514.79296875 
[2025-02-18 10:22:19 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.028511036187410355 norm:0.00010753089736681432 max memory_allocated 22514.79296875 
[2025-02-18 10:22:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.028358664363622665 norm:9.303777187597007e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:23:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.028251200914382935 norm:8.041899127420038e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:23:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.028162745758891106 norm:7.132186874514446e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:24:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.028106030076742172 norm:6.621232023462653e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:25:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.02807023376226425 norm:6.3204592152033e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:25:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.028031906113028526 norm:6.226970435818657e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:26:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.028006311506032944 norm:6.394207593984902e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:26:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.027991723269224167 norm:6.57455311738886e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:27:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.02798893302679062 norm:6.845236930530518e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:27:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.02797815576195717 norm:6.902957102283835e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:28:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.02796083502471447 norm:7.090179860824719e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:28:52 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.027944402769207954 norm:7.051333523122594e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:29:25 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.027927685528993607 norm:7.079635543050244e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:29:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.027931278571486473 norm:7.430074765579775e-05 max memory_allocated 22514.79296875 
[2025-02-18 10:30:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 10:30:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.06107329949736595 norm:0.0028182442765682936 max memory_allocated 22514.96484375 
[2025-02-18 10:31:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.05737770348787308 norm:0.0017902757972478867 max memory_allocated 22514.96484375 
[2025-02-18 10:31:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.0550621822476387 norm:0.001208946923725307 max memory_allocated 22514.96484375 
[2025-02-18 10:32:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.053727008402347565 norm:0.0008047226001508534 max memory_allocated 22514.96484375 
[2025-02-18 10:32:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.052651576697826385 norm:0.0005869559245184064 max memory_allocated 22514.96484375 
[2025-02-18 10:33:27 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.05191216990351677 norm:0.00043467278010211885 max memory_allocated 22514.96484375 
[2025-02-18 10:34:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.05132810026407242 norm:0.00035020915674977005 max memory_allocated 22514.96484375 
[2025-02-18 10:34:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.05080433934926987 norm:0.00029851170256733894 max memory_allocated 22514.96484375 
[2025-02-18 10:35:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.0505196712911129 norm:0.00026505248388275504 max memory_allocated 22514.96484375 
[2025-02-18 10:35:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.050261009484529495 norm:0.00022994389291852713 max memory_allocated 22514.96484375 
[2025-02-18 10:36:11 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.050102658569812775 norm:0.00022722184075973928 max memory_allocated 22514.96484375 
[2025-02-18 10:36:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.04991721361875534 norm:0.00023709566448815167 max memory_allocated 22514.96484375 
[2025-02-18 10:37:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.04981613904237747 norm:0.0002213320549344644 max memory_allocated 22514.96484375 
[2025-02-18 10:37:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.04968626797199249 norm:0.00021566799841821194 max memory_allocated 22514.96484375 
[2025-02-18 10:38:22 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.049604035913944244 norm:0.00021241162903606892 max memory_allocated 22514.96484375 
[2025-02-18 10:38:55 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.04951141029596329 norm:0.00021247129188850522 max memory_allocated 22514.96484375 
[2025-02-18 10:39:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.04946243390440941 norm:0.00020684543414972723 max memory_allocated 22514.96484375 
[2025-02-18 10:40:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.04941708594560623 norm:0.00020823837257921696 max memory_allocated 22514.96484375 
[2025-02-18 10:40:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.04940788447856903 norm:0.00020379223860800266 max memory_allocated 22514.96484375 
[2025-02-18 10:41:06 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.04942881688475609 norm:0.0002297642786288634 max memory_allocated 22514.96484375 
[2025-02-18 10:41:16 root] (main_calibration.py 365): INFO 21377.90486598015
[2025-02-18 10:41:52 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-18 10:43:02 root] (main_calibration.py 158): INFO wikitext2 : 5.689600944519043
[2025-02-18 10:43:02 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-18 10:44:51 root] (main_calibration.py 158): INFO c4 : 7.096212387084961
[2025-02-18 12:24:58 root] (main_calibration.py 169): INFO {'wikitext2': 5.689600944519043, 'c4': 7.096212387084961, 'results': {'boolq': {'acc': 0.7336391437308869, 'acc_stderr': 0.007731593077316942}, 'hellaswag': {'acc': 0.5642302330213105, 'acc_stderr': 0.004948439229523918, 'acc_norm': 0.7298346942840072, 'acc_norm_stderr': 0.004431375549911364}, 'winogrande': {'acc': 0.6708760852407262, 'acc_stderr': 0.013206387089091453}, 'arc_challenge': {'acc': 0.386518771331058, 'acc_stderr': 0.014230084761910483, 'acc_norm': 0.41552901023890787, 'acc_norm_stderr': 0.014401366641216391}, 'piqa': {'acc': 0.7823721436343852, 'acc_stderr': 0.009627407474840878, 'acc_norm': 0.7725788900979326, 'acc_norm_stderr': 0.009779850767847228}, 'arc_easy': {'acc': 0.6721380471380471, 'acc_stderr': 0.009632587076170018, 'acc_norm': 0.5218855218855218, 'acc_norm_stderr': 0.010249950427234157}}, 'versions': {'boolq': 1, 'hellaswag': 0, 'winogrande': 0, 'arc_challenge': 0, 'piqa': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
