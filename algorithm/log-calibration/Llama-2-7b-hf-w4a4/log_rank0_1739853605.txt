[2025-02-18 04:40:05 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration/Llama-2-7b-hf-w4a4', save_dir='./log-calibration/quant/Llama-2-7b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-18 04:44:20 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 04:44:20 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-18 04:44:20 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 04:44:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 04:44:58 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.07907401770353317 norm:0.019394459202885628 max memory_allocated 22512.63671875 
[2025-02-18 04:45:30 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0592145174741745 norm:0.011852052062749863 max memory_allocated 22512.63671875 
[2025-02-18 04:46:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.08541569858789444 norm:0.05083524435758591 max memory_allocated 22512.63671875 
[2025-02-18 04:46:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.09215740114450455 norm:0.047842394560575485 max memory_allocated 22512.63671875 
[2025-02-18 04:47:07 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.08683403581380844 norm:0.05764772370457649 max memory_allocated 22512.63671875 
[2025-02-18 04:47:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.08803042024374008 norm:0.04678982496261597 max memory_allocated 22512.63671875 
[2025-02-18 04:48:12 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.09104445576667786 norm:0.04690159857273102 max memory_allocated 22512.63671875 
[2025-02-18 04:48:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.0931982770562172 norm:0.04579968377947807 max memory_allocated 22512.63671875 
[2025-02-18 04:49:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.09330710023641586 norm:0.05391652137041092 max memory_allocated 22512.63671875 
[2025-02-18 04:49:50 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.09471365809440613 norm:0.11397449672222137 max memory_allocated 22512.63671875 
[2025-02-18 04:50:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.09481363743543625 norm:0.0768081396818161 max memory_allocated 22512.63671875 
[2025-02-18 04:50:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.09123492985963821 norm:0.057293761521577835 max memory_allocated 22512.63671875 
[2025-02-18 04:51:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.09066714346408844 norm:0.07249704003334045 max memory_allocated 22512.63671875 
[2025-02-18 04:52:00 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.08833524584770203 norm:0.05640473961830139 max memory_allocated 22512.63671875 
[2025-02-18 04:52:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.08961591869592667 norm:0.08611255884170532 max memory_allocated 22512.63671875 
[2025-02-18 04:53:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0835815817117691 norm:0.04791488125920296 max memory_allocated 22512.63671875 
[2025-02-18 04:53:38 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.07836921513080597 norm:0.03929898887872696 max memory_allocated 22512.63671875 
[2025-02-18 04:54:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.07689179480075836 norm:0.03215348348021507 max memory_allocated 22512.63671875 
[2025-02-18 04:54:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.0771695151925087 norm:0.05735723674297333 max memory_allocated 22512.63671875 
[2025-02-18 04:55:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.07589901238679886 norm:0.043786704540252686 max memory_allocated 22512.63671875 
[2025-02-18 04:55:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 04:56:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.6179640889167786 norm:0.08192633837461472 max memory_allocated 22512.80859375 
[2025-02-18 04:56:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.5101861357688904 norm:0.05141571909189224 max memory_allocated 22512.80859375 
[2025-02-18 04:57:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.4676000475883484 norm:0.03800158575177193 max memory_allocated 22512.80859375 
[2025-02-18 04:57:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.43285706639289856 norm:0.0318581685423851 max memory_allocated 22512.80859375 
[2025-02-18 04:58:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.4220803380012512 norm:0.03136438876390457 max memory_allocated 22512.80859375 
[2025-02-18 04:58:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.42061516642570496 norm:0.03125419095158577 max memory_allocated 22512.80859375 
[2025-02-18 04:59:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.41699671745300293 norm:0.030119096860289574 max memory_allocated 22512.80859375 
[2025-02-18 04:59:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.41994717717170715 norm:0.03792649880051613 max memory_allocated 22512.80859375 
[2025-02-18 05:00:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.4139921963214874 norm:0.03791900724172592 max memory_allocated 22512.80859375 
[2025-02-18 05:00:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.42458367347717285 norm:0.04049599543213844 max memory_allocated 22512.80859375 
[2025-02-18 05:01:28 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.4211353659629822 norm:0.03174079582095146 max memory_allocated 22512.80859375 
[2025-02-18 05:02:01 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.42060211300849915 norm:0.03279506415128708 max memory_allocated 22512.80859375 
[2025-02-18 05:02:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.4170672595500946 norm:0.030634259805083275 max memory_allocated 22512.80859375 
[2025-02-18 05:03:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.41263049840927124 norm:0.032149892300367355 max memory_allocated 22512.80859375 
[2025-02-18 05:03:39 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.3935791850090027 norm:0.031946226954460144 max memory_allocated 22512.80859375 
[2025-02-18 05:04:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.40658023953437805 norm:0.03512110933661461 max memory_allocated 22512.80859375 
[2025-02-18 05:04:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.4141058325767517 norm:0.03246467188000679 max memory_allocated 22512.80859375 
[2025-02-18 05:05:17 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.41311880946159363 norm:0.03358803689479828 max memory_allocated 22512.80859375 
[2025-02-18 05:05:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.3980947434902191 norm:0.04534398391842842 max memory_allocated 22512.80859375 
[2025-02-18 05:06:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.4161567687988281 norm:0.036696597933769226 max memory_allocated 22512.80859375 
[2025-02-18 05:06:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 05:07:08 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.46803125739097595 norm:0.06489193439483643 max memory_allocated 22512.98046875 
[2025-02-18 05:07:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.4442886710166931 norm:0.027885450050234795 max memory_allocated 22512.98046875 
[2025-02-18 05:08:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.4360957145690918 norm:0.01723870262503624 max memory_allocated 22512.98046875 
[2025-02-18 05:08:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.4326472282409668 norm:0.012095066718757153 max memory_allocated 22512.98046875 
[2025-02-18 05:09:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.4278157949447632 norm:0.00915534421801567 max memory_allocated 22512.98046875 
[2025-02-18 05:09:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.4236111342906952 norm:0.006853396072983742 max memory_allocated 22512.98046875 
[2025-02-18 05:10:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.4202406704425812 norm:0.005661279894411564 max memory_allocated 22512.98046875 
[2025-02-18 05:10:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.4189717471599579 norm:0.0051688156090676785 max memory_allocated 22512.98046875 
[2025-02-18 05:11:31 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.4196772575378418 norm:0.004911181051284075 max memory_allocated 22512.98046875 
[2025-02-18 05:12:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.4176523685455322 norm:0.0040810187347233295 max memory_allocated 22512.98046875 
[2025-02-18 05:12:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.4173896610736847 norm:0.0038106567226350307 max memory_allocated 22512.98046875 
[2025-02-18 05:13:09 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.4167547821998596 norm:0.003528138855472207 max memory_allocated 22512.98046875 
[2025-02-18 05:13:42 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.4164304733276367 norm:0.0034535115119069815 max memory_allocated 22512.98046875 
[2025-02-18 05:14:15 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.41629594564437866 norm:0.00332810590043664 max memory_allocated 22512.98046875 
[2025-02-18 05:14:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.41573911905288696 norm:0.003269264940172434 max memory_allocated 22512.98046875 
[2025-02-18 05:15:21 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.4169192910194397 norm:0.0032647924963384867 max memory_allocated 22512.98046875 
[2025-02-18 05:15:54 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.416914701461792 norm:0.0031966750975698233 max memory_allocated 22512.98046875 
[2025-02-18 05:16:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.41597357392311096 norm:0.0031527094542980194 max memory_allocated 22512.98046875 
[2025-02-18 05:16:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.4159119129180908 norm:0.0030832430347800255 max memory_allocated 22512.98046875 
[2025-02-18 05:17:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.41572073101997375 norm:0.003112693317234516 max memory_allocated 22512.98046875 
[2025-02-18 05:17:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 05:18:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.5007356405258179 norm:0.02120814472436905 max memory_allocated 22513.15234375 
[2025-02-18 05:18:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.47594836354255676 norm:0.007507523521780968 max memory_allocated 22513.15234375 
[2025-02-18 05:19:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.4699080288410187 norm:0.004428427666425705 max memory_allocated 22513.15234375 
[2025-02-18 05:19:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.46685510873794556 norm:0.0031566605903208256 max memory_allocated 22513.15234375 
[2025-02-18 05:20:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.46564099192619324 norm:0.0026444841641932726 max memory_allocated 22513.15234375 
[2025-02-18 05:21:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.46383136510849 norm:0.0024103764444589615 max memory_allocated 22513.15234375 
[2025-02-18 05:21:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.46286076307296753 norm:0.002204020507633686 max memory_allocated 22513.15234375 
[2025-02-18 05:22:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.4625316262245178 norm:0.0020612534135580063 max memory_allocated 22513.15234375 
[2025-02-18 05:22:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.46284306049346924 norm:0.0020379330962896347 max memory_allocated 22513.15234375 
[2025-02-18 05:23:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.46221446990966797 norm:0.001929932739585638 max memory_allocated 22513.15234375 
[2025-02-18 05:23:46 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.46183106303215027 norm:0.0019741528667509556 max memory_allocated 22513.15234375 
[2025-02-18 05:24:19 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.4609571397304535 norm:0.0018995300633832812 max memory_allocated 22513.15234375 
[2025-02-18 05:24:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.4604254364967346 norm:0.001851228647865355 max memory_allocated 22513.15234375 
[2025-02-18 05:25:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.46024471521377563 norm:0.001862824778072536 max memory_allocated 22513.15234375 
[2025-02-18 05:25:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.4602305293083191 norm:0.001837075804360211 max memory_allocated 22513.15234375 
[2025-02-18 05:26:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.45993179082870483 norm:0.0018425974994897842 max memory_allocated 22513.15234375 
[2025-02-18 05:27:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.4594919681549072 norm:0.0017944079590961337 max memory_allocated 22513.15234375 
[2025-02-18 05:27:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.45947661995887756 norm:0.0018321636598557234 max memory_allocated 22513.15234375 
[2025-02-18 05:28:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.4595157206058502 norm:0.0017944095889106393 max memory_allocated 22513.15234375 
[2025-02-18 05:28:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.45930901169776917 norm:0.0018238857155665755 max memory_allocated 22513.15234375 
[2025-02-18 05:28:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 05:29:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.5781828165054321 norm:0.048836976289749146 max memory_allocated 22513.32421875 
[2025-02-18 05:30:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.5495816469192505 norm:0.020993361249566078 max memory_allocated 22513.32421875 
[2025-02-18 05:30:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.5364199876785278 norm:0.011757398955523968 max memory_allocated 22513.32421875 
[2025-02-18 05:31:05 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.527667760848999 norm:0.007320955861359835 max memory_allocated 22513.32421875 
[2025-02-18 05:31:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.5218467116355896 norm:0.005176595877856016 max memory_allocated 22513.32421875 
[2025-02-18 05:32:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.5187149047851562 norm:0.003997798077762127 max memory_allocated 22513.32421875 
[2025-02-18 05:32:44 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.5162333250045776 norm:0.0032788782846182585 max memory_allocated 22513.32421875 
[2025-02-18 05:33:17 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.5145522952079773 norm:0.002754256362095475 max memory_allocated 22513.32421875 
[2025-02-18 05:33:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.5136566758155823 norm:0.0025182771496474743 max memory_allocated 22513.32421875 
[2025-02-18 05:34:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.5135109424591064 norm:0.0022977986373007298 max memory_allocated 22513.32421875 
[2025-02-18 05:34:55 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.5134326219558716 norm:0.0021585668437182903 max memory_allocated 22513.32421875 
[2025-02-18 05:35:28 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.5135548710823059 norm:0.0020692383404821157 max memory_allocated 22513.32421875 
[2025-02-18 05:36:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.513349175453186 norm:0.0020039533264935017 max memory_allocated 22513.32421875 
[2025-02-18 05:36:34 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.5125640034675598 norm:0.0019347363850101829 max memory_allocated 22513.32421875 
[2025-02-18 05:37:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.5121845006942749 norm:0.0019144758116453886 max memory_allocated 22513.32421875 
[2025-02-18 05:37:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.5120325088500977 norm:0.0018989612581208348 max memory_allocated 22513.32421875 
[2025-02-18 05:38:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.5117867588996887 norm:0.001892313128337264 max memory_allocated 22513.32421875 
[2025-02-18 05:38:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.5117817521095276 norm:0.0019083411898463964 max memory_allocated 22513.32421875 
[2025-02-18 05:39:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.5118133425712585 norm:0.0018578980816528201 max memory_allocated 22513.32421875 
[2025-02-18 05:39:50 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.5117421746253967 norm:0.001879162504337728 max memory_allocated 22513.32421875 
[2025-02-18 05:40:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 05:40:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.6112782955169678 norm:0.018719669431447983 max memory_allocated 22513.49609375 
[2025-02-18 05:41:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.5932450294494629 norm:0.010096105746924877 max memory_allocated 22513.49609375 
[2025-02-18 05:41:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.5835951566696167 norm:0.005068235099315643 max memory_allocated 22513.49609375 
[2025-02-18 05:42:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.5772517919540405 norm:0.0030452862847596407 max memory_allocated 22513.49609375 
[2025-02-18 05:42:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.5731068849563599 norm:0.00231159501709044 max memory_allocated 22513.49609375 
[2025-02-18 05:43:19 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.571153461933136 norm:0.002094923984259367 max memory_allocated 22513.49609375 
[2025-02-18 05:43:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.5696981549263 norm:0.0020260990131646395 max memory_allocated 22513.49609375 
[2025-02-18 05:44:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.5670633912086487 norm:0.001949972938746214 max memory_allocated 22513.49609375 
[2025-02-18 05:44:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.5671452879905701 norm:0.0019696038216352463 max memory_allocated 22513.49609375 
[2025-02-18 05:45:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.568455696105957 norm:0.002008326817303896 max memory_allocated 22513.49609375 
[2025-02-18 05:46:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.568863034248352 norm:0.0019991283770650625 max memory_allocated 22513.49609375 
[2025-02-18 05:46:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.5683373808860779 norm:0.0020025745034217834 max memory_allocated 22513.49609375 
[2025-02-18 05:47:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.5688193440437317 norm:0.001991014461964369 max memory_allocated 22513.49609375 
[2025-02-18 05:47:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.5677236318588257 norm:0.0019509532721713185 max memory_allocated 22513.49609375 
[2025-02-18 05:48:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.5668045878410339 norm:0.0019182090181857347 max memory_allocated 22513.49609375 
[2025-02-18 05:48:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.5663883686065674 norm:0.001901304698549211 max memory_allocated 22513.49609375 
[2025-02-18 05:49:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.5653119683265686 norm:0.0018761202227324247 max memory_allocated 22513.49609375 
[2025-02-18 05:49:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.5648393630981445 norm:0.0018705534748733044 max memory_allocated 22513.49609375 
[2025-02-18 05:50:26 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.5646066665649414 norm:0.0018786049913614988 max memory_allocated 22513.49609375 
[2025-02-18 05:50:59 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.5645482540130615 norm:0.0018800251418724656 max memory_allocated 22513.49609375 
[2025-02-18 05:51:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 05:51:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.7228798270225525 norm:0.04928365349769592 max memory_allocated 22513.66796875 
[2025-02-18 05:52:17 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.6787198185920715 norm:0.021424759179353714 max memory_allocated 22513.66796875 
[2025-02-18 05:52:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.6633938550949097 norm:0.01290001068264246 max memory_allocated 22513.66796875 
[2025-02-18 05:53:22 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.652772068977356 norm:0.009101834148168564 max memory_allocated 22513.66796875 
[2025-02-18 05:53:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.6466258764266968 norm:0.006846493575721979 max memory_allocated 22513.66796875 
[2025-02-18 05:54:28 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.6436569690704346 norm:0.005321328993886709 max memory_allocated 22513.66796875 
[2025-02-18 05:55:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.6427918672561646 norm:0.004762861877679825 max memory_allocated 22513.66796875 
[2025-02-18 05:55:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.641681969165802 norm:0.00452559906989336 max memory_allocated 22513.66796875 
[2025-02-18 05:56:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.6404386758804321 norm:0.004193423315882683 max memory_allocated 22513.66796875 
[2025-02-18 05:56:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.6405966877937317 norm:0.004108968656510115 max memory_allocated 22513.66796875 
[2025-02-18 05:57:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.6410712599754333 norm:0.004088288638740778 max memory_allocated 22513.66796875 
[2025-02-18 05:57:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.6418206691741943 norm:0.004063502885401249 max memory_allocated 22513.66796875 
[2025-02-18 05:58:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.64063960313797 norm:0.004016476217657328 max memory_allocated 22513.66796875 
[2025-02-18 05:58:51 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.641162633895874 norm:0.004088427405804396 max memory_allocated 22513.66796875 
[2025-02-18 05:59:23 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.6411024332046509 norm:0.003970624879002571 max memory_allocated 22513.66796875 
[2025-02-18 05:59:56 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.6420694589614868 norm:0.00396072119474411 max memory_allocated 22513.66796875 
[2025-02-18 06:00:29 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.6409497857093811 norm:0.003956025931984186 max memory_allocated 22513.66796875 
[2025-02-18 06:01:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.6405919790267944 norm:0.003917892463505268 max memory_allocated 22513.66796875 
[2025-02-18 06:01:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.6406271457672119 norm:0.003961940761655569 max memory_allocated 22513.66796875 
[2025-02-18 06:02:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.6400960683822632 norm:0.0038945465348660946 max memory_allocated 22513.66796875 
[2025-02-18 06:02:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 06:02:53 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.8035650849342346 norm:0.05929136276245117 max memory_allocated 22513.83984375 
[2025-02-18 06:03:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.7337258458137512 norm:0.02277117781341076 max memory_allocated 22513.83984375 
[2025-02-18 06:03:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.7128771543502808 norm:0.013098502531647682 max memory_allocated 22513.83984375 
[2025-02-18 06:04:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.7021443247795105 norm:0.008676916360855103 max memory_allocated 22513.83984375 
[2025-02-18 06:05:04 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.6971105933189392 norm:0.006616908125579357 max memory_allocated 22513.83984375 
[2025-02-18 06:05:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.6944343447685242 norm:0.005576954688876867 max memory_allocated 22513.83984375 
[2025-02-18 06:06:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.6882480382919312 norm:0.004541938193142414 max memory_allocated 22513.83984375 
[2025-02-18 06:06:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.6835002303123474 norm:0.0040799956768751144 max memory_allocated 22513.83984375 
[2025-02-18 06:07:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.6818740367889404 norm:0.0038139845710247755 max memory_allocated 22513.83984375 
[2025-02-18 06:07:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.6822345852851868 norm:0.003790382295846939 max memory_allocated 22513.83984375 
[2025-02-18 06:08:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.6821811199188232 norm:0.0037329941987991333 max memory_allocated 22513.83984375 
[2025-02-18 06:08:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.6817048788070679 norm:0.0036833540070801973 max memory_allocated 22513.83984375 
[2025-02-18 06:09:27 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.6813869476318359 norm:0.003677722066640854 max memory_allocated 22513.83984375 
[2025-02-18 06:10:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.6810245513916016 norm:0.0037477174773812294 max memory_allocated 22513.83984375 
[2025-02-18 06:10:33 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.6798611879348755 norm:0.0037602868396788836 max memory_allocated 22513.83984375 
[2025-02-18 06:11:06 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.6795964241027832 norm:0.00371067994274199 max memory_allocated 22513.83984375 
[2025-02-18 06:11:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.6792992949485779 norm:0.0036872851196676493 max memory_allocated 22513.83984375 
[2025-02-18 06:12:11 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.6793325543403625 norm:0.0036642143968492746 max memory_allocated 22513.83984375 
[2025-02-18 06:12:44 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.6793808937072754 norm:0.0037006144411861897 max memory_allocated 22513.83984375 
[2025-02-18 06:13:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.6792629361152649 norm:0.003683418035507202 max memory_allocated 22513.83984375 
[2025-02-18 06:13:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 06:14:02 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.7916298508644104 norm:0.03129294887185097 max memory_allocated 22514.01171875 
[2025-02-18 06:14:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.7573242783546448 norm:0.014001606032252312 max memory_allocated 22514.01171875 
[2025-02-18 06:15:08 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.744138777256012 norm:0.008680140599608421 max memory_allocated 22514.01171875 
[2025-02-18 06:15:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.7398051619529724 norm:0.006504354067146778 max memory_allocated 22514.01171875 
[2025-02-18 06:16:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.7376497387886047 norm:0.0052948701195418835 max memory_allocated 22514.01171875 
[2025-02-18 06:16:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.7343713045120239 norm:0.0043992334976792336 max memory_allocated 22514.01171875 
[2025-02-18 06:17:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.7319691777229309 norm:0.0038949837908148766 max memory_allocated 22514.01171875 
[2025-02-18 06:17:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.7321045398712158 norm:0.003565119579434395 max memory_allocated 22514.01171875 
[2025-02-18 06:18:25 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.7332688570022583 norm:0.0035296976566314697 max memory_allocated 22514.01171875 
[2025-02-18 06:18:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.730331301689148 norm:0.0033123281318694353 max memory_allocated 22514.01171875 
[2025-02-18 06:19:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.7293278574943542 norm:0.00322932843118906 max memory_allocated 22514.01171875 
[2025-02-18 06:20:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.7287591695785522 norm:0.003181817475706339 max memory_allocated 22514.01171875 
[2025-02-18 06:20:36 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.7276234030723572 norm:0.003088215133175254 max memory_allocated 22514.01171875 
[2025-02-18 06:21:09 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.7264524698257446 norm:0.0029852697625756264 max memory_allocated 22514.01171875 
[2025-02-18 06:21:42 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.7254135012626648 norm:0.0029239566065371037 max memory_allocated 22514.01171875 
[2025-02-18 06:22:15 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.7247602939605713 norm:0.0029043916147202253 max memory_allocated 22514.01171875 
[2025-02-18 06:22:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.7242123484611511 norm:0.002891490701586008 max memory_allocated 22514.01171875 
[2025-02-18 06:23:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.7242898941040039 norm:0.0029123136773705482 max memory_allocated 22514.01171875 
[2025-02-18 06:23:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.723548948764801 norm:0.0028711995109915733 max memory_allocated 22514.01171875 
[2025-02-18 06:24:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.723155677318573 norm:0.0028938327450305223 max memory_allocated 22514.01171875 
[2025-02-18 06:24:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 06:25:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.8586158752441406 norm:0.0489211305975914 max memory_allocated 22514.18359375 
[2025-02-18 06:25:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.8145762085914612 norm:0.026125691831111908 max memory_allocated 22514.18359375 
[2025-02-18 06:26:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.7881723642349243 norm:0.01391382422298193 max memory_allocated 22514.18359375 
[2025-02-18 06:26:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.7754616737365723 norm:0.008810258470475674 max memory_allocated 22514.18359375 
[2025-02-18 06:27:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.7679312229156494 norm:0.006305532064288855 max memory_allocated 22514.18359375 
[2025-02-18 06:27:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.7655144333839417 norm:0.005285901948809624 max memory_allocated 22514.18359375 
[2025-02-18 06:28:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.7625799179077148 norm:0.004435739014297724 max memory_allocated 22514.18359375 
[2025-02-18 06:29:01 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.7585617899894714 norm:0.0037324579898267984 max memory_allocated 22514.18359375 
[2025-02-18 06:29:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.7555323839187622 norm:0.003385811112821102 max memory_allocated 22514.18359375 
[2025-02-18 06:30:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.7536420226097107 norm:0.003097422420978546 max memory_allocated 22514.18359375 
[2025-02-18 06:30:40 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.7505062222480774 norm:0.002850398188456893 max memory_allocated 22514.18359375 
[2025-02-18 06:31:13 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.7495934367179871 norm:0.0027734581381082535 max memory_allocated 22514.18359375 
[2025-02-18 06:31:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.7507749199867249 norm:0.0027002531569451094 max memory_allocated 22514.18359375 
[2025-02-18 06:32:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.7504726648330688 norm:0.0025762575678527355 max memory_allocated 22514.18359375 
[2025-02-18 06:32:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.7514429688453674 norm:0.002552732592448592 max memory_allocated 22514.18359375 
[2025-02-18 06:33:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.7508077025413513 norm:0.0025248550809919834 max memory_allocated 22514.18359375 
[2025-02-18 06:33:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.7492694854736328 norm:0.0024243711959570646 max memory_allocated 22514.18359375 
[2025-02-18 06:34:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.7490774989128113 norm:0.0024153690319508314 max memory_allocated 22514.18359375 
[2025-02-18 06:35:02 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.7489778995513916 norm:0.0024009873159229755 max memory_allocated 22514.18359375 
[2025-02-18 06:35:35 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.7474742531776428 norm:0.00232961168512702 max memory_allocated 22514.18359375 
[2025-02-18 06:35:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 06:36:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.8413020372390747 norm:0.042898379266262054 max memory_allocated 22514.35546875 
[2025-02-18 06:36:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.8051375150680542 norm:0.0195566825568676 max memory_allocated 22514.35546875 
[2025-02-18 06:37:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.7915118336677551 norm:0.012028852477669716 max memory_allocated 22514.35546875 
[2025-02-18 06:37:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.7856897115707397 norm:0.009187471121549606 max memory_allocated 22514.35546875 
[2025-02-18 06:38:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.7811859250068665 norm:0.006952112540602684 max memory_allocated 22514.35546875 
[2025-02-18 06:39:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.7797757387161255 norm:0.005596029572188854 max memory_allocated 22514.35546875 
[2025-02-18 06:39:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.7761034369468689 norm:0.00423504738137126 max memory_allocated 22514.35546875 
[2025-02-18 06:40:11 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.772419273853302 norm:0.003591244574636221 max memory_allocated 22514.35546875 
[2025-02-18 06:40:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.7707435488700867 norm:0.0032952255569398403 max memory_allocated 22514.35546875 
[2025-02-18 06:41:17 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.7701425552368164 norm:0.003040086943656206 max memory_allocated 22514.35546875 
[2025-02-18 06:41:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.7692641019821167 norm:0.0028359172865748405 max memory_allocated 22514.35546875 
[2025-02-18 06:42:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.7689601182937622 norm:0.0026978403329849243 max memory_allocated 22514.35546875 
[2025-02-18 06:42:56 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.7658757567405701 norm:0.002518802648410201 max memory_allocated 22514.35546875 
[2025-02-18 06:43:28 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.762203574180603 norm:0.0022236532531678677 max memory_allocated 22514.35546875 
[2025-02-18 06:44:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.7614416480064392 norm:0.0021277181804180145 max memory_allocated 22514.35546875 
[2025-02-18 06:44:34 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.76031494140625 norm:0.0020076057408005 max memory_allocated 22514.35546875 
[2025-02-18 06:45:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.7585841417312622 norm:0.0018797003431245685 max memory_allocated 22514.35546875 
[2025-02-18 06:45:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.7576277256011963 norm:0.0017881117528304458 max memory_allocated 22514.35546875 
[2025-02-18 06:46:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.7573744654655457 norm:0.0017381168436259031 max memory_allocated 22514.35546875 
[2025-02-18 06:46:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.7580631375312805 norm:0.0017178949201479554 max memory_allocated 22514.35546875 
[2025-02-18 06:46:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 06:47:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.8564784526824951 norm:0.025199145078659058 max memory_allocated 22514.52734375 
[2025-02-18 06:48:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.8191880583763123 norm:0.011635962873697281 max memory_allocated 22514.52734375 
[2025-02-18 06:48:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.804420530796051 norm:0.007014026865363121 max memory_allocated 22514.52734375 
[2025-02-18 06:49:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.7998647689819336 norm:0.005014716647565365 max memory_allocated 22514.52734375 
[2025-02-18 06:49:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.7952807545661926 norm:0.004110425710678101 max memory_allocated 22514.52734375 
[2025-02-18 06:50:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.7938758134841919 norm:0.0036666272208094597 max memory_allocated 22514.52734375 
[2025-02-18 06:50:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.7898097038269043 norm:0.003167823888361454 max memory_allocated 22514.52734375 
[2025-02-18 06:51:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.7879769206047058 norm:0.0028772782534360886 max memory_allocated 22514.52734375 
[2025-02-18 06:51:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.7849364280700684 norm:0.0026585878804326057 max memory_allocated 22514.52734375 
[2025-02-18 06:52:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.7850649356842041 norm:0.0026518837548792362 max memory_allocated 22514.52734375 
[2025-02-18 06:53:01 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.7866003513336182 norm:0.0025766268372535706 max memory_allocated 22514.52734375 
[2025-02-18 06:53:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.7866327166557312 norm:0.002577804960310459 max memory_allocated 22514.52734375 
[2025-02-18 06:54:07 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.787575900554657 norm:0.00257886596955359 max memory_allocated 22514.52734375 
[2025-02-18 06:54:40 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.7878473997116089 norm:0.0025570066645741463 max memory_allocated 22514.52734375 
[2025-02-18 06:55:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.7889105081558228 norm:0.002542700618505478 max memory_allocated 22514.52734375 
[2025-02-18 06:55:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.7900248169898987 norm:0.0025548723060637712 max memory_allocated 22514.52734375 
[2025-02-18 06:56:18 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.7909775376319885 norm:0.0025712859351187944 max memory_allocated 22514.52734375 
[2025-02-18 06:56:51 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.7907745242118835 norm:0.0025619249790906906 max memory_allocated 22514.52734375 
[2025-02-18 06:57:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.7908629179000854 norm:0.0025980775244534016 max memory_allocated 22514.52734375 
[2025-02-18 06:57:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.7909505367279053 norm:0.002601807937026024 max memory_allocated 22514.52734375 
[2025-02-18 06:58:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 06:58:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.8566709756851196 norm:0.01690620556473732 max memory_allocated 22514.69921875 
[2025-02-18 06:59:15 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.8330806493759155 norm:0.009003161452710629 max memory_allocated 22514.69921875 
[2025-02-18 06:59:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.8226852416992188 norm:0.005620263982564211 max memory_allocated 22514.69921875 
[2025-02-18 07:00:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.8175820112228394 norm:0.003944677766412497 max memory_allocated 22514.69921875 
[2025-02-18 07:00:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.8140287399291992 norm:0.003077705856412649 max memory_allocated 22514.69921875 
[2025-02-18 07:01:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.8108083009719849 norm:0.0025437125004827976 max memory_allocated 22514.69921875 
[2025-02-18 07:02:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.8089931607246399 norm:0.0022866318468004465 max memory_allocated 22514.69921875 
[2025-02-18 07:02:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.8081204891204834 norm:0.002223777351900935 max memory_allocated 22514.69921875 
[2025-02-18 07:03:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.8063426613807678 norm:0.0020986611489206553 max memory_allocated 22514.69921875 
[2025-02-18 07:03:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.8067843914031982 norm:0.002050700131803751 max memory_allocated 22514.69921875 
[2025-02-18 07:04:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.8054838180541992 norm:0.001972723985090852 max memory_allocated 22514.69921875 
[2025-02-18 07:04:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.8045408129692078 norm:0.0019172029569745064 max memory_allocated 22514.69921875 
[2025-02-18 07:05:18 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.8045071959495544 norm:0.0018841472920030355 max memory_allocated 22514.69921875 
[2025-02-18 07:05:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.8043338656425476 norm:0.0018813024507835507 max memory_allocated 22514.69921875 
[2025-02-18 07:06:24 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.8042913675308228 norm:0.0018515483243390918 max memory_allocated 22514.69921875 
[2025-02-18 07:06:56 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.8052123785018921 norm:0.001891563879325986 max memory_allocated 22514.69921875 
[2025-02-18 07:07:29 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.8056075572967529 norm:0.0019012503325939178 max memory_allocated 22514.69921875 
[2025-02-18 07:08:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.8050618171691895 norm:0.0018675312167033553 max memory_allocated 22514.69921875 
[2025-02-18 07:08:35 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.8064913153648376 norm:0.0018848413601517677 max memory_allocated 22514.69921875 
[2025-02-18 07:09:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.8068374991416931 norm:0.0019252129131928086 max memory_allocated 22514.69921875 
[2025-02-18 07:09:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 07:09:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.9052521586418152 norm:0.056705497205257416 max memory_allocated 22514.87109375 
[2025-02-18 07:10:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.8699277639389038 norm:0.02959459088742733 max memory_allocated 22514.87109375 
[2025-02-18 07:11:00 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.8475157618522644 norm:0.016662877053022385 max memory_allocated 22514.87109375 
[2025-02-18 07:11:33 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.8422289490699768 norm:0.012196470983326435 max memory_allocated 22514.87109375 
[2025-02-18 07:12:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.837827742099762 norm:0.00864808913320303 max memory_allocated 22514.87109375 
[2025-02-18 07:12:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.8348256349563599 norm:0.006731177214533091 max memory_allocated 22514.87109375 
[2025-02-18 07:13:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.8313093185424805 norm:0.005435043480247259 max memory_allocated 22514.87109375 
[2025-02-18 07:13:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.8286952376365662 norm:0.004729731939733028 max memory_allocated 22514.87109375 
[2025-02-18 07:14:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.826286792755127 norm:0.00410713255405426 max memory_allocated 22514.87109375 
[2025-02-18 07:14:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.8249528408050537 norm:0.0038297809660434723 max memory_allocated 22514.87109375 
[2025-02-18 07:15:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.8254761099815369 norm:0.003685569856315851 max memory_allocated 22514.87109375 
[2025-02-18 07:15:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.8232600092887878 norm:0.0034471561666578054 max memory_allocated 22514.87109375 
[2025-02-18 07:16:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.8211115598678589 norm:0.00321755139157176 max memory_allocated 22514.87109375 
[2025-02-18 07:17:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.8201074004173279 norm:0.0031086767558008432 max memory_allocated 22514.87109375 
[2025-02-18 07:17:35 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.8188518285751343 norm:0.0030532917007803917 max memory_allocated 22514.87109375 
[2025-02-18 07:18:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.8184704780578613 norm:0.002891046926379204 max memory_allocated 22514.87109375 
[2025-02-18 07:18:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.8196063041687012 norm:0.0028418998699635267 max memory_allocated 22514.87109375 
[2025-02-18 07:19:14 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.8215822577476501 norm:0.002826516516506672 max memory_allocated 22514.87109375 
[2025-02-18 07:19:46 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.8211103677749634 norm:0.0027774616610258818 max memory_allocated 22514.87109375 
[2025-02-18 07:20:19 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.8199712038040161 norm:0.002698803786188364 max memory_allocated 22514.87109375 
[2025-02-18 07:20:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 07:21:05 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.8603366017341614 norm:0.017691347748041153 max memory_allocated 22515.04296875 
[2025-02-18 07:21:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.8383828997612 norm:0.009904762730002403 max memory_allocated 22515.04296875 
[2025-02-18 07:22:10 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.8289863467216492 norm:0.0062959264032542706 max memory_allocated 22515.04296875 
[2025-02-18 07:22:43 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.8233781456947327 norm:0.004396262113004923 max memory_allocated 22515.04296875 
[2025-02-18 07:23:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.8197205066680908 norm:0.0033586286008358 max memory_allocated 22515.04296875 
[2025-02-18 07:23:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.8162990808486938 norm:0.0027135079726576805 max memory_allocated 22515.04296875 
[2025-02-18 07:24:22 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.8132898807525635 norm:0.0022818304132670164 max memory_allocated 22515.04296875 
[2025-02-18 07:24:55 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.8117706775665283 norm:0.0020868817809969187 max memory_allocated 22515.04296875 
[2025-02-18 07:25:28 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.8100335597991943 norm:0.0019686371088027954 max memory_allocated 22515.04296875 
[2025-02-18 07:26:01 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.8092907071113586 norm:0.0018769947346299887 max memory_allocated 22515.04296875 
[2025-02-18 07:26:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.8088917136192322 norm:0.0018623005598783493 max memory_allocated 22515.04296875 
[2025-02-18 07:27:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.8095442652702332 norm:0.0018431167118251324 max memory_allocated 22515.04296875 
[2025-02-18 07:27:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.8112087249755859 norm:0.0018595370929688215 max memory_allocated 22515.04296875 
[2025-02-18 07:28:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.8114968538284302 norm:0.00196609809063375 max memory_allocated 22515.04296875 
[2025-02-18 07:28:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.8117638826370239 norm:0.001912336447276175 max memory_allocated 22515.04296875 
[2025-02-18 07:29:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.8113876581192017 norm:0.0018548071384429932 max memory_allocated 22515.04296875 
[2025-02-18 07:29:52 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.810427188873291 norm:0.001801029429771006 max memory_allocated 22515.04296875 
[2025-02-18 07:30:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.8103124499320984 norm:0.0018126575741916895 max memory_allocated 22515.04296875 
[2025-02-18 07:30:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.8096573352813721 norm:0.0017535489751026034 max memory_allocated 22515.04296875 
[2025-02-18 07:31:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.8089259266853333 norm:0.001740352250635624 max memory_allocated 22515.04296875 
[2025-02-18 07:31:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 07:32:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.8922823071479797 norm:0.03925430029630661 max memory_allocated 22515.21484375 
[2025-02-18 07:32:48 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.8636408448219299 norm:0.021228253841400146 max memory_allocated 22515.21484375 
[2025-02-18 07:33:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.8452373743057251 norm:0.012712129391729832 max memory_allocated 22515.21484375 
[2025-02-18 07:33:54 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.8333724737167358 norm:0.007984381169080734 max memory_allocated 22515.21484375 
[2025-02-18 07:34:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.829677939414978 norm:0.006328117102384567 max memory_allocated 22515.21484375 
[2025-02-18 07:35:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.8263299465179443 norm:0.004797544330358505 max memory_allocated 22515.21484375 
[2025-02-18 07:35:33 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.8226432204246521 norm:0.003931769635528326 max memory_allocated 22515.21484375 
[2025-02-18 07:36:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.8200002312660217 norm:0.0033567931968718767 max memory_allocated 22515.21484375 
[2025-02-18 07:36:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.8187440633773804 norm:0.003025378566235304 max memory_allocated 22515.21484375 
[2025-02-18 07:37:12 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.8165966868400574 norm:0.00279669975861907 max memory_allocated 22515.21484375 
[2025-02-18 07:37:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.8156895637512207 norm:0.002649478381499648 max memory_allocated 22515.21484375 
[2025-02-18 07:38:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.8153606653213501 norm:0.0025250245817005634 max memory_allocated 22515.21484375 
[2025-02-18 07:38:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.8140180110931396 norm:0.0023519182577729225 max memory_allocated 22515.21484375 
[2025-02-18 07:39:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.8121042251586914 norm:0.0022246025037020445 max memory_allocated 22515.21484375 
[2025-02-18 07:39:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.8119056820869446 norm:0.002210278995335102 max memory_allocated 22515.21484375 
[2025-02-18 07:40:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.811345100402832 norm:0.0021293889731168747 max memory_allocated 22515.21484375 
[2025-02-18 07:41:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.8117207288742065 norm:0.002100977348163724 max memory_allocated 22515.21484375 
[2025-02-18 07:41:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.8121004700660706 norm:0.002054189797490835 max memory_allocated 22515.21484375 
[2025-02-18 07:42:08 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.812411367893219 norm:0.0020194207318127155 max memory_allocated 22515.21484375 
[2025-02-18 07:42:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.812203049659729 norm:0.0019647039007395506 max memory_allocated 22515.21484375 
[2025-02-18 07:42:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 07:43:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.9003886580467224 norm:0.043085288256406784 max memory_allocated 22515.38671875 
[2025-02-18 07:43:59 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.8697926998138428 norm:0.02288188226521015 max memory_allocated 22515.38671875 
[2025-02-18 07:44:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.8523668646812439 norm:0.014291426166892052 max memory_allocated 22515.38671875 
[2025-02-18 07:45:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.8415323495864868 norm:0.009985069744288921 max memory_allocated 22515.38671875 
[2025-02-18 07:45:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.8342999815940857 norm:0.0073580630123615265 max memory_allocated 22515.38671875 
[2025-02-18 07:46:11 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.8287601470947266 norm:0.005530929192900658 max memory_allocated 22515.38671875 
[2025-02-18 07:46:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.8256536722183228 norm:0.0044629755429923534 max memory_allocated 22515.38671875 
[2025-02-18 07:47:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.823072612285614 norm:0.0037551415152847767 max memory_allocated 22515.38671875 
[2025-02-18 07:47:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.8215897083282471 norm:0.003309733234345913 max memory_allocated 22515.38671875 
[2025-02-18 07:48:22 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.8197025060653687 norm:0.0029331149999052286 max memory_allocated 22515.38671875 
[2025-02-18 07:48:55 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.8183997869491577 norm:0.0026556358207017183 max memory_allocated 22515.38671875 
[2025-02-18 07:49:28 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.8177785277366638 norm:0.0024870652705430984 max memory_allocated 22515.38671875 
[2025-02-18 07:50:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.8172008991241455 norm:0.0023578854743391275 max memory_allocated 22515.38671875 
[2025-02-18 07:50:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.8170725703239441 norm:0.0022411642130464315 max memory_allocated 22515.38671875 
[2025-02-18 07:51:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.8168708086013794 norm:0.002134596696123481 max memory_allocated 22515.38671875 
[2025-02-18 07:51:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.8164175748825073 norm:0.0020212330855429173 max memory_allocated 22515.38671875 
[2025-02-18 07:52:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.8156808614730835 norm:0.001941462978720665 max memory_allocated 22515.38671875 
[2025-02-18 07:52:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.8155531287193298 norm:0.0018809514585882425 max memory_allocated 22515.38671875 
[2025-02-18 07:53:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.8164722323417664 norm:0.0018580395262688398 max memory_allocated 22515.38671875 
[2025-02-18 07:53:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.8171475529670715 norm:0.0018313615582883358 max memory_allocated 22515.38671875 
[2025-02-18 07:54:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 07:54:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.8922575116157532 norm:0.03020368702709675 max memory_allocated 22515.55859375 
[2025-02-18 07:55:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.8765789270401001 norm:0.019871966913342476 max memory_allocated 22515.55859375 
[2025-02-18 07:55:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.8666294813156128 norm:0.012968511320650578 max memory_allocated 22515.55859375 
[2025-02-18 07:56:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.8564079999923706 norm:0.008191011846065521 max memory_allocated 22515.55859375 
[2025-02-18 07:56:49 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.8512087464332581 norm:0.006099099293351173 max memory_allocated 22515.55859375 
[2025-02-18 07:57:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.847693681716919 norm:0.0047270371578633785 max memory_allocated 22515.55859375 
[2025-02-18 07:57:54 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.8452818393707275 norm:0.0038078671786934137 max memory_allocated 22515.55859375 
[2025-02-18 07:58:27 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.8438293933868408 norm:0.0031912035774439573 max memory_allocated 22515.55859375 
[2025-02-18 07:59:00 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.8417761921882629 norm:0.0028426977805793285 max memory_allocated 22515.55859375 
[2025-02-18 07:59:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.8397935032844543 norm:0.002571260090917349 max memory_allocated 22515.55859375 
[2025-02-18 08:00:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.8400749564170837 norm:0.002561295870691538 max memory_allocated 22515.55859375 
[2025-02-18 08:00:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.8389022350311279 norm:0.0029109828174114227 max memory_allocated 22515.55859375 
[2025-02-18 08:01:12 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.836876630783081 norm:0.0033100778236985207 max memory_allocated 22515.55859375 
[2025-02-18 08:01:45 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.8351550698280334 norm:0.0033928093034774065 max memory_allocated 22515.55859375 
[2025-02-18 08:02:18 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.8350674510002136 norm:0.003500585909932852 max memory_allocated 22515.55859375 
[2025-02-18 08:02:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.8360031843185425 norm:0.0036877505481243134 max memory_allocated 22515.55859375 
[2025-02-18 08:03:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.8360463976860046 norm:0.003593768458813429 max memory_allocated 22515.55859375 
[2025-02-18 08:03:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.8355051875114441 norm:0.0035921179223805666 max memory_allocated 22515.55859375 
[2025-02-18 08:04:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.8355395793914795 norm:0.003584248013794422 max memory_allocated 22515.55859375 
[2025-02-18 08:05:03 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.8359092473983765 norm:0.003570717526599765 max memory_allocated 22515.55859375 
[2025-02-18 08:05:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 08:05:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.9760318398475647 norm:0.06912718713283539 max memory_allocated 22515.73046875 
[2025-02-18 08:06:21 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.9536496996879578 norm:0.03866241127252579 max memory_allocated 22515.73046875 
[2025-02-18 08:06:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.9379587173461914 norm:0.022541508078575134 max memory_allocated 22515.73046875 
[2025-02-18 08:07:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.9253789186477661 norm:0.014957671985030174 max memory_allocated 22515.73046875 
[2025-02-18 08:07:59 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.9262672066688538 norm:0.01140573713928461 max memory_allocated 22515.73046875 
[2025-02-18 08:08:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.9238584637641907 norm:0.008784177713096142 max memory_allocated 22515.73046875 
[2025-02-18 08:09:05 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.9193554520606995 norm:0.007208867929875851 max memory_allocated 22515.73046875 
[2025-02-18 08:09:38 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.920864999294281 norm:0.006651397328823805 max memory_allocated 22515.73046875 
[2025-02-18 08:10:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.9188125133514404 norm:0.006058477330952883 max memory_allocated 22515.73046875 
[2025-02-18 08:10:44 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.9169806838035583 norm:0.005496941041201353 max memory_allocated 22515.73046875 
[2025-02-18 08:11:17 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.9170876741409302 norm:0.00516332546249032 max memory_allocated 22515.73046875 
[2025-02-18 08:11:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.9167219400405884 norm:0.004946000408381224 max memory_allocated 22515.73046875 
[2025-02-18 08:12:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.9163122773170471 norm:0.004850110504776239 max memory_allocated 22515.73046875 
[2025-02-18 08:12:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.9145820140838623 norm:0.00465695234015584 max memory_allocated 22515.73046875 
[2025-02-18 08:13:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.9138286113739014 norm:0.00455662002786994 max memory_allocated 22515.73046875 
[2025-02-18 08:14:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.912683367729187 norm:0.004482965916395187 max memory_allocated 22515.73046875 
[2025-02-18 08:14:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.9115625619888306 norm:0.004359278362244368 max memory_allocated 22515.73046875 
[2025-02-18 08:15:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.910799503326416 norm:0.004337763413786888 max memory_allocated 22515.73046875 
[2025-02-18 08:15:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.9091646671295166 norm:0.004202880430966616 max memory_allocated 22515.73046875 
[2025-02-18 08:16:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.9085237979888916 norm:0.004133041016757488 max memory_allocated 22515.73046875 
[2025-02-18 08:16:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 08:16:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:1.0004538297653198 norm:0.03965093195438385 max memory_allocated 22515.90234375 
[2025-02-18 08:17:31 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.9812602400779724 norm:0.022699490189552307 max memory_allocated 22515.90234375 
[2025-02-18 08:18:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.9731956124305725 norm:0.015044457279145718 max memory_allocated 22515.90234375 
[2025-02-18 08:18:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.9654618501663208 norm:0.01003753300756216 max memory_allocated 22515.90234375 
[2025-02-18 08:19:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.9609537720680237 norm:0.007387762889266014 max memory_allocated 22515.90234375 
[2025-02-18 08:19:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.9615647792816162 norm:0.005928272381424904 max memory_allocated 22515.90234375 
[2025-02-18 08:20:15 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.9609419703483582 norm:0.004903467837721109 max memory_allocated 22515.90234375 
[2025-02-18 08:20:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.9594898223876953 norm:0.004128756932914257 max memory_allocated 22515.90234375 
[2025-02-18 08:21:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.958425760269165 norm:0.0037138150073587894 max memory_allocated 22515.90234375 
[2025-02-18 08:21:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.9570755362510681 norm:0.0033537547569721937 max memory_allocated 22515.90234375 
[2025-02-18 08:22:27 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.9557870030403137 norm:0.0030755149200558662 max memory_allocated 22515.90234375 
[2025-02-18 08:23:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.9554542899131775 norm:0.0029793691355735064 max memory_allocated 22515.90234375 
[2025-02-18 08:23:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.9539120197296143 norm:0.002829180099070072 max memory_allocated 22515.90234375 
[2025-02-18 08:24:06 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.9530378580093384 norm:0.0027173878625035286 max memory_allocated 22515.90234375 
[2025-02-18 08:24:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.9524218440055847 norm:0.002605322515591979 max memory_allocated 22515.90234375 
[2025-02-18 08:25:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.952456533908844 norm:0.002550882752984762 max memory_allocated 22515.90234375 
[2025-02-18 08:25:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.9514007568359375 norm:0.0025079406332224607 max memory_allocated 22515.90234375 
[2025-02-18 08:26:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.9503759145736694 norm:0.0024163448251783848 max memory_allocated 22515.90234375 
[2025-02-18 08:26:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.9494661688804626 norm:0.002376341260969639 max memory_allocated 22515.90234375 
[2025-02-18 08:27:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.9494054913520813 norm:0.0023858053609728813 max memory_allocated 22515.90234375 
[2025-02-18 08:27:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 08:28:08 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:1.0432188510894775 norm:0.025311429053544998 max memory_allocated 22516.07421875 
[2025-02-18 08:28:41 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:1.0330582857131958 norm:0.016904480755329132 max memory_allocated 22516.07421875 
[2025-02-18 08:29:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:1.024734377861023 norm:0.011548827402293682 max memory_allocated 22516.07421875 
[2025-02-18 08:29:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:1.020997166633606 norm:0.008508363738656044 max memory_allocated 22516.07421875 
[2025-02-18 08:30:20 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:1.0190500020980835 norm:0.006722029764205217 max memory_allocated 22516.07421875 
[2025-02-18 08:30:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:1.0174050331115723 norm:0.005373080726712942 max memory_allocated 22516.07421875 
[2025-02-18 08:31:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:1.0142441987991333 norm:0.004346462432295084 max memory_allocated 22516.07421875 
[2025-02-18 08:31:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:1.0122404098510742 norm:0.003742384258657694 max memory_allocated 22516.07421875 
[2025-02-18 08:32:32 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:1.0102981328964233 norm:0.003342284122481942 max memory_allocated 22516.07421875 
[2025-02-18 08:33:04 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:1.0091438293457031 norm:0.0029746778309345245 max memory_allocated 22516.07421875 
[2025-02-18 08:33:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:1.008172869682312 norm:0.002647378481924534 max memory_allocated 22516.07421875 
[2025-02-18 08:34:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:1.0065031051635742 norm:0.002441919408738613 max memory_allocated 22516.07421875 
[2025-02-18 08:34:43 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:1.0054563283920288 norm:0.0022370642982423306 max memory_allocated 22516.07421875 
[2025-02-18 08:35:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:1.0053941011428833 norm:0.002143031219020486 max memory_allocated 22516.07421875 
[2025-02-18 08:35:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:1.004659652709961 norm:0.0020539630204439163 max memory_allocated 22516.07421875 
[2025-02-18 08:36:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:1.0040286779403687 norm:0.0019720960408449173 max memory_allocated 22516.07421875 
[2025-02-18 08:36:55 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:1.0035724639892578 norm:0.0019274101359769702 max memory_allocated 22516.07421875 
[2025-02-18 08:37:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:1.0027751922607422 norm:0.0018799715908244252 max memory_allocated 22516.07421875 
[2025-02-18 08:38:01 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:1.0023927688598633 norm:0.0018210643902420998 max memory_allocated 22516.07421875 
[2025-02-18 08:38:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:1.0020689964294434 norm:0.001778341829776764 max memory_allocated 22516.07421875 
[2025-02-18 08:38:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 08:39:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:1.1122039556503296 norm:0.02671106532216072 max memory_allocated 22516.24609375 
[2025-02-18 08:39:52 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:1.096418023109436 norm:0.0149854626506567 max memory_allocated 22516.24609375 
[2025-02-18 08:40:25 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:1.0855234861373901 norm:0.009289920330047607 max memory_allocated 22516.24609375 
[2025-02-18 08:40:58 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:1.0813223123550415 norm:0.006785843055695295 max memory_allocated 22516.24609375 
[2025-02-18 08:41:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:1.0788792371749878 norm:0.005197878461331129 max memory_allocated 22516.24609375 
[2025-02-18 08:42:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:1.0759894847869873 norm:0.003629337064921856 max memory_allocated 22516.24609375 
[2025-02-18 08:42:36 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:1.0746501684188843 norm:0.0029306376818567514 max memory_allocated 22516.24609375 
[2025-02-18 08:43:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:1.0733380317687988 norm:0.002483827993273735 max memory_allocated 22516.24609375 
[2025-02-18 08:43:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:1.0714524984359741 norm:0.0020844556856900454 max memory_allocated 22516.24609375 
[2025-02-18 08:44:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:1.0708496570587158 norm:0.0018471833318471909 max memory_allocated 22516.24609375 
[2025-02-18 08:44:48 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:1.0702519416809082 norm:0.0016727629117667675 max memory_allocated 22516.24609375 
[2025-02-18 08:45:21 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:1.0696029663085938 norm:0.001561374170705676 max memory_allocated 22516.24609375 
[2025-02-18 08:45:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:1.06930673122406 norm:0.0014854073524475098 max memory_allocated 22516.24609375 
[2025-02-18 08:46:27 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:1.0689009428024292 norm:0.0014435512712225318 max memory_allocated 22516.24609375 
[2025-02-18 08:47:00 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:1.0683784484863281 norm:0.0013976851478219032 max memory_allocated 22516.24609375 
[2025-02-18 08:47:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:1.067801594734192 norm:0.001350957085378468 max memory_allocated 22516.24609375 
[2025-02-18 08:48:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:1.067483901977539 norm:0.0013389377854764462 max memory_allocated 22516.24609375 
[2025-02-18 08:48:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:1.0673706531524658 norm:0.0013251808704808354 max memory_allocated 22516.24609375 
[2025-02-18 08:49:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:1.0671029090881348 norm:0.001309298793785274 max memory_allocated 22516.24609375 
[2025-02-18 08:49:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:1.0670702457427979 norm:0.0013082074001431465 max memory_allocated 22516.24609375 
[2025-02-18 08:49:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 08:50:29 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:1.2204327583312988 norm:0.027149517089128494 max memory_allocated 22516.41796875 
[2025-02-18 08:51:02 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:1.2076627016067505 norm:0.01712690107524395 max memory_allocated 22516.41796875 
[2025-02-18 08:51:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:1.2015995979309082 norm:0.012208562344312668 max memory_allocated 22516.41796875 
[2025-02-18 08:52:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:1.1993545293807983 norm:0.00909383688122034 max memory_allocated 22516.41796875 
[2025-02-18 08:52:41 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:1.196911096572876 norm:0.005970539525151253 max memory_allocated 22516.41796875 
[2025-02-18 08:53:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:1.1945483684539795 norm:0.0044493707828223705 max memory_allocated 22516.41796875 
[2025-02-18 08:53:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:1.191318154335022 norm:0.003590493928641081 max memory_allocated 22516.41796875 
[2025-02-18 08:54:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:1.18877112865448 norm:0.0031945593655109406 max memory_allocated 22516.41796875 
[2025-02-18 08:54:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:1.186952829360962 norm:0.003018980612978339 max memory_allocated 22516.41796875 
[2025-02-18 08:55:26 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:1.1870677471160889 norm:0.0030258751939982176 max memory_allocated 22516.41796875 
[2025-02-18 08:55:59 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:1.1874146461486816 norm:0.0029659424908459187 max memory_allocated 22516.41796875 
[2025-02-18 08:56:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:1.1859092712402344 norm:0.0028646818827837706 max memory_allocated 22516.41796875 
[2025-02-18 08:57:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:1.1856485605239868 norm:0.0028675918001681566 max memory_allocated 22516.41796875 
[2025-02-18 08:57:38 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:1.1852197647094727 norm:0.0028456347063183784 max memory_allocated 22516.41796875 
[2025-02-18 08:58:11 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:1.1847801208496094 norm:0.002819906920194626 max memory_allocated 22516.41796875 
[2025-02-18 08:58:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:1.1850372552871704 norm:0.002846467075869441 max memory_allocated 22516.41796875 
[2025-02-18 08:59:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:1.185611367225647 norm:0.0028834426775574684 max memory_allocated 22516.41796875 
[2025-02-18 08:59:49 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:1.184922218322754 norm:0.0028704770375043154 max memory_allocated 22516.41796875 
[2025-02-18 09:00:22 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:1.184457540512085 norm:0.0028646814171224833 max memory_allocated 22516.41796875 
[2025-02-18 09:00:55 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:1.184222936630249 norm:0.0028599004726856947 max memory_allocated 22516.41796875 
[2025-02-18 09:01:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 09:01:40 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:1.3146897554397583 norm:0.016044026240706444 max memory_allocated 22516.58984375 
[2025-02-18 09:02:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:1.3069229125976562 norm:0.010676324367523193 max memory_allocated 22516.58984375 
[2025-02-18 09:02:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:1.3015408515930176 norm:0.007146837189793587 max memory_allocated 22516.58984375 
[2025-02-18 09:03:19 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:1.2981512546539307 norm:0.004929557908326387 max memory_allocated 22516.58984375 
[2025-02-18 09:03:52 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:1.2952451705932617 norm:0.003460041480138898 max memory_allocated 22516.58984375 
[2025-02-18 09:04:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:1.2935032844543457 norm:0.0025615268386900425 max memory_allocated 22516.58984375 
[2025-02-18 09:04:58 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:1.2926149368286133 norm:0.0021161078475415707 max memory_allocated 22516.58984375 
[2025-02-18 09:05:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:1.2921570539474487 norm:0.0018744112458080053 max memory_allocated 22516.58984375 
[2025-02-18 09:06:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:1.2909424304962158 norm:0.0016597018111497164 max memory_allocated 22516.58984375 
[2025-02-18 09:06:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:1.2904142141342163 norm:0.0015515198465436697 max memory_allocated 22516.58984375 
[2025-02-18 09:07:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:1.290936827659607 norm:0.0015391570050269365 max memory_allocated 22516.58984375 
[2025-02-18 09:07:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:1.2905874252319336 norm:0.0014886551070958376 max memory_allocated 22516.58984375 
[2025-02-18 09:08:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:1.289925217628479 norm:0.0014319237088784575 max memory_allocated 22516.58984375 
[2025-02-18 09:08:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:1.2894349098205566 norm:0.0013946460094302893 max memory_allocated 22516.58984375 
[2025-02-18 09:09:21 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:1.2892082929611206 norm:0.0013667445164173841 max memory_allocated 22516.58984375 
[2025-02-18 09:09:54 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:1.2891466617584229 norm:0.001355193555355072 max memory_allocated 22516.58984375 
[2025-02-18 09:10:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:1.2892719507217407 norm:0.0013670058688148856 max memory_allocated 22516.58984375 
[2025-02-18 09:11:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:1.2893164157867432 norm:0.0013710097409784794 max memory_allocated 22516.58984375 
[2025-02-18 09:11:33 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:1.2891086339950562 norm:0.0013712721411138773 max memory_allocated 22516.58984375 
[2025-02-18 09:12:06 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:1.2887752056121826 norm:0.001351361395791173 max memory_allocated 22516.58984375 
[2025-02-18 09:12:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 09:12:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:1.4541351795196533 norm:0.0199829563498497 max memory_allocated 22516.76171875 
[2025-02-18 09:13:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:1.4408679008483887 norm:0.011843612417578697 max memory_allocated 22516.76171875 
[2025-02-18 09:13:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:1.4336246252059937 norm:0.007601375225931406 max memory_allocated 22516.76171875 
[2025-02-18 09:14:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:1.4304765462875366 norm:0.005284075625240803 max memory_allocated 22516.76171875 
[2025-02-18 09:15:03 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:1.4272420406341553 norm:0.0037994934245944023 max memory_allocated 22516.76171875 
[2025-02-18 09:15:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:1.425790786743164 norm:0.003069221507757902 max memory_allocated 22516.76171875 
[2025-02-18 09:16:09 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:1.424419641494751 norm:0.002673574024811387 max memory_allocated 22516.76171875 
[2025-02-18 09:16:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:1.42339026927948 norm:0.0023698441218584776 max memory_allocated 22516.76171875 
[2025-02-18 09:17:15 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:1.422635555267334 norm:0.0022036689333617687 max memory_allocated 22516.76171875 
[2025-02-18 09:17:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:1.420127511024475 norm:0.0020499126985669136 max memory_allocated 22516.76171875 
[2025-02-18 09:18:20 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:1.4192200899124146 norm:0.001916045555844903 max memory_allocated 22516.76171875 
[2025-02-18 09:18:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:1.4186651706695557 norm:0.0018255622126162052 max memory_allocated 22516.76171875 
[2025-02-18 09:19:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:1.4184799194335938 norm:0.0017362652579322457 max memory_allocated 22516.76171875 
[2025-02-18 09:19:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:1.418371319770813 norm:0.0016990411095321178 max memory_allocated 22516.76171875 
[2025-02-18 09:20:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:1.4185158014297485 norm:0.0016692819772288203 max memory_allocated 22516.76171875 
[2025-02-18 09:21:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:1.418120265007019 norm:0.001626154175028205 max memory_allocated 22516.76171875 
[2025-02-18 09:21:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:1.4179787635803223 norm:0.0016067473916336894 max memory_allocated 22516.76171875 
[2025-02-18 09:22:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:1.4186490774154663 norm:0.001601729542016983 max memory_allocated 22516.76171875 
[2025-02-18 09:22:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:1.4190151691436768 norm:0.0016114577883854508 max memory_allocated 22516.76171875 
[2025-02-18 09:23:17 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:1.4189175367355347 norm:0.0015683160163462162 max memory_allocated 22516.76171875 
[2025-02-18 09:23:26 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 09:24:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:1.6261134147644043 norm:0.0359189435839653 max memory_allocated 22516.93359375 
[2025-02-18 09:24:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:1.6173192262649536 norm:0.024987071752548218 max memory_allocated 22516.93359375 
[2025-02-18 09:25:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:1.612388253211975 norm:0.018149208277463913 max memory_allocated 22516.93359375 
[2025-02-18 09:25:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:1.6109973192214966 norm:0.013541851192712784 max memory_allocated 22516.93359375 
[2025-02-18 09:26:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:1.609323501586914 norm:0.010860806331038475 max memory_allocated 22516.93359375 
[2025-02-18 09:26:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:1.6085789203643799 norm:0.008967746049165726 max memory_allocated 22516.93359375 
[2025-02-18 09:27:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:1.606752634048462 norm:0.007741523440927267 max memory_allocated 22516.93359375 
[2025-02-18 09:27:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:1.6042919158935547 norm:0.0067034438252449036 max memory_allocated 22516.93359375 
[2025-02-18 09:28:26 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:1.6020054817199707 norm:0.00589574221521616 max memory_allocated 22516.93359375 
[2025-02-18 09:28:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:1.6001149415969849 norm:0.005257399287074804 max memory_allocated 22516.93359375 
[2025-02-18 09:29:31 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:1.5981611013412476 norm:0.004860562272369862 max memory_allocated 22516.93359375 
[2025-02-18 09:30:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:1.5970999002456665 norm:0.00449735252186656 max memory_allocated 22516.93359375 
[2025-02-18 09:30:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:1.5956964492797852 norm:0.004159285221248865 max memory_allocated 22516.93359375 
[2025-02-18 09:31:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:1.5942260026931763 norm:0.0038856028113514185 max memory_allocated 22516.93359375 
[2025-02-18 09:31:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:1.5928114652633667 norm:0.0036577368155121803 max memory_allocated 22516.93359375 
[2025-02-18 09:32:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:1.5914429426193237 norm:0.0034261641558259726 max memory_allocated 22516.93359375 
[2025-02-18 09:32:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:1.5908372402191162 norm:0.0032912639435380697 max memory_allocated 22516.93359375 
[2025-02-18 09:33:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:1.5903911590576172 norm:0.003164221765473485 max memory_allocated 22516.93359375 
[2025-02-18 09:33:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:1.5894020795822144 norm:0.0030084066092967987 max memory_allocated 22516.93359375 
[2025-02-18 09:34:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:1.5883188247680664 norm:0.002884192392230034 max memory_allocated 22516.93359375 
[2025-02-18 09:34:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 09:35:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:1.7988194227218628 norm:0.017290856689214706 max memory_allocated 22517.10546875 
[2025-02-18 09:35:46 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:1.7886834144592285 norm:0.011697999201714993 max memory_allocated 22517.10546875 
[2025-02-18 09:36:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:1.7803047895431519 norm:0.007967581041157246 max memory_allocated 22517.10546875 
[2025-02-18 09:36:52 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:1.772660493850708 norm:0.005468235816806555 max memory_allocated 22517.10546875 
[2025-02-18 09:37:25 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:1.7689895629882812 norm:0.003976292908191681 max memory_allocated 22517.10546875 
[2025-02-18 09:37:58 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:1.7665742635726929 norm:0.003061251947656274 max memory_allocated 22517.10546875 
[2025-02-18 09:38:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:1.7646167278289795 norm:0.002512820763513446 max memory_allocated 22517.10546875 
[2025-02-18 09:39:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:1.7631055116653442 norm:0.002388609107583761 max memory_allocated 22517.10546875 
[2025-02-18 09:39:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:1.7627484798431396 norm:0.0022036770824342966 max memory_allocated 22517.10546875 
[2025-02-18 09:40:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:1.763934850692749 norm:0.002173218410462141 max memory_allocated 22517.10546875 
[2025-02-18 09:40:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:1.7629083395004272 norm:0.0021015089005231857 max memory_allocated 22517.10546875 
[2025-02-18 09:41:15 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:1.7623908519744873 norm:0.0021103089675307274 max memory_allocated 22517.10546875 
[2025-02-18 09:41:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:1.762076735496521 norm:0.0022800068836659193 max memory_allocated 22517.10546875 
[2025-02-18 09:42:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:1.7608897686004639 norm:0.0023726364597678185 max memory_allocated 22517.10546875 
[2025-02-18 09:42:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:1.760467767715454 norm:0.0024759271182119846 max memory_allocated 22517.10546875 
[2025-02-18 09:43:27 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:1.7601690292358398 norm:0.002387303626164794 max memory_allocated 22517.10546875 
[2025-02-18 09:44:00 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:1.7603703737258911 norm:0.002445374382659793 max memory_allocated 22517.10546875 
[2025-02-18 09:44:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:1.7601308822631836 norm:0.0025211141910403967 max memory_allocated 22517.10546875 
[2025-02-18 09:45:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:1.7601007223129272 norm:0.002560653258115053 max memory_allocated 22517.10546875 
[2025-02-18 09:45:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:1.760729432106018 norm:0.00260748527944088 max memory_allocated 22517.10546875 
[2025-02-18 09:45:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 09:46:24 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:2.0169122219085693 norm:0.029448848217725754 max memory_allocated 22517.27734375 
[2025-02-18 09:46:57 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:2.002225637435913 norm:0.01882977783679962 max memory_allocated 22517.27734375 
[2025-02-18 09:47:30 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:1.9914422035217285 norm:0.012647315859794617 max memory_allocated 22517.27734375 
[2025-02-18 09:48:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:1.9843919277191162 norm:0.008972433395683765 max memory_allocated 22517.27734375 
[2025-02-18 09:48:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:1.980649471282959 norm:0.006798615213483572 max memory_allocated 22517.27734375 
[2025-02-18 09:49:09 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:1.9773932695388794 norm:0.005128764547407627 max memory_allocated 22517.27734375 
[2025-02-18 09:49:42 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:1.9737257957458496 norm:0.003669834230095148 max memory_allocated 22517.27734375 
[2025-02-18 09:50:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:1.9708411693572998 norm:0.0027485783211886883 max memory_allocated 22517.27734375 
[2025-02-18 09:50:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:1.9693050384521484 norm:0.0023910426534712315 max memory_allocated 22517.27734375 
[2025-02-18 09:51:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:1.9680707454681396 norm:0.002204079646617174 max memory_allocated 22517.27734375 
[2025-02-18 09:51:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:1.9674077033996582 norm:0.00203316705301404 max memory_allocated 22517.27734375 
[2025-02-18 09:52:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:1.967214584350586 norm:0.0019167098216712475 max memory_allocated 22517.27734375 
[2025-02-18 09:53:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:1.9668951034545898 norm:0.0018730235751718283 max memory_allocated 22517.27734375 
[2025-02-18 09:53:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:1.9663445949554443 norm:0.0018533660331740975 max memory_allocated 22517.27734375 
[2025-02-18 09:54:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:1.9665231704711914 norm:0.001851503737270832 max memory_allocated 22517.27734375 
[2025-02-18 09:54:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:1.9664089679718018 norm:0.001835757284425199 max memory_allocated 22517.27734375 
[2025-02-18 09:55:11 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:1.9665426015853882 norm:0.0018558850279077888 max memory_allocated 22517.27734375 
[2025-02-18 09:55:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:1.9661259651184082 norm:0.0018400636035948992 max memory_allocated 22517.27734375 
[2025-02-18 09:56:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:1.9659696817398071 norm:0.0018403431167826056 max memory_allocated 22517.27734375 
[2025-02-18 09:56:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:1.9657747745513916 norm:0.001813023234717548 max memory_allocated 22517.27734375 
[2025-02-18 09:57:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 09:57:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:2.2837445735931396 norm:0.030651111155748367 max memory_allocated 22517.44921875 
[2025-02-18 09:58:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:2.266357660293579 norm:0.02063409425318241 max memory_allocated 22517.44921875 
[2025-02-18 09:58:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:2.2588043212890625 norm:0.014586302451789379 max memory_allocated 22517.44921875 
[2025-02-18 09:59:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:2.2559521198272705 norm:0.010849125683307648 max memory_allocated 22517.44921875 
[2025-02-18 09:59:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:2.251633405685425 norm:0.008405258879065514 max memory_allocated 22517.44921875 
[2025-02-18 10:00:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:2.249342918395996 norm:0.006472476292401552 max memory_allocated 22517.44921875 
[2025-02-18 10:00:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:2.247439384460449 norm:0.005027559585869312 max memory_allocated 22517.44921875 
[2025-02-18 10:01:26 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:2.245757818222046 norm:0.00416139978915453 max memory_allocated 22517.44921875 
[2025-02-18 10:01:59 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:2.243293523788452 norm:0.003833177499473095 max memory_allocated 22517.44921875 
[2025-02-18 10:02:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:2.2405879497528076 norm:0.0033900411799550056 max memory_allocated 22517.44921875 
[2025-02-18 10:03:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:2.238987445831299 norm:0.003042674157768488 max memory_allocated 22517.44921875 
[2025-02-18 10:03:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:2.238415241241455 norm:0.0028509818948805332 max memory_allocated 22517.44921875 
[2025-02-18 10:04:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:2.2381081581115723 norm:0.0027244766242802143 max memory_allocated 22517.44921875 
[2025-02-18 10:04:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:2.2369067668914795 norm:0.0025836131535470486 max memory_allocated 22517.44921875 
[2025-02-18 10:05:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:2.2363243103027344 norm:0.002507697558030486 max memory_allocated 22517.44921875 
[2025-02-18 10:05:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:2.23642897605896 norm:0.0024810589384287596 max memory_allocated 22517.44921875 
[2025-02-18 10:06:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:2.2360997200012207 norm:0.002437448827549815 max memory_allocated 22517.44921875 
[2025-02-18 10:06:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:2.2352824211120605 norm:0.00239422800950706 max memory_allocated 22517.44921875 
[2025-02-18 10:07:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:2.234856367111206 norm:0.0023724758066236973 max memory_allocated 22517.44921875 
[2025-02-18 10:08:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:2.233574867248535 norm:0.002336714882403612 max memory_allocated 22517.44921875 
[2025-02-18 10:08:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 10:08:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:2.595186233520508 norm:0.023034200072288513 max memory_allocated 22517.62109375 
[2025-02-18 10:09:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:2.5688066482543945 norm:0.013368088752031326 max memory_allocated 22517.62109375 
[2025-02-18 10:09:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:2.559704303741455 norm:0.009088223800063133 max memory_allocated 22517.62109375 
[2025-02-18 10:10:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:2.5553395748138428 norm:0.00651758536696434 max memory_allocated 22517.62109375 
[2025-02-18 10:10:57 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:2.5523681640625 norm:0.0051362584345042706 max memory_allocated 22517.62109375 
[2025-02-18 10:11:30 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:2.549046277999878 norm:0.004353742580860853 max memory_allocated 22517.62109375 
[2025-02-18 10:12:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:2.5467827320098877 norm:0.003669440746307373 max memory_allocated 22517.62109375 
[2025-02-18 10:12:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:2.545391321182251 norm:0.0031706704758107662 max memory_allocated 22517.62109375 
[2025-02-18 10:13:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:2.5449419021606445 norm:0.0028202759567648172 max memory_allocated 22517.62109375 
[2025-02-18 10:13:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:2.5449750423431396 norm:0.002625811845064163 max memory_allocated 22517.62109375 
[2025-02-18 10:14:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:2.5443825721740723 norm:0.0025368514470756054 max memory_allocated 22517.62109375 
[2025-02-18 10:14:47 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:2.5433731079101562 norm:0.0024643070064485073 max memory_allocated 22517.62109375 
[2025-02-18 10:15:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:2.542483329772949 norm:0.002387609099969268 max memory_allocated 22517.62109375 
[2025-02-18 10:15:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:2.54329252243042 norm:0.002398824319243431 max memory_allocated 22517.62109375 
[2025-02-18 10:16:26 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:2.5433273315429688 norm:0.0023618771228939295 max memory_allocated 22517.62109375 
[2025-02-18 10:16:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:2.5423073768615723 norm:0.002341889776289463 max memory_allocated 22517.62109375 
[2025-02-18 10:17:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:2.5409393310546875 norm:0.0023378804326057434 max memory_allocated 22517.62109375 
[2025-02-18 10:18:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:2.5403637886047363 norm:0.002338539343327284 max memory_allocated 22517.62109375 
[2025-02-18 10:18:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:2.5396764278411865 norm:0.0023327008821070194 max memory_allocated 22517.62109375 
[2025-02-18 10:19:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:2.5394234657287598 norm:0.0023389323614537716 max memory_allocated 22517.62109375 
[2025-02-18 10:19:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 10:19:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:3.9495863914489746 norm:0.11344276368618011 max memory_allocated 22517.79296875 
[2025-02-18 10:20:30 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:3.9751193523406982 norm:0.23913191258907318 max memory_allocated 22517.79296875 
[2025-02-18 10:21:03 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:3.9693307876586914 norm:0.17641311883926392 max memory_allocated 22517.79296875 
[2025-02-18 10:21:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:3.964951753616333 norm:0.14239615201950073 max memory_allocated 22517.79296875 
[2025-02-18 10:22:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:3.9659295082092285 norm:0.12253744155168533 max memory_allocated 22517.79296875 
[2025-02-18 10:22:42 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:3.957157611846924 norm:0.09681392461061478 max memory_allocated 22517.79296875 
[2025-02-18 10:23:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:3.947141170501709 norm:0.08428087830543518 max memory_allocated 22517.79296875 
[2025-02-18 10:23:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:3.9389777183532715 norm:0.08109176158905029 max memory_allocated 22517.79296875 
[2025-02-18 10:24:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:3.9257872104644775 norm:0.07145802676677704 max memory_allocated 22517.79296875 
[2025-02-18 10:24:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:3.904207468032837 norm:0.06382815539836884 max memory_allocated 22517.79296875 
[2025-02-18 10:25:26 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:3.8875951766967773 norm:0.061106398701667786 max memory_allocated 22517.79296875 
[2025-02-18 10:25:59 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:3.874857187271118 norm:0.059373993426561356 max memory_allocated 22517.79296875 
[2025-02-18 10:26:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:3.86309814453125 norm:0.05534018948674202 max memory_allocated 22517.79296875 
[2025-02-18 10:27:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:3.8541595935821533 norm:0.053825221955776215 max memory_allocated 22517.79296875 
[2025-02-18 10:27:37 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:3.844879627227783 norm:0.05347542464733124 max memory_allocated 22517.79296875 
[2025-02-18 10:28:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:3.843017101287842 norm:0.05663231387734413 max memory_allocated 22517.79296875 
[2025-02-18 10:28:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:3.830690383911133 norm:0.05299008637666702 max memory_allocated 22517.79296875 
[2025-02-18 10:29:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:3.8285679817199707 norm:0.054787538945674896 max memory_allocated 22517.79296875 
[2025-02-18 10:29:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:3.8161706924438477 norm:0.05012580379843712 max memory_allocated 22517.79296875 
[2025-02-18 10:30:22 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:3.8177685737609863 norm:0.05066009983420372 max memory_allocated 22517.79296875 
[2025-02-18 10:30:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 10:31:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:9.120769500732422 norm:0.13981232047080994 max memory_allocated 22517.96484375 
[2025-02-18 10:31:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:8.899003982543945 norm:0.22167834639549255 max memory_allocated 22517.96484375 
[2025-02-18 10:32:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:8.677755355834961 norm:0.3444537818431854 max memory_allocated 22517.96484375 
[2025-02-18 10:32:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:8.65829086303711 norm:0.5049486756324768 max memory_allocated 22517.96484375 
[2025-02-18 10:33:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:8.767731666564941 norm:1.9487502574920654 max memory_allocated 22517.96484375 
[2025-02-18 10:33:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:9.607244491577148 norm:17.736011505126953 max memory_allocated 22517.96484375 
[2025-02-18 10:34:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:10.0338716506958 norm:62.598182678222656 max memory_allocated 22517.96484375 
[2025-02-18 10:34:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:9.964868545532227 norm:76.42094421386719 max memory_allocated 22517.96484375 
[2025-02-18 10:35:29 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:9.941728591918945 norm:99.44319152832031 max memory_allocated 22517.96484375 
[2025-02-18 10:36:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:9.934441566467285 norm:113.06269073486328 max memory_allocated 22517.96484375 
[2025-02-18 10:36:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:9.880088806152344 norm:371.2967529296875 max memory_allocated 22517.96484375 
[2025-02-18 10:37:07 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:9.858983039855957 norm:668.168701171875 max memory_allocated 22517.96484375 
[2025-02-18 10:37:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:9.843087196350098 norm:400.6048583984375 max memory_allocated 22517.96484375 
[2025-02-18 10:38:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:9.854293823242188 norm:266.98040771484375 max memory_allocated 22517.96484375 
[2025-02-18 10:38:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:9.84035873413086 norm:257.8441162109375 max memory_allocated 22517.96484375 
[2025-02-18 10:39:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:9.82723331451416 norm:237.85154724121094 max memory_allocated 22517.96484375 
[2025-02-18 10:39:49 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:9.829529762268066 norm:266.529541015625 max memory_allocated 22517.96484375 
[2025-02-18 10:40:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:9.824851989746094 norm:286.76470947265625 max memory_allocated 22517.96484375 
[2025-02-18 10:40:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:9.82526969909668 norm:295.6416320800781 max memory_allocated 22517.96484375 
[2025-02-18 10:41:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:9.824664115905762 norm:303.5728454589844 max memory_allocated 22517.96484375 
[2025-02-18 10:41:35 root] (main_calibration.py 365): INFO 21435.371212482452
[2025-02-18 10:42:11 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-18 10:43:20 root] (main_calibration.py 158): INFO wikitext2 : 17.95801544189453
[2025-02-18 10:43:20 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-18 10:45:08 root] (main_calibration.py 158): INFO c4 : 25.953975677490234
[2025-02-18 12:22:43 root] (main_calibration.py 169): INFO {'wikitext2': 17.95801544189453, 'c4': 25.953975677490234, 'results': {'piqa': {'acc': 0.6077257889009793, 'acc_stderr': 0.01139184674407223, 'acc_norm': 0.6071817192600653, 'acc_norm_stderr': 0.011394640056759789}, 'boolq': {'acc': 0.6061162079510704, 'acc_stderr': 0.008545835792614984}, 'hellaswag': {'acc': 0.382194781915953, 'acc_stderr': 0.0048493069987277605, 'acc_norm': 0.48526190001991637, 'acc_norm_stderr': 0.004987613263678172}, 'arc_challenge': {'acc': 0.257679180887372, 'acc_stderr': 0.012780770562768409, 'acc_norm': 0.2986348122866894, 'acc_norm_stderr': 0.013374078615068745}, 'arc_easy': {'acc': 0.3977272727272727, 'acc_stderr': 0.010042861602178061, 'acc_norm': 0.3707912457912458, 'acc_norm_stderr': 0.009911292822056923}, 'winogrande': {'acc': 0.5074980268350434, 'acc_stderr': 0.014050905521228571}}, 'versions': {'piqa': 0, 'boolq': 1, 'hellaswag': 0, 'arc_challenge': 0, 'arc_easy': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
