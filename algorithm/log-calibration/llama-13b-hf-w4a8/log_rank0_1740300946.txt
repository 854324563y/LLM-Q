[2025-02-23 08:55:46 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration/llama-13b-hf-w4a8', save_dir='./log-calibration/quant/llama-13b-hf-w4a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-23 08:56:08 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-23 08:56:08 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-23 08:56:09 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-23 08:56:11 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-23 08:57:03 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.018160667270421982 norm:0.002792862942442298 max memory_allocated 29226.177734375 
[2025-02-23 08:57:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.015460744500160217 norm:0.005862491670995951 max memory_allocated 29226.177734375 
[2025-02-23 08:58:39 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.011594997718930244 norm:0.003273583482950926 max memory_allocated 29226.177734375 
[2025-02-23 08:59:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.007936222478747368 norm:0.0006292072357609868 max memory_allocated 29226.177734375 
[2025-02-23 09:00:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.011463398113846779 norm:0.00320958299562335 max memory_allocated 29226.177734375 
[2025-02-23 09:01:05 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.01202054787427187 norm:0.004200157709419727 max memory_allocated 29226.177734375 
[2025-02-23 09:01:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.012273087166249752 norm:0.00336353643797338 max memory_allocated 29226.177734375 
[2025-02-23 09:02:42 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.011795416474342346 norm:0.002921814564615488 max memory_allocated 29226.177734375 
[2025-02-23 09:03:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.010991789400577545 norm:0.0026698866859078407 max memory_allocated 29226.177734375 
[2025-02-23 09:04:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.010674030520021915 norm:0.0022056610323488712 max memory_allocated 29226.177734375 
[2025-02-23 09:05:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.007309100590646267 norm:0.0010372396791353822 max memory_allocated 29226.177734375 
[2025-02-23 09:05:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.007208272814750671 norm:0.0006513679982163012 max memory_allocated 29226.177734375 
[2025-02-23 09:06:45 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.012746945023536682 norm:0.006084362044930458 max memory_allocated 29226.177734375 
[2025-02-23 09:07:34 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.011797391809523106 norm:0.0036217726301401854 max memory_allocated 29226.177734375 
[2025-02-23 09:08:23 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.0120686125010252 norm:0.0044824592769145966 max memory_allocated 29226.177734375 
[2025-02-23 09:09:11 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.010634577833116055 norm:0.00279877963475883 max memory_allocated 29226.177734375 
[2025-02-23 09:10:00 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.008823828771710396 norm:0.0019285969901829958 max memory_allocated 29226.177734375 
[2025-02-23 09:10:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.007289219181984663 norm:0.0005759000778198242 max memory_allocated 29226.177734375 
[2025-02-23 09:11:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.011871417984366417 norm:0.003149571130052209 max memory_allocated 29226.177734375 
[2025-02-23 09:12:26 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.013211659155786037 norm:0.006193028762936592 max memory_allocated 29226.177734375 
[2025-02-23 09:12:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-23 09:13:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.03909488767385483 norm:0.00729426508769393 max memory_allocated 29226.365234375 
[2025-02-23 09:14:21 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.02252061292529106 norm:0.0013011755654588342 max memory_allocated 29226.365234375 
[2025-02-23 09:15:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.019206009805202484 norm:0.0006472732056863606 max memory_allocated 29226.365234375 
[2025-02-23 09:15:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.017203422263264656 norm:0.0004219449474476278 max memory_allocated 29226.365234375 
[2025-02-23 09:16:48 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.016657957807183266 norm:0.0003664624528028071 max memory_allocated 29226.365234375 
[2025-02-23 09:17:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.017100263386964798 norm:0.0004031092394143343 max memory_allocated 29226.365234375 
[2025-02-23 09:18:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.016217239201068878 norm:0.0004100847290828824 max memory_allocated 29226.365234375 
[2025-02-23 09:19:14 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.01644066721200943 norm:0.00043290675967000425 max memory_allocated 29226.365234375 
[2025-02-23 09:20:03 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.016188273206353188 norm:0.00034255237551406026 max memory_allocated 29226.365234375 
[2025-02-23 09:20:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.01571729965507984 norm:0.00039287336403504014 max memory_allocated 29226.365234375 
[2025-02-23 09:21:40 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.01511652022600174 norm:0.00027921213768422604 max memory_allocated 29226.365234375 
[2025-02-23 09:22:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.01553608849644661 norm:0.00032654101960361004 max memory_allocated 29226.365234375 
[2025-02-23 09:23:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.016350410878658295 norm:0.0003410609206184745 max memory_allocated 29226.365234375 
[2025-02-23 09:24:07 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.018151383846998215 norm:0.0006152275600470603 max memory_allocated 29226.365234375 
[2025-02-23 09:24:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.016384722664952278 norm:0.0004266448668204248 max memory_allocated 29226.365234375 
[2025-02-23 09:25:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.014678800478577614 norm:0.00021706143161281943 max memory_allocated 29226.365234375 
[2025-02-23 09:26:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.016310520470142365 norm:0.0003598518669605255 max memory_allocated 29226.365234375 
[2025-02-23 09:27:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.01762573793530464 norm:0.0004604100249707699 max memory_allocated 29226.365234375 
[2025-02-23 09:28:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.016789181157946587 norm:0.00037686870200559497 max memory_allocated 29226.365234375 
[2025-02-23 09:28:59 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.017716437578201294 norm:0.000503792311064899 max memory_allocated 29226.365234375 
[2025-02-23 09:29:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-23 09:30:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.030565347522497177 norm:0.0008945363224484026 max memory_allocated 29226.552734375 
[2025-02-23 09:30:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.026467036455869675 norm:0.0005356527399271727 max memory_allocated 29226.552734375 
[2025-02-23 09:31:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.025101976469159126 norm:0.00043109318357892334 max memory_allocated 29226.552734375 
[2025-02-23 09:32:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.024615120142698288 norm:0.0004157100338488817 max memory_allocated 29226.552734375 
[2025-02-23 09:33:22 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.024585751816630363 norm:0.0004168224986642599 max memory_allocated 29226.552734375 
[2025-02-23 09:34:10 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.024625523015856743 norm:0.0004120429512113333 max memory_allocated 29226.552734375 
[2025-02-23 09:34:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.02471480332314968 norm:0.00043635221663862467 max memory_allocated 29226.552734375 
[2025-02-23 09:35:48 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.02472909539937973 norm:0.00044290267396718264 max memory_allocated 29226.552734375 
[2025-02-23 09:36:36 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.024661943316459656 norm:0.00042256584856659174 max memory_allocated 29226.552734375 
[2025-02-23 09:37:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.024693340063095093 norm:0.0004385657957755029 max memory_allocated 29226.552734375 
[2025-02-23 09:38:14 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.02436307817697525 norm:0.0003802041173912585 max memory_allocated 29226.552734375 
[2025-02-23 09:39:03 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.024887558072805405 norm:0.0004391489492263645 max memory_allocated 29226.552734375 
[2025-02-23 09:39:52 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.02495916746556759 norm:0.0005684065981768072 max memory_allocated 29226.552734375 
[2025-02-23 09:40:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.023799967020750046 norm:0.0003604667726904154 max memory_allocated 29226.552734375 
[2025-02-23 09:41:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.024267900735139847 norm:0.00036219405592419207 max memory_allocated 29226.552734375 
[2025-02-23 09:42:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.023850658908486366 norm:0.0003443320747464895 max memory_allocated 29226.552734375 
[2025-02-23 09:43:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.023984670639038086 norm:0.00035613469663076103 max memory_allocated 29226.552734375 
[2025-02-23 09:43:55 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.024192234501242638 norm:0.00037237670039758086 max memory_allocated 29226.552734375 
[2025-02-23 09:44:44 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.024177368730306625 norm:0.0003734963829629123 max memory_allocated 29226.552734375 
[2025-02-23 09:45:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.024450531229376793 norm:0.0004087318084202707 max memory_allocated 29226.552734375 
[2025-02-23 09:45:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-23 09:46:40 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.040183424949645996 norm:0.002368874615058303 max memory_allocated 29226.740234375 
[2025-02-23 09:47:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.03389719873666763 norm:0.000823632930405438 max memory_allocated 29226.740234375 
[2025-02-23 09:48:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.03170430660247803 norm:0.0005331783904694021 max memory_allocated 29226.740234375 
[2025-02-23 09:49:06 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.030581021681427956 norm:0.0003400864079594612 max memory_allocated 29226.740234375 
[2025-02-23 09:49:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.030349239706993103 norm:0.00023295445134863257 max memory_allocated 29226.740234375 
[2025-02-23 09:50:43 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.029907435178756714 norm:0.00015667849220335484 max memory_allocated 29226.740234375 
[2025-02-23 09:51:32 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.02970067597925663 norm:0.00014651179662905633 max memory_allocated 29226.740234375 
[2025-02-23 09:52:21 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.029838331043720245 norm:0.00011989413178525865 max memory_allocated 29226.740234375 
[2025-02-23 09:53:10 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.029933111742138863 norm:0.00011747219105018303 max memory_allocated 29226.740234375 
[2025-02-23 09:53:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.02991788275539875 norm:0.00010620893590385094 max memory_allocated 29226.740234375 
[2025-02-23 09:54:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.029898913577198982 norm:0.00010730724170571193 max memory_allocated 29226.740234375 
[2025-02-23 09:55:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0296732559800148 norm:9.99577168840915e-05 max memory_allocated 29226.740234375 
[2025-02-23 09:56:24 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.02986866794526577 norm:0.00010643476707627997 max memory_allocated 29226.740234375 
[2025-02-23 09:57:13 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.029969818890094757 norm:0.0001106666459236294 max memory_allocated 29226.740234375 
[2025-02-23 09:58:02 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.029828498139977455 norm:0.00010588687291601673 max memory_allocated 29226.740234375 
[2025-02-23 09:58:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.03015056625008583 norm:0.00011529440234880894 max memory_allocated 29226.740234375 
[2025-02-23 09:59:39 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.029934387654066086 norm:0.00011025289131794125 max memory_allocated 29226.740234375 
[2025-02-23 10:00:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.02996031381189823 norm:0.0001104520633816719 max memory_allocated 29226.740234375 
[2025-02-23 10:01:16 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.030016779899597168 norm:0.00011121496936539188 max memory_allocated 29226.740234375 
[2025-02-23 10:02:05 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.030012326315045357 norm:0.00011151793296448886 max memory_allocated 29226.740234375 
[2025-02-23 10:02:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-23 10:03:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.04928400740027428 norm:0.005663265939801931 max memory_allocated 29226.927734375 
[2025-02-23 10:04:01 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.041392892599105835 norm:0.0018590932013466954 max memory_allocated 29226.927734375 
[2025-02-23 10:04:49 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0391806960105896 norm:0.0010293764062225819 max memory_allocated 29226.927734375 
[2025-02-23 10:05:38 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.03819180652499199 norm:0.0007359517039731145 max memory_allocated 29226.927734375 
[2025-02-23 10:06:27 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.037541113793849945 norm:0.0005858509102836251 max memory_allocated 29226.927734375 
[2025-02-23 10:07:16 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.03689863160252571 norm:0.00040467505459673703 max memory_allocated 29226.927734375 
[2025-02-23 10:08:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.03693550080060959 norm:0.0002457216614857316 max memory_allocated 29226.927734375 
[2025-02-23 10:08:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0371038056910038 norm:0.00021128983644302934 max memory_allocated 29226.927734375 
[2025-02-23 10:09:41 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.037154171615839005 norm:0.00017171319632325321 max memory_allocated 29226.927734375 
[2025-02-23 10:10:30 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.03711996227502823 norm:0.00015505315968766809 max memory_allocated 29226.927734375 
[2025-02-23 10:11:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.03700828552246094 norm:0.00014881626702845097 max memory_allocated 29226.927734375 
[2025-02-23 10:12:08 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.03681372478604317 norm:0.00014374015154317021 max memory_allocated 29226.927734375 
[2025-02-23 10:12:56 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.036748725920915604 norm:0.00013987028796691447 max memory_allocated 29226.927734375 
[2025-02-23 10:13:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.036698583513498306 norm:0.0001335252309218049 max memory_allocated 29226.927734375 
[2025-02-23 10:14:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.036547426134347916 norm:0.00012787932064384222 max memory_allocated 29226.927734375 
[2025-02-23 10:15:22 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.03665854409337044 norm:0.00012815382797271013 max memory_allocated 29226.927734375 
[2025-02-23 10:16:11 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.03678407147526741 norm:0.00012990107643418014 max memory_allocated 29226.927734375 
[2025-02-23 10:17:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.03667370602488518 norm:0.0001279328134842217 max memory_allocated 29226.927734375 
[2025-02-23 10:17:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.036603547632694244 norm:0.00012555127614177763 max memory_allocated 29226.927734375 
[2025-02-23 10:18:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.03653360903263092 norm:0.00012515949492808431 max memory_allocated 29226.927734375 
[2025-02-23 10:18:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-23 10:19:44 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.056271180510520935 norm:0.004273668862879276 max memory_allocated 29227.115234375 
[2025-02-23 10:20:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.04666462540626526 norm:0.0013778379652649164 max memory_allocated 29227.115234375 
[2025-02-23 10:21:21 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.04385857656598091 norm:0.0007791667012497783 max memory_allocated 29227.115234375 
[2025-02-23 10:22:10 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.042554162442684174 norm:0.0005918998504057527 max memory_allocated 29227.115234375 
[2025-02-23 10:22:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.04162388667464256 norm:0.00039738943451084197 max memory_allocated 29227.115234375 
[2025-02-23 10:23:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.041200704872608185 norm:0.00038164539728313684 max memory_allocated 29227.115234375 
[2025-02-23 10:24:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.04065653681755066 norm:0.00026521331164985895 max memory_allocated 29227.115234375 
[2025-02-23 10:25:24 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.04064926132559776 norm:0.0002055716176982969 max memory_allocated 29227.115234375 
[2025-02-23 10:26:12 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.04106065630912781 norm:0.00018553895642980933 max memory_allocated 29227.115234375 
[2025-02-23 10:27:01 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.04065430164337158 norm:0.0001600035757292062 max memory_allocated 29227.115234375 
[2025-02-23 10:27:50 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.040472205728292465 norm:0.0001491466537117958 max memory_allocated 29227.115234375 
[2025-02-23 10:28:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.040622517466545105 norm:0.00014807448314968497 max memory_allocated 29227.115234375 
[2025-02-23 10:29:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.04047581925988197 norm:0.00013203814160078764 max memory_allocated 29227.115234375 
[2025-02-23 10:30:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.040274396538734436 norm:0.00011920779797947034 max memory_allocated 29227.115234375 
[2025-02-23 10:31:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.04023218899965286 norm:0.00011073598579969257 max memory_allocated 29227.115234375 
[2025-02-23 10:31:52 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.04042888805270195 norm:0.00011706905934261158 max memory_allocated 29227.115234375 
[2025-02-23 10:32:41 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.04018303379416466 norm:0.00011517528764670715 max memory_allocated 29227.115234375 
[2025-02-23 10:33:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.04039047285914421 norm:0.00011929853644687682 max memory_allocated 29227.115234375 
[2025-02-23 10:34:18 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.040341321378946304 norm:0.00011730271216947585 max memory_allocated 29227.115234375 
[2025-02-23 10:35:07 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.040428172796964645 norm:0.00011989980703219771 max memory_allocated 29227.115234375 
[2025-02-23 10:35:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-23 10:36:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.06314702332019806 norm:0.0011584139429032803 max memory_allocated 29227.302734375 
[2025-02-23 10:37:02 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.05481601879000664 norm:0.0005519762053154409 max memory_allocated 29227.302734375 
[2025-02-23 10:37:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.051513731479644775 norm:0.0003604876110330224 max memory_allocated 29227.302734375 
[2025-02-23 10:38:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.05019034817814827 norm:0.00027635955484583974 max memory_allocated 29227.302734375 
[2025-02-23 10:39:27 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.04928036779165268 norm:0.0002151499647879973 max memory_allocated 29227.302734375 
[2025-02-23 10:40:16 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.04899459704756737 norm:0.00016830941603984684 max memory_allocated 29227.302734375 
[2025-02-23 10:41:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.04904703050851822 norm:0.00022600858937948942 max memory_allocated 29227.302734375 
[2025-02-23 10:41:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.04837873578071594 norm:0.0001243713777512312 max memory_allocated 29227.302734375 
[2025-02-23 10:42:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.0482490137219429 norm:0.00011607148917391896 max memory_allocated 29227.302734375 
[2025-02-23 10:43:30 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0481983944773674 norm:0.00012192440044600517 max memory_allocated 29227.302734375 
[2025-02-23 10:44:18 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.04832248017191887 norm:0.00013830166426487267 max memory_allocated 29227.302734375 
[2025-02-23 10:45:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.04809349775314331 norm:0.00011648639338091016 max memory_allocated 29227.302734375 
[2025-02-23 10:45:55 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.04814613610506058 norm:0.00011450445163063705 max memory_allocated 29227.302734375 
[2025-02-23 10:46:44 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.04820550978183746 norm:0.00011635071859927848 max memory_allocated 29227.302734375 
[2025-02-23 10:47:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.048241984099149704 norm:0.00012410178896971047 max memory_allocated 29227.302734375 
[2025-02-23 10:48:21 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.048325903713703156 norm:0.00011936960800085217 max memory_allocated 29227.302734375 
[2025-02-23 10:49:09 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0481792651116848 norm:0.0001161509717348963 max memory_allocated 29227.302734375 
[2025-02-23 10:49:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.048163045197725296 norm:0.00012170933041488752 max memory_allocated 29227.302734375 
[2025-02-23 10:50:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.048300981521606445 norm:0.0001379956811433658 max memory_allocated 29227.302734375 
[2025-02-23 10:51:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.04826873540878296 norm:0.0001238016993738711 max memory_allocated 29227.302734375 
[2025-02-23 10:51:49 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-23 10:52:41 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.06237278878688812 norm:0.0004842813068535179 max memory_allocated 29227.490234375 
[2025-02-23 10:53:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.05699307098984718 norm:0.00024846618180163205 max memory_allocated 29227.490234375 
[2025-02-23 10:54:18 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.05468595027923584 norm:0.00017309535178355873 max memory_allocated 29227.490234375 
[2025-02-23 10:55:07 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.05368248373270035 norm:0.00013767497148364782 max memory_allocated 29227.490234375 
[2025-02-23 10:55:55 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0530373677611351 norm:0.00011479890235932544 max memory_allocated 29227.490234375 
[2025-02-23 10:56:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.052680306136608124 norm:9.613017755327746e-05 max memory_allocated 29227.490234375 
[2025-02-23 10:57:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.05252407118678093 norm:8.904921560315415e-05 max memory_allocated 29227.490234375 
[2025-02-23 10:58:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.05246491730213165 norm:8.331635763170198e-05 max memory_allocated 29227.490234375 
[2025-02-23 10:59:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.052393458783626556 norm:8.166076440829784e-05 max memory_allocated 29227.490234375 
[2025-02-23 10:59:58 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.052430879324674606 norm:8.140097634168342e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:00:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.05239556357264519 norm:8.200194133678451e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:01:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.05235816538333893 norm:8.089744369499385e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:02:23 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.05238104984164238 norm:8.209898805944249e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:03:12 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.05241820216178894 norm:8.281480404548347e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:04:00 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.05250537768006325 norm:8.504127617925406e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:04:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.05252506956458092 norm:8.6487882072106e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:05:38 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0525355190038681 norm:8.658399747218937e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:06:26 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.05255930498242378 norm:8.68464630912058e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:07:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.05247991159558296 norm:8.401178638450801e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:08:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.05245070159435272 norm:8.437377982772887e-05 max memory_allocated 29227.490234375 
[2025-02-23 11:08:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-23 11:09:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.06918255239725113 norm:0.0007013689610175788 max memory_allocated 29227.677734375 
[2025-02-23 11:09:58 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0625687763094902 norm:0.0003594435111153871 max memory_allocated 29227.677734375 
[2025-02-23 11:10:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06004394218325615 norm:0.0002548268239479512 max memory_allocated 29227.677734375 
[2025-02-23 11:11:35 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.05885842442512512 norm:0.0002082132559735328 max memory_allocated 29227.677734375 
[2025-02-23 11:12:24 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.05825651064515114 norm:0.00016511583817191422 max memory_allocated 29227.677734375 
[2025-02-23 11:13:12 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.05779581144452095 norm:0.00014178991841617972 max memory_allocated 29227.677734375 
[2025-02-23 11:14:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.057541318237781525 norm:0.00012212080764584243 max memory_allocated 29227.677734375 
[2025-02-23 11:14:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.05728290230035782 norm:0.00011197868298040703 max memory_allocated 29227.677734375 
[2025-02-23 11:15:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.05722866952419281 norm:0.00010110426956089213 max memory_allocated 29227.677734375 
[2025-02-23 11:16:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.057218149304389954 norm:9.388333273818716e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:17:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.05727224797010422 norm:9.249793947674334e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:18:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.057438530027866364 norm:9.411943756276742e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:18:51 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.05754106491804123 norm:9.825055894907564e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:19:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0576135627925396 norm:9.999879694078118e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:20:28 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.057657621800899506 norm:0.00010029035911429673 max memory_allocated 29227.677734375 
[2025-02-23 11:21:17 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.057609785348176956 norm:9.999257599702105e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:22:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.05759134143590927 norm:9.910953667713329e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:22:54 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.05754230171442032 norm:9.646006947150454e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:23:43 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.0575299970805645 norm:9.929143561748788e-05 max memory_allocated 29227.677734375 
[2025-02-23 11:24:31 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.057550374418497086 norm:0.00010114448377862573 max memory_allocated 29227.677734375 
[2025-02-23 11:24:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-23 11:25:38 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.07243330031633377 norm:0.0005545462481677532 max memory_allocated 29227.865234375 
[2025-02-23 11:26:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.06727153807878494 norm:0.0003056009591091424 max memory_allocated 29227.865234375 
[2025-02-23 11:27:14 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0650787204504013 norm:0.00022911072301212698 max memory_allocated 29227.865234375 
[2025-02-23 11:28:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.06403214484453201 norm:0.0001872381108114496 max memory_allocated 29227.865234375 
[2025-02-23 11:28:51 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.06320635974407196 norm:0.00015664332022424787 max memory_allocated 29227.865234375 
[2025-02-23 11:29:39 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0628509670495987 norm:0.0001326519122812897 max memory_allocated 29227.865234375 
[2025-02-23 11:30:28 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.06265006214380264 norm:0.00011449455632828176 max memory_allocated 29227.865234375 
[2025-02-23 11:31:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.06253233551979065 norm:0.00010255313100060448 max memory_allocated 29227.865234375 
[2025-02-23 11:32:05 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.06259557604789734 norm:9.806459274841473e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:32:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.06261450052261353 norm:9.242715168511495e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:33:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.06267158687114716 norm:9.157189197139814e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:34:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.06292306631803513 norm:9.564064384903759e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:35:18 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.06245129927992821 norm:8.490836626151577e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:36:07 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.062172550708055496 norm:7.849212852306664e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:36:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.06256163120269775 norm:8.381351653952152e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:37:44 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.06262204051017761 norm:8.434374467469752e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:38:32 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.06241060048341751 norm:7.804477354511619e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:39:20 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06215101107954979 norm:7.249870395753533e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:40:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.06243034079670906 norm:7.750342774670571e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:40:58 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.06290990859270096 norm:8.78825449035503e-05 max memory_allocated 29227.865234375 
[2025-02-23 11:41:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-23 11:42:04 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.07743962109088898 norm:0.0005935649969615042 max memory_allocated 29228.052734375 
[2025-02-23 11:42:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.07207717001438141 norm:0.00031022101757116616 max memory_allocated 29228.052734375 
[2025-02-23 11:43:42 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.07015842944383621 norm:0.0002380838559474796 max memory_allocated 29228.052734375 
[2025-02-23 11:44:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.06892194598913193 norm:0.0001878508774098009 max memory_allocated 29228.052734375 
[2025-02-23 11:45:18 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.06825637817382812 norm:0.0001573117624502629 max memory_allocated 29228.052734375 
[2025-02-23 11:46:07 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.06792858242988586 norm:0.00013486928946804255 max memory_allocated 29228.052734375 
[2025-02-23 11:46:55 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.06764151155948639 norm:0.0001169130191556178 max memory_allocated 29228.052734375 
[2025-02-23 11:47:44 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.06760112941265106 norm:0.00010411636321805418 max memory_allocated 29228.052734375 
[2025-02-23 11:48:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.06738296896219254 norm:9.171888086711988e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:49:21 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0673103928565979 norm:8.480474207317457e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:50:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.06728692352771759 norm:7.889300468377769e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:50:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.06726683676242828 norm:7.336206908803433e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:51:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.06727693974971771 norm:6.837688852101564e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:52:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.06724225729703903 norm:6.493283581221476e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:53:23 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.06725168228149414 norm:6.364373257383704e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:54:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.06727412343025208 norm:6.341760308714584e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:55:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.06734289973974228 norm:6.38714263914153e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:55:48 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.06731627881526947 norm:6.34318494121544e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:56:37 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.06730730086565018 norm:6.301024404820055e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:57:25 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.06736910343170166 norm:6.327919254545122e-05 max memory_allocated 29228.052734375 
[2025-02-23 11:57:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-23 11:58:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.08394436538219452 norm:0.0005787624977529049 max memory_allocated 29228.240234375 
[2025-02-23 11:59:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.07869521528482437 norm:0.00036207010271027684 max memory_allocated 29228.240234375 
[2025-02-23 12:00:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.07630109786987305 norm:0.0002723312936723232 max memory_allocated 29228.240234375 
[2025-02-23 12:00:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.07511678338050842 norm:0.00020925092394463718 max memory_allocated 29228.240234375 
[2025-02-23 12:01:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.07447261363267899 norm:0.00017472347826696932 max memory_allocated 29228.240234375 
[2025-02-23 12:02:34 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.07412821054458618 norm:0.00015267670096363872 max memory_allocated 29228.240234375 
[2025-02-23 12:03:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.07377533614635468 norm:0.0001302648743148893 max memory_allocated 29228.240234375 
[2025-02-23 12:04:11 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0734824538230896 norm:0.00011255883146077394 max memory_allocated 29228.240234375 
[2025-02-23 12:04:59 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.07329525053501129 norm:0.0001020043418975547 max memory_allocated 29228.240234375 
[2025-02-23 12:05:48 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.07317036390304565 norm:9.391723870066926e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:06:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.07323102653026581 norm:8.345876267412677e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:07:25 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.07325664162635803 norm:7.833239942556247e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:08:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.07312598079442978 norm:7.307433406822383e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:09:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.07311119884252548 norm:7.152740727178752e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:09:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0732656940817833 norm:6.978197052376345e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:10:38 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0732722282409668 norm:6.904629844939336e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:11:27 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.07327326387166977 norm:6.8093795562163e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:12:15 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.07328244298696518 norm:6.851217767689377e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:13:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.07331038266420364 norm:6.833776569692418e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:13:53 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.07329767197370529 norm:6.849790952401236e-05 max memory_allocated 29228.240234375 
[2025-02-23 12:14:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-23 12:15:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.08876819163560867 norm:0.0004603777197189629 max memory_allocated 29228.427734375 
[2025-02-23 12:15:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.08406846225261688 norm:0.0002929886104539037 max memory_allocated 29228.427734375 
[2025-02-23 12:16:37 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.08189420402050018 norm:0.00022209537564776838 max memory_allocated 29228.427734375 
[2025-02-23 12:17:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.0808749571442604 norm:0.00018205110973212868 max memory_allocated 29228.427734375 
[2025-02-23 12:18:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.08014333993196487 norm:0.00015338064986281097 max memory_allocated 29228.427734375 
[2025-02-23 12:19:02 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.07967212051153183 norm:0.0001319694856647402 max memory_allocated 29228.427734375 
[2025-02-23 12:19:51 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.07943210005760193 norm:0.00011239719606237486 max memory_allocated 29228.427734375 
[2025-02-23 12:20:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.07947113364934921 norm:0.00011019138037227094 max memory_allocated 29228.427734375 
[2025-02-23 12:21:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.07927077263593674 norm:0.00010738839046098292 max memory_allocated 29228.427734375 
[2025-02-23 12:22:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.07917901873588562 norm:9.952375694410875e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:23:05 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.07930045574903488 norm:9.609982225811109e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:23:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0793512761592865 norm:9.183103975374252e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:24:42 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.07936187088489532 norm:9.124905045609921e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:25:31 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.07923037558794022 norm:8.797119517112151e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:26:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.07913215458393097 norm:8.626017370261252e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:27:08 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.07913804054260254 norm:8.674145647091791e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:27:57 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.07918426394462585 norm:8.746605453779921e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:28:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.07921433448791504 norm:8.85054178070277e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:29:34 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.07924304157495499 norm:8.997801342047751e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:30:22 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0791991576552391 norm:8.951495692599565e-05 max memory_allocated 29228.427734375 
[2025-02-23 12:30:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-23 12:31:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.09432859718799591 norm:0.00044530193554237485 max memory_allocated 29228.615234375 
[2025-02-23 12:32:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.08938536047935486 norm:0.0002897254889830947 max memory_allocated 29228.615234375 
[2025-02-23 12:33:06 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.08715525269508362 norm:0.00022273075592238456 max memory_allocated 29228.615234375 
[2025-02-23 12:33:54 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.08582586795091629 norm:0.0001792456750990823 max memory_allocated 29228.615234375 
[2025-02-23 12:34:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.08510684967041016 norm:0.00015188151155598462 max memory_allocated 29228.615234375 
[2025-02-23 12:35:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.08464381098747253 norm:0.00013339673751033843 max memory_allocated 29228.615234375 
[2025-02-23 12:36:20 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.08438189327716827 norm:0.00011213046673219651 max memory_allocated 29228.615234375 
[2025-02-23 12:37:08 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0842597484588623 norm:0.00010166218999074772 max memory_allocated 29228.615234375 
[2025-02-23 12:37:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.08420071005821228 norm:9.352850611321628e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:38:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.08406828343868256 norm:8.947795868152753e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:39:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.08393895626068115 norm:8.707780216354877e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:40:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.08389344811439514 norm:8.723302016733214e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:41:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.08393404632806778 norm:8.633558172732592e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:41:59 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.08412807434797287 norm:8.566327596781775e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:42:48 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.08405665308237076 norm:8.399621583521366e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:43:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.08399132639169693 norm:8.429669833276421e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:44:25 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.08389774709939957 norm:8.258820162154734e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:45:13 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.0838879719376564 norm:8.25825918582268e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:46:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.08388924598693848 norm:8.292075654026121e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:46:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.08392278850078583 norm:8.337017789017409e-05 max memory_allocated 29228.615234375 
[2025-02-23 12:47:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-23 12:47:57 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.10082168877124786 norm:0.0005710317636840045 max memory_allocated 29228.802734375 
[2025-02-23 12:48:45 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.09544163197278976 norm:0.00033983762841671705 max memory_allocated 29228.802734375 
[2025-02-23 12:49:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.09302576631307602 norm:0.00024394571664743125 max memory_allocated 29228.802734375 
[2025-02-23 12:50:22 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.09159757941961288 norm:0.00019164159311912954 max memory_allocated 29228.802734375 
[2025-02-23 12:51:11 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.09064905345439911 norm:0.00015418363909702748 max memory_allocated 29228.802734375 
[2025-02-23 12:51:59 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.09014780074357986 norm:0.00013007600500714034 max memory_allocated 29228.802734375 
[2025-02-23 12:52:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.08995524048805237 norm:0.00011760385677916929 max memory_allocated 29228.802734375 
[2025-02-23 12:53:36 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.0898258313536644 norm:0.00011242365872021765 max memory_allocated 29228.802734375 
[2025-02-23 12:54:25 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.08958977460861206 norm:0.00010576075874269009 max memory_allocated 29228.802734375 
[2025-02-23 12:55:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.08937495201826096 norm:9.735218191053718e-05 max memory_allocated 29228.802734375 
[2025-02-23 12:56:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.08927087485790253 norm:9.328492888016626e-05 max memory_allocated 29228.802734375 
[2025-02-23 12:56:50 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.08907788246870041 norm:8.957368845585734e-05 max memory_allocated 29228.802734375 
[2025-02-23 12:57:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.08904271572828293 norm:8.944142609834671e-05 max memory_allocated 29228.802734375 
[2025-02-23 12:58:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.08916231244802475 norm:9.010219946503639e-05 max memory_allocated 29228.802734375 
[2025-02-23 12:59:16 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.08928459882736206 norm:9.382278949487954e-05 max memory_allocated 29228.802734375 
[2025-02-23 13:00:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.08928427845239639 norm:9.140657493844628e-05 max memory_allocated 29228.802734375 
[2025-02-23 13:00:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0892823338508606 norm:9.288728324463591e-05 max memory_allocated 29228.802734375 
[2025-02-23 13:01:41 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.08932995796203613 norm:9.385502198711038e-05 max memory_allocated 29228.802734375 
[2025-02-23 13:02:30 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.08946957439184189 norm:9.664316166890785e-05 max memory_allocated 29228.802734375 
[2025-02-23 13:03:18 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.08954568952322006 norm:9.907345520332456e-05 max memory_allocated 29228.802734375 
[2025-02-23 13:03:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-23 13:04:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.10736192762851715 norm:0.0005710284458473325 max memory_allocated 29228.990234375 
[2025-02-23 13:05:13 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.10216424614191055 norm:0.00028866907814517617 max memory_allocated 29228.990234375 
[2025-02-23 13:06:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.09995872527360916 norm:0.00022253037604968995 max memory_allocated 29228.990234375 
[2025-02-23 13:06:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.09866039454936981 norm:0.0001799418096197769 max memory_allocated 29228.990234375 
[2025-02-23 13:07:39 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0978303998708725 norm:0.00015179124602582306 max memory_allocated 29228.990234375 
[2025-02-23 13:08:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0973096638917923 norm:0.00012945765047334135 max memory_allocated 29228.990234375 
[2025-02-23 13:09:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.09698624908924103 norm:0.00011729708057828248 max memory_allocated 29228.990234375 
[2025-02-23 13:10:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.09688471257686615 norm:0.00010954701429000124 max memory_allocated 29228.990234375 
[2025-02-23 13:10:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.09672669321298599 norm:0.0001023109070956707 max memory_allocated 29228.990234375 
[2025-02-23 13:11:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.09661763906478882 norm:9.543129272060469e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:12:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.09651270508766174 norm:9.241025691153482e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:13:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.09646409004926682 norm:9.151046833721921e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:14:06 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.09647510945796967 norm:9.133965795626864e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:14:55 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.09658412635326385 norm:9.341312397737056e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:15:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.09648101776838303 norm:9.178812615573406e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:16:32 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.09641061723232269 norm:9.070463420357555e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:17:21 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.09632217884063721 norm:9.031716035678983e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:18:09 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.09617575258016586 norm:8.623317262390628e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:18:57 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.09616374969482422 norm:8.718975732335821e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:19:46 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.09620027244091034 norm:8.839597285259515e-05 max memory_allocated 29228.990234375 
[2025-02-23 13:20:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-23 13:20:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.11170355975627899 norm:0.00033438511309213936 max memory_allocated 29229.177734375 
[2025-02-23 13:21:41 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.10807590931653976 norm:0.00019777752459049225 max memory_allocated 29229.177734375 
[2025-02-23 13:22:30 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.10626758635044098 norm:0.00015058477583806962 max memory_allocated 29229.177734375 
[2025-02-23 13:23:18 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.10517728328704834 norm:0.00012283438991289586 max memory_allocated 29229.177734375 
[2025-02-23 13:24:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.10438699275255203 norm:0.00010574222687864676 max memory_allocated 29229.177734375 
[2025-02-23 13:24:56 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.10395988821983337 norm:9.549395326757804e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:25:44 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.1037055104970932 norm:8.871511818142608e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:26:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.10359111428260803 norm:8.487234299536794e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:27:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.10364564508199692 norm:8.55529069667682e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:28:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.10363513976335526 norm:8.466511644655839e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:28:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.10356327146291733 norm:8.36043618619442e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:29:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.1035531610250473 norm:8.329873526236042e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:30:35 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.10344831645488739 norm:8.218382572522387e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:31:24 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.1033993661403656 norm:8.189051004592329e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:32:12 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.10336440801620483 norm:8.158604759955779e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:33:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.10335057973861694 norm:8.176309347618371e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:33:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.10332818329334259 norm:8.146686741383746e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:34:37 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.10331464558839798 norm:8.151743531925604e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:35:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.10329766571521759 norm:8.089370385278016e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:36:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.10328231006860733 norm:8.077066013356671e-05 max memory_allocated 29229.177734375 
[2025-02-23 13:36:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-23 13:37:21 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.12300750613212585 norm:0.00037372109363786876 max memory_allocated 29229.365234375 
[2025-02-23 13:38:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.11855058372020721 norm:0.00023393890296574682 max memory_allocated 29229.365234375 
[2025-02-23 13:38:58 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.11626823246479034 norm:0.00018189678667113185 max memory_allocated 29229.365234375 
[2025-02-23 13:39:46 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.11487720161676407 norm:0.00015183057985268533 max memory_allocated 29229.365234375 
[2025-02-23 13:40:34 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.11405861377716064 norm:0.0001288472703890875 max memory_allocated 29229.365234375 
[2025-02-23 13:41:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.11348126083612442 norm:0.0001139929227065295 max memory_allocated 29229.365234375 
[2025-02-23 13:42:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.11307269334793091 norm:0.00010113566531799734 max memory_allocated 29229.365234375 
[2025-02-23 13:42:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.11285371333360672 norm:9.282835526391864e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:43:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.11269757896661758 norm:8.972165960585698e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:44:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.11248552054166794 norm:8.73556055012159e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:45:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.11220386624336243 norm:8.434687333647162e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:46:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.11218228936195374 norm:8.362063817912713e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:47:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.11202175170183182 norm:8.033726771827787e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:47:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.11198678612709045 norm:7.938783528516069e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:48:39 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.11190453916788101 norm:7.880881457822397e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:49:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.11192906647920609 norm:7.875472510932013e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:50:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.11190915107727051 norm:7.883639045758173e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:51:05 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.11191613972187042 norm:7.934789755381644e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:51:53 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.11194202303886414 norm:7.90925623732619e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:52:41 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.11192411929368973 norm:7.92524588177912e-05 max memory_allocated 29229.365234375 
[2025-02-23 13:52:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-23 13:53:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.13577024638652802 norm:0.0005773046868853271 max memory_allocated 29229.552734375 
[2025-02-23 13:54:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.13112184405326843 norm:0.0003679012879729271 max memory_allocated 29229.552734375 
[2025-02-23 13:55:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.12874363362789154 norm:0.00027444888837635517 max memory_allocated 29229.552734375 
[2025-02-23 13:56:13 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.12716610729694366 norm:0.00021725479746237397 max memory_allocated 29229.552734375 
[2025-02-23 13:57:02 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.12609562277793884 norm:0.00017940931138582528 max memory_allocated 29229.552734375 
[2025-02-23 13:57:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.12549573183059692 norm:0.0001540678640594706 max memory_allocated 29229.552734375 
[2025-02-23 13:58:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.12505100667476654 norm:0.00013030378613620996 max memory_allocated 29229.552734375 
[2025-02-23 13:59:27 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.12472042441368103 norm:0.00011483500566100702 max memory_allocated 29229.552734375 
[2025-02-23 14:00:16 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.12463396042585373 norm:0.00010638320964062586 max memory_allocated 29229.552734375 
[2025-02-23 14:01:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.12441465258598328 norm:9.976395085686818e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:01:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.12425047904253006 norm:9.422080620424822e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:02:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.12403741478919983 norm:9.0220601123292e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:03:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.12391628324985504 norm:8.677082951180637e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:04:18 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.12384507060050964 norm:8.462758705718443e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:05:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.12391220778226852 norm:8.48635972943157e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:05:55 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.12387224286794662 norm:8.354091551154852e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:06:43 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.12382615357637405 norm:8.284119394375011e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:07:32 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.12389317154884338 norm:8.303773211082444e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:08:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.12391680479049683 norm:8.339381020050496e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:09:09 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.12393295019865036 norm:8.345632522832602e-05 max memory_allocated 29229.552734375 
[2025-02-23 14:09:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-23 14:10:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.14962926506996155 norm:0.0004084355023223907 max memory_allocated 29229.740234375 
[2025-02-23 14:11:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.14558717608451843 norm:0.00026883307145908475 max memory_allocated 29229.740234375 
[2025-02-23 14:11:53 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.14333826303482056 norm:0.0002083943982142955 max memory_allocated 29229.740234375 
[2025-02-23 14:12:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.1417943239212036 norm:0.00017276399012189358 max memory_allocated 29229.740234375 
[2025-02-23 14:13:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.1407748907804489 norm:0.00014739541802555323 max memory_allocated 29229.740234375 
[2025-02-23 14:14:18 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.14003634452819824 norm:0.000129433159600012 max memory_allocated 29229.740234375 
[2025-02-23 14:15:07 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.13944950699806213 norm:0.00011481631372589618 max memory_allocated 29229.740234375 
[2025-02-23 14:15:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.13907381892204285 norm:0.00010476823808858171 max memory_allocated 29229.740234375 
[2025-02-23 14:16:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.13884587585926056 norm:9.949044761015102e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:17:33 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.138631671667099 norm:9.52174814301543e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:18:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.13853870332241058 norm:9.294032497564331e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:19:09 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.13839183747768402 norm:8.984660962596536e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:19:58 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.13829056918621063 norm:8.796065230853856e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:20:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.1381894201040268 norm:8.689719106769189e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:21:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.13809029757976532 norm:8.583321323385462e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:22:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.13799497485160828 norm:8.434705523541197e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:23:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.13797688484191895 norm:8.43663074192591e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:24:00 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.13796547055244446 norm:8.37487677927129e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:24:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.13796329498291016 norm:8.384014654438943e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:25:38 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.13793286681175232 norm:8.321140194311738e-05 max memory_allocated 29229.740234375 
[2025-02-23 14:25:52 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-23 14:26:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.16721677780151367 norm:0.000681548728607595 max memory_allocated 29229.927734375 
[2025-02-23 14:27:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.16195330023765564 norm:0.0004182337725069374 max memory_allocated 29229.927734375 
[2025-02-23 14:28:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.15907487273216248 norm:0.00031097716419026256 max memory_allocated 29229.927734375 
[2025-02-23 14:29:09 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.15711960196495056 norm:0.00024352309992536902 max memory_allocated 29229.927734375 
[2025-02-23 14:29:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.15568219125270844 norm:0.00020239342120476067 max memory_allocated 29229.927734375 
[2025-02-23 14:30:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.15473787486553192 norm:0.00017194183601532131 max memory_allocated 29229.927734375 
[2025-02-23 14:31:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.15409788489341736 norm:0.00014958377869334072 max memory_allocated 29229.927734375 
[2025-02-23 14:32:23 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.15360702574253082 norm:0.0001309052313445136 max memory_allocated 29229.927734375 
[2025-02-23 14:33:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.15322433412075043 norm:0.00011875761265400797 max memory_allocated 29229.927734375 
[2025-02-23 14:34:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.1530245691537857 norm:0.00011141738650621846 max memory_allocated 29229.927734375 
[2025-02-23 14:34:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.1528511494398117 norm:0.0001038219197653234 max memory_allocated 29229.927734375 
[2025-02-23 14:35:37 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.15269646048545837 norm:9.920067532220855e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:36:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.15258248150348663 norm:9.559616592014208e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:37:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.15249256789684296 norm:9.315714851254597e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:38:02 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.15243364870548248 norm:9.12576651899144e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:38:50 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.15241995453834534 norm:9.032876550918445e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:39:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.15235163271427155 norm:8.837938366923481e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:40:28 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.15235303342342377 norm:8.843326941132545e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:41:16 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.15235696732997894 norm:8.839851943776011e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:42:05 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.15232893824577332 norm:8.748145046411082e-05 max memory_allocated 29229.927734375 
[2025-02-23 14:42:19 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-23 14:43:12 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.18901148438453674 norm:0.0006720868404954672 max memory_allocated 29230.115234375 
[2025-02-23 14:44:01 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.1832801252603531 norm:0.0003338923561386764 max memory_allocated 29230.115234375 
[2025-02-23 14:44:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.1803269386291504 norm:0.0002556063700467348 max memory_allocated 29230.115234375 
[2025-02-23 14:45:37 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.17824523150920868 norm:0.00021215726155787706 max memory_allocated 29230.115234375 
[2025-02-23 14:46:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.17675518989562988 norm:0.0001890963176265359 max memory_allocated 29230.115234375 
[2025-02-23 14:47:14 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.17567244172096252 norm:0.0001689123164396733 max memory_allocated 29230.115234375 
[2025-02-23 14:48:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.17487458884716034 norm:0.00015023227024357766 max memory_allocated 29230.115234375 
[2025-02-23 14:48:51 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.17423026263713837 norm:0.0001373681443510577 max memory_allocated 29230.115234375 
[2025-02-23 14:49:40 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.1737585961818695 norm:0.0001296373811783269 max memory_allocated 29230.115234375 
[2025-02-23 14:50:28 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.17332717776298523 norm:0.00011955993977608159 max memory_allocated 29230.115234375 
[2025-02-23 14:51:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.17299453914165497 norm:0.00011188831558683887 max memory_allocated 29230.115234375 
[2025-02-23 14:52:05 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.17277446389198303 norm:0.00010714792733779177 max memory_allocated 29230.115234375 
[2025-02-23 14:52:54 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.1726052612066269 norm:0.00010287692566635087 max memory_allocated 29230.115234375 
[2025-02-23 14:53:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.17241567373275757 norm:9.973366104532033e-05 max memory_allocated 29230.115234375 
[2025-02-23 14:54:31 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.1722574532032013 norm:9.687153942650184e-05 max memory_allocated 29230.115234375 
[2025-02-23 14:55:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.17218655347824097 norm:9.603138460079208e-05 max memory_allocated 29230.115234375 
[2025-02-23 14:56:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.17213237285614014 norm:9.42642436712049e-05 max memory_allocated 29230.115234375 
[2025-02-23 14:56:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.1720893234014511 norm:9.371228952659294e-05 max memory_allocated 29230.115234375 
[2025-02-23 14:57:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.172082781791687 norm:9.327329462394118e-05 max memory_allocated 29230.115234375 
[2025-02-23 14:58:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.17208385467529297 norm:9.333628986496478e-05 max memory_allocated 29230.115234375 
[2025-02-23 14:58:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-23 14:59:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.21372732520103455 norm:0.0005566601757891476 max memory_allocated 29230.302734375 
[2025-02-23 15:00:28 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.20832866430282593 norm:0.00038290699012577534 max memory_allocated 29230.302734375 
[2025-02-23 15:01:17 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.2052077203989029 norm:0.00030154112027958035 max memory_allocated 29230.302734375 
[2025-02-23 15:02:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.20296728610992432 norm:0.00025123852537944913 max memory_allocated 29230.302734375 
[2025-02-23 15:02:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.20128339529037476 norm:0.00021835596999153495 max memory_allocated 29230.302734375 
[2025-02-23 15:03:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.2000480443239212 norm:0.0001938349159900099 max memory_allocated 29230.302734375 
[2025-02-23 15:04:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.19914358854293823 norm:0.00017479961388744414 max memory_allocated 29230.302734375 
[2025-02-23 15:05:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.19846312701702118 norm:0.00016015584697015584 max memory_allocated 29230.302734375 
[2025-02-23 15:06:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.19797903299331665 norm:0.0001491083239670843 max memory_allocated 29230.302734375 
[2025-02-23 15:06:56 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.19745531678199768 norm:0.00014038266090210527 max memory_allocated 29230.302734375 
[2025-02-23 15:07:44 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.19705456495285034 norm:0.0001334140688413754 max memory_allocated 29230.302734375 
[2025-02-23 15:08:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.1966651976108551 norm:0.0001261812140000984 max memory_allocated 29230.302734375 
[2025-02-23 15:09:21 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.19644609093666077 norm:0.00012170104309916496 max memory_allocated 29230.302734375 
[2025-02-23 15:10:10 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.19640040397644043 norm:0.00011997506226180121 max memory_allocated 29230.302734375 
[2025-02-23 15:10:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.19626359641551971 norm:0.00011733193969121203 max memory_allocated 29230.302734375 
[2025-02-23 15:11:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.19617493450641632 norm:0.00011527911556186154 max memory_allocated 29230.302734375 
[2025-02-23 15:12:35 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.19614145159721375 norm:0.00011435188935138285 max memory_allocated 29230.302734375 
[2025-02-23 15:13:24 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.19609609246253967 norm:0.00011376509792171419 max memory_allocated 29230.302734375 
[2025-02-23 15:14:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.19600054621696472 norm:0.0001122403409681283 max memory_allocated 29230.302734375 
[2025-02-23 15:15:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.19593943655490875 norm:0.00011175606778124347 max memory_allocated 29230.302734375 
[2025-02-23 15:15:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-23 15:16:07 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.24381563067436218 norm:0.0007517187623307109 max memory_allocated 29230.490234375 
[2025-02-23 15:16:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.23822763562202454 norm:0.0004829951503779739 max memory_allocated 29230.490234375 
[2025-02-23 15:17:44 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.2347480207681656 norm:0.0003736057551577687 max memory_allocated 29230.490234375 
[2025-02-23 15:18:32 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.2323533147573471 norm:0.00030180905014276505 max memory_allocated 29230.490234375 
[2025-02-23 15:19:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.23059363663196564 norm:0.00026220493600703776 max memory_allocated 29230.490234375 
[2025-02-23 15:20:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.22912783920764923 norm:0.00022703756985720247 max memory_allocated 29230.490234375 
[2025-02-23 15:20:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.22805610299110413 norm:0.00020246922213118523 max memory_allocated 29230.490234375 
[2025-02-23 15:21:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.22725044190883636 norm:0.00018158595776185393 max memory_allocated 29230.490234375 
[2025-02-23 15:22:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.22658702731132507 norm:0.00016543160018045455 max memory_allocated 29230.490234375 
[2025-02-23 15:23:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.2260693460702896 norm:0.0001520871592219919 max memory_allocated 29230.490234375 
[2025-02-23 15:24:11 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.22569122910499573 norm:0.00014455548080150038 max memory_allocated 29230.490234375 
[2025-02-23 15:25:00 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.22540730237960815 norm:0.00013847659283783287 max memory_allocated 29230.490234375 
[2025-02-23 15:25:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.22517108917236328 norm:0.0001346604694845155 max memory_allocated 29230.490234375 
[2025-02-23 15:26:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.2250010073184967 norm:0.00013180571841076016 max memory_allocated 29230.490234375 
[2025-02-23 15:27:25 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.22484445571899414 norm:0.00012947767390869558 max memory_allocated 29230.490234375 
[2025-02-23 15:28:13 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.2246384173631668 norm:0.0001266798353753984 max memory_allocated 29230.490234375 
[2025-02-23 15:29:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.22452206909656525 norm:0.00012509999214671552 max memory_allocated 29230.490234375 
[2025-02-23 15:29:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.22446157038211823 norm:0.00012413770309649408 max memory_allocated 29230.490234375 
[2025-02-23 15:30:39 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.22436627745628357 norm:0.00012323894770815969 max memory_allocated 29230.490234375 
[2025-02-23 15:31:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.22428502142429352 norm:0.00012245595280546695 max memory_allocated 29230.490234375 
[2025-02-23 15:31:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-23 15:32:34 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.27203646302223206 norm:0.0004999453667551279 max memory_allocated 29230.677734375 
[2025-02-23 15:33:22 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.2667555809020996 norm:0.0003407921176403761 max memory_allocated 29230.677734375 
[2025-02-23 15:34:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.2634468078613281 norm:0.0002741249627433717 max memory_allocated 29230.677734375 
[2025-02-23 15:35:00 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.26095378398895264 norm:0.00023278442677110434 max memory_allocated 29230.677734375 
[2025-02-23 15:35:48 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.2591295838356018 norm:0.00020704236521851271 max memory_allocated 29230.677734375 
[2025-02-23 15:36:36 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.2577317953109741 norm:0.00018677962361834943 max memory_allocated 29230.677734375 
[2025-02-23 15:37:25 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.25665396451950073 norm:0.00017368925909977406 max memory_allocated 29230.677734375 
[2025-02-23 15:38:14 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.25585588812828064 norm:0.00015592244744766504 max memory_allocated 29230.677734375 
[2025-02-23 15:39:02 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.25523343682289124 norm:0.00014476962678600103 max memory_allocated 29230.677734375 
[2025-02-23 15:39:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.2546590566635132 norm:0.000136037910124287 max memory_allocated 29230.677734375 
[2025-02-23 15:40:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.25413307547569275 norm:0.00012968710507266223 max memory_allocated 29230.677734375 
[2025-02-23 15:41:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.2537851929664612 norm:0.00012555436114780605 max memory_allocated 29230.677734375 
[2025-02-23 15:42:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.25343063473701477 norm:0.00012059252912877128 max memory_allocated 29230.677734375 
[2025-02-23 15:43:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.2532002329826355 norm:0.00011834270117105916 max memory_allocated 29230.677734375 
[2025-02-23 15:43:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.2530284821987152 norm:0.00011624373291851953 max memory_allocated 29230.677734375 
[2025-02-23 15:44:41 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.2528555393218994 norm:0.00011453172191977501 max memory_allocated 29230.677734375 
[2025-02-23 15:45:30 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.252693235874176 norm:0.00011295922740828246 max memory_allocated 29230.677734375 
[2025-02-23 15:46:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.2525942623615265 norm:0.00011175795225426555 max memory_allocated 29230.677734375 
[2025-02-23 15:47:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.2525533437728882 norm:0.00011173071834491566 max memory_allocated 29230.677734375 
[2025-02-23 15:47:55 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.25252896547317505 norm:0.00011222813918720931 max memory_allocated 29230.677734375 
[2025-02-23 15:48:10 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-23 15:49:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.30780619382858276 norm:0.0006393224466592073 max memory_allocated 29230.865234375 
[2025-02-23 15:49:50 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.30173784494400024 norm:0.0004396945587359369 max memory_allocated 29230.865234375 
[2025-02-23 15:50:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.2979133129119873 norm:0.0003470945521257818 max memory_allocated 29230.865234375 
[2025-02-23 15:51:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.2953038513660431 norm:0.00029133260250091553 max memory_allocated 29230.865234375 
[2025-02-23 15:52:16 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.2931640148162842 norm:0.00024811227922327816 max memory_allocated 29230.865234375 
[2025-02-23 15:53:04 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.29147979617118835 norm:0.00021622002532240003 max memory_allocated 29230.865234375 
[2025-02-23 15:53:53 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.29029443860054016 norm:0.00019529582641553134 max memory_allocated 29230.865234375 
[2025-02-23 15:54:41 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.28927668929100037 norm:0.0001796338037820533 max memory_allocated 29230.865234375 
[2025-02-23 15:55:30 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.2885039746761322 norm:0.00016658424283377826 max memory_allocated 29230.865234375 
[2025-02-23 15:56:18 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.28789272904396057 norm:0.0001561297249281779 max memory_allocated 29230.865234375 
[2025-02-23 15:57:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.28744903206825256 norm:0.00014922427362762392 max memory_allocated 29230.865234375 
[2025-02-23 15:57:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.2870696485042572 norm:0.00014267468941397965 max memory_allocated 29230.865234375 
[2025-02-23 15:58:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.28678733110427856 norm:0.00013945120736025274 max memory_allocated 29230.865234375 
[2025-02-23 15:59:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.28650742769241333 norm:0.00013511122961062938 max memory_allocated 29230.865234375 
[2025-02-23 16:00:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.28636637330055237 norm:0.00013242833665572107 max memory_allocated 29230.865234375 
[2025-02-23 16:01:09 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.28624069690704346 norm:0.00013091886648908257 max memory_allocated 29230.865234375 
[2025-02-23 16:01:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.2861500084400177 norm:0.0001289084175368771 max memory_allocated 29230.865234375 
[2025-02-23 16:02:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.28604018688201904 norm:0.00012926531780976802 max memory_allocated 29230.865234375 
[2025-02-23 16:03:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.285944402217865 norm:0.0001264905877178535 max memory_allocated 29230.865234375 
[2025-02-23 16:04:23 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.28584709763526917 norm:0.0001257287076441571 max memory_allocated 29230.865234375 
[2025-02-23 16:04:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-23 16:05:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.3417162001132965 norm:0.0005575012764893472 max memory_allocated 29231.052734375 
[2025-02-23 16:06:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.33578282594680786 norm:0.00038148710154928267 max memory_allocated 29231.052734375 
[2025-02-23 16:07:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.33199387788772583 norm:0.00030418747337535024 max memory_allocated 29231.052734375 
[2025-02-23 16:07:54 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.32920220494270325 norm:0.0002583616878837347 max memory_allocated 29231.052734375 
[2025-02-23 16:08:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.3271292746067047 norm:0.00022594451729673892 max memory_allocated 29231.052734375 
[2025-02-23 16:09:31 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.32553255558013916 norm:0.00020169050549156964 max memory_allocated 29231.052734375 
[2025-02-23 16:10:19 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.3241940140724182 norm:0.00018137876759283245 max memory_allocated 29231.052734375 
[2025-02-23 16:11:08 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.32309412956237793 norm:0.00016597961075603962 max memory_allocated 29231.052734375 
[2025-02-23 16:11:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.32223543524742126 norm:0.0001531147863715887 max memory_allocated 29231.052734375 
[2025-02-23 16:12:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.32156237959861755 norm:0.00014281000767368823 max memory_allocated 29231.052734375 
[2025-02-23 16:13:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.3209223747253418 norm:0.0001334697735728696 max memory_allocated 29231.052734375 
[2025-02-23 16:14:22 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.32045695185661316 norm:0.00012713945761788636 max memory_allocated 29231.052734375 
[2025-02-23 16:15:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.3201149106025696 norm:0.00012238671479281038 max memory_allocated 29231.052734375 
[2025-02-23 16:15:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.3198047876358032 norm:0.00011838805949082598 max memory_allocated 29231.052734375 
[2025-02-23 16:16:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.3195419907569885 norm:0.00011505538714118302 max memory_allocated 29231.052734375 
[2025-02-23 16:17:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.3193720281124115 norm:0.00011295265721855685 max memory_allocated 29231.052734375 
[2025-02-23 16:18:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.31923675537109375 norm:0.00011122507567051798 max memory_allocated 29231.052734375 
[2025-02-23 16:19:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.31912925839424133 norm:0.00011012805043719709 max memory_allocated 29231.052734375 
[2025-02-23 16:20:01 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.3190470039844513 norm:0.00010987641144311056 max memory_allocated 29231.052734375 
[2025-02-23 16:20:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.3189772367477417 norm:0.00010964174725813791 max memory_allocated 29231.052734375 
[2025-02-23 16:21:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-23 16:21:56 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.38491976261138916 norm:0.0005607467028312385 max memory_allocated 29231.240234375 
[2025-02-23 16:22:44 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.3784099221229553 norm:0.00040389358764514327 max memory_allocated 29231.240234375 
[2025-02-23 16:23:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.3741231858730316 norm:0.0003326440346427262 max memory_allocated 29231.240234375 
[2025-02-23 16:24:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.3709898591041565 norm:0.000289723597234115 max memory_allocated 29231.240234375 
[2025-02-23 16:25:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.36860179901123047 norm:0.0002559815184213221 max memory_allocated 29231.240234375 
[2025-02-23 16:25:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.36661773920059204 norm:0.00022920739138498902 max memory_allocated 29231.240234375 
[2025-02-23 16:26:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.3650587499141693 norm:0.00020476947247516364 max memory_allocated 29231.240234375 
[2025-02-23 16:27:35 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.36377760767936707 norm:0.00018937664572149515 max memory_allocated 29231.240234375 
[2025-02-23 16:28:23 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.3626914322376251 norm:0.00017409742577001452 max memory_allocated 29231.240234375 
[2025-02-23 16:29:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.3618651330471039 norm:0.00016270470223389566 max memory_allocated 29231.240234375 
[2025-02-23 16:30:00 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.3610982298851013 norm:0.00015227246331050992 max memory_allocated 29231.240234375 
[2025-02-23 16:30:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.36047908663749695 norm:0.0001444877270841971 max memory_allocated 29231.240234375 
[2025-02-23 16:31:37 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.3599826991558075 norm:0.00013872419367544353 max memory_allocated 29231.240234375 
[2025-02-23 16:32:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.3596038222312927 norm:0.00013383745681494474 max memory_allocated 29231.240234375 
[2025-02-23 16:33:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.3593185842037201 norm:0.0001291254593525082 max memory_allocated 29231.240234375 
[2025-02-23 16:34:02 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.3590644598007202 norm:0.00012582354247570038 max memory_allocated 29231.240234375 
[2025-02-23 16:34:50 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.35886746644973755 norm:0.0001228250184794888 max memory_allocated 29231.240234375 
[2025-02-23 16:35:38 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.35868605971336365 norm:0.00012083017645636573 max memory_allocated 29231.240234375 
[2025-02-23 16:36:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.35857704281806946 norm:0.00011909682507393882 max memory_allocated 29231.240234375 
[2025-02-23 16:37:15 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.35843512415885925 norm:0.00011760137567762285 max memory_allocated 29231.240234375 
[2025-02-23 16:37:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-23 16:38:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.42843833565711975 norm:0.0008395072654820979 max memory_allocated 29231.427734375 
[2025-02-23 16:39:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.4208862781524658 norm:0.00045553233940154314 max memory_allocated 29231.427734375 
[2025-02-23 16:39:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.4162544906139374 norm:0.00037531249108724296 max memory_allocated 29231.427734375 
[2025-02-23 16:40:46 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.412806898355484 norm:0.00032523428671993315 max memory_allocated 29231.427734375 
[2025-02-23 16:41:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.4102150797843933 norm:0.00029580623959191144 max memory_allocated 29231.427734375 
[2025-02-23 16:42:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.4082081913948059 norm:0.0002721524506341666 max memory_allocated 29231.427734375 
[2025-02-23 16:43:12 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.4065953195095062 norm:0.0002537023101467639 max memory_allocated 29231.427734375 
[2025-02-23 16:44:00 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.40524163842201233 norm:0.00023588494514115155 max memory_allocated 29231.427734375 
[2025-02-23 16:44:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.40403494238853455 norm:0.00022684995201416314 max memory_allocated 29231.427734375 
[2025-02-23 16:45:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.4031175971031189 norm:0.000220179368625395 max memory_allocated 29231.427734375 
[2025-02-23 16:46:25 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.40233278274536133 norm:0.00021263818780425936 max memory_allocated 29231.427734375 
[2025-02-23 16:47:14 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.4016605615615845 norm:0.00020867666171398014 max memory_allocated 29231.427734375 
[2025-02-23 16:48:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.4011478126049042 norm:0.00020232121460139751 max memory_allocated 29231.427734375 
[2025-02-23 16:48:51 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.40074625611305237 norm:0.00020197695994284004 max memory_allocated 29231.427734375 
[2025-02-23 16:49:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.4003801941871643 norm:0.00019520219939295202 max memory_allocated 29231.427734375 
[2025-02-23 16:50:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.4001404345035553 norm:0.00019500829512253404 max memory_allocated 29231.427734375 
[2025-02-23 16:51:16 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.39988765120506287 norm:0.00018950900994241238 max memory_allocated 29231.427734375 
[2025-02-23 16:52:04 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.39963504672050476 norm:0.00018607666424941272 max memory_allocated 29231.427734375 
[2025-02-23 16:52:53 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.39947518706321716 norm:0.0001854033034760505 max memory_allocated 29231.427734375 
[2025-02-23 16:53:41 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.3993549346923828 norm:0.00018610501138027757 max memory_allocated 29231.427734375 
[2025-02-23 16:53:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-23 16:54:48 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.4666188657283783 norm:0.00046900296001695096 max memory_allocated 29231.615234375 
[2025-02-23 16:55:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.46025267243385315 norm:0.0003588601539377123 max memory_allocated 29231.615234375 
[2025-02-23 16:56:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.4557585120201111 norm:0.00030325137777253985 max memory_allocated 29231.615234375 
[2025-02-23 16:57:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.4524109363555908 norm:0.00026956258807331324 max memory_allocated 29231.615234375 
[2025-02-23 16:58:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.4498273730278015 norm:0.00024311812012456357 max memory_allocated 29231.615234375 
[2025-02-23 16:58:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.44780537486076355 norm:0.00022313918452709913 max memory_allocated 29231.615234375 
[2025-02-23 16:59:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.44613388180732727 norm:0.00020622761803679168 max memory_allocated 29231.615234375 
[2025-02-23 17:00:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.444749116897583 norm:0.00019062298815697432 max memory_allocated 29231.615234375 
[2025-02-23 17:01:15 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.44364550709724426 norm:0.0001783600018825382 max memory_allocated 29231.615234375 
[2025-02-23 17:02:03 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.44270697236061096 norm:0.00016908379620872438 max memory_allocated 29231.615234375 
[2025-02-23 17:02:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.44190436601638794 norm:0.00015897626872174442 max memory_allocated 29231.615234375 
[2025-02-23 17:03:40 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.44123294949531555 norm:0.00015056821575853974 max memory_allocated 29231.615234375 
[2025-02-23 17:04:29 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.44063979387283325 norm:0.00014360100612975657 max memory_allocated 29231.615234375 
[2025-02-23 17:05:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.4401969909667969 norm:0.0001371323742205277 max memory_allocated 29231.615234375 
[2025-02-23 17:06:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.43983691930770874 norm:0.00013227057934273034 max memory_allocated 29231.615234375 
[2025-02-23 17:06:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.4395447373390198 norm:0.00012729063746519387 max memory_allocated 29231.615234375 
[2025-02-23 17:07:42 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.43928176164627075 norm:0.00012349260214250535 max memory_allocated 29231.615234375 
[2025-02-23 17:08:31 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.439057856798172 norm:0.0001209647161886096 max memory_allocated 29231.615234375 
[2025-02-23 17:09:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.43888548016548157 norm:0.00011822000669781119 max memory_allocated 29231.615234375 
[2025-02-23 17:10:08 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.4387354552745819 norm:0.00011549963528523222 max memory_allocated 29231.615234375 
[2025-02-23 17:10:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-23 17:11:14 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.5138595104217529 norm:0.0005637972499243915 max memory_allocated 29231.802734375 
[2025-02-23 17:12:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.5068753957748413 norm:0.0004211636260151863 max memory_allocated 29231.802734375 
[2025-02-23 17:12:51 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.50221848487854 norm:0.0003443199093453586 max memory_allocated 29231.802734375 
[2025-02-23 17:13:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.4987346827983856 norm:0.00029230077052488923 max memory_allocated 29231.802734375 
[2025-02-23 17:14:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.4961394965648651 norm:0.00025867385556921363 max memory_allocated 29231.802734375 
[2025-02-23 17:15:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.494026780128479 norm:0.0002332040312467143 max memory_allocated 29231.802734375 
[2025-02-23 17:16:04 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.49233508110046387 norm:0.00021147163351997733 max memory_allocated 29231.802734375 
[2025-02-23 17:16:53 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.49094074964523315 norm:0.00019731343491002917 max memory_allocated 29231.802734375 
[2025-02-23 17:17:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.4898046553134918 norm:0.00018540641758590937 max memory_allocated 29231.802734375 
[2025-02-23 17:18:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.488864541053772 norm:0.00017570806085132062 max memory_allocated 29231.802734375 
[2025-02-23 17:19:18 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.48807990550994873 norm:0.0001681428257143125 max memory_allocated 29231.802734375 
[2025-02-23 17:20:06 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.4873831570148468 norm:0.00015895302931312472 max memory_allocated 29231.802734375 
[2025-02-23 17:20:55 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.4868156313896179 norm:0.00015351493493653834 max memory_allocated 29231.802734375 
[2025-02-23 17:21:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.48636412620544434 norm:0.0001486690598540008 max memory_allocated 29231.802734375 
[2025-02-23 17:22:32 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.4860115647315979 norm:0.000143956218380481 max memory_allocated 29231.802734375 
[2025-02-23 17:23:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.4857098460197449 norm:0.0001402766938554123 max memory_allocated 29231.802734375 
[2025-02-23 17:24:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.4854494631290436 norm:0.00013709458289667964 max memory_allocated 29231.802734375 
[2025-02-23 17:24:57 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.4852600693702698 norm:0.00013382936595007777 max memory_allocated 29231.802734375 
[2025-02-23 17:25:45 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.48512470722198486 norm:0.00013164903793949634 max memory_allocated 29231.802734375 
[2025-02-23 17:26:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.48499950766563416 norm:0.00012969253293704242 max memory_allocated 29231.802734375 
[2025-02-23 17:26:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-23 17:27:40 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.5629243850708008 norm:0.00044748024083673954 max memory_allocated 29231.990234375 
[2025-02-23 17:28:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.5556358098983765 norm:0.0003453963145148009 max memory_allocated 29231.990234375 
[2025-02-23 17:29:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.5507474541664124 norm:0.0003008430066984147 max memory_allocated 29231.990234375 
[2025-02-23 17:30:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.5471415519714355 norm:0.00026982123381458223 max memory_allocated 29231.990234375 
[2025-02-23 17:30:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.5443800091743469 norm:0.0002467326994519681 max memory_allocated 29231.990234375 
[2025-02-23 17:31:42 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.5421613454818726 norm:0.0002270180848427117 max memory_allocated 29231.990234375 
[2025-02-23 17:32:30 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.5402753353118896 norm:0.00021124177146703005 max memory_allocated 29231.990234375 
[2025-02-23 17:33:19 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.5387425422668457 norm:0.00019879771571140736 max memory_allocated 29231.990234375 
[2025-02-23 17:34:08 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.5375018119812012 norm:0.00018715356418397278 max memory_allocated 29231.990234375 
[2025-02-23 17:34:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.5365287065505981 norm:0.00017520636902190745 max memory_allocated 29231.990234375 
[2025-02-23 17:35:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.535652220249176 norm:0.00016756934928707778 max memory_allocated 29231.990234375 
[2025-02-23 17:36:33 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.5349856615066528 norm:0.00016253413923550397 max memory_allocated 29231.990234375 
[2025-02-23 17:37:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.5344341397285461 norm:0.000157613045303151 max memory_allocated 29231.990234375 
[2025-02-23 17:38:10 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.533973753452301 norm:0.00015208133845590055 max memory_allocated 29231.990234375 
[2025-02-23 17:38:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.5335551500320435 norm:0.00014912486949469894 max memory_allocated 29231.990234375 
[2025-02-23 17:39:46 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.5332630276679993 norm:0.00014452426694333553 max memory_allocated 29231.990234375 
[2025-02-23 17:40:35 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.5330385565757751 norm:0.000141631142469123 max memory_allocated 29231.990234375 
[2025-02-23 17:41:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.5328631401062012 norm:0.0001395094586769119 max memory_allocated 29231.990234375 
[2025-02-23 17:42:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.532719612121582 norm:0.00013820907042827457 max memory_allocated 29231.990234375 
[2025-02-23 17:43:00 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.5325866341590881 norm:0.0001374539569951594 max memory_allocated 29231.990234375 
[2025-02-23 17:43:15 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-23 17:44:07 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.6127519607543945 norm:0.0005462758126668632 max memory_allocated 29232.177734375 
[2025-02-23 17:44:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.6051108837127686 norm:0.0003951644175685942 max memory_allocated 29232.177734375 
[2025-02-23 17:45:43 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.6000500321388245 norm:0.00033703900407999754 max memory_allocated 29232.177734375 
[2025-02-23 17:46:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.5963510274887085 norm:0.0003006857295986265 max memory_allocated 29232.177734375 
[2025-02-23 17:47:20 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.593407392501831 norm:0.00026807826361618936 max memory_allocated 29232.177734375 
[2025-02-23 17:48:09 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.5910147428512573 norm:0.00024316403141710907 max memory_allocated 29232.177734375 
[2025-02-23 17:48:57 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.5891017317771912 norm:0.00022457816521637142 max memory_allocated 29232.177734375 
[2025-02-23 17:49:45 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.5875846743583679 norm:0.00021065732289571315 max memory_allocated 29232.177734375 
[2025-02-23 17:50:34 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.5863276720046997 norm:0.00019753923697862774 max memory_allocated 29232.177734375 
[2025-02-23 17:51:23 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.5851990580558777 norm:0.00018656915926840156 max memory_allocated 29232.177734375 
[2025-02-23 17:52:11 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.5843023657798767 norm:0.00017761116032488644 max memory_allocated 29232.177734375 
[2025-02-23 17:52:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.5835351347923279 norm:0.0001711265358608216 max memory_allocated 29232.177734375 
[2025-02-23 17:53:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.5829579830169678 norm:0.000165974983246997 max memory_allocated 29232.177734375 
[2025-02-23 17:54:36 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.5825060606002808 norm:0.0001601877884240821 max memory_allocated 29232.177734375 
[2025-02-23 17:55:25 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.582146167755127 norm:0.00015606188389938325 max memory_allocated 29232.177734375 
[2025-02-23 17:56:13 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.5818516612052917 norm:0.00015179628098849207 max memory_allocated 29232.177734375 
[2025-02-23 17:57:01 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.5816669464111328 norm:0.0001492310402682051 max memory_allocated 29232.177734375 
[2025-02-23 17:57:50 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.5814785361289978 norm:0.00014672389079350978 max memory_allocated 29232.177734375 
[2025-02-23 17:58:38 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.5813348293304443 norm:0.0001462913933210075 max memory_allocated 29232.177734375 
[2025-02-23 17:59:27 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.5812345743179321 norm:0.00014418763748835772 max memory_allocated 29232.177734375 
[2025-02-23 17:59:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-23 18:00:33 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.6721971035003662 norm:0.000651528243906796 max memory_allocated 29232.365234375 
[2025-02-23 18:01:22 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.6639145016670227 norm:0.00045593554386869073 max memory_allocated 29232.365234375 
[2025-02-23 18:02:10 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.6584661602973938 norm:0.0003774480428546667 max memory_allocated 29232.365234375 
[2025-02-23 18:02:59 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.6545225381851196 norm:0.000330991402734071 max memory_allocated 29232.365234375 
[2025-02-23 18:03:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.6515324115753174 norm:0.00029288677615113556 max memory_allocated 29232.365234375 
[2025-02-23 18:04:35 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.6491309404373169 norm:0.00026735267601907253 max memory_allocated 29232.365234375 
[2025-02-23 18:05:24 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.6472389698028564 norm:0.0002430962776998058 max memory_allocated 29232.365234375 
[2025-02-23 18:06:12 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.6456841230392456 norm:0.0002239215245936066 max memory_allocated 29232.365234375 
[2025-02-23 18:07:00 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.644517719745636 norm:0.00020972425409127027 max memory_allocated 29232.365234375 
[2025-02-23 18:07:49 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.6434720158576965 norm:0.00019993679597973824 max memory_allocated 29232.365234375 
[2025-02-23 18:08:37 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.642666220664978 norm:0.00018820384866558015 max memory_allocated 29232.365234375 
[2025-02-23 18:09:25 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.6420989632606506 norm:0.00018269028805661947 max memory_allocated 29232.365234375 
[2025-02-23 18:10:14 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.6415956020355225 norm:0.00017735543951857835 max memory_allocated 29232.365234375 
[2025-02-23 18:11:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.6412125825881958 norm:0.00017335801385343075 max memory_allocated 29232.365234375 
[2025-02-23 18:11:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.6409037113189697 norm:0.0001722082815831527 max memory_allocated 29232.365234375 
[2025-02-23 18:12:39 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.6406383514404297 norm:0.0001673070073593408 max memory_allocated 29232.365234375 
[2025-02-23 18:13:27 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.6404857635498047 norm:0.00016546904225833714 max memory_allocated 29232.365234375 
[2025-02-23 18:14:16 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.6403465270996094 norm:0.00016424167552031577 max memory_allocated 29232.365234375 
[2025-02-23 18:15:04 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.6402150392532349 norm:0.00016378791769966483 max memory_allocated 29232.365234375 
[2025-02-23 18:15:52 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.640178918838501 norm:0.00016496801981702447 max memory_allocated 29232.365234375 
[2025-02-23 18:16:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-23 18:16:59 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.7469112277030945 norm:0.0009878450073301792 max memory_allocated 29232.552734375 
[2025-02-23 18:17:48 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.7367055416107178 norm:0.0006820909329690039 max memory_allocated 29232.552734375 
[2025-02-23 18:18:36 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.7304538488388062 norm:0.0005408662254922092 max memory_allocated 29232.552734375 
[2025-02-23 18:19:24 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.7259168028831482 norm:0.00045005709398537874 max memory_allocated 29232.552734375 
[2025-02-23 18:20:13 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.7225710153579712 norm:0.0003900570154655725 max memory_allocated 29232.552734375 
[2025-02-23 18:21:01 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.7198155522346497 norm:0.0003426495532039553 max memory_allocated 29232.552734375 
[2025-02-23 18:21:49 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.7174726724624634 norm:0.0003049529914278537 max memory_allocated 29232.552734375 
[2025-02-23 18:22:38 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.715822696685791 norm:0.00027175407740287483 max memory_allocated 29232.552734375 
[2025-02-23 18:23:26 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.7143852710723877 norm:0.0002499143301974982 max memory_allocated 29232.552734375 
[2025-02-23 18:24:15 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.7133306860923767 norm:0.0002324950328329578 max memory_allocated 29232.552734375 
[2025-02-23 18:25:03 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.7124678492546082 norm:0.0002199574519181624 max memory_allocated 29232.552734375 
[2025-02-23 18:25:51 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.7117434740066528 norm:0.00020877020142506808 max memory_allocated 29232.552734375 
[2025-02-23 18:26:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.711274266242981 norm:0.00020103434508200735 max memory_allocated 29232.552734375 
[2025-02-23 18:27:28 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.7107594013214111 norm:0.0001948587887454778 max memory_allocated 29232.552734375 
[2025-02-23 18:28:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.7103999853134155 norm:0.00018896404071711004 max memory_allocated 29232.552734375 
[2025-02-23 18:29:05 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.7100763916969299 norm:0.00018331393948756158 max memory_allocated 29232.552734375 
[2025-02-23 18:29:54 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.7099079489707947 norm:0.00018116574210580438 max memory_allocated 29232.552734375 
[2025-02-23 18:30:42 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.7097004055976868 norm:0.00018034136155620217 max memory_allocated 29232.552734375 
[2025-02-23 18:31:30 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.7095536589622498 norm:0.0001783017796697095 max memory_allocated 29232.552734375 
[2025-02-23 18:32:19 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.7094414234161377 norm:0.00017814830061979592 max memory_allocated 29232.552734375 
[2025-02-23 18:32:33 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-23 18:33:25 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.8206149339675903 norm:0.0010145214619114995 max memory_allocated 29232.740234375 
[2025-02-23 18:34:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.8098663091659546 norm:0.0007313189562410116 max memory_allocated 29232.740234375 
[2025-02-23 18:35:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.8027790188789368 norm:0.000584644905757159 max memory_allocated 29232.740234375 
[2025-02-23 18:35:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.797739565372467 norm:0.0004952262388542295 max memory_allocated 29232.740234375 
[2025-02-23 18:36:38 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.7938413023948669 norm:0.00043294214992783964 max memory_allocated 29232.740234375 
[2025-02-23 18:37:27 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.7907305359840393 norm:0.0003825126332230866 max memory_allocated 29232.740234375 
[2025-02-23 18:38:15 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.7884756922721863 norm:0.00034023012267425656 max memory_allocated 29232.740234375 
[2025-02-23 18:39:04 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.7865078449249268 norm:0.00031171084265224636 max memory_allocated 29232.740234375 
[2025-02-23 18:39:52 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.7851378917694092 norm:0.00029150384943932295 max memory_allocated 29232.740234375 
[2025-02-23 18:40:40 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.7839378118515015 norm:0.00027380179380998015 max memory_allocated 29232.740234375 
[2025-02-23 18:41:29 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.7830296754837036 norm:0.000263056019321084 max memory_allocated 29232.740234375 
[2025-02-23 18:42:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.7822591066360474 norm:0.0002499992842786014 max memory_allocated 29232.740234375 
[2025-02-23 18:43:05 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.7816591262817383 norm:0.00023878688807599247 max memory_allocated 29232.740234375 
[2025-02-23 18:43:54 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.781234860420227 norm:0.00023193976085167378 max memory_allocated 29232.740234375 
[2025-02-23 18:44:42 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.7808516621589661 norm:0.00022787356283515692 max memory_allocated 29232.740234375 
[2025-02-23 18:45:30 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.780567467212677 norm:0.00022718246327713132 max memory_allocated 29232.740234375 
[2025-02-23 18:46:18 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.7803050875663757 norm:0.0002202648320235312 max memory_allocated 29232.740234375 
[2025-02-23 18:47:07 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.7801252007484436 norm:0.00022191098832990974 max memory_allocated 29232.740234375 
[2025-02-23 18:47:55 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.7800084948539734 norm:0.00021569887758232653 max memory_allocated 29232.740234375 
[2025-02-23 18:48:43 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.7798440456390381 norm:0.00021382073464337736 max memory_allocated 29232.740234375 
[2025-02-23 18:48:58 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-23 18:49:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.9067343473434448 norm:0.0009642121149227023 max memory_allocated 29232.927734375 
[2025-02-23 18:50:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.8942352533340454 norm:0.0007211451302282512 max memory_allocated 29232.927734375 
[2025-02-23 18:51:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.8859553337097168 norm:0.0005828881403431296 max memory_allocated 29232.927734375 
[2025-02-23 18:52:15 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.8802415132522583 norm:0.0005028416053391993 max memory_allocated 29232.927734375 
[2025-02-23 18:53:03 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.8757595419883728 norm:0.00043787015601992607 max memory_allocated 29232.927734375 
[2025-02-23 18:53:52 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.8723113536834717 norm:0.0003891975502483547 max memory_allocated 29232.927734375 
[2025-02-23 18:54:40 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.8694857954978943 norm:0.00034856804995797575 max memory_allocated 29232.927734375 
[2025-02-23 18:55:28 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.8673341870307922 norm:0.00031555548775941133 max memory_allocated 29232.927734375 
[2025-02-23 18:56:17 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.8657313585281372 norm:0.0002875266072805971 max memory_allocated 29232.927734375 
[2025-02-23 18:57:05 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.8643381595611572 norm:0.00026489817537367344 max memory_allocated 29232.927734375 
[2025-02-23 18:57:54 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.8633010387420654 norm:0.0002470871258992702 max memory_allocated 29232.927734375 
[2025-02-23 18:58:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.8624616861343384 norm:0.00023326845257543027 max memory_allocated 29232.927734375 
[2025-02-23 18:59:31 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.861771821975708 norm:0.00022073660511523485 max memory_allocated 29232.927734375 
[2025-02-23 19:00:19 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.8613141179084778 norm:0.00021137529984116554 max memory_allocated 29232.927734375 
[2025-02-23 19:01:08 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.8607967495918274 norm:0.00020271321409381926 max memory_allocated 29232.927734375 
[2025-02-23 19:01:56 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.8603798747062683 norm:0.0001955518964678049 max memory_allocated 29232.927734375 
[2025-02-23 19:02:44 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.8600660562515259 norm:0.00019229701138101518 max memory_allocated 29232.927734375 
[2025-02-23 19:03:33 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.8597885370254517 norm:0.00018844116129912436 max memory_allocated 29232.927734375 
[2025-02-23 19:04:21 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.8595508337020874 norm:0.00018470305076334625 max memory_allocated 29232.927734375 
[2025-02-23 19:05:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.8594669103622437 norm:0.0001822282065404579 max memory_allocated 29232.927734375 
[2025-02-23 19:05:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-23 19:06:16 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:1.031347632408142 norm:0.001419450854882598 max memory_allocated 29233.115234375 
[2025-02-23 19:07:05 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:1.013153076171875 norm:0.001036999048665166 max memory_allocated 29233.115234375 
[2025-02-23 19:07:53 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:1.0017106533050537 norm:0.0008334855665452778 max memory_allocated 29233.115234375 
[2025-02-23 19:08:42 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.9934117794036865 norm:0.00070507253985852 max memory_allocated 29233.115234375 
[2025-02-23 19:09:30 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.9874604940414429 norm:0.0006099656457081437 max memory_allocated 29233.115234375 
[2025-02-23 19:10:18 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.9827762842178345 norm:0.0005430009332485497 max memory_allocated 29233.115234375 
[2025-02-23 19:11:07 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.9792966246604919 norm:0.0004945789696648717 max memory_allocated 29233.115234375 
[2025-02-23 19:11:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.9763911962509155 norm:0.00045936551759950817 max memory_allocated 29233.115234375 
[2025-02-23 19:12:44 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.9740716218948364 norm:0.00042489959741942585 max memory_allocated 29233.115234375 
[2025-02-23 19:13:32 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.972435712814331 norm:0.00039903828292153776 max memory_allocated 29233.115234375 
[2025-02-23 19:14:20 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.9708950519561768 norm:0.00037903516204096377 max memory_allocated 29233.115234375 
[2025-02-23 19:15:09 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.9697099924087524 norm:0.0003590691485442221 max memory_allocated 29233.115234375 
[2025-02-23 19:15:57 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.9688531756401062 norm:0.0003413628146518022 max memory_allocated 29233.115234375 
[2025-02-23 19:16:46 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.9681867361068726 norm:0.00032922130776569247 max memory_allocated 29233.115234375 
[2025-02-23 19:17:34 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.967536985874176 norm:0.00032051015296019614 max memory_allocated 29233.115234375 
[2025-02-23 19:18:23 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.9668129086494446 norm:0.00030786319985054433 max memory_allocated 29233.115234375 
[2025-02-23 19:19:11 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.9664136171340942 norm:0.0002981829456984997 max memory_allocated 29233.115234375 
[2025-02-23 19:20:00 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.9660513997077942 norm:0.0002923010033555329 max memory_allocated 29233.115234375 
[2025-02-23 19:20:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.9653517603874207 norm:0.000296357786282897 max memory_allocated 29233.115234375 
[2025-02-23 19:21:37 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.9651440382003784 norm:0.0002922833664342761 max memory_allocated 29233.115234375 
[2025-02-23 19:21:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-23 19:22:43 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:1.2523447275161743 norm:0.0035854342859238386 max memory_allocated 29233.302734375 
[2025-02-23 19:23:31 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:1.2243624925613403 norm:0.0026487624272704124 max memory_allocated 29233.302734375 
[2025-02-23 19:24:20 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:1.2088770866394043 norm:0.0021625736262649298 max memory_allocated 29233.302734375 
[2025-02-23 19:25:08 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:1.1980079412460327 norm:0.0018133739940822124 max memory_allocated 29233.302734375 
[2025-02-23 19:25:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:1.1907598972320557 norm:0.0014865336706861854 max memory_allocated 29233.302734375 
[2025-02-23 19:26:45 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:1.185714602470398 norm:0.0013020449550822377 max memory_allocated 29233.302734375 
[2025-02-23 19:27:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:1.1818411350250244 norm:0.0011264380300417542 max memory_allocated 29233.302734375 
[2025-02-23 19:28:22 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:1.1787991523742676 norm:0.0009352877968922257 max memory_allocated 29233.302734375 
[2025-02-23 19:29:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:1.1762309074401855 norm:0.0008684665081091225 max memory_allocated 29233.302734375 
[2025-02-23 19:29:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:1.1746450662612915 norm:0.0008325144881382585 max memory_allocated 29233.302734375 
[2025-02-23 19:30:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:1.1734557151794434 norm:0.0008095537195913494 max memory_allocated 29233.302734375 
[2025-02-23 19:31:36 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:1.1726267337799072 norm:0.0008089187904261053 max memory_allocated 29233.302734375 
[2025-02-23 19:32:24 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:1.1717462539672852 norm:0.0007652416243217885 max memory_allocated 29233.302734375 
[2025-02-23 19:33:13 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:1.1710116863250732 norm:0.0007423204369843006 max memory_allocated 29233.302734375 
[2025-02-23 19:34:02 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:1.1703996658325195 norm:0.0007305929902940989 max memory_allocated 29233.302734375 
[2025-02-23 19:34:50 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:1.170013427734375 norm:0.0007300693541765213 max memory_allocated 29233.302734375 
[2025-02-23 19:35:38 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:1.1696852445602417 norm:0.0007445912924595177 max memory_allocated 29233.302734375 
[2025-02-23 19:36:27 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:1.1692554950714111 norm:0.0007325984770432115 max memory_allocated 29233.302734375 
[2025-02-23 19:37:15 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:1.169332504272461 norm:0.0007407028460875154 max memory_allocated 29233.302734375 
[2025-02-23 19:38:04 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:1.1692605018615723 norm:0.0007208110764622688 max memory_allocated 29233.302734375 
[2025-02-23 19:38:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-23 19:39:10 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:2.008700132369995 norm:0.008836662396788597 max memory_allocated 29233.490234375 
[2025-02-23 19:39:59 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:1.9406133890151978 norm:0.006928155664354563 max memory_allocated 29233.490234375 
[2025-02-23 19:40:47 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:1.9030085802078247 norm:0.005496904719620943 max memory_allocated 29233.490234375 
[2025-02-23 19:41:35 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:1.8801418542861938 norm:0.005199074745178223 max memory_allocated 29233.490234375 
[2025-02-23 19:42:24 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:1.8620785474777222 norm:0.005498146638274193 max memory_allocated 29233.490234375 
[2025-02-23 19:43:12 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:1.84419846534729 norm:0.005315782967954874 max memory_allocated 29233.490234375 
[2025-02-23 19:44:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:1.8360896110534668 norm:0.005650615785270929 max memory_allocated 29233.490234375 
[2025-02-23 19:44:48 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:1.8322439193725586 norm:0.005598770454525948 max memory_allocated 29233.490234375 
[2025-02-23 19:45:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:1.8260130882263184 norm:0.005712129175662994 max memory_allocated 29233.490234375 
[2025-02-23 19:46:25 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:1.8184032440185547 norm:0.005979318171739578 max memory_allocated 29233.490234375 
[2025-02-23 19:47:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:1.8138847351074219 norm:0.006506772246211767 max memory_allocated 29233.490234375 
[2025-02-23 19:48:02 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:1.81389319896698 norm:0.006555074360221624 max memory_allocated 29233.490234375 
[2025-02-23 19:48:50 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:1.8019299507141113 norm:0.005757456179708242 max memory_allocated 29233.490234375 
[2025-02-23 19:49:38 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:1.8067781925201416 norm:0.007273809984326363 max memory_allocated 29233.490234375 
[2025-02-23 19:50:27 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:1.7993344068527222 norm:0.005723022390156984 max memory_allocated 29233.490234375 
[2025-02-23 19:51:15 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:1.7985002994537354 norm:0.00637606717646122 max memory_allocated 29233.490234375 
[2025-02-23 19:52:04 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:1.7979536056518555 norm:0.006412718445062637 max memory_allocated 29233.490234375 
[2025-02-23 19:52:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:1.7949241399765015 norm:0.006716298405081034 max memory_allocated 29233.490234375 
[2025-02-23 19:53:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:1.791027545928955 norm:0.006009726319462061 max memory_allocated 29233.490234375 
[2025-02-23 19:54:29 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:1.7903436422348022 norm:0.005659677088260651 max memory_allocated 29233.490234375 
[2025-02-23 19:54:44 root] (main_calibration.py 365): INFO 39515.92145872116
[2025-02-23 19:55:46 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-02-23 19:57:40 root] (main_calibration.py 158): INFO wikitext2 : 5.394495964050293
[2025-02-23 19:57:40 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-02-23 20:00:38 root] (main_calibration.py 158): INFO c4 : 6.908613204956055
[2025-02-23 22:04:53 root] (main_calibration.py 169): INFO {'wikitext2': 5.394495964050293, 'c4': 6.908613204956055, 'results': {'piqa': {'acc': 0.7850924918389554, 'acc_stderr': 0.00958366508265331, 'acc_norm': 0.7834602829162133, 'acc_norm_stderr': 0.009609984714384592}, 'arc_challenge': {'acc': 0.4351535836177474, 'acc_stderr': 0.014487986197186048, 'acc_norm': 0.4377133105802048, 'acc_norm_stderr': 0.014497573881108285}, 'boolq': {'acc': 0.6813455657492354, 'acc_stderr': 0.008149598998538518}, 'arc_easy': {'acc': 0.7108585858585859, 'acc_stderr': 0.009302827114597427, 'acc_norm': 0.5698653198653199, 'acc_norm_stderr': 0.010159130445178511}, 'winogrande': {'acc': 0.7032359905288083, 'acc_stderr': 0.012839239695202023}, 'hellaswag': {'acc': 0.579964150567616, 'acc_stderr': 0.0049255561046794285, 'acc_norm': 0.7502489543915555, 'acc_norm_stderr': 0.004319842107724388}}, 'versions': {'piqa': 0, 'arc_challenge': 0, 'boolq': 1, 'arc_easy': 0, 'winogrande': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
