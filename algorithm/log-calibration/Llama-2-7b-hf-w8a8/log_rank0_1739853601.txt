[2025-02-18 04:40:01 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration/Llama-2-7b-hf-w8a8', save_dir='./log-calibration/quant/Llama-2-7b-hf-w8a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=8, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-18 04:44:20 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-18 04:44:20 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-18 04:44:20 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-18 04:44:23 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-18 04:44:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.0005291522829793394 norm:0.00017621071310713887 max memory_allocated 22512.63671875 
[2025-02-18 04:45:31 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.0003202323568984866 norm:5.093095023767091e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:46:04 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.0002651379909366369 norm:2.916943049058318e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:46:37 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.00024003635917324573 norm:2.1057561752968468e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:47:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.00022772795637138188 norm:1.7318689060630277e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:47:43 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.00022196130885276943 norm:1.582045661052689e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:48:16 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.00021776431822218 norm:1.6351628801203333e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:48:49 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.00021704169921576977 norm:2.285066875629127e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:49:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.0002133656817022711 norm:1.4068848940951284e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:49:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00021430166088975966 norm:2.1234531232039444e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:50:29 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.0002129463682649657 norm:1.5426108802785166e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:51:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.00021281572117004544 norm:2.1023060980951414e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:51:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.00021238003682810813 norm:1.7710490283207037e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:52:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.0002106533502228558 norm:1.4623113202105742e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:52:41 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.00021024646412115544 norm:1.5209536286420189e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:53:14 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0002111507928930223 norm:2.4936729460023344e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:53:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.0002091572096105665 norm:1.4542542885465082e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:54:21 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.00021116543211974204 norm:2.2866512153996155e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:54:54 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.00021006005408708006 norm:1.678125227044802e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:55:27 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.00021014614321757108 norm:1.5761430404381827e-05 max memory_allocated 22512.63671875 
[2025-02-18 04:55:36 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-18 04:56:12 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.0017504902789369226 norm:0.00022783201711717993 max memory_allocated 22512.80859375 
[2025-02-18 04:56:45 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.0013400770258158445 norm:0.0001052746592904441 max memory_allocated 22512.80859375 
[2025-02-18 04:57:18 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.0011825845576822758 norm:7.212119817268103e-05 max memory_allocated 22512.80859375 
[2025-02-18 04:57:52 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.0010915501043200493 norm:5.736184539273381e-05 max memory_allocated 22512.80859375 
[2025-02-18 04:58:25 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.0010474321898072958 norm:5.4079977417131886e-05 max memory_allocated 22512.80859375 
[2025-02-18 04:58:58 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.0009972606785595417 norm:4.1127837903331965e-05 max memory_allocated 22512.80859375 
[2025-02-18 04:59:31 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.0009767301380634308 norm:3.891617598128505e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:00:04 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.0009564089123159647 norm:3.565122824511491e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:00:37 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.0009444891475141048 norm:3.672290040412918e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:01:10 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.0009258643258363008 norm:3.572017885744572e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:01:43 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.0009244627435691655 norm:4.166814687778242e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:02:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.0009072611574083567 norm:3.856266266666353e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:02:50 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.0009022701997309923 norm:4.093246388947591e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:03:23 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.0008984630112536252 norm:4.010025440948084e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:03:56 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.0008904259302653372 norm:3.8708469219272956e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:04:29 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.0008820220245979726 norm:4.199388058623299e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:05:02 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.0008841957896947861 norm:4.087537672603503e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:05:35 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.0008749001426622272 norm:3.817582182819024e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:06:08 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.0008716481970623136 norm:4.957879355060868e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:06:41 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.0008672953117638826 norm:4.48870487161912e-05 max memory_allocated 22512.80859375 
[2025-02-18 05:06:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-18 05:07:27 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.001837619231082499 norm:0.0011034219060093164 max memory_allocated 22512.98046875 
[2025-02-18 05:08:00 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.001297782058827579 norm:0.00038316083373501897 max memory_allocated 22512.98046875 
[2025-02-18 05:08:33 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.0011320703197270632 norm:0.00021415243099909276 max memory_allocated 22512.98046875 
[2025-02-18 05:09:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.0010535601759329438 norm:0.00014278397429734468 max memory_allocated 22512.98046875 
[2025-02-18 05:09:40 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.0010052304714918137 norm:9.820333070820197e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:10:13 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.000984037178568542 norm:6.974112329771742e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:10:46 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0009759574895724654 norm:5.014878115616739e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:11:19 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.000972013280261308 norm:3.665171243483201e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:11:53 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.000968962674960494 norm:2.760895949904807e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:12:26 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.0009648855775594711 norm:2.1683532395400107e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:12:59 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.0009660772047936916 norm:1.7090842447942123e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:13:32 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.0009628169937059283 norm:1.6285639503621496e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:14:05 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.0009617310715839267 norm:1.4861073395877611e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:14:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.0009631091961637139 norm:1.3470993508235551e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:15:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.000963174388743937 norm:1.2744907508022152e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:15:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.0009622506331652403 norm:1.2657669685722794e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:16:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.0009590237168595195 norm:1.257530675502494e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:16:51 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.0009605406085029244 norm:1.2539488125185017e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:17:25 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.000959241995587945 norm:1.2463989151001442e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:17:58 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.0009596932213753462 norm:1.2638100088224746e-05 max memory_allocated 22512.98046875 
[2025-02-18 05:18:07 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-18 05:18:43 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.0020958995446562767 norm:0.0007152479374781251 max memory_allocated 22513.15234375 
[2025-02-18 05:19:17 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.0015980367315933108 norm:0.0002999066491611302 max memory_allocated 22513.15234375 
[2025-02-18 05:19:50 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.001430649310350418 norm:0.00018633024592418224 max memory_allocated 22513.15234375 
[2025-02-18 05:20:23 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.0013394425623118877 norm:0.00012333541235420853 max memory_allocated 22513.15234375 
[2025-02-18 05:20:56 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.0012883133022114635 norm:8.83422908373177e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:21:29 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0012578837340697646 norm:6.334098725346848e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:22:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.0012462425511330366 norm:4.685357998823747e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:22:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.0012406024616211653 norm:3.365299926372245e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:23:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.001236478565260768 norm:2.4803619453450665e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:23:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.0012336462968960404 norm:1.8478091078577563e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:24:15 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.0012352794874459505 norm:1.4368657502927817e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:24:49 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.0012333873892202973 norm:1.3474365914589725e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:25:22 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.0012308541918173432 norm:1.240363508259179e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:25:55 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.0012340244138613343 norm:1.1732717211998533e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:26:28 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.001231950125657022 norm:1.1749516488634981e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:27:01 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0012306664139032364 norm:1.164568129752297e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:27:35 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.001227874425239861 norm:1.152905315393582e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:28:08 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.0012263774406164885 norm:1.1490908036648761e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:28:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.0012261051451787353 norm:1.1478581654955633e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:29:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.0012254896573722363 norm:1.1400741641409695e-05 max memory_allocated 22513.15234375 
[2025-02-18 05:29:24 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-18 05:30:00 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.0020799527410417795 norm:0.00041911625885404646 max memory_allocated 22513.32421875 
[2025-02-18 05:30:33 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.0016763060120865703 norm:0.00018061709124594927 max memory_allocated 22513.32421875 
[2025-02-18 05:31:07 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.0015456881374120712 norm:0.00011098681716248393 max memory_allocated 22513.32421875 
[2025-02-18 05:31:40 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.001484059146605432 norm:7.43097989470698e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:32:13 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.0014545801095664501 norm:5.199478619033471e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:32:46 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.0014358407352119684 norm:3.7233294278848916e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:33:19 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.0014214866096153855 norm:2.7731981390388682e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:33:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.0014172196388244629 norm:2.1170208128751256e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:34:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.00141395244281739 norm:1.730126496113371e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:34:59 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.001409200020134449 norm:1.513259849161841e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:35:32 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0014092037454247475 norm:1.3678884897672106e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:36:06 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.001405178802087903 norm:3.529527020873502e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:36:39 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.001405592542141676 norm:1.1872973118443042e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:37:12 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.0014056613435968757 norm:1.1334154805808794e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:37:45 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.001403586007654667 norm:1.1291787814116105e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:38:18 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.0014027931028977036 norm:1.1146988981636241e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:38:52 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.0014024791307747364 norm:1.101223097066395e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:39:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.0013990127481520176 norm:1.1062101293646265e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:39:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.0013987505808472633 norm:1.1103173164883628e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:40:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.0013979292707517743 norm:1.0886462405323982e-05 max memory_allocated 22513.32421875 
[2025-02-18 05:40:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-18 05:41:17 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.0019234478240832686 norm:0.0002558724663686007 max memory_allocated 22513.49609375 
[2025-02-18 05:41:50 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.001654839375987649 norm:0.00011196162085980177 max memory_allocated 22513.49609375 
[2025-02-18 05:42:23 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.0015590866096317768 norm:6.979080353630707e-05 max memory_allocated 22513.49609375 
[2025-02-18 05:42:56 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.0015177929308265448 norm:4.4866606913274154e-05 max memory_allocated 22513.49609375 
[2025-02-18 05:43:30 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.0015024548629298806 norm:3.067384386667982e-05 max memory_allocated 22513.49609375 
[2025-02-18 05:44:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.0014878559159114957 norm:2.1400011974037625e-05 max memory_allocated 22513.49609375 
[2025-02-18 05:44:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.0014772191643714905 norm:1.5811703633517027e-05 max memory_allocated 22513.49609375 
[2025-02-18 05:45:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.0014719163300469518 norm:1.2474497452785727e-05 max memory_allocated 22513.49609375 
[2025-02-18 05:45:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0014708234230056405 norm:1.0858078894671053e-05 max memory_allocated 22513.49609375 
[2025-02-18 05:46:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.0014688982628285885 norm:9.515698366158176e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:46:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0014697220176458359 norm:8.972835530585144e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:47:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.001463430467993021 norm:8.796592737780884e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:47:55 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.001461525447666645 norm:8.63171590026468e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:48:29 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.001461127889342606 norm:8.525119483238086e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:49:02 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.0014616509433835745 norm:8.408386747760233e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:49:35 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.0014612870290875435 norm:8.388047717744485e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:50:08 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.0014608886558562517 norm:8.445602361462079e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:50:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.0014606161275878549 norm:8.400128535868134e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:51:15 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.0014602657174691558 norm:8.447796062682755e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:51:48 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.0014590923674404621 norm:8.364739187527448e-06 max memory_allocated 22513.49609375 
[2025-02-18 05:51:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-18 05:52:34 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.0020328217651695013 norm:0.00020453926117625087 max memory_allocated 22513.66796875 
[2025-02-18 05:53:07 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.0017881449311971664 norm:8.987911132862791e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:53:40 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.001697720494121313 norm:5.561232683248818e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:54:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.0016588575672358274 norm:3.944693162338808e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:54:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.0016355256084352732 norm:2.803467577905394e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:55:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.0016211702022701502 norm:2.2150530639919452e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:55:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.0016110209980979562 norm:1.6762149243731983e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:56:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.001603880780749023 norm:1.386553321935935e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:56:59 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.001599869690835476 norm:1.1718097084667534e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:57:33 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.0015968790976330638 norm:1.0249148544971831e-05 max memory_allocated 22513.66796875 
[2025-02-18 05:58:06 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.0015941564925014973 norm:9.75980674411403e-06 max memory_allocated 22513.66796875 
[2025-02-18 05:58:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.0015937555581331253 norm:9.146909178525675e-06 max memory_allocated 22513.66796875 
[2025-02-18 05:59:12 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.0015956796705722809 norm:8.82087078935001e-06 max memory_allocated 22513.66796875 
[2025-02-18 05:59:45 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.0015906136250123382 norm:8.610962140664924e-06 max memory_allocated 22513.66796875 
[2025-02-18 06:00:19 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.0015898963902145624 norm:8.577639164286666e-06 max memory_allocated 22513.66796875 
[2025-02-18 06:00:52 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0015902642626315355 norm:8.407989298575558e-06 max memory_allocated 22513.66796875 
[2025-02-18 06:01:25 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.0015905104810371995 norm:8.395539225602988e-06 max memory_allocated 22513.66796875 
[2025-02-18 06:01:58 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0015894761309027672 norm:8.409639121964574e-06 max memory_allocated 22513.66796875 
[2025-02-18 06:02:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.0015878455014899373 norm:8.456531759293284e-06 max memory_allocated 22513.66796875 
[2025-02-18 06:03:05 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.001588229089975357 norm:8.485435500915628e-06 max memory_allocated 22513.66796875 
[2025-02-18 06:03:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-18 06:03:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.002193460939452052 norm:0.0002459076058585197 max memory_allocated 22513.83984375 
[2025-02-18 06:04:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.0019155853660777211 norm:0.00010320825094822794 max memory_allocated 22513.83984375 
[2025-02-18 06:04:57 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.0018236135365441442 norm:6.65179468342103e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:05:30 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.0017762985080480576 norm:4.7152563638519496e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:06:03 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.0017493532504886389 norm:3.452967939665541e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:06:36 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.0017351328860968351 norm:2.681220939848572e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:07:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.0017243614420294762 norm:2.1051075236755423e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:07:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.001716820988804102 norm:1.717730992822908e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:08:16 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.001709764008410275 norm:1.4007116078573745e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:08:49 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.0017048704903572798 norm:1.2111179785279091e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:09:22 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.0017033766489475965 norm:1.054472249961691e-05 max memory_allocated 22513.83984375 
[2025-02-18 06:09:56 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.001704862923361361 norm:9.459211469220463e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:10:29 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.0017031681491062045 norm:8.441806130576879e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:11:02 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.001700249733403325 norm:8.078213795670308e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:11:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.0016988655552268028 norm:7.73961073718965e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:12:09 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.0017000052612274885 norm:7.654964065295644e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:12:42 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.0016972275916486979 norm:7.624757927260362e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:13:15 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0016962866066023707 norm:7.438121883751592e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:13:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.001696406863629818 norm:7.437843578372849e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:14:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.001696333521977067 norm:7.265682597790146e-06 max memory_allocated 22513.83984375 
[2025-02-18 06:14:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-18 06:15:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.0020324510987848043 norm:7.682891009608284e-05 max memory_allocated 22514.01171875 
[2025-02-18 06:15:40 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0018923062598332763 norm:3.6545337934512645e-05 max memory_allocated 22514.01171875 
[2025-02-18 06:16:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.0018434025114402175 norm:2.2444475689553656e-05 max memory_allocated 22514.01171875 
[2025-02-18 06:16:47 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.0018189745023846626 norm:1.5307765352190472e-05 max memory_allocated 22514.01171875 
[2025-02-18 06:17:20 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.0018031495856121182 norm:1.192990384879522e-05 max memory_allocated 22514.01171875 
[2025-02-18 06:17:53 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.001793252187781036 norm:9.522836990072392e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:18:27 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.001787525718100369 norm:8.383530257560778e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:19:00 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.001785511733032763 norm:7.684954653086606e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:19:33 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.0017840105574578047 norm:6.989568191784201e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:20:06 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.0017819860950112343 norm:6.900622793182265e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:20:39 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.001780070597305894 norm:6.75838327879319e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:21:13 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.0017768887337297201 norm:6.678627414657967e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:21:46 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.0017768382094800472 norm:6.656131063209614e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:22:19 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.0017765701049938798 norm:6.66414189254283e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:22:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.0017762265633791685 norm:6.59758143228828e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:23:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.0017758066533133388 norm:6.664708962489385e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:23:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.001774375792592764 norm:6.658749043708667e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:24:32 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.0017737698508426547 norm:6.650289833487477e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:25:05 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.001773593365214765 norm:6.599822881980799e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:25:38 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.0017729921964928508 norm:6.554031187988585e-06 max memory_allocated 22514.01171875 
[2025-02-18 06:25:48 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-18 06:26:24 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.002325303852558136 norm:0.0002469222235959023 max memory_allocated 22514.18359375 
[2025-02-18 06:26:57 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.0020228216890245676 norm:8.604706090409309e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:27:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.0019532786682248116 norm:6.0987527831457555e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:28:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0019157896749675274 norm:4.227503450238146e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:28:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.0018927602795884013 norm:3.2137177186086774e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:29:10 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.0018743807449936867 norm:2.602245513116941e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:29:43 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.0018659131601452827 norm:2.2241638362174854e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:30:17 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0018606397788971663 norm:1.986994902836159e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:30:50 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.0018581078620627522 norm:1.7706919607007876e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:31:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0018522029276937246 norm:1.4924076822353527e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:31:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.0018481985898688436 norm:1.3300873433763627e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:32:29 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.0018452024087309837 norm:1.1799189451267011e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:33:03 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.0018445021705701947 norm:1.068886285793269e-05 max memory_allocated 22514.18359375 
[2025-02-18 06:33:36 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.00184067920781672 norm:9.245417459169403e-06 max memory_allocated 22514.18359375 
[2025-02-18 06:34:09 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.0018403029534965754 norm:8.18487023934722e-06 max memory_allocated 22514.18359375 
[2025-02-18 06:34:42 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.0018375772051513195 norm:7.585741968796356e-06 max memory_allocated 22514.18359375 
[2025-02-18 06:35:16 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.001836791867390275 norm:6.635584213654511e-06 max memory_allocated 22514.18359375 
[2025-02-18 06:35:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.0018369239987805486 norm:6.250224032555707e-06 max memory_allocated 22514.18359375 
[2025-02-18 06:36:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.0018375655636191368 norm:5.99005761614535e-06 max memory_allocated 22514.18359375 
[2025-02-18 06:36:55 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.0018379809334874153 norm:5.867806066817138e-06 max memory_allocated 22514.18359375 
[2025-02-18 06:37:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-18 06:37:41 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0021789735183119774 norm:0.00013959407806396484 max memory_allocated 22514.35546875 
[2025-02-18 06:38:14 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.002023761859163642 norm:6.383048457792029e-05 max memory_allocated 22514.35546875 
[2025-02-18 06:38:47 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.0019667502492666245 norm:4.0050337702268735e-05 max memory_allocated 22514.35546875 
[2025-02-18 06:39:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.001942841219715774 norm:2.8265027140150778e-05 max memory_allocated 22514.35546875 
[2025-02-18 06:39:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.0019272019853815436 norm:2.1318350263754837e-05 max memory_allocated 22514.35546875 
[2025-02-18 06:40:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.00191697315312922 norm:1.6523539670743048e-05 max memory_allocated 22514.35546875 
[2025-02-18 06:41:00 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.0019120180513709784 norm:1.3119709365128074e-05 max memory_allocated 22514.35546875 
[2025-02-18 06:41:33 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.0019051686394959688 norm:1.1054313290514983e-05 max memory_allocated 22514.35546875 
[2025-02-18 06:42:06 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.0019019459141418338 norm:9.082079486688599e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:42:40 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0018998815212398767 norm:7.76611705077812e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:43:13 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.001898517832159996 norm:7.135799933166709e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:43:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.0018975351704284549 norm:6.530940027005272e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:44:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.0018951238598674536 norm:6.120866146375192e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:44:53 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.0018952623941004276 norm:5.908737875870429e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:45:26 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0018949548248201609 norm:5.8067644204129465e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:45:59 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.0018955013947561383 norm:5.653754669765476e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:46:32 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.001896001398563385 norm:5.649098056892399e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:47:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.0018958409782499075 norm:5.632788543152856e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:47:39 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.0018945662304759026 norm:5.649907052429626e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:48:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.001893076580017805 norm:5.612043423752766e-06 max memory_allocated 22514.35546875 
[2025-02-18 06:48:21 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-18 06:48:57 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.002183946082368493 norm:7.56971348891966e-05 max memory_allocated 22514.52734375 
[2025-02-18 06:49:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.0020676564890891314 norm:3.970324542024173e-05 max memory_allocated 22514.52734375 
[2025-02-18 06:50:04 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.0020170786883682013 norm:2.5741815989022143e-05 max memory_allocated 22514.52734375 
[2025-02-18 06:50:37 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0019928847905248404 norm:1.930000144056976e-05 max memory_allocated 22514.52734375 
[2025-02-18 06:51:10 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.0019793508108705282 norm:1.5375531802419573e-05 max memory_allocated 22514.52734375 
[2025-02-18 06:51:44 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.001968467142432928 norm:1.2225102182128467e-05 max memory_allocated 22514.52734375 
[2025-02-18 06:52:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.001964183058589697 norm:1.0452057722432073e-05 max memory_allocated 22514.52734375 
[2025-02-18 06:52:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.0019596663769334555 norm:9.082215001399163e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:53:23 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.001956289866939187 norm:7.6220831033424474e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:53:56 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0019535664469003677 norm:7.126984201022424e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:54:30 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.0019524535164237022 norm:6.3879683693812694e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:55:03 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.001949395751580596 norm:6.0449729062383994e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:55:36 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.0019508174154907465 norm:5.8533664741844404e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:56:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.0019488492980599403 norm:5.696679636457702e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:56:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.0019487757235765457 norm:5.6251874411827885e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:57:16 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.0019484591903164983 norm:5.452019195217872e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:57:49 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.0019471512641757727 norm:5.3781150199938565e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:58:22 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.001947570824995637 norm:5.354861968953628e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:58:55 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.0019483781652525067 norm:5.306907951307949e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:59:29 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.0019467257661744952 norm:5.2997270358901005e-06 max memory_allocated 22514.52734375 
[2025-02-18 06:59:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-18 07:00:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.0022087031975388527 norm:5.317800605553202e-05 max memory_allocated 22514.69921875 
[2025-02-18 07:00:47 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.002115872222930193 norm:2.9422091756714508e-05 max memory_allocated 22514.69921875 
[2025-02-18 07:01:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.0020723973866552114 norm:1.9076042008236982e-05 max memory_allocated 22514.69921875 
[2025-02-18 07:01:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.002053481061011553 norm:1.4587350960937329e-05 max memory_allocated 22514.69921875 
[2025-02-18 07:02:27 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.0020420707296580076 norm:1.1423861906223465e-05 max memory_allocated 22514.69921875 
[2025-02-18 07:03:00 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.0020358196925371885 norm:9.652093467593659e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:03:33 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.002030409872531891 norm:8.176850315066986e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:04:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.0020276610739529133 norm:7.152924808906391e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:04:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.0020240782760083675 norm:6.243797088245628e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:05:13 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.0020224975887686014 norm:5.719800356018823e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:05:46 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.0020219916477799416 norm:5.4540359997190535e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:06:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0020224065519869328 norm:5.28847704117652e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:06:53 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.0020224356558173895 norm:5.140982466400601e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:07:26 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.002020827028900385 norm:5.0799353630281985e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:07:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.0020215436816215515 norm:5.0088406169379596e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:08:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.002020798623561859 norm:4.971604994352674e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:09:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.002020777203142643 norm:4.950173206452746e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:09:39 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.002021158579736948 norm:4.9199188651982695e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:10:12 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.0020217904821038246 norm:4.901881766272709e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:10:45 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.0020206067711114883 norm:4.832484137295978e-06 max memory_allocated 22514.69921875 
[2025-02-18 07:10:55 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-18 07:11:31 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.0023603616282343864 norm:0.00012994420831091702 max memory_allocated 22514.87109375 
[2025-02-18 07:12:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0022046437952667475 norm:6.318336818367243e-05 max memory_allocated 22514.87109375 
[2025-02-18 07:12:37 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.002139679156243801 norm:3.9936767279868945e-05 max memory_allocated 22514.87109375 
[2025-02-18 07:13:11 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.002105539431795478 norm:2.8160047804703936e-05 max memory_allocated 22514.87109375 
[2025-02-18 07:13:44 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.002085912274196744 norm:2.139114985766355e-05 max memory_allocated 22514.87109375 
[2025-02-18 07:14:17 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0020743869245052338 norm:1.7466240024077706e-05 max memory_allocated 22514.87109375 
[2025-02-18 07:14:50 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.0020647214259952307 norm:1.4161732906359248e-05 max memory_allocated 22514.87109375 
[2025-02-18 07:15:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.002058119047433138 norm:1.1692883163050283e-05 max memory_allocated 22514.87109375 
[2025-02-18 07:15:57 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.0020541520789265633 norm:1.014276404021075e-05 max memory_allocated 22514.87109375 
[2025-02-18 07:16:30 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.002049942733719945 norm:8.626420822110958e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:17:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.0020471306052058935 norm:7.763765097479336e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:17:36 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.0020450162701308727 norm:7.17977400199743e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:18:10 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.0020432230085134506 norm:6.379583737725625e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:18:43 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.0020427899435162544 norm:5.722012247133534e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:19:16 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.002042677253484726 norm:5.3863941502640955e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:19:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.002043959219008684 norm:5.132923888595542e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:20:22 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.002043799962848425 norm:4.933676336804638e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:20:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.002043389016762376 norm:4.8736173994257115e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:21:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.0020434949547052383 norm:4.77426601719344e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:22:02 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.0020433145109564066 norm:4.75040133096627e-06 max memory_allocated 22514.87109375 
[2025-02-18 07:22:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-18 07:22:48 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.002332468284294009 norm:5.612733366433531e-05 max memory_allocated 22515.04296875 
[2025-02-18 07:23:21 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.002239104826003313 norm:2.9076631108182482e-05 max memory_allocated 22515.04296875 
[2025-02-18 07:23:54 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0021987881045788527 norm:2.0016283087898046e-05 max memory_allocated 22515.04296875 
[2025-02-18 07:24:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.002175931353121996 norm:1.5052740309329238e-05 max memory_allocated 22515.04296875 
[2025-02-18 07:25:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.0021600292529910803 norm:1.1995708518952597e-05 max memory_allocated 22515.04296875 
[2025-02-18 07:25:34 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.002150891115888953 norm:1.0245355042570736e-05 max memory_allocated 22515.04296875 
[2025-02-18 07:26:07 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.002144495490938425 norm:8.548818186682183e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:26:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.002139283576980233 norm:7.3787959991022944e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:27:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0021359543316066265 norm:6.828207915532403e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:27:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.002133495407178998 norm:6.165329978102818e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:28:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0021318397484719753 norm:5.735446848120773e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:28:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.0021294376347213984 norm:5.487340786203276e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:29:26 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.002127697691321373 norm:5.345526005839929e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:30:00 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.002127634361386299 norm:5.2188838708389085e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:30:33 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0021260548382997513 norm:5.137076186656486e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:31:06 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.0021255870815366507 norm:5.070201041235123e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:31:39 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.0021259526256471872 norm:4.994008577341447e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:32:12 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.0021250685676932335 norm:4.930402155878255e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:32:46 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.0021250899881124496 norm:4.937469384458382e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:33:19 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.002124546794220805 norm:4.905578862235416e-06 max memory_allocated 22515.04296875 
[2025-02-18 07:33:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-18 07:34:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.002802868140861392 norm:0.00025793336681090295 max memory_allocated 22515.21484375 
[2025-02-18 07:34:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.0023974827490746975 norm:6.869831122457981e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:35:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.0023305544164031744 norm:4.8459394747624174e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:35:44 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.00229520327411592 norm:3.855816612485796e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:36:17 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.0022582728415727615 norm:2.7866954042110592e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:36:50 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.0022386189084500074 norm:2.2622545657213777e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:37:24 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.002224419265985489 norm:1.8625576558406465e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:37:57 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.002215977758169174 norm:1.609257742529735e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:38:30 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.0022065183147788048 norm:1.3655157999892253e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:39:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.0022011548280715942 norm:1.2014110325253569e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:39:36 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.002197665860876441 norm:1.083921506506158e-05 max memory_allocated 22515.21484375 
[2025-02-18 07:40:10 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.002193860709667206 norm:9.7786178230308e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:40:43 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0021909780334681273 norm:8.685335160407703e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:41:16 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.002188759157434106 norm:7.937408554425929e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:41:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.002187947044149041 norm:7.196710612333845e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:42:23 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.0021864711306989193 norm:6.574664439540356e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:42:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.0021856504026800394 norm:6.0482207118184306e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:43:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.0021858508698642254 norm:5.815668828290654e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:44:02 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.0021858103573322296 norm:5.5415762290067505e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:44:35 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.0021855481900274754 norm:5.303110810928047e-06 max memory_allocated 22515.21484375 
[2025-02-18 07:44:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-18 07:45:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.003100084140896797 norm:0.0004071737639605999 max memory_allocated 22515.38671875 
[2025-02-18 07:45:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.002595067722722888 norm:0.00010206429578829557 max memory_allocated 22515.38671875 
[2025-02-18 07:46:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.0024990420788526535 norm:6.773333734599873e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:47:01 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.002448166022077203 norm:5.2053495892323554e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:47:34 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.0024093925021588802 norm:4.028654802823439e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:48:07 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.002384686842560768 norm:3.247070708312094e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:48:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.002363544190302491 norm:2.6474972401047125e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:49:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.0023503194097429514 norm:2.2976768377702683e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:49:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.0023396839387714863 norm:2.0265992134227417e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:50:20 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.002330315764993429 norm:1.7996977476286702e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:50:53 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.0023229694925248623 norm:1.6104107999126427e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:51:26 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.0023171156644821167 norm:1.479207548982231e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:52:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.0023139161057770252 norm:1.3613464034278877e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:52:33 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.0023099344689399004 norm:1.2380623047647532e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:53:06 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.0023077107034623623 norm:1.0850459148059599e-05 max memory_allocated 22515.38671875 
[2025-02-18 07:53:39 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.002307537477463484 norm:9.602532372809947e-06 max memory_allocated 22515.38671875 
[2025-02-18 07:54:13 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.002306044101715088 norm:8.93186097528087e-06 max memory_allocated 22515.38671875 
[2025-02-18 07:54:46 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.0023046659771353006 norm:8.27368967293296e-06 max memory_allocated 22515.38671875 
[2025-02-18 07:55:19 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.0023040345404297113 norm:8.031683137232903e-06 max memory_allocated 22515.38671875 
[2025-02-18 07:55:52 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.002303618937730789 norm:7.775472113280557e-06 max memory_allocated 22515.38671875 
[2025-02-18 07:56:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-18 07:56:38 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.0031560896895825863 norm:0.00040818940033204854 max memory_allocated 22515.55859375 
[2025-02-18 07:57:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.0026703299954533577 norm:0.00010050284618046135 max memory_allocated 22515.55859375 
[2025-02-18 07:57:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.002584496047347784 norm:6.427225889638066e-05 max memory_allocated 22515.55859375 
[2025-02-18 07:58:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.0025465108919888735 norm:5.2801518904743716e-05 max memory_allocated 22515.55859375 
[2025-02-18 07:58:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.00252147251740098 norm:4.280557914171368e-05 max memory_allocated 22515.55859375 
[2025-02-18 07:59:24 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.0024948115460574627 norm:3.2311651011696085e-05 max memory_allocated 22515.55859375 
[2025-02-18 07:59:57 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.0024790107272565365 norm:2.6897956558968872e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:00:30 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.0024695838801562786 norm:2.310166382812895e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:01:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.0024635721929371357 norm:2.045897963398602e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:01:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.002456590998917818 norm:1.8217077013105154e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:02:10 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.002451495733112097 norm:1.6466365195810795e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:02:43 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.0024474773090332747 norm:1.5151183106354438e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:03:16 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.0024427541065961123 norm:1.3797968676954042e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:03:50 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.0024392458144575357 norm:1.2680943655141164e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:04:23 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.002437348710373044 norm:1.1582686965994071e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:04:56 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.002436181530356407 norm:1.0799727533594705e-05 max memory_allocated 22515.55859375 
[2025-02-18 08:05:29 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.002435330767184496 norm:9.91916203929577e-06 max memory_allocated 22515.55859375 
[2025-02-18 08:06:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.002432948909699917 norm:9.20491038414184e-06 max memory_allocated 22515.55859375 
[2025-02-18 08:06:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.002432226436212659 norm:8.42364715936128e-06 max memory_allocated 22515.55859375 
[2025-02-18 08:07:09 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.0024317579809576273 norm:7.811077921360265e-06 max memory_allocated 22515.55859375 
[2025-02-18 08:07:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-18 08:07:54 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.0035289262887090445 norm:0.0004012132412753999 max memory_allocated 22515.73046875 
[2025-02-18 08:08:28 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.0030024820007383823 norm:0.00011460995301604271 max memory_allocated 22515.73046875 
[2025-02-18 08:09:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.0029059734661132097 norm:8.22443253127858e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:09:34 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.0028579377103596926 norm:6.70163644826971e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:10:07 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.002807827666401863 norm:5.020754906581715e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:10:40 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.002776256762444973 norm:3.9506932807853445e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:11:14 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.002755329944193363 norm:3.322083648527041e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:11:47 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.002741199918091297 norm:2.8008886147290468e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:12:20 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.002732343040406704 norm:2.371098889852874e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:12:53 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.002726383972913027 norm:2.0432840756257065e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:13:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.002717844443395734 norm:1.7286118236370385e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:14:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.0027134728152304888 norm:1.4901652320986614e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:14:33 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.002707700477913022 norm:1.3191491234465502e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:15:06 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.0027050389908254147 norm:1.1918085874640383e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:15:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.0027027446776628494 norm:1.1060828910558484e-05 max memory_allocated 22515.73046875 
[2025-02-18 08:16:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.0027002892456948757 norm:9.862997103482485e-06 max memory_allocated 22515.73046875 
[2025-02-18 08:16:45 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.002698793774470687 norm:9.610005690774415e-06 max memory_allocated 22515.73046875 
[2025-02-18 08:17:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.0026978508103638887 norm:8.593192433181684e-06 max memory_allocated 22515.73046875 
[2025-02-18 08:17:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.0026970175094902515 norm:8.126150532916654e-06 max memory_allocated 22515.73046875 
[2025-02-18 08:18:25 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.0026952940970659256 norm:7.861332051106729e-06 max memory_allocated 22515.73046875 
[2025-02-18 08:18:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-18 08:19:11 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.0034544621594250202 norm:0.00023451869492419064 max memory_allocated 22515.90234375 
[2025-02-18 08:19:44 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.003169179428368807 norm:9.088760270969942e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:20:17 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.0031030792742967606 norm:6.689634756185114e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:20:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.003055376000702381 norm:4.8971563956001773e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:21:23 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.0030219461768865585 norm:3.6596386053133756e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:21:57 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.0030018447432667017 norm:2.932316238002386e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:22:30 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0029851607978343964 norm:2.4128092263708822e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:23:03 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.0029740154277533293 norm:1.991102908505127e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:23:36 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0029670679941773415 norm:1.66637146321591e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:24:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.0029616677202284336 norm:1.4295623259386048e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:24:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.0029570001643151045 norm:1.2633155165531207e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:25:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.00295325368642807 norm:1.1141156392113771e-05 max memory_allocated 22515.90234375 
[2025-02-18 08:25:49 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.0029510287567973137 norm:9.826578207139391e-06 max memory_allocated 22515.90234375 
[2025-02-18 08:26:22 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.002949470654129982 norm:8.774775778874755e-06 max memory_allocated 22515.90234375 
[2025-02-18 08:26:56 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.0029479716904461384 norm:7.978948815434705e-06 max memory_allocated 22515.90234375 
[2025-02-18 08:27:29 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.002945094835013151 norm:7.44276348996209e-06 max memory_allocated 22515.90234375 
[2025-02-18 08:28:02 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.002944791689515114 norm:6.948224836378358e-06 max memory_allocated 22515.90234375 
[2025-02-18 08:28:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.002943754196166992 norm:6.646385827480117e-06 max memory_allocated 22515.90234375 
[2025-02-18 08:29:08 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.0029434857424348593 norm:6.3927345763659105e-06 max memory_allocated 22515.90234375 
[2025-02-18 08:29:42 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.0029418878257274628 norm:6.105313786974875e-06 max memory_allocated 22515.90234375 
[2025-02-18 08:29:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-18 08:30:27 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.004443657584488392 norm:0.000671049696393311 max memory_allocated 22516.07421875 
[2025-02-18 08:31:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.003714085789397359 norm:0.0001847194944275543 max memory_allocated 22516.07421875 
[2025-02-18 08:31:34 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.003540023695677519 norm:0.00010184841812588274 max memory_allocated 22516.07421875 
[2025-02-18 08:32:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.003478954778984189 norm:7.950823055580258e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:32:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.0034428685903549194 norm:6.732859037583694e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:33:13 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.0034089242108166218 norm:5.524617154151201e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:33:47 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.0033793749753385782 norm:4.34430330642499e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:34:20 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.003357933135703206 norm:3.619097333285026e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:34:53 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.00334565294906497 norm:3.078092777286656e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:35:26 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.003335540182888508 norm:2.6626217731973156e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:35:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.00332648865878582 norm:2.3819560738047585e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:36:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.0033183249179273844 norm:2.103759652527515e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:37:06 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.003313605207949877 norm:1.9201499526388943e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:37:39 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.0033067981712520123 norm:1.727215749269817e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:38:12 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.003301922231912613 norm:1.594241439306643e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:38:46 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.003300168551504612 norm:1.4314680811366998e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:39:19 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.0032976726070046425 norm:1.3048436812823638e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:39:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0032948930747807026 norm:1.21387829494779e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:40:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.0032923799008131027 norm:1.1223758519918192e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:40:58 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.0032906758133322 norm:1.0545041732257232e-05 max memory_allocated 22516.07421875 
[2025-02-18 08:41:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-18 08:41:44 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.004389158915728331 norm:0.0004531895974650979 max memory_allocated 22516.24609375 
[2025-02-18 08:42:17 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.0039267875254154205 norm:0.00013635271170642227 max memory_allocated 22516.24609375 
[2025-02-18 08:42:50 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.0038314294070005417 norm:8.678283484186977e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:43:24 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.003793231677263975 norm:7.016982999630272e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:43:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.0037601329386234283 norm:5.534183219424449e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:44:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.0037331758067011833 norm:4.363409607321955e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:45:03 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.0037138797342777252 norm:3.6188317608321086e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:45:36 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.003699597204104066 norm:3.12815245706588e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:46:10 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.003688163124024868 norm:2.729072730289772e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:46:43 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.0036789034493267536 norm:2.419999691483099e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:47:16 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.0036694351583719254 norm:2.1566298528341576e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:47:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.0036636684089899063 norm:1.938796958711464e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:48:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.0036589009687304497 norm:1.741568667057436e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:48:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.003655454143881798 norm:1.5767214790685102e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:49:29 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.003652075072750449 norm:1.3959175703348592e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:50:02 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.0036504340823739767 norm:1.3008587302465457e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:50:35 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.0036479481495916843 norm:1.2009090823994484e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:51:09 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.0036461721174418926 norm:1.084509403881384e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:51:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.003643984440714121 norm:1.0081316759169567e-05 max memory_allocated 22516.24609375 
[2025-02-18 08:52:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.0036419376265257597 norm:9.25519907468697e-06 max memory_allocated 22516.24609375 
[2025-02-18 08:52:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-18 08:53:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.005131854675710201 norm:0.0006514968117699027 max memory_allocated 22516.41796875 
[2025-02-18 08:53:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.004404516890645027 norm:0.00016092995065264404 max memory_allocated 22516.41796875 
[2025-02-18 08:54:07 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.004269334487617016 norm:9.125796350417659e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:54:40 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.004236789885908365 norm:7.831858238205314e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:55:14 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.004217705689370632 norm:7.128845754778013e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:55:47 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.004185983445495367 norm:5.6198754464276135e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:56:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.0041612605564296246 norm:4.355038981884718e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:56:53 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.004148160573095083 norm:8.063659333856776e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:57:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.0041403863579034805 norm:3.037959322682582e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:58:00 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.004135143477469683 norm:2.6109169994015247e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:58:33 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.0041305385529994965 norm:2.2903674107510597e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:59:06 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.004124400671571493 norm:2.0517754819593392e-05 max memory_allocated 22516.41796875 
[2025-02-18 08:59:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.004120125900954008 norm:1.7973206922761165e-05 max memory_allocated 22516.41796875 
[2025-02-18 09:00:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.004117184784263372 norm:1.6225418221438304e-05 max memory_allocated 22516.41796875 
[2025-02-18 09:00:45 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.0041158986277878284 norm:1.417247131030308e-05 max memory_allocated 22516.41796875 
[2025-02-18 09:01:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.00411379337310791 norm:1.2658730156545062e-05 max memory_allocated 22516.41796875 
[2025-02-18 09:01:52 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.004114347510039806 norm:1.1363710655132309e-05 max memory_allocated 22516.41796875 
[2025-02-18 09:02:25 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.004112520255148411 norm:1.0652719538484234e-05 max memory_allocated 22516.41796875 
[2025-02-18 09:02:58 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.004111520946025848 norm:9.704014701128472e-06 max memory_allocated 22516.41796875 
[2025-02-18 09:03:31 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.004111588932573795 norm:8.656666977913119e-06 max memory_allocated 22516.41796875 
[2025-02-18 09:03:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-18 09:04:17 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.005146699957549572 norm:0.00028192761237733066 max memory_allocated 22516.58984375 
[2025-02-18 09:04:50 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.004894778598099947 norm:0.00011807559349108487 max memory_allocated 22516.58984375 
[2025-02-18 09:05:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.004819502588361502 norm:8.018573134904727e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:05:56 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.00478003267198801 norm:6.0158792621223256e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:06:29 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.004749822895973921 norm:4.563254697131924e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:07:03 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.004729049280285835 norm:3.624862074502744e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:07:36 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.004715775139629841 norm:2.866625800379552e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:08:09 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.0047067697159945965 norm:2.337662772333715e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:08:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.004700839519500732 norm:1.955399966391269e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:09:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.004696229472756386 norm:1.6628573575871997e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:09:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.00469197379425168 norm:1.4126001588010695e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:10:22 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.004689045250415802 norm:1.2021116162941325e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:10:55 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.004687059670686722 norm:1.0474521332071163e-05 max memory_allocated 22516.58984375 
[2025-02-18 09:11:28 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.004684149287641048 norm:9.152491657005157e-06 max memory_allocated 22516.58984375 
[2025-02-18 09:12:02 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.0046822503209114075 norm:8.188529136532452e-06 max memory_allocated 22516.58984375 
[2025-02-18 09:12:35 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.0046820323914289474 norm:7.312286925298395e-06 max memory_allocated 22516.58984375 
[2025-02-18 09:13:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.004680668003857136 norm:6.6390257416060194e-06 max memory_allocated 22516.58984375 
[2025-02-18 09:13:41 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.004678723867982626 norm:6.328483777906513e-06 max memory_allocated 22516.58984375 
[2025-02-18 09:14:15 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.004678819794207811 norm:5.9595672610157635e-06 max memory_allocated 22516.58984375 
[2025-02-18 09:14:48 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.004677456803619862 norm:5.9378662626841106e-06 max memory_allocated 22516.58984375 
[2025-02-18 09:14:57 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-18 09:15:33 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.005947188474237919 norm:0.00044841127237305045 max memory_allocated 22516.76171875 
[2025-02-18 09:16:07 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.0055423094891011715 norm:0.000154281675349921 max memory_allocated 22516.76171875 
[2025-02-18 09:16:40 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.00544312596321106 norm:9.503510955255479e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:17:13 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.005396591499447823 norm:7.275156531250104e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:17:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.00536364084109664 norm:5.760283602285199e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:18:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.005339425522834063 norm:4.589726449921727e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:18:52 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.005323844030499458 norm:3.6794655898120254e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:19:26 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.005313928239047527 norm:3.0012453862582333e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:19:59 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.005306565202772617 norm:2.4552658942411654e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:20:32 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.005295696668326855 norm:2.0897803551633842e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:21:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.005289147607982159 norm:1.810690446291119e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:21:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.0052856579422950745 norm:1.5552728655165993e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:22:11 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.005281399004161358 norm:1.3360139746509958e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:22:44 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.0052781300619244576 norm:1.1574010386539157e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:23:18 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.005275134462863207 norm:1.0232746717520058e-05 max memory_allocated 22516.76171875 
[2025-02-18 09:23:51 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.005274940747767687 norm:8.967182111518923e-06 max memory_allocated 22516.76171875 
[2025-02-18 09:24:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.005273382645100355 norm:8.410680493398104e-06 max memory_allocated 22516.76171875 
[2025-02-18 09:24:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.005271340720355511 norm:7.685246600885876e-06 max memory_allocated 22516.76171875 
[2025-02-18 09:25:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.005271801725029945 norm:7.041467142698821e-06 max memory_allocated 22516.76171875 
[2025-02-18 09:26:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.00527225062251091 norm:6.546257282025181e-06 max memory_allocated 22516.76171875 
[2025-02-18 09:26:13 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-18 09:26:49 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.007117417175322771 norm:0.0004258926201146096 max memory_allocated 22516.93359375 
[2025-02-18 09:27:22 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.00663172360509634 norm:0.00021190629922784865 max memory_allocated 22516.93359375 
[2025-02-18 09:27:56 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.006473387125879526 norm:0.00015084256301634014 max memory_allocated 22516.93359375 
[2025-02-18 09:28:29 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.0063840714283287525 norm:0.00011818968050647527 max memory_allocated 22516.93359375 
[2025-02-18 09:29:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.006316752173006535 norm:9.510420204605907e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:29:35 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.006265687756240368 norm:7.86501113907434e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:30:08 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.006226152181625366 norm:6.629231211263686e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:30:42 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.0061911121010780334 norm:5.790785144199617e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:31:15 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.006163888610899448 norm:4.955056283506565e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:31:48 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.006143763195723295 norm:4.3426007323432714e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:32:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.006131390575319529 norm:3.8422203942900524e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:32:54 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.0061117298901081085 norm:3.433983147260733e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:33:27 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.006103686057031155 norm:3.0698240152560174e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:34:01 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.006094624288380146 norm:2.780125942081213e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:34:34 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.006084943190217018 norm:2.5108211048063822e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:35:07 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.006076394580304623 norm:2.27884065679973e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:35:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.006071957293897867 norm:2.07913362828549e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:36:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.006071601528674364 norm:1.938773311849218e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:36:46 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.006062238477170467 norm:1.786692882888019e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:37:20 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.006060193292796612 norm:1.6165948181878775e-05 max memory_allocated 22516.93359375 
[2025-02-18 09:37:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-18 09:38:05 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.007673345040529966 norm:0.00047555993660353124 max memory_allocated 22517.10546875 
[2025-02-18 09:38:38 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.0072107077576220036 norm:0.00018408479809295386 max memory_allocated 22517.10546875 
[2025-02-18 09:39:12 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.007074002176523209 norm:0.00011324926890665665 max memory_allocated 22517.10546875 
[2025-02-18 09:39:45 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.007015460170805454 norm:8.611448720330372e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:40:18 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.006975491531193256 norm:6.817471876274794e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:40:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.006940677762031555 norm:5.7086657761828974e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:41:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.006915011443197727 norm:4.756666021421552e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:41:57 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.006897070445120335 norm:3.889313302352093e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:42:30 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.006884990260004997 norm:3.246016422053799e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:43:04 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.006875484716147184 norm:2.78170446108561e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:43:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.006870483048260212 norm:2.3839791538193822e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:44:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.006864600814878941 norm:2.066694105451461e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:44:43 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.006860391236841679 norm:1.81654941115994e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:45:16 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.006854867096990347 norm:1.599481765879318e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:45:50 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.006855772342532873 norm:1.36316866701236e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:46:23 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.006853731349110603 norm:1.2053625141561497e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:46:56 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.006853572092950344 norm:1.0397450751042925e-05 max memory_allocated 22517.10546875 
[2025-02-18 09:47:29 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.006848938763141632 norm:9.492193385085557e-06 max memory_allocated 22517.10546875 
[2025-02-18 09:48:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.006848702672868967 norm:8.456901014142204e-06 max memory_allocated 22517.10546875 
[2025-02-18 09:48:35 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.006849353201687336 norm:7.610692591697443e-06 max memory_allocated 22517.10546875 
[2025-02-18 09:48:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-18 09:49:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.008540808223187923 norm:0.0004193389613647014 max memory_allocated 22517.27734375 
[2025-02-18 09:49:54 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.008123664185404778 norm:0.000157566464622505 max memory_allocated 22517.27734375 
[2025-02-18 09:50:27 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.008027390576899052 norm:0.00011253851698711514 max memory_allocated 22517.27734375 
[2025-02-18 09:51:01 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.007969100959599018 norm:8.623457688372582e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:51:34 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.007924137637019157 norm:6.693125033052638e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:52:07 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.007894814014434814 norm:5.3679803386330605e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:52:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.0078694773837924 norm:6.812171341152862e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:53:13 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.007854343391954899 norm:3.6163801269140095e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:53:46 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.00784139521420002 norm:3.129389006062411e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:54:20 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.007830841466784477 norm:2.7602256523096003e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:54:53 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.007822712883353233 norm:2.4123779439833015e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:55:26 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.007816335186362267 norm:2.0954976207576692e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:55:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.007811121642589569 norm:1.793710544006899e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:56:33 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.007806034293025732 norm:1.5783312846906483e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:57:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.007803058717399836 norm:1.3703912372875493e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:57:39 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.00780112249776721 norm:1.2015776519547217e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:58:12 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.007799119222909212 norm:1.0588412806100678e-05 max memory_allocated 22517.27734375 
[2025-02-18 09:58:45 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.007796957157552242 norm:9.37158620217815e-06 max memory_allocated 22517.27734375 
[2025-02-18 09:59:18 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.007794713135808706 norm:8.439094926870894e-06 max memory_allocated 22517.27734375 
[2025-02-18 09:59:52 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.007795738987624645 norm:7.510249815823045e-06 max memory_allocated 22517.27734375 
[2025-02-18 10:00:01 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-18 10:00:37 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.00989675521850586 norm:0.00044470978900790215 max memory_allocated 22517.44921875 
[2025-02-18 10:01:10 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.009416812099516392 norm:0.00018392747733742 max memory_allocated 22517.44921875 
[2025-02-18 10:01:44 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.009286610409617424 norm:0.00012843574222642928 max memory_allocated 22517.44921875 
[2025-02-18 10:02:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.009210105054080486 norm:9.880190918920562e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:02:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.009151998907327652 norm:7.751942030154169e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:03:23 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.009100565686821938 norm:6.11328287050128e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:03:56 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.00906631164252758 norm:5.005339698982425e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:04:29 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.009043657220900059 norm:4.2710089473985136e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:05:03 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.009025493636727333 norm:3.650200960692018e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:05:36 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.009013590402901173 norm:3.131605262751691e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:06:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.008998312056064606 norm:2.7060123102273792e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:06:42 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.008987199515104294 norm:2.3544853320345283e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:07:15 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.00897816102951765 norm:2.0549776309053414e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:07:49 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.00897280778735876 norm:1.8059312424156815e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:08:22 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.0089670205488801 norm:1.588608392921742e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:08:55 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.008963351137936115 norm:1.3877184755983762e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:09:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.008961684070527554 norm:1.2050119039486162e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:10:01 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.008961292915046215 norm:1.0835482498805504e-05 max memory_allocated 22517.44921875 
[2025-02-18 10:10:34 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.00896071270108223 norm:9.728286386234686e-06 max memory_allocated 22517.44921875 
[2025-02-18 10:11:08 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.00895709078758955 norm:9.073452929442283e-06 max memory_allocated 22517.44921875 
[2025-02-18 10:11:17 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-18 10:11:53 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.011195995844900608 norm:0.00033174650161527097 max memory_allocated 22517.62109375 
[2025-02-18 10:12:27 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.010786309838294983 norm:0.0001467226684326306 max memory_allocated 22517.62109375 
[2025-02-18 10:13:00 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.010629580356180668 norm:9.934595436789095e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:13:33 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.010536880232393742 norm:7.026488310657442e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:14:06 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.010481356643140316 norm:5.329572013579309e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:14:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.010438553988933563 norm:4.3324707803549245e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:15:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.010408909991383553 norm:3.63116487278603e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:15:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.010387538000941277 norm:3.119771645287983e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:16:19 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.010370556265115738 norm:2.6271909518982284e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:16:52 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.010356047190725803 norm:2.267408126499504e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:17:25 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.010345435701310635 norm:1.9600673113018274e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:17:59 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.010337448678910732 norm:1.7144928278867155e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:18:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.010330041870474815 norm:1.5282968888641335e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:19:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.01032380573451519 norm:1.3690045307157561e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:19:38 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.010317424312233925 norm:1.260970293515129e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:20:11 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.01031468901783228 norm:1.14340882646502e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:20:45 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.010310241021215916 norm:1.0665412446542177e-05 max memory_allocated 22517.62109375 
[2025-02-18 10:21:18 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.010307525284588337 norm:9.875237992673647e-06 max memory_allocated 22517.62109375 
[2025-02-18 10:21:51 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.0103038614615798 norm:9.205472451867536e-06 max memory_allocated 22517.62109375 
[2025-02-18 10:22:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.010301223956048489 norm:8.779770723776892e-06 max memory_allocated 22517.62109375 
[2025-02-18 10:22:34 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-18 10:23:10 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.02145887352526188 norm:0.0011350512504577637 max memory_allocated 22517.79296875 
[2025-02-18 10:23:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.016914334148168564 norm:0.00044794727000407875 max memory_allocated 22517.79296875 
[2025-02-18 10:24:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.016091546043753624 norm:0.0003360988339409232 max memory_allocated 22517.79296875 
[2025-02-18 10:24:49 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.015638962388038635 norm:0.0002462496340740472 max memory_allocated 22517.79296875 
[2025-02-18 10:25:23 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.01540062204003334 norm:0.00019868387607857585 max memory_allocated 22517.79296875 
[2025-02-18 10:25:56 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.015282366424798965 norm:0.00018274736066814512 max memory_allocated 22517.79296875 
[2025-02-18 10:26:29 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.01515208464115858 norm:0.00016120928921736777 max memory_allocated 22517.79296875 
[2025-02-18 10:27:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.015108190476894379 norm:0.00015994877321645617 max memory_allocated 22517.79296875 
[2025-02-18 10:27:35 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.015040834434330463 norm:0.00015160240582190454 max memory_allocated 22517.79296875 
[2025-02-18 10:28:08 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.014992034062743187 norm:0.00014134908269625157 max memory_allocated 22517.79296875 
[2025-02-18 10:28:41 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.01497594639658928 norm:0.00014272281259763986 max memory_allocated 22517.79296875 
[2025-02-18 10:29:15 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.01491895504295826 norm:0.000137795927003026 max memory_allocated 22517.79296875 
[2025-02-18 10:29:48 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.014917980879545212 norm:0.000139927287818864 max memory_allocated 22517.79296875 
[2025-02-18 10:30:21 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.01486828550696373 norm:0.00013162741379346699 max memory_allocated 22517.79296875 
[2025-02-18 10:30:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.014872923493385315 norm:0.0001360635505989194 max memory_allocated 22517.79296875 
[2025-02-18 10:31:27 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.014834274537861347 norm:0.0001256157411262393 max memory_allocated 22517.79296875 
[2025-02-18 10:32:00 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.014837010763585567 norm:0.00012851812061853707 max memory_allocated 22517.79296875 
[2025-02-18 10:32:34 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.014815266244113445 norm:0.00012080000306013972 max memory_allocated 22517.79296875 
[2025-02-18 10:33:07 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.014806653372943401 norm:0.00012499924923758954 max memory_allocated 22517.79296875 
[2025-02-18 10:33:40 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.01478864811360836 norm:0.00012208263797219843 max memory_allocated 22517.79296875 
[2025-02-18 10:33:50 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-18 10:34:26 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.03298189863562584 norm:0.0016236919909715652 max memory_allocated 22517.96484375 
[2025-02-18 10:34:59 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.030757717788219452 norm:0.0011364881647750735 max memory_allocated 22517.96484375 
[2025-02-18 10:35:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.029333220794796944 norm:0.0008263997733592987 max memory_allocated 22517.96484375 
[2025-02-18 10:36:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.028470346704125404 norm:0.0006111403345130384 max memory_allocated 22517.96484375 
[2025-02-18 10:36:38 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.027809634804725647 norm:0.0004699065175373107 max memory_allocated 22517.96484375 
[2025-02-18 10:37:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.02738945558667183 norm:0.00036387384170666337 max memory_allocated 22517.96484375 
[2025-02-18 10:37:45 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.02718391641974449 norm:0.0002701868652366102 max memory_allocated 22517.96484375 
[2025-02-18 10:38:18 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.026939449831843376 norm:0.00021520622249227017 max memory_allocated 22517.96484375 
[2025-02-18 10:38:51 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.026819029822945595 norm:0.00017553084762766957 max memory_allocated 22517.96484375 
[2025-02-18 10:39:24 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.026684794574975967 norm:0.00014756963355466723 max memory_allocated 22517.96484375 
[2025-02-18 10:39:57 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.026599718257784843 norm:0.0001261801371583715 max memory_allocated 22517.96484375 
[2025-02-18 10:40:31 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.02654109336435795 norm:0.00011461674876045436 max memory_allocated 22517.96484375 
[2025-02-18 10:41:04 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.026474647223949432 norm:0.00010654659126885235 max memory_allocated 22517.96484375 
[2025-02-18 10:41:37 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.02644231729209423 norm:0.00010367060895077884 max memory_allocated 22517.96484375 
[2025-02-18 10:42:10 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.026382435113191605 norm:0.00010079564526677132 max memory_allocated 22517.96484375 
[2025-02-18 10:42:44 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.026380330324172974 norm:9.671173029346392e-05 max memory_allocated 22517.96484375 
[2025-02-18 10:43:17 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.02634851634502411 norm:9.47273729252629e-05 max memory_allocated 22517.96484375 
[2025-02-18 10:43:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.02631206437945366 norm:9.223262895829976e-05 max memory_allocated 22517.96484375 
[2025-02-18 10:44:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.026341881603002548 norm:9.70817418419756e-05 max memory_allocated 22517.96484375 
[2025-02-18 10:44:56 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.026319699361920357 norm:9.337191295344383e-05 max memory_allocated 22517.96484375 
[2025-02-18 10:45:06 root] (main_calibration.py 365): INFO 21646.12416100502
[2025-02-18 10:45:38 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-18 10:46:49 root] (main_calibration.py 158): INFO wikitext2 : 5.4847517013549805
[2025-02-18 10:46:49 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-18 10:48:40 root] (main_calibration.py 158): INFO c4 : 6.998055934906006
[2025-02-18 12:26:21 root] (main_calibration.py 169): INFO {'wikitext2': 5.4847517013549805, 'c4': 6.998055934906006, 'results': {'hellaswag': {'acc': 0.5674168492332204, 'acc_stderr': 0.004944215937021395, 'acc_norm': 0.729237203744274, 'acc_norm_stderr': 0.004434456717097587}, 'arc_challenge': {'acc': 0.3984641638225256, 'acc_stderr': 0.014306946052735562, 'acc_norm': 0.40784982935153585, 'acc_norm_stderr': 0.0143610972884497}, 'arc_easy': {'acc': 0.6965488215488216, 'acc_stderr': 0.009433837434252275, 'acc_norm': 0.5353535353535354, 'acc_norm_stderr': 0.010234104543411426}, 'boolq': {'acc': 0.7140672782874617, 'acc_stderr': 0.00790303735916362}, 'winogrande': {'acc': 0.675611681136543, 'acc_stderr': 0.013157225726641634}, 'piqa': {'acc': 0.7845484221980413, 'acc_stderr': 0.009592463115658117, 'acc_norm': 0.7693144722524483, 'acc_norm_stderr': 0.009828959550983096}}, 'versions': {'hellaswag': 0, 'arc_challenge': 0, 'arc_easy': 0, 'boolq': 1, 'winogrande': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
