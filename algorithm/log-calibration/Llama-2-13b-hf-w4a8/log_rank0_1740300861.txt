[2025-02-23 08:54:21 root] (main_calibration.py 269): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration/Llama-2-13b-hf-w4a8', save_dir='./log-calibration/quant/Llama-2-13b-hf-w4a8', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=8, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=False)
[2025-02-23 08:54:23 root] (main_calibration.py 336): INFO === start quantization ===
[2025-02-23 08:54:23 root] (main_calibration.py 342): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-23 08:54:23 root] (abq_llm_calibration.py 62): INFO Starting ...
[2025-02-23 08:54:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 0 ===
[2025-02-23 08:55:20 root] (abq_llm_calibration.py 358): INFO layer 0 iter 0 loss:0.012202056124806404 norm:0.001624967553652823 max memory_allocated 29229.177734375 
[2025-02-23 08:56:08 root] (abq_llm_calibration.py 358): INFO layer 0 iter 1 loss:0.007712153717875481 norm:0.0011659541632980108 max memory_allocated 29229.177734375 
[2025-02-23 08:56:57 root] (abq_llm_calibration.py 358): INFO layer 0 iter 2 loss:0.008099829778075218 norm:0.004194589797407389 max memory_allocated 29229.177734375 
[2025-02-23 08:57:46 root] (abq_llm_calibration.py 358): INFO layer 0 iter 3 loss:0.0065340204164385796 norm:0.001915814122185111 max memory_allocated 29229.177734375 
[2025-02-23 08:58:35 root] (abq_llm_calibration.py 358): INFO layer 0 iter 4 loss:0.0056630331091582775 norm:0.0009113270207308233 max memory_allocated 29229.177734375 
[2025-02-23 08:59:24 root] (abq_llm_calibration.py 358): INFO layer 0 iter 5 loss:0.00854185875505209 norm:0.007182644214481115 max memory_allocated 29229.177734375 
[2025-02-23 09:00:13 root] (abq_llm_calibration.py 358): INFO layer 0 iter 6 loss:0.0067482818849384785 norm:0.0023106448352336884 max memory_allocated 29229.177734375 
[2025-02-23 09:01:02 root] (abq_llm_calibration.py 358): INFO layer 0 iter 7 loss:0.005911604035645723 norm:0.001938855042681098 max memory_allocated 29229.177734375 
[2025-02-23 09:01:51 root] (abq_llm_calibration.py 358): INFO layer 0 iter 8 loss:0.005324456375092268 norm:0.000742534059099853 max memory_allocated 29229.177734375 
[2025-02-23 09:02:40 root] (abq_llm_calibration.py 358): INFO layer 0 iter 9 loss:0.00727832643315196 norm:0.0027376620564609766 max memory_allocated 29229.177734375 
[2025-02-23 09:03:28 root] (abq_llm_calibration.py 358): INFO layer 0 iter 10 loss:0.006159526761621237 norm:0.0016003703931346536 max memory_allocated 29229.177734375 
[2025-02-23 09:04:17 root] (abq_llm_calibration.py 358): INFO layer 0 iter 11 loss:0.006700064055621624 norm:0.0021962053142488003 max memory_allocated 29229.177734375 
[2025-02-23 09:05:06 root] (abq_llm_calibration.py 358): INFO layer 0 iter 12 loss:0.0060900067910552025 norm:0.001273430185392499 max memory_allocated 29229.177734375 
[2025-02-23 09:05:55 root] (abq_llm_calibration.py 358): INFO layer 0 iter 13 loss:0.005399775691330433 norm:0.0006154971197247505 max memory_allocated 29229.177734375 
[2025-02-23 09:06:44 root] (abq_llm_calibration.py 358): INFO layer 0 iter 14 loss:0.008309546858072281 norm:0.004560224246233702 max memory_allocated 29229.177734375 
[2025-02-23 09:07:33 root] (abq_llm_calibration.py 358): INFO layer 0 iter 15 loss:0.0066071669571101665 norm:0.002064542844891548 max memory_allocated 29229.177734375 
[2025-02-23 09:08:22 root] (abq_llm_calibration.py 358): INFO layer 0 iter 16 loss:0.005396122112870216 norm:0.000521987269166857 max memory_allocated 29229.177734375 
[2025-02-23 09:09:10 root] (abq_llm_calibration.py 358): INFO layer 0 iter 17 loss:0.008321943692862988 norm:0.003283439902588725 max memory_allocated 29229.177734375 
[2025-02-23 09:09:59 root] (abq_llm_calibration.py 358): INFO layer 0 iter 18 loss:0.016617856919765472 norm:0.02216143161058426 max memory_allocated 29229.177734375 
[2025-02-23 09:10:48 root] (abq_llm_calibration.py 358): INFO layer 0 iter 19 loss:0.017846910282969475 norm:0.02183849737048149 max memory_allocated 29229.177734375 
[2025-02-23 09:11:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 1 ===
[2025-02-23 09:11:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 0 loss:0.03370942547917366 norm:0.0011574692325666547 max memory_allocated 29229.365234375 
[2025-02-23 09:12:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 1 loss:0.026389362290501595 norm:0.0007165980059653521 max memory_allocated 29229.365234375 
[2025-02-23 09:13:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 2 loss:0.02429524064064026 norm:0.0006081665633246303 max memory_allocated 29229.365234375 
[2025-02-23 09:14:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 3 loss:0.023376643657684326 norm:0.000509673438500613 max memory_allocated 29229.365234375 
[2025-02-23 09:15:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 4 loss:0.023266267031431198 norm:0.0006299559026956558 max memory_allocated 29229.365234375 
[2025-02-23 09:16:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 5 loss:0.02270376868546009 norm:0.0005179488216526806 max memory_allocated 29229.365234375 
[2025-02-23 09:16:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 6 loss:0.023012233898043633 norm:0.000677576579619199 max memory_allocated 29229.365234375 
[2025-02-23 09:17:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 7 loss:0.022945065051317215 norm:0.0006090649403631687 max memory_allocated 29229.365234375 
[2025-02-23 09:18:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 8 loss:0.022690201178193092 norm:0.0005861933459527791 max memory_allocated 29229.365234375 
[2025-02-23 09:19:16 root] (abq_llm_calibration.py 358): INFO layer 1 iter 9 loss:0.022937580943107605 norm:0.0008845537668094039 max memory_allocated 29229.365234375 
[2025-02-23 09:20:06 root] (abq_llm_calibration.py 358): INFO layer 1 iter 10 loss:0.02265417017042637 norm:0.0005555968964472413 max memory_allocated 29229.365234375 
[2025-02-23 09:20:55 root] (abq_llm_calibration.py 358): INFO layer 1 iter 11 loss:0.022665154188871384 norm:0.0005516046076081693 max memory_allocated 29229.365234375 
[2025-02-23 09:21:44 root] (abq_llm_calibration.py 358): INFO layer 1 iter 12 loss:0.023031895980238914 norm:0.00088121322914958 max memory_allocated 29229.365234375 
[2025-02-23 09:22:33 root] (abq_llm_calibration.py 358): INFO layer 1 iter 13 loss:0.023071961477398872 norm:0.0005613769171759486 max memory_allocated 29229.365234375 
[2025-02-23 09:23:22 root] (abq_llm_calibration.py 358): INFO layer 1 iter 14 loss:0.02298029139637947 norm:0.0005975376116111875 max memory_allocated 29229.365234375 
[2025-02-23 09:24:11 root] (abq_llm_calibration.py 358): INFO layer 1 iter 15 loss:0.023029817268252373 norm:0.0005928893806412816 max memory_allocated 29229.365234375 
[2025-02-23 09:25:00 root] (abq_llm_calibration.py 358): INFO layer 1 iter 16 loss:0.02300151064991951 norm:0.000572523451410234 max memory_allocated 29229.365234375 
[2025-02-23 09:25:49 root] (abq_llm_calibration.py 358): INFO layer 1 iter 17 loss:0.023062443360686302 norm:0.0006034938851371408 max memory_allocated 29229.365234375 
[2025-02-23 09:26:38 root] (abq_llm_calibration.py 358): INFO layer 1 iter 18 loss:0.02306310646235943 norm:0.0005541452555917203 max memory_allocated 29229.365234375 
[2025-02-23 09:27:27 root] (abq_llm_calibration.py 358): INFO layer 1 iter 19 loss:0.023111805319786072 norm:0.0007903227815404534 max memory_allocated 29229.365234375 
[2025-02-23 09:27:41 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 2 ===
[2025-02-23 09:28:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 0 loss:0.037212926894426346 norm:0.002334626391530037 max memory_allocated 29229.552734375 
[2025-02-23 09:29:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 1 loss:0.031960923224687576 norm:0.0006957885343581438 max memory_allocated 29229.552734375 
[2025-02-23 09:30:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 2 loss:0.029794344678521156 norm:0.0004289262287784368 max memory_allocated 29229.552734375 
[2025-02-23 09:31:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 3 loss:0.028952961787581444 norm:0.0002618478611111641 max memory_allocated 29229.552734375 
[2025-02-23 09:31:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 4 loss:0.02852628566324711 norm:0.00020547315943986177 max memory_allocated 29229.552734375 
[2025-02-23 09:32:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 5 loss:0.028275256976485252 norm:0.0002058772515738383 max memory_allocated 29229.552734375 
[2025-02-23 09:33:29 root] (abq_llm_calibration.py 358): INFO layer 2 iter 6 loss:0.0283038392663002 norm:0.00019259925466030836 max memory_allocated 29229.552734375 
[2025-02-23 09:34:18 root] (abq_llm_calibration.py 358): INFO layer 2 iter 7 loss:0.028546826913952827 norm:0.0002061035338556394 max memory_allocated 29229.552734375 
[2025-02-23 09:35:07 root] (abq_llm_calibration.py 358): INFO layer 2 iter 8 loss:0.029015181586146355 norm:0.00022406873176805675 max memory_allocated 29229.552734375 
[2025-02-23 09:35:56 root] (abq_llm_calibration.py 358): INFO layer 2 iter 9 loss:0.02905290387570858 norm:0.00023305672220885754 max memory_allocated 29229.552734375 
[2025-02-23 09:36:45 root] (abq_llm_calibration.py 358): INFO layer 2 iter 10 loss:0.029098544269800186 norm:0.0002327505499124527 max memory_allocated 29229.552734375 
[2025-02-23 09:37:34 root] (abq_llm_calibration.py 358): INFO layer 2 iter 11 loss:0.029105626046657562 norm:0.00023728003725409508 max memory_allocated 29229.552734375 
[2025-02-23 09:38:23 root] (abq_llm_calibration.py 358): INFO layer 2 iter 12 loss:0.02942776307463646 norm:0.00024670851416885853 max memory_allocated 29229.552734375 
[2025-02-23 09:39:12 root] (abq_llm_calibration.py 358): INFO layer 2 iter 13 loss:0.02948065474629402 norm:0.00025794439716264606 max memory_allocated 29229.552734375 
[2025-02-23 09:40:01 root] (abq_llm_calibration.py 358): INFO layer 2 iter 14 loss:0.029662109911441803 norm:0.00027046725153923035 max memory_allocated 29229.552734375 
[2025-02-23 09:40:50 root] (abq_llm_calibration.py 358): INFO layer 2 iter 15 loss:0.029557205736637115 norm:0.00025473645655438304 max memory_allocated 29229.552734375 
[2025-02-23 09:41:39 root] (abq_llm_calibration.py 358): INFO layer 2 iter 16 loss:0.029562845826148987 norm:0.00025659363018348813 max memory_allocated 29229.552734375 
[2025-02-23 09:42:28 root] (abq_llm_calibration.py 358): INFO layer 2 iter 17 loss:0.029789848253130913 norm:0.00027815462090075016 max memory_allocated 29229.552734375 
[2025-02-23 09:43:17 root] (abq_llm_calibration.py 358): INFO layer 2 iter 18 loss:0.029741104692220688 norm:0.00028284877771511674 max memory_allocated 29229.552734375 
[2025-02-23 09:44:06 root] (abq_llm_calibration.py 358): INFO layer 2 iter 19 loss:0.02968618832528591 norm:0.0002785679535008967 max memory_allocated 29229.552734375 
[2025-02-23 09:44:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 3 ===
[2025-02-23 09:45:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 0 loss:0.059301551431417465 norm:0.003068465506657958 max memory_allocated 29229.740234375 
[2025-02-23 09:46:04 root] (abq_llm_calibration.py 358): INFO layer 3 iter 1 loss:0.05031098425388336 norm:0.0018771789036691189 max memory_allocated 29229.740234375 
[2025-02-23 09:46:53 root] (abq_llm_calibration.py 358): INFO layer 3 iter 2 loss:0.04904681071639061 norm:0.0019053699215874076 max memory_allocated 29229.740234375 
[2025-02-23 09:47:42 root] (abq_llm_calibration.py 358): INFO layer 3 iter 3 loss:0.046434953808784485 norm:0.0016200231621041894 max memory_allocated 29229.740234375 
[2025-02-23 09:48:31 root] (abq_llm_calibration.py 358): INFO layer 3 iter 4 loss:0.04638014733791351 norm:0.001640461152419448 max memory_allocated 29229.740234375 
[2025-02-23 09:49:20 root] (abq_llm_calibration.py 358): INFO layer 3 iter 5 loss:0.0448196679353714 norm:0.0015723647084087133 max memory_allocated 29229.740234375 
[2025-02-23 09:50:09 root] (abq_llm_calibration.py 358): INFO layer 3 iter 6 loss:0.045099981129169464 norm:0.0016431324183940887 max memory_allocated 29229.740234375 
[2025-02-23 09:50:58 root] (abq_llm_calibration.py 358): INFO layer 3 iter 7 loss:0.04593926668167114 norm:0.0018378726672381163 max memory_allocated 29229.740234375 
[2025-02-23 09:51:47 root] (abq_llm_calibration.py 358): INFO layer 3 iter 8 loss:0.047461166977882385 norm:0.0021742661483585835 max memory_allocated 29229.740234375 
[2025-02-23 09:52:36 root] (abq_llm_calibration.py 358): INFO layer 3 iter 9 loss:0.048641636967659 norm:0.0023754301946610212 max memory_allocated 29229.740234375 
[2025-02-23 09:53:25 root] (abq_llm_calibration.py 358): INFO layer 3 iter 10 loss:0.04860822111368179 norm:0.0023984198924154043 max memory_allocated 29229.740234375 
[2025-02-23 09:54:14 root] (abq_llm_calibration.py 358): INFO layer 3 iter 11 loss:0.04771637171506882 norm:0.0022258274257183075 max memory_allocated 29229.740234375 
[2025-02-23 09:55:03 root] (abq_llm_calibration.py 358): INFO layer 3 iter 12 loss:0.04764042794704437 norm:0.0022445437498390675 max memory_allocated 29229.740234375 
[2025-02-23 09:55:52 root] (abq_llm_calibration.py 358): INFO layer 3 iter 13 loss:0.047036901116371155 norm:0.0022163409739732742 max memory_allocated 29229.740234375 
[2025-02-23 09:56:41 root] (abq_llm_calibration.py 358): INFO layer 3 iter 14 loss:0.04542665183544159 norm:0.0020488041918724775 max memory_allocated 29229.740234375 
[2025-02-23 09:57:30 root] (abq_llm_calibration.py 358): INFO layer 3 iter 15 loss:0.0442834347486496 norm:0.0019123131642118096 max memory_allocated 29229.740234375 
[2025-02-23 09:58:18 root] (abq_llm_calibration.py 358): INFO layer 3 iter 16 loss:0.04429613798856735 norm:0.0019451860571280122 max memory_allocated 29229.740234375 
[2025-02-23 09:59:07 root] (abq_llm_calibration.py 358): INFO layer 3 iter 17 loss:0.04292525723576546 norm:0.0019060648046433926 max memory_allocated 29229.740234375 
[2025-02-23 09:59:57 root] (abq_llm_calibration.py 358): INFO layer 3 iter 18 loss:0.03866616636514664 norm:0.0008479124517180026 max memory_allocated 29229.740234375 
[2025-02-23 10:00:45 root] (abq_llm_calibration.py 358): INFO layer 3 iter 19 loss:0.03860780596733093 norm:0.0008474540081806481 max memory_allocated 29229.740234375 
[2025-02-23 10:01:00 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 4 ===
[2025-02-23 10:01:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 0 loss:0.05159743130207062 norm:0.0007449240074492991 max memory_allocated 29229.927734375 
[2025-02-23 10:02:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 1 loss:0.04560123756527901 norm:0.00032459062640555203 max memory_allocated 29229.927734375 
[2025-02-23 10:03:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 2 loss:0.04366505146026611 norm:0.00023396496544592083 max memory_allocated 29229.927734375 
[2025-02-23 10:04:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 3 loss:0.0425214022397995 norm:0.00018140077008865774 max memory_allocated 29229.927734375 
[2025-02-23 10:05:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 4 loss:0.04196958616375923 norm:0.00014736055163666606 max memory_allocated 29229.927734375 
[2025-02-23 10:05:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 5 loss:0.041723717004060745 norm:0.00013754020619671792 max memory_allocated 29229.927734375 
[2025-02-23 10:06:48 root] (abq_llm_calibration.py 358): INFO layer 4 iter 6 loss:0.04152921587228775 norm:0.0001233123621204868 max memory_allocated 29229.927734375 
[2025-02-23 10:07:37 root] (abq_llm_calibration.py 358): INFO layer 4 iter 7 loss:0.041727252304553986 norm:0.00012420592247508466 max memory_allocated 29229.927734375 
[2025-02-23 10:08:26 root] (abq_llm_calibration.py 358): INFO layer 4 iter 8 loss:0.04182546213269234 norm:0.00011694869317580014 max memory_allocated 29229.927734375 
[2025-02-23 10:09:15 root] (abq_llm_calibration.py 358): INFO layer 4 iter 9 loss:0.04187597334384918 norm:0.00011928450112463906 max memory_allocated 29229.927734375 
[2025-02-23 10:10:04 root] (abq_llm_calibration.py 358): INFO layer 4 iter 10 loss:0.0420098602771759 norm:0.0001330220402451232 max memory_allocated 29229.927734375 
[2025-02-23 10:10:53 root] (abq_llm_calibration.py 358): INFO layer 4 iter 11 loss:0.041985880583524704 norm:0.00012656573380809277 max memory_allocated 29229.927734375 
[2025-02-23 10:11:42 root] (abq_llm_calibration.py 358): INFO layer 4 iter 12 loss:0.04197196289896965 norm:0.00012560494360513985 max memory_allocated 29229.927734375 
[2025-02-23 10:12:31 root] (abq_llm_calibration.py 358): INFO layer 4 iter 13 loss:0.04196035861968994 norm:0.00013081806537229568 max memory_allocated 29229.927734375 
[2025-02-23 10:13:20 root] (abq_llm_calibration.py 358): INFO layer 4 iter 14 loss:0.041958145797252655 norm:0.0001336520945187658 max memory_allocated 29229.927734375 
[2025-02-23 10:14:09 root] (abq_llm_calibration.py 358): INFO layer 4 iter 15 loss:0.041956253349781036 norm:0.0001302156742895022 max memory_allocated 29229.927734375 
[2025-02-23 10:14:58 root] (abq_llm_calibration.py 358): INFO layer 4 iter 16 loss:0.04199976474046707 norm:0.00012500965385697782 max memory_allocated 29229.927734375 
[2025-02-23 10:15:47 root] (abq_llm_calibration.py 358): INFO layer 4 iter 17 loss:0.04200732335448265 norm:0.0001358840090688318 max memory_allocated 29229.927734375 
[2025-02-23 10:16:36 root] (abq_llm_calibration.py 358): INFO layer 4 iter 18 loss:0.04214174672961235 norm:0.00012792262714356184 max memory_allocated 29229.927734375 
[2025-02-23 10:17:25 root] (abq_llm_calibration.py 358): INFO layer 4 iter 19 loss:0.042153939604759216 norm:0.00012956885620951653 max memory_allocated 29229.927734375 
[2025-02-23 10:17:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 5 ===
[2025-02-23 10:18:33 root] (abq_llm_calibration.py 358): INFO layer 5 iter 0 loss:0.05864172801375389 norm:0.0011852455791085958 max memory_allocated 29230.115234375 
[2025-02-23 10:19:22 root] (abq_llm_calibration.py 358): INFO layer 5 iter 1 loss:0.05091102421283722 norm:0.0004608156159520149 max memory_allocated 29230.115234375 
[2025-02-23 10:20:11 root] (abq_llm_calibration.py 358): INFO layer 5 iter 2 loss:0.048589352518320084 norm:0.0002945167652796954 max memory_allocated 29230.115234375 
[2025-02-23 10:21:00 root] (abq_llm_calibration.py 358): INFO layer 5 iter 3 loss:0.04724336415529251 norm:0.0002227063087048009 max memory_allocated 29230.115234375 
[2025-02-23 10:21:49 root] (abq_llm_calibration.py 358): INFO layer 5 iter 4 loss:0.046643730252981186 norm:0.00019340845756232738 max memory_allocated 29230.115234375 
[2025-02-23 10:22:38 root] (abq_llm_calibration.py 358): INFO layer 5 iter 5 loss:0.046214908361434937 norm:0.00016276759561151266 max memory_allocated 29230.115234375 
[2025-02-23 10:23:27 root] (abq_llm_calibration.py 358): INFO layer 5 iter 6 loss:0.045960672199726105 norm:0.000148797407746315 max memory_allocated 29230.115234375 
[2025-02-23 10:24:16 root] (abq_llm_calibration.py 358): INFO layer 5 iter 7 loss:0.045755062252283096 norm:0.00012663580127991736 max memory_allocated 29230.115234375 
[2025-02-23 10:25:04 root] (abq_llm_calibration.py 358): INFO layer 5 iter 8 loss:0.0457734689116478 norm:0.00013133211177773774 max memory_allocated 29230.115234375 
[2025-02-23 10:25:53 root] (abq_llm_calibration.py 358): INFO layer 5 iter 9 loss:0.04567801207304001 norm:0.00011952091881539673 max memory_allocated 29230.115234375 
[2025-02-23 10:26:42 root] (abq_llm_calibration.py 358): INFO layer 5 iter 10 loss:0.0456438884139061 norm:0.00011464186536613852 max memory_allocated 29230.115234375 
[2025-02-23 10:27:31 root] (abq_llm_calibration.py 358): INFO layer 5 iter 11 loss:0.045888759195804596 norm:0.00012321666872594506 max memory_allocated 29230.115234375 
[2025-02-23 10:28:20 root] (abq_llm_calibration.py 358): INFO layer 5 iter 12 loss:0.045875437557697296 norm:0.00011759572953451425 max memory_allocated 29230.115234375 
[2025-02-23 10:29:09 root] (abq_llm_calibration.py 358): INFO layer 5 iter 13 loss:0.04598795250058174 norm:0.00012090204836567864 max memory_allocated 29230.115234375 
[2025-02-23 10:29:58 root] (abq_llm_calibration.py 358): INFO layer 5 iter 14 loss:0.04605201631784439 norm:0.0001194389842567034 max memory_allocated 29230.115234375 
[2025-02-23 10:30:47 root] (abq_llm_calibration.py 358): INFO layer 5 iter 15 loss:0.046058423817157745 norm:0.00012401083949953318 max memory_allocated 29230.115234375 
[2025-02-23 10:31:36 root] (abq_llm_calibration.py 358): INFO layer 5 iter 16 loss:0.046110447496175766 norm:0.00012242661614436656 max memory_allocated 29230.115234375 
[2025-02-23 10:32:25 root] (abq_llm_calibration.py 358): INFO layer 5 iter 17 loss:0.04611032456159592 norm:0.0001214362055179663 max memory_allocated 29230.115234375 
[2025-02-23 10:33:14 root] (abq_llm_calibration.py 358): INFO layer 5 iter 18 loss:0.04612237587571144 norm:0.000124904589029029 max memory_allocated 29230.115234375 
[2025-02-23 10:34:03 root] (abq_llm_calibration.py 358): INFO layer 5 iter 19 loss:0.04601851850748062 norm:0.00012439227430149913 max memory_allocated 29230.115234375 
[2025-02-23 10:34:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 6 ===
[2025-02-23 10:35:11 root] (abq_llm_calibration.py 358): INFO layer 6 iter 0 loss:0.06541901081800461 norm:0.0013242758577689528 max memory_allocated 29230.302734375 
[2025-02-23 10:36:00 root] (abq_llm_calibration.py 358): INFO layer 6 iter 1 loss:0.057072218507528305 norm:0.0005805319524370134 max memory_allocated 29230.302734375 
[2025-02-23 10:36:49 root] (abq_llm_calibration.py 358): INFO layer 6 iter 2 loss:0.05398213118314743 norm:0.0003705912095028907 max memory_allocated 29230.302734375 
[2025-02-23 10:37:37 root] (abq_llm_calibration.py 358): INFO layer 6 iter 3 loss:0.052505794912576675 norm:0.00027087173657491803 max memory_allocated 29230.302734375 
[2025-02-23 10:38:26 root] (abq_llm_calibration.py 358): INFO layer 6 iter 4 loss:0.051698219031095505 norm:0.00019578877254389226 max memory_allocated 29230.302734375 
[2025-02-23 10:39:15 root] (abq_llm_calibration.py 358): INFO layer 6 iter 5 loss:0.050996169447898865 norm:0.0001408421667292714 max memory_allocated 29230.302734375 
[2025-02-23 10:40:04 root] (abq_llm_calibration.py 358): INFO layer 6 iter 6 loss:0.050700437277555466 norm:0.00012010587670374662 max memory_allocated 29230.302734375 
[2025-02-23 10:40:53 root] (abq_llm_calibration.py 358): INFO layer 6 iter 7 loss:0.050673115998506546 norm:0.00010616013605613261 max memory_allocated 29230.302734375 
[2025-02-23 10:41:42 root] (abq_llm_calibration.py 358): INFO layer 6 iter 8 loss:0.05066467821598053 norm:9.590136323822662e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:42:31 root] (abq_llm_calibration.py 358): INFO layer 6 iter 9 loss:0.05072842538356781 norm:9.225539542967454e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:43:20 root] (abq_llm_calibration.py 358): INFO layer 6 iter 10 loss:0.05061669647693634 norm:8.925235306378454e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:44:08 root] (abq_llm_calibration.py 358): INFO layer 6 iter 11 loss:0.05059367045760155 norm:8.786299440544099e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:44:57 root] (abq_llm_calibration.py 358): INFO layer 6 iter 12 loss:0.05075326934456825 norm:8.902154513634741e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:45:46 root] (abq_llm_calibration.py 358): INFO layer 6 iter 13 loss:0.05078979581594467 norm:8.72123782755807e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:46:35 root] (abq_llm_calibration.py 358): INFO layer 6 iter 14 loss:0.050791025161743164 norm:8.649417577544227e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:47:24 root] (abq_llm_calibration.py 358): INFO layer 6 iter 15 loss:0.0508403554558754 norm:8.614731632405892e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:48:13 root] (abq_llm_calibration.py 358): INFO layer 6 iter 16 loss:0.05078968033194542 norm:8.607548079453409e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:49:01 root] (abq_llm_calibration.py 358): INFO layer 6 iter 17 loss:0.0506347119808197 norm:8.394501492148265e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:49:50 root] (abq_llm_calibration.py 358): INFO layer 6 iter 18 loss:0.05056930333375931 norm:8.296920714201406e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:50:39 root] (abq_llm_calibration.py 358): INFO layer 6 iter 19 loss:0.05066026374697685 norm:8.49578864290379e-05 max memory_allocated 29230.302734375 
[2025-02-23 10:50:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 7 ===
[2025-02-23 10:51:46 root] (abq_llm_calibration.py 358): INFO layer 7 iter 0 loss:0.07951390743255615 norm:0.0019303662702441216 max memory_allocated 29230.490234375 
[2025-02-23 10:52:35 root] (abq_llm_calibration.py 358): INFO layer 7 iter 1 loss:0.06690897047519684 norm:0.0007926443358883262 max memory_allocated 29230.490234375 
[2025-02-23 10:53:24 root] (abq_llm_calibration.py 358): INFO layer 7 iter 2 loss:0.06276022642850876 norm:0.0005045077996328473 max memory_allocated 29230.490234375 
[2025-02-23 10:54:13 root] (abq_llm_calibration.py 358): INFO layer 7 iter 3 loss:0.06092039495706558 norm:0.00040385033935308456 max memory_allocated 29230.490234375 
[2025-02-23 10:55:01 root] (abq_llm_calibration.py 358): INFO layer 7 iter 4 loss:0.059507131576538086 norm:0.0002782024384941906 max memory_allocated 29230.490234375 
[2025-02-23 10:55:50 root] (abq_llm_calibration.py 358): INFO layer 7 iter 5 loss:0.05885234847664833 norm:0.00024065149773377925 max memory_allocated 29230.490234375 
[2025-02-23 10:56:39 root] (abq_llm_calibration.py 358): INFO layer 7 iter 6 loss:0.05809435993432999 norm:0.00018239713972434402 max memory_allocated 29230.490234375 
[2025-02-23 10:57:28 root] (abq_llm_calibration.py 358): INFO layer 7 iter 7 loss:0.057924672961235046 norm:0.00016863051860127598 max memory_allocated 29230.490234375 
[2025-02-23 10:58:17 root] (abq_llm_calibration.py 358): INFO layer 7 iter 8 loss:0.057731665670871735 norm:0.00014504173304885626 max memory_allocated 29230.490234375 
[2025-02-23 10:59:05 root] (abq_llm_calibration.py 358): INFO layer 7 iter 9 loss:0.05741965025663376 norm:0.00012220771168358624 max memory_allocated 29230.490234375 
[2025-02-23 10:59:54 root] (abq_llm_calibration.py 358): INFO layer 7 iter 10 loss:0.05732598528265953 norm:0.0001191829505842179 max memory_allocated 29230.490234375 
[2025-02-23 11:00:43 root] (abq_llm_calibration.py 358): INFO layer 7 iter 11 loss:0.05739099904894829 norm:0.0001177919257315807 max memory_allocated 29230.490234375 
[2025-02-23 11:01:32 root] (abq_llm_calibration.py 358): INFO layer 7 iter 12 loss:0.05760782212018967 norm:0.0001152847326011397 max memory_allocated 29230.490234375 
[2025-02-23 11:02:21 root] (abq_llm_calibration.py 358): INFO layer 7 iter 13 loss:0.05753789842128754 norm:0.00011147036275360733 max memory_allocated 29230.490234375 
[2025-02-23 11:03:10 root] (abq_llm_calibration.py 358): INFO layer 7 iter 14 loss:0.05739463120698929 norm:0.00010585092968540266 max memory_allocated 29230.490234375 
[2025-02-23 11:03:59 root] (abq_llm_calibration.py 358): INFO layer 7 iter 15 loss:0.05782143026590347 norm:0.00010428177483845502 max memory_allocated 29230.490234375 
[2025-02-23 11:04:48 root] (abq_llm_calibration.py 358): INFO layer 7 iter 16 loss:0.058062732219696045 norm:0.00010413881682325155 max memory_allocated 29230.490234375 
[2025-02-23 11:05:37 root] (abq_llm_calibration.py 358): INFO layer 7 iter 17 loss:0.0579942986369133 norm:0.00010506664693821222 max memory_allocated 29230.490234375 
[2025-02-23 11:06:25 root] (abq_llm_calibration.py 358): INFO layer 7 iter 18 loss:0.057944897562265396 norm:0.00010274310625391081 max memory_allocated 29230.490234375 
[2025-02-23 11:07:14 root] (abq_llm_calibration.py 358): INFO layer 7 iter 19 loss:0.05819742754101753 norm:0.0001072534141712822 max memory_allocated 29230.490234375 
[2025-02-23 11:07:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 8 ===
[2025-02-23 11:08:21 root] (abq_llm_calibration.py 358): INFO layer 8 iter 0 loss:0.07816945761442184 norm:0.0015686795813962817 max memory_allocated 29230.677734375 
[2025-02-23 11:09:10 root] (abq_llm_calibration.py 358): INFO layer 8 iter 1 loss:0.0696541890501976 norm:0.0007733016391284764 max memory_allocated 29230.677734375 
[2025-02-23 11:09:59 root] (abq_llm_calibration.py 358): INFO layer 8 iter 2 loss:0.06597373634576797 norm:0.0004934999160468578 max memory_allocated 29230.677734375 
[2025-02-23 11:10:48 root] (abq_llm_calibration.py 358): INFO layer 8 iter 3 loss:0.06418754905462265 norm:0.0003684067924041301 max memory_allocated 29230.677734375 
[2025-02-23 11:11:37 root] (abq_llm_calibration.py 358): INFO layer 8 iter 4 loss:0.06358557939529419 norm:0.000302563130389899 max memory_allocated 29230.677734375 
[2025-02-23 11:12:26 root] (abq_llm_calibration.py 358): INFO layer 8 iter 5 loss:0.06283953785896301 norm:0.00025064212968572974 max memory_allocated 29230.677734375 
[2025-02-23 11:13:14 root] (abq_llm_calibration.py 358): INFO layer 8 iter 6 loss:0.062374427914619446 norm:0.00020907794532831758 max memory_allocated 29230.677734375 
[2025-02-23 11:14:03 root] (abq_llm_calibration.py 358): INFO layer 8 iter 7 loss:0.06213214248418808 norm:0.00018585132784210145 max memory_allocated 29230.677734375 
[2025-02-23 11:14:52 root] (abq_llm_calibration.py 358): INFO layer 8 iter 8 loss:0.061941955238580704 norm:0.00016069297271315008 max memory_allocated 29230.677734375 
[2025-02-23 11:15:41 root] (abq_llm_calibration.py 358): INFO layer 8 iter 9 loss:0.061884354799985886 norm:0.0001400430192006752 max memory_allocated 29230.677734375 
[2025-02-23 11:16:29 root] (abq_llm_calibration.py 358): INFO layer 8 iter 10 loss:0.06181494891643524 norm:0.00012258738570380956 max memory_allocated 29230.677734375 
[2025-02-23 11:17:18 root] (abq_llm_calibration.py 358): INFO layer 8 iter 11 loss:0.061868805438280106 norm:0.00011037255171686411 max memory_allocated 29230.677734375 
[2025-02-23 11:18:07 root] (abq_llm_calibration.py 358): INFO layer 8 iter 12 loss:0.061769433319568634 norm:9.854331437963992e-05 max memory_allocated 29230.677734375 
[2025-02-23 11:18:56 root] (abq_llm_calibration.py 358): INFO layer 8 iter 13 loss:0.06187523901462555 norm:9.520262392470613e-05 max memory_allocated 29230.677734375 
[2025-02-23 11:19:45 root] (abq_llm_calibration.py 358): INFO layer 8 iter 14 loss:0.06184940040111542 norm:9.229495481122285e-05 max memory_allocated 29230.677734375 
[2025-02-23 11:20:34 root] (abq_llm_calibration.py 358): INFO layer 8 iter 15 loss:0.061980754137039185 norm:9.360109834233299e-05 max memory_allocated 29230.677734375 
[2025-02-23 11:21:23 root] (abq_llm_calibration.py 358): INFO layer 8 iter 16 loss:0.061875514686107635 norm:9.132632840191945e-05 max memory_allocated 29230.677734375 
[2025-02-23 11:22:11 root] (abq_llm_calibration.py 358): INFO layer 8 iter 17 loss:0.06188277527689934 norm:9.09564842004329e-05 max memory_allocated 29230.677734375 
[2025-02-23 11:23:01 root] (abq_llm_calibration.py 358): INFO layer 8 iter 18 loss:0.06179752200841904 norm:8.849356527207419e-05 max memory_allocated 29230.677734375 
[2025-02-23 11:23:49 root] (abq_llm_calibration.py 358): INFO layer 8 iter 19 loss:0.06167307868599892 norm:8.713714487385005e-05 max memory_allocated 29230.677734375 
[2025-02-23 11:24:04 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 9 ===
[2025-02-23 11:24:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 0 loss:0.10760326683521271 norm:0.003673710161820054 max memory_allocated 29230.865234375 
[2025-02-23 11:25:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 1 loss:0.08708241581916809 norm:0.0017890380695462227 max memory_allocated 29230.865234375 
[2025-02-23 11:26:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 2 loss:0.07972288131713867 norm:0.0011251128744333982 max memory_allocated 29230.865234375 
[2025-02-23 11:27:22 root] (abq_llm_calibration.py 358): INFO layer 9 iter 3 loss:0.0756099596619606 norm:0.0007728732889518142 max memory_allocated 29230.865234375 
[2025-02-23 11:28:11 root] (abq_llm_calibration.py 358): INFO layer 9 iter 4 loss:0.07329171150922775 norm:0.0005845812847837806 max memory_allocated 29230.865234375 
[2025-02-23 11:29:00 root] (abq_llm_calibration.py 358): INFO layer 9 iter 5 loss:0.07174534350633621 norm:0.0004792380495928228 max memory_allocated 29230.865234375 
[2025-02-23 11:29:49 root] (abq_llm_calibration.py 358): INFO layer 9 iter 6 loss:0.07077816128730774 norm:0.00038645847234874964 max memory_allocated 29230.865234375 
[2025-02-23 11:30:37 root] (abq_llm_calibration.py 358): INFO layer 9 iter 7 loss:0.0703059658408165 norm:0.00033335573971271515 max memory_allocated 29230.865234375 
[2025-02-23 11:31:26 root] (abq_llm_calibration.py 358): INFO layer 9 iter 8 loss:0.06989128887653351 norm:0.0002893648052122444 max memory_allocated 29230.865234375 
[2025-02-23 11:32:15 root] (abq_llm_calibration.py 358): INFO layer 9 iter 9 loss:0.0695539265871048 norm:0.00026157349930144846 max memory_allocated 29230.865234375 
[2025-02-23 11:33:04 root] (abq_llm_calibration.py 358): INFO layer 9 iter 10 loss:0.06956702470779419 norm:0.00023271646932698786 max memory_allocated 29230.865234375 
[2025-02-23 11:33:53 root] (abq_llm_calibration.py 358): INFO layer 9 iter 11 loss:0.06924010813236237 norm:0.00020975231018383056 max memory_allocated 29230.865234375 
[2025-02-23 11:34:41 root] (abq_llm_calibration.py 358): INFO layer 9 iter 12 loss:0.06914448738098145 norm:0.0001890577404992655 max memory_allocated 29230.865234375 
[2025-02-23 11:35:30 root] (abq_llm_calibration.py 358): INFO layer 9 iter 13 loss:0.06928957998752594 norm:0.0001754879194777459 max memory_allocated 29230.865234375 
[2025-02-23 11:36:19 root] (abq_llm_calibration.py 358): INFO layer 9 iter 14 loss:0.06918270140886307 norm:0.0001647167664486915 max memory_allocated 29230.865234375 
[2025-02-23 11:37:08 root] (abq_llm_calibration.py 358): INFO layer 9 iter 15 loss:0.06923553347587585 norm:0.00015788798918947577 max memory_allocated 29230.865234375 
[2025-02-23 11:37:56 root] (abq_llm_calibration.py 358): INFO layer 9 iter 16 loss:0.06945285201072693 norm:0.0001564176200190559 max memory_allocated 29230.865234375 
[2025-02-23 11:38:45 root] (abq_llm_calibration.py 358): INFO layer 9 iter 17 loss:0.06943953037261963 norm:0.00015008363698143512 max memory_allocated 29230.865234375 
[2025-02-23 11:39:34 root] (abq_llm_calibration.py 358): INFO layer 9 iter 18 loss:0.06934982538223267 norm:0.00014382424706127495 max memory_allocated 29230.865234375 
[2025-02-23 11:40:23 root] (abq_llm_calibration.py 358): INFO layer 9 iter 19 loss:0.06910931318998337 norm:0.00013581667735707015 max memory_allocated 29230.865234375 
[2025-02-23 11:40:38 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 10 ===
[2025-02-23 11:41:30 root] (abq_llm_calibration.py 358): INFO layer 10 iter 0 loss:0.0964248776435852 norm:0.0015955223934724927 max memory_allocated 29231.052734375 
[2025-02-23 11:42:19 root] (abq_llm_calibration.py 358): INFO layer 10 iter 1 loss:0.08470337837934494 norm:0.0008848905563354492 max memory_allocated 29231.052734375 
[2025-02-23 11:43:08 root] (abq_llm_calibration.py 358): INFO layer 10 iter 2 loss:0.08049608767032623 norm:0.0006613032310269773 max memory_allocated 29231.052734375 
[2025-02-23 11:43:57 root] (abq_llm_calibration.py 358): INFO layer 10 iter 3 loss:0.0780501514673233 norm:0.0005128165939822793 max memory_allocated 29231.052734375 
[2025-02-23 11:44:46 root] (abq_llm_calibration.py 358): INFO layer 10 iter 4 loss:0.07691456377506256 norm:0.00043087024823762476 max memory_allocated 29231.052734375 
[2025-02-23 11:45:35 root] (abq_llm_calibration.py 358): INFO layer 10 iter 5 loss:0.07611827552318573 norm:0.0003625787503551692 max memory_allocated 29231.052734375 
[2025-02-23 11:46:24 root] (abq_llm_calibration.py 358): INFO layer 10 iter 6 loss:0.07540357112884521 norm:0.0003123742062598467 max memory_allocated 29231.052734375 
[2025-02-23 11:47:12 root] (abq_llm_calibration.py 358): INFO layer 10 iter 7 loss:0.07496361434459686 norm:0.0002723445068113506 max memory_allocated 29231.052734375 
[2025-02-23 11:48:01 root] (abq_llm_calibration.py 358): INFO layer 10 iter 8 loss:0.07464518398046494 norm:0.00023389965645037591 max memory_allocated 29231.052734375 
[2025-02-23 11:48:50 root] (abq_llm_calibration.py 358): INFO layer 10 iter 9 loss:0.0744566097855568 norm:0.00021885117166675627 max memory_allocated 29231.052734375 
[2025-02-23 11:49:38 root] (abq_llm_calibration.py 358): INFO layer 10 iter 10 loss:0.07415740936994553 norm:0.00020369485719129443 max memory_allocated 29231.052734375 
[2025-02-23 11:50:27 root] (abq_llm_calibration.py 358): INFO layer 10 iter 11 loss:0.07409799098968506 norm:0.00018752148025669158 max memory_allocated 29231.052734375 
[2025-02-23 11:51:16 root] (abq_llm_calibration.py 358): INFO layer 10 iter 12 loss:0.07407469302415848 norm:0.0001707302435534075 max memory_allocated 29231.052734375 
[2025-02-23 11:52:05 root] (abq_llm_calibration.py 358): INFO layer 10 iter 13 loss:0.07406237721443176 norm:0.00015051933587528765 max memory_allocated 29231.052734375 
[2025-02-23 11:52:54 root] (abq_llm_calibration.py 358): INFO layer 10 iter 14 loss:0.0743628665804863 norm:0.0001318824797635898 max memory_allocated 29231.052734375 
[2025-02-23 11:53:43 root] (abq_llm_calibration.py 358): INFO layer 10 iter 15 loss:0.07451095432043076 norm:0.00012628623517230153 max memory_allocated 29231.052734375 
[2025-02-23 11:54:31 root] (abq_llm_calibration.py 358): INFO layer 10 iter 16 loss:0.07464779168367386 norm:0.00012164133659098297 max memory_allocated 29231.052734375 
[2025-02-23 11:55:20 root] (abq_llm_calibration.py 358): INFO layer 10 iter 17 loss:0.07459082454442978 norm:0.00011809082934632897 max memory_allocated 29231.052734375 
[2025-02-23 11:56:09 root] (abq_llm_calibration.py 358): INFO layer 10 iter 18 loss:0.07433422654867172 norm:0.00010345986811444163 max memory_allocated 29231.052734375 
[2025-02-23 11:56:58 root] (abq_llm_calibration.py 358): INFO layer 10 iter 19 loss:0.07429630309343338 norm:0.00010682887659640983 max memory_allocated 29231.052734375 
[2025-02-23 11:57:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 11 ===
[2025-02-23 11:58:05 root] (abq_llm_calibration.py 358): INFO layer 11 iter 0 loss:0.09651386737823486 norm:0.0012645787792280316 max memory_allocated 29231.240234375 
[2025-02-23 11:58:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 1 loss:0.08675658702850342 norm:0.0006859825807623565 max memory_allocated 29231.240234375 
[2025-02-23 11:59:42 root] (abq_llm_calibration.py 358): INFO layer 11 iter 2 loss:0.08295480906963348 norm:0.0004943520179949701 max memory_allocated 29231.240234375 
[2025-02-23 12:00:31 root] (abq_llm_calibration.py 358): INFO layer 11 iter 3 loss:0.0809427872300148 norm:0.0003883202443830669 max memory_allocated 29231.240234375 
[2025-02-23 12:01:20 root] (abq_llm_calibration.py 358): INFO layer 11 iter 4 loss:0.079937644302845 norm:0.000330380949890241 max memory_allocated 29231.240234375 
[2025-02-23 12:02:09 root] (abq_llm_calibration.py 358): INFO layer 11 iter 5 loss:0.07932955026626587 norm:0.0002910302428063005 max memory_allocated 29231.240234375 
[2025-02-23 12:02:58 root] (abq_llm_calibration.py 358): INFO layer 11 iter 6 loss:0.07878953218460083 norm:0.00025960084167309105 max memory_allocated 29231.240234375 
[2025-02-23 12:03:46 root] (abq_llm_calibration.py 358): INFO layer 11 iter 7 loss:0.07839757949113846 norm:0.00023315077123697847 max memory_allocated 29231.240234375 
[2025-02-23 12:04:35 root] (abq_llm_calibration.py 358): INFO layer 11 iter 8 loss:0.07803328335285187 norm:0.00020924434647895396 max memory_allocated 29231.240234375 
[2025-02-23 12:05:24 root] (abq_llm_calibration.py 358): INFO layer 11 iter 9 loss:0.0778089091181755 norm:0.00018982926849275827 max memory_allocated 29231.240234375 
[2025-02-23 12:06:13 root] (abq_llm_calibration.py 358): INFO layer 11 iter 10 loss:0.07774052768945694 norm:0.0001686080067884177 max memory_allocated 29231.240234375 
[2025-02-23 12:07:02 root] (abq_llm_calibration.py 358): INFO layer 11 iter 11 loss:0.07759777456521988 norm:0.00015608668036293238 max memory_allocated 29231.240234375 
[2025-02-23 12:07:50 root] (abq_llm_calibration.py 358): INFO layer 11 iter 12 loss:0.07745804637670517 norm:0.00014564750017598271 max memory_allocated 29231.240234375 
[2025-02-23 12:08:39 root] (abq_llm_calibration.py 358): INFO layer 11 iter 13 loss:0.07761553674936295 norm:0.00014075457875151187 max memory_allocated 29231.240234375 
[2025-02-23 12:09:28 root] (abq_llm_calibration.py 358): INFO layer 11 iter 14 loss:0.07770941406488419 norm:0.0001324467157246545 max memory_allocated 29231.240234375 
[2025-02-23 12:10:17 root] (abq_llm_calibration.py 358): INFO layer 11 iter 15 loss:0.07765093445777893 norm:0.00011746498057618737 max memory_allocated 29231.240234375 
[2025-02-23 12:11:06 root] (abq_llm_calibration.py 358): INFO layer 11 iter 16 loss:0.07761655747890472 norm:0.00010511792788747698 max memory_allocated 29231.240234375 
[2025-02-23 12:11:54 root] (abq_llm_calibration.py 358): INFO layer 11 iter 17 loss:0.07778307795524597 norm:0.00010197350638918579 max memory_allocated 29231.240234375 
[2025-02-23 12:12:43 root] (abq_llm_calibration.py 358): INFO layer 11 iter 18 loss:0.07810038328170776 norm:9.579262405168265e-05 max memory_allocated 29231.240234375 
[2025-02-23 12:13:32 root] (abq_llm_calibration.py 358): INFO layer 11 iter 19 loss:0.07812996953725815 norm:9.338861127616838e-05 max memory_allocated 29231.240234375 
[2025-02-23 12:13:47 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 12 ===
[2025-02-23 12:14:40 root] (abq_llm_calibration.py 358): INFO layer 12 iter 0 loss:0.10345770418643951 norm:0.0015429737977683544 max memory_allocated 29231.427734375 
[2025-02-23 12:15:28 root] (abq_llm_calibration.py 358): INFO layer 12 iter 1 loss:0.09201516211032867 norm:0.0008253985433839262 max memory_allocated 29231.427734375 
[2025-02-23 12:16:17 root] (abq_llm_calibration.py 358): INFO layer 12 iter 2 loss:0.08787146955728531 norm:0.0006052250973880291 max memory_allocated 29231.427734375 
[2025-02-23 12:17:06 root] (abq_llm_calibration.py 358): INFO layer 12 iter 3 loss:0.08530747145414352 norm:0.0004593044868670404 max memory_allocated 29231.427734375 
[2025-02-23 12:17:54 root] (abq_llm_calibration.py 358): INFO layer 12 iter 4 loss:0.08404558897018433 norm:0.0003942549228668213 max memory_allocated 29231.427734375 
[2025-02-23 12:18:43 root] (abq_llm_calibration.py 358): INFO layer 12 iter 5 loss:0.08311517536640167 norm:0.00034422584576532245 max memory_allocated 29231.427734375 
[2025-02-23 12:19:32 root] (abq_llm_calibration.py 358): INFO layer 12 iter 6 loss:0.08251261711120605 norm:0.00030682486249133945 max memory_allocated 29231.427734375 
[2025-02-23 12:20:21 root] (abq_llm_calibration.py 358): INFO layer 12 iter 7 loss:0.08213464915752411 norm:0.0002647640649229288 max memory_allocated 29231.427734375 
[2025-02-23 12:21:10 root] (abq_llm_calibration.py 358): INFO layer 12 iter 8 loss:0.08199343085289001 norm:0.00023346146917901933 max memory_allocated 29231.427734375 
[2025-02-23 12:21:59 root] (abq_llm_calibration.py 358): INFO layer 12 iter 9 loss:0.08180766552686691 norm:0.00021563685731962323 max memory_allocated 29231.427734375 
[2025-02-23 12:22:48 root] (abq_llm_calibration.py 358): INFO layer 12 iter 10 loss:0.08135350048542023 norm:0.00020015159680042416 max memory_allocated 29231.427734375 
[2025-02-23 12:23:36 root] (abq_llm_calibration.py 358): INFO layer 12 iter 11 loss:0.0809076577425003 norm:0.00018700068176258355 max memory_allocated 29231.427734375 
[2025-02-23 12:24:25 root] (abq_llm_calibration.py 358): INFO layer 12 iter 12 loss:0.08083809167146683 norm:0.00016858841991052032 max memory_allocated 29231.427734375 
[2025-02-23 12:25:14 root] (abq_llm_calibration.py 358): INFO layer 12 iter 13 loss:0.08094573765993118 norm:0.0001568251318531111 max memory_allocated 29231.427734375 
[2025-02-23 12:26:03 root] (abq_llm_calibration.py 358): INFO layer 12 iter 14 loss:0.08072296530008316 norm:0.00013410915562417358 max memory_allocated 29231.427734375 
[2025-02-23 12:26:52 root] (abq_llm_calibration.py 358): INFO layer 12 iter 15 loss:0.08063951134681702 norm:0.0001202722851303406 max memory_allocated 29231.427734375 
[2025-02-23 12:27:41 root] (abq_llm_calibration.py 358): INFO layer 12 iter 16 loss:0.08081428706645966 norm:0.00010248805483570322 max memory_allocated 29231.427734375 
[2025-02-23 12:28:30 root] (abq_llm_calibration.py 358): INFO layer 12 iter 17 loss:0.08059129863977432 norm:0.00010291085345670581 max memory_allocated 29231.427734375 
[2025-02-23 12:29:19 root] (abq_llm_calibration.py 358): INFO layer 12 iter 18 loss:0.08048190176486969 norm:9.360058174934238e-05 max memory_allocated 29231.427734375 
[2025-02-23 12:30:07 root] (abq_llm_calibration.py 358): INFO layer 12 iter 19 loss:0.08043717592954636 norm:8.800117939244956e-05 max memory_allocated 29231.427734375 
[2025-02-23 12:30:22 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 13 ===
[2025-02-23 12:31:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 0 loss:0.10352898389101028 norm:0.0013905413215979934 max memory_allocated 29231.615234375 
[2025-02-23 12:32:03 root] (abq_llm_calibration.py 358): INFO layer 13 iter 1 loss:0.0931810513138771 norm:0.0007383443880826235 max memory_allocated 29231.615234375 
[2025-02-23 12:32:52 root] (abq_llm_calibration.py 358): INFO layer 13 iter 2 loss:0.08904712647199631 norm:0.0005130760255269706 max memory_allocated 29231.615234375 
[2025-02-23 12:33:41 root] (abq_llm_calibration.py 358): INFO layer 13 iter 3 loss:0.08695141226053238 norm:0.0004193157656118274 max memory_allocated 29231.615234375 
[2025-02-23 12:34:29 root] (abq_llm_calibration.py 358): INFO layer 13 iter 4 loss:0.08583185076713562 norm:0.0003610830754041672 max memory_allocated 29231.615234375 
[2025-02-23 12:35:18 root] (abq_llm_calibration.py 358): INFO layer 13 iter 5 loss:0.0848202332854271 norm:0.00030129062361083925 max memory_allocated 29231.615234375 
[2025-02-23 12:36:07 root] (abq_llm_calibration.py 358): INFO layer 13 iter 6 loss:0.08431634306907654 norm:0.00025822920724749565 max memory_allocated 29231.615234375 
[2025-02-23 12:36:56 root] (abq_llm_calibration.py 358): INFO layer 13 iter 7 loss:0.0839032381772995 norm:0.0002312412834726274 max memory_allocated 29231.615234375 
[2025-02-23 12:37:45 root] (abq_llm_calibration.py 358): INFO layer 13 iter 8 loss:0.08368118852376938 norm:0.00020159837731625885 max memory_allocated 29231.615234375 
[2025-02-23 12:38:34 root] (abq_llm_calibration.py 358): INFO layer 13 iter 9 loss:0.08372387290000916 norm:0.00017680144810583442 max memory_allocated 29231.615234375 
[2025-02-23 12:39:23 root] (abq_llm_calibration.py 358): INFO layer 13 iter 10 loss:0.08342449367046356 norm:0.00015815507504157722 max memory_allocated 29231.615234375 
[2025-02-23 12:40:12 root] (abq_llm_calibration.py 358): INFO layer 13 iter 11 loss:0.08331094682216644 norm:0.0001427028328180313 max memory_allocated 29231.615234375 
[2025-02-23 12:41:01 root] (abq_llm_calibration.py 358): INFO layer 13 iter 12 loss:0.083270363509655 norm:0.00012991268886253238 max memory_allocated 29231.615234375 
[2025-02-23 12:41:49 root] (abq_llm_calibration.py 358): INFO layer 13 iter 13 loss:0.08313079178333282 norm:0.00011919531971216202 max memory_allocated 29231.615234375 
[2025-02-23 12:42:38 root] (abq_llm_calibration.py 358): INFO layer 13 iter 14 loss:0.0830485075712204 norm:0.00010509895219001919 max memory_allocated 29231.615234375 
[2025-02-23 12:43:27 root] (abq_llm_calibration.py 358): INFO layer 13 iter 15 loss:0.08308089524507523 norm:0.00010008846584241837 max memory_allocated 29231.615234375 
[2025-02-23 12:44:15 root] (abq_llm_calibration.py 358): INFO layer 13 iter 16 loss:0.08293353766202927 norm:8.908055315259844e-05 max memory_allocated 29231.615234375 
[2025-02-23 12:45:04 root] (abq_llm_calibration.py 358): INFO layer 13 iter 17 loss:0.08287665992975235 norm:8.342898217961192e-05 max memory_allocated 29231.615234375 
[2025-02-23 12:45:53 root] (abq_llm_calibration.py 358): INFO layer 13 iter 18 loss:0.08273478597402573 norm:7.895746966823936e-05 max memory_allocated 29231.615234375 
[2025-02-23 12:46:42 root] (abq_llm_calibration.py 358): INFO layer 13 iter 19 loss:0.08308839797973633 norm:7.716165418969467e-05 max memory_allocated 29231.615234375 
[2025-02-23 12:46:56 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 14 ===
[2025-02-23 12:47:49 root] (abq_llm_calibration.py 358): INFO layer 14 iter 0 loss:0.10736748576164246 norm:0.0013777785934507847 max memory_allocated 29231.802734375 
[2025-02-23 12:48:38 root] (abq_llm_calibration.py 358): INFO layer 14 iter 1 loss:0.09756344556808472 norm:0.0008441233658231795 max memory_allocated 29231.802734375 
[2025-02-23 12:49:27 root] (abq_llm_calibration.py 358): INFO layer 14 iter 2 loss:0.0933905616402626 norm:0.0006334055215120316 max memory_allocated 29231.802734375 
[2025-02-23 12:50:15 root] (abq_llm_calibration.py 358): INFO layer 14 iter 3 loss:0.09115777909755707 norm:0.0005245084757916629 max memory_allocated 29231.802734375 
[2025-02-23 12:51:04 root] (abq_llm_calibration.py 358): INFO layer 14 iter 4 loss:0.0898292288184166 norm:0.00045183661859482527 max memory_allocated 29231.802734375 
[2025-02-23 12:51:53 root] (abq_llm_calibration.py 358): INFO layer 14 iter 5 loss:0.08879834413528442 norm:0.0003946082724723965 max memory_allocated 29231.802734375 
[2025-02-23 12:52:42 root] (abq_llm_calibration.py 358): INFO layer 14 iter 6 loss:0.08810696005821228 norm:0.000353470619302243 max memory_allocated 29231.802734375 
[2025-02-23 12:53:31 root] (abq_llm_calibration.py 358): INFO layer 14 iter 7 loss:0.08769332617521286 norm:0.00032450867001898587 max memory_allocated 29231.802734375 
[2025-02-23 12:54:20 root] (abq_llm_calibration.py 358): INFO layer 14 iter 8 loss:0.0874326154589653 norm:0.0003084295312874019 max memory_allocated 29231.802734375 
[2025-02-23 12:55:09 root] (abq_llm_calibration.py 358): INFO layer 14 iter 9 loss:0.08706944435834885 norm:0.0002828797441907227 max memory_allocated 29231.802734375 
[2025-02-23 12:55:58 root] (abq_llm_calibration.py 358): INFO layer 14 iter 10 loss:0.0867612361907959 norm:0.00025760161224752665 max memory_allocated 29231.802734375 
[2025-02-23 12:56:47 root] (abq_llm_calibration.py 358): INFO layer 14 iter 11 loss:0.08654633164405823 norm:0.00023471852182410657 max memory_allocated 29231.802734375 
[2025-02-23 12:57:35 root] (abq_llm_calibration.py 358): INFO layer 14 iter 12 loss:0.08640256524085999 norm:0.00021504616597667336 max memory_allocated 29231.802734375 
[2025-02-23 12:58:24 root] (abq_llm_calibration.py 358): INFO layer 14 iter 13 loss:0.0862724706530571 norm:0.0002044711000053212 max memory_allocated 29231.802734375 
[2025-02-23 12:59:13 root] (abq_llm_calibration.py 358): INFO layer 14 iter 14 loss:0.0861976146697998 norm:0.00018356599321123213 max memory_allocated 29231.802734375 
[2025-02-23 13:00:02 root] (abq_llm_calibration.py 358): INFO layer 14 iter 15 loss:0.08616678416728973 norm:0.00016528795822523534 max memory_allocated 29231.802734375 
[2025-02-23 13:00:51 root] (abq_llm_calibration.py 358): INFO layer 14 iter 16 loss:0.08612272143363953 norm:0.00014486617874354124 max memory_allocated 29231.802734375 
[2025-02-23 13:01:40 root] (abq_llm_calibration.py 358): INFO layer 14 iter 17 loss:0.08606919646263123 norm:0.00013386536738835275 max memory_allocated 29231.802734375 
[2025-02-23 13:02:29 root] (abq_llm_calibration.py 358): INFO layer 14 iter 18 loss:0.08596456050872803 norm:0.00012824343866668642 max memory_allocated 29231.802734375 
[2025-02-23 13:03:17 root] (abq_llm_calibration.py 358): INFO layer 14 iter 19 loss:0.08579345792531967 norm:0.00011815008474513888 max memory_allocated 29231.802734375 
[2025-02-23 13:03:32 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 15 ===
[2025-02-23 13:04:25 root] (abq_llm_calibration.py 358): INFO layer 15 iter 0 loss:0.10223966836929321 norm:0.001052612904459238 max memory_allocated 29231.990234375 
[2025-02-23 13:05:14 root] (abq_llm_calibration.py 358): INFO layer 15 iter 1 loss:0.09496770054101944 norm:0.0005623780307359993 max memory_allocated 29231.990234375 
[2025-02-23 13:06:03 root] (abq_llm_calibration.py 358): INFO layer 15 iter 2 loss:0.09171383827924728 norm:0.0003997336607426405 max memory_allocated 29231.990234375 
[2025-02-23 13:06:52 root] (abq_llm_calibration.py 358): INFO layer 15 iter 3 loss:0.09019442647695541 norm:0.00033388673909939826 max memory_allocated 29231.990234375 
[2025-02-23 13:07:41 root] (abq_llm_calibration.py 358): INFO layer 15 iter 4 loss:0.08913011848926544 norm:0.0002775087486952543 max memory_allocated 29231.990234375 
[2025-02-23 13:08:29 root] (abq_llm_calibration.py 358): INFO layer 15 iter 5 loss:0.08825738728046417 norm:0.00024150038370862603 max memory_allocated 29231.990234375 
[2025-02-23 13:09:18 root] (abq_llm_calibration.py 358): INFO layer 15 iter 6 loss:0.0876396894454956 norm:0.00020589194900821894 max memory_allocated 29231.990234375 
[2025-02-23 13:10:07 root] (abq_llm_calibration.py 358): INFO layer 15 iter 7 loss:0.08741074055433273 norm:0.0001868943072622642 max memory_allocated 29231.990234375 
[2025-02-23 13:10:56 root] (abq_llm_calibration.py 358): INFO layer 15 iter 8 loss:0.08708417415618896 norm:0.00016808745567686856 max memory_allocated 29231.990234375 
[2025-02-23 13:11:45 root] (abq_llm_calibration.py 358): INFO layer 15 iter 9 loss:0.08699968457221985 norm:0.00015357705706264824 max memory_allocated 29231.990234375 
[2025-02-23 13:12:34 root] (abq_llm_calibration.py 358): INFO layer 15 iter 10 loss:0.08691258728504181 norm:0.000145651341881603 max memory_allocated 29231.990234375 
[2025-02-23 13:13:22 root] (abq_llm_calibration.py 358): INFO layer 15 iter 11 loss:0.08686839789152145 norm:0.00013813996338285506 max memory_allocated 29231.990234375 
[2025-02-23 13:14:11 root] (abq_llm_calibration.py 358): INFO layer 15 iter 12 loss:0.0868377611041069 norm:0.00012960951426066458 max memory_allocated 29231.990234375 
[2025-02-23 13:15:00 root] (abq_llm_calibration.py 358): INFO layer 15 iter 13 loss:0.08682989329099655 norm:0.0001246872270712629 max memory_allocated 29231.990234375 
[2025-02-23 13:15:49 root] (abq_llm_calibration.py 358): INFO layer 15 iter 14 loss:0.0865245908498764 norm:0.00010629151074681431 max memory_allocated 29231.990234375 
[2025-02-23 13:16:38 root] (abq_llm_calibration.py 358): INFO layer 15 iter 15 loss:0.08650356531143188 norm:9.724294068291783e-05 max memory_allocated 29231.990234375 
[2025-02-23 13:17:27 root] (abq_llm_calibration.py 358): INFO layer 15 iter 16 loss:0.08658736199140549 norm:8.980712300399318e-05 max memory_allocated 29231.990234375 
[2025-02-23 13:18:15 root] (abq_llm_calibration.py 358): INFO layer 15 iter 17 loss:0.08652473241090775 norm:8.129833440762013e-05 max memory_allocated 29231.990234375 
[2025-02-23 13:19:04 root] (abq_llm_calibration.py 358): INFO layer 15 iter 18 loss:0.086634561419487 norm:8.095742668956518e-05 max memory_allocated 29231.990234375 
[2025-02-23 13:19:53 root] (abq_llm_calibration.py 358): INFO layer 15 iter 19 loss:0.08678490668535233 norm:7.884117076173425e-05 max memory_allocated 29231.990234375 
[2025-02-23 13:20:08 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 16 ===
[2025-02-23 13:21:00 root] (abq_llm_calibration.py 358): INFO layer 16 iter 0 loss:0.1102600246667862 norm:0.0018231519497931004 max memory_allocated 29232.177734375 
[2025-02-23 13:21:49 root] (abq_llm_calibration.py 358): INFO layer 16 iter 1 loss:0.10018187761306763 norm:0.0009012353839352727 max memory_allocated 29232.177734375 
[2025-02-23 13:22:38 root] (abq_llm_calibration.py 358): INFO layer 16 iter 2 loss:0.09542187303304672 norm:0.0005806103581562638 max memory_allocated 29232.177734375 
[2025-02-23 13:23:27 root] (abq_llm_calibration.py 358): INFO layer 16 iter 3 loss:0.09330648183822632 norm:0.00046216181362979114 max memory_allocated 29232.177734375 
[2025-02-23 13:24:16 root] (abq_llm_calibration.py 358): INFO layer 16 iter 4 loss:0.09174633771181107 norm:0.0003851489454973489 max memory_allocated 29232.177734375 
[2025-02-23 13:25:05 root] (abq_llm_calibration.py 358): INFO layer 16 iter 5 loss:0.09089603275060654 norm:0.00034932111157104373 max memory_allocated 29232.177734375 
[2025-02-23 13:25:54 root] (abq_llm_calibration.py 358): INFO layer 16 iter 6 loss:0.09011910855770111 norm:0.00029073047335259616 max memory_allocated 29232.177734375 
[2025-02-23 13:26:43 root] (abq_llm_calibration.py 358): INFO layer 16 iter 7 loss:0.08958091586828232 norm:0.0002540377900004387 max memory_allocated 29232.177734375 
[2025-02-23 13:27:32 root] (abq_llm_calibration.py 358): INFO layer 16 iter 8 loss:0.08911138772964478 norm:0.00022259009710978717 max memory_allocated 29232.177734375 
[2025-02-23 13:28:21 root] (abq_llm_calibration.py 358): INFO layer 16 iter 9 loss:0.08884784579277039 norm:0.00020053263870067894 max memory_allocated 29232.177734375 
[2025-02-23 13:29:10 root] (abq_llm_calibration.py 358): INFO layer 16 iter 10 loss:0.08867093920707703 norm:0.00017421274969819933 max memory_allocated 29232.177734375 
[2025-02-23 13:29:58 root] (abq_llm_calibration.py 358): INFO layer 16 iter 11 loss:0.08858893811702728 norm:0.0001598755334271118 max memory_allocated 29232.177734375 
[2025-02-23 13:30:47 root] (abq_llm_calibration.py 358): INFO layer 16 iter 12 loss:0.08837450295686722 norm:0.00014165989705361426 max memory_allocated 29232.177734375 
[2025-02-23 13:31:36 root] (abq_llm_calibration.py 358): INFO layer 16 iter 13 loss:0.08800125867128372 norm:0.0001237889373442158 max memory_allocated 29232.177734375 
[2025-02-23 13:32:25 root] (abq_llm_calibration.py 358): INFO layer 16 iter 14 loss:0.08782681822776794 norm:0.00011086893937317654 max memory_allocated 29232.177734375 
[2025-02-23 13:33:14 root] (abq_llm_calibration.py 358): INFO layer 16 iter 15 loss:0.08784960210323334 norm:0.00010200968972640112 max memory_allocated 29232.177734375 
[2025-02-23 13:34:02 root] (abq_llm_calibration.py 358): INFO layer 16 iter 16 loss:0.08776817470788956 norm:9.585852967575192e-05 max memory_allocated 29232.177734375 
[2025-02-23 13:34:51 root] (abq_llm_calibration.py 358): INFO layer 16 iter 17 loss:0.08770111948251724 norm:9.499498264631256e-05 max memory_allocated 29232.177734375 
[2025-02-23 13:35:40 root] (abq_llm_calibration.py 358): INFO layer 16 iter 18 loss:0.08764661848545074 norm:9.044593753060326e-05 max memory_allocated 29232.177734375 
[2025-02-23 13:36:29 root] (abq_llm_calibration.py 358): INFO layer 16 iter 19 loss:0.08772322535514832 norm:8.633194374851882e-05 max memory_allocated 29232.177734375 
[2025-02-23 13:36:43 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 17 ===
[2025-02-23 13:37:36 root] (abq_llm_calibration.py 358): INFO layer 17 iter 0 loss:0.10344818234443665 norm:0.0009386768797412515 max memory_allocated 29232.365234375 
[2025-02-23 13:38:25 root] (abq_llm_calibration.py 358): INFO layer 17 iter 1 loss:0.09644002467393875 norm:0.0004615522630047053 max memory_allocated 29232.365234375 
[2025-02-23 13:39:14 root] (abq_llm_calibration.py 358): INFO layer 17 iter 2 loss:0.09369700402021408 norm:0.00032425939571112394 max memory_allocated 29232.365234375 
[2025-02-23 13:40:02 root] (abq_llm_calibration.py 358): INFO layer 17 iter 3 loss:0.09222200512886047 norm:0.00025623320834711194 max memory_allocated 29232.365234375 
[2025-02-23 13:40:51 root] (abq_llm_calibration.py 358): INFO layer 17 iter 4 loss:0.09114383906126022 norm:0.00020520034013316035 max memory_allocated 29232.365234375 
[2025-02-23 13:41:40 root] (abq_llm_calibration.py 358): INFO layer 17 iter 5 loss:0.09034499526023865 norm:0.00017547959578223526 max memory_allocated 29232.365234375 
[2025-02-23 13:42:28 root] (abq_llm_calibration.py 358): INFO layer 17 iter 6 loss:0.08993718773126602 norm:0.0001505096151959151 max memory_allocated 29232.365234375 
[2025-02-23 13:43:17 root] (abq_llm_calibration.py 358): INFO layer 17 iter 7 loss:0.08960353583097458 norm:0.00013124594988767058 max memory_allocated 29232.365234375 
[2025-02-23 13:44:06 root] (abq_llm_calibration.py 358): INFO layer 17 iter 8 loss:0.08936584740877151 norm:0.00011725476360879838 max memory_allocated 29232.365234375 
[2025-02-23 13:44:55 root] (abq_llm_calibration.py 358): INFO layer 17 iter 9 loss:0.08917472511529922 norm:0.00010947803821181878 max memory_allocated 29232.365234375 
[2025-02-23 13:45:44 root] (abq_llm_calibration.py 358): INFO layer 17 iter 10 loss:0.0891043096780777 norm:0.00010562689567450434 max memory_allocated 29232.365234375 
[2025-02-23 13:46:33 root] (abq_llm_calibration.py 358): INFO layer 17 iter 11 loss:0.08900613337755203 norm:9.881253208732232e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:47:22 root] (abq_llm_calibration.py 358): INFO layer 17 iter 12 loss:0.0890548899769783 norm:9.578157914802432e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:48:11 root] (abq_llm_calibration.py 358): INFO layer 17 iter 13 loss:0.08902320265769958 norm:9.337579103885219e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:48:59 root] (abq_llm_calibration.py 358): INFO layer 17 iter 14 loss:0.08901003003120422 norm:8.999135752674192e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:49:48 root] (abq_llm_calibration.py 358): INFO layer 17 iter 15 loss:0.08899090439081192 norm:8.253035775851458e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:50:37 root] (abq_llm_calibration.py 358): INFO layer 17 iter 16 loss:0.08913346379995346 norm:8.456479554297403e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:51:26 root] (abq_llm_calibration.py 358): INFO layer 17 iter 17 loss:0.08929304778575897 norm:8.99065998964943e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:52:15 root] (abq_llm_calibration.py 358): INFO layer 17 iter 18 loss:0.08915175497531891 norm:8.543955482309684e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:53:04 root] (abq_llm_calibration.py 358): INFO layer 17 iter 19 loss:0.08897273987531662 norm:7.993575127329677e-05 max memory_allocated 29232.365234375 
[2025-02-23 13:53:18 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 18 ===
[2025-02-23 13:54:11 root] (abq_llm_calibration.py 358): INFO layer 18 iter 0 loss:0.10462770611047745 norm:0.0010456707095727324 max memory_allocated 29232.552734375 
[2025-02-23 13:55:00 root] (abq_llm_calibration.py 358): INFO layer 18 iter 1 loss:0.09762585908174515 norm:0.0004629241593647748 max memory_allocated 29232.552734375 
[2025-02-23 13:55:48 root] (abq_llm_calibration.py 358): INFO layer 18 iter 2 loss:0.09489575028419495 norm:0.0003074706473853439 max memory_allocated 29232.552734375 
[2025-02-23 13:56:37 root] (abq_llm_calibration.py 358): INFO layer 18 iter 3 loss:0.09346985071897507 norm:0.00023496197536587715 max memory_allocated 29232.552734375 
[2025-02-23 13:57:26 root] (abq_llm_calibration.py 358): INFO layer 18 iter 4 loss:0.09255021065473557 norm:0.00019845394126605242 max memory_allocated 29232.552734375 
[2025-02-23 13:58:15 root] (abq_llm_calibration.py 358): INFO layer 18 iter 5 loss:0.09181657433509827 norm:0.0001678015396464616 max memory_allocated 29232.552734375 
[2025-02-23 13:59:04 root] (abq_llm_calibration.py 358): INFO layer 18 iter 6 loss:0.09137207269668579 norm:0.00014942308189347386 max memory_allocated 29232.552734375 
[2025-02-23 13:59:52 root] (abq_llm_calibration.py 358): INFO layer 18 iter 7 loss:0.09108409285545349 norm:0.00013356712588574737 max memory_allocated 29232.552734375 
[2025-02-23 14:00:41 root] (abq_llm_calibration.py 358): INFO layer 18 iter 8 loss:0.0908256322145462 norm:0.00011697165609803051 max memory_allocated 29232.552734375 
[2025-02-23 14:01:30 root] (abq_llm_calibration.py 358): INFO layer 18 iter 9 loss:0.09058438986539841 norm:0.00010902816575253382 max memory_allocated 29232.552734375 
[2025-02-23 14:02:19 root] (abq_llm_calibration.py 358): INFO layer 18 iter 10 loss:0.09051113575696945 norm:0.00010475967428646982 max memory_allocated 29232.552734375 
[2025-02-23 14:03:08 root] (abq_llm_calibration.py 358): INFO layer 18 iter 11 loss:0.09054511785507202 norm:0.00010096593905473128 max memory_allocated 29232.552734375 
[2025-02-23 14:03:57 root] (abq_llm_calibration.py 358): INFO layer 18 iter 12 loss:0.09047991782426834 norm:9.550616232445464e-05 max memory_allocated 29232.552734375 
[2025-02-23 14:04:46 root] (abq_llm_calibration.py 358): INFO layer 18 iter 13 loss:0.09035857766866684 norm:8.776849426794797e-05 max memory_allocated 29232.552734375 
[2025-02-23 14:05:35 root] (abq_llm_calibration.py 358): INFO layer 18 iter 14 loss:0.09034134447574615 norm:8.596322004450485e-05 max memory_allocated 29232.552734375 
[2025-02-23 14:06:23 root] (abq_llm_calibration.py 358): INFO layer 18 iter 15 loss:0.09035839885473251 norm:8.509765029884875e-05 max memory_allocated 29232.552734375 
[2025-02-23 14:07:12 root] (abq_llm_calibration.py 358): INFO layer 18 iter 16 loss:0.09025972336530685 norm:8.267102384706959e-05 max memory_allocated 29232.552734375 
[2025-02-23 14:08:01 root] (abq_llm_calibration.py 358): INFO layer 18 iter 17 loss:0.09019680321216583 norm:7.726111653028056e-05 max memory_allocated 29232.552734375 
[2025-02-23 14:08:50 root] (abq_llm_calibration.py 358): INFO layer 18 iter 18 loss:0.090272918343544 norm:7.77064124122262e-05 max memory_allocated 29232.552734375 
[2025-02-23 14:09:39 root] (abq_llm_calibration.py 358): INFO layer 18 iter 19 loss:0.09039289504289627 norm:7.84568110248074e-05 max memory_allocated 29232.552734375 
[2025-02-23 14:09:53 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 19 ===
[2025-02-23 14:10:46 root] (abq_llm_calibration.py 358): INFO layer 19 iter 0 loss:0.10702298581600189 norm:0.000874398450832814 max memory_allocated 29232.740234375 
[2025-02-23 14:11:35 root] (abq_llm_calibration.py 358): INFO layer 19 iter 1 loss:0.10117263346910477 norm:0.00042553641833364964 max memory_allocated 29232.740234375 
[2025-02-23 14:12:24 root] (abq_llm_calibration.py 358): INFO layer 19 iter 2 loss:0.09870564937591553 norm:0.00027828302700072527 max memory_allocated 29232.740234375 
[2025-02-23 14:13:12 root] (abq_llm_calibration.py 358): INFO layer 19 iter 3 loss:0.0972553938627243 norm:0.00021100138837937266 max memory_allocated 29232.740234375 
[2025-02-23 14:14:01 root] (abq_llm_calibration.py 358): INFO layer 19 iter 4 loss:0.09646059572696686 norm:0.0001794690324459225 max memory_allocated 29232.740234375 
[2025-02-23 14:14:50 root] (abq_llm_calibration.py 358): INFO layer 19 iter 5 loss:0.09576640278100967 norm:0.00014708731032442302 max memory_allocated 29232.740234375 
[2025-02-23 14:15:39 root] (abq_llm_calibration.py 358): INFO layer 19 iter 6 loss:0.0952497199177742 norm:0.00012949539814144373 max memory_allocated 29232.740234375 
[2025-02-23 14:16:28 root] (abq_llm_calibration.py 358): INFO layer 19 iter 7 loss:0.09494876116514206 norm:0.00011818455095635727 max memory_allocated 29232.740234375 
[2025-02-23 14:17:16 root] (abq_llm_calibration.py 358): INFO layer 19 iter 8 loss:0.0947684496641159 norm:0.00010409750393591821 max memory_allocated 29232.740234375 
[2025-02-23 14:18:05 root] (abq_llm_calibration.py 358): INFO layer 19 iter 9 loss:0.09461895376443863 norm:9.326302824774757e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:18:54 root] (abq_llm_calibration.py 358): INFO layer 19 iter 10 loss:0.09454797208309174 norm:8.48350755404681e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:19:43 root] (abq_llm_calibration.py 358): INFO layer 19 iter 11 loss:0.09441517293453217 norm:7.677359826629981e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:20:32 root] (abq_llm_calibration.py 358): INFO layer 19 iter 12 loss:0.09438588470220566 norm:7.47799567761831e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:21:21 root] (abq_llm_calibration.py 358): INFO layer 19 iter 13 loss:0.0943986251950264 norm:7.256303069880232e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:22:10 root] (abq_llm_calibration.py 358): INFO layer 19 iter 14 loss:0.09435068070888519 norm:7.016305607976392e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:22:59 root] (abq_llm_calibration.py 358): INFO layer 19 iter 15 loss:0.09426446259021759 norm:6.916954589542001e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:23:48 root] (abq_llm_calibration.py 358): INFO layer 19 iter 16 loss:0.09428463131189346 norm:6.87420615577139e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:24:37 root] (abq_llm_calibration.py 358): INFO layer 19 iter 17 loss:0.09413661062717438 norm:6.532348925247788e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:25:26 root] (abq_llm_calibration.py 358): INFO layer 19 iter 18 loss:0.09403756260871887 norm:6.332505290629342e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:26:14 root] (abq_llm_calibration.py 358): INFO layer 19 iter 19 loss:0.09408032894134521 norm:6.291261524893343e-05 max memory_allocated 29232.740234375 
[2025-02-23 14:26:28 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 20 ===
[2025-02-23 14:27:21 root] (abq_llm_calibration.py 358): INFO layer 20 iter 0 loss:0.11037839204072952 norm:0.0007928370614536107 max memory_allocated 29232.927734375 
[2025-02-23 14:28:10 root] (abq_llm_calibration.py 358): INFO layer 20 iter 1 loss:0.10538964718580246 norm:0.0003859612043015659 max memory_allocated 29232.927734375 
[2025-02-23 14:28:59 root] (abq_llm_calibration.py 358): INFO layer 20 iter 2 loss:0.10296144336462021 norm:0.0002655262069310993 max memory_allocated 29232.927734375 
[2025-02-23 14:29:48 root] (abq_llm_calibration.py 358): INFO layer 20 iter 3 loss:0.1016484797000885 norm:0.00021158599702175707 max memory_allocated 29232.927734375 
[2025-02-23 14:30:36 root] (abq_llm_calibration.py 358): INFO layer 20 iter 4 loss:0.1007588654756546 norm:0.00016992252494674176 max memory_allocated 29232.927734375 
[2025-02-23 14:31:25 root] (abq_llm_calibration.py 358): INFO layer 20 iter 5 loss:0.1001126617193222 norm:0.00014582110452465713 max memory_allocated 29232.927734375 
[2025-02-23 14:32:14 root] (abq_llm_calibration.py 358): INFO layer 20 iter 6 loss:0.09964028000831604 norm:0.0001273220550501719 max memory_allocated 29232.927734375 
[2025-02-23 14:33:03 root] (abq_llm_calibration.py 358): INFO layer 20 iter 7 loss:0.09925101697444916 norm:0.00011775756138376892 max memory_allocated 29232.927734375 
[2025-02-23 14:33:52 root] (abq_llm_calibration.py 358): INFO layer 20 iter 8 loss:0.09895063191652298 norm:0.00010497539187781513 max memory_allocated 29232.927734375 
[2025-02-23 14:34:40 root] (abq_llm_calibration.py 358): INFO layer 20 iter 9 loss:0.09882897883653641 norm:9.428182238480076e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:35:29 root] (abq_llm_calibration.py 358): INFO layer 20 iter 10 loss:0.09881220757961273 norm:8.7231514044106e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:36:18 root] (abq_llm_calibration.py 358): INFO layer 20 iter 11 loss:0.09865441173315048 norm:7.751276280032471e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:37:07 root] (abq_llm_calibration.py 358): INFO layer 20 iter 12 loss:0.09855017811059952 norm:7.367593934759498e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:37:56 root] (abq_llm_calibration.py 358): INFO layer 20 iter 13 loss:0.0984804555773735 norm:6.809059414081275e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:38:44 root] (abq_llm_calibration.py 358): INFO layer 20 iter 14 loss:0.09851212054491043 norm:6.559620669577271e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:39:33 root] (abq_llm_calibration.py 358): INFO layer 20 iter 15 loss:0.09855158627033234 norm:6.796753586968407e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:40:22 root] (abq_llm_calibration.py 358): INFO layer 20 iter 16 loss:0.09859864413738251 norm:6.488845247076824e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:41:11 root] (abq_llm_calibration.py 358): INFO layer 20 iter 17 loss:0.0985790491104126 norm:6.767819286324084e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:42:00 root] (abq_llm_calibration.py 358): INFO layer 20 iter 18 loss:0.09856881201267242 norm:6.279699300648645e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:42:49 root] (abq_llm_calibration.py 358): INFO layer 20 iter 19 loss:0.09847401082515717 norm:6.343067798297852e-05 max memory_allocated 29232.927734375 
[2025-02-23 14:43:03 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 21 ===
[2025-02-23 14:43:56 root] (abq_llm_calibration.py 358): INFO layer 21 iter 0 loss:0.1195693388581276 norm:0.0007943021482788026 max memory_allocated 29233.115234375 
[2025-02-23 14:44:45 root] (abq_llm_calibration.py 358): INFO layer 21 iter 1 loss:0.11449384689331055 norm:0.000434744986705482 max memory_allocated 29233.115234375 
[2025-02-23 14:45:33 root] (abq_llm_calibration.py 358): INFO layer 21 iter 2 loss:0.11196229606866837 norm:0.0003053632390219718 max memory_allocated 29233.115234375 
[2025-02-23 14:46:22 root] (abq_llm_calibration.py 358): INFO layer 21 iter 3 loss:0.11045291274785995 norm:0.0002506899181753397 max memory_allocated 29233.115234375 
[2025-02-23 14:47:11 root] (abq_llm_calibration.py 358): INFO layer 21 iter 4 loss:0.10929598659276962 norm:0.00019521488866303116 max memory_allocated 29233.115234375 
[2025-02-23 14:48:00 root] (abq_llm_calibration.py 358): INFO layer 21 iter 5 loss:0.10852283239364624 norm:0.00016654374485369772 max memory_allocated 29233.115234375 
[2025-02-23 14:48:49 root] (abq_llm_calibration.py 358): INFO layer 21 iter 6 loss:0.10794417560100555 norm:0.00014959252439439297 max memory_allocated 29233.115234375 
[2025-02-23 14:49:38 root] (abq_llm_calibration.py 358): INFO layer 21 iter 7 loss:0.10751727968454361 norm:0.00013228401076048613 max memory_allocated 29233.115234375 
[2025-02-23 14:50:26 root] (abq_llm_calibration.py 358): INFO layer 21 iter 8 loss:0.10726895183324814 norm:0.00012207582767587155 max memory_allocated 29233.115234375 
[2025-02-23 14:51:15 root] (abq_llm_calibration.py 358): INFO layer 21 iter 9 loss:0.10705471783876419 norm:0.00011338997865095735 max memory_allocated 29233.115234375 
[2025-02-23 14:52:04 root] (abq_llm_calibration.py 358): INFO layer 21 iter 10 loss:0.10697664320468903 norm:0.00010462004865985364 max memory_allocated 29233.115234375 
[2025-02-23 14:52:53 root] (abq_llm_calibration.py 358): INFO layer 21 iter 11 loss:0.10685411095619202 norm:9.56010480877012e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:53:42 root] (abq_llm_calibration.py 358): INFO layer 21 iter 12 loss:0.10681430250406265 norm:8.640464511699975e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:54:30 root] (abq_llm_calibration.py 358): INFO layer 21 iter 13 loss:0.10675807297229767 norm:8.117831021081656e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:55:19 root] (abq_llm_calibration.py 358): INFO layer 21 iter 14 loss:0.10664191842079163 norm:7.97597604105249e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:56:08 root] (abq_llm_calibration.py 358): INFO layer 21 iter 15 loss:0.1065426617860794 norm:7.654160435777158e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:56:57 root] (abq_llm_calibration.py 358): INFO layer 21 iter 16 loss:0.10642170906066895 norm:7.259169069584459e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:57:46 root] (abq_llm_calibration.py 358): INFO layer 21 iter 17 loss:0.10638811439275742 norm:7.072674634400755e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:58:34 root] (abq_llm_calibration.py 358): INFO layer 21 iter 18 loss:0.10644326359033585 norm:7.069387356750667e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:59:23 root] (abq_llm_calibration.py 358): INFO layer 21 iter 19 loss:0.10645219683647156 norm:6.995228613959625e-05 max memory_allocated 29233.115234375 
[2025-02-23 14:59:37 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 22 ===
[2025-02-23 15:00:30 root] (abq_llm_calibration.py 358): INFO layer 22 iter 0 loss:0.1241641715168953 norm:0.00048374864854849875 max memory_allocated 29233.302734375 
[2025-02-23 15:01:19 root] (abq_llm_calibration.py 358): INFO layer 22 iter 1 loss:0.1205078661441803 norm:0.0002619599981699139 max memory_allocated 29233.302734375 
[2025-02-23 15:02:08 root] (abq_llm_calibration.py 358): INFO layer 22 iter 2 loss:0.11870366334915161 norm:0.00019360671285539865 max memory_allocated 29233.302734375 
[2025-02-23 15:02:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 3 loss:0.11746848374605179 norm:0.0001573197077959776 max memory_allocated 29233.302734375 
[2025-02-23 15:03:46 root] (abq_llm_calibration.py 358): INFO layer 22 iter 4 loss:0.11663547158241272 norm:0.0001313914981437847 max memory_allocated 29233.302734375 
[2025-02-23 15:04:34 root] (abq_llm_calibration.py 358): INFO layer 22 iter 5 loss:0.11593584716320038 norm:0.00011399309005355462 max memory_allocated 29233.302734375 
[2025-02-23 15:05:23 root] (abq_llm_calibration.py 358): INFO layer 22 iter 6 loss:0.11545199155807495 norm:0.00010520677460590377 max memory_allocated 29233.302734375 
[2025-02-23 15:06:12 root] (abq_llm_calibration.py 358): INFO layer 22 iter 7 loss:0.11505697667598724 norm:9.214112651534379e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:07:01 root] (abq_llm_calibration.py 358): INFO layer 22 iter 8 loss:0.11473146080970764 norm:8.154135139193386e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:07:50 root] (abq_llm_calibration.py 358): INFO layer 22 iter 9 loss:0.11448460817337036 norm:8.292479469673708e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:08:39 root] (abq_llm_calibration.py 358): INFO layer 22 iter 10 loss:0.11434455960988998 norm:6.973752897465602e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:09:27 root] (abq_llm_calibration.py 358): INFO layer 22 iter 11 loss:0.11418341100215912 norm:6.65632906020619e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:10:16 root] (abq_llm_calibration.py 358): INFO layer 22 iter 12 loss:0.11408355087041855 norm:6.325221329461783e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:11:05 root] (abq_llm_calibration.py 358): INFO layer 22 iter 13 loss:0.11405883729457855 norm:6.103641499066725e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:11:54 root] (abq_llm_calibration.py 358): INFO layer 22 iter 14 loss:0.11403608322143555 norm:5.918771057622507e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:12:42 root] (abq_llm_calibration.py 358): INFO layer 22 iter 15 loss:0.1139865294098854 norm:5.7634726545074955e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:13:32 root] (abq_llm_calibration.py 358): INFO layer 22 iter 16 loss:0.11390640586614609 norm:5.6790966482367367e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:14:20 root] (abq_llm_calibration.py 358): INFO layer 22 iter 17 loss:0.1138291209936142 norm:5.6624121498316526e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:15:09 root] (abq_llm_calibration.py 358): INFO layer 22 iter 18 loss:0.11378486454486847 norm:5.574091483140364e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:15:57 root] (abq_llm_calibration.py 358): INFO layer 22 iter 19 loss:0.11375706642866135 norm:6.0510548792080954e-05 max memory_allocated 29233.302734375 
[2025-02-23 15:16:12 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 23 ===
[2025-02-23 15:17:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 0 loss:0.1350957453250885 norm:0.0005522577557712793 max memory_allocated 29233.490234375 
[2025-02-23 15:17:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 1 loss:0.1315091848373413 norm:0.0003499825834296644 max memory_allocated 29233.490234375 
[2025-02-23 15:18:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 2 loss:0.12953612208366394 norm:0.00025994685711339116 max memory_allocated 29233.490234375 
[2025-02-23 15:19:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 3 loss:0.12819913029670715 norm:0.00020466271962504834 max memory_allocated 29233.490234375 
[2025-02-23 15:20:20 root] (abq_llm_calibration.py 358): INFO layer 23 iter 4 loss:0.1272236406803131 norm:0.00016744760796427727 max memory_allocated 29233.490234375 
[2025-02-23 15:21:08 root] (abq_llm_calibration.py 358): INFO layer 23 iter 5 loss:0.12650921940803528 norm:0.00014294678112491965 max memory_allocated 29233.490234375 
[2025-02-23 15:21:57 root] (abq_llm_calibration.py 358): INFO layer 23 iter 6 loss:0.1259835660457611 norm:0.0001245087623829022 max memory_allocated 29233.490234375 
[2025-02-23 15:22:46 root] (abq_llm_calibration.py 358): INFO layer 23 iter 7 loss:0.12555818259716034 norm:0.00010918569751083851 max memory_allocated 29233.490234375 
[2025-02-23 15:23:34 root] (abq_llm_calibration.py 358): INFO layer 23 iter 8 loss:0.12521713972091675 norm:9.821671119425446e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:24:23 root] (abq_llm_calibration.py 358): INFO layer 23 iter 9 loss:0.12499427795410156 norm:9.021320875035599e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:25:12 root] (abq_llm_calibration.py 358): INFO layer 23 iter 10 loss:0.1247846782207489 norm:8.137438271660358e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:26:01 root] (abq_llm_calibration.py 358): INFO layer 23 iter 11 loss:0.12461328506469727 norm:7.36560468794778e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:26:49 root] (abq_llm_calibration.py 358): INFO layer 23 iter 12 loss:0.12447706609964371 norm:6.892454985063523e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:27:38 root] (abq_llm_calibration.py 358): INFO layer 23 iter 13 loss:0.12436045706272125 norm:6.502320320578292e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:28:27 root] (abq_llm_calibration.py 358): INFO layer 23 iter 14 loss:0.12429380416870117 norm:6.14235905231908e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:29:16 root] (abq_llm_calibration.py 358): INFO layer 23 iter 15 loss:0.12423571944236755 norm:5.92734977544751e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:30:04 root] (abq_llm_calibration.py 358): INFO layer 23 iter 16 loss:0.12417802959680557 norm:5.77463251829613e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:30:53 root] (abq_llm_calibration.py 358): INFO layer 23 iter 17 loss:0.1241387352347374 norm:5.7034354540519416e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:31:42 root] (abq_llm_calibration.py 358): INFO layer 23 iter 18 loss:0.12411302328109741 norm:5.5902739404700696e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:32:31 root] (abq_llm_calibration.py 358): INFO layer 23 iter 19 loss:0.12407360970973969 norm:5.552941365749575e-05 max memory_allocated 29233.490234375 
[2025-02-23 15:32:45 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 24 ===
[2025-02-23 15:33:38 root] (abq_llm_calibration.py 358): INFO layer 24 iter 0 loss:0.14614443480968475 norm:0.0004651997587643564 max memory_allocated 29233.677734375 
[2025-02-23 15:34:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 1 loss:0.1425953358411789 norm:0.0002860759268514812 max memory_allocated 29233.677734375 
[2025-02-23 15:35:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 2 loss:0.1406974196434021 norm:0.00021381812985055149 max memory_allocated 29233.677734375 
[2025-02-23 15:36:04 root] (abq_llm_calibration.py 358): INFO layer 24 iter 3 loss:0.13940058648586273 norm:0.0001653257932048291 max memory_allocated 29233.677734375 
[2025-02-23 15:36:53 root] (abq_llm_calibration.py 358): INFO layer 24 iter 4 loss:0.1383974552154541 norm:0.0001340756134595722 max memory_allocated 29233.677734375 
[2025-02-23 15:37:42 root] (abq_llm_calibration.py 358): INFO layer 24 iter 5 loss:0.1376989632844925 norm:0.0001177562735392712 max memory_allocated 29233.677734375 
[2025-02-23 15:38:31 root] (abq_llm_calibration.py 358): INFO layer 24 iter 6 loss:0.137168288230896 norm:0.00010852626292034984 max memory_allocated 29233.677734375 
[2025-02-23 15:39:19 root] (abq_llm_calibration.py 358): INFO layer 24 iter 7 loss:0.136739119887352 norm:9.586849046172574e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:40:08 root] (abq_llm_calibration.py 358): INFO layer 24 iter 8 loss:0.13638021051883698 norm:8.562010043533519e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:40:57 root] (abq_llm_calibration.py 358): INFO layer 24 iter 9 loss:0.13610467314720154 norm:7.90082267485559e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:41:46 root] (abq_llm_calibration.py 358): INFO layer 24 iter 10 loss:0.13586696982383728 norm:7.31434702174738e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:42:35 root] (abq_llm_calibration.py 358): INFO layer 24 iter 11 loss:0.13574260473251343 norm:6.991389818722382e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:43:24 root] (abq_llm_calibration.py 358): INFO layer 24 iter 12 loss:0.13561317324638367 norm:6.688074790872633e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:44:12 root] (abq_llm_calibration.py 358): INFO layer 24 iter 13 loss:0.13552725315093994 norm:6.55851763440296e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:45:01 root] (abq_llm_calibration.py 358): INFO layer 24 iter 14 loss:0.13539616763591766 norm:6.495331035694107e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:45:50 root] (abq_llm_calibration.py 358): INFO layer 24 iter 15 loss:0.13531054556369781 norm:6.38389028608799e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:46:39 root] (abq_llm_calibration.py 358): INFO layer 24 iter 16 loss:0.13530388474464417 norm:6.311584002105519e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:47:27 root] (abq_llm_calibration.py 358): INFO layer 24 iter 17 loss:0.13522252440452576 norm:6.216370820766315e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:48:16 root] (abq_llm_calibration.py 358): INFO layer 24 iter 18 loss:0.1351810097694397 norm:6.238628702703863e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:49:05 root] (abq_llm_calibration.py 358): INFO layer 24 iter 19 loss:0.13514076173305511 norm:6.162186764413491e-05 max memory_allocated 29233.677734375 
[2025-02-23 15:49:20 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 25 ===
[2025-02-23 15:50:13 root] (abq_llm_calibration.py 358): INFO layer 25 iter 0 loss:0.1599428802728653 norm:0.0004513380117714405 max memory_allocated 29233.865234375 
[2025-02-23 15:51:02 root] (abq_llm_calibration.py 358): INFO layer 25 iter 1 loss:0.15671604871749878 norm:0.00028670462779700756 max memory_allocated 29233.865234375 
[2025-02-23 15:51:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 2 loss:0.15472537279129028 norm:0.0002108182670781389 max memory_allocated 29233.865234375 
[2025-02-23 15:52:39 root] (abq_llm_calibration.py 358): INFO layer 25 iter 3 loss:0.1533387452363968 norm:0.00016920114285312593 max memory_allocated 29233.865234375 
[2025-02-23 15:53:28 root] (abq_llm_calibration.py 358): INFO layer 25 iter 4 loss:0.15231317281723022 norm:0.00014306219236459583 max memory_allocated 29233.865234375 
[2025-02-23 15:54:17 root] (abq_llm_calibration.py 358): INFO layer 25 iter 5 loss:0.15150924026966095 norm:0.0001215760494233109 max memory_allocated 29233.865234375 
[2025-02-23 15:55:06 root] (abq_llm_calibration.py 358): INFO layer 25 iter 6 loss:0.1508944034576416 norm:0.00010746654879767448 max memory_allocated 29233.865234375 
[2025-02-23 15:55:55 root] (abq_llm_calibration.py 358): INFO layer 25 iter 7 loss:0.1503751128911972 norm:9.45392093854025e-05 max memory_allocated 29233.865234375 
[2025-02-23 15:56:43 root] (abq_llm_calibration.py 358): INFO layer 25 iter 8 loss:0.14995913207530975 norm:8.470470493193716e-05 max memory_allocated 29233.865234375 
[2025-02-23 15:57:32 root] (abq_llm_calibration.py 358): INFO layer 25 iter 9 loss:0.1496458351612091 norm:7.77070308686234e-05 max memory_allocated 29233.865234375 
[2025-02-23 15:58:21 root] (abq_llm_calibration.py 358): INFO layer 25 iter 10 loss:0.14940115809440613 norm:7.117258792277426e-05 max memory_allocated 29233.865234375 
[2025-02-23 15:59:10 root] (abq_llm_calibration.py 358): INFO layer 25 iter 11 loss:0.14920392632484436 norm:6.61692101857625e-05 max memory_allocated 29233.865234375 
[2025-02-23 15:59:58 root] (abq_llm_calibration.py 358): INFO layer 25 iter 12 loss:0.14906597137451172 norm:6.221664079930633e-05 max memory_allocated 29233.865234375 
[2025-02-23 16:00:47 root] (abq_llm_calibration.py 358): INFO layer 25 iter 13 loss:0.14895735681056976 norm:5.969702760921791e-05 max memory_allocated 29233.865234375 
[2025-02-23 16:01:37 root] (abq_llm_calibration.py 358): INFO layer 25 iter 14 loss:0.1488446444272995 norm:5.7502391427988186e-05 max memory_allocated 29233.865234375 
[2025-02-23 16:02:25 root] (abq_llm_calibration.py 358): INFO layer 25 iter 15 loss:0.14874912798404694 norm:5.5691012676106766e-05 max memory_allocated 29233.865234375 
[2025-02-23 16:03:14 root] (abq_llm_calibration.py 358): INFO layer 25 iter 16 loss:0.14868317544460297 norm:5.4361204092856497e-05 max memory_allocated 29233.865234375 
[2025-02-23 16:04:03 root] (abq_llm_calibration.py 358): INFO layer 25 iter 17 loss:0.14861665666103363 norm:5.307624815031886e-05 max memory_allocated 29233.865234375 
[2025-02-23 16:04:51 root] (abq_llm_calibration.py 358): INFO layer 25 iter 18 loss:0.14855632185935974 norm:5.2478164434432983e-05 max memory_allocated 29233.865234375 
[2025-02-23 16:05:40 root] (abq_llm_calibration.py 358): INFO layer 25 iter 19 loss:0.14849138259887695 norm:5.18897722940892e-05 max memory_allocated 29233.865234375 
[2025-02-23 16:05:54 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 26 ===
[2025-02-23 16:06:47 root] (abq_llm_calibration.py 358): INFO layer 26 iter 0 loss:0.1766332983970642 norm:0.000521375797688961 max memory_allocated 29234.052734375 
[2025-02-23 16:07:36 root] (abq_llm_calibration.py 358): INFO layer 26 iter 1 loss:0.17296643555164337 norm:0.00031850635423325 max memory_allocated 29234.052734375 
[2025-02-23 16:08:24 root] (abq_llm_calibration.py 358): INFO layer 26 iter 2 loss:0.17091108858585358 norm:0.00024445357848890126 max memory_allocated 29234.052734375 
[2025-02-23 16:09:13 root] (abq_llm_calibration.py 358): INFO layer 26 iter 3 loss:0.16934886574745178 norm:0.00019275947124697268 max memory_allocated 29234.052734375 
[2025-02-23 16:10:02 root] (abq_llm_calibration.py 358): INFO layer 26 iter 4 loss:0.1682065725326538 norm:0.00016001662879716605 max memory_allocated 29234.052734375 
[2025-02-23 16:10:51 root] (abq_llm_calibration.py 358): INFO layer 26 iter 5 loss:0.16731974482536316 norm:0.0001388562232023105 max memory_allocated 29234.052734375 
[2025-02-23 16:11:39 root] (abq_llm_calibration.py 358): INFO layer 26 iter 6 loss:0.16661867499351501 norm:0.0001241581339854747 max memory_allocated 29234.052734375 
[2025-02-23 16:12:28 root] (abq_llm_calibration.py 358): INFO layer 26 iter 7 loss:0.16606047749519348 norm:0.00011452107719378546 max memory_allocated 29234.052734375 
[2025-02-23 16:13:17 root] (abq_llm_calibration.py 358): INFO layer 26 iter 8 loss:0.16557204723358154 norm:0.00010244079021504149 max memory_allocated 29234.052734375 
[2025-02-23 16:14:06 root] (abq_llm_calibration.py 358): INFO layer 26 iter 9 loss:0.16520509123802185 norm:9.576755110174417e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:14:55 root] (abq_llm_calibration.py 358): INFO layer 26 iter 10 loss:0.1648966670036316 norm:9.241846419172361e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:15:44 root] (abq_llm_calibration.py 358): INFO layer 26 iter 11 loss:0.16471704840660095 norm:8.846797572914511e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:16:33 root] (abq_llm_calibration.py 358): INFO layer 26 iter 12 loss:0.1645580530166626 norm:8.06161406217143e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:17:21 root] (abq_llm_calibration.py 358): INFO layer 26 iter 13 loss:0.16447708010673523 norm:7.747716153971851e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:18:10 root] (abq_llm_calibration.py 358): INFO layer 26 iter 14 loss:0.16442294418811798 norm:7.596756768180057e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:18:59 root] (abq_llm_calibration.py 358): INFO layer 26 iter 15 loss:0.16438867151737213 norm:7.532792369602248e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:19:48 root] (abq_llm_calibration.py 358): INFO layer 26 iter 16 loss:0.1643463373184204 norm:7.540766819147393e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:20:37 root] (abq_llm_calibration.py 358): INFO layer 26 iter 17 loss:0.16427020728588104 norm:7.517948688473552e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:21:26 root] (abq_llm_calibration.py 358): INFO layer 26 iter 18 loss:0.16423164308071136 norm:7.540007209172472e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:22:14 root] (abq_llm_calibration.py 358): INFO layer 26 iter 19 loss:0.16419437527656555 norm:7.555260526714846e-05 max memory_allocated 29234.052734375 
[2025-02-23 16:22:29 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 27 ===
[2025-02-23 16:23:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 0 loss:0.19090917706489563 norm:0.0002695377916097641 max memory_allocated 29234.240234375 
[2025-02-23 16:24:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 1 loss:0.187798410654068 norm:0.00019015926227439195 max memory_allocated 29234.240234375 
[2025-02-23 16:24:59 root] (abq_llm_calibration.py 358): INFO layer 27 iter 2 loss:0.18582266569137573 norm:0.00015108133084140718 max memory_allocated 29234.240234375 
[2025-02-23 16:25:48 root] (abq_llm_calibration.py 358): INFO layer 27 iter 3 loss:0.1844460815191269 norm:0.00012973230332136154 max memory_allocated 29234.240234375 
[2025-02-23 16:26:36 root] (abq_llm_calibration.py 358): INFO layer 27 iter 4 loss:0.18339473009109497 norm:0.00011213529069209471 max memory_allocated 29234.240234375 
[2025-02-23 16:27:25 root] (abq_llm_calibration.py 358): INFO layer 27 iter 5 loss:0.1825028955936432 norm:9.937021241057664e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:28:14 root] (abq_llm_calibration.py 358): INFO layer 27 iter 6 loss:0.18183787167072296 norm:9.023919847095385e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:29:03 root] (abq_llm_calibration.py 358): INFO layer 27 iter 7 loss:0.18130317330360413 norm:8.188752690330148e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:29:51 root] (abq_llm_calibration.py 358): INFO layer 27 iter 8 loss:0.18087418377399445 norm:7.509562419727445e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:30:40 root] (abq_llm_calibration.py 358): INFO layer 27 iter 9 loss:0.18049558997154236 norm:6.947013025637716e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:31:29 root] (abq_llm_calibration.py 358): INFO layer 27 iter 10 loss:0.1801740527153015 norm:6.351819320116192e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:32:17 root] (abq_llm_calibration.py 358): INFO layer 27 iter 11 loss:0.1799616664648056 norm:6.0246282373555005e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:33:06 root] (abq_llm_calibration.py 358): INFO layer 27 iter 12 loss:0.17975804209709167 norm:5.785750909126364e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:33:55 root] (abq_llm_calibration.py 358): INFO layer 27 iter 13 loss:0.1795957386493683 norm:5.495383811648935e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:34:43 root] (abq_llm_calibration.py 358): INFO layer 27 iter 14 loss:0.17946270108222961 norm:5.264462379273027e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:35:32 root] (abq_llm_calibration.py 358): INFO layer 27 iter 15 loss:0.17936722934246063 norm:5.152227095095441e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:36:21 root] (abq_llm_calibration.py 358): INFO layer 27 iter 16 loss:0.17927594482898712 norm:4.9764334107749164e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:37:10 root] (abq_llm_calibration.py 358): INFO layer 27 iter 17 loss:0.1791706383228302 norm:4.905117384623736e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:37:58 root] (abq_llm_calibration.py 358): INFO layer 27 iter 18 loss:0.17911297082901 norm:4.763917240779847e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:38:47 root] (abq_llm_calibration.py 358): INFO layer 27 iter 19 loss:0.179072767496109 norm:4.66431665699929e-05 max memory_allocated 29234.240234375 
[2025-02-23 16:39:02 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 28 ===
[2025-02-23 16:39:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 0 loss:0.21052153408527374 norm:0.0005233067204244435 max memory_allocated 29234.427734375 
[2025-02-23 16:40:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 1 loss:0.20682764053344727 norm:0.0003474620170891285 max memory_allocated 29234.427734375 
[2025-02-23 16:41:31 root] (abq_llm_calibration.py 358): INFO layer 28 iter 2 loss:0.20448338985443115 norm:0.0002635402197483927 max memory_allocated 29234.427734375 
[2025-02-23 16:42:20 root] (abq_llm_calibration.py 358): INFO layer 28 iter 3 loss:0.2028944343328476 norm:0.00022075448941905051 max memory_allocated 29234.427734375 
[2025-02-23 16:43:09 root] (abq_llm_calibration.py 358): INFO layer 28 iter 4 loss:0.20167911052703857 norm:0.0001884262601379305 max memory_allocated 29234.427734375 
[2025-02-23 16:43:58 root] (abq_llm_calibration.py 358): INFO layer 28 iter 5 loss:0.20074401795864105 norm:0.00016480499471072108 max memory_allocated 29234.427734375 
[2025-02-23 16:44:47 root] (abq_llm_calibration.py 358): INFO layer 28 iter 6 loss:0.20006245374679565 norm:0.00014752738934475929 max memory_allocated 29234.427734375 
[2025-02-23 16:45:35 root] (abq_llm_calibration.py 358): INFO layer 28 iter 7 loss:0.19945940375328064 norm:0.00013463910727296025 max memory_allocated 29234.427734375 
[2025-02-23 16:46:24 root] (abq_llm_calibration.py 358): INFO layer 28 iter 8 loss:0.1989656686782837 norm:0.00012076062557753175 max memory_allocated 29234.427734375 
[2025-02-23 16:47:13 root] (abq_llm_calibration.py 358): INFO layer 28 iter 9 loss:0.198600172996521 norm:0.00011268244998063892 max memory_allocated 29234.427734375 
[2025-02-23 16:48:02 root] (abq_llm_calibration.py 358): INFO layer 28 iter 10 loss:0.19826292991638184 norm:0.00010017167369369417 max memory_allocated 29234.427734375 
[2025-02-23 16:48:50 root] (abq_llm_calibration.py 358): INFO layer 28 iter 11 loss:0.1979956328868866 norm:9.613315341994166e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:49:39 root] (abq_llm_calibration.py 358): INFO layer 28 iter 12 loss:0.19776290655136108 norm:8.625948976259679e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:50:28 root] (abq_llm_calibration.py 358): INFO layer 28 iter 13 loss:0.19756968319416046 norm:8.286799857160076e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:51:17 root] (abq_llm_calibration.py 358): INFO layer 28 iter 14 loss:0.19740068912506104 norm:8.275888103526086e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:52:05 root] (abq_llm_calibration.py 358): INFO layer 28 iter 15 loss:0.1973733752965927 norm:7.817575533408672e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:52:54 root] (abq_llm_calibration.py 358): INFO layer 28 iter 16 loss:0.19728678464889526 norm:7.549682050012052e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:53:43 root] (abq_llm_calibration.py 358): INFO layer 28 iter 17 loss:0.19721998274326324 norm:7.4798270361498e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:54:32 root] (abq_llm_calibration.py 358): INFO layer 28 iter 18 loss:0.1971684992313385 norm:7.474231097148731e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:55:21 root] (abq_llm_calibration.py 358): INFO layer 28 iter 19 loss:0.19714564085006714 norm:7.303817255888134e-05 max memory_allocated 29234.427734375 
[2025-02-23 16:55:35 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 29 ===
[2025-02-23 16:56:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 0 loss:0.22920796275138855 norm:0.00026659996365197003 max memory_allocated 29234.615234375 
[2025-02-23 16:57:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 1 loss:0.22596850991249084 norm:0.0001910472201416269 max memory_allocated 29234.615234375 
[2025-02-23 16:58:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 2 loss:0.22387245297431946 norm:0.0001555608760099858 max memory_allocated 29234.615234375 
[2025-02-23 16:58:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 3 loss:0.22231483459472656 norm:0.0001328568469034508 max memory_allocated 29234.615234375 
[2025-02-23 16:59:43 root] (abq_llm_calibration.py 358): INFO layer 29 iter 4 loss:0.22109046578407288 norm:0.00011413180618546903 max memory_allocated 29234.615234375 
[2025-02-23 17:00:32 root] (abq_llm_calibration.py 358): INFO layer 29 iter 5 loss:0.2201513648033142 norm:0.00010286371980328113 max memory_allocated 29234.615234375 
[2025-02-23 17:01:20 root] (abq_llm_calibration.py 358): INFO layer 29 iter 6 loss:0.21940460801124573 norm:9.236001642420888e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:02:09 root] (abq_llm_calibration.py 358): INFO layer 29 iter 7 loss:0.21879537403583527 norm:8.239963790401816e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:02:58 root] (abq_llm_calibration.py 358): INFO layer 29 iter 8 loss:0.21830250322818756 norm:7.54034481360577e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:03:46 root] (abq_llm_calibration.py 358): INFO layer 29 iter 9 loss:0.21789106726646423 norm:7.12819310138002e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:04:36 root] (abq_llm_calibration.py 358): INFO layer 29 iter 10 loss:0.21754193305969238 norm:6.79762742947787e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:05:24 root] (abq_llm_calibration.py 358): INFO layer 29 iter 11 loss:0.21726717054843903 norm:6.438063428504393e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:06:13 root] (abq_llm_calibration.py 358): INFO layer 29 iter 12 loss:0.2170463651418686 norm:6.096098150010221e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:07:02 root] (abq_llm_calibration.py 358): INFO layer 29 iter 13 loss:0.2168557047843933 norm:5.766826507169753e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:07:50 root] (abq_llm_calibration.py 358): INFO layer 29 iter 14 loss:0.21668872237205505 norm:5.588389831245877e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:08:39 root] (abq_llm_calibration.py 358): INFO layer 29 iter 15 loss:0.2165636420249939 norm:5.404635157901794e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:09:28 root] (abq_llm_calibration.py 358): INFO layer 29 iter 16 loss:0.21644780039787292 norm:5.2049239457119256e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:10:17 root] (abq_llm_calibration.py 358): INFO layer 29 iter 17 loss:0.21636705100536346 norm:5.091341881779954e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:11:05 root] (abq_llm_calibration.py 358): INFO layer 29 iter 18 loss:0.21629483997821808 norm:5.001377212465741e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:11:54 root] (abq_llm_calibration.py 358): INFO layer 29 iter 19 loss:0.2162303775548935 norm:4.9296784709440544e-05 max memory_allocated 29234.615234375 
[2025-02-23 17:12:09 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 30 ===
[2025-02-23 17:13:01 root] (abq_llm_calibration.py 358): INFO layer 30 iter 0 loss:0.25365447998046875 norm:0.00047418283065780997 max memory_allocated 29234.802734375 
[2025-02-23 17:13:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 1 loss:0.2494649738073349 norm:0.00030335161136463284 max memory_allocated 29234.802734375 
[2025-02-23 17:14:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 2 loss:0.24685847759246826 norm:0.0002319349441677332 max memory_allocated 29234.802734375 
[2025-02-23 17:15:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 3 loss:0.24495267868041992 norm:0.00018537655705586076 max memory_allocated 29234.802734375 
[2025-02-23 17:16:16 root] (abq_llm_calibration.py 358): INFO layer 30 iter 4 loss:0.24355098605155945 norm:0.00015401709242723882 max memory_allocated 29234.802734375 
[2025-02-23 17:17:05 root] (abq_llm_calibration.py 358): INFO layer 30 iter 5 loss:0.2424609214067459 norm:0.0001324666664004326 max memory_allocated 29234.802734375 
[2025-02-23 17:17:54 root] (abq_llm_calibration.py 358): INFO layer 30 iter 6 loss:0.24159598350524902 norm:0.00011452759645180777 max memory_allocated 29234.802734375 
[2025-02-23 17:18:43 root] (abq_llm_calibration.py 358): INFO layer 30 iter 7 loss:0.24083003401756287 norm:0.00010374449630035087 max memory_allocated 29234.802734375 
[2025-02-23 17:19:31 root] (abq_llm_calibration.py 358): INFO layer 30 iter 8 loss:0.24021005630493164 norm:9.397078247275203e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:20:20 root] (abq_llm_calibration.py 358): INFO layer 30 iter 9 loss:0.2396903932094574 norm:8.572309889132157e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:21:09 root] (abq_llm_calibration.py 358): INFO layer 30 iter 10 loss:0.23927776515483856 norm:7.955688488436863e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:21:58 root] (abq_llm_calibration.py 358): INFO layer 30 iter 11 loss:0.23891833424568176 norm:7.430305413436145e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:22:47 root] (abq_llm_calibration.py 358): INFO layer 30 iter 12 loss:0.23867648839950562 norm:6.986339576542377e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:23:36 root] (abq_llm_calibration.py 358): INFO layer 30 iter 13 loss:0.23843897879123688 norm:6.571698031621054e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:24:24 root] (abq_llm_calibration.py 358): INFO layer 30 iter 14 loss:0.23821383714675903 norm:6.236061017261818e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:25:13 root] (abq_llm_calibration.py 358): INFO layer 30 iter 15 loss:0.23803390562534332 norm:5.938556569162756e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:26:02 root] (abq_llm_calibration.py 358): INFO layer 30 iter 16 loss:0.23788872361183167 norm:5.775670433649793e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:26:50 root] (abq_llm_calibration.py 358): INFO layer 30 iter 17 loss:0.23780235648155212 norm:5.6099881476256996e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:27:39 root] (abq_llm_calibration.py 358): INFO layer 30 iter 18 loss:0.23773005604743958 norm:5.535046147997491e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:28:28 root] (abq_llm_calibration.py 358): INFO layer 30 iter 19 loss:0.23766469955444336 norm:5.469682582770474e-05 max memory_allocated 29234.802734375 
[2025-02-23 17:28:42 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 31 ===
[2025-02-23 17:29:34 root] (abq_llm_calibration.py 358): INFO layer 31 iter 0 loss:0.27627137303352356 norm:0.00040070226532407105 max memory_allocated 29234.990234375 
[2025-02-23 17:30:23 root] (abq_llm_calibration.py 358): INFO layer 31 iter 1 loss:0.2722763419151306 norm:0.0002903405693359673 max memory_allocated 29234.990234375 
[2025-02-23 17:31:12 root] (abq_llm_calibration.py 358): INFO layer 31 iter 2 loss:0.26966845989227295 norm:0.00022994770552031696 max memory_allocated 29234.990234375 
[2025-02-23 17:32:01 root] (abq_llm_calibration.py 358): INFO layer 31 iter 3 loss:0.26776495575904846 norm:0.00018870274652726948 max memory_allocated 29234.990234375 
[2025-02-23 17:32:50 root] (abq_llm_calibration.py 358): INFO layer 31 iter 4 loss:0.26630258560180664 norm:0.00016104460519272834 max memory_allocated 29234.990234375 
[2025-02-23 17:33:39 root] (abq_llm_calibration.py 358): INFO layer 31 iter 5 loss:0.26519742608070374 norm:0.00013936354662291706 max memory_allocated 29234.990234375 
[2025-02-23 17:34:28 root] (abq_llm_calibration.py 358): INFO layer 31 iter 6 loss:0.26435959339141846 norm:0.00012157812307123095 max memory_allocated 29234.990234375 
[2025-02-23 17:35:16 root] (abq_llm_calibration.py 358): INFO layer 31 iter 7 loss:0.2636204957962036 norm:0.00010838617163244635 max memory_allocated 29234.990234375 
[2025-02-23 17:36:05 root] (abq_llm_calibration.py 358): INFO layer 31 iter 8 loss:0.26301145553588867 norm:9.839452104642987e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:36:54 root] (abq_llm_calibration.py 358): INFO layer 31 iter 9 loss:0.2625713348388672 norm:9.228252747561783e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:37:43 root] (abq_llm_calibration.py 358): INFO layer 31 iter 10 loss:0.2621552646160126 norm:8.627459465060383e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:38:32 root] (abq_llm_calibration.py 358): INFO layer 31 iter 11 loss:0.2617926597595215 norm:7.941345393192023e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:39:21 root] (abq_llm_calibration.py 358): INFO layer 31 iter 12 loss:0.26148492097854614 norm:7.568699220428243e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:40:09 root] (abq_llm_calibration.py 358): INFO layer 31 iter 13 loss:0.26125550270080566 norm:7.236406236188486e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:40:58 root] (abq_llm_calibration.py 358): INFO layer 31 iter 14 loss:0.26107257604599 norm:6.985507934587076e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:41:47 root] (abq_llm_calibration.py 358): INFO layer 31 iter 15 loss:0.2609193027019501 norm:6.752864283043891e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:42:36 root] (abq_llm_calibration.py 358): INFO layer 31 iter 16 loss:0.26080819964408875 norm:6.466737249866128e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:43:25 root] (abq_llm_calibration.py 358): INFO layer 31 iter 17 loss:0.26068219542503357 norm:6.34530806564726e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:44:13 root] (abq_llm_calibration.py 358): INFO layer 31 iter 18 loss:0.26057469844818115 norm:6.18121848674491e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:45:02 root] (abq_llm_calibration.py 358): INFO layer 31 iter 19 loss:0.2604907155036926 norm:6.04521956120152e-05 max memory_allocated 29234.990234375 
[2025-02-23 17:45:16 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 32 ===
[2025-02-23 17:46:09 root] (abq_llm_calibration.py 358): INFO layer 32 iter 0 loss:0.30428677797317505 norm:0.0005192772950977087 max memory_allocated 29235.177734375 
[2025-02-23 17:46:58 root] (abq_llm_calibration.py 358): INFO layer 32 iter 1 loss:0.29963093996047974 norm:0.000347029825206846 max memory_allocated 29235.177734375 
[2025-02-23 17:47:46 root] (abq_llm_calibration.py 358): INFO layer 32 iter 2 loss:0.29672229290008545 norm:0.0002683164202608168 max memory_allocated 29235.177734375 
[2025-02-23 17:48:35 root] (abq_llm_calibration.py 358): INFO layer 32 iter 3 loss:0.2945995628833771 norm:0.0002201391471317038 max memory_allocated 29235.177734375 
[2025-02-23 17:49:24 root] (abq_llm_calibration.py 358): INFO layer 32 iter 4 loss:0.2929995357990265 norm:0.0001844424259616062 max memory_allocated 29235.177734375 
[2025-02-23 17:50:13 root] (abq_llm_calibration.py 358): INFO layer 32 iter 5 loss:0.2917217016220093 norm:0.0001581928227096796 max memory_allocated 29235.177734375 
[2025-02-23 17:51:02 root] (abq_llm_calibration.py 358): INFO layer 32 iter 6 loss:0.2907834053039551 norm:0.00014194220420904458 max memory_allocated 29235.177734375 
[2025-02-23 17:51:51 root] (abq_llm_calibration.py 358): INFO layer 32 iter 7 loss:0.28997981548309326 norm:0.00012789589527528733 max memory_allocated 29235.177734375 
[2025-02-23 17:52:39 root] (abq_llm_calibration.py 358): INFO layer 32 iter 8 loss:0.2893033027648926 norm:0.000113106012577191 max memory_allocated 29235.177734375 
[2025-02-23 17:53:28 root] (abq_llm_calibration.py 358): INFO layer 32 iter 9 loss:0.28874680399894714 norm:9.97311290120706e-05 max memory_allocated 29235.177734375 
[2025-02-23 17:54:17 root] (abq_llm_calibration.py 358): INFO layer 32 iter 10 loss:0.28830036520957947 norm:9.077751019503921e-05 max memory_allocated 29235.177734375 
[2025-02-23 17:55:06 root] (abq_llm_calibration.py 358): INFO layer 32 iter 11 loss:0.28797462582588196 norm:8.524589065928012e-05 max memory_allocated 29235.177734375 
[2025-02-23 17:55:55 root] (abq_llm_calibration.py 358): INFO layer 32 iter 12 loss:0.2876666486263275 norm:7.92752398410812e-05 max memory_allocated 29235.177734375 
[2025-02-23 17:56:44 root] (abq_llm_calibration.py 358): INFO layer 32 iter 13 loss:0.28740713000297546 norm:7.409487443510443e-05 max memory_allocated 29235.177734375 
[2025-02-23 17:57:32 root] (abq_llm_calibration.py 358): INFO layer 32 iter 14 loss:0.28717872500419617 norm:7.0150890678633e-05 max memory_allocated 29235.177734375 
[2025-02-23 17:58:21 root] (abq_llm_calibration.py 358): INFO layer 32 iter 15 loss:0.28699731826782227 norm:6.66592241032049e-05 max memory_allocated 29235.177734375 
[2025-02-23 17:59:10 root] (abq_llm_calibration.py 358): INFO layer 32 iter 16 loss:0.2868347764015198 norm:6.337163358693942e-05 max memory_allocated 29235.177734375 
[2025-02-23 17:59:59 root] (abq_llm_calibration.py 358): INFO layer 32 iter 17 loss:0.28672438859939575 norm:6.190915883053094e-05 max memory_allocated 29235.177734375 
[2025-02-23 18:00:48 root] (abq_llm_calibration.py 358): INFO layer 32 iter 18 loss:0.28665071725845337 norm:6.035109981894493e-05 max memory_allocated 29235.177734375 
[2025-02-23 18:01:36 root] (abq_llm_calibration.py 358): INFO layer 32 iter 19 loss:0.28659942746162415 norm:5.9001671615988016e-05 max memory_allocated 29235.177734375 
[2025-02-23 18:01:51 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 33 ===
[2025-02-23 18:02:43 root] (abq_llm_calibration.py 358): INFO layer 33 iter 0 loss:0.3317996561527252 norm:0.00044604690629057586 max memory_allocated 29235.365234375 
[2025-02-23 18:03:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 1 loss:0.326956182718277 norm:0.00030154050909914076 max memory_allocated 29235.365234375 
[2025-02-23 18:04:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 2 loss:0.32392486929893494 norm:0.0002451147302053869 max memory_allocated 29235.365234375 
[2025-02-23 18:05:10 root] (abq_llm_calibration.py 358): INFO layer 33 iter 3 loss:0.3217554986476898 norm:0.00020357385801617056 max memory_allocated 29235.365234375 
[2025-02-23 18:05:58 root] (abq_llm_calibration.py 358): INFO layer 33 iter 4 loss:0.32011085748672485 norm:0.00017575184756424278 max memory_allocated 29235.365234375 
[2025-02-23 18:06:47 root] (abq_llm_calibration.py 358): INFO layer 33 iter 5 loss:0.3188890218734741 norm:0.00015684953541494906 max memory_allocated 29235.365234375 
[2025-02-23 18:07:36 root] (abq_llm_calibration.py 358): INFO layer 33 iter 6 loss:0.31784766912460327 norm:0.0001401635818183422 max memory_allocated 29235.365234375 
[2025-02-23 18:08:25 root] (abq_llm_calibration.py 358): INFO layer 33 iter 7 loss:0.31704846024513245 norm:0.00012857350520789623 max memory_allocated 29235.365234375 
[2025-02-23 18:09:13 root] (abq_llm_calibration.py 358): INFO layer 33 iter 8 loss:0.3163357377052307 norm:0.00012118929589632899 max memory_allocated 29235.365234375 
[2025-02-23 18:10:02 root] (abq_llm_calibration.py 358): INFO layer 33 iter 9 loss:0.3157387971878052 norm:0.00011302132770651951 max memory_allocated 29235.365234375 
[2025-02-23 18:10:51 root] (abq_llm_calibration.py 358): INFO layer 33 iter 10 loss:0.3152424693107605 norm:0.00010529403516557068 max memory_allocated 29235.365234375 
[2025-02-23 18:11:40 root] (abq_llm_calibration.py 358): INFO layer 33 iter 11 loss:0.31487736105918884 norm:9.947931539500132e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:12:28 root] (abq_llm_calibration.py 358): INFO layer 33 iter 12 loss:0.31457164883613586 norm:9.552397386869416e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:13:17 root] (abq_llm_calibration.py 358): INFO layer 33 iter 13 loss:0.31429389119148254 norm:9.229934948962182e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:14:06 root] (abq_llm_calibration.py 358): INFO layer 33 iter 14 loss:0.3140626549720764 norm:8.81891610333696e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:14:55 root] (abq_llm_calibration.py 358): INFO layer 33 iter 15 loss:0.3138406574726105 norm:8.530110062565655e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:15:44 root] (abq_llm_calibration.py 358): INFO layer 33 iter 16 loss:0.3136354386806488 norm:8.32214645924978e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:16:32 root] (abq_llm_calibration.py 358): INFO layer 33 iter 17 loss:0.31350091099739075 norm:8.058027742663398e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:17:21 root] (abq_llm_calibration.py 358): INFO layer 33 iter 18 loss:0.31339752674102783 norm:7.891302811913192e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:18:10 root] (abq_llm_calibration.py 358): INFO layer 33 iter 19 loss:0.31329014897346497 norm:7.685494347242638e-05 max memory_allocated 29235.365234375 
[2025-02-23 18:18:25 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 34 ===
[2025-02-23 18:19:17 root] (abq_llm_calibration.py 358): INFO layer 34 iter 0 loss:0.3669552803039551 norm:0.0005361448274925351 max memory_allocated 29235.552734375 
[2025-02-23 18:20:06 root] (abq_llm_calibration.py 358): INFO layer 34 iter 1 loss:0.36134815216064453 norm:0.0003432055818848312 max memory_allocated 29235.552734375 
[2025-02-23 18:20:55 root] (abq_llm_calibration.py 358): INFO layer 34 iter 2 loss:0.3578908145427704 norm:0.00026820035418495536 max memory_allocated 29235.552734375 
[2025-02-23 18:21:43 root] (abq_llm_calibration.py 358): INFO layer 34 iter 3 loss:0.35550200939178467 norm:0.00021852865756954998 max memory_allocated 29235.552734375 
[2025-02-23 18:22:32 root] (abq_llm_calibration.py 358): INFO layer 34 iter 4 loss:0.3537493944168091 norm:0.00018906238256022334 max memory_allocated 29235.552734375 
[2025-02-23 18:23:21 root] (abq_llm_calibration.py 358): INFO layer 34 iter 5 loss:0.3524385094642639 norm:0.00017032460891641676 max memory_allocated 29235.552734375 
[2025-02-23 18:24:10 root] (abq_llm_calibration.py 358): INFO layer 34 iter 6 loss:0.35132765769958496 norm:0.00015122817421797663 max memory_allocated 29235.552734375 
[2025-02-23 18:24:58 root] (abq_llm_calibration.py 358): INFO layer 34 iter 7 loss:0.35039520263671875 norm:0.00013382236647885293 max memory_allocated 29235.552734375 
[2025-02-23 18:25:47 root] (abq_llm_calibration.py 358): INFO layer 34 iter 8 loss:0.3495859205722809 norm:0.00012046400661347434 max memory_allocated 29235.552734375 
[2025-02-23 18:26:36 root] (abq_llm_calibration.py 358): INFO layer 34 iter 9 loss:0.3490006625652313 norm:0.00011066180013585836 max memory_allocated 29235.552734375 
[2025-02-23 18:27:25 root] (abq_llm_calibration.py 358): INFO layer 34 iter 10 loss:0.34854164719581604 norm:9.614532609703019e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:28:14 root] (abq_llm_calibration.py 358): INFO layer 34 iter 11 loss:0.34811723232269287 norm:9.00331069715321e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:29:03 root] (abq_llm_calibration.py 358): INFO layer 34 iter 12 loss:0.3478233814239502 norm:8.2317681517452e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:29:52 root] (abq_llm_calibration.py 358): INFO layer 34 iter 13 loss:0.34755393862724304 norm:7.622913108207285e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:30:40 root] (abq_llm_calibration.py 358): INFO layer 34 iter 14 loss:0.3472740650177002 norm:7.147344149416313e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:31:29 root] (abq_llm_calibration.py 358): INFO layer 34 iter 15 loss:0.34712034463882446 norm:8.020419772947207e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:32:18 root] (abq_llm_calibration.py 358): INFO layer 34 iter 16 loss:0.34697040915489197 norm:6.634683086303994e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:33:07 root] (abq_llm_calibration.py 358): INFO layer 34 iter 17 loss:0.3468354344367981 norm:6.413352093659341e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:33:55 root] (abq_llm_calibration.py 358): INFO layer 34 iter 18 loss:0.3467614948749542 norm:6.258761277422309e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:34:44 root] (abq_llm_calibration.py 358): INFO layer 34 iter 19 loss:0.34668827056884766 norm:6.105523789301515e-05 max memory_allocated 29235.552734375 
[2025-02-23 18:34:59 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 35 ===
[2025-02-23 18:35:51 root] (abq_llm_calibration.py 358): INFO layer 35 iter 0 loss:0.4025048017501831 norm:0.0003799710830207914 max memory_allocated 29235.740234375 
[2025-02-23 18:36:40 root] (abq_llm_calibration.py 358): INFO layer 35 iter 1 loss:0.3968874514102936 norm:0.0002719264302868396 max memory_allocated 29235.740234375 
[2025-02-23 18:37:28 root] (abq_llm_calibration.py 358): INFO layer 35 iter 2 loss:0.39351093769073486 norm:0.0002229341771453619 max memory_allocated 29235.740234375 
[2025-02-23 18:38:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 3 loss:0.39111173152923584 norm:0.00019225242431275547 max memory_allocated 29235.740234375 
[2025-02-23 18:39:06 root] (abq_llm_calibration.py 358): INFO layer 35 iter 4 loss:0.3892413377761841 norm:0.00016978733765427023 max memory_allocated 29235.740234375 
[2025-02-23 18:39:55 root] (abq_llm_calibration.py 358): INFO layer 35 iter 5 loss:0.3877463936805725 norm:0.00015134617569856346 max memory_allocated 29235.740234375 
[2025-02-23 18:40:44 root] (abq_llm_calibration.py 358): INFO layer 35 iter 6 loss:0.3866012394428253 norm:0.00013584602857008576 max memory_allocated 29235.740234375 
[2025-02-23 18:41:32 root] (abq_llm_calibration.py 358): INFO layer 35 iter 7 loss:0.38577964901924133 norm:0.00012404826702550054 max memory_allocated 29235.740234375 
[2025-02-23 18:42:21 root] (abq_llm_calibration.py 358): INFO layer 35 iter 8 loss:0.3850737512111664 norm:0.00011278856254648417 max memory_allocated 29235.740234375 
[2025-02-23 18:43:10 root] (abq_llm_calibration.py 358): INFO layer 35 iter 9 loss:0.3844512701034546 norm:0.00010360142186982557 max memory_allocated 29235.740234375 
[2025-02-23 18:43:58 root] (abq_llm_calibration.py 358): INFO layer 35 iter 10 loss:0.38393843173980713 norm:9.56067960942164e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:44:47 root] (abq_llm_calibration.py 358): INFO layer 35 iter 11 loss:0.38361814618110657 norm:9.14385964279063e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:45:36 root] (abq_llm_calibration.py 358): INFO layer 35 iter 12 loss:0.3833283483982086 norm:8.602107118349522e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:46:24 root] (abq_llm_calibration.py 358): INFO layer 35 iter 13 loss:0.38302451372146606 norm:8.174741378752515e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:47:13 root] (abq_llm_calibration.py 358): INFO layer 35 iter 14 loss:0.38277119398117065 norm:7.682063733227551e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:48:02 root] (abq_llm_calibration.py 358): INFO layer 35 iter 15 loss:0.38259410858154297 norm:7.379208545899019e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:48:50 root] (abq_llm_calibration.py 358): INFO layer 35 iter 16 loss:0.38250383734703064 norm:7.144843402784318e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:49:39 root] (abq_llm_calibration.py 358): INFO layer 35 iter 17 loss:0.382396936416626 norm:6.927225331310183e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:50:28 root] (abq_llm_calibration.py 358): INFO layer 35 iter 18 loss:0.382365345954895 norm:6.888270581839606e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:51:17 root] (abq_llm_calibration.py 358): INFO layer 35 iter 19 loss:0.3823329210281372 norm:6.851060607004911e-05 max memory_allocated 29235.740234375 
[2025-02-23 18:51:31 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 36 ===
[2025-02-23 18:52:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 0 loss:0.45489147305488586 norm:0.0010667769238352776 max memory_allocated 29235.927734375 
[2025-02-23 18:53:12 root] (abq_llm_calibration.py 358): INFO layer 36 iter 1 loss:0.4461452066898346 norm:0.0006651798030361533 max memory_allocated 29235.927734375 
[2025-02-23 18:54:01 root] (abq_llm_calibration.py 358): INFO layer 36 iter 2 loss:0.44122570753097534 norm:0.00048614252591505647 max memory_allocated 29235.927734375 
[2025-02-23 18:54:50 root] (abq_llm_calibration.py 358): INFO layer 36 iter 3 loss:0.4379155933856964 norm:0.000399277254473418 max memory_allocated 29235.927734375 
[2025-02-23 18:55:38 root] (abq_llm_calibration.py 358): INFO layer 36 iter 4 loss:0.4354899823665619 norm:0.00033726677065715194 max memory_allocated 29235.927734375 
[2025-02-23 18:56:27 root] (abq_llm_calibration.py 358): INFO layer 36 iter 5 loss:0.43366751074790955 norm:0.00028805999318137765 max memory_allocated 29235.927734375 
[2025-02-23 18:57:16 root] (abq_llm_calibration.py 358): INFO layer 36 iter 6 loss:0.4322092831134796 norm:0.0002574844693299383 max memory_allocated 29235.927734375 
[2025-02-23 18:58:05 root] (abq_llm_calibration.py 358): INFO layer 36 iter 7 loss:0.43105655908584595 norm:0.00022784211614634842 max memory_allocated 29235.927734375 
[2025-02-23 18:58:54 root] (abq_llm_calibration.py 358): INFO layer 36 iter 8 loss:0.4300726652145386 norm:0.0002036616060649976 max memory_allocated 29235.927734375 
[2025-02-23 18:59:42 root] (abq_llm_calibration.py 358): INFO layer 36 iter 9 loss:0.4292748272418976 norm:0.00018373550847172737 max memory_allocated 29235.927734375 
[2025-02-23 19:00:32 root] (abq_llm_calibration.py 358): INFO layer 36 iter 10 loss:0.4286750257015228 norm:0.0001667452888796106 max memory_allocated 29235.927734375 
[2025-02-23 19:01:20 root] (abq_llm_calibration.py 358): INFO layer 36 iter 11 loss:0.4281485974788666 norm:0.00014792216825298965 max memory_allocated 29235.927734375 
[2025-02-23 19:02:09 root] (abq_llm_calibration.py 358): INFO layer 36 iter 12 loss:0.4277651011943817 norm:0.00013242161367088556 max memory_allocated 29235.927734375 
[2025-02-23 19:02:58 root] (abq_llm_calibration.py 358): INFO layer 36 iter 13 loss:0.42747724056243896 norm:0.00012210351997055113 max memory_allocated 29235.927734375 
[2025-02-23 19:03:46 root] (abq_llm_calibration.py 358): INFO layer 36 iter 14 loss:0.4272196888923645 norm:0.00011335404997225851 max memory_allocated 29235.927734375 
[2025-02-23 19:04:35 root] (abq_llm_calibration.py 358): INFO layer 36 iter 15 loss:0.42696815729141235 norm:0.00010604463022900745 max memory_allocated 29235.927734375 
[2025-02-23 19:05:24 root] (abq_llm_calibration.py 358): INFO layer 36 iter 16 loss:0.426870197057724 norm:0.00010019067121902481 max memory_allocated 29235.927734375 
[2025-02-23 19:06:13 root] (abq_llm_calibration.py 358): INFO layer 36 iter 17 loss:0.42668983340263367 norm:9.801972919376567e-05 max memory_allocated 29235.927734375 
[2025-02-23 19:07:02 root] (abq_llm_calibration.py 358): INFO layer 36 iter 18 loss:0.4265369176864624 norm:9.453447273699567e-05 max memory_allocated 29235.927734375 
[2025-02-23 19:07:51 root] (abq_llm_calibration.py 358): INFO layer 36 iter 19 loss:0.4264068603515625 norm:9.23729530768469e-05 max memory_allocated 29235.927734375 
[2025-02-23 19:08:05 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 37 ===
[2025-02-23 19:08:58 root] (abq_llm_calibration.py 358): INFO layer 37 iter 0 loss:0.5046225190162659 norm:0.000920363119803369 max memory_allocated 29236.115234375 
[2025-02-23 19:09:47 root] (abq_llm_calibration.py 358): INFO layer 37 iter 1 loss:0.49589014053344727 norm:0.0006107537774369121 max memory_allocated 29236.115234375 
[2025-02-23 19:10:36 root] (abq_llm_calibration.py 358): INFO layer 37 iter 2 loss:0.49098604917526245 norm:0.00048528355546295643 max memory_allocated 29236.115234375 
[2025-02-23 19:11:25 root] (abq_llm_calibration.py 358): INFO layer 37 iter 3 loss:0.48761072754859924 norm:0.00041484017856419086 max memory_allocated 29236.115234375 
[2025-02-23 19:12:13 root] (abq_llm_calibration.py 358): INFO layer 37 iter 4 loss:0.4851543605327606 norm:0.0003616731846705079 max memory_allocated 29236.115234375 
[2025-02-23 19:13:02 root] (abq_llm_calibration.py 358): INFO layer 37 iter 5 loss:0.48333674669265747 norm:0.0003166923997923732 max memory_allocated 29236.115234375 
[2025-02-23 19:13:51 root] (abq_llm_calibration.py 358): INFO layer 37 iter 6 loss:0.48210304975509644 norm:0.00027810427127406 max memory_allocated 29236.115234375 
[2025-02-23 19:14:39 root] (abq_llm_calibration.py 358): INFO layer 37 iter 7 loss:0.48078858852386475 norm:0.00025013487902469933 max memory_allocated 29236.115234375 
[2025-02-23 19:15:28 root] (abq_llm_calibration.py 358): INFO layer 37 iter 8 loss:0.4798527657985687 norm:0.0002279885084135458 max memory_allocated 29236.115234375 
[2025-02-23 19:16:17 root] (abq_llm_calibration.py 358): INFO layer 37 iter 9 loss:0.4792577922344208 norm:0.00021902358275838196 max memory_allocated 29236.115234375 
[2025-02-23 19:17:06 root] (abq_llm_calibration.py 358): INFO layer 37 iter 10 loss:0.47867268323898315 norm:0.00019417700241319835 max memory_allocated 29236.115234375 
[2025-02-23 19:17:55 root] (abq_llm_calibration.py 358): INFO layer 37 iter 11 loss:0.4782743752002716 norm:0.000178631060407497 max memory_allocated 29236.115234375 
[2025-02-23 19:18:43 root] (abq_llm_calibration.py 358): INFO layer 37 iter 12 loss:0.47798359394073486 norm:0.00017066077271010727 max memory_allocated 29236.115234375 
[2025-02-23 19:19:32 root] (abq_llm_calibration.py 358): INFO layer 37 iter 13 loss:0.477687805891037 norm:0.00017122695862781256 max memory_allocated 29236.115234375 
[2025-02-23 19:20:21 root] (abq_llm_calibration.py 358): INFO layer 37 iter 14 loss:0.47734305262565613 norm:0.00016059023619163781 max memory_allocated 29236.115234375 
[2025-02-23 19:21:10 root] (abq_llm_calibration.py 358): INFO layer 37 iter 15 loss:0.4771403670310974 norm:0.000160673022037372 max memory_allocated 29236.115234375 
[2025-02-23 19:21:59 root] (abq_llm_calibration.py 358): INFO layer 37 iter 16 loss:0.4770026206970215 norm:0.00015760674432385713 max memory_allocated 29236.115234375 
[2025-02-23 19:22:48 root] (abq_llm_calibration.py 358): INFO layer 37 iter 17 loss:0.4768514633178711 norm:0.0001581693795742467 max memory_allocated 29236.115234375 
[2025-02-23 19:23:36 root] (abq_llm_calibration.py 358): INFO layer 37 iter 18 loss:0.47680234909057617 norm:0.00015346483269240707 max memory_allocated 29236.115234375 
[2025-02-23 19:24:25 root] (abq_llm_calibration.py 358): INFO layer 37 iter 19 loss:0.476812481880188 norm:0.00015472118684556335 max memory_allocated 29236.115234375 
[2025-02-23 19:24:40 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 38 ===
[2025-02-23 19:25:32 root] (abq_llm_calibration.py 358): INFO layer 38 iter 0 loss:0.5947412848472595 norm:0.0011871543247252703 max memory_allocated 29236.302734375 
[2025-02-23 19:26:21 root] (abq_llm_calibration.py 358): INFO layer 38 iter 1 loss:0.5816680788993835 norm:0.0008016733918339014 max memory_allocated 29236.302734375 
[2025-02-23 19:27:10 root] (abq_llm_calibration.py 358): INFO layer 38 iter 2 loss:0.5749696493148804 norm:0.0006590830744244158 max memory_allocated 29236.302734375 
[2025-02-23 19:27:59 root] (abq_llm_calibration.py 358): INFO layer 38 iter 3 loss:0.5703274607658386 norm:0.0006643926608376205 max memory_allocated 29236.302734375 
[2025-02-23 19:28:48 root] (abq_llm_calibration.py 358): INFO layer 38 iter 4 loss:0.5661965012550354 norm:0.0006629146519117057 max memory_allocated 29236.302734375 
[2025-02-23 19:29:37 root] (abq_llm_calibration.py 358): INFO layer 38 iter 5 loss:0.5636268258094788 norm:0.0006945927161723375 max memory_allocated 29236.302734375 
[2025-02-23 19:30:25 root] (abq_llm_calibration.py 358): INFO layer 38 iter 6 loss:0.5624564290046692 norm:0.0006508853984996676 max memory_allocated 29236.302734375 
[2025-02-23 19:31:14 root] (abq_llm_calibration.py 358): INFO layer 38 iter 7 loss:0.5613234043121338 norm:0.0006371577619574964 max memory_allocated 29236.302734375 
[2025-02-23 19:32:03 root] (abq_llm_calibration.py 358): INFO layer 38 iter 8 loss:0.5605263113975525 norm:0.0006644365494139493 max memory_allocated 29236.302734375 
[2025-02-23 19:32:52 root] (abq_llm_calibration.py 358): INFO layer 38 iter 9 loss:0.5598698854446411 norm:0.0006480112206190825 max memory_allocated 29236.302734375 
[2025-02-23 19:33:41 root] (abq_llm_calibration.py 358): INFO layer 38 iter 10 loss:0.5592920780181885 norm:0.000670151668600738 max memory_allocated 29236.302734375 
[2025-02-23 19:34:30 root] (abq_llm_calibration.py 358): INFO layer 38 iter 11 loss:0.5588861703872681 norm:0.0006627393886446953 max memory_allocated 29236.302734375 
[2025-02-23 19:35:18 root] (abq_llm_calibration.py 358): INFO layer 38 iter 12 loss:0.5588857531547546 norm:0.0006583493668586016 max memory_allocated 29236.302734375 
[2025-02-23 19:36:07 root] (abq_llm_calibration.py 358): INFO layer 38 iter 13 loss:0.55870121717453 norm:0.0008478381205350161 max memory_allocated 29236.302734375 
[2025-02-23 19:36:56 root] (abq_llm_calibration.py 358): INFO layer 38 iter 14 loss:0.5583975315093994 norm:0.0007720125140622258 max memory_allocated 29236.302734375 
[2025-02-23 19:37:45 root] (abq_llm_calibration.py 358): INFO layer 38 iter 15 loss:0.5582132339477539 norm:0.0007442311616614461 max memory_allocated 29236.302734375 
[2025-02-23 19:38:34 root] (abq_llm_calibration.py 358): INFO layer 38 iter 16 loss:0.558198869228363 norm:0.0008365540998056531 max memory_allocated 29236.302734375 
[2025-02-23 19:39:23 root] (abq_llm_calibration.py 358): INFO layer 38 iter 17 loss:0.5581017136573792 norm:0.0008023185073398054 max memory_allocated 29236.302734375 
[2025-02-23 19:40:11 root] (abq_llm_calibration.py 358): INFO layer 38 iter 18 loss:0.5581082701683044 norm:0.0008342452347278595 max memory_allocated 29236.302734375 
[2025-02-23 19:41:00 root] (abq_llm_calibration.py 358): INFO layer 38 iter 19 loss:0.5581985116004944 norm:0.0007835582364350557 max memory_allocated 29236.302734375 
[2025-02-23 19:41:14 root] (abq_llm_calibration.py 212): INFO === Start quantize layer 39 ===
[2025-02-23 19:42:07 root] (abq_llm_calibration.py 358): INFO layer 39 iter 0 loss:1.0847026109695435 norm:0.029690368101000786 max memory_allocated 29236.490234375 
[2025-02-23 19:42:56 root] (abq_llm_calibration.py 358): INFO layer 39 iter 1 loss:1.0032179355621338 norm:0.02194308489561081 max memory_allocated 29236.490234375 
[2025-02-23 19:43:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 2 loss:0.9762686491012573 norm:0.01674535498023033 max memory_allocated 29236.490234375 
[2025-02-23 19:44:33 root] (abq_llm_calibration.py 358): INFO layer 39 iter 3 loss:0.9873594045639038 norm:0.01943218894302845 max memory_allocated 29236.490234375 
[2025-02-23 19:45:22 root] (abq_llm_calibration.py 358): INFO layer 39 iter 4 loss:0.9678263664245605 norm:0.01741940900683403 max memory_allocated 29236.490234375 
[2025-02-23 19:46:11 root] (abq_llm_calibration.py 358): INFO layer 39 iter 5 loss:0.9266312718391418 norm:0.015282293781638145 max memory_allocated 29236.490234375 
[2025-02-23 19:47:00 root] (abq_llm_calibration.py 358): INFO layer 39 iter 6 loss:0.9546367526054382 norm:0.016513057053089142 max memory_allocated 29236.490234375 
[2025-02-23 19:47:48 root] (abq_llm_calibration.py 358): INFO layer 39 iter 7 loss:0.9255232810974121 norm:0.01496328879147768 max memory_allocated 29236.490234375 
[2025-02-23 19:48:37 root] (abq_llm_calibration.py 358): INFO layer 39 iter 8 loss:0.9320101737976074 norm:0.018452923744916916 max memory_allocated 29236.490234375 
[2025-02-23 19:49:26 root] (abq_llm_calibration.py 358): INFO layer 39 iter 9 loss:0.9379615783691406 norm:0.02438425086438656 max memory_allocated 29236.490234375 
[2025-02-23 19:50:14 root] (abq_llm_calibration.py 358): INFO layer 39 iter 10 loss:0.9160499572753906 norm:0.024333572015166283 max memory_allocated 29236.490234375 
[2025-02-23 19:51:03 root] (abq_llm_calibration.py 358): INFO layer 39 iter 11 loss:0.9056186676025391 norm:0.022454097867012024 max memory_allocated 29236.490234375 
[2025-02-23 19:51:52 root] (abq_llm_calibration.py 358): INFO layer 39 iter 12 loss:0.9005767703056335 norm:0.022487271577119827 max memory_allocated 29236.490234375 
[2025-02-23 19:52:41 root] (abq_llm_calibration.py 358): INFO layer 39 iter 13 loss:0.9012794494628906 norm:0.02342912182211876 max memory_allocated 29236.490234375 
[2025-02-23 19:53:30 root] (abq_llm_calibration.py 358): INFO layer 39 iter 14 loss:0.8895049095153809 norm:0.030012447386980057 max memory_allocated 29236.490234375 
[2025-02-23 19:54:19 root] (abq_llm_calibration.py 358): INFO layer 39 iter 15 loss:0.8954926133155823 norm:0.028167735785245895 max memory_allocated 29236.490234375 
[2025-02-23 19:55:08 root] (abq_llm_calibration.py 358): INFO layer 39 iter 16 loss:0.8870095014572144 norm:0.028949696570634842 max memory_allocated 29236.490234375 
[2025-02-23 19:55:57 root] (abq_llm_calibration.py 358): INFO layer 39 iter 17 loss:0.8912633657455444 norm:0.03167766332626343 max memory_allocated 29236.490234375 
[2025-02-23 19:56:45 root] (abq_llm_calibration.py 358): INFO layer 39 iter 18 loss:0.8862809538841248 norm:0.035003453493118286 max memory_allocated 29236.490234375 
[2025-02-23 19:57:34 root] (abq_llm_calibration.py 358): INFO layer 39 iter 19 loss:0.8857125043869019 norm:0.03873678296804428 max memory_allocated 29236.490234375 
[2025-02-23 19:57:49 root] (main_calibration.py 365): INFO 39806.26799798012
[2025-02-23 19:58:58 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-23 20:00:53 root] (main_calibration.py 158): INFO wikitext2 : 5.197153091430664
[2025-02-23 20:00:53 root] (main_calibration.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-23 20:03:52 root] (main_calibration.py 158): INFO c4 : 6.836655616760254
[2025-02-23 20:04:02 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/super_glue/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed (last modified on Tue Feb 18 02:57:49 2025) since it couldn't be found locally at super_glue., or remotely on the Hugging Face Hub.
[2025-02-23 22:05:17 root] (main_calibration.py 169): INFO {'wikitext2': 5.197153091430664, 'c4': 6.836655616760254, 'results': {'boolq': {'acc': 0.6605504587155964, 'acc_stderr': 0.008281960446071346}, 'hellaswag': {'acc': 0.5851424019119698, 'acc_stderr': 0.0049169050958108446, 'acc_norm': 0.7481577375024896, 'acc_norm_stderr': 0.0043318400127878446}, 'arc_challenge': {'acc': 0.4334470989761092, 'acc_stderr': 0.0144813762245589, 'acc_norm': 0.4308873720136519, 'acc_norm_stderr': 0.014471133392642471}, 'winogrande': {'acc': 0.675611681136543, 'acc_stderr': 0.013157225726641637}, 'piqa': {'acc': 0.7856365614798694, 'acc_stderr': 0.00957484213605097, 'acc_norm': 0.7894450489662677, 'acc_norm_stderr': 0.00951237808123875}, 'arc_easy': {'acc': 0.7323232323232324, 'acc_stderr': 0.009085000147099356, 'acc_norm': 0.5669191919191919, 'acc_norm_stderr': 0.010167478013701796}}, 'versions': {'boolq': 1, 'hellaswag': 0, 'arc_challenge': 0, 'winogrande': 0, 'piqa': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
