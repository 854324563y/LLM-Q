[2025-03-19 10:26:04 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide4-adaptive-calibration-attnloss/Llama-2-7b-hf-0.45', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide4-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.45.pkl', blocks_pkl='./log-divide4/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-19 10:30:24 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-19 10:30:24 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-19 10:30:24 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-19 10:30:24 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide4-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.45.pkl
[2025-03-19 10:30:24 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide4/Llama-2-7b-hf-w4a4/Llama-2-7b-hf_blocks.pkl: [(0, 2), (2, 6), (6, 10), (10, 14), (14, 18), (18, 22), (22, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-19 10:30:24 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0, 1], [2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12, 13], [14, 15, 16, 17], [18, 19, 20, 21], [22, 23, 24, 25], [26, 27, 28], [29], [30], [31]]
[2025-03-19 10:30:26 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0, 1] ===
[2025-03-19 10:30:26 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 10:30:26 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 10:31:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 0 loss:0.06308179348707199 norm:0.2020101696252823 max memory_allocated 40879.322265625 
[2025-03-19 10:32:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 1 loss:0.025285208597779274 norm:0.05135919526219368 max memory_allocated 40879.322265625 
[2025-03-19 10:33:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 2 loss:0.019263315945863724 norm:0.05030256509780884 max memory_allocated 40879.322265625 
[2025-03-19 10:34:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 3 loss:0.02264290675520897 norm:0.05793575942516327 max memory_allocated 40879.322265625 
[2025-03-19 10:34:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 4 loss:0.014790273271501064 norm:0.029030507430434227 max memory_allocated 40879.322265625 
[2025-03-19 10:35:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 5 loss:0.014168301597237587 norm:0.029488839209079742 max memory_allocated 40879.322265625 
[2025-03-19 10:36:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 6 loss:0.014064372517168522 norm:0.030876189470291138 max memory_allocated 40879.322265625 
[2025-03-19 10:37:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 7 loss:1.3734029531478882 norm:2.824424982070923 max memory_allocated 40879.322265625 
[2025-03-19 10:38:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 8 loss:0.6126657724380493 norm:0.9732860922813416 max memory_allocated 40879.322265625 
[2025-03-19 10:39:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 9 loss:0.14790405333042145 norm:0.2734605669975281 max memory_allocated 40879.322265625 
[2025-03-19 10:40:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 10 loss:0.06290693581104279 norm:0.14705994725227356 max memory_allocated 40879.322265625 
[2025-03-19 10:41:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 11 loss:0.040103551000356674 norm:0.08361329883337021 max memory_allocated 40879.322265625 
[2025-03-19 10:42:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 12 loss:0.03007606789469719 norm:0.05985280126333237 max memory_allocated 40879.322265625 
[2025-03-19 10:43:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 13 loss:0.02468177117407322 norm:0.046252720057964325 max memory_allocated 40879.322265625 
[2025-03-19 10:43:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 14 loss:0.021748049184679985 norm:0.041051119565963745 max memory_allocated 40879.322265625 
[2025-03-19 10:44:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 15 loss:0.01975942589342594 norm:0.03450567275285721 max memory_allocated 40879.322265625 
[2025-03-19 10:45:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 16 loss:0.018673136830329895 norm:0.02912754938006401 max memory_allocated 40879.322265625 
[2025-03-19 10:46:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 17 loss:0.01785891130566597 norm:0.028197094798088074 max memory_allocated 40879.322265625 
[2025-03-19 10:47:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 18 loss:0.016864966601133347 norm:0.024086657911539078 max memory_allocated 40879.322265625 
[2025-03-19 10:48:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 19 loss:0.016695411875844002 norm:0.02825041487812996 max memory_allocated 40879.322265625 
[2025-03-19 10:49:35 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0, 1]
[2025-03-19 10:49:37 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [2, 3, 4, 5] ===
[2025-03-19 10:49:38 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 10:51:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 0 loss:0.09988690912723541 norm:0.22945888340473175 max memory_allocated 53673.8955078125 
[2025-03-19 10:53:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 1 loss:0.06149766221642494 norm:0.025129718706011772 max memory_allocated 53673.8955078125 
[2025-03-19 10:55:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 2 loss:0.04832740128040314 norm:0.025663316249847412 max memory_allocated 53673.8955078125 
[2025-03-19 10:56:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 3 loss:0.04179912060499191 norm:0.024624669924378395 max memory_allocated 53673.8955078125 
[2025-03-19 10:58:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 4 loss:0.038043729960918427 norm:0.022753141820430756 max memory_allocated 53673.8955078125 
[2025-03-19 11:00:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 5 loss:0.03569968789815903 norm:0.0218686293810606 max memory_allocated 53673.8955078125 
[2025-03-19 11:02:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 6 loss:0.034170858561992645 norm:0.021111229434609413 max memory_allocated 53673.8955078125 
[2025-03-19 11:03:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 7 loss:0.03300153464078903 norm:0.01821546070277691 max memory_allocated 53673.8955078125 
[2025-03-19 11:05:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 8 loss:0.03234386816620827 norm:0.017543716356158257 max memory_allocated 53673.8955078125 
[2025-03-19 11:07:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 9 loss:0.03184380382299423 norm:0.0158527884632349 max memory_allocated 53673.8955078125 
[2025-03-19 11:09:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 10 loss:0.03133891895413399 norm:0.015112562105059624 max memory_allocated 53673.8955078125 
[2025-03-19 11:11:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 11 loss:0.03107353486120701 norm:0.014228124171495438 max memory_allocated 53673.8955078125 
[2025-03-19 11:12:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 12 loss:0.03080132231116295 norm:0.013715730048716068 max memory_allocated 53673.8955078125 
[2025-03-19 11:14:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 13 loss:0.030603114515542984 norm:0.013411195948719978 max memory_allocated 53673.8955078125 
[2025-03-19 11:16:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 14 loss:0.030491050332784653 norm:0.012212840840220451 max memory_allocated 53673.8955078125 
[2025-03-19 11:18:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 15 loss:0.030250802636146545 norm:0.011205791495740414 max memory_allocated 53673.8955078125 
[2025-03-19 11:19:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 16 loss:0.03008994087576866 norm:0.010936573147773743 max memory_allocated 53673.8955078125 
[2025-03-19 11:21:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 17 loss:0.03004135750234127 norm:0.010367752984166145 max memory_allocated 53673.8955078125 
[2025-03-19 11:23:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 18 loss:0.029879719018936157 norm:0.009734854102134705 max memory_allocated 53673.8955078125 
[2025-03-19 11:25:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 19 loss:0.029739849269390106 norm:0.009527635760605335 max memory_allocated 53673.8955078125 
[2025-03-19 11:27:40 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [2, 3, 4, 5]
[2025-03-19 11:27:40 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [6, 7, 8, 9] ===
[2025-03-19 11:29:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 0 loss:0.10437288135290146 norm:0.0025761364959180355 max memory_allocated 53674.1455078125 
[2025-03-19 11:31:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 1 loss:0.07742857933044434 norm:0.0010248796315863729 max memory_allocated 53674.1455078125 
[2025-03-19 11:33:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 2 loss:0.06139658764004707 norm:0.0005952274077571929 max memory_allocated 53674.1455078125 
[2025-03-19 11:34:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 3 loss:0.05324963480234146 norm:0.00046726770233362913 max memory_allocated 53674.1455078125 
[2025-03-19 11:36:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 4 loss:0.049106959253549576 norm:0.0004191554617136717 max memory_allocated 53674.1455078125 
[2025-03-19 11:38:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 5 loss:0.04645225405693054 norm:0.00041320163290947676 max memory_allocated 53674.1455078125 
[2025-03-19 11:40:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 6 loss:0.04473648965358734 norm:0.00039207402733154595 max memory_allocated 53674.1455078125 
[2025-03-19 11:42:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 7 loss:0.04353373125195503 norm:0.00038435947499237955 max memory_allocated 53674.1455078125 
[2025-03-19 11:43:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 8 loss:0.042609695345163345 norm:0.0003897125134244561 max memory_allocated 53674.1455078125 
[2025-03-19 11:45:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 9 loss:0.042110927402973175 norm:0.0004001869820058346 max memory_allocated 53674.1455078125 
[2025-03-19 11:47:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 10 loss:0.0416545607149601 norm:0.00038225893513299525 max memory_allocated 53674.1455078125 
[2025-03-19 11:49:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 11 loss:0.04133477061986923 norm:0.00040379283018410206 max memory_allocated 53674.1455078125 
[2025-03-19 11:50:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 12 loss:0.041022274643182755 norm:0.00037984579103067517 max memory_allocated 53674.1455078125 
[2025-03-19 11:52:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 13 loss:0.04083404317498207 norm:0.00040877037099562585 max memory_allocated 53674.1455078125 
[2025-03-19 11:54:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 14 loss:0.04070571810007095 norm:0.00040619095670990646 max memory_allocated 53674.1455078125 
[2025-03-19 11:56:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 15 loss:0.04054246097803116 norm:0.00040274843922816217 max memory_allocated 53674.1455078125 
[2025-03-19 11:58:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 16 loss:0.04032585769891739 norm:0.00038664299063384533 max memory_allocated 53674.1455078125 
[2025-03-19 11:59:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 17 loss:0.04031090810894966 norm:0.000425604433985427 max memory_allocated 53674.1455078125 
[2025-03-19 12:01:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 18 loss:0.04024355486035347 norm:0.00038488616701215506 max memory_allocated 53674.1455078125 
[2025-03-19 12:03:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 19 loss:0.040139760822057724 norm:0.0003763196582440287 max memory_allocated 53674.1455078125 
[2025-03-19 12:05:48 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [6, 7, 8, 9]
[2025-03-19 12:05:48 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [10, 11, 12, 13] ===
[2025-03-19 12:07:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 0 loss:0.10095953196287155 norm:0.001961259637027979 max memory_allocated 53674.3955078125 
[2025-03-19 12:09:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 1 loss:0.07855632901191711 norm:0.0009486109483987093 max memory_allocated 53674.3955078125 
[2025-03-19 12:11:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 2 loss:0.06313901394605637 norm:0.0005623153992928565 max memory_allocated 53674.3955078125 
[2025-03-19 12:13:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 3 loss:0.055204927921295166 norm:0.0004034820303786546 max memory_allocated 53674.3955078125 
[2025-03-19 12:14:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 4 loss:0.051065146923065186 norm:0.00033074611565098166 max memory_allocated 53674.3955078125 
[2025-03-19 12:16:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 5 loss:0.04852709546685219 norm:0.00029622085276059806 max memory_allocated 53674.3955078125 
[2025-03-19 12:18:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 6 loss:0.046973247081041336 norm:0.0002793734602164477 max memory_allocated 53674.3955078125 
[2025-03-19 12:20:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 7 loss:0.04594473913311958 norm:0.0002661778125911951 max memory_allocated 53674.3955078125 
[2025-03-19 12:21:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 8 loss:0.04520720615983009 norm:0.00026621491997502744 max memory_allocated 53674.3955078125 
[2025-03-19 12:23:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 9 loss:0.044663116335868835 norm:0.0002539791166782379 max memory_allocated 53674.3955078125 
[2025-03-19 12:25:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 10 loss:0.044328488409519196 norm:0.00026662854361347854 max memory_allocated 53674.3955078125 
[2025-03-19 12:27:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 11 loss:0.04400763288140297 norm:0.0002555284881964326 max memory_allocated 53674.3955078125 
[2025-03-19 12:29:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 12 loss:0.04380904883146286 norm:0.0002575819962657988 max memory_allocated 53674.3955078125 
[2025-03-19 12:30:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 13 loss:0.043675053864717484 norm:0.00025628245202824473 max memory_allocated 53674.3955078125 
[2025-03-19 12:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 14 loss:0.04349173232913017 norm:0.0002484802098479122 max memory_allocated 53674.3955078125 
[2025-03-19 12:34:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 15 loss:0.04337073862552643 norm:0.00024846039013937116 max memory_allocated 53674.3955078125 
[2025-03-19 12:36:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 16 loss:0.043288446962833405 norm:0.00024978554574772716 max memory_allocated 53674.3955078125 
[2025-03-19 12:37:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 17 loss:0.04318070784211159 norm:0.000244459806708619 max memory_allocated 53674.3955078125 
[2025-03-19 12:39:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 18 loss:0.043110642582178116 norm:0.00024885288439691067 max memory_allocated 53674.3955078125 
[2025-03-19 12:41:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 19 loss:0.042992547154426575 norm:0.0002487434248905629 max memory_allocated 53674.3955078125 
[2025-03-19 12:43:55 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [10, 11, 12, 13]
[2025-03-19 12:43:56 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [14, 15, 16, 17] ===
[2025-03-19 12:45:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 0 loss:0.09187081456184387 norm:0.0014431364834308624 max memory_allocated 53674.6455078125 
[2025-03-19 12:47:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 1 loss:0.07335040718317032 norm:0.0005804975517094135 max memory_allocated 53674.6455078125 
[2025-03-19 12:49:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 2 loss:0.05943111330270767 norm:0.0003466957132332027 max memory_allocated 53674.6455078125 
[2025-03-19 12:51:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 3 loss:0.05352891981601715 norm:0.0002772811276372522 max memory_allocated 53674.6455078125 
[2025-03-19 12:52:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 4 loss:0.0503738708794117 norm:0.00023798589245416224 max memory_allocated 53674.6455078125 
[2025-03-19 12:54:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 5 loss:0.048378169536590576 norm:0.00022153054305817932 max memory_allocated 53674.6455078125 
[2025-03-19 12:56:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 6 loss:0.0470857173204422 norm:0.00021421480050776154 max memory_allocated 53674.6455078125 
[2025-03-19 12:58:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 7 loss:0.04620538279414177 norm:0.0002157658600481227 max memory_allocated 53674.6455078125 
[2025-03-19 13:00:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 8 loss:0.04554584622383118 norm:0.00020061436225660145 max memory_allocated 53674.6455078125 
[2025-03-19 13:01:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 9 loss:0.04506118223071098 norm:0.0001972214668057859 max memory_allocated 53674.6455078125 
[2025-03-19 13:03:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 10 loss:0.04471978545188904 norm:0.00018954294500872493 max memory_allocated 53674.6455078125 
[2025-03-19 13:05:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 11 loss:0.04444184899330139 norm:0.00018789395107887685 max memory_allocated 53674.6455078125 
[2025-03-19 13:07:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 12 loss:0.044224005192518234 norm:0.00018777025979943573 max memory_allocated 53674.6455078125 
[2025-03-19 13:08:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 13 loss:0.04402142018079758 norm:0.00018560091848485172 max memory_allocated 53674.6455078125 
[2025-03-19 13:10:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 14 loss:0.04386650770902634 norm:0.0001822980266297236 max memory_allocated 53674.6455078125 
[2025-03-19 13:12:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 15 loss:0.04377657175064087 norm:0.00018075815751217306 max memory_allocated 53674.6455078125 
[2025-03-19 13:14:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 16 loss:0.04371045157313347 norm:0.00018261650984641165 max memory_allocated 53674.6455078125 
[2025-03-19 13:16:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 17 loss:0.043614208698272705 norm:0.0001790786045603454 max memory_allocated 53674.6455078125 
[2025-03-19 13:17:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 18 loss:0.04351780563592911 norm:0.00017747731180861592 max memory_allocated 53674.6455078125 
[2025-03-19 13:19:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15, 16, 17]) iter 19 loss:0.04343290999531746 norm:0.0001780131133273244 max memory_allocated 53674.6455078125 
[2025-03-19 13:22:03 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [14, 15, 16, 17]
[2025-03-19 13:22:03 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [18, 19, 20, 21] ===
[2025-03-19 13:24:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 0 loss:0.12208649516105652 norm:0.0032478105276823044 max memory_allocated 53674.8955078125 
[2025-03-19 13:25:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 1 loss:0.09741612523794174 norm:0.00092012231471017 max memory_allocated 53674.8955078125 
[2025-03-19 13:27:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 2 loss:0.0788646787405014 norm:0.0004435261944308877 max memory_allocated 53674.8955078125 
[2025-03-19 13:29:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 3 loss:0.07214687019586563 norm:0.0003208090492989868 max memory_allocated 53674.8955078125 
[2025-03-19 13:31:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 4 loss:0.06845421344041824 norm:0.0002906257868744433 max memory_allocated 53674.8955078125 
[2025-03-19 13:32:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 5 loss:0.06575369089841843 norm:0.000270848540822044 max memory_allocated 53674.8955078125 
[2025-03-19 13:34:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 6 loss:0.06407630443572998 norm:0.0002618725411593914 max memory_allocated 53674.8955078125 
[2025-03-19 13:36:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 7 loss:0.06305958330631256 norm:0.0002537237305659801 max memory_allocated 53674.8955078125 
[2025-03-19 13:38:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 8 loss:0.06238030642271042 norm:0.00024342404503840953 max memory_allocated 53674.8955078125 
[2025-03-19 13:39:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 9 loss:0.06186723709106445 norm:0.0002398753131274134 max memory_allocated 53674.8955078125 
[2025-03-19 13:41:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 10 loss:0.06148183345794678 norm:0.00023645276087336242 max memory_allocated 53674.8955078125 
[2025-03-19 13:43:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 11 loss:0.06114748865365982 norm:0.00023097367375157773 max memory_allocated 53674.8955078125 
[2025-03-19 13:45:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 12 loss:0.060854554176330566 norm:0.0002232545957667753 max memory_allocated 53674.8955078125 
[2025-03-19 13:47:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 13 loss:0.06057533249258995 norm:0.00021921847655903548 max memory_allocated 53674.8955078125 
[2025-03-19 13:48:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 14 loss:0.060341544449329376 norm:0.0002166118356399238 max memory_allocated 53674.8955078125 
[2025-03-19 13:50:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 15 loss:0.06015855073928833 norm:0.0002138133131666109 max memory_allocated 53674.8955078125 
[2025-03-19 13:52:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 16 loss:0.059993844479322433 norm:0.00021050810755696148 max memory_allocated 53674.8955078125 
[2025-03-19 13:54:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 17 loss:0.05982787162065506 norm:0.0002034255157923326 max memory_allocated 53674.8955078125 
[2025-03-19 13:56:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 18 loss:0.05966602638363838 norm:0.0002035901416093111 max memory_allocated 53674.8955078125 
[2025-03-19 13:57:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [18, 19, 20, 21]) iter 19 loss:0.05951058864593506 norm:0.00020190246868878603 max memory_allocated 53674.8955078125 
[2025-03-19 14:00:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [18, 19, 20, 21]
[2025-03-19 14:00:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [22, 23, 24, 25] ===
[2025-03-19 14:02:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 0 loss:0.1873554289340973 norm:0.005193909630179405 max memory_allocated 53675.1455078125 
[2025-03-19 14:03:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 1 loss:0.14870594441890717 norm:0.0009765051072463393 max memory_allocated 53675.1455078125 
[2025-03-19 14:05:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 2 loss:0.12154656648635864 norm:0.0005319876945577562 max memory_allocated 53675.1455078125 
[2025-03-19 14:07:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 3 loss:0.11240329593420029 norm:0.00041748530929908156 max memory_allocated 53675.1455078125 
[2025-03-19 14:09:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 4 loss:0.10714537650346756 norm:0.00037930032704025507 max memory_allocated 53675.1455078125 
[2025-03-19 14:10:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 5 loss:0.1041787713766098 norm:0.00036698856274597347 max memory_allocated 53675.1455078125 
[2025-03-19 14:12:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 6 loss:0.10261507332324982 norm:0.00033884134609252214 max memory_allocated 53675.1455078125 
[2025-03-19 14:14:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 7 loss:0.10162600129842758 norm:0.0003356213856022805 max memory_allocated 53675.1455078125 
[2025-03-19 14:16:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 8 loss:0.1009240448474884 norm:0.00033697273465804756 max memory_allocated 53675.1455078125 
[2025-03-19 14:18:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 9 loss:0.10034739971160889 norm:0.00031183395185507834 max memory_allocated 53675.1455078125 
[2025-03-19 14:19:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 10 loss:0.09977637231349945 norm:0.000308564689476043 max memory_allocated 53675.1455078125 
[2025-03-19 14:21:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 11 loss:0.09930824488401413 norm:0.0003075134300161153 max memory_allocated 53675.1455078125 
[2025-03-19 14:23:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 12 loss:0.09885911643505096 norm:0.000314293458359316 max memory_allocated 53675.1455078125 
[2025-03-19 14:25:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 13 loss:0.09844602644443512 norm:0.0002866325667127967 max memory_allocated 53675.1455078125 
[2025-03-19 14:26:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 14 loss:0.09813014417886734 norm:0.000295657548122108 max memory_allocated 53675.1455078125 
[2025-03-19 14:28:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 15 loss:0.09779642522335052 norm:0.0002912249183282256 max memory_allocated 53675.1455078125 
[2025-03-19 14:30:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 16 loss:0.09754225611686707 norm:0.00028622712125070393 max memory_allocated 53675.1455078125 
[2025-03-19 14:32:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 17 loss:0.09731438010931015 norm:0.0002899898972827941 max memory_allocated 53675.1455078125 
[2025-03-19 14:34:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 18 loss:0.0970909520983696 norm:0.00028327354812063277 max memory_allocated 53675.1455078125 
[2025-03-19 14:35:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [22, 23, 24, 25]) iter 19 loss:0.0969209223985672 norm:0.0002697948075365275 max memory_allocated 53675.1455078125 
[2025-03-19 14:38:07 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [22, 23, 24, 25]
[2025-03-19 14:38:07 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [26, 27, 28] ===
[2025-03-19 14:38:08 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 14:39:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 0 loss:0.20876507461071014 norm:0.016293048858642578 max memory_allocated 53675.1455078125 
[2025-03-19 14:40:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 1 loss:0.18096484243869781 norm:0.011434596963226795 max memory_allocated 53675.1455078125 
[2025-03-19 14:42:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 2 loss:0.15597692131996155 norm:0.0071254391223192215 max memory_allocated 53675.1455078125 
[2025-03-19 14:43:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 3 loss:0.14771509170532227 norm:0.005807653069496155 max memory_allocated 53675.1455078125 
[2025-03-19 14:44:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 4 loss:0.14385366439819336 norm:0.004864887334406376 max memory_allocated 53675.1455078125 
[2025-03-19 14:46:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 5 loss:0.14218780398368835 norm:0.004164596553891897 max memory_allocated 53675.1455078125 
[2025-03-19 14:47:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 6 loss:0.1411488801240921 norm:0.0035891488660126925 max memory_allocated 53675.1455078125 
[2025-03-19 14:48:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 7 loss:0.140272319316864 norm:0.0030031560454517603 max memory_allocated 53675.1455078125 
[2025-03-19 14:50:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 8 loss:0.13960470259189606 norm:0.002770599676296115 max memory_allocated 53675.1455078125 
[2025-03-19 14:51:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 9 loss:0.13910163938999176 norm:0.0028079620096832514 max memory_allocated 53675.1455078125 
[2025-03-19 14:52:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 10 loss:0.13869260251522064 norm:0.0028455567080527544 max memory_allocated 53675.1455078125 
[2025-03-19 14:54:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 11 loss:0.13818588852882385 norm:0.0024644052609801292 max memory_allocated 53675.1455078125 
[2025-03-19 14:55:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 12 loss:0.13780896365642548 norm:0.002219423418864608 max memory_allocated 53675.1455078125 
[2025-03-19 14:56:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 13 loss:0.1374930590391159 norm:0.0023545571602880955 max memory_allocated 53675.1455078125 
[2025-03-19 14:58:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 14 loss:0.13724547624588013 norm:0.0023635716643184423 max memory_allocated 53675.1455078125 
[2025-03-19 14:59:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 15 loss:0.1370847523212433 norm:0.0024456228129565716 max memory_allocated 53675.1455078125 
[2025-03-19 15:00:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 16 loss:0.1367940753698349 norm:0.002326696179807186 max memory_allocated 53675.1455078125 
[2025-03-19 15:02:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 17 loss:0.1365615278482437 norm:0.0021499108988791704 max memory_allocated 53675.1455078125 
[2025-03-19 15:03:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 18 loss:0.13633021712303162 norm:0.002078922698274255 max memory_allocated 53675.1455078125 
[2025-03-19 15:04:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [26, 27, 28]) iter 19 loss:0.13614265620708466 norm:0.0019525984534993768 max memory_allocated 53675.1455078125 
[2025-03-19 15:06:44 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [26, 27, 28]
[2025-03-19 15:06:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [29] ===
[2025-03-19 15:06:44 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 15:07:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 0 loss:0.17579057812690735 norm:0.011587968096137047 max memory_allocated 53675.1455078125 
[2025-03-19 15:07:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 1 loss:0.16233627498149872 norm:0.008862484246492386 max memory_allocated 53675.1455078125 
[2025-03-19 15:08:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 2 loss:0.15189538896083832 norm:0.0054522245191037655 max memory_allocated 53675.1455078125 
[2025-03-19 15:08:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 3 loss:0.1487215906381607 norm:0.004556525032967329 max memory_allocated 53675.1455078125 
[2025-03-19 15:09:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 4 loss:0.14765799045562744 norm:0.0038762902840971947 max memory_allocated 53675.1455078125 
[2025-03-19 15:09:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 5 loss:0.14713101089000702 norm:0.003323002252727747 max memory_allocated 53675.1455078125 
[2025-03-19 15:09:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 6 loss:0.14674051105976105 norm:0.0028073361609131098 max memory_allocated 53675.1455078125 
[2025-03-19 15:10:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 7 loss:0.14643177390098572 norm:0.0024641782511025667 max memory_allocated 53675.1455078125 
[2025-03-19 15:10:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 8 loss:0.14624252915382385 norm:0.002411680528894067 max memory_allocated 53675.1455078125 
[2025-03-19 15:11:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 9 loss:0.14614127576351166 norm:0.0024546266067773104 max memory_allocated 53675.1455078125 
[2025-03-19 15:11:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 10 loss:0.14593884348869324 norm:0.002325798152014613 max memory_allocated 53675.1455078125 
[2025-03-19 15:12:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 11 loss:0.14576655626296997 norm:0.002083990490064025 max memory_allocated 53675.1455078125 
[2025-03-19 15:12:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 12 loss:0.14566129446029663 norm:0.0021283712703734636 max memory_allocated 53675.1455078125 
[2025-03-19 15:13:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 13 loss:0.14561031758785248 norm:0.0020823669619858265 max memory_allocated 53675.1455078125 
[2025-03-19 15:13:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 14 loss:0.145517498254776 norm:0.002095717005431652 max memory_allocated 53675.1455078125 
[2025-03-19 15:13:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 15 loss:0.14545521140098572 norm:0.001958835171535611 max memory_allocated 53675.1455078125 
[2025-03-19 15:14:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 16 loss:0.1453266143798828 norm:0.0019452389096841216 max memory_allocated 53675.1455078125 
[2025-03-19 15:14:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 17 loss:0.1452367901802063 norm:0.0018168039387091994 max memory_allocated 53675.1455078125 
[2025-03-19 15:15:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 18 loss:0.1451410949230194 norm:0.0018188739195466042 max memory_allocated 53675.1455078125 
[2025-03-19 15:15:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [29]) iter 19 loss:0.1450798660516739 norm:0.0017544296570122242 max memory_allocated 53675.1455078125 
[2025-03-19 15:16:23 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [29]
[2025-03-19 15:16:23 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [30] ===
[2025-03-19 15:16:23 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 15:16:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 0 loss:0.2519375681877136 norm:0.030573347583413124 max memory_allocated 53675.1455078125 
[2025-03-19 15:17:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 1 loss:0.21875500679016113 norm:0.01991359144449234 max memory_allocated 53675.1455078125 
[2025-03-19 15:17:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 2 loss:0.198554128408432 norm:0.012031454592943192 max memory_allocated 53675.1455078125 
[2025-03-19 15:18:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 3 loss:0.19386053085327148 norm:0.010505232028663158 max memory_allocated 53675.1455078125 
[2025-03-19 15:18:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 4 loss:0.19234097003936768 norm:0.010208316147327423 max memory_allocated 53675.1455078125 
[2025-03-19 15:19:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 5 loss:0.1919516623020172 norm:0.009377360343933105 max memory_allocated 53675.1455078125 
[2025-03-19 15:19:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 6 loss:0.19110378623008728 norm:0.009207149967551231 max memory_allocated 53675.1455078125 
[2025-03-19 15:20:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 7 loss:0.1905200481414795 norm:0.008556797169148922 max memory_allocated 53675.1455078125 
[2025-03-19 15:20:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 8 loss:0.18997277319431305 norm:0.008258777670562267 max memory_allocated 53675.1455078125 
[2025-03-19 15:20:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 9 loss:0.1904768943786621 norm:0.008383451029658318 max memory_allocated 53675.1455078125 
[2025-03-19 15:21:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 10 loss:0.18946638703346252 norm:0.007589625660330057 max memory_allocated 53675.1455078125 
[2025-03-19 15:21:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 11 loss:0.18941891193389893 norm:0.007447351701557636 max memory_allocated 53675.1455078125 
[2025-03-19 15:22:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 12 loss:0.189202681183815 norm:0.007380175404250622 max memory_allocated 53675.1455078125 
[2025-03-19 15:22:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 13 loss:0.18943120539188385 norm:0.006985804066061974 max memory_allocated 53675.1455078125 
[2025-03-19 15:23:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 14 loss:0.18876267969608307 norm:0.006493701599538326 max memory_allocated 53675.1455078125 
[2025-03-19 15:23:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 15 loss:0.1891871839761734 norm:0.006907674949616194 max memory_allocated 53675.1455078125 
[2025-03-19 15:24:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 16 loss:0.18897008895874023 norm:0.006762924138456583 max memory_allocated 53675.1455078125 
[2025-03-19 15:24:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 17 loss:0.18929360806941986 norm:0.006986820138990879 max memory_allocated 53675.1455078125 
[2025-03-19 15:25:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 18 loss:0.18866179883480072 norm:0.006349491886794567 max memory_allocated 53675.1455078125 
[2025-03-19 15:25:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [30]) iter 19 loss:0.18950998783111572 norm:0.007005659863352776 max memory_allocated 53675.1455078125 
[2025-03-19 15:26:02 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [30]
[2025-03-19 15:26:03 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [31] ===
[2025-03-19 15:26:03 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 15:26:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 0 loss:0.44096794724464417 norm:0.061334408819675446 max memory_allocated 53675.1455078125 
[2025-03-19 15:26:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 1 loss:0.38557136058807373 norm:0.04323265701532364 max memory_allocated 53675.1455078125 
[2025-03-19 15:27:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 2 loss:0.3442752957344055 norm:0.026804346591234207 max memory_allocated 53675.1455078125 
[2025-03-19 15:27:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 3 loss:0.3338865637779236 norm:0.02372589148581028 max memory_allocated 53675.1455078125 
[2025-03-19 15:28:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 4 loss:0.32926031947135925 norm:0.021471837535500526 max memory_allocated 53675.1455078125 
[2025-03-19 15:28:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 5 loss:0.3268570005893707 norm:0.019106142222881317 max memory_allocated 53675.1455078125 
[2025-03-19 15:29:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 6 loss:0.3245995044708252 norm:0.017689809203147888 max memory_allocated 53675.1455078125 
[2025-03-19 15:29:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 7 loss:0.32312867045402527 norm:0.01643419824540615 max memory_allocated 53675.1455078125 
[2025-03-19 15:30:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 8 loss:0.32182493805885315 norm:0.01566309481859207 max memory_allocated 53675.1455078125 
[2025-03-19 15:30:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 9 loss:0.3204783499240875 norm:0.013660745695233345 max memory_allocated 53675.1455078125 
[2025-03-19 15:31:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 10 loss:0.31954047083854675 norm:0.013164790347218513 max memory_allocated 53675.1455078125 
[2025-03-19 15:31:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 11 loss:0.3194420039653778 norm:0.013057425618171692 max memory_allocated 53675.1455078125 
[2025-03-19 15:31:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 12 loss:0.31899014115333557 norm:0.012626142241060734 max memory_allocated 53675.1455078125 
[2025-03-19 15:32:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 13 loss:0.3182366192340851 norm:0.012224666774272919 max memory_allocated 53675.1455078125 
[2025-03-19 15:32:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 14 loss:0.31789952516555786 norm:0.0127959493547678 max memory_allocated 53675.1455078125 
[2025-03-19 15:33:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 15 loss:0.3167656660079956 norm:0.011580645106732845 max memory_allocated 53675.1455078125 
[2025-03-19 15:33:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 16 loss:0.3163236677646637 norm:0.011122648604214191 max memory_allocated 53675.1455078125 
[2025-03-19 15:34:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 17 loss:0.31626570224761963 norm:0.011261038482189178 max memory_allocated 53675.1455078125 
[2025-03-19 15:34:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 18 loss:0.31611356139183044 norm:0.011707584373652935 max memory_allocated 53675.1455078125 
[2025-03-19 15:35:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [31]) iter 19 loss:0.31606030464172363 norm:0.011750527657568455 max memory_allocated 53675.1455078125 
[2025-03-19 15:35:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [31]
[2025-03-19 15:35:42 root] (main_calib_config3_attn.py 379): INFO 18317.964083194733
[2025-03-19 15:35:46 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-19 15:36:33 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.637176036834717
[2025-03-19 15:36:33 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-19 15:37:44 root] (main_calib_config3_attn.py 161): INFO c4 : 7.204087734222412
[2025-03-19 16:11:23 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.637176036834717, 'c4': 7.204087734222412, 'results': {'hellaswag': {'acc': 0.5549691296554471, 'acc_stderr': 0.004959535443170608, 'acc_norm': 0.7157936666002789, 'acc_norm_stderr': 0.004501137895230717}, 'winogrande': {'acc': 0.6764009471191792, 'acc_stderr': 0.01314888332092315}, 'arc_easy': {'acc': 0.6944444444444444, 'acc_stderr': 0.009452181213593468, 'acc_norm': 0.5298821548821548, 'acc_norm_stderr': 0.010241444322886433}, 'piqa': {'acc': 0.7774755168661589, 'acc_stderr': 0.009704600975718238, 'acc_norm': 0.76550598476605, 'acc_norm_stderr': 0.00988520314324054}, 'boolq': {'acc': 0.7327217125382263, 'acc_stderr': 0.00774005256694996}, 'arc_challenge': {'acc': 0.40187713310580203, 'acc_stderr': 0.014327268614578278, 'acc_norm': 0.386518771331058, 'acc_norm_stderr': 0.014230084761910476}}, 'versions': {'hellaswag': 0, 'winogrande': 0, 'arc_easy': 0, 'piqa': 0, 'boolq': 1, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-19 16:11:23 root] (main_calib_config3_attn.py 175): INFO 40.19,69.44,73.27,55.50,77.75,67.64
