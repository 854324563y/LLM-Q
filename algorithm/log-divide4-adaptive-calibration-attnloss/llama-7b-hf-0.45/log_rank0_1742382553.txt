[2025-03-19 11:09:13 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-divide4-adaptive-calibration-attnloss/llama-7b-hf-0.45', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide4-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.45.pkl', blocks_pkl='./log-divide4/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-19 11:14:16 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-19 11:14:17 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-19 11:14:17 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-19 11:14:17 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide4-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.45.pkl
[2025-03-19 11:14:17 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide4/llama-7b-hf-w4a4/llama-7b-hf_blocks.pkl: [(0, 2), (2, 6), (6, 10), (10, 14), (14, 16), (16, 20), (20, 23), (23, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-19 11:14:17 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0, 1], [2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12, 13], [14, 15], [16, 17, 18, 19], [20, 21, 22], [23, 24], [25, 26], [27], [28], [29], [30], [31]]
[2025-03-19 11:14:25 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0, 1] ===
[2025-03-19 11:14:31 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 11:14:33 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 11:15:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 0 loss:0.03598976880311966 norm:0.0736965537071228 max memory_allocated 40873.322265625 
[2025-03-19 11:16:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 1 loss:0.018432680517435074 norm:0.0312899574637413 max memory_allocated 40873.322265625 
[2025-03-19 11:17:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 2 loss:0.013829227536916733 norm:0.0367712564766407 max memory_allocated 40873.322265625 
[2025-03-19 11:18:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 3 loss:0.011509852483868599 norm:0.027252163738012314 max memory_allocated 40873.322265625 
[2025-03-19 11:19:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 4 loss:0.010310760699212551 norm:0.02352631278336048 max memory_allocated 40873.322265625 
[2025-03-19 11:20:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 5 loss:0.00902116484940052 norm:0.015492542646825314 max memory_allocated 40873.322265625 
[2025-03-19 11:20:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 6 loss:0.008547415025532246 norm:0.014988555572926998 max memory_allocated 40873.322265625 
[2025-03-19 11:21:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 7 loss:0.008059702813625336 norm:0.012596407905220985 max memory_allocated 40873.322265625 
[2025-03-19 11:22:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 8 loss:0.0077284518629312515 norm:0.01094946451485157 max memory_allocated 40873.322265625 
[2025-03-19 11:23:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 9 loss:0.007328093983232975 norm:0.009784509427845478 max memory_allocated 40873.322265625 
[2025-03-19 11:24:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 10 loss:0.007141278125345707 norm:0.009165621362626553 max memory_allocated 40873.322265625 
[2025-03-19 11:25:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 11 loss:0.006943366024643183 norm:0.006916125304996967 max memory_allocated 40873.322265625 
[2025-03-19 11:26:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 12 loss:0.006780060473829508 norm:0.006231667473912239 max memory_allocated 40873.322265625 
[2025-03-19 11:27:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 13 loss:0.006655368488281965 norm:0.005785140208899975 max memory_allocated 40873.322265625 
[2025-03-19 11:28:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 14 loss:0.006626021582633257 norm:0.0054624504409730434 max memory_allocated 40873.322265625 
[2025-03-19 11:28:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 15 loss:0.0065874336287379265 norm:0.005357452668249607 max memory_allocated 40873.322265625 
[2025-03-19 11:29:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 16 loss:0.006492353044450283 norm:0.004249717108905315 max memory_allocated 40873.322265625 
[2025-03-19 11:30:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 17 loss:0.006446927785873413 norm:0.004703320097178221 max memory_allocated 40873.322265625 
[2025-03-19 11:31:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 18 loss:0.006410953123122454 norm:0.004488029982894659 max memory_allocated 40873.322265625 
[2025-03-19 11:32:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0, 1]) iter 19 loss:0.006474391091614962 norm:0.004863466136157513 max memory_allocated 40873.322265625 
[2025-03-19 11:33:48 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0, 1]
[2025-03-19 11:33:48 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [2, 3, 4, 5] ===
[2025-03-19 11:33:48 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 11:35:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 0 loss:0.06715882569551468 norm:0.03215910121798515 max memory_allocated 53661.8955078125 
[2025-03-19 11:37:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 1 loss:0.04904491454362869 norm:0.02236654795706272 max memory_allocated 53661.8955078125 
[2025-03-19 11:39:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 2 loss:0.038610078394412994 norm:0.015846114605665207 max memory_allocated 53661.8955078125 
[2025-03-19 11:41:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 3 loss:0.03288523480296135 norm:0.011424679309129715 max memory_allocated 53661.8955078125 
[2025-03-19 11:42:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 4 loss:0.030038028955459595 norm:0.00823119655251503 max memory_allocated 53661.8955078125 
[2025-03-19 11:44:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 5 loss:0.02895619533956051 norm:0.007375446148216724 max memory_allocated 53661.8955078125 
[2025-03-19 11:46:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 6 loss:0.027460241690278053 norm:0.006124745588749647 max memory_allocated 53661.8955078125 
[2025-03-19 11:48:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 7 loss:0.0263681598007679 norm:0.0055561126209795475 max memory_allocated 53661.8955078125 
[2025-03-19 11:49:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 8 loss:0.0255891066044569 norm:0.005013326182961464 max memory_allocated 53661.8955078125 
[2025-03-19 11:51:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 9 loss:0.02540898136794567 norm:0.0047433082945644855 max memory_allocated 53661.8955078125 
[2025-03-19 11:53:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 10 loss:0.026566047221422195 norm:0.0061773802153766155 max memory_allocated 53661.8955078125 
[2025-03-19 11:55:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 11 loss:0.024927504360675812 norm:0.004411605186760426 max memory_allocated 53661.8955078125 
[2025-03-19 11:57:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 12 loss:0.02485477738082409 norm:0.004204113967716694 max memory_allocated 53661.8955078125 
[2025-03-19 11:58:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 13 loss:0.024809053167700768 norm:0.0041947513818740845 max memory_allocated 53661.8955078125 
[2025-03-19 12:00:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 14 loss:0.024935537949204445 norm:0.004089539870619774 max memory_allocated 53661.8955078125 
[2025-03-19 12:02:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 15 loss:0.024729978293180466 norm:0.0034952771384269 max memory_allocated 53661.8955078125 
[2025-03-19 12:04:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 16 loss:0.024642188102006912 norm:0.0033202494960278273 max memory_allocated 53661.8955078125 
[2025-03-19 12:05:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 17 loss:0.024532198905944824 norm:0.0030155025888234377 max memory_allocated 53661.8955078125 
[2025-03-19 12:07:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 18 loss:0.024492191150784492 norm:0.0030438138637691736 max memory_allocated 53661.8955078125 
[2025-03-19 12:09:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [2, 3, 4, 5]) iter 19 loss:0.02460356429219246 norm:0.003114644903689623 max memory_allocated 53661.8955078125 
[2025-03-19 12:11:55 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [2, 3, 4, 5]
[2025-03-19 12:11:55 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [6, 7, 8, 9] ===
[2025-03-19 12:13:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 0 loss:0.09719329327344894 norm:0.0037776813842356205 max memory_allocated 53662.1455078125 
[2025-03-19 12:15:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 1 loss:0.06965135037899017 norm:0.000920731807127595 max memory_allocated 53662.1455078125 
[2025-03-19 12:17:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 2 loss:0.05466361343860626 norm:0.0004776713903993368 max memory_allocated 53662.1455078125 
[2025-03-19 12:19:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 3 loss:0.047528043389320374 norm:0.00040743208955973387 max memory_allocated 53662.1455078125 
[2025-03-19 12:20:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 4 loss:0.043948110193014145 norm:0.0003738547384273261 max memory_allocated 53662.1455078125 
[2025-03-19 12:22:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 5 loss:0.04164490848779678 norm:0.0003477378922980279 max memory_allocated 53662.1455078125 
[2025-03-19 12:24:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 6 loss:0.04005219787359238 norm:0.00032203379669226706 max memory_allocated 53662.1455078125 
[2025-03-19 12:26:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 7 loss:0.03908054530620575 norm:0.0003350621263962239 max memory_allocated 53662.1455078125 
[2025-03-19 12:28:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 8 loss:0.038352105766534805 norm:0.0003113128477707505 max memory_allocated 53662.1455078125 
[2025-03-19 12:29:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 9 loss:0.03795528784394264 norm:0.00033168939989991486 max memory_allocated 53662.1455078125 
[2025-03-19 12:31:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 10 loss:0.037657130509614944 norm:0.0003305492573417723 max memory_allocated 53662.1455078125 
[2025-03-19 12:33:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 11 loss:0.037465788424015045 norm:0.0003395621897652745 max memory_allocated 53662.1455078125 
[2025-03-19 12:35:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 12 loss:0.03737148270010948 norm:0.0003392679791431874 max memory_allocated 53662.1455078125 
[2025-03-19 12:36:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 13 loss:0.037158310413360596 norm:0.00031495123403146863 max memory_allocated 53662.1455078125 
[2025-03-19 12:38:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 14 loss:0.03706720471382141 norm:0.00031229719752445817 max memory_allocated 53662.1455078125 
[2025-03-19 12:40:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 15 loss:0.037003081291913986 norm:0.0003197620389983058 max memory_allocated 53662.1455078125 
[2025-03-19 12:42:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 16 loss:0.03690893203020096 norm:0.000314490869641304 max memory_allocated 53662.1455078125 
[2025-03-19 12:44:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 17 loss:0.03684813156723976 norm:0.00031605345429852605 max memory_allocated 53662.1455078125 
[2025-03-19 12:45:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 18 loss:0.03690022602677345 norm:0.00032544261193834245 max memory_allocated 53662.1455078125 
[2025-03-19 12:47:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [6, 7, 8, 9]) iter 19 loss:0.03691413998603821 norm:0.00031329161720350385 max memory_allocated 53662.1455078125 
[2025-03-19 12:50:02 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [6, 7, 8, 9]
[2025-03-19 12:50:02 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [10, 11, 12, 13] ===
[2025-03-19 12:51:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 0 loss:0.09707202762365341 norm:0.0016108342679217458 max memory_allocated 53662.3955078125 
[2025-03-19 12:53:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 1 loss:0.07646408677101135 norm:0.0006959805032238364 max memory_allocated 53662.3955078125 
[2025-03-19 12:55:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 2 loss:0.061812225729227066 norm:0.0003981617046520114 max memory_allocated 53662.3955078125 
[2025-03-19 12:57:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 3 loss:0.05476132780313492 norm:0.00030298501951619983 max memory_allocated 53662.3955078125 
[2025-03-19 12:59:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 4 loss:0.051234882324934006 norm:0.0002704016224015504 max memory_allocated 53662.3955078125 
[2025-03-19 13:00:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 5 loss:0.049079470336437225 norm:0.0002506451273802668 max memory_allocated 53662.3955078125 
[2025-03-19 13:02:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 6 loss:0.04765116423368454 norm:0.0002427712024655193 max memory_allocated 53662.3955078125 
[2025-03-19 13:04:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 7 loss:0.046728748828172684 norm:0.00023447524290531874 max memory_allocated 53662.3955078125 
[2025-03-19 13:06:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 8 loss:0.046081267297267914 norm:0.00023079686798155308 max memory_allocated 53662.3955078125 
[2025-03-19 13:07:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 9 loss:0.045648686587810516 norm:0.00022645456192549318 max memory_allocated 53662.3955078125 
[2025-03-19 13:09:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 10 loss:0.04531925171613693 norm:0.00022154691396281123 max memory_allocated 53662.3955078125 
[2025-03-19 13:11:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 11 loss:0.04515937715768814 norm:0.0002215678832726553 max memory_allocated 53662.3955078125 
[2025-03-19 13:13:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 12 loss:0.04501061886548996 norm:0.00022350861399900168 max memory_allocated 53662.3955078125 
[2025-03-19 13:15:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 13 loss:0.04490746185183525 norm:0.00022428347438108176 max memory_allocated 53662.3955078125 
[2025-03-19 13:16:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 14 loss:0.04480234533548355 norm:0.00022203411208465695 max memory_allocated 53662.3955078125 
[2025-03-19 13:18:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 15 loss:0.0447438545525074 norm:0.00022111751604825258 max memory_allocated 53662.3955078125 
[2025-03-19 13:20:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 16 loss:0.04464896023273468 norm:0.00022737725521437824 max memory_allocated 53662.3955078125 
[2025-03-19 13:22:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 17 loss:0.04457307606935501 norm:0.00022361971787177026 max memory_allocated 53662.3955078125 
[2025-03-19 13:23:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 18 loss:0.044547341763973236 norm:0.0002179039438487962 max memory_allocated 53662.3955078125 
[2025-03-19 13:25:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [10, 11, 12, 13]) iter 19 loss:0.04451872035861015 norm:0.00021949585061520338 max memory_allocated 53662.3955078125 
[2025-03-19 13:28:12 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [10, 11, 12, 13]
[2025-03-19 13:28:12 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [14, 15] ===
[2025-03-19 13:29:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 0 loss:0.07405488193035126 norm:0.0007921974174678326 max memory_allocated 53662.3955078125 
[2025-03-19 13:30:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 1 loss:0.061444077640771866 norm:0.00037523373612202704 max memory_allocated 53662.3955078125 
[2025-03-19 13:30:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 2 loss:0.051883578300476074 norm:0.00023858477652538568 max memory_allocated 53662.3955078125 
[2025-03-19 13:31:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 3 loss:0.04844556376338005 norm:0.00020462536485865712 max memory_allocated 53662.3955078125 
[2025-03-19 13:32:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 4 loss:0.046578820794820786 norm:0.0001855802402133122 max memory_allocated 53662.3955078125 
[2025-03-19 13:33:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 5 loss:0.04555349424481392 norm:0.00017439696239307523 max memory_allocated 53662.3955078125 
[2025-03-19 13:34:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 6 loss:0.04499633610248566 norm:0.00015507254283875227 max memory_allocated 53662.3955078125 
[2025-03-19 13:35:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 7 loss:0.04467355087399483 norm:0.00015547256043646485 max memory_allocated 53662.3955078125 
[2025-03-19 13:36:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 8 loss:0.04443712905049324 norm:0.00014393290621228516 max memory_allocated 53662.3955078125 
[2025-03-19 13:37:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 9 loss:0.044329509139060974 norm:0.0001529472938273102 max memory_allocated 53662.3955078125 
[2025-03-19 13:38:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 10 loss:0.044219378381967545 norm:0.00013938653864897788 max memory_allocated 53662.3955078125 
[2025-03-19 13:38:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 11 loss:0.04412379488348961 norm:0.00013049501285422593 max memory_allocated 53662.3955078125 
[2025-03-19 13:39:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 12 loss:0.044033996760845184 norm:0.00012575164146255702 max memory_allocated 53662.3955078125 
[2025-03-19 13:40:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 13 loss:0.04397109895944595 norm:0.00013096773182041943 max memory_allocated 53662.3955078125 
[2025-03-19 13:41:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 14 loss:0.043894242495298386 norm:0.00013114073954056948 max memory_allocated 53662.3955078125 
[2025-03-19 13:42:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 15 loss:0.04388808086514473 norm:0.00013572454918175936 max memory_allocated 53662.3955078125 
[2025-03-19 13:43:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 16 loss:0.043845608830451965 norm:0.00013347220374271274 max memory_allocated 53662.3955078125 
[2025-03-19 13:44:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 17 loss:0.04377995803952217 norm:0.00012932956451550126 max memory_allocated 53662.3955078125 
[2025-03-19 13:45:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 18 loss:0.04372338950634003 norm:0.00012676794722210616 max memory_allocated 53662.3955078125 
[2025-03-19 13:46:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [14, 15]) iter 19 loss:0.043687108904123306 norm:0.00012815739319194108 max memory_allocated 53662.3955078125 
[2025-03-19 13:47:17 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [14, 15]
[2025-03-19 13:47:17 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [16, 17, 18, 19] ===
[2025-03-19 13:49:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 0 loss:0.15819570422172546 norm:0.0020998867694288492 max memory_allocated 53662.7705078125 
[2025-03-19 13:51:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 1 loss:0.1268756240606308 norm:0.0007647277088835835 max memory_allocated 53662.7705078125 
[2025-03-19 13:52:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 2 loss:0.10196782648563385 norm:0.0004385162319522351 max memory_allocated 53662.7705078125 
[2025-03-19 13:54:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 3 loss:0.09317567944526672 norm:0.0003718092630151659 max memory_allocated 53662.7705078125 
[2025-03-19 13:56:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 4 loss:0.08815661817789078 norm:0.0003420566499698907 max memory_allocated 53662.7705078125 
[2025-03-19 13:58:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 5 loss:0.08557871729135513 norm:0.0003332258784212172 max memory_allocated 53662.7705078125 
[2025-03-19 13:59:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 6 loss:0.08412256091833115 norm:0.0003252801252529025 max memory_allocated 53662.7705078125 
[2025-03-19 14:01:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 7 loss:0.08310319483280182 norm:0.00031203916296362877 max memory_allocated 53662.7705078125 
[2025-03-19 14:03:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 8 loss:0.08242245018482208 norm:0.00030739069916307926 max memory_allocated 53662.7705078125 
[2025-03-19 14:05:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 9 loss:0.081905297935009 norm:0.00029742909828200936 max memory_allocated 53662.7705078125 
[2025-03-19 14:06:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 10 loss:0.08152688294649124 norm:0.00030129350489005446 max memory_allocated 53662.7705078125 
[2025-03-19 14:08:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 11 loss:0.08115973323583603 norm:0.00030434998916462064 max memory_allocated 53662.7705078125 
[2025-03-19 14:10:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 12 loss:0.08082598447799683 norm:0.00029389432165771723 max memory_allocated 53662.7705078125 
[2025-03-19 14:12:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 13 loss:0.08056964725255966 norm:0.0002967288892250508 max memory_allocated 53662.7705078125 
[2025-03-19 14:14:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 14 loss:0.08037018775939941 norm:0.0002923536521848291 max memory_allocated 53662.7705078125 
[2025-03-19 14:15:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 15 loss:0.08011890947818756 norm:0.0002856814826373011 max memory_allocated 53662.7705078125 
[2025-03-19 14:17:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 16 loss:0.07991984486579895 norm:0.0002821247617248446 max memory_allocated 53662.7705078125 
[2025-03-19 14:19:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 17 loss:0.07976365089416504 norm:0.0002864317502826452 max memory_allocated 53662.7705078125 
[2025-03-19 14:21:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 18 loss:0.07962165772914886 norm:0.0002850388700608164 max memory_allocated 53662.7705078125 
[2025-03-19 14:22:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [16, 17, 18, 19]) iter 19 loss:0.07950161397457123 norm:0.00027938809944316745 max memory_allocated 53662.7705078125 
[2025-03-19 14:25:23 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [16, 17, 18, 19]
[2025-03-19 14:25:23 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [20, 21, 22] ===
[2025-03-19 14:26:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 0 loss:0.2028900533914566 norm:0.002145285252481699 max memory_allocated 53662.7705078125 
[2025-03-19 14:28:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 1 loss:0.1696530133485794 norm:0.0008316630264744163 max memory_allocated 53662.7705078125 
[2025-03-19 14:29:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 2 loss:0.1409866213798523 norm:0.000538005493581295 max memory_allocated 53662.7705078125 
[2025-03-19 14:30:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 3 loss:0.13217762112617493 norm:0.0005018218653276563 max memory_allocated 53662.7705078125 
[2025-03-19 14:32:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 4 loss:0.1284361183643341 norm:0.0004809398087672889 max memory_allocated 53662.7705078125 
[2025-03-19 14:33:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 5 loss:0.12666131556034088 norm:0.00043593536247499287 max memory_allocated 53662.7705078125 
[2025-03-19 14:34:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 6 loss:0.1256614625453949 norm:0.0004214760847389698 max memory_allocated 53662.7705078125 
[2025-03-19 14:36:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 7 loss:0.12483936548233032 norm:0.00042786067933775485 max memory_allocated 53662.7705078125 
[2025-03-19 14:37:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 8 loss:0.124101422727108 norm:0.0004136162460781634 max memory_allocated 53662.7705078125 
[2025-03-19 14:38:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 9 loss:0.12334492802619934 norm:0.0003799122932832688 max memory_allocated 53662.7705078125 
[2025-03-19 14:40:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 10 loss:0.1227400004863739 norm:0.00037690968019887805 max memory_allocated 53662.7705078125 
[2025-03-19 14:41:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 11 loss:0.12218189239501953 norm:0.0003765460569411516 max memory_allocated 53662.7705078125 
[2025-03-19 14:42:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 12 loss:0.12166909128427505 norm:0.00037869380321353674 max memory_allocated 53662.7705078125 
[2025-03-19 14:44:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 13 loss:0.12133891135454178 norm:0.0003652900632005185 max memory_allocated 53662.7705078125 
[2025-03-19 14:45:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 14 loss:0.12099502980709076 norm:0.0003430406213738024 max memory_allocated 53662.7705078125 
[2025-03-19 14:46:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 15 loss:0.1207185760140419 norm:0.0003602579818107188 max memory_allocated 53662.7705078125 
[2025-03-19 14:48:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 16 loss:0.1203940361738205 norm:0.0003521129838190973 max memory_allocated 53662.7705078125 
[2025-03-19 14:49:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 17 loss:0.1201668381690979 norm:0.0003343776916153729 max memory_allocated 53662.7705078125 
[2025-03-19 14:50:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 18 loss:0.11996959894895554 norm:0.0003423055459279567 max memory_allocated 53662.7705078125 
[2025-03-19 14:52:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [20, 21, 22]) iter 19 loss:0.11974422633647919 norm:0.0003327241283841431 max memory_allocated 53662.7705078125 
[2025-03-19 14:53:57 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [20, 21, 22]
[2025-03-19 14:53:57 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [23, 24] ===
[2025-03-19 14:54:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 0 loss:0.217601016163826 norm:0.0015296003548428416 max memory_allocated 53662.7705078125 
[2025-03-19 14:55:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 1 loss:0.19088661670684814 norm:0.0007159902597777545 max memory_allocated 53662.7705078125 
[2025-03-19 14:56:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 2 loss:0.1672438383102417 norm:0.00042570781079120934 max memory_allocated 53662.7705078125 
[2025-03-19 14:57:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 3 loss:0.16033363342285156 norm:0.00036799878580495715 max memory_allocated 53662.7705078125 
[2025-03-19 14:58:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 4 loss:0.158014178276062 norm:0.00034594873432070017 max memory_allocated 53662.7705078125 
[2025-03-19 14:59:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 5 loss:0.15701813995838165 norm:0.00033308230922557414 max memory_allocated 53662.7705078125 
[2025-03-19 15:00:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 6 loss:0.15627190470695496 norm:0.0003057386784348637 max memory_allocated 53662.7705078125 
[2025-03-19 15:01:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 7 loss:0.15573054552078247 norm:0.0002974102972075343 max memory_allocated 53662.7705078125 
[2025-03-19 15:02:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 8 loss:0.15518444776535034 norm:0.00028889632085338235 max memory_allocated 53662.7705078125 
[2025-03-19 15:02:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 9 loss:0.15483051538467407 norm:0.0002908388851210475 max memory_allocated 53662.7705078125 
[2025-03-19 15:03:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 10 loss:0.15440991520881653 norm:0.00027848107856698334 max memory_allocated 53662.7705078125 
[2025-03-19 15:04:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 11 loss:0.15396854281425476 norm:0.00028077291790395975 max memory_allocated 53662.7705078125 
[2025-03-19 15:05:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 12 loss:0.1536315530538559 norm:0.0002734237350523472 max memory_allocated 53662.7705078125 
[2025-03-19 15:06:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 13 loss:0.15342429280281067 norm:0.00026708008954301476 max memory_allocated 53662.7705078125 
[2025-03-19 15:07:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 14 loss:0.15316899120807648 norm:0.0002701726043596864 max memory_allocated 53662.7705078125 
[2025-03-19 15:08:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 15 loss:0.1529473215341568 norm:0.0002673852723091841 max memory_allocated 53662.7705078125 
[2025-03-19 15:09:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 16 loss:0.15276044607162476 norm:0.0002735625021159649 max memory_allocated 53662.7705078125 
[2025-03-19 15:10:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 17 loss:0.15254825353622437 norm:0.00026530615286901593 max memory_allocated 53662.7705078125 
[2025-03-19 15:10:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 18 loss:0.1524852216243744 norm:0.00027892953949049115 max memory_allocated 53662.7705078125 
[2025-03-19 15:11:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [23, 24]) iter 19 loss:0.15228676795959473 norm:0.0002608630165923387 max memory_allocated 53662.7705078125 
[2025-03-19 15:13:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [23, 24]
[2025-03-19 15:13:01 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [25, 26] ===
[2025-03-19 15:14:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 0 loss:0.2694033980369568 norm:0.001211264869198203 max memory_allocated 53662.7705078125 
[2025-03-19 15:14:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 1 loss:0.24055303633213043 norm:0.0006036328268237412 max memory_allocated 53662.7705078125 
[2025-03-19 15:15:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 2 loss:0.21394270658493042 norm:0.0003663600655272603 max memory_allocated 53662.7705078125 
[2025-03-19 15:16:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 3 loss:0.20699091255664825 norm:0.00032965210266411304 max memory_allocated 53662.7705078125 
[2025-03-19 15:17:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 4 loss:0.20525602996349335 norm:0.00031101907370612025 max memory_allocated 53662.7705078125 
[2025-03-19 15:18:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 5 loss:0.20431016385555267 norm:0.00028019314049743116 max memory_allocated 53662.7705078125 
[2025-03-19 15:19:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 6 loss:0.2036263644695282 norm:0.0002776109613478184 max memory_allocated 53662.7705078125 
[2025-03-19 15:20:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 7 loss:0.20301562547683716 norm:0.00025733085931278765 max memory_allocated 53662.7705078125 
[2025-03-19 15:21:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 8 loss:0.202394500374794 norm:0.00024699364439584315 max memory_allocated 53662.7705078125 
[2025-03-19 15:22:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 9 loss:0.2019234299659729 norm:0.0002511907950975001 max memory_allocated 53662.7705078125 
[2025-03-19 15:22:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 10 loss:0.20146460831165314 norm:0.00024323702382389456 max memory_allocated 53662.7705078125 
[2025-03-19 15:23:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 11 loss:0.2010945975780487 norm:0.00024086901976261288 max memory_allocated 53662.7705078125 
[2025-03-19 15:24:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 12 loss:0.20071402192115784 norm:0.00023582048015668988 max memory_allocated 53662.7705078125 
[2025-03-19 15:25:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 13 loss:0.20042158663272858 norm:0.00023516723013017327 max memory_allocated 53662.7705078125 
[2025-03-19 15:26:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 14 loss:0.20016300678253174 norm:0.00023190508363768458 max memory_allocated 53662.7705078125 
[2025-03-19 15:27:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 15 loss:0.19990558922290802 norm:0.00023099654936231673 max memory_allocated 53662.7705078125 
[2025-03-19 15:28:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 16 loss:0.19972632825374603 norm:0.00023556161613669246 max memory_allocated 53662.7705078125 
[2025-03-19 15:29:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 17 loss:0.19954071938991547 norm:0.00022843954502604902 max memory_allocated 53662.7705078125 
[2025-03-19 15:30:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 18 loss:0.1993645876646042 norm:0.00023369499831460416 max memory_allocated 53662.7705078125 
[2025-03-19 15:30:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [25, 26]) iter 19 loss:0.19919896125793457 norm:0.00023853027960285544 max memory_allocated 53662.7705078125 
[2025-03-19 15:32:06 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [25, 26]
[2025-03-19 15:32:06 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [27] ===
[2025-03-19 15:32:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 0 loss:0.25080373883247375 norm:0.0006250441656447947 max memory_allocated 53662.7705078125 
[2025-03-19 15:33:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 1 loss:0.23587140440940857 norm:0.00040231284219771624 max memory_allocated 53662.7705078125 
[2025-03-19 15:33:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 2 loss:0.22242334485054016 norm:0.00027039373526349664 max memory_allocated 53662.7705078125 
[2025-03-19 15:33:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 3 loss:0.21911583840847015 norm:0.00023437169147655368 max memory_allocated 53662.7705078125 
[2025-03-19 15:34:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 4 loss:0.21836329996585846 norm:0.00022910854022484273 max memory_allocated 53662.7705078125 
[2025-03-19 15:34:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 5 loss:0.21791274845600128 norm:0.00021275601466186345 max memory_allocated 53662.7705078125 
[2025-03-19 15:35:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 6 loss:0.2175622582435608 norm:0.00019552595040295273 max memory_allocated 53662.7705078125 
[2025-03-19 15:35:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 7 loss:0.21731477975845337 norm:0.00019718398107215762 max memory_allocated 53662.7705078125 
[2025-03-19 15:36:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 8 loss:0.21707788109779358 norm:0.00017644929175730795 max memory_allocated 53662.7705078125 
[2025-03-19 15:36:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 9 loss:0.21688048541545868 norm:0.00017150526400655508 max memory_allocated 53662.7705078125 
[2025-03-19 15:37:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 10 loss:0.2167082577943802 norm:0.0001629543403396383 max memory_allocated 53662.7705078125 
[2025-03-19 15:37:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 11 loss:0.21652455627918243 norm:0.00016381932073272765 max memory_allocated 53662.7705078125 
[2025-03-19 15:37:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 12 loss:0.21637170016765594 norm:0.00016526556282769889 max memory_allocated 53662.7705078125 
[2025-03-19 15:38:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 13 loss:0.21623694896697998 norm:0.00017046707216650248 max memory_allocated 53662.7705078125 
[2025-03-19 15:38:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 14 loss:0.2161140888929367 norm:0.00017155335808638483 max memory_allocated 53662.7705078125 
[2025-03-19 15:39:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 15 loss:0.21602711081504822 norm:0.00016369119111914188 max memory_allocated 53662.7705078125 
[2025-03-19 15:39:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 16 loss:0.21592752635478973 norm:0.0001742528984323144 max memory_allocated 53662.7705078125 
[2025-03-19 15:40:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 17 loss:0.2158866673707962 norm:0.00017220326117239892 max memory_allocated 53662.7705078125 
[2025-03-19 15:40:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 18 loss:0.21586206555366516 norm:0.00016532375593669713 max memory_allocated 53662.7705078125 
[2025-03-19 15:41:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [27]) iter 19 loss:0.21576882898807526 norm:0.00017055022181011736 max memory_allocated 53662.7705078125 
[2025-03-19 15:41:41 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [27]
[2025-03-19 15:41:41 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [28] ===
[2025-03-19 15:41:42 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 15:42:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 0 loss:0.29573094844818115 norm:0.01742631010711193 max memory_allocated 53662.7705078125 
[2025-03-19 15:42:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 1 loss:0.27626368403434753 norm:0.013328718021512032 max memory_allocated 53662.7705078125 
[2025-03-19 15:43:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 2 loss:0.2596391439437866 norm:0.00883579347282648 max memory_allocated 53662.7705078125 
[2025-03-19 15:43:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 3 loss:0.25537002086639404 norm:0.007514125667512417 max memory_allocated 53662.7705078125 
[2025-03-19 15:43:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 4 loss:0.2542111873626709 norm:0.0065034073777496815 max memory_allocated 53662.7705078125 
[2025-03-19 15:44:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 5 loss:0.253500759601593 norm:0.005592919886112213 max memory_allocated 53662.7705078125 
[2025-03-19 15:44:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 6 loss:0.252951443195343 norm:0.004794518928974867 max memory_allocated 53662.7705078125 
[2025-03-19 15:45:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 7 loss:0.2525309920310974 norm:0.004439711570739746 max memory_allocated 53662.7705078125 
[2025-03-19 15:45:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 8 loss:0.2524873614311218 norm:0.00448571564629674 max memory_allocated 53662.7705078125 
[2025-03-19 15:46:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 9 loss:0.25216740369796753 norm:0.004507785197347403 max memory_allocated 53662.7705078125 
[2025-03-19 15:46:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 10 loss:0.25183823704719543 norm:0.003950189333409071 max memory_allocated 53662.7705078125 
[2025-03-19 15:47:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 11 loss:0.25149014592170715 norm:0.0037711590994149446 max memory_allocated 53662.7705078125 
[2025-03-19 15:47:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 12 loss:0.25123265385627747 norm:0.003418524982407689 max memory_allocated 53662.7705078125 
[2025-03-19 15:48:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 13 loss:0.25112384557724 norm:0.0034584077075123787 max memory_allocated 53662.7705078125 
[2025-03-19 15:48:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 14 loss:0.25100529193878174 norm:0.0033441949635744095 max memory_allocated 53662.7705078125 
[2025-03-19 15:48:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 15 loss:0.25089511275291443 norm:0.0034149971324950457 max memory_allocated 53662.7705078125 
[2025-03-19 15:49:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 16 loss:0.2507826089859009 norm:0.003203336149454117 max memory_allocated 53662.7705078125 
[2025-03-19 15:49:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 17 loss:0.2506469488143921 norm:0.003240077756345272 max memory_allocated 53662.7705078125 
[2025-03-19 15:50:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 18 loss:0.2505667507648468 norm:0.003125506453216076 max memory_allocated 53662.7705078125 
[2025-03-19 15:50:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [28]) iter 19 loss:0.25045907497406006 norm:0.0031494894064962864 max memory_allocated 53662.7705078125 
[2025-03-19 15:51:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [28]
[2025-03-19 15:51:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [29] ===
[2025-03-19 15:51:20 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 15:51:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 0 loss:0.34221240878105164 norm:0.020107798278331757 max memory_allocated 53662.7705078125 
[2025-03-19 15:52:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 1 loss:0.320279061794281 norm:0.014175278134644032 max memory_allocated 53662.7705078125 
[2025-03-19 15:52:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 2 loss:0.30145442485809326 norm:0.009123711846768856 max memory_allocated 53662.7705078125 
[2025-03-19 15:53:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 3 loss:0.2972005009651184 norm:0.00773024745285511 max memory_allocated 53662.7705078125 
[2025-03-19 15:53:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 4 loss:0.29605337977409363 norm:0.006712086498737335 max memory_allocated 53662.7705078125 
[2025-03-19 15:54:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 5 loss:0.29528358578681946 norm:0.005801821127533913 max memory_allocated 53662.7705078125 
[2025-03-19 15:54:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 6 loss:0.29472097754478455 norm:0.005035019479691982 max memory_allocated 53662.7705078125 
[2025-03-19 15:54:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 7 loss:0.2942699193954468 norm:0.004734296351671219 max memory_allocated 53662.7705078125 
[2025-03-19 15:55:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 8 loss:0.2940616309642792 norm:0.004613419063389301 max memory_allocated 53662.7705078125 
[2025-03-19 15:55:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 9 loss:0.293673574924469 norm:0.004485157318413258 max memory_allocated 53662.7705078125 
[2025-03-19 15:56:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 10 loss:0.29343360662460327 norm:0.003964633215218782 max memory_allocated 53662.7705078125 
[2025-03-19 15:56:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 11 loss:0.2931492030620575 norm:0.003996805287897587 max memory_allocated 53662.7705078125 
[2025-03-19 15:57:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 12 loss:0.293010413646698 norm:0.0037517030723392963 max memory_allocated 53662.7705078125 
[2025-03-19 15:57:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 13 loss:0.2927967309951782 norm:0.00385281047783792 max memory_allocated 53662.7705078125 
[2025-03-19 15:58:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 14 loss:0.2925928831100464 norm:0.0035156887024641037 max memory_allocated 53662.7705078125 
[2025-03-19 15:58:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 15 loss:0.2924014925956726 norm:0.0035317011643201113 max memory_allocated 53662.7705078125 
[2025-03-19 15:59:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 16 loss:0.29231786727905273 norm:0.0033489165361970663 max memory_allocated 53662.7705078125 
[2025-03-19 15:59:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 17 loss:0.29214635491371155 norm:0.003424145048484206 max memory_allocated 53662.7705078125 
[2025-03-19 15:59:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 18 loss:0.2920530140399933 norm:0.0032670728396624327 max memory_allocated 53662.7705078125 
[2025-03-19 16:00:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [29]) iter 19 loss:0.29188328981399536 norm:0.003220951184630394 max memory_allocated 53662.7705078125 
[2025-03-19 16:00:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [29]
[2025-03-19 16:00:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [30] ===
[2025-03-19 16:01:00 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 16:01:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 0 loss:0.4587278962135315 norm:0.03347592055797577 max memory_allocated 53662.7705078125 
[2025-03-19 16:01:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 1 loss:0.41661831736564636 norm:0.022211499512195587 max memory_allocated 53662.7705078125 
[2025-03-19 16:02:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 2 loss:0.3824644982814789 norm:0.014912351965904236 max memory_allocated 53662.7705078125 
[2025-03-19 16:02:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 3 loss:0.3759923279285431 norm:0.012807768769562244 max memory_allocated 53662.7705078125 
[2025-03-19 16:03:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 4 loss:0.37332823872566223 norm:0.011184213683009148 max memory_allocated 53662.7705078125 
[2025-03-19 16:03:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 5 loss:0.3715519607067108 norm:0.009557245299220085 max memory_allocated 53662.7705078125 
[2025-03-19 16:04:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 6 loss:0.37051552534103394 norm:0.008680913597345352 max memory_allocated 53662.7705078125 
[2025-03-19 16:04:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 7 loss:0.3694220185279846 norm:0.007591901812702417 max memory_allocated 53662.7705078125 
[2025-03-19 16:05:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 8 loss:0.36862608790397644 norm:0.007156470790505409 max memory_allocated 53662.7705078125 
[2025-03-19 16:05:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 9 loss:0.36816635727882385 norm:0.006877182517200708 max memory_allocated 53662.7705078125 
[2025-03-19 16:05:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 10 loss:0.3677106499671936 norm:0.006641746498644352 max memory_allocated 53662.7705078125 
[2025-03-19 16:06:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 11 loss:0.36722853779792786 norm:0.006609172094613314 max memory_allocated 53662.7705078125 
[2025-03-19 16:06:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 12 loss:0.3669373691082001 norm:0.006639769300818443 max memory_allocated 53662.7705078125 
[2025-03-19 16:07:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 13 loss:0.36673372983932495 norm:0.00630153575912118 max memory_allocated 53662.7705078125 
[2025-03-19 16:07:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 14 loss:0.36632809042930603 norm:0.00616287998855114 max memory_allocated 53662.7705078125 
[2025-03-19 16:08:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 15 loss:0.36609986424446106 norm:0.005993007216602564 max memory_allocated 53662.7705078125 
[2025-03-19 16:08:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 16 loss:0.36588072776794434 norm:0.006064555607736111 max memory_allocated 53662.7705078125 
[2025-03-19 16:09:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 17 loss:0.3655744791030884 norm:0.005705174989998341 max memory_allocated 53662.7705078125 
[2025-03-19 16:09:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 18 loss:0.36533454060554504 norm:0.005833758506923914 max memory_allocated 53662.7705078125 
[2025-03-19 16:10:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [30]) iter 19 loss:0.3654709756374359 norm:0.00544404610991478 max memory_allocated 53662.7705078125 
[2025-03-19 16:10:38 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [30]
[2025-03-19 16:10:38 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [31] ===
[2025-03-19 16:10:38 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-19 16:11:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 0 loss:0.7949317097663879 norm:0.08412662148475647 max memory_allocated 53662.7705078125 
[2025-03-19 16:11:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 1 loss:0.7030952572822571 norm:0.05777144059538841 max memory_allocated 53662.7705078125 
[2025-03-19 16:12:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 2 loss:0.6373975872993469 norm:0.037124283611774445 max memory_allocated 53662.7705078125 
[2025-03-19 16:12:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 3 loss:0.6220483779907227 norm:0.03421259671449661 max memory_allocated 53662.7705078125 
[2025-03-19 16:12:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 4 loss:0.6143932342529297 norm:0.03107260912656784 max memory_allocated 53662.7705078125 
[2025-03-19 16:13:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 5 loss:0.6089932918548584 norm:0.02776605635881424 max memory_allocated 53662.7705078125 
[2025-03-19 16:13:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 6 loss:0.6053460240364075 norm:0.024599403142929077 max memory_allocated 53662.7705078125 
[2025-03-19 16:14:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 7 loss:0.6020535230636597 norm:0.02314171940088272 max memory_allocated 53662.7705078125 
[2025-03-19 16:14:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 8 loss:0.5993081331253052 norm:0.021372616291046143 max memory_allocated 53662.7705078125 
[2025-03-19 16:15:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 9 loss:0.5991016030311584 norm:0.023618431761860847 max memory_allocated 53662.7705078125 
[2025-03-19 16:15:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 10 loss:0.5963403582572937 norm:0.020449653267860413 max memory_allocated 53662.7705078125 
[2025-03-19 16:16:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 11 loss:0.5950459241867065 norm:0.020239179953932762 max memory_allocated 53662.7705078125 
[2025-03-19 16:16:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 12 loss:0.5939396023750305 norm:0.0200064554810524 max memory_allocated 53662.7705078125 
[2025-03-19 16:16:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 13 loss:0.5936871767044067 norm:0.020785335451364517 max memory_allocated 53662.7705078125 
[2025-03-19 16:17:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 14 loss:0.5918623208999634 norm:0.018775418400764465 max memory_allocated 53662.7705078125 
[2025-03-19 16:17:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 15 loss:0.5909532308578491 norm:0.018167341127991676 max memory_allocated 53662.7705078125 
[2025-03-19 16:18:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 16 loss:0.5904561281204224 norm:0.01752297766506672 max memory_allocated 53662.7705078125 
[2025-03-19 16:18:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 17 loss:0.5911051630973816 norm:0.020430555567145348 max memory_allocated 53662.7705078125 
[2025-03-19 16:19:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 18 loss:0.589932918548584 norm:0.01889994740486145 max memory_allocated 53662.7705078125 
[2025-03-19 16:19:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [31]) iter 19 loss:0.5887574553489685 norm:0.016956564038991928 max memory_allocated 53662.7705078125 
[2025-03-19 16:20:16 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [31]
[2025-03-19 16:20:16 root] (main_calib_config3_attn.py 379): INFO 18359.47747206688
[2025-03-19 16:20:23 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-19 16:21:09 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.829530715942383
[2025-03-19 16:21:09 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-19 16:22:21 root] (main_calib_config3_attn.py 161): INFO c4 : 7.292747974395752
