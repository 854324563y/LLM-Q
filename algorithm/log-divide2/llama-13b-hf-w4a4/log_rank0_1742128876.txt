[2025-03-16 12:41:16 root] (main_divide_blocks.py 279): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-divide2/llama-13b-hf-w4a4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.3, similarity_threshold=0.999, sensitivity_threshold=0.1, max_block_size=2, reload=False)
[2025-03-16 12:45:24 root] (main_divide_blocks.py 360): INFO === start quantization ===
[2025-03-16 12:45:25 root] (main_divide_blocks.py 366): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-16 12:45:25 root] (abq_llm_divide_blocks.py 66): INFO Starting ...
[2025-03-16 12:45:27 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-03-16 12:45:28 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.320279788970947
[2025-03-16 12:45:28 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-03-16 12:45:29 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9615949051720756
[2025-03-16 12:45:29 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.6141661882400515
[2025-03-16 12:45:29 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-03-16 12:45:31 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.981979250907898
[2025-03-16 12:45:31 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 4.066954597405025
[2025-03-16 12:45:31 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-03-16 12:45:32 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9900599632944379
[2025-03-16 12:45:32 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7448118703705924
[2025-03-16 12:45:32 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-03-16 12:45:34 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.999608039855957
[2025-03-16 12:45:34 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.762608848299299
[2025-03-16 12:45:34 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-03-16 12:45:35 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996611731392997
[2025-03-16 12:45:35 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.693883468423571
[2025-03-16 12:45:35 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-03-16 12:45:37 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998395698411124
[2025-03-16 12:45:37 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7523171663284303
[2025-03-16 12:45:37 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-03-16 12:45:38 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9992820194789341
[2025-03-16 12:45:39 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.731710420336042
[2025-03-16 12:45:39 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-03-16 12:45:40 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9989871723311288
[2025-03-16 12:45:40 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.6759087102753774
[2025-03-16 12:45:40 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-03-16 12:45:42 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.99907933814185
[2025-03-16 12:45:42 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.715382119587489
[2025-03-16 12:45:42 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-03-16 12:45:43 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996614541326251
[2025-03-16 12:45:43 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.69973144020353
[2025-03-16 12:45:43 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-03-16 12:45:45 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999208705765861
[2025-03-16 12:45:45 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.701703068188259
[2025-03-16 12:45:45 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-03-16 12:45:46 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999057565416608
[2025-03-16 12:45:47 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.753949097224644
[2025-03-16 12:45:47 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-03-16 12:45:48 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9992532985551017
[2025-03-16 12:45:48 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.6923445037433082
[2025-03-16 12:45:48 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-03-16 12:45:50 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998063615390232
[2025-03-16 12:45:50 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7501482844352725
[2025-03-16 12:45:50 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-03-16 12:45:51 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998798029763358
[2025-03-16 12:45:51 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7240645374570573
[2025-03-16 12:45:51 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-03-16 12:45:53 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997897403580802
[2025-03-16 12:45:53 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7159415824072703
[2025-03-16 12:45:53 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-03-16 12:45:54 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999437417302813
[2025-03-16 12:45:55 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7310768008232116
[2025-03-16 12:45:55 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-03-16 12:45:56 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997287307466779
[2025-03-16 12:45:56 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7162366492407664
[2025-03-16 12:45:56 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-03-16 12:45:58 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998310378619603
[2025-03-16 12:45:58 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7300322192055835
[2025-03-16 12:45:58 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-03-16 12:45:59 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998751538140433
[2025-03-16 12:45:59 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7048156329563686
[2025-03-16 12:45:59 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-03-16 12:46:01 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995440074375698
[2025-03-16 12:46:01 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.715273421151297
[2025-03-16 12:46:01 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-03-16 12:46:03 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999141778264727
[2025-03-16 12:46:03 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7128319569996426
[2025-03-16 12:46:03 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-03-16 12:46:04 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996859942163739
[2025-03-16 12:46:04 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.738313409260342
[2025-03-16 12:46:04 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-03-16 12:46:06 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998900975499835
[2025-03-16 12:46:06 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.718493754523141
[2025-03-16 12:46:06 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-03-16 12:46:07 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998655489512852
[2025-03-16 12:46:08 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.733975638662066
[2025-03-16 12:46:08 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-03-16 12:46:09 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999262179647174
[2025-03-16 12:46:09 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.737500841276986
[2025-03-16 12:46:09 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-03-16 12:46:11 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999001026153564
[2025-03-16 12:46:11 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7577734027590073
[2025-03-16 12:46:11 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-03-16 12:46:12 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997256653649467
[2025-03-16 12:46:12 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7355907474245345
[2025-03-16 12:46:12 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-03-16 12:46:14 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.999829820224217
[2025-03-16 12:46:14 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7765845741544455
[2025-03-16 12:46:14 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-03-16 12:46:16 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999498214040484
[2025-03-16 12:46:16 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7676370246069766
[2025-03-16 12:46:16 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-03-16 12:46:17 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998759201594761
[2025-03-16 12:46:17 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.783556478364127
[2025-03-16 12:46:17 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 32 similarity and sensitivity ===
[2025-03-16 12:46:19 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999594432967049
[2025-03-16 12:46:19 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.8043460028512137
[2025-03-16 12:46:19 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 33 similarity and sensitivity ===
[2025-03-16 12:46:23 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999618870871407
[2025-03-16 12:46:23 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.8258079392569404
[2025-03-16 12:46:23 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 34 similarity and sensitivity ===
[2025-03-16 12:46:24 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.999891860144479
[2025-03-16 12:46:24 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.8291522434779575
[2025-03-16 12:46:24 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 35 similarity and sensitivity ===
[2025-03-16 12:46:26 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997418693133763
[2025-03-16 12:46:26 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.785220425469535
[2025-03-16 12:46:26 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 36 similarity and sensitivity ===
[2025-03-16 12:46:28 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996395707130432
[2025-03-16 12:46:28 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.805184800284249
[2025-03-16 12:46:28 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 37 similarity and sensitivity ===
[2025-03-16 12:46:29 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998987827982221
[2025-03-16 12:46:29 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.836404524530683
[2025-03-16 12:46:29 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 38 similarity and sensitivity ===
[2025-03-16 12:46:31 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997256909097944
[2025-03-16 12:46:31 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.853971035139902
[2025-03-16 12:46:31 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 39 similarity and sensitivity ===
[2025-03-16 12:46:32 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9977353811264038
[2025-03-16 12:46:32 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 3.7991774899618966
[2025-03-16 12:46:32 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 0 ===
[2025-03-16 12:46:46 root] (abq_llm_divide_blocks.py 283): INFO layer 0 loss_mean: 0.002226109616458416
[2025-03-16 12:46:46 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 1 ===
[2025-03-16 12:47:00 root] (abq_llm_divide_blocks.py 283): INFO layer 1 loss_mean: 0.004792365245521069
[2025-03-16 12:47:00 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 2 ===
[2025-03-16 12:47:14 root] (abq_llm_divide_blocks.py 283): INFO layer 2 loss_mean: 1.3331352472305298
[2025-03-16 12:47:14 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 3 ===
[2025-03-16 12:47:28 root] (abq_llm_divide_blocks.py 283): INFO layer 3 loss_mean: 0.004918871447443962
[2025-03-16 12:47:28 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 4 ===
[2025-03-16 12:47:41 root] (abq_llm_divide_blocks.py 283): INFO layer 4 loss_mean: 0.011435621418058872
[2025-03-16 12:47:41 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 5 ===
[2025-03-16 12:47:55 root] (abq_llm_divide_blocks.py 283): INFO layer 5 loss_mean: 0.017215542495250702
[2025-03-16 12:47:55 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 6 ===
[2025-03-16 12:48:09 root] (abq_llm_divide_blocks.py 283): INFO layer 6 loss_mean: 4.561468124389648
[2025-03-16 12:48:09 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 7 ===
[2025-03-16 12:48:23 root] (abq_llm_divide_blocks.py 283): INFO layer 7 loss_mean: 0.012619167566299438
[2025-03-16 12:48:23 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 8 ===
[2025-03-16 12:48:36 root] (abq_llm_divide_blocks.py 283): INFO layer 8 loss_mean: 0.014345774427056313
[2025-03-16 12:48:36 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 9 ===
[2025-03-16 12:48:50 root] (abq_llm_divide_blocks.py 283): INFO layer 9 loss_mean: 0.015526057220995426
[2025-03-16 12:48:50 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 10 ===
[2025-03-16 12:49:04 root] (abq_llm_divide_blocks.py 283): INFO layer 10 loss_mean: 0.01991034299135208
[2025-03-16 12:49:04 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 11 ===
[2025-03-16 12:49:18 root] (abq_llm_divide_blocks.py 283): INFO layer 11 loss_mean: 0.022336026653647423
[2025-03-16 12:49:20 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 12 ===
[2025-03-16 12:49:35 root] (abq_llm_divide_blocks.py 283): INFO layer 12 loss_mean: 0.025731509551405907
[2025-03-16 12:49:36 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 13 ===
[2025-03-16 12:49:49 root] (abq_llm_divide_blocks.py 283): INFO layer 13 loss_mean: 0.030220109969377518
[2025-03-16 12:49:49 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 14 ===
[2025-03-16 12:50:03 root] (abq_llm_divide_blocks.py 283): INFO layer 14 loss_mean: 0.03975162282586098
[2025-03-16 12:50:03 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 15 ===
[2025-03-16 12:50:17 root] (abq_llm_divide_blocks.py 283): INFO layer 15 loss_mean: 0.0356915146112442
[2025-03-16 12:50:17 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 16 ===
[2025-03-16 12:50:31 root] (abq_llm_divide_blocks.py 283): INFO layer 16 loss_mean: 0.04594811052083969
[2025-03-16 12:50:31 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 17 ===
[2025-03-16 12:50:44 root] (abq_llm_divide_blocks.py 283): INFO layer 17 loss_mean: 0.05367680639028549
[2025-03-16 12:50:44 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 18 ===
[2025-03-16 12:50:58 root] (abq_llm_divide_blocks.py 283): INFO layer 18 loss_mean: 0.06273846328258514
[2025-03-16 12:50:58 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 19 ===
[2025-03-16 12:51:12 root] (abq_llm_divide_blocks.py 283): INFO layer 19 loss_mean: 0.06803489476442337
[2025-03-16 12:51:12 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 20 ===
[2025-03-16 12:51:26 root] (abq_llm_divide_blocks.py 283): INFO layer 20 loss_mean: 0.0823562741279602
[2025-03-16 12:51:26 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 21 ===
[2025-03-16 12:51:39 root] (abq_llm_divide_blocks.py 283): INFO layer 21 loss_mean: 0.08792723715305328
[2025-03-16 12:51:39 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 22 ===
[2025-03-16 12:51:53 root] (abq_llm_divide_blocks.py 283): INFO layer 22 loss_mean: 0.09362228214740753
[2025-03-16 12:51:53 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 23 ===
[2025-03-16 12:52:07 root] (abq_llm_divide_blocks.py 283): INFO layer 23 loss_mean: 0.08363264799118042
[2025-03-16 12:52:07 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 24 ===
[2025-03-16 12:52:21 root] (abq_llm_divide_blocks.py 283): INFO layer 24 loss_mean: 0.08683845400810242
[2025-03-16 12:52:22 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 25 ===
[2025-03-16 12:52:36 root] (abq_llm_divide_blocks.py 283): INFO layer 25 loss_mean: 0.09167320281267166
[2025-03-16 12:52:36 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 26 ===
[2025-03-16 12:52:50 root] (abq_llm_divide_blocks.py 283): INFO layer 26 loss_mean: 0.09354615956544876
[2025-03-16 12:52:50 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 27 ===
[2025-03-16 12:53:04 root] (abq_llm_divide_blocks.py 283): INFO layer 27 loss_mean: 0.09397178888320923
[2025-03-16 12:53:04 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 28 ===
[2025-03-16 12:53:17 root] (abq_llm_divide_blocks.py 283): INFO layer 28 loss_mean: 0.12101263552904129
[2025-03-16 12:53:17 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 29 ===
[2025-03-16 12:53:33 root] (abq_llm_divide_blocks.py 283): INFO layer 29 loss_mean: 0.09979680180549622
[2025-03-16 12:53:33 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 30 ===
[2025-03-16 12:53:47 root] (abq_llm_divide_blocks.py 283): INFO layer 30 loss_mean: 0.10218530893325806
[2025-03-16 12:53:47 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 31 ===
[2025-03-16 12:54:01 root] (abq_llm_divide_blocks.py 283): INFO layer 31 loss_mean: 0.11705990880727768
[2025-03-16 12:54:01 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 32 ===
[2025-03-16 12:54:15 root] (abq_llm_divide_blocks.py 283): INFO layer 32 loss_mean: 0.1194460466504097
[2025-03-16 12:54:15 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 33 ===
[2025-03-16 12:54:33 root] (abq_llm_divide_blocks.py 283): INFO layer 33 loss_mean: 0.14123192429542542
[2025-03-16 12:54:34 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 34 ===
[2025-03-16 12:54:48 root] (abq_llm_divide_blocks.py 283): INFO layer 34 loss_mean: 0.19288724660873413
[2025-03-16 12:54:48 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 35 ===
[2025-03-16 12:55:02 root] (abq_llm_divide_blocks.py 283): INFO layer 35 loss_mean: 0.21538779139518738
[2025-03-16 12:55:02 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 36 ===
[2025-03-16 12:55:16 root] (abq_llm_divide_blocks.py 283): INFO layer 36 loss_mean: 0.285514771938324
[2025-03-16 12:55:16 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 37 ===
[2025-03-16 12:55:30 root] (abq_llm_divide_blocks.py 283): INFO layer 37 loss_mean: 0.45537593960762024
[2025-03-16 12:55:30 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 38 ===
[2025-03-16 12:55:43 root] (abq_llm_divide_blocks.py 283): INFO layer 38 loss_mean: 1.735478162765503
[2025-03-16 12:55:43 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 39 ===
[2025-03-16 12:55:57 root] (abq_llm_divide_blocks.py 283): INFO layer 39 loss_mean: 9.131796836853027
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-1: size=1, error_sum=0.0022, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 1-2: size=1, error_sum=0.0048, min_similarity=0.9616, max_sensitivity_diff=1.2939
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-3: size=1, error_sum=1.3331, min_similarity=0.9820, max_sensitivity_diff=0.4528
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 3-5: size=2, error_sum=0.0164, min_similarity=0.9996, max_sensitivity_diff=0.0178
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 5-6: size=1, error_sum=0.0172, min_similarity=0.9997, max_sensitivity_diff=0.0687
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-7: size=1, error_sum=4.5615, min_similarity=0.9998, max_sensitivity_diff=0.0584
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 7-8: size=1, error_sum=0.0126, min_similarity=0.9993, max_sensitivity_diff=0.0206
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 8-10: size=2, error_sum=0.0299, min_similarity=0.9991, max_sensitivity_diff=0.0395
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 10-12: size=2, error_sum=0.0422, min_similarity=0.9999, max_sensitivity_diff=0.0020
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 12-14: size=2, error_sum=0.0560, min_similarity=0.9993, max_sensitivity_diff=0.0616
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 14-16: size=2, error_sum=0.0754, min_similarity=0.9999, max_sensitivity_diff=0.0261
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 16-18: size=2, error_sum=0.0996, min_similarity=0.9999, max_sensitivity_diff=0.0151
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 18-20: size=2, error_sum=0.1308, min_similarity=0.9998, max_sensitivity_diff=0.0138
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 20-22: size=2, error_sum=0.1703, min_similarity=0.9995, max_sensitivity_diff=0.0105
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 22-24: size=2, error_sum=0.1773, min_similarity=0.9997, max_sensitivity_diff=0.0255
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 24-26: size=2, error_sum=0.1785, min_similarity=0.9999, max_sensitivity_diff=0.0155
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 26-28: size=2, error_sum=0.1875, min_similarity=0.9999, max_sensitivity_diff=0.0203
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 28-30: size=2, error_sum=0.2208, min_similarity=0.9998, max_sensitivity_diff=0.0410
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-32: size=2, error_sum=0.2192, min_similarity=0.9999, max_sensitivity_diff=0.0159
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 32-34: size=2, error_sum=0.2607, min_similarity=1.0000, max_sensitivity_diff=0.0215
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 34-35: size=1, error_sum=0.1929, min_similarity=0.9999, max_sensitivity_diff=0.0033
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 35-36: size=1, error_sum=0.2154, min_similarity=0.9997, max_sensitivity_diff=0.0439
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 36-37: size=1, error_sum=0.2855, min_similarity=0.9996, max_sensitivity_diff=0.0200
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 37-38: size=1, error_sum=0.4554, min_similarity=0.9999, max_sensitivity_diff=0.0312
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 38-39: size=1, error_sum=1.7355, min_similarity=0.9997, max_sensitivity_diff=0.0176
[2025-03-16 12:55:57 quantize.utils_divide] (utils_divide.py 110): INFO Block 39-40: size=1, error_sum=9.1318, min_similarity=0.9977, max_sensitivity_diff=0.0548
[2025-03-16 12:55:57 root] (abq_llm_divide_blocks.py 299): INFO blocks: [(0, 1), (1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 8), (8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30), (30, 32), (32, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-16 12:55:57 root] (main_divide_blocks.py 389): INFO 632.7764966487885
