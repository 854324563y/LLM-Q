[2025-03-15 13:36:26 root] (main_divide_blocks.py 279): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide2/Llama-2-7b-hf-w4a4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.3, similarity_threshold=0.999, sensitivity_threshold=0.1, max_block_size=2, reload=False)
[2025-03-15 13:36:42 root] (main_divide_blocks.py 360): INFO === start quantization ===
[2025-03-15 13:36:42 root] (main_divide_blocks.py 366): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-15 13:36:42 root] (abq_llm_divide_blocks.py 66): INFO Starting ...
[2025-03-15 13:36:44 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-03-15 13:36:44 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 1.3323338668261253
[2025-03-15 13:36:44 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-03-15 13:36:48 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9360119700431824
[2025-03-15 13:36:48 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.05508659694876
[2025-03-15 13:36:48 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-03-15 13:36:52 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9443559306008475
[2025-03-15 13:36:52 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.30888820375715
[2025-03-15 13:36:52 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-03-15 13:36:56 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.998046338558197
[2025-03-15 13:36:57 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.228854945727757
[2025-03-15 13:36:57 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-03-15 13:37:01 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9985570311546326
[2025-03-15 13:37:01 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2994153703962055
[2025-03-15 13:37:01 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-03-15 13:37:05 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995824609483991
[2025-03-15 13:37:05 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3817724253450123
[2025-03-15 13:37:05 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-03-15 13:37:10 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.99713534116745
[2025-03-15 13:37:10 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.211996038470949
[2025-03-15 13:37:10 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-03-15 13:37:15 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999115807669503
[2025-03-15 13:37:15 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2239421299525675
[2025-03-15 13:37:15 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-03-15 13:37:19 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995706677436829
[2025-03-15 13:37:19 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.256654325553349
[2025-03-15 13:37:19 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-03-15 13:37:24 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.99960515328816
[2025-03-15 13:37:24 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2691674751894815
[2025-03-15 13:37:24 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-03-15 13:37:28 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998485020228794
[2025-03-15 13:37:28 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.278890650612967
[2025-03-15 13:37:28 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-03-15 13:37:32 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9980622359684536
[2025-03-15 13:37:33 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2135756126471926
[2025-03-15 13:37:33 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-03-15 13:37:37 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9990899392536708
[2025-03-15 13:37:37 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.256435146502086
[2025-03-15 13:37:37 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-03-15 13:37:41 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995642900466919
[2025-03-15 13:37:41 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2720475690705437
[2025-03-15 13:37:41 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-03-15 13:37:46 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998711517878941
[2025-03-15 13:37:46 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2464947044849395
[2025-03-15 13:37:46 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-03-15 13:37:50 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995314053126744
[2025-03-15 13:37:50 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.289426358257021
[2025-03-15 13:37:50 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-03-15 13:37:54 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9994118979998997
[2025-03-15 13:37:54 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.301100126334599
[2025-03-15 13:37:54 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-03-15 13:37:59 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998564379555839
[2025-03-15 13:37:59 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.289118843419211
[2025-03-15 13:37:59 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-03-15 13:38:03 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9993684376989093
[2025-03-15 13:38:03 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2899765474455696
[2025-03-15 13:38:03 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-03-15 13:38:08 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998666729245868
[2025-03-15 13:38:08 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2878581813403542
[2025-03-15 13:38:08 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-03-15 13:38:13 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998541985239301
[2025-03-15 13:38:13 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.304877885750362
[2025-03-15 13:38:13 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-03-15 13:38:17 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996138555662972
[2025-03-15 13:38:17 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.32213111604963
[2025-03-15 13:38:17 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-03-15 13:38:21 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997891357966832
[2025-03-15 13:38:22 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.341258045605251
[2025-03-15 13:38:22 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-03-15 13:38:26 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9992668543543134
[2025-03-15 13:38:26 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3750375355993
[2025-03-15 13:38:26 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-03-15 13:38:30 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9993422968047005
[2025-03-15 13:38:30 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3461611066545762
[2025-03-15 13:38:30 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-03-15 13:38:35 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9992739728518895
[2025-03-15 13:38:35 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3996644701276506
[2025-03-15 13:38:35 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-03-15 13:38:39 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997433253696987
[2025-03-15 13:38:39 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3983743224825176
[2025-03-15 13:38:39 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-03-15 13:38:43 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9990935495921544
[2025-03-15 13:38:44 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.464378094673157
[2025-03-15 13:38:44 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-03-15 13:38:48 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9994541917528424
[2025-03-15 13:38:48 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.452523902484349
[2025-03-15 13:38:48 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-03-15 13:38:52 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996153882571629
[2025-03-15 13:38:52 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.451868186678205
[2025-03-15 13:38:52 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-03-15 13:38:56 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995548810277667
[2025-03-15 13:38:56 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.547680197443281
[2025-03-15 13:38:56 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-03-15 13:39:01 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9964709963117327
[2025-03-15 13:39:01 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.5638455935886926
[2025-03-15 13:39:01 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 0 ===
[2025-03-15 13:39:12 root] (abq_llm_divide_blocks.py 283): INFO layer 0 loss_mean: 0.00021139730233699083
[2025-03-15 13:39:12 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 1 ===
[2025-03-15 13:39:22 root] (abq_llm_divide_blocks.py 283): INFO layer 1 loss_mean: 0.029242858290672302
[2025-03-15 13:39:22 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 2 ===
[2025-03-15 13:39:33 root] (abq_llm_divide_blocks.py 283): INFO layer 2 loss_mean: 0.0020485015120357275
[2025-03-15 13:39:33 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 3 ===
[2025-03-15 13:39:44 root] (abq_llm_divide_blocks.py 283): INFO layer 3 loss_mean: 0.002238768618553877
[2025-03-15 13:39:44 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 4 ===
[2025-03-15 13:39:54 root] (abq_llm_divide_blocks.py 283): INFO layer 4 loss_mean: 0.0015604542568325996
[2025-03-15 13:39:54 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 5 ===
[2025-03-15 13:40:05 root] (abq_llm_divide_blocks.py 283): INFO layer 5 loss_mean: 0.002369323978200555
[2025-03-15 13:40:05 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 6 ===
[2025-03-15 13:40:17 root] (abq_llm_divide_blocks.py 283): INFO layer 6 loss_mean: 0.007093082647770643
[2025-03-15 13:40:17 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 7 ===
[2025-03-15 13:40:27 root] (abq_llm_divide_blocks.py 283): INFO layer 7 loss_mean: 0.007695554755628109
[2025-03-15 13:40:27 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 8 ===
[2025-03-15 13:40:38 root] (abq_llm_divide_blocks.py 283): INFO layer 8 loss_mean: 0.00911157950758934
[2025-03-15 13:40:38 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 9 ===
[2025-03-15 13:40:49 root] (abq_llm_divide_blocks.py 283): INFO layer 9 loss_mean: 0.008946052752435207
[2025-03-15 13:40:49 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 10 ===
[2025-03-15 13:41:00 root] (abq_llm_divide_blocks.py 283): INFO layer 10 loss_mean: 0.00989164412021637
[2025-03-15 13:41:00 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 11 ===
[2025-03-15 13:41:11 root] (abq_llm_divide_blocks.py 283): INFO layer 11 loss_mean: 0.009533513337373734
[2025-03-15 13:41:11 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 12 ===
[2025-03-15 13:41:22 root] (abq_llm_divide_blocks.py 283): INFO layer 12 loss_mean: 0.00902614276856184
[2025-03-15 13:41:22 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 13 ===
[2025-03-15 13:41:32 root] (abq_llm_divide_blocks.py 283): INFO layer 13 loss_mean: 0.011870363727211952
[2025-03-15 13:41:32 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 14 ===
[2025-03-15 13:41:43 root] (abq_llm_divide_blocks.py 283): INFO layer 14 loss_mean: 0.012384077534079552
[2025-03-15 13:41:43 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 15 ===
[2025-03-15 13:41:54 root] (abq_llm_divide_blocks.py 283): INFO layer 15 loss_mean: 0.017059123143553734
[2025-03-15 13:41:54 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 16 ===
[2025-03-15 13:42:05 root] (abq_llm_divide_blocks.py 283): INFO layer 16 loss_mean: 0.019212007522583008
[2025-03-15 13:42:05 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 17 ===
[2025-03-15 13:42:16 root] (abq_llm_divide_blocks.py 283): INFO layer 17 loss_mean: 0.020454565063118935
[2025-03-15 13:42:16 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 18 ===
[2025-03-15 13:42:27 root] (abq_llm_divide_blocks.py 283): INFO layer 18 loss_mean: 0.02325049601495266
[2025-03-15 13:42:27 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 19 ===
[2025-03-15 13:42:38 root] (abq_llm_divide_blocks.py 283): INFO layer 19 loss_mean: 0.02166445553302765
[2025-03-15 13:42:38 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 20 ===
[2025-03-15 13:42:49 root] (abq_llm_divide_blocks.py 283): INFO layer 20 loss_mean: 0.022523287683725357
[2025-03-15 13:42:49 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 21 ===
[2025-03-15 13:42:59 root] (abq_llm_divide_blocks.py 283): INFO layer 21 loss_mean: 0.021931037306785583
[2025-03-15 13:42:59 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 22 ===
[2025-03-15 13:43:10 root] (abq_llm_divide_blocks.py 283): INFO layer 22 loss_mean: 0.0259387344121933
[2025-03-15 13:43:10 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 23 ===
[2025-03-15 13:43:21 root] (abq_llm_divide_blocks.py 283): INFO layer 23 loss_mean: 0.023562829941511154
[2025-03-15 13:43:21 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 24 ===
[2025-03-15 13:43:32 root] (abq_llm_divide_blocks.py 283): INFO layer 24 loss_mean: 0.02637310139834881
[2025-03-15 13:43:32 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 25 ===
[2025-03-15 13:43:43 root] (abq_llm_divide_blocks.py 283): INFO layer 25 loss_mean: 0.03390054404735565
[2025-03-15 13:43:43 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 26 ===
[2025-03-15 13:43:54 root] (abq_llm_divide_blocks.py 283): INFO layer 26 loss_mean: 0.039924174547195435
[2025-03-15 13:43:54 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 27 ===
[2025-03-15 13:44:05 root] (abq_llm_divide_blocks.py 283): INFO layer 27 loss_mean: 0.04188976436853409
[2025-03-15 13:44:05 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 28 ===
[2025-03-15 13:44:15 root] (abq_llm_divide_blocks.py 283): INFO layer 28 loss_mean: 0.058319635689258575
[2025-03-15 13:44:15 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 29 ===
[2025-03-15 13:44:26 root] (abq_llm_divide_blocks.py 283): INFO layer 29 loss_mean: 0.08634884655475616
[2025-03-15 13:44:26 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 30 ===
[2025-03-15 13:44:37 root] (abq_llm_divide_blocks.py 283): INFO layer 30 loss_mean: 0.2876339256763458
[2025-03-15 13:44:37 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 31 ===
[2025-03-15 13:44:48 root] (abq_llm_divide_blocks.py 283): INFO layer 31 loss_mean: 1.0892561674118042
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-1: size=1, error_sum=0.0002, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 1-2: size=1, error_sum=0.0292, min_similarity=0.9360, max_sensitivity_diff=0.7228
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-3: size=1, error_sum=0.0020, min_similarity=0.9444, max_sensitivity_diff=0.2538
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 3-4: size=1, error_sum=0.0022, min_similarity=0.9980, max_sensitivity_diff=0.0800
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 4-6: size=2, error_sum=0.0039, min_similarity=0.9996, max_sensitivity_diff=0.0824
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-8: size=2, error_sum=0.0148, min_similarity=0.9999, max_sensitivity_diff=0.0119
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 8-10: size=2, error_sum=0.0181, min_similarity=0.9996, max_sensitivity_diff=0.0125
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 10-11: size=1, error_sum=0.0099, min_similarity=0.9998, max_sensitivity_diff=0.0097
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 11-13: size=2, error_sum=0.0186, min_similarity=0.9991, max_sensitivity_diff=0.0429
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 13-15: size=2, error_sum=0.0243, min_similarity=0.9999, max_sensitivity_diff=0.0256
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 15-17: size=2, error_sum=0.0363, min_similarity=0.9994, max_sensitivity_diff=0.0117
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 17-19: size=2, error_sum=0.0437, min_similarity=0.9994, max_sensitivity_diff=0.0009
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 19-21: size=2, error_sum=0.0442, min_similarity=0.9999, max_sensitivity_diff=0.0170
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 21-23: size=2, error_sum=0.0479, min_similarity=0.9998, max_sensitivity_diff=0.0191
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 23-25: size=2, error_sum=0.0499, min_similarity=0.9993, max_sensitivity_diff=0.0289
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 25-27: size=2, error_sum=0.0738, min_similarity=0.9997, max_sensitivity_diff=0.0013
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 27-29: size=2, error_sum=0.1002, min_similarity=0.9995, max_sensitivity_diff=0.0119
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 29-30: size=1, error_sum=0.0863, min_similarity=0.9996, max_sensitivity_diff=0.0007
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-31: size=1, error_sum=0.2876, min_similarity=0.9996, max_sensitivity_diff=0.0958
[2025-03-15 13:44:48 quantize.utils_divide] (utils_divide.py 110): INFO Block 31-32: size=1, error_sum=1.0893, min_similarity=0.9965, max_sensitivity_diff=0.0162
[2025-03-15 13:44:48 root] (abq_llm_divide_blocks.py 299): INFO blocks: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 6), (6, 8), (8, 10), (10, 11), (11, 13), (13, 15), (15, 17), (17, 19), (19, 21), (21, 23), (23, 25), (25, 27), (27, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-15 13:44:48 root] (main_divide_blocks.py 389): INFO 486.24892926216125
