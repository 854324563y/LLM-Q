[2025-03-17 13:10:28 root] (main_divide_blocks.py 279): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-divide4/Llama-2-7b-hf-w4a4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.2, similarity_threshold=0.999, sensitivity_threshold=0.1, max_block_size=4, reload=False)
[2025-03-17 13:13:51 root] (main_divide_blocks.py 360): INFO === start quantization ===
[2025-03-17 13:13:51 root] (main_divide_blocks.py 366): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-17 13:13:51 root] (abq_llm_divide_blocks.py 66): INFO Starting ...
[2025-03-17 13:13:53 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-03-17 13:13:53 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 1.3323338668261253
[2025-03-17 13:13:53 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-03-17 13:13:57 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9360119700431824
[2025-03-17 13:13:57 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.05508659694876
[2025-03-17 13:13:57 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-03-17 13:14:01 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9443559306008475
[2025-03-17 13:14:01 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.30888820375715
[2025-03-17 13:14:01 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-03-17 13:14:05 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.998046338558197
[2025-03-17 13:14:05 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.228854945727757
[2025-03-17 13:14:05 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-03-17 13:14:09 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9985570311546326
[2025-03-17 13:14:10 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2994153703962055
[2025-03-17 13:14:10 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-03-17 13:14:13 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995824609483991
[2025-03-17 13:14:14 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3817724253450123
[2025-03-17 13:14:14 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-03-17 13:14:18 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.99713534116745
[2025-03-17 13:14:18 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.211996038470949
[2025-03-17 13:14:18 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-03-17 13:14:22 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9999115807669503
[2025-03-17 13:14:22 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2239421299525675
[2025-03-17 13:14:22 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-03-17 13:14:26 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995706677436829
[2025-03-17 13:14:27 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.256654325553349
[2025-03-17 13:14:27 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-03-17 13:14:30 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.99960515328816
[2025-03-17 13:14:30 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2691674751894815
[2025-03-17 13:14:30 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-03-17 13:14:33 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998485020228794
[2025-03-17 13:14:34 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.278890650612967
[2025-03-17 13:14:34 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-03-17 13:14:38 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9980622359684536
[2025-03-17 13:14:38 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2135756126471926
[2025-03-17 13:14:38 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-03-17 13:14:41 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9990899392536708
[2025-03-17 13:14:41 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.256435146502086
[2025-03-17 13:14:41 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-03-17 13:14:45 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995642900466919
[2025-03-17 13:14:45 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2720475690705437
[2025-03-17 13:14:45 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-03-17 13:14:49 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998711517878941
[2025-03-17 13:14:49 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2464947044849395
[2025-03-17 13:14:49 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-03-17 13:14:53 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995314053126744
[2025-03-17 13:14:53 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.289426358257021
[2025-03-17 13:14:53 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-03-17 13:14:57 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9994118979998997
[2025-03-17 13:14:57 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.301100126334599
[2025-03-17 13:14:57 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-03-17 13:15:02 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998564379555839
[2025-03-17 13:15:03 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.289118843419211
[2025-03-17 13:15:03 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-03-17 13:15:07 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9993684376989093
[2025-03-17 13:15:08 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2899765474455696
[2025-03-17 13:15:08 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-03-17 13:15:13 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998666729245868
[2025-03-17 13:15:14 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.2878581813403542
[2025-03-17 13:15:14 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-03-17 13:15:18 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9998541985239301
[2025-03-17 13:15:19 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.304877885750362
[2025-03-17 13:15:19 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-03-17 13:15:22 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996138555662972
[2025-03-17 13:15:23 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.32213111604963
[2025-03-17 13:15:23 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-03-17 13:15:27 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997891357966832
[2025-03-17 13:15:27 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.341258045605251
[2025-03-17 13:15:27 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-03-17 13:15:31 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9992668543543134
[2025-03-17 13:15:31 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3750375355993
[2025-03-17 13:15:31 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-03-17 13:15:35 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9993422968047005
[2025-03-17 13:15:35 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3461611066545762
[2025-03-17 13:15:35 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-03-17 13:15:39 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9992739728518895
[2025-03-17 13:15:39 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3996644701276506
[2025-03-17 13:15:39 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-03-17 13:15:43 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9997433253696987
[2025-03-17 13:15:44 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.3983743224825176
[2025-03-17 13:15:44 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-03-17 13:15:48 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9990935495921544
[2025-03-17 13:15:48 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.464378094673157
[2025-03-17 13:15:48 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-03-17 13:15:52 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9994541917528424
[2025-03-17 13:15:52 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.452523902484349
[2025-03-17 13:15:52 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-03-17 13:15:56 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9996153882571629
[2025-03-17 13:15:56 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.451868186678205
[2025-03-17 13:15:56 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-03-17 13:16:00 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9995548810277667
[2025-03-17 13:16:00 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.547680197443281
[2025-03-17 13:16:00 root] (abq_llm_divide_blocks.py 222): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-03-17 13:16:04 quantize.utils_divide] (utils_divide.py 234): INFO Layer similarity: 0.9964709963117327
[2025-03-17 13:16:04 quantize.utils_divide] (utils_divide.py 275): INFO Hessian sensitivity: 2.5638455935886926
[2025-03-17 13:16:04 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 0 ===
[2025-03-17 13:16:15 root] (abq_llm_divide_blocks.py 283): INFO layer 0 loss_mean: 0.00021139730233699083
[2025-03-17 13:16:15 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 1 ===
[2025-03-17 13:16:26 root] (abq_llm_divide_blocks.py 283): INFO layer 1 loss_mean: 0.029242858290672302
[2025-03-17 13:16:26 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 2 ===
[2025-03-17 13:16:36 root] (abq_llm_divide_blocks.py 283): INFO layer 2 loss_mean: 0.0020485015120357275
[2025-03-17 13:16:36 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 3 ===
[2025-03-17 13:16:47 root] (abq_llm_divide_blocks.py 283): INFO layer 3 loss_mean: 0.002238768618553877
[2025-03-17 13:16:47 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 4 ===
[2025-03-17 13:16:57 root] (abq_llm_divide_blocks.py 283): INFO layer 4 loss_mean: 0.0015604542568325996
[2025-03-17 13:16:57 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 5 ===
[2025-03-17 13:17:08 root] (abq_llm_divide_blocks.py 283): INFO layer 5 loss_mean: 0.002369323978200555
[2025-03-17 13:17:08 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 6 ===
[2025-03-17 13:17:19 root] (abq_llm_divide_blocks.py 283): INFO layer 6 loss_mean: 0.007093082647770643
[2025-03-17 13:17:19 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 7 ===
[2025-03-17 13:17:30 root] (abq_llm_divide_blocks.py 283): INFO layer 7 loss_mean: 0.007695554755628109
[2025-03-17 13:17:30 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 8 ===
[2025-03-17 13:17:41 root] (abq_llm_divide_blocks.py 283): INFO layer 8 loss_mean: 0.00911157950758934
[2025-03-17 13:17:41 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 9 ===
[2025-03-17 13:17:51 root] (abq_llm_divide_blocks.py 283): INFO layer 9 loss_mean: 0.008946052752435207
[2025-03-17 13:17:51 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 10 ===
[2025-03-17 13:18:02 root] (abq_llm_divide_blocks.py 283): INFO layer 10 loss_mean: 0.00989164412021637
[2025-03-17 13:18:02 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 11 ===
[2025-03-17 13:18:13 root] (abq_llm_divide_blocks.py 283): INFO layer 11 loss_mean: 0.009533513337373734
[2025-03-17 13:18:13 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 12 ===
[2025-03-17 13:18:24 root] (abq_llm_divide_blocks.py 283): INFO layer 12 loss_mean: 0.00902614276856184
[2025-03-17 13:18:24 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 13 ===
[2025-03-17 13:18:34 root] (abq_llm_divide_blocks.py 283): INFO layer 13 loss_mean: 0.011870363727211952
[2025-03-17 13:18:34 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 14 ===
[2025-03-17 13:18:45 root] (abq_llm_divide_blocks.py 283): INFO layer 14 loss_mean: 0.012384077534079552
[2025-03-17 13:18:45 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 15 ===
[2025-03-17 13:18:56 root] (abq_llm_divide_blocks.py 283): INFO layer 15 loss_mean: 0.017059123143553734
[2025-03-17 13:18:56 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 16 ===
[2025-03-17 13:19:07 root] (abq_llm_divide_blocks.py 283): INFO layer 16 loss_mean: 0.019212007522583008
[2025-03-17 13:19:07 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 17 ===
[2025-03-17 13:19:17 root] (abq_llm_divide_blocks.py 283): INFO layer 17 loss_mean: 0.020454565063118935
[2025-03-17 13:19:17 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 18 ===
[2025-03-17 13:19:28 root] (abq_llm_divide_blocks.py 283): INFO layer 18 loss_mean: 0.02325049601495266
[2025-03-17 13:19:28 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 19 ===
[2025-03-17 13:19:39 root] (abq_llm_divide_blocks.py 283): INFO layer 19 loss_mean: 0.02166445553302765
[2025-03-17 13:19:39 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 20 ===
[2025-03-17 13:19:50 root] (abq_llm_divide_blocks.py 283): INFO layer 20 loss_mean: 0.022523287683725357
[2025-03-17 13:19:50 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 21 ===
[2025-03-17 13:20:00 root] (abq_llm_divide_blocks.py 283): INFO layer 21 loss_mean: 0.021931037306785583
[2025-03-17 13:20:01 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 22 ===
[2025-03-17 13:20:11 root] (abq_llm_divide_blocks.py 283): INFO layer 22 loss_mean: 0.0259387344121933
[2025-03-17 13:20:12 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 23 ===
[2025-03-17 13:20:23 root] (abq_llm_divide_blocks.py 283): INFO layer 23 loss_mean: 0.023562829941511154
[2025-03-17 13:20:23 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 24 ===
[2025-03-17 13:20:34 root] (abq_llm_divide_blocks.py 283): INFO layer 24 loss_mean: 0.02637310139834881
[2025-03-17 13:20:34 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 25 ===
[2025-03-17 13:20:45 root] (abq_llm_divide_blocks.py 283): INFO layer 25 loss_mean: 0.03390054404735565
[2025-03-17 13:20:45 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 26 ===
[2025-03-17 13:20:56 root] (abq_llm_divide_blocks.py 283): INFO layer 26 loss_mean: 0.039924174547195435
[2025-03-17 13:20:56 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 27 ===
[2025-03-17 13:21:06 root] (abq_llm_divide_blocks.py 283): INFO layer 27 loss_mean: 0.04188976436853409
[2025-03-17 13:21:06 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 28 ===
[2025-03-17 13:21:17 root] (abq_llm_divide_blocks.py 283): INFO layer 28 loss_mean: 0.058319635689258575
[2025-03-17 13:21:17 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 29 ===
[2025-03-17 13:21:28 root] (abq_llm_divide_blocks.py 283): INFO layer 29 loss_mean: 0.08634884655475616
[2025-03-17 13:21:28 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 30 ===
[2025-03-17 13:21:39 root] (abq_llm_divide_blocks.py 283): INFO layer 30 loss_mean: 0.2876339256763458
[2025-03-17 13:21:39 root] (abq_llm_divide_blocks.py 237): INFO === Start quantize layer 31 ===
[2025-03-17 13:21:49 root] (abq_llm_divide_blocks.py 283): INFO layer 31 loss_mean: 1.0892561674118042
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-2: size=2, error_sum=0.0295, min_similarity=0.9360, max_sensitivity_diff=0.7228
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-6: size=4, error_sum=0.0082, min_similarity=0.9980, max_sensitivity_diff=0.0824
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-10: size=4, error_sum=0.0328, min_similarity=0.9996, max_sensitivity_diff=0.0327
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 10-14: size=4, error_sum=0.0403, min_similarity=0.9981, max_sensitivity_diff=0.0653
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 14-18: size=4, error_sum=0.0691, min_similarity=0.9994, max_sensitivity_diff=0.0429
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 18-22: size=4, error_sum=0.0894, min_similarity=0.9996, max_sensitivity_diff=0.0173
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 22-26: size=4, error_sum=0.1098, min_similarity=0.9993, max_sensitivity_diff=0.0535
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 26-29: size=3, error_sum=0.1401, min_similarity=0.9991, max_sensitivity_diff=0.0660
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 29-30: size=1, error_sum=0.0863, min_similarity=0.9996, max_sensitivity_diff=0.0007
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-31: size=1, error_sum=0.2876, min_similarity=0.9996, max_sensitivity_diff=0.0958
[2025-03-17 13:21:49 quantize.utils_divide] (utils_divide.py 110): INFO Block 31-32: size=1, error_sum=1.0893, min_similarity=0.9965, max_sensitivity_diff=0.0162
[2025-03-17 13:21:49 root] (abq_llm_divide_blocks.py 299): INFO blocks: [(0, 2), (2, 6), (6, 10), (10, 14), (14, 18), (18, 22), (22, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-03-17 13:21:50 root] (main_divide_blocks.py 389): INFO 479.1872763633728
