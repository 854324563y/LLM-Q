[2025-03-12 07:30:49 root] (main_calibration_5_1.py 275): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration-5-1/Llama-2-7b-hf-w4a4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, look_ahead_layers=2, analyze_per_layer_mse=True)
[2025-03-12 07:30:58 root] (main_calibration_5_1.py 342): INFO === start quantization ===
[2025-03-12 07:30:58 root] (main_calibration_5_1.py 348): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-12 07:30:58 root] (abq_llm_calibration_5_1.py 62): INFO Starting ...
[2025-03-12 07:31:00 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 0 ===
[2025-03-12 07:31:04 root] (abq_llm_calibration_5_1.py 278): INFO use compensation vector
[2025-03-12 07:31:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 0 loss:0.12200230360031128 norm:nan max memory_allocated 29363.18115234375 
[2025-03-12 07:32:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 1 loss:0.0703657940030098 norm:0.23511429131031036 max memory_allocated 29363.18115234375 
[2025-03-12 07:32:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 2 loss:0.03550206124782562 norm:0.09358212351799011 max memory_allocated 29363.18115234375 
[2025-03-12 07:33:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 3 loss:0.032308757305145264 norm:0.11056114733219147 max memory_allocated 29363.18115234375 
[2025-03-12 07:34:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 4 loss:0.07402416318655014 norm:0.29696738719940186 max memory_allocated 29363.18115234375 
[2025-03-12 07:34:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 5 loss:0.032031770795583725 norm:0.08910106122493744 max memory_allocated 29363.18115234375 
[2025-03-12 07:35:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 6 loss:0.028517717495560646 norm:0.08518104255199432 max memory_allocated 29363.18115234375 
[2025-03-12 07:36:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 7 loss:0.02516872063279152 norm:0.06859540194272995 max memory_allocated 29363.18115234375 
[2025-03-12 07:36:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 8 loss:0.023547593504190445 norm:0.06367714703083038 max memory_allocated 29363.18115234375 
[2025-03-12 07:37:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 9 loss:0.03074045665562153 norm:0.09309933334589005 max memory_allocated 29363.18115234375 
[2025-03-12 07:37:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 10 loss:1.2756446599960327 norm:7.728400230407715 max memory_allocated 29363.18115234375 
[2025-03-12 07:38:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 11 loss:0.3178137540817261 norm:1.1027106046676636 max memory_allocated 29363.18115234375 
[2025-03-12 07:39:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 12 loss:0.09141895174980164 norm:0.22100168466567993 max memory_allocated 29363.18115234375 
[2025-03-12 07:39:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 13 loss:0.03842659294605255 norm:0.05647953972220421 max memory_allocated 29363.18115234375 
[2025-03-12 07:40:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 14 loss:0.030246660113334656 norm:0.04871503263711929 max memory_allocated 29363.18115234375 
[2025-03-12 07:41:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 15 loss:0.026612242683768272 norm:0.046720050275325775 max memory_allocated 29363.18115234375 
[2025-03-12 07:41:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 16 loss:0.02505498006939888 norm:0.0498528927564621 max memory_allocated 29363.18115234375 
[2025-03-12 07:42:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 17 loss:0.023546922951936722 norm:0.04965154826641083 max memory_allocated 29363.18115234375 
[2025-03-12 07:42:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 18 loss:0.022587385028600693 norm:0.05121869966387749 max memory_allocated 29363.18115234375 
[2025-03-12 07:43:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 0 iter 19 loss:0.022160770371556282 norm:0.051280755549669266 max memory_allocated 29363.18115234375 
[2025-03-12 07:43:33 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 0 ===
[2025-03-12 07:52:04 root] (abq_llm_calibration_5_1.py 511): INFO Layer 0 Results:
[2025-03-12 07:52:04 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 07:52:04 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.049724
[2025-03-12 07:52:04 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996783
[2025-03-12 07:52:13 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 1 ===
[2025-03-12 07:52:15 root] (abq_llm_calibration_5_1.py 278): INFO use compensation vector
[2025-03-12 07:53:30 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 0 loss:0.1269918531179428 norm:0.15240511298179626 max memory_allocated 55791.26513671875 
[2025-03-12 07:54:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 1 loss:0.08743369579315186 norm:0.08058889210224152 max memory_allocated 55791.26513671875 
[2025-03-12 07:56:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 2 loss:0.07241574674844742 norm:0.06221289932727814 max memory_allocated 55791.26513671875 
[2025-03-12 07:57:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 3 loss:0.06483499705791473 norm:0.050678785890340805 max memory_allocated 55791.26513671875 
[2025-03-12 07:58:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 4 loss:0.06072869524359703 norm:0.04264042526483536 max memory_allocated 55791.26513671875 
[2025-03-12 07:59:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 5 loss:0.05656660720705986 norm:0.0353190079331398 max memory_allocated 55791.26513671875 
[2025-03-12 08:00:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 6 loss:0.05424456670880318 norm:0.03322172909975052 max memory_allocated 55791.26513671875 
[2025-03-12 08:02:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 7 loss:0.05246923863887787 norm:0.031265806406736374 max memory_allocated 55791.26513671875 
[2025-03-12 08:03:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 8 loss:0.05133552476763725 norm:0.03105952963232994 max memory_allocated 55791.26513671875 
[2025-03-12 08:04:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 9 loss:0.05070503056049347 norm:0.028258536010980606 max memory_allocated 55791.26513671875 
[2025-03-12 08:05:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 10 loss:0.04977632313966751 norm:0.02777392417192459 max memory_allocated 55791.26513671875 
[2025-03-12 08:07:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 11 loss:0.050481945276260376 norm:0.025129614397883415 max memory_allocated 55791.26513671875 
[2025-03-12 08:08:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 12 loss:0.049978047609329224 norm:0.023934772238135338 max memory_allocated 55791.26513671875 
[2025-03-12 08:09:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 13 loss:0.04929346218705177 norm:0.02294597029685974 max memory_allocated 55791.26513671875 
[2025-03-12 08:10:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 14 loss:0.04933520779013634 norm:0.022783879190683365 max memory_allocated 55791.26513671875 
[2025-03-12 08:12:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 15 loss:0.04882477596402168 norm:0.021364321932196617 max memory_allocated 55791.26513671875 
[2025-03-12 08:13:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 16 loss:0.04866962134838104 norm:0.021016379818320274 max memory_allocated 55791.26513671875 
[2025-03-12 08:14:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 17 loss:0.04809901490807533 norm:0.020446937531232834 max memory_allocated 55791.26513671875 
[2025-03-12 08:15:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 18 loss:0.04837362840771675 norm:0.020378857851028442 max memory_allocated 55791.26513671875 
[2025-03-12 08:17:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 1 iter 19 loss:0.04747512936592102 norm:0.018970567733049393 max memory_allocated 55791.26513671875 
[2025-03-12 08:17:12 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 1 ===
[2025-03-12 08:25:23 root] (abq_llm_calibration_5_1.py 511): INFO Layer 1 Results:
[2025-03-12 08:25:23 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 08:25:23 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.089083
[2025-03-12 08:25:23 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994435
[2025-03-12 08:25:32 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 2 ===
[2025-03-12 08:25:35 root] (abq_llm_calibration_5_1.py 278): INFO use compensation vector
[2025-03-12 08:26:50 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 0 loss:0.09827274829149246 norm:0.07846812903881073 max memory_allocated 55791.26513671875 
[2025-03-12 08:28:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 1 loss:0.08364659547805786 norm:0.05531245097517967 max memory_allocated 55791.26513671875 
[2025-03-12 08:29:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 2 loss:0.07763984054327011 norm:0.04594043642282486 max memory_allocated 55791.26513671875 
[2025-03-12 08:30:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 3 loss:0.07286975532770157 norm:0.036430537700653076 max memory_allocated 55791.26513671875 
[2025-03-12 08:31:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 4 loss:0.07046496868133545 norm:0.032363180071115494 max memory_allocated 55791.26513671875 
[2025-03-12 08:33:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 5 loss:0.06864678859710693 norm:0.026760214939713478 max memory_allocated 55791.26513671875 
[2025-03-12 08:34:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 6 loss:0.06753285229206085 norm:0.023477133363485336 max memory_allocated 55791.26513671875 
[2025-03-12 08:35:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 7 loss:0.06678944826126099 norm:0.021947966888546944 max memory_allocated 55791.26513671875 
[2025-03-12 08:36:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 8 loss:0.06596746295690536 norm:0.01955174095928669 max memory_allocated 55791.26513671875 
[2025-03-12 08:38:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 9 loss:0.06525716185569763 norm:0.01737225241959095 max memory_allocated 55791.26513671875 
[2025-03-12 08:39:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 10 loss:0.0647292286157608 norm:0.01585899293422699 max memory_allocated 55791.26513671875 
[2025-03-12 08:40:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 11 loss:0.06436628848314285 norm:0.015391574241220951 max memory_allocated 55791.26513671875 
[2025-03-12 08:41:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 12 loss:0.06390498578548431 norm:0.01432892307639122 max memory_allocated 55791.26513671875 
[2025-03-12 08:43:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 13 loss:0.06354070454835892 norm:0.014051837846636772 max memory_allocated 55791.26513671875 
[2025-03-12 08:44:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 14 loss:0.06325143575668335 norm:0.013035506941378117 max memory_allocated 55791.26513671875 
[2025-03-12 08:45:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 15 loss:0.06305865943431854 norm:0.011935252696275711 max memory_allocated 55791.26513671875 
[2025-03-12 08:46:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 16 loss:0.06271881610155106 norm:0.011022201739251614 max memory_allocated 55791.26513671875 
[2025-03-12 08:48:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 17 loss:0.06252852827310562 norm:0.010454867035150528 max memory_allocated 55791.26513671875 
[2025-03-12 08:49:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 18 loss:0.062366385012865067 norm:0.009986954741179943 max memory_allocated 55791.26513671875 
[2025-03-12 08:50:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 2 iter 19 loss:0.06227942556142807 norm:0.00962081179022789 max memory_allocated 55791.26513671875 
[2025-03-12 08:50:33 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 2 ===
[2025-03-12 08:58:15 root] (abq_llm_calibration_5_1.py 511): INFO Layer 2 Results:
[2025-03-12 08:58:15 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 08:58:15 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.069000
[2025-03-12 08:58:15 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.995720
[2025-03-12 08:58:24 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 3 ===
[2025-03-12 08:59:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 0 loss:0.115828737616539 norm:0.014475591480731964 max memory_allocated 55791.26513671875 
[2025-03-12 09:00:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 1 loss:0.10207332670688629 norm:0.005028955172747374 max memory_allocated 55791.26513671875 
[2025-03-12 09:02:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 2 loss:0.09179551899433136 norm:0.002344616921618581 max memory_allocated 55791.26513671875 
[2025-03-12 09:03:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 3 loss:0.08733376115560532 norm:0.0014964542351663113 max memory_allocated 55791.26513671875 
[2025-03-12 09:04:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 4 loss:0.08522842824459076 norm:0.0012089047813788056 max memory_allocated 55791.26513671875 
[2025-03-12 09:05:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 5 loss:0.084014892578125 norm:0.000994207221083343 max memory_allocated 55791.26513671875 
[2025-03-12 09:07:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 6 loss:0.08303794264793396 norm:0.0008743866346776485 max memory_allocated 55791.26513671875 
[2025-03-12 09:08:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 7 loss:0.08245086669921875 norm:0.0008251764811575413 max memory_allocated 55791.26513671875 
[2025-03-12 09:09:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 8 loss:0.08203190565109253 norm:0.0007777947466820478 max memory_allocated 55791.26513671875 
[2025-03-12 09:10:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 9 loss:0.0817229375243187 norm:0.0007503537926822901 max memory_allocated 55791.26513671875 
[2025-03-12 09:12:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 10 loss:0.0815143883228302 norm:0.0007456483435817063 max memory_allocated 55791.26513671875 
[2025-03-12 09:13:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 11 loss:0.0812443345785141 norm:0.0006726287538185716 max memory_allocated 55791.26513671875 
[2025-03-12 09:14:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 12 loss:0.0808839350938797 norm:0.0006432802183553576 max memory_allocated 55791.26513671875 
[2025-03-12 09:15:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 13 loss:0.0808146744966507 norm:0.0006306978175416589 max memory_allocated 55791.26513671875 
[2025-03-12 09:17:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 14 loss:0.08058595657348633 norm:0.0006192137952893972 max memory_allocated 55791.26513671875 
[2025-03-12 09:18:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 15 loss:0.08048253506422043 norm:0.0006022366578690708 max memory_allocated 55791.26513671875 
[2025-03-12 09:19:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 16 loss:0.08048175275325775 norm:0.0006077539292164147 max memory_allocated 55791.26513671875 
[2025-03-12 09:20:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 17 loss:0.08029811084270477 norm:0.0005741882487200201 max memory_allocated 55791.26513671875 
[2025-03-12 09:22:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 18 loss:0.08011607080698013 norm:0.0005612330860458314 max memory_allocated 55791.26513671875 
[2025-03-12 09:23:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 3 iter 19 loss:0.08018513768911362 norm:0.0005809235735796392 max memory_allocated 55791.26513671875 
[2025-03-12 09:23:25 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 3 ===
[2025-03-12 09:30:56 root] (abq_llm_calibration_5_1.py 511): INFO Layer 3 Results:
[2025-03-12 09:30:56 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 09:30:56 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.061153
[2025-03-12 09:30:56 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996129
[2025-03-12 09:31:04 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 4 ===
[2025-03-12 09:32:22 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 0 loss:0.14271098375320435 norm:0.026874156668782234 max memory_allocated 55791.26513671875 
[2025-03-12 09:33:36 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 1 loss:0.12572188675403595 norm:0.009378170594573021 max memory_allocated 55791.26513671875 
[2025-03-12 09:34:51 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 2 loss:0.11322937160730362 norm:0.00448339618742466 max memory_allocated 55791.26513671875 
[2025-03-12 09:36:05 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 3 loss:0.10735262930393219 norm:0.0028226813301444054 max memory_allocated 55791.26513671875 
[2025-03-12 09:37:20 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 4 loss:0.10434197634458542 norm:0.0022106613032519817 max memory_allocated 55791.26513671875 
[2025-03-12 09:38:35 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 5 loss:0.10237672924995422 norm:0.0018258525524288416 max memory_allocated 55791.26513671875 
[2025-03-12 09:39:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 6 loss:0.10092835128307343 norm:0.0016052208375185728 max memory_allocated 55791.26513671875 
[2025-03-12 09:41:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 7 loss:0.09996623545885086 norm:0.0015616832533851266 max memory_allocated 55791.26513671875 
[2025-03-12 09:42:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 8 loss:0.09917658567428589 norm:0.0012320077512413263 max memory_allocated 55791.26513671875 
[2025-03-12 09:43:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 9 loss:0.09832703322172165 norm:0.0010953070595860481 max memory_allocated 55791.26513671875 
[2025-03-12 09:44:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 10 loss:0.09764398634433746 norm:0.00101572647690773 max memory_allocated 55791.26513671875 
[2025-03-12 09:46:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 11 loss:0.09707950055599213 norm:0.0009086184436455369 max memory_allocated 55791.26513671875 
[2025-03-12 09:47:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 12 loss:0.09666004776954651 norm:0.0008124285377562046 max memory_allocated 55791.26513671875 
[2025-03-12 09:48:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 13 loss:0.09634891152381897 norm:0.0007697694236412644 max memory_allocated 55791.26513671875 
[2025-03-12 09:49:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 14 loss:0.09622637927532196 norm:0.0008234346751123667 max memory_allocated 55791.26513671875 
[2025-03-12 09:51:01 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 15 loss:0.09612703323364258 norm:0.0007775546400807798 max memory_allocated 55791.26513671875 
[2025-03-12 09:52:16 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 16 loss:0.09601448476314545 norm:0.0007625844446010888 max memory_allocated 55791.26513671875 
[2025-03-12 09:53:31 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 17 loss:0.09590190649032593 norm:0.0007268418557941914 max memory_allocated 55791.26513671875 
[2025-03-12 09:54:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 18 loss:0.09570431709289551 norm:0.0007066986872814596 max memory_allocated 55791.26513671875 
[2025-03-12 09:56:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 4 iter 19 loss:0.09559266269207001 norm:0.0007059054332785308 max memory_allocated 55791.26513671875 
[2025-03-12 09:56:01 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 4 ===
[2025-03-12 10:03:10 root] (abq_llm_calibration_5_1.py 511): INFO Layer 4 Results:
[2025-03-12 10:03:10 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 10:03:10 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.062445
[2025-03-12 10:03:10 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996044
[2025-03-12 10:03:18 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 5 ===
[2025-03-12 10:04:36 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 0 loss:0.1444137543439865 norm:0.00804851297289133 max memory_allocated 55791.26513671875 
[2025-03-12 10:05:51 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 1 loss:0.13391005992889404 norm:0.003612994449213147 max memory_allocated 55791.26513671875 
[2025-03-12 10:07:05 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 2 loss:0.12167157232761383 norm:0.0012806133599951863 max memory_allocated 55791.26513671875 
[2025-03-12 10:08:20 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 3 loss:0.11636123806238174 norm:0.0008377320482395589 max memory_allocated 55791.26513671875 
[2025-03-12 10:09:35 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 4 loss:0.11398906260728836 norm:0.0006890459335409105 max memory_allocated 55791.26513671875 
[2025-03-12 10:10:50 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 5 loss:0.1126871258020401 norm:0.0006258041830733418 max memory_allocated 55791.26513671875 
[2025-03-12 10:12:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 6 loss:0.11178912967443466 norm:0.0006030048243701458 max memory_allocated 55791.26513671875 
[2025-03-12 10:13:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 7 loss:0.11113950610160828 norm:0.0005641463794745505 max memory_allocated 55791.26513671875 
[2025-03-12 10:14:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 8 loss:0.11066756397485733 norm:0.0005421655951067805 max memory_allocated 55791.26513671875 
[2025-03-12 10:15:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 9 loss:0.11027118563652039 norm:0.0014593781670555472 max memory_allocated 55791.26513671875 
[2025-03-12 10:17:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 10 loss:0.10995404422283173 norm:0.0005369742284528911 max memory_allocated 55791.26513671875 
[2025-03-12 10:18:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 11 loss:0.10957995057106018 norm:0.0005234997952356935 max memory_allocated 55791.26513671875 
[2025-03-12 10:19:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 12 loss:0.10913673788309097 norm:0.0005162270390428603 max memory_allocated 55791.26513671875 
[2025-03-12 10:20:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 13 loss:0.10896633565425873 norm:0.0004999149241484702 max memory_allocated 55791.26513671875 
[2025-03-12 10:22:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 14 loss:0.10885049402713776 norm:0.0004934771568514407 max memory_allocated 55791.26513671875 
[2025-03-12 10:23:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 15 loss:0.1087198406457901 norm:0.0004980559460818768 max memory_allocated 55791.26513671875 
[2025-03-12 10:24:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 16 loss:0.10851754993200302 norm:0.0004811602411791682 max memory_allocated 55791.26513671875 
[2025-03-12 10:25:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 17 loss:0.10834016650915146 norm:0.0004736965347547084 max memory_allocated 55791.26513671875 
[2025-03-12 10:27:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 18 loss:0.1081724464893341 norm:0.00047387342783622444 max memory_allocated 55791.26513671875 
[2025-03-12 10:28:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 5 iter 19 loss:0.10813586413860321 norm:0.00046074282727204263 max memory_allocated 55791.26513671875 
[2025-03-12 10:28:18 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 5 ===
[2025-03-12 10:35:18 root] (abq_llm_calibration_5_1.py 511): INFO Layer 5 Results:
[2025-03-12 10:35:18 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 10:35:18 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.058919
[2025-03-12 10:35:18 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996251
[2025-03-12 10:35:27 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 6 ===
[2025-03-12 10:36:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 0 loss:0.17720156908035278 norm:0.018934983760118484 max memory_allocated 55791.26513671875 
[2025-03-12 10:37:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 1 loss:0.15392819046974182 norm:0.005621439311653376 max memory_allocated 55791.26513671875 
[2025-03-12 10:39:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 2 loss:0.1412765383720398 norm:0.00274658203125 max memory_allocated 55791.26513671875 
[2025-03-12 10:40:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 3 loss:0.13411885499954224 norm:0.0015834267251193523 max memory_allocated 55791.26513671875 
[2025-03-12 10:41:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 4 loss:0.13094206154346466 norm:0.0012407242320477962 max memory_allocated 55791.26513671875 
[2025-03-12 10:42:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 5 loss:0.12916451692581177 norm:0.0009578694589436054 max memory_allocated 55791.26513671875 
[2025-03-12 10:44:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 6 loss:0.12799394130706787 norm:0.0008441307581961155 max memory_allocated 55791.26513671875 
[2025-03-12 10:45:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 7 loss:0.12705574929714203 norm:0.0007601112592965364 max memory_allocated 55791.26513671875 
[2025-03-12 10:46:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 8 loss:0.12638446688652039 norm:0.0006960079772397876 max memory_allocated 55791.26513671875 
[2025-03-12 10:47:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 9 loss:0.1256471574306488 norm:0.0006470719818025827 max memory_allocated 55791.26513671875 
[2025-03-12 10:49:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 10 loss:0.12513601779937744 norm:0.0006165837403386831 max memory_allocated 55791.26513671875 
[2025-03-12 10:50:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 11 loss:0.12486866116523743 norm:0.0006146937957964838 max memory_allocated 55791.26513671875 
[2025-03-12 10:51:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 12 loss:0.12454989552497864 norm:0.0006051822565495968 max memory_allocated 55791.26513671875 
[2025-03-12 10:52:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 13 loss:0.12405435740947723 norm:0.000571827869862318 max memory_allocated 55791.26513671875 
[2025-03-12 10:54:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 14 loss:0.12376946210861206 norm:0.000544292270205915 max memory_allocated 55791.26513671875 
[2025-03-12 10:55:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 15 loss:0.12368734925985336 norm:0.0005457697552628815 max memory_allocated 55791.26513671875 
[2025-03-12 10:56:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 16 loss:0.1234649270772934 norm:0.0005456689395941794 max memory_allocated 55791.26513671875 
[2025-03-12 10:57:54 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 17 loss:0.12341325730085373 norm:0.0005500298575498164 max memory_allocated 55791.26513671875 
[2025-03-12 10:59:08 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 18 loss:0.12327849864959717 norm:0.0005444902344606817 max memory_allocated 55791.26513671875 
[2025-03-12 11:00:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 6 iter 19 loss:0.12306447327136993 norm:0.000538096996024251 max memory_allocated 55791.26513671875 
[2025-03-12 11:00:24 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 6 ===
[2025-03-12 11:07:03 root] (abq_llm_calibration_5_1.py 511): INFO Layer 6 Results:
[2025-03-12 11:07:03 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 11:07:03 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.063218
[2025-03-12 11:07:03 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.995959
[2025-03-12 11:07:12 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 7 ===
[2025-03-12 11:08:30 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 0 loss:0.19199028611183167 norm:0.01424640417098999 max memory_allocated 55791.26513671875 
[2025-03-12 11:09:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 1 loss:0.16854776442050934 norm:0.004678335040807724 max memory_allocated 55791.26513671875 
[2025-03-12 11:10:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 2 loss:0.154973566532135 norm:0.0022915471345186234 max memory_allocated 55791.26513671875 
[2025-03-12 11:12:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 3 loss:0.1479976773262024 norm:0.0012728648725897074 max memory_allocated 55791.26513671875 
[2025-03-12 11:13:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 4 loss:0.14472004771232605 norm:0.000890046707354486 max memory_allocated 55791.26513671875 
[2025-03-12 11:14:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 5 loss:0.14311917126178741 norm:0.0007928032428026199 max memory_allocated 55791.26513671875 
[2025-03-12 11:15:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 6 loss:0.14202912151813507 norm:0.0007127129356376827 max memory_allocated 55791.26513671875 
[2025-03-12 11:17:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 7 loss:0.14133930206298828 norm:0.0006757939700037241 max memory_allocated 55791.26513671875 
[2025-03-12 11:18:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 8 loss:0.14065098762512207 norm:0.0006406144239008427 max memory_allocated 55791.26513671875 
[2025-03-12 11:19:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 9 loss:0.14011433720588684 norm:0.0005950600025244057 max memory_allocated 55791.26513671875 
[2025-03-12 11:20:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 10 loss:0.13969583809375763 norm:0.0005909710889682174 max memory_allocated 55791.26513671875 
[2025-03-12 11:22:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 11 loss:0.13937444984912872 norm:0.0005916183581575751 max memory_allocated 55791.26513671875 
[2025-03-12 11:23:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 12 loss:0.1389617621898651 norm:0.0005681143375113606 max memory_allocated 55791.26513671875 
[2025-03-12 11:24:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 13 loss:0.13868629932403564 norm:0.0005570597713813186 max memory_allocated 55791.26513671875 
[2025-03-12 11:25:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 14 loss:0.13852371275424957 norm:0.0005463263369165361 max memory_allocated 55791.26513671875 
[2025-03-12 11:27:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 15 loss:0.13841933012008667 norm:0.0005214792327024043 max memory_allocated 55791.26513671875 
[2025-03-12 11:28:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 16 loss:0.1382451355457306 norm:0.0005388658028095961 max memory_allocated 55791.26513671875 
[2025-03-12 11:29:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 17 loss:0.13819438219070435 norm:0.000531838508322835 max memory_allocated 55791.26513671875 
[2025-03-12 11:30:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 18 loss:0.13797485828399658 norm:0.000521092617418617 max memory_allocated 55791.26513671875 
[2025-03-12 11:32:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 7 iter 19 loss:0.13794441521167755 norm:0.0005142566515132785 max memory_allocated 55791.26513671875 
[2025-03-12 11:32:12 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 7 ===
[2025-03-12 11:38:35 root] (abq_llm_calibration_5_1.py 511): INFO Layer 7 Results:
[2025-03-12 11:38:35 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 11:38:35 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.060684
[2025-03-12 11:38:35 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996122
[2025-03-12 11:38:44 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 8 ===
[2025-03-12 11:40:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 0 loss:0.18393298983573914 norm:0.00777971837669611 max memory_allocated 55791.26513671875 
[2025-03-12 11:41:16 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 1 loss:0.16991828382015228 norm:0.0025350095238536596 max memory_allocated 55791.26513671875 
[2025-03-12 11:42:31 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 2 loss:0.16002950072288513 norm:0.001114357146434486 max memory_allocated 55791.26513671875 
[2025-03-12 11:43:46 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 3 loss:0.15519195795059204 norm:0.0007172613404691219 max memory_allocated 55791.26513671875 
[2025-03-12 11:45:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 4 loss:0.15302784740924835 norm:0.0005935092340223491 max memory_allocated 55791.26513671875 
[2025-03-12 11:46:15 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 5 loss:0.15173640847206116 norm:0.0005383520037867129 max memory_allocated 55791.26513671875 
[2025-03-12 11:47:30 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 6 loss:0.15076075494289398 norm:0.0005193730466999114 max memory_allocated 55791.26513671875 
[2025-03-12 11:48:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 7 loss:0.15012800693511963 norm:0.0005006187129765749 max memory_allocated 55791.26513671875 
[2025-03-12 11:49:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 8 loss:0.14963263273239136 norm:0.00048271517152898014 max memory_allocated 55791.26513671875 
[2025-03-12 11:51:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 9 loss:0.1491173356771469 norm:0.00046359284897334874 max memory_allocated 55791.26513671875 
[2025-03-12 11:52:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 10 loss:0.14877110719680786 norm:0.0004608153540175408 max memory_allocated 55791.26513671875 
[2025-03-12 11:53:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 11 loss:0.14835725724697113 norm:0.00045424181735143065 max memory_allocated 55791.26513671875 
[2025-03-12 11:54:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 12 loss:0.14807343482971191 norm:0.0004424124490469694 max memory_allocated 55791.26513671875 
[2025-03-12 11:56:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 13 loss:0.1478283703327179 norm:0.0004399541940074414 max memory_allocated 55791.26513671875 
[2025-03-12 11:57:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 14 loss:0.14759311079978943 norm:0.0004357935977168381 max memory_allocated 55791.26513671875 
[2025-03-12 11:58:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 15 loss:0.14748461544513702 norm:0.0004296380502637476 max memory_allocated 55791.26513671875 
[2025-03-12 11:59:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 16 loss:0.14733806252479553 norm:0.0004235785745549947 max memory_allocated 55791.26513671875 
[2025-03-12 12:01:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 17 loss:0.14721927046775818 norm:0.0004300875007174909 max memory_allocated 55791.26513671875 
[2025-03-12 12:02:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 18 loss:0.1472109854221344 norm:0.000428045226726681 max memory_allocated 55791.26513671875 
[2025-03-12 12:03:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 8 iter 19 loss:0.14710760116577148 norm:0.00043001031735911965 max memory_allocated 55791.26513671875 
[2025-03-12 12:03:41 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 8 ===
[2025-03-12 12:09:57 root] (abq_llm_calibration_5_1.py 511): INFO Layer 8 Results:
[2025-03-12 12:09:57 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 12:09:57 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.053966
[2025-03-12 12:09:57 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996549
[2025-03-12 12:10:05 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 9 ===
[2025-03-12 12:11:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 0 loss:0.19971035420894623 norm:0.01051441952586174 max memory_allocated 55791.26513671875 
[2025-03-12 12:12:37 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 1 loss:0.1815788596868515 norm:0.004052978940308094 max memory_allocated 55791.26513671875 
[2025-03-12 12:13:52 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 2 loss:0.1678737998008728 norm:0.0012558273738250136 max memory_allocated 55791.26513671875 
[2025-03-12 12:15:07 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 3 loss:0.16256430745124817 norm:0.0006761541008017957 max memory_allocated 55791.26513671875 
[2025-03-12 12:16:21 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 4 loss:0.1603442132472992 norm:0.0005658769514411688 max memory_allocated 55791.26513671875 
[2025-03-12 12:17:36 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 5 loss:0.15913596749305725 norm:0.0004928976995870471 max memory_allocated 55791.26513671875 
[2025-03-12 12:18:50 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 6 loss:0.15818801522254944 norm:0.0004560071974992752 max memory_allocated 55791.26513671875 
[2025-03-12 12:20:05 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 7 loss:0.1574561595916748 norm:0.00043441582238301635 max memory_allocated 55791.26513671875 
[2025-03-12 12:21:20 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 8 loss:0.1569107323884964 norm:0.00041322753531858325 max memory_allocated 55791.26513671875 
[2025-03-12 12:22:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 9 loss:0.15644140541553497 norm:0.00039637094596400857 max memory_allocated 55791.26513671875 
[2025-03-12 12:23:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 10 loss:0.15603823959827423 norm:0.0003863452293444425 max memory_allocated 55791.26513671875 
[2025-03-12 12:25:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 11 loss:0.15571805834770203 norm:0.0003756950900424272 max memory_allocated 55791.26513671875 
[2025-03-12 12:26:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 12 loss:0.15551815927028656 norm:0.00037418800638988614 max memory_allocated 55791.26513671875 
[2025-03-12 12:27:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 13 loss:0.1552366018295288 norm:0.00037312787026166916 max memory_allocated 55791.26513671875 
[2025-03-12 12:28:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 14 loss:0.15501344203948975 norm:0.0003619563940446824 max memory_allocated 55791.26513671875 
[2025-03-12 12:30:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 15 loss:0.15483292937278748 norm:0.0003643638629000634 max memory_allocated 55791.26513671875 
[2025-03-12 12:31:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 16 loss:0.15475350618362427 norm:0.00036846811417490244 max memory_allocated 55791.26513671875 
[2025-03-12 12:32:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 17 loss:0.15459975600242615 norm:0.0003598803887143731 max memory_allocated 55791.26513671875 
[2025-03-12 12:33:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 18 loss:0.15454041957855225 norm:0.00035110636963509023 max memory_allocated 55791.26513671875 
[2025-03-12 12:35:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 9 iter 19 loss:0.15447357296943665 norm:0.0003491351963020861 max memory_allocated 55791.26513671875 
[2025-03-12 12:35:02 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 9 ===
[2025-03-12 12:40:56 root] (abq_llm_calibration_5_1.py 511): INFO Layer 9 Results:
[2025-03-12 12:40:56 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 12:40:56 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.050756
[2025-03-12 12:40:56 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996760
[2025-03-12 12:41:05 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 10 ===
[2025-03-12 12:42:22 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 0 loss:0.19570356607437134 norm:0.010399538092315197 max memory_allocated 55791.26513671875 
[2025-03-12 12:43:37 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 1 loss:0.18450051546096802 norm:0.0044518401846289635 max memory_allocated 55791.26513671875 
[2025-03-12 12:44:51 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 2 loss:0.17516431212425232 norm:0.0020510207396000624 max memory_allocated 55791.26513671875 
[2025-03-12 12:46:06 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 3 loss:0.16957789659500122 norm:0.0008117272518575191 max memory_allocated 55791.26513671875 
[2025-03-12 12:47:21 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 4 loss:0.16688697040081024 norm:0.0004273782833479345 max memory_allocated 55791.26513671875 
[2025-03-12 12:48:35 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 5 loss:0.16548945009708405 norm:0.0003712925536092371 max memory_allocated 55791.26513671875 
[2025-03-12 12:49:50 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 6 loss:0.16460736095905304 norm:0.0003426873008720577 max memory_allocated 55791.26513671875 
[2025-03-12 12:51:05 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 7 loss:0.16392701864242554 norm:0.0003310070314910263 max memory_allocated 55791.26513671875 
[2025-03-12 12:52:20 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 8 loss:0.16343189775943756 norm:0.00032022048253566027 max memory_allocated 55791.26513671875 
[2025-03-12 12:53:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 9 loss:0.16303811967372894 norm:0.00031651323661208153 max memory_allocated 55791.26513671875 
[2025-03-12 12:54:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 10 loss:0.16271059215068817 norm:0.00031104410300031304 max memory_allocated 55791.26513671875 
[2025-03-12 12:56:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 11 loss:0.1624203771352768 norm:0.00030762472306378186 max memory_allocated 55791.26513671875 
[2025-03-12 12:57:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 12 loss:0.1621992290019989 norm:0.00030436707311309874 max memory_allocated 55791.26513671875 
[2025-03-12 12:58:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 13 loss:0.16198426485061646 norm:0.00030416055233217776 max memory_allocated 55791.26513671875 
[2025-03-12 12:59:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 14 loss:0.16181974112987518 norm:0.0003027204074896872 max memory_allocated 55791.26513671875 
[2025-03-12 13:01:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 15 loss:0.1616838276386261 norm:0.00030082245939411223 max memory_allocated 55791.26513671875 
[2025-03-12 13:02:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 16 loss:0.1615394949913025 norm:0.00030200209585018456 max memory_allocated 55791.26513671875 
[2025-03-12 13:03:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 17 loss:0.1614541858434677 norm:0.0003050634986720979 max memory_allocated 55791.26513671875 
[2025-03-12 13:04:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 18 loss:0.16134728491306305 norm:0.0003070307429879904 max memory_allocated 55791.26513671875 
[2025-03-12 13:06:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 10 iter 19 loss:0.16125454008579254 norm:0.00030929059721529484 max memory_allocated 55791.26513671875 
[2025-03-12 13:06:03 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 10 ===
[2025-03-12 13:11:43 root] (abq_llm_calibration_5_1.py 511): INFO Layer 10 Results:
[2025-03-12 13:11:43 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 13:11:43 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.048790
[2025-03-12 13:11:43 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996889
[2025-03-12 13:11:51 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 11 ===
[2025-03-12 13:13:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 0 loss:0.1892358511686325 norm:0.00559784984216094 max memory_allocated 55791.26513671875 
[2025-03-12 13:14:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 1 loss:0.17743779718875885 norm:0.0022909878753125668 max memory_allocated 55791.26513671875 
[2025-03-12 13:15:38 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 2 loss:0.16951435804367065 norm:0.0011183999013155699 max memory_allocated 55791.26513671875 
[2025-03-12 13:16:53 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 3 loss:0.16518732905387878 norm:0.0006943715270608664 max memory_allocated 55791.26513671875 
[2025-03-12 13:18:08 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 4 loss:0.16319909691810608 norm:0.000542070425581187 max memory_allocated 55791.26513671875 
[2025-03-12 13:19:22 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 5 loss:0.1620529294013977 norm:0.0004661452549044043 max memory_allocated 55791.26513671875 
[2025-03-12 13:20:37 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 6 loss:0.16128818690776825 norm:0.00042011571349576116 max memory_allocated 55791.26513671875 
[2025-03-12 13:21:52 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 7 loss:0.1607014387845993 norm:0.0003891379164997488 max memory_allocated 55791.26513671875 
[2025-03-12 13:23:07 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 8 loss:0.16018085181713104 norm:0.0003655073815025389 max memory_allocated 55791.26513671875 
[2025-03-12 13:24:22 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 9 loss:0.15979203581809998 norm:0.0003536044096108526 max memory_allocated 55791.26513671875 
[2025-03-12 13:25:36 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 10 loss:0.1594422161579132 norm:0.00033900997368618846 max memory_allocated 55791.26513671875 
[2025-03-12 13:26:51 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 11 loss:0.1591278612613678 norm:0.00033055333187803626 max memory_allocated 55791.26513671875 
[2025-03-12 13:28:06 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 12 loss:0.1589130312204361 norm:0.00031963485525920987 max memory_allocated 55791.26513671875 
[2025-03-12 13:29:21 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 13 loss:0.15872086584568024 norm:0.0003156937600579113 max memory_allocated 55791.26513671875 
[2025-03-12 13:30:35 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 14 loss:0.1585541069507599 norm:0.00031198502983897924 max memory_allocated 55791.26513671875 
[2025-03-12 13:31:50 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 15 loss:0.15841399133205414 norm:0.0003048405342269689 max memory_allocated 55791.26513671875 
[2025-03-12 13:33:05 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 16 loss:0.15830008685588837 norm:0.000304725079331547 max memory_allocated 55791.26513671875 
[2025-03-12 13:34:20 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 17 loss:0.15818436443805695 norm:0.0003046402125619352 max memory_allocated 55791.26513671875 
[2025-03-12 13:35:35 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 18 loss:0.1580553948879242 norm:0.0002981623401865363 max memory_allocated 55791.26513671875 
[2025-03-12 13:36:50 root] (abq_llm_calibration_5_1.py 420): INFO layer 11 iter 19 loss:0.15797698497772217 norm:0.00029843710944987833 max memory_allocated 55791.26513671875 
[2025-03-12 13:36:50 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 11 ===
[2025-03-12 13:42:15 root] (abq_llm_calibration_5_1.py 511): INFO Layer 11 Results:
[2025-03-12 13:42:15 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 13:42:15 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.052813
[2025-03-12 13:42:15 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996640
[2025-03-12 13:42:23 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 12 ===
[2025-03-12 13:43:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 0 loss:0.1871245950460434 norm:0.0031335041858255863 max memory_allocated 55791.26513671875 
[2025-03-12 13:44:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 1 loss:0.17828503251075745 norm:0.0014732195995748043 max memory_allocated 55791.26513671875 
[2025-03-12 13:46:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 2 loss:0.1713193655014038 norm:0.0008238708251155913 max memory_allocated 55791.26513671875 
[2025-03-12 13:47:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 3 loss:0.16712096333503723 norm:0.0005150260985828936 max memory_allocated 55791.26513671875 
[2025-03-12 13:48:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 4 loss:0.16519047319889069 norm:0.0004126733692828566 max memory_allocated 55791.26513671875 
[2025-03-12 13:49:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 5 loss:0.16405458748340607 norm:0.0003546670777723193 max memory_allocated 55791.26513671875 
[2025-03-12 13:51:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 6 loss:0.16331204771995544 norm:0.0003241136728320271 max memory_allocated 55791.26513671875 
[2025-03-12 13:52:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 7 loss:0.16273190081119537 norm:0.0003014601534232497 max memory_allocated 55791.26513671875 
[2025-03-12 13:53:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 8 loss:0.16229231655597687 norm:0.000290445052087307 max memory_allocated 55791.26513671875 
[2025-03-12 13:54:54 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 9 loss:0.16194017231464386 norm:0.00028584932442754507 max memory_allocated 55791.26513671875 
[2025-03-12 13:56:08 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 10 loss:0.16166308522224426 norm:0.0002805967233143747 max memory_allocated 55791.26513671875 
[2025-03-12 13:57:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 11 loss:0.16144108772277832 norm:0.0002758017217274755 max memory_allocated 55791.26513671875 
[2025-03-12 13:58:38 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 12 loss:0.16122746467590332 norm:0.0002751766878645867 max memory_allocated 55791.26513671875 
[2025-03-12 13:59:53 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 13 loss:0.161037415266037 norm:0.00027039353153668344 max memory_allocated 55791.26513671875 
[2025-03-12 14:01:08 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 14 loss:0.1608978658914566 norm:0.00027269491693004966 max memory_allocated 55791.26513671875 
[2025-03-12 14:02:22 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 15 loss:0.16076433658599854 norm:0.0002710421977099031 max memory_allocated 55791.26513671875 
[2025-03-12 14:03:37 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 16 loss:0.16065619885921478 norm:0.0002667192311491817 max memory_allocated 55791.26513671875 
[2025-03-12 14:04:52 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 17 loss:0.16054695844650269 norm:0.00029389141127467155 max memory_allocated 55791.26513671875 
[2025-03-12 14:06:07 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 18 loss:0.16045966744422913 norm:0.0002621312451083213 max memory_allocated 55791.26513671875 
[2025-03-12 14:07:21 root] (abq_llm_calibration_5_1.py 420): INFO layer 12 iter 19 loss:0.1603628545999527 norm:0.00026155312662012875 max memory_allocated 55791.26513671875 
[2025-03-12 14:07:22 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 12 ===
[2025-03-12 14:12:31 root] (abq_llm_calibration_5_1.py 511): INFO Layer 12 Results:
[2025-03-12 14:12:31 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 14:12:31 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.048002
[2025-03-12 14:12:31 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996929
[2025-03-12 14:12:40 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 13 ===
[2025-03-12 14:13:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 0 loss:0.19822877645492554 norm:0.01418902538716793 max memory_allocated 55791.26513671875 
[2025-03-12 14:15:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 1 loss:0.18263205885887146 norm:0.0047718798741698265 max memory_allocated 55791.26513671875 
[2025-03-12 14:16:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 2 loss:0.173028826713562 norm:0.002108111511915922 max memory_allocated 55791.26513671875 
[2025-03-12 14:17:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 3 loss:0.16771632432937622 norm:0.0009741439716890454 max memory_allocated 55791.26513671875 
[2025-03-12 14:18:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 4 loss:0.16541416943073273 norm:0.000573045457713306 max memory_allocated 55791.26513671875 
[2025-03-12 14:20:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 5 loss:0.16422735154628754 norm:0.0004652093630284071 max memory_allocated 55791.26513671875 
[2025-03-12 14:21:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 6 loss:0.16343466937541962 norm:0.0004121987731195986 max memory_allocated 55791.26513671875 
[2025-03-12 14:22:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 7 loss:0.16283193230628967 norm:0.00037510533002205193 max memory_allocated 55791.26513671875 
[2025-03-12 14:23:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 8 loss:0.1623702049255371 norm:0.0003475594858173281 max memory_allocated 55791.26513671875 
[2025-03-12 14:25:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 9 loss:0.1620289385318756 norm:0.00033731982694007456 max memory_allocated 55791.26513671875 
[2025-03-12 14:26:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 10 loss:0.16174033284187317 norm:0.00032815858139656484 max memory_allocated 55791.26513671875 
[2025-03-12 14:27:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 11 loss:0.16143593192100525 norm:0.00030766098643653095 max memory_allocated 55791.26513671875 
[2025-03-12 14:28:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 12 loss:0.16122274100780487 norm:0.00029717126744799316 max memory_allocated 55791.26513671875 
[2025-03-12 14:30:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 13 loss:0.16104751825332642 norm:0.0003011943190358579 max memory_allocated 55791.26513671875 
[2025-03-12 14:31:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 14 loss:0.16089345514774323 norm:0.0002961036516353488 max memory_allocated 55791.26513671875 
[2025-03-12 14:32:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 15 loss:0.1607363522052765 norm:0.0002870197640731931 max memory_allocated 55791.26513671875 
[2025-03-12 14:33:54 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 16 loss:0.16059505939483643 norm:0.0002803249517455697 max memory_allocated 55791.26513671875 
[2025-03-12 14:35:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 17 loss:0.16048838198184967 norm:0.00027530465740710497 max memory_allocated 55791.26513671875 
[2025-03-12 14:36:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 18 loss:0.16034166514873505 norm:0.0002712096320465207 max memory_allocated 55791.26513671875 
[2025-03-12 14:37:38 root] (abq_llm_calibration_5_1.py 420): INFO layer 13 iter 19 loss:0.16020996868610382 norm:0.0002657913137227297 max memory_allocated 55791.26513671875 
[2025-03-12 14:37:39 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 13 ===
[2025-03-12 14:42:34 root] (abq_llm_calibration_5_1.py 511): INFO Layer 13 Results:
[2025-03-12 14:42:34 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 14:42:34 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.057227
[2025-03-12 14:42:34 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996333
[2025-03-12 14:42:42 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 14 ===
[2025-03-12 14:44:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 0 loss:0.18825140595436096 norm:0.004188301973044872 max memory_allocated 55791.26513671875 
[2025-03-12 14:45:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 1 loss:0.17938293516635895 norm:0.0018412049394100904 max memory_allocated 55791.26513671875 
[2025-03-12 14:46:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 2 loss:0.1730043590068817 norm:0.0009223483502864838 max memory_allocated 55791.26513671875 
[2025-03-12 14:47:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 3 loss:0.16907629370689392 norm:0.00048441480612382293 max memory_allocated 55791.26513671875 
[2025-03-12 14:48:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 4 loss:0.16732533276081085 norm:0.00037522337515838444 max memory_allocated 55791.26513671875 
[2025-03-12 14:50:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 5 loss:0.16628147661685944 norm:0.0003275938506703824 max memory_allocated 55791.26513671875 
[2025-03-12 14:51:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 6 loss:0.1656312793493271 norm:0.00031056799343787134 max memory_allocated 55791.26513671875 
[2025-03-12 14:52:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 7 loss:0.16512319445610046 norm:0.0002992653753608465 max memory_allocated 55791.26513671875 
[2025-03-12 14:53:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 8 loss:0.16474100947380066 norm:0.0002891066833399236 max memory_allocated 55791.26513671875 
[2025-03-12 14:55:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 9 loss:0.16441650688648224 norm:0.000278659223113209 max memory_allocated 55791.26513671875 
[2025-03-12 14:56:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 10 loss:0.16413702070713043 norm:0.00027300362125970423 max memory_allocated 55791.26513671875 
[2025-03-12 14:57:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 11 loss:0.16393153369426727 norm:0.00026880900259129703 max memory_allocated 55791.26513671875 
[2025-03-12 14:58:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 12 loss:0.16373133659362793 norm:0.00026651693042367697 max memory_allocated 55791.26513671875 
[2025-03-12 15:00:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 13 loss:0.16356217861175537 norm:0.0002638773585204035 max memory_allocated 55791.26513671875 
[2025-03-12 15:01:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 14 loss:0.16342979669570923 norm:0.0002618936123326421 max memory_allocated 55791.26513671875 
[2025-03-12 15:02:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 15 loss:0.163317009806633 norm:0.0002595265978015959 max memory_allocated 55791.26513671875 
[2025-03-12 15:03:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 16 loss:0.16320829093456268 norm:0.00025746921892277896 max memory_allocated 55791.26513671875 
[2025-03-12 15:05:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 17 loss:0.16310523450374603 norm:0.00025802964228205383 max memory_allocated 55791.26513671875 
[2025-03-12 15:06:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 18 loss:0.16305474936962128 norm:0.00025600040680728853 max memory_allocated 55791.26513671875 
[2025-03-12 15:07:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 14 iter 19 loss:0.1629364788532257 norm:0.0002564425521995872 max memory_allocated 55791.26513671875 
[2025-03-12 15:07:41 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 14 ===
[2025-03-12 15:12:21 root] (abq_llm_calibration_5_1.py 511): INFO Layer 14 Results:
[2025-03-12 15:12:21 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 15:12:21 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.060294
[2025-03-12 15:12:21 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.996132
[2025-03-12 15:12:30 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 15 ===
[2025-03-12 15:13:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 0 loss:0.2178214192390442 norm:0.010723761282861233 max memory_allocated 55791.26513671875 
[2025-03-12 15:15:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 1 loss:0.20292750000953674 norm:0.004556363448500633 max memory_allocated 55791.26513671875 
[2025-03-12 15:16:16 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 2 loss:0.1902145892381668 norm:0.001996453385800123 max memory_allocated 55791.26513671875 
[2025-03-12 15:17:31 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 3 loss:0.18450579047203064 norm:0.0010634687496349216 max memory_allocated 55791.26513671875 
[2025-03-12 15:18:46 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 4 loss:0.18180376291275024 norm:0.0005585576873272657 max memory_allocated 55791.26513671875 
[2025-03-12 15:20:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 5 loss:0.18047310411930084 norm:0.0004545839037746191 max memory_allocated 55791.26513671875 
[2025-03-12 15:21:15 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 6 loss:0.17970450222492218 norm:0.00043445121264085174 max memory_allocated 55791.26513671875 
[2025-03-12 15:22:30 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 7 loss:0.17907631397247314 norm:0.00039834665949456394 max memory_allocated 55791.26513671875 
[2025-03-12 15:23:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 8 loss:0.17867134511470795 norm:0.0003807071188930422 max memory_allocated 55791.26513671875 
[2025-03-12 15:25:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 9 loss:0.17832182347774506 norm:0.00036748943966813385 max memory_allocated 55791.26513671875 
[2025-03-12 15:26:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 10 loss:0.17798979580402374 norm:0.0003411794896237552 max memory_allocated 55791.26513671875 
[2025-03-12 15:27:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 11 loss:0.17770783603191376 norm:0.0003282200777903199 max memory_allocated 55791.26513671875 
[2025-03-12 15:28:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 12 loss:0.17740744352340698 norm:0.00031943971407599747 max memory_allocated 55791.26513671875 
[2025-03-12 15:29:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 13 loss:0.1771565079689026 norm:0.00031104666413739324 max memory_allocated 55791.26513671875 
[2025-03-12 15:31:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 14 loss:0.1769964098930359 norm:0.00030614904244430363 max memory_allocated 55791.26513671875 
[2025-03-12 15:32:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 15 loss:0.17682406306266785 norm:0.00031153217423707247 max memory_allocated 55791.26513671875 
[2025-03-12 15:33:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 16 loss:0.17668503522872925 norm:0.00030945491744205356 max memory_allocated 55791.26513671875 
[2025-03-12 15:34:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 17 loss:0.17657122015953064 norm:0.0003085212083533406 max memory_allocated 55791.26513671875 
[2025-03-12 15:36:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 18 loss:0.1764374077320099 norm:0.0003026148770004511 max memory_allocated 55791.26513671875 
[2025-03-12 15:37:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 15 iter 19 loss:0.17633655667304993 norm:0.0003002175653818995 max memory_allocated 55791.26513671875 
[2025-03-12 15:37:28 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 15 ===
[2025-03-12 15:41:53 root] (abq_llm_calibration_5_1.py 511): INFO Layer 15 Results:
[2025-03-12 15:41:53 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 15:41:53 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.071149
[2025-03-12 15:41:53 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.995435
[2025-03-12 15:42:02 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 16 ===
[2025-03-12 15:43:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 0 loss:0.244452565908432 norm:0.013720445334911346 max memory_allocated 55791.26513671875 
[2025-03-12 15:44:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 1 loss:0.23081174492835999 norm:0.006445156875997782 max memory_allocated 55791.26513671875 
[2025-03-12 15:45:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 2 loss:0.21584835648536682 norm:0.002554615493863821 max memory_allocated 55791.26513671875 
[2025-03-12 15:47:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 3 loss:0.20788037776947021 norm:0.0010145247215405107 max memory_allocated 55791.26513671875 
[2025-03-12 15:48:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 4 loss:0.20549125969409943 norm:0.0007348867948167026 max memory_allocated 55791.26513671875 
[2025-03-12 15:49:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 5 loss:0.204310342669487 norm:0.0006562645430676639 max memory_allocated 55791.26513671875 
[2025-03-12 15:50:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 6 loss:0.20328615605831146 norm:0.0005928456084802747 max memory_allocated 55791.26513671875 
[2025-03-12 15:52:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 7 loss:0.2026529163122177 norm:0.0005654910346493125 max memory_allocated 55791.26513671875 
[2025-03-12 15:53:16 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 8 loss:0.2021506279706955 norm:0.0005324992234818637 max memory_allocated 55791.26513671875 
[2025-03-12 15:54:31 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 9 loss:0.20170150697231293 norm:0.0005046742735430598 max memory_allocated 55791.26513671875 
[2025-03-12 15:55:46 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 10 loss:0.2012234330177307 norm:0.0004645127337425947 max memory_allocated 55791.26513671875 
[2025-03-12 15:57:01 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 11 loss:0.20081961154937744 norm:0.0004461313073989004 max memory_allocated 55791.26513671875 
[2025-03-12 15:58:15 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 12 loss:0.20054948329925537 norm:0.0004431112320162356 max memory_allocated 55791.26513671875 
[2025-03-12 15:59:30 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 13 loss:0.20026157796382904 norm:0.0004327560600358993 max memory_allocated 55791.26513671875 
[2025-03-12 16:00:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 14 loss:0.1999727338552475 norm:0.00042743689846247435 max memory_allocated 55791.26513671875 
[2025-03-12 16:01:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 15 loss:0.19973346590995789 norm:0.0004166570433881134 max memory_allocated 55791.26513671875 
[2025-03-12 16:03:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 16 loss:0.1995583325624466 norm:0.00040993004222400486 max memory_allocated 55791.26513671875 
[2025-03-12 16:04:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 17 loss:0.19943036139011383 norm:0.00040368360350839794 max memory_allocated 55791.26513671875 
[2025-03-12 16:05:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 18 loss:0.1992517113685608 norm:0.0003916120040230453 max memory_allocated 55791.26513671875 
[2025-03-12 16:06:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 16 iter 19 loss:0.19914141297340393 norm:0.0003920313320122659 max memory_allocated 55791.26513671875 
[2025-03-12 16:06:59 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 16 ===
[2025-03-12 16:11:08 root] (abq_llm_calibration_5_1.py 511): INFO Layer 16 Results:
[2025-03-12 16:11:08 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 16:11:08 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.085521
[2025-03-12 16:11:08 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994493
[2025-03-12 16:11:17 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 17 ===
[2025-03-12 16:12:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 0 loss:0.267469197511673 norm:0.011362435296177864 max memory_allocated 55791.26513671875 
[2025-03-12 16:13:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 1 loss:0.2545565068721771 norm:0.005428396165370941 max memory_allocated 55791.26513671875 
[2025-03-12 16:15:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 2 loss:0.24276335537433624 norm:0.00251190597191453 max memory_allocated 55791.26513671875 
[2025-03-12 16:16:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 3 loss:0.23604080080986023 norm:0.0009923707693815231 max memory_allocated 55791.26513671875 
[2025-03-12 16:17:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 4 loss:0.2340899556875229 norm:0.0007627131417393684 max memory_allocated 55791.26513671875 
[2025-03-12 16:18:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 5 loss:0.23293118178844452 norm:0.0006602286593988538 max memory_allocated 55791.26513671875 
[2025-03-12 16:20:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 6 loss:0.2321242243051529 norm:0.0006204310338944197 max memory_allocated 55791.26513671875 
[2025-03-12 16:21:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 7 loss:0.23159436881542206 norm:0.0005978627596050501 max memory_allocated 55791.26513671875 
[2025-03-12 16:22:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 8 loss:0.23100897669792175 norm:0.0005727244424633682 max memory_allocated 55791.26513671875 
[2025-03-12 16:23:46 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 9 loss:0.23047493398189545 norm:0.0005441851681098342 max memory_allocated 55791.26513671875 
[2025-03-12 16:25:01 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 10 loss:0.23006045818328857 norm:0.0005376707413233817 max memory_allocated 55791.26513671875 
[2025-03-12 16:26:16 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 11 loss:0.22977448999881744 norm:0.0005288003594614565 max memory_allocated 55791.26513671875 
[2025-03-12 16:27:31 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 12 loss:0.2294890582561493 norm:0.000503349001519382 max memory_allocated 55791.26513671875 
[2025-03-12 16:28:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 13 loss:0.22921720147132874 norm:0.0004837067681364715 max memory_allocated 55791.26513671875 
[2025-03-12 16:30:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 14 loss:0.22903071343898773 norm:0.0004749608342535794 max memory_allocated 55791.26513671875 
[2025-03-12 16:31:15 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 15 loss:0.2288263738155365 norm:0.0004590809403453022 max memory_allocated 55791.26513671875 
[2025-03-12 16:32:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 16 loss:0.2286432534456253 norm:0.00044477119809016585 max memory_allocated 55791.26513671875 
[2025-03-12 16:33:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 17 loss:0.22842709720134735 norm:0.00042808859143406153 max memory_allocated 55791.26513671875 
[2025-03-12 16:34:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 18 loss:0.22832757234573364 norm:0.00041745873750187457 max memory_allocated 55791.26513671875 
[2025-03-12 16:36:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 17 iter 19 loss:0.22821402549743652 norm:0.0004079426871612668 max memory_allocated 55791.26513671875 
[2025-03-12 16:36:14 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 17 ===
[2025-03-12 16:40:08 root] (abq_llm_calibration_5_1.py 511): INFO Layer 17 Results:
[2025-03-12 16:40:08 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 16:40:08 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.085429
[2025-03-12 16:40:08 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994486
[2025-03-12 16:40:17 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 18 ===
[2025-03-12 16:41:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 0 loss:0.31545886397361755 norm:0.025636255741119385 max memory_allocated 55791.26513671875 
[2025-03-12 16:42:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 1 loss:0.29822462797164917 norm:0.01125402096658945 max memory_allocated 55791.26513671875 
[2025-03-12 16:44:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 2 loss:0.2837981581687927 norm:0.005303616169840097 max memory_allocated 55791.26513671875 
[2025-03-12 16:45:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 3 loss:0.2744549810886383 norm:0.0024789629969745874 max memory_allocated 55791.26513671875 
[2025-03-12 16:46:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 4 loss:0.2705758810043335 norm:0.0016023736679926515 max memory_allocated 55791.26513671875 
[2025-03-12 16:47:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 5 loss:0.2687366008758545 norm:0.0010705286404117942 max memory_allocated 55791.26513671875 
[2025-03-12 16:49:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 6 loss:0.26728716492652893 norm:0.0006742446566931903 max memory_allocated 55791.26513671875 
[2025-03-12 16:50:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 7 loss:0.26642531156539917 norm:0.0006199338822625577 max memory_allocated 55791.26513671875 
[2025-03-12 16:51:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 8 loss:0.265730082988739 norm:0.000572300108615309 max memory_allocated 55791.26513671875 
[2025-03-12 16:52:46 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 9 loss:0.2651582956314087 norm:0.0005599327269010246 max memory_allocated 55791.26513671875 
[2025-03-12 16:54:01 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 10 loss:0.26474082469940186 norm:0.0005485055735334754 max memory_allocated 55791.26513671875 
[2025-03-12 16:55:16 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 11 loss:0.26434019207954407 norm:0.0005353589658625424 max memory_allocated 55791.26513671875 
[2025-03-12 16:56:30 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 12 loss:0.2640044093132019 norm:0.0005240461905486882 max memory_allocated 55791.26513671875 
[2025-03-12 16:57:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 13 loss:0.2637781500816345 norm:0.0005252894479781389 max memory_allocated 55791.26513671875 
[2025-03-12 16:59:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 14 loss:0.26353907585144043 norm:0.0005136734107509255 max memory_allocated 55791.26513671875 
[2025-03-12 17:00:15 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 15 loss:0.26344746351242065 norm:0.0005250987014733255 max memory_allocated 55791.26513671875 
[2025-03-12 17:01:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 16 loss:0.2631968557834625 norm:0.0005042615230195224 max memory_allocated 55791.26513671875 
[2025-03-12 17:02:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 17 loss:0.26293638348579407 norm:0.0004873770521953702 max memory_allocated 55791.26513671875 
[2025-03-12 17:03:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 18 loss:0.26284536719322205 norm:0.0004775089619215578 max memory_allocated 55791.26513671875 
[2025-03-12 17:05:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 18 iter 19 loss:0.26271992921829224 norm:0.0004669523041229695 max memory_allocated 55791.26513671875 
[2025-03-12 17:05:14 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 18 ===
[2025-03-12 17:08:46 root] (abq_llm_calibration_5_1.py 511): INFO Layer 18 Results:
[2025-03-12 17:08:46 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 17:08:46 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.093467
[2025-03-12 17:08:46 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.993972
[2025-03-12 17:08:55 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 19 ===
[2025-03-12 17:10:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 0 loss:0.3485110402107239 norm:0.01912848651409149 max memory_allocated 55791.26513671875 
[2025-03-12 17:11:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 1 loss:0.33521854877471924 norm:0.009400489740073681 max memory_allocated 55791.26513671875 
[2025-03-12 17:12:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 2 loss:0.32303333282470703 norm:0.004645621869713068 max memory_allocated 55791.26513671875 
[2025-03-12 17:13:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 3 loss:0.3148875832557678 norm:0.002281973836943507 max memory_allocated 55791.26513671875 
[2025-03-12 17:15:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 4 loss:0.31227171421051025 norm:0.0017306074732914567 max memory_allocated 55791.26513671875 
[2025-03-12 17:16:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 5 loss:0.31084731221199036 norm:0.001426533330231905 max memory_allocated 55791.26513671875 
[2025-03-12 17:17:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 6 loss:0.3094318211078644 norm:0.0010096229379996657 max memory_allocated 55791.26513671875 
[2025-03-12 17:18:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 7 loss:0.30834710597991943 norm:0.0005515622324310243 max memory_allocated 55791.26513671875 
[2025-03-12 17:20:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 8 loss:0.307771772146225 norm:0.0005328557454049587 max memory_allocated 55791.26513671875 
[2025-03-12 17:21:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 9 loss:0.30726876854896545 norm:0.0004996586358174682 max memory_allocated 55791.26513671875 
[2025-03-12 17:22:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 10 loss:0.3068138062953949 norm:0.0004719999560620636 max memory_allocated 55791.26513671875 
[2025-03-12 17:23:54 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 11 loss:0.3063846230506897 norm:0.0004744523321278393 max memory_allocated 55791.26513671875 
[2025-03-12 17:25:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 12 loss:0.30607640743255615 norm:0.0004734510730486363 max memory_allocated 55791.26513671875 
[2025-03-12 17:26:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 13 loss:0.30581608414649963 norm:0.0004711103392764926 max memory_allocated 55791.26513671875 
[2025-03-12 17:27:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 14 loss:0.3056259751319885 norm:0.0004650438786484301 max memory_allocated 55791.26513671875 
[2025-03-12 17:28:53 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 15 loss:0.30536213517189026 norm:0.00044300785521045327 max memory_allocated 55791.26513671875 
[2025-03-12 17:30:08 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 16 loss:0.30513083934783936 norm:0.0004391759284771979 max memory_allocated 55791.26513671875 
[2025-03-12 17:31:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 17 loss:0.3049396872520447 norm:0.0004349443188402802 max memory_allocated 55791.26513671875 
[2025-03-12 17:32:37 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 18 loss:0.3048364520072937 norm:0.00043386113247834146 max memory_allocated 55791.26513671875 
[2025-03-12 17:33:52 root] (abq_llm_calibration_5_1.py 420): INFO layer 19 iter 19 loss:0.3047102689743042 norm:0.00042289725388400257 max memory_allocated 55791.26513671875 
[2025-03-12 17:33:53 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 19 ===
[2025-03-12 17:37:16 root] (abq_llm_calibration_5_1.py 511): INFO Layer 19 Results:
[2025-03-12 17:37:16 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 17:37:16 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.092718
[2025-03-12 17:37:16 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994013
[2025-03-12 17:37:25 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 20 ===
[2025-03-12 17:38:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 0 loss:0.40504777431488037 norm:0.013699996285140514 max memory_allocated 55791.26513671875 
[2025-03-12 17:39:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 1 loss:0.39297792315483093 norm:0.007368957158178091 max memory_allocated 55791.26513671875 
[2025-03-12 17:41:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 2 loss:0.3791717290878296 norm:0.003536135656759143 max memory_allocated 55791.26513671875 
[2025-03-12 17:42:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 3 loss:0.37056660652160645 norm:0.0020026941783726215 max memory_allocated 55791.26513671875 
[2025-03-12 17:43:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 4 loss:0.36683952808380127 norm:0.0011118303518742323 max memory_allocated 55791.26513671875 
[2025-03-12 17:44:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 5 loss:0.3650602400302887 norm:0.0008592254016548395 max memory_allocated 55791.26513671875 
[2025-03-12 17:46:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 6 loss:0.363983154296875 norm:0.0007741183508187532 max memory_allocated 55791.26513671875 
[2025-03-12 17:47:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 7 loss:0.3631005883216858 norm:0.0007006749510765076 max memory_allocated 55791.26513671875 
[2025-03-12 17:48:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 8 loss:0.3623717427253723 norm:0.00066183548187837 max memory_allocated 55791.26513671875 
[2025-03-12 17:49:54 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 9 loss:0.3618268072605133 norm:0.0006360278348438442 max memory_allocated 55791.26513671875 
[2025-03-12 17:51:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 10 loss:0.3613412380218506 norm:0.0006286947755143046 max memory_allocated 55791.26513671875 
[2025-03-12 17:52:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 11 loss:0.3608926832675934 norm:0.0006153401918709278 max memory_allocated 55791.26513671875 
[2025-03-12 17:53:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 12 loss:0.36049896478652954 norm:0.0005951845087110996 max memory_allocated 55791.26513671875 
[2025-03-12 17:54:53 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 13 loss:0.3600980043411255 norm:0.0005771564901806414 max memory_allocated 55791.26513671875 
[2025-03-12 17:56:08 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 14 loss:0.3597659766674042 norm:0.0005594977410510182 max memory_allocated 55791.26513671875 
[2025-03-12 17:57:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 15 loss:0.3594667911529541 norm:0.0005605206824839115 max memory_allocated 55791.26513671875 
[2025-03-12 17:58:37 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 16 loss:0.35924041271209717 norm:0.000556353887077421 max memory_allocated 55791.26513671875 
[2025-03-12 17:59:52 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 17 loss:0.35898536443710327 norm:0.0005511763738468289 max memory_allocated 55791.26513671875 
[2025-03-12 18:01:07 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 18 loss:0.35880210995674133 norm:0.000551849661860615 max memory_allocated 55791.26513671875 
[2025-03-12 18:02:22 root] (abq_llm_calibration_5_1.py 420): INFO layer 20 iter 19 loss:0.35870784521102905 norm:0.0005402080132625997 max memory_allocated 55791.26513671875 
[2025-03-12 18:02:23 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 20 ===
[2025-03-12 18:05:31 root] (abq_llm_calibration_5_1.py 511): INFO Layer 20 Results:
[2025-03-12 18:05:31 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 18:05:31 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.093809
[2025-03-12 18:05:31 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.993937
[2025-03-12 18:05:40 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 21 ===
[2025-03-12 18:06:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 0 loss:0.45000991225242615 norm:0.011182637885212898 max memory_allocated 55791.26513671875 
[2025-03-12 18:08:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 1 loss:0.43918079137802124 norm:0.004735145252197981 max memory_allocated 55791.26513671875 
[2025-03-12 18:09:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 2 loss:0.43049150705337524 norm:0.002535646315664053 max memory_allocated 55791.26513671875 
[2025-03-12 18:10:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 3 loss:0.42486292123794556 norm:0.0012658252380788326 max memory_allocated 55791.26513671875 
[2025-03-12 18:11:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 4 loss:0.4227079451084137 norm:0.000753731292206794 max memory_allocated 55791.26513671875 
[2025-03-12 18:13:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 5 loss:0.42152249813079834 norm:0.000608422385994345 max memory_allocated 55791.26513671875 
[2025-03-12 18:14:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 6 loss:0.42075324058532715 norm:0.000562210800126195 max memory_allocated 55791.26513671875 
[2025-03-12 18:15:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 7 loss:0.420074999332428 norm:0.0005420757224783301 max memory_allocated 55791.26513671875 
[2025-03-12 18:16:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 8 loss:0.4194958508014679 norm:0.0005188446375541389 max memory_allocated 55791.26513671875 
[2025-03-12 18:18:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 9 loss:0.4188966751098633 norm:0.0004898935439996421 max memory_allocated 55791.26513671875 
[2025-03-12 18:19:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 10 loss:0.418413907289505 norm:0.00047940629883669317 max memory_allocated 55791.26513671875 
[2025-03-12 18:20:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 11 loss:0.4179520606994629 norm:0.0004633213975466788 max memory_allocated 55791.26513671875 
[2025-03-12 18:21:53 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 12 loss:0.4175809919834137 norm:0.0004594811762217432 max memory_allocated 55791.26513671875 
[2025-03-12 18:23:08 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 13 loss:0.4172408878803253 norm:0.00045109630445949733 max memory_allocated 55791.26513671875 
[2025-03-12 18:24:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 14 loss:0.41694051027297974 norm:0.00045284387306310236 max memory_allocated 55791.26513671875 
[2025-03-12 18:25:38 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 15 loss:0.4166794717311859 norm:0.00044102041283622384 max memory_allocated 55791.26513671875 
[2025-03-12 18:26:52 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 16 loss:0.4164682626724243 norm:0.00044138223165646195 max memory_allocated 55791.26513671875 
[2025-03-12 18:28:07 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 17 loss:0.4163214862346649 norm:0.0004405848158057779 max memory_allocated 55791.26513671875 
[2025-03-12 18:29:22 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 18 loss:0.4161529839038849 norm:0.0004323585599195212 max memory_allocated 55791.26513671875 
[2025-03-12 18:30:36 root] (abq_llm_calibration_5_1.py 420): INFO layer 21 iter 19 loss:0.41606584191322327 norm:0.0004367791989352554 max memory_allocated 55791.26513671875 
[2025-03-12 18:30:37 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 21 ===
[2025-03-12 18:33:25 root] (abq_llm_calibration_5_1.py 511): INFO Layer 21 Results:
[2025-03-12 18:33:25 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 18:33:25 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.084667
[2025-03-12 18:33:25 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994526
[2025-03-12 18:33:33 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 22 ===
[2025-03-12 18:34:51 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 0 loss:0.5339735746383667 norm:0.016522733494639397 max memory_allocated 55791.26513671875 
[2025-03-12 18:36:05 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 1 loss:0.517115592956543 norm:0.007458282168954611 max memory_allocated 55791.26513671875 
[2025-03-12 18:37:20 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 2 loss:0.5005141496658325 norm:0.0033754794858396053 max memory_allocated 55791.26513671875 
[2025-03-12 18:38:35 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 3 loss:0.49059030413627625 norm:0.0015534423291683197 max memory_allocated 55791.26513671875 
[2025-03-12 18:39:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 4 loss:0.4878648519515991 norm:0.0010035900631919503 max memory_allocated 55791.26513671875 
[2025-03-12 18:41:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 5 loss:0.48636358976364136 norm:0.0007795258425176144 max memory_allocated 55791.26513671875 
[2025-03-12 18:42:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 6 loss:0.4853871464729309 norm:0.000702687248121947 max memory_allocated 55791.26513671875 
[2025-03-12 18:43:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 7 loss:0.4844483733177185 norm:0.0006289910525083542 max memory_allocated 55791.26513671875 
[2025-03-12 18:44:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 8 loss:0.48374953866004944 norm:0.0006049902294762433 max memory_allocated 55791.26513671875 
[2025-03-12 18:46:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 9 loss:0.48315873742103577 norm:0.0005862163379788399 max memory_allocated 55791.26513671875 
[2025-03-12 18:47:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 10 loss:0.4826427698135376 norm:0.0005651367246173322 max memory_allocated 55791.26513671875 
[2025-03-12 18:48:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 11 loss:0.482153981924057 norm:0.0005513461073860526 max memory_allocated 55791.26513671875 
[2025-03-12 18:49:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 12 loss:0.481701523065567 norm:0.0005413976032286882 max memory_allocated 55791.26513671875 
[2025-03-12 18:51:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 13 loss:0.4813550114631653 norm:0.000522803224157542 max memory_allocated 55791.26513671875 
[2025-03-12 18:52:16 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 14 loss:0.480701208114624 norm:0.0004883986548520625 max memory_allocated 55791.26513671875 
[2025-03-12 18:53:31 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 15 loss:0.4800836443901062 norm:0.0005546233151108027 max memory_allocated 55791.26513671875 
[2025-03-12 18:54:46 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 16 loss:0.47983619570732117 norm:0.0005304989754222333 max memory_allocated 55791.26513671875 
[2025-03-12 18:56:01 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 17 loss:0.4799045920372009 norm:0.0005238301819190383 max memory_allocated 55791.26513671875 
[2025-03-12 18:57:15 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 18 loss:0.48012807965278625 norm:0.0005244457279331982 max memory_allocated 55791.26513671875 
[2025-03-12 18:58:30 root] (abq_llm_calibration_5_1.py 420): INFO layer 22 iter 19 loss:0.480174720287323 norm:0.0005217767902649939 max memory_allocated 55791.26513671875 
[2025-03-12 18:58:31 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 22 ===
[2025-03-12 19:01:09 root] (abq_llm_calibration_5_1.py 511): INFO Layer 22 Results:
[2025-03-12 19:01:09 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 19:01:09 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.086519
[2025-03-12 19:01:09 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994412
[2025-03-12 19:01:18 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 23 ===
[2025-03-12 19:02:35 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 0 loss:0.5902787446975708 norm:0.008232434280216694 max memory_allocated 55791.26513671875 
[2025-03-12 19:03:50 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 1 loss:0.5794728994369507 norm:0.0038115696515887976 max memory_allocated 55791.26513671875 
[2025-03-12 19:05:05 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 2 loss:0.5687407851219177 norm:0.0019290418131276965 max memory_allocated 55791.26513671875 
[2025-03-12 19:06:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 3 loss:0.5622972249984741 norm:0.000949314737226814 max memory_allocated 55791.26513671875 
[2025-03-12 19:07:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 4 loss:0.5601758360862732 norm:0.0006800037226639688 max memory_allocated 55791.26513671875 
[2025-03-12 19:08:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 5 loss:0.5590022206306458 norm:0.0005892406916245818 max memory_allocated 55791.26513671875 
[2025-03-12 19:10:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 6 loss:0.5581888556480408 norm:0.0005552691291086376 max memory_allocated 55791.26513671875 
[2025-03-12 19:11:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 7 loss:0.5573916435241699 norm:0.0005318362964317203 max memory_allocated 55791.26513671875 
[2025-03-12 19:12:33 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 8 loss:0.5565693378448486 norm:0.0005112902726978064 max memory_allocated 55791.26513671875 
[2025-03-12 19:13:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 9 loss:0.5557894110679626 norm:0.0004890852724201977 max memory_allocated 55791.26513671875 
[2025-03-12 19:15:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 10 loss:0.5552107095718384 norm:0.00047788931988179684 max memory_allocated 55791.26513671875 
[2025-03-12 19:16:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 11 loss:0.5547265410423279 norm:0.00047296728007495403 max memory_allocated 55791.26513671875 
[2025-03-12 19:17:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 12 loss:0.554309606552124 norm:0.00046446805936284363 max memory_allocated 55791.26513671875 
[2025-03-12 19:18:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 13 loss:0.5539845824241638 norm:0.00045997725101187825 max memory_allocated 55791.26513671875 
[2025-03-12 19:20:01 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 14 loss:0.5537110567092896 norm:0.00046044992632232606 max memory_allocated 55791.26513671875 
[2025-03-12 19:21:16 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 15 loss:0.5534449815750122 norm:0.0004556459898594767 max memory_allocated 55791.26513671875 
[2025-03-12 19:22:31 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 16 loss:0.5532521605491638 norm:0.0004528881108853966 max memory_allocated 55791.26513671875 
[2025-03-12 19:23:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 17 loss:0.5530436038970947 norm:0.00044929166324436665 max memory_allocated 55791.26513671875 
[2025-03-12 19:25:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 18 loss:0.5529118180274963 norm:0.00044675031676888466 max memory_allocated 55791.26513671875 
[2025-03-12 19:26:15 root] (abq_llm_calibration_5_1.py 420): INFO layer 23 iter 19 loss:0.5527834892272949 norm:0.0004455777525436133 max memory_allocated 55791.26513671875 
[2025-03-12 19:26:16 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 23 ===
[2025-03-12 19:28:33 root] (abq_llm_calibration_5_1.py 511): INFO Layer 23 Results:
[2025-03-12 19:28:33 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 19:28:33 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.083002
[2025-03-12 19:28:33 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994653
[2025-03-12 19:28:41 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 24 ===
[2025-03-12 19:29:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 0 loss:0.6793162822723389 norm:0.013606786727905273 max memory_allocated 55791.26513671875 
[2025-03-12 19:31:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 1 loss:0.663917601108551 norm:0.005794623866677284 max memory_allocated 55791.26513671875 
[2025-03-12 19:32:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 2 loss:0.6506158113479614 norm:0.0027575702406466007 max memory_allocated 55791.26513671875 
[2025-03-12 19:33:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 3 loss:0.6426525712013245 norm:0.0012501110322773457 max memory_allocated 55791.26513671875 
[2025-03-12 19:34:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 4 loss:0.6394558548927307 norm:0.0007324950420297682 max memory_allocated 55791.26513671875 
[2025-03-12 19:36:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 5 loss:0.6382876634597778 norm:0.0007162386318668723 max memory_allocated 55791.26513671875 
[2025-03-12 19:37:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 6 loss:0.6370247006416321 norm:0.0006486164056695998 max memory_allocated 55791.26513671875 
[2025-03-12 19:38:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 7 loss:0.6360209584236145 norm:0.0006303737754933536 max memory_allocated 55791.26513671875 
[2025-03-12 19:39:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 8 loss:0.6352002024650574 norm:0.0006210425635799766 max memory_allocated 55791.26513671875 
[2025-03-12 19:41:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 9 loss:0.63448166847229 norm:0.0006296398350968957 max memory_allocated 55791.26513671875 
[2025-03-12 19:42:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 10 loss:0.63374924659729 norm:0.0006298389635048807 max memory_allocated 55791.26513671875 
[2025-03-12 19:43:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 11 loss:0.6331734657287598 norm:0.0006177958566695452 max memory_allocated 55791.26513671875 
[2025-03-12 19:44:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 12 loss:0.6327373385429382 norm:0.000616918841842562 max memory_allocated 55791.26513671875 
[2025-03-12 19:46:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 13 loss:0.6321724057197571 norm:0.0005899512907490134 max memory_allocated 55791.26513671875 
[2025-03-12 19:47:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 14 loss:0.631729245185852 norm:0.0005738638574257493 max memory_allocated 55791.26513671875 
[2025-03-12 19:48:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 15 loss:0.6314408779144287 norm:0.0005671438993886113 max memory_allocated 55791.26513671875 
[2025-03-12 19:49:54 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 16 loss:0.6312029361724854 norm:0.0005633135442622006 max memory_allocated 55791.26513671875 
[2025-03-12 19:51:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 17 loss:0.6310030221939087 norm:0.000555682519916445 max memory_allocated 55791.26513671875 
[2025-03-12 19:52:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 18 loss:0.6308264136314392 norm:0.0005442052497528493 max memory_allocated 55791.26513671875 
[2025-03-12 19:53:38 root] (abq_llm_calibration_5_1.py 420): INFO layer 24 iter 19 loss:0.6306931972503662 norm:0.0005424710107035935 max memory_allocated 55791.26513671875 
[2025-03-12 19:53:39 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 24 ===
[2025-03-12 19:55:48 root] (abq_llm_calibration_5_1.py 511): INFO Layer 24 Results:
[2025-03-12 19:55:48 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 19:55:48 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.076606
[2025-03-12 19:55:48 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.995061
[2025-03-12 19:55:57 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 25 ===
[2025-03-12 19:57:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 0 loss:0.7806077599525452 norm:0.021025169640779495 max memory_allocated 55791.26513671875 
[2025-03-12 19:58:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 1 loss:0.7648406028747559 norm:0.012982664629817009 max memory_allocated 55791.26513671875 
[2025-03-12 19:59:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 2 loss:0.7498937249183655 norm:0.008288215845823288 max memory_allocated 55791.26513671875 
[2025-03-12 20:00:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 3 loss:0.7428026795387268 norm:0.005790929310023785 max memory_allocated 55791.26513671875 
[2025-03-12 20:02:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 4 loss:0.739905595779419 norm:0.004544312134385109 max memory_allocated 55791.26513671875 
[2025-03-12 20:03:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 5 loss:0.738215446472168 norm:0.003795199329033494 max memory_allocated 55791.26513671875 
[2025-03-12 20:04:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 6 loss:0.7363499999046326 norm:0.003272846806794405 max memory_allocated 55791.26513671875 
[2025-03-12 20:05:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 7 loss:0.734537661075592 norm:0.002815061481669545 max memory_allocated 55791.26513671875 
[2025-03-12 20:07:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 8 loss:0.7329229116439819 norm:0.0024772505275905132 max memory_allocated 55791.26513671875 
[2025-03-12 20:08:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 9 loss:0.731549084186554 norm:0.0021972174290567636 max memory_allocated 55791.26513671875 
[2025-03-12 20:09:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 10 loss:0.7306081652641296 norm:0.0020010925363749266 max memory_allocated 55791.26513671875 
[2025-03-12 20:10:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 11 loss:0.7298461198806763 norm:0.0018317897338420153 max memory_allocated 55791.26513671875 
[2025-03-12 20:12:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 12 loss:0.7288633584976196 norm:0.0016508946428075433 max memory_allocated 55791.26513671875 
[2025-03-12 20:13:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 13 loss:0.7279313206672668 norm:0.0015020972350612283 max memory_allocated 55791.26513671875 
[2025-03-12 20:14:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 14 loss:0.727152943611145 norm:0.001383271417580545 max memory_allocated 55791.26513671875 
[2025-03-12 20:15:54 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 15 loss:0.7265903949737549 norm:0.0012847117614001036 max memory_allocated 55791.26513671875 
[2025-03-12 20:17:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 16 loss:0.7262354493141174 norm:0.0012127523077651858 max memory_allocated 55791.26513671875 
[2025-03-12 20:18:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 17 loss:0.7253816723823547 norm:0.0010085311951115727 max memory_allocated 55791.26513671875 
[2025-03-12 20:19:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 18 loss:0.7237717509269714 norm:0.0006547519005835056 max memory_allocated 55791.26513671875 
[2025-03-12 20:20:53 root] (abq_llm_calibration_5_1.py 420): INFO layer 25 iter 19 loss:0.7231689095497131 norm:0.0006494913250207901 max memory_allocated 55791.26513671875 
[2025-03-12 20:20:54 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 25 ===
[2025-03-12 20:22:40 root] (abq_llm_calibration_5_1.py 511): INFO Layer 25 Results:
[2025-03-12 20:22:40 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 20:22:40 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.084104
[2025-03-12 20:22:40 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994571
[2025-03-12 20:22:49 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 26 ===
[2025-03-12 20:24:07 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 0 loss:0.8740131258964539 norm:0.008886289782822132 max memory_allocated 55791.26513671875 
[2025-03-12 20:25:21 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 1 loss:0.861291766166687 norm:0.0043694195337593555 max memory_allocated 55791.26513671875 
[2025-03-12 20:26:36 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 2 loss:0.8462907671928406 norm:0.002020588843151927 max memory_allocated 55791.26513671875 
[2025-03-12 20:27:51 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 3 loss:0.8381449580192566 norm:0.0010905414819717407 max memory_allocated 55791.26513671875 
[2025-03-12 20:29:05 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 4 loss:0.8351977467536926 norm:0.000825944181997329 max memory_allocated 55791.26513671875 
[2025-03-12 20:30:20 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 5 loss:0.833349347114563 norm:0.00071942456997931 max memory_allocated 55791.26513671875 
[2025-03-12 20:31:35 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 6 loss:0.8319789171218872 norm:0.0006556487642228603 max memory_allocated 55791.26513671875 
[2025-03-12 20:32:49 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 7 loss:0.8306267857551575 norm:0.0006176609313115478 max memory_allocated 55791.26513671875 
[2025-03-12 20:34:04 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 8 loss:0.829308032989502 norm:0.0005796150071546435 max memory_allocated 55791.26513671875 
[2025-03-12 20:35:19 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 9 loss:0.8282263278961182 norm:0.0005593838868662715 max memory_allocated 55791.26513671875 
[2025-03-12 20:36:34 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 10 loss:0.8272665739059448 norm:0.0005549229681491852 max memory_allocated 55791.26513671875 
[2025-03-12 20:37:48 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 11 loss:0.82645583152771 norm:0.0005391138838604093 max memory_allocated 55791.26513671875 
[2025-03-12 20:39:03 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 12 loss:0.8257938623428345 norm:0.00053038151236251 max memory_allocated 55791.26513671875 
[2025-03-12 20:40:18 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 13 loss:0.825235903263092 norm:0.0005242089391686022 max memory_allocated 55791.26513671875 
[2025-03-12 20:41:32 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 14 loss:0.8248445391654968 norm:0.0005171896773390472 max memory_allocated 55791.26513671875 
[2025-03-12 20:42:47 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 15 loss:0.8245068192481995 norm:0.0005113776423968375 max memory_allocated 55791.26513671875 
[2025-03-12 20:44:02 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 16 loss:0.8242842555046082 norm:0.000505421485286206 max memory_allocated 55791.26513671875 
[2025-03-12 20:45:17 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 17 loss:0.8240481019020081 norm:0.0005044196732342243 max memory_allocated 55791.26513671875 
[2025-03-12 20:46:31 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 18 loss:0.8238264322280884 norm:0.0004962841048836708 max memory_allocated 55791.26513671875 
[2025-03-12 20:47:46 root] (abq_llm_calibration_5_1.py 420): INFO layer 26 iter 19 loss:0.8236581683158875 norm:0.0004939916543662548 max memory_allocated 55791.26513671875 
[2025-03-12 20:47:48 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 26 ===
[2025-03-12 20:49:19 root] (abq_llm_calibration_5_1.py 511): INFO Layer 26 Results:
[2025-03-12 20:49:19 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 20:49:19 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.078719
[2025-03-12 20:49:19 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994944
[2025-03-12 20:49:28 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 27 ===
[2025-03-12 20:50:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 0 loss:0.999687910079956 norm:0.016036348417401314 max memory_allocated 55791.26513671875 
[2025-03-12 20:52:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 1 loss:0.9817376136779785 norm:0.00682250177487731 max memory_allocated 55791.26513671875 
[2025-03-12 20:53:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 2 loss:0.9659420251846313 norm:0.003297321032732725 max memory_allocated 55791.26513671875 
[2025-03-12 20:54:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 3 loss:0.9566075801849365 norm:0.0018665025709196925 max memory_allocated 55791.26513671875 
[2025-03-12 20:55:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 4 loss:0.9524255990982056 norm:0.0013763648457825184 max memory_allocated 55791.26513671875 
[2025-03-12 20:56:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 5 loss:0.949951708316803 norm:0.0009821277344599366 max memory_allocated 55791.26513671875 
[2025-03-12 20:58:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 6 loss:0.9480825066566467 norm:0.0007097618654370308 max memory_allocated 55791.26513671875 
[2025-03-12 20:59:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 7 loss:0.946487545967102 norm:0.0006495918496511877 max memory_allocated 55791.26513671875 
[2025-03-12 21:00:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 8 loss:0.9449269771575928 norm:0.0006189881823956966 max memory_allocated 55791.26513671875 
[2025-03-12 21:01:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 9 loss:0.943729043006897 norm:0.0006138210883364081 max memory_allocated 55791.26513671875 
[2025-03-12 21:03:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 10 loss:0.94267338514328 norm:0.0005920641124248505 max memory_allocated 55791.26513671875 
[2025-03-12 21:04:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 11 loss:0.9417888522148132 norm:0.0005758337210863829 max memory_allocated 55791.26513671875 
[2025-03-12 21:05:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 12 loss:0.941022515296936 norm:0.0005709706456400454 max memory_allocated 55791.26513671875 
[2025-03-12 21:06:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 13 loss:0.9404071569442749 norm:0.0005666504148393869 max memory_allocated 55791.26513671875 
[2025-03-12 21:08:11 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 14 loss:0.9399175047874451 norm:0.0005656973808072507 max memory_allocated 55791.26513671875 
[2025-03-12 21:09:26 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 15 loss:0.9395560026168823 norm:0.0005581449950113893 max memory_allocated 55791.26513671875 
[2025-03-12 21:10:41 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 16 loss:0.9391816854476929 norm:0.000554444151930511 max memory_allocated 55791.26513671875 
[2025-03-12 21:11:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 17 loss:0.9388189315795898 norm:0.0005635770503431559 max memory_allocated 55791.26513671875 
[2025-03-12 21:13:10 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 18 loss:0.9385433197021484 norm:0.0005588562344200909 max memory_allocated 55791.26513671875 
[2025-03-12 21:14:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 27 iter 19 loss:0.9383346438407898 norm:0.0005546779138967395 max memory_allocated 55791.26513671875 
[2025-03-12 21:14:26 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 27 ===
[2025-03-12 21:15:49 root] (abq_llm_calibration_5_1.py 511): INFO Layer 27 Results:
[2025-03-12 21:15:49 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 21:15:49 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.083488
[2025-03-12 21:15:49 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.994624
[2025-03-12 21:15:58 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 28 ===
[2025-03-12 21:16:01 root] (abq_llm_calibration_5_1.py 278): INFO use compensation vector
[2025-03-12 21:17:15 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 0 loss:178.22291564941406 norm:2414.624267578125 max memory_allocated 55791.26513671875 
[2025-03-12 21:18:30 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 1 loss:1.3836085796356201 norm:0.04661662131547928 max memory_allocated 55791.26513671875 
[2025-03-12 21:19:45 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 2 loss:1.3835781812667847 norm:0.04667283594608307 max memory_allocated 55791.26513671875 
[2025-03-12 21:21:00 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 3 loss:1.3834636211395264 norm:0.04660999774932861 max memory_allocated 55791.26513671875 
[2025-03-12 21:22:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 4 loss:1.3834514617919922 norm:0.046686626970767975 max memory_allocated 55791.26513671875 
[2025-03-12 21:23:29 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 5 loss:1.3834258317947388 norm:0.046643540263175964 max memory_allocated 55791.26513671875 
[2025-03-12 21:24:44 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 6 loss:1.3834428787231445 norm:0.04654710367321968 max memory_allocated 55791.26513671875 
[2025-03-12 21:25:59 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 7 loss:1.3832902908325195 norm:0.04645906016230583 max memory_allocated 55791.26513671875 
[2025-03-12 21:27:14 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 8 loss:1.383176565170288 norm:0.04648049175739288 max memory_allocated 55791.26513671875 
[2025-03-12 21:28:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 9 loss:1.3829491138458252 norm:0.046538155525922775 max memory_allocated 55791.26513671875 
[2025-03-12 21:29:43 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 10 loss:1.3829665184020996 norm:0.04642282426357269 max memory_allocated 55791.26513671875 
[2025-03-12 21:30:58 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 11 loss:1.3829654455184937 norm:0.046427927911281586 max memory_allocated 55791.26513671875 
[2025-03-12 21:32:13 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 12 loss:1.3827190399169922 norm:0.04641488194465637 max memory_allocated 55791.26513671875 
[2025-03-12 21:33:28 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 13 loss:1.382350206375122 norm:0.04650690034031868 max memory_allocated 55791.26513671875 
[2025-03-12 21:34:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 14 loss:1.3822649717330933 norm:0.04645129293203354 max memory_allocated 55791.26513671875 
[2025-03-12 21:35:57 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 15 loss:1.3819122314453125 norm:0.046397797763347626 max memory_allocated 55791.26513671875 
[2025-03-12 21:37:12 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 16 loss:1.3795727491378784 norm:0.04630517214536667 max memory_allocated 55791.26513671875 
[2025-03-12 21:38:27 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 17 loss:1.379130482673645 norm:0.045676086097955704 max memory_allocated 55791.26513671875 
[2025-03-12 21:39:42 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 18 loss:1.3784573078155518 norm:0.04540897160768509 max memory_allocated 55791.26513671875 
[2025-03-12 21:40:56 root] (abq_llm_calibration_5_1.py 420): INFO layer 28 iter 19 loss:1.3782374858856201 norm:0.04533162713050842 max memory_allocated 55791.26513671875 
[2025-03-12 21:40:57 root] (abq_llm_calibration_5_1.py 439): INFO === Analyzing end-to-end MSE for layer 28 ===
[2025-03-12 21:41:59 root] (abq_llm_calibration_5_1.py 511): INFO Layer 28 Results:
[2025-03-12 21:41:59 root] (abq_llm_calibration_5_1.py 512): INFO   Look-ahead layers: 2
[2025-03-12 21:41:59 root] (abq_llm_calibration_5_1.py 513): INFO   End-to-End MSE: 0.210548
[2025-03-12 21:41:59 root] (abq_llm_calibration_5_1.py 514): INFO   Cosine Similarity: 0.986535
[2025-03-12 21:42:07 root] (abq_llm_calibration_5_1.py 218): INFO === Start quantize layer 29 ===
[2025-03-12 21:42:10 root] (abq_llm_calibration_5_1.py 278): INFO use compensation vector
[2025-03-12 21:43:25 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 0 loss:3.279611110687256 norm:6.985357284545898 max memory_allocated 55791.26513671875 
[2025-03-12 21:44:40 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 1 loss:2.367781162261963 norm:0.40183037519454956 max memory_allocated 55791.26513671875 
[2025-03-12 21:45:55 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 2 loss:2.242863655090332 norm:0.17731252312660217 max memory_allocated 55791.26513671875 
[2025-03-12 21:47:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 3 loss:2.194434881210327 norm:0.12784957885742188 max memory_allocated 55791.26513671875 
[2025-03-12 21:48:24 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 4 loss:2.161646842956543 norm:0.11943361163139343 max memory_allocated 55791.26513671875 
[2025-03-12 21:49:39 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 5 loss:2.1273040771484375 norm:0.11254937946796417 max memory_allocated 55791.26513671875 
[2025-03-12 21:50:54 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 6 loss:2.0951242446899414 norm:0.14998701214790344 max memory_allocated 55791.26513671875 
[2025-03-12 21:52:09 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 7 loss:2.0626373291015625 norm:0.12844650447368622 max memory_allocated 55791.26513671875 
[2025-03-12 21:53:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 8 loss:2.037916421890259 norm:0.20949745178222656 max memory_allocated 55791.26513671875 
[2025-03-12 21:54:38 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 9 loss:2.0172154903411865 norm:0.2009810358285904 max memory_allocated 55791.26513671875 
[2025-03-12 21:55:53 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 10 loss:1.9816999435424805 norm:0.2044905126094818 max memory_allocated 55791.26513671875 
[2025-03-12 21:57:08 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 11 loss:1.9929205179214478 norm:0.32398611307144165 max memory_allocated 55791.26513671875 
[2025-03-12 21:58:23 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 12 loss:1.9643714427947998 norm:0.16925030946731567 max memory_allocated 55791.26513671875 
[2025-03-12 21:59:37 root] (abq_llm_calibration_5_1.py 420): INFO layer 29 iter 13 loss:1.9410089254379272 norm:0.19481614232063293 max memory_allocated 55791.26513671875 
