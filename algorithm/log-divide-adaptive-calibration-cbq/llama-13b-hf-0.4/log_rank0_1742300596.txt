[2025-03-18 12:23:16 root] (main_calib_config3_cbq.py 280): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-divide-adaptive-calibration-cbq/llama-13b-hf-0.4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.4.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-18 12:29:31 root] (main_calib_config3_cbq.py 347): INFO === start quantization ===
[2025-03-18 12:29:31 root] (main_calib_config3_cbq.py 353): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-18 12:29:32 root] (abq_llm_calib_config3_cbq.py 86): INFO Starting ...
[2025-03-18 12:29:32 root] (abq_llm_calib_config3_cbq.py 93): INFO Loaded quant_map from log-divide-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.4.pkl
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[0]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 0 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 0 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 0 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 0 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 0 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 0 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[1]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 0}
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 1 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 1 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 1 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 1 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 1 module mlp.down_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 1 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[2]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 1, 'mlp.down_proj': 0}
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 2 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 2 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 2 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 2 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 2 module mlp.down_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 2 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[3]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 3 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 3 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 3 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 3 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 3 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:38 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 3 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[4]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 4 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 4 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 4 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 4 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 4 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 4 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[5]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 5 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 5 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 5 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 5 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 5 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 5 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[6]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 0, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 6 module self_attn.k_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 6 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 6 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 6 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 6 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 6 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[7]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 7 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 7 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 7 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 7 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 7 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 7 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[8]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 8 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 8 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 8 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 8 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 8 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:39 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 8 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[9]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 9 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 9 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 9 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 9 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 9 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 9 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[10]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 10 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 10 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 10 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 10 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 10 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 10 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[11]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 11 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 11 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 11 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 11 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 11 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 11 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[12]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 12 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 12 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 12 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 12 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 12 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 12 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[13]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 13 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 13 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 13 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 13 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 13 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:40 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 13 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[14]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 14 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 14 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 14 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 14 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 14 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 14 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[15]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 15 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 15 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 15 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 15 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 15 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 15 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[16]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 16 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 16 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 16 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 16 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 16 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 16 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[17]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 17 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 17 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 17 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 17 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 17 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:41 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 17 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[18]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 18 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 18 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 18 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 18 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 18 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 18 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[19]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 19 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 19 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 19 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 19 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 19 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 19 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[20]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 20 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 20 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 20 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 20 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 20 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 20 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[21]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 21 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 21 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 21 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 21 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 21 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 21 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[22]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 22 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 22 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 22 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 22 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 22 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:42 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 22 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[23]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 23 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 23 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 23 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 23 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 23 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 23 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[24]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 24 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 24 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 24 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 24 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 24 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 24 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[25]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 25 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 25 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 25 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 25 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 25 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 25 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[26]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 26 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 26 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 26 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 26 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 26 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 26 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[27]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 27 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 27 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 27 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 27 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 27 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:43 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 27 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[28]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 28 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 28 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 28 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 28 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 28 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 28 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[29]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 29 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 29 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 29 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 29 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 29 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 29 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[30]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 30 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 30 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 30 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 30 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 30 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 30 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[31]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 31 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 31 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 31 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 31 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 31 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:44 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 31 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[32]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 32 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 32 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 32 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 32 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 32 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 32 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[33]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 33 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 33 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 33 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 33 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 33 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 33 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[34]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 34 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 34 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 34 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 34 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 34 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 34 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[35]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 35 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 35 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 35 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 35 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 35 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 35 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[36]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 0, 'self_attn.o_proj': 1, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 36 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 36 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 36 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 36 module self_attn.o_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 36 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:45 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 36 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[37]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 1, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 37 module self_attn.k_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 37 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 37 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 37 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 37 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 37 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[38]: {'self_attn.q_proj': 1, 'self_attn.k_proj': 0, 'self_attn.v_proj': 1, 'self_attn.o_proj': 0, 'mlp.up_proj': 0, 'mlp.down_proj': 1}
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 38 module self_attn.k_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 38 module self_attn.v_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 38 module self_attn.q_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 38 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 38 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 38 module mlp.up_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 254): INFO quant_map[39]: {'self_attn.q_proj': 0, 'self_attn.k_proj': 0, 'self_attn.v_proj': 0, 'self_attn.o_proj': 0, 'mlp.up_proj': 1, 'mlp.down_proj': 1}
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 39 module self_attn.k_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 39 module self_attn.v_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 39 module self_attn.q_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 39 module self_attn.o_proj scheme w4a4 wbit 4 abits 4
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 39 module mlp.down_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 259): INFO layer 39 module mlp.up_proj scheme w4a8 wbit 4 abits 8
[2025-03-18 12:29:46 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 0 to 1 ===
[2025-03-18 12:31:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 0 loss:0.12242323160171509 norm:0.11796549707651138 max memory_allocated 54635.62939453125 
[2025-03-18 12:33:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 1 loss:0.08453290164470673 norm:0.10264705121517181 max memory_allocated 54635.62939453125 
[2025-03-18 12:35:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 2 loss:0.06956057995557785 norm:0.07012465596199036 max memory_allocated 54635.62939453125 
[2025-03-18 12:36:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 3 loss:0.06312809884548187 norm:0.059773676097393036 max memory_allocated 54635.62939453125 
[2025-03-18 12:38:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 4 loss:0.05979522317647934 norm:0.053194720298051834 max memory_allocated 54635.62939453125 
[2025-03-18 12:40:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 5 loss:0.057186514139175415 norm:0.04766252264380455 max memory_allocated 54635.62939453125 
[2025-03-18 12:41:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 6 loss:0.055114466696977615 norm:0.03710327297449112 max memory_allocated 54635.62939453125 
[2025-03-18 12:43:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 7 loss:0.05349750071763992 norm:0.030474815517663956 max memory_allocated 54635.62939453125 
[2025-03-18 12:45:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 8 loss:0.05238274112343788 norm:0.028862250968813896 max memory_allocated 54635.62939453125 
[2025-03-18 12:46:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 9 loss:0.051511991769075394 norm:0.025198524817824364 max memory_allocated 54635.62939453125 
[2025-03-18 12:48:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 10 loss:0.05102170631289482 norm:0.02247912436723709 max memory_allocated 54635.62939453125 
[2025-03-18 12:50:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 11 loss:0.050470806658267975 norm:0.01948102004826069 max memory_allocated 54635.62939453125 
[2025-03-18 12:51:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 12 loss:0.050169169902801514 norm:0.018849462270736694 max memory_allocated 54635.62939453125 
[2025-03-18 12:53:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 13 loss:0.04984385520219803 norm:0.016763310879468918 max memory_allocated 54635.62939453125 
[2025-03-18 12:54:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 14 loss:0.04943351447582245 norm:0.01648833602666855 max memory_allocated 54635.62939453125 
[2025-03-18 12:56:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 15 loss:0.0492100715637207 norm:0.015583114698529243 max memory_allocated 54635.62939453125 
[2025-03-18 12:57:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 16 loss:0.04899270460009575 norm:0.014461491256952286 max memory_allocated 54635.62939453125 
[2025-03-18 12:59:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 17 loss:0.04885904863476753 norm:0.014195742085576057 max memory_allocated 54635.62939453125 
[2025-03-18 13:01:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 18 loss:0.04849501699209213 norm:0.012374643236398697 max memory_allocated 54635.62939453125 
[2025-03-18 13:02:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 0-1 epoch 19 loss:0.04841409623622894 norm:0.012624205090105534 max memory_allocated 54635.62939453125 
[2025-03-18 13:03:26 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 0-1
[2025-03-18 13:03:26 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 1 to 2 ===
[2025-03-18 13:05:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 0 loss:0.19305098056793213 norm:0.04240444302558899 max memory_allocated 64876.46533203125 
[2025-03-18 13:07:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 1 loss:0.14064322412014008 norm:0.031036119908094406 max memory_allocated 64876.46533203125 
[2025-03-18 13:08:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 2 loss:0.12275190651416779 norm:0.029062515124678612 max memory_allocated 64876.46533203125 
[2025-03-18 13:10:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 3 loss:0.11070119589567184 norm:0.02064692974090576 max memory_allocated 64876.46533203125 
[2025-03-18 13:12:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 4 loss:0.10435989499092102 norm:0.01889721304178238 max memory_allocated 64876.46533203125 
[2025-03-18 13:13:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 5 loss:0.10075181722640991 norm:0.0172325000166893 max memory_allocated 64876.46533203125 
[2025-03-18 13:15:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 6 loss:0.09850244224071503 norm:0.015579313039779663 max memory_allocated 64876.46533203125 
[2025-03-18 13:17:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 7 loss:0.09655983746051788 norm:0.015113910660147667 max memory_allocated 64876.46533203125 
[2025-03-18 13:18:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 8 loss:0.09521815180778503 norm:0.015388929285109043 max memory_allocated 64876.46533203125 
[2025-03-18 13:20:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 9 loss:0.09346024692058563 norm:0.013952582143247128 max memory_allocated 64876.46533203125 
[2025-03-18 13:22:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 10 loss:0.09232468903064728 norm:0.014027419500052929 max memory_allocated 64876.46533203125 
[2025-03-18 13:24:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 11 loss:0.09129559993743896 norm:0.01297084428369999 max memory_allocated 64876.46533203125 
[2025-03-18 13:25:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 12 loss:0.08973144739866257 norm:0.012259026989340782 max memory_allocated 64876.46533203125 
[2025-03-18 13:27:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 13 loss:0.08953956514596939 norm:0.013223318383097649 max memory_allocated 64876.46533203125 
[2025-03-18 13:29:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 14 loss:0.08901268243789673 norm:0.012014240026473999 max memory_allocated 64876.46533203125 
[2025-03-18 13:31:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 15 loss:0.08901842683553696 norm:0.012439311482012272 max memory_allocated 64876.46533203125 
[2025-03-18 13:32:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 16 loss:0.08813930302858353 norm:0.012154169380664825 max memory_allocated 64876.46533203125 
[2025-03-18 13:34:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 17 loss:0.08776413649320602 norm:0.010501008480787277 max memory_allocated 64876.46533203125 
[2025-03-18 13:36:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 18 loss:0.08751972019672394 norm:0.011612609960138798 max memory_allocated 64876.46533203125 
[2025-03-18 13:37:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 1-2 epoch 19 loss:0.08785850554704666 norm:0.011751489713788033 max memory_allocated 64876.46533203125 
[2025-03-18 13:38:16 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 1-2
[2025-03-18 13:38:17 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 2 to 3 ===
[2025-03-18 13:40:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 0 loss:0.3409753143787384 norm:0.05734360218048096 max memory_allocated 64876.46533203125 
[2025-03-18 13:42:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 1 loss:0.13868194818496704 norm:0.036510590463876724 max memory_allocated 64876.46533203125 
[2025-03-18 13:43:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 2 loss:0.09485520422458649 norm:0.020810917019844055 max memory_allocated 64876.46533203125 
[2025-03-18 13:45:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 3 loss:0.08336126804351807 norm:0.01627616584300995 max memory_allocated 64876.46533203125 
[2025-03-18 13:47:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 4 loss:0.07854105532169342 norm:0.01647532917559147 max memory_allocated 64876.46533203125 
[2025-03-18 13:48:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 5 loss:0.07306641340255737 norm:0.015353033319115639 max memory_allocated 64876.46533203125 
[2025-03-18 13:50:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 6 loss:0.07000787556171417 norm:0.018730761483311653 max memory_allocated 64876.46533203125 
[2025-03-18 13:52:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 7 loss:0.06694816797971725 norm:0.01939920149743557 max memory_allocated 64876.46533203125 
[2025-03-18 13:53:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 8 loss:0.06446287035942078 norm:0.02116360142827034 max memory_allocated 64876.46533203125 
[2025-03-18 13:55:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 9 loss:0.06231241673231125 norm:0.02039686217904091 max memory_allocated 64876.46533203125 
[2025-03-18 13:57:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 10 loss:0.06233822926878929 norm:0.024905752390623093 max memory_allocated 64876.46533203125 
[2025-03-18 13:58:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 11 loss:0.06295785307884216 norm:0.02598046325147152 max memory_allocated 64876.46533203125 
[2025-03-18 14:00:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 12 loss:0.060457102954387665 norm:0.020744159817695618 max memory_allocated 64876.46533203125 
[2025-03-18 14:02:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 13 loss:0.060458578169345856 norm:0.020566236227750778 max memory_allocated 64876.46533203125 
[2025-03-18 14:04:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 14 loss:0.06016306206583977 norm:0.02038666605949402 max memory_allocated 64876.46533203125 
[2025-03-18 14:05:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 15 loss:0.06002186983823776 norm:0.019254609942436218 max memory_allocated 64876.46533203125 
[2025-03-18 14:07:35 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 16 loss:0.060520462691783905 norm:0.02070789411664009 max memory_allocated 64876.46533203125 
[2025-03-18 14:09:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 17 loss:0.05979008972644806 norm:0.017814867198467255 max memory_allocated 64876.46533203125 
[2025-03-18 14:11:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 18 loss:0.059531502425670624 norm:0.01699581742286682 max memory_allocated 64876.46533203125 
[2025-03-18 14:12:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 2-3 epoch 19 loss:0.05925874412059784 norm:0.016550790518522263 max memory_allocated 64876.46533203125 
[2025-03-18 14:13:12 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 2-3
[2025-03-18 14:13:12 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 3 to 4 ===
[2025-03-18 14:15:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 0 loss:0.09386429935693741 norm:0.010745832696557045 max memory_allocated 64876.46533203125 
[2025-03-18 14:17:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 1 loss:0.05675125494599342 norm:0.003423200221732259 max memory_allocated 64876.46533203125 
[2025-03-18 14:18:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 2 loss:0.044658925384283066 norm:0.0021711159497499466 max memory_allocated 64876.46533203125 
[2025-03-18 14:20:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 3 loss:0.03946357220411301 norm:0.001445875852368772 max memory_allocated 64876.46533203125 
[2025-03-18 14:22:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 4 loss:0.036501605063676834 norm:0.0010784990154206753 max memory_allocated 64876.46533203125 
[2025-03-18 14:23:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 5 loss:0.03480009362101555 norm:0.0008887422736734152 max memory_allocated 64876.46533203125 
[2025-03-18 14:25:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 6 loss:0.033824093639850616 norm:0.0007852985290810466 max memory_allocated 64876.46533203125 
[2025-03-18 14:27:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 7 loss:0.03320694714784622 norm:0.0007147856522351503 max memory_allocated 64876.46533203125 
[2025-03-18 14:28:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 8 loss:0.03274672478437424 norm:0.0006688725552521646 max memory_allocated 64876.46533203125 
[2025-03-18 14:30:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 9 loss:0.03235938772559166 norm:0.0006836778484284878 max memory_allocated 64876.46533203125 
[2025-03-18 14:32:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 10 loss:0.03209672123193741 norm:0.0006556320586241782 max memory_allocated 64876.46533203125 
[2025-03-18 14:33:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 11 loss:0.031841475516557693 norm:0.0006137198070064187 max memory_allocated 64876.46533203125 
[2025-03-18 14:35:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 12 loss:0.03161710128188133 norm:0.0006005590548738837 max memory_allocated 64876.46533203125 
[2025-03-18 14:37:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 13 loss:0.031326472759246826 norm:0.0005798048223368824 max memory_allocated 64876.46533203125 
[2025-03-18 14:38:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 14 loss:0.03111947700381279 norm:0.0005859213415533304 max memory_allocated 64876.46533203125 
[2025-03-18 14:40:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 15 loss:0.030935438349843025 norm:0.0005844281404279172 max memory_allocated 64876.46533203125 
[2025-03-18 14:42:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 16 loss:0.030812010169029236 norm:0.0006136990850791335 max memory_allocated 64876.46533203125 
[2025-03-18 14:44:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 17 loss:0.030729684978723526 norm:0.0005762988002970815 max memory_allocated 64876.46533203125 
[2025-03-18 14:45:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 18 loss:0.03057008609175682 norm:0.000582081382162869 max memory_allocated 64876.46533203125 
[2025-03-18 14:47:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 3-4 epoch 19 loss:0.030444884672760963 norm:0.0005301431519910693 max memory_allocated 64876.46533203125 
[2025-03-18 14:48:08 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 3-4
[2025-03-18 14:48:08 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 4 to 5 ===
[2025-03-18 14:50:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 0 loss:0.11705121397972107 norm:0.024708986282348633 max memory_allocated 64876.59423828125 
[2025-03-18 14:52:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 1 loss:0.07341069728136063 norm:0.008173856884241104 max memory_allocated 64876.59423828125 
[2025-03-18 14:53:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 2 loss:0.05779233202338219 norm:0.004750286228954792 max memory_allocated 64876.59423828125 
[2025-03-18 14:55:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 3 loss:0.05109072104096413 norm:0.003406126517802477 max memory_allocated 64876.59423828125 
[2025-03-18 14:57:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 4 loss:0.04767286032438278 norm:0.0027870151679962873 max memory_allocated 64876.59423828125 
[2025-03-18 14:58:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 5 loss:0.04558570683002472 norm:0.0022917259484529495 max memory_allocated 64876.59423828125 
[2025-03-18 15:00:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 6 loss:0.044126346707344055 norm:0.001988636329770088 max memory_allocated 64876.59423828125 
[2025-03-18 15:02:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 7 loss:0.043443117290735245 norm:0.0017864257097244263 max memory_allocated 64876.59423828125 
[2025-03-18 15:03:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 8 loss:0.04273434728384018 norm:0.001619587535969913 max memory_allocated 64876.59423828125 
[2025-03-18 15:05:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 9 loss:0.04216698184609413 norm:0.0014636667910963297 max memory_allocated 64876.59423828125 
[2025-03-18 15:07:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 10 loss:0.04172488674521446 norm:0.0013145080301910639 max memory_allocated 64876.59423828125 
[2025-03-18 15:08:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 11 loss:0.04142723232507706 norm:0.001202276675030589 max memory_allocated 64876.59423828125 
[2025-03-18 15:10:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 12 loss:0.04131705313920975 norm:0.0011586889158934355 max memory_allocated 64876.59423828125 
[2025-03-18 15:12:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 13 loss:0.041245296597480774 norm:0.0011084787547588348 max memory_allocated 64876.59423828125 
[2025-03-18 15:13:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 14 loss:0.04110400378704071 norm:0.0010316349798813462 max memory_allocated 64876.59423828125 
[2025-03-18 15:15:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 15 loss:0.040962960571050644 norm:0.00103805644903332 max memory_allocated 64876.59423828125 
[2025-03-18 15:17:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 16 loss:0.04077230021357536 norm:0.000985573511570692 max memory_allocated 64876.59423828125 
[2025-03-18 15:18:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 17 loss:0.04077862203121185 norm:0.0009253781172446907 max memory_allocated 64876.59423828125 
[2025-03-18 15:20:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 18 loss:0.040777429938316345 norm:0.0009283606195822358 max memory_allocated 64876.59423828125 
[2025-03-18 15:22:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 4-5 epoch 19 loss:0.04089159518480301 norm:0.0009575590956956148 max memory_allocated 64876.59423828125 
[2025-03-18 15:22:59 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 4-5
[2025-03-18 15:23:00 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 5 to 6 ===
[2025-03-18 15:25:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 0 loss:0.26557666063308716 norm:0.05338228493928909 max memory_allocated 64876.78173828125 
[2025-03-18 15:26:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 1 loss:0.1318366527557373 norm:0.009086349047720432 max memory_allocated 64876.78173828125 
[2025-03-18 15:28:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 2 loss:0.09772083163261414 norm:0.005543295294046402 max memory_allocated 64876.78173828125 
[2025-03-18 15:30:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 3 loss:0.08288034796714783 norm:0.004245983902364969 max memory_allocated 64876.78173828125 
[2025-03-18 15:31:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 4 loss:0.07486582547426224 norm:0.003474865108728409 max memory_allocated 64876.78173828125 
[2025-03-18 15:33:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 5 loss:0.06962398439645767 norm:0.0030649383552372456 max memory_allocated 64876.78173828125 
[2025-03-18 15:35:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 6 loss:0.06547384709119797 norm:0.0027798283845186234 max memory_allocated 64876.78173828125 
[2025-03-18 15:36:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 7 loss:0.0627567321062088 norm:0.003093043342232704 max memory_allocated 64876.78173828125 
[2025-03-18 15:38:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 8 loss:0.06056428700685501 norm:0.0025920099578797817 max memory_allocated 64876.78173828125 
[2025-03-18 15:40:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 9 loss:0.05859770625829697 norm:0.0025418568402528763 max memory_allocated 64876.78173828125 
[2025-03-18 15:41:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 10 loss:0.05750761181116104 norm:0.002952917478978634 max memory_allocated 64876.78173828125 
[2025-03-18 15:43:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 11 loss:0.05660053342580795 norm:0.0023292656987905502 max memory_allocated 64876.78173828125 
[2025-03-18 15:45:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 12 loss:0.055432092398405075 norm:0.0023511319886893034 max memory_allocated 64876.78173828125 
[2025-03-18 15:46:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 13 loss:0.05459750071167946 norm:0.002179558388888836 max memory_allocated 64876.78173828125 
[2025-03-18 15:48:35 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 14 loss:0.05418187752366066 norm:0.0020908042788505554 max memory_allocated 64876.78173828125 
[2025-03-18 15:50:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 15 loss:0.054256949573755264 norm:0.002451782813295722 max memory_allocated 64876.78173828125 
[2025-03-18 15:52:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 16 loss:0.05347313731908798 norm:0.0019499571062624454 max memory_allocated 64876.78173828125 
[2025-03-18 15:53:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 17 loss:0.05316673964262009 norm:0.0018783170962706208 max memory_allocated 64876.78173828125 
[2025-03-18 15:55:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 18 loss:0.052776459604501724 norm:0.0018048554193228483 max memory_allocated 64876.78173828125 
[2025-03-18 15:57:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 5-6 epoch 19 loss:0.052752017974853516 norm:0.0019916179589927197 max memory_allocated 64876.78173828125 
[2025-03-18 15:57:44 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 5-6
[2025-03-18 15:57:44 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 6 to 7 ===
[2025-03-18 16:00:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 0 loss:0.45238736271858215 norm:0.13304032385349274 max memory_allocated 64876.96923828125 
[2025-03-18 16:01:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 1 loss:0.1370367854833603 norm:0.024341922253370285 max memory_allocated 64876.96923828125 
[2025-03-18 16:03:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 2 loss:0.08989371359348297 norm:0.0065047587268054485 max memory_allocated 64876.96923828125 
[2025-03-18 16:05:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 3 loss:0.07497037202119827 norm:0.004573406185954809 max memory_allocated 64876.96923828125 
[2025-03-18 16:06:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 4 loss:0.06647933274507523 norm:0.0032889079302549362 max memory_allocated 64876.96923828125 
[2025-03-18 16:08:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 5 loss:0.06199835240840912 norm:0.0031539988704025745 max memory_allocated 64876.96923828125 
[2025-03-18 16:10:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 6 loss:0.05883505940437317 norm:0.0031535178422927856 max memory_allocated 64876.96923828125 
[2025-03-18 16:11:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 7 loss:0.0570700578391552 norm:0.003349799197167158 max memory_allocated 64876.96923828125 
[2025-03-18 16:13:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 8 loss:0.055232834070920944 norm:0.0026668799109756947 max memory_allocated 64876.96923828125 
[2025-03-18 16:15:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 9 loss:0.05334129184484482 norm:0.0023978326935321093 max memory_allocated 64876.96923828125 
[2025-03-18 16:16:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 10 loss:0.052160169929265976 norm:0.0022948132827878 max memory_allocated 64876.96923828125 
[2025-03-18 16:18:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 11 loss:0.05139414593577385 norm:0.0022227184381335974 max memory_allocated 64876.96923828125 
[2025-03-18 16:20:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 12 loss:0.05075790733098984 norm:0.0020692446269094944 max memory_allocated 64876.96923828125 
[2025-03-18 16:21:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 13 loss:0.050206031650304794 norm:0.001825863029807806 max memory_allocated 64876.96923828125 
[2025-03-18 16:23:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 14 loss:0.04979290813207626 norm:0.001709431642666459 max memory_allocated 64876.96923828125 
[2025-03-18 16:25:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 15 loss:0.049229882657527924 norm:0.0017257953295484185 max memory_allocated 64876.96923828125 
[2025-03-18 16:26:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 16 loss:0.04891681671142578 norm:0.0016779669094830751 max memory_allocated 64876.96923828125 
[2025-03-18 16:28:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 17 loss:0.04859737306833267 norm:0.0015581997577100992 max memory_allocated 64876.96923828125 
[2025-03-18 16:30:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 18 loss:0.04829178750514984 norm:0.0017601758008822799 max memory_allocated 64876.96923828125 
[2025-03-18 16:31:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 6-7 epoch 19 loss:0.04790963605046272 norm:0.0016353251412510872 max memory_allocated 64876.96923828125 
[2025-03-18 16:32:33 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 6-7
[2025-03-18 16:32:34 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 7 to 8 ===
[2025-03-18 16:34:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 0 loss:0.10577412694692612 norm:0.00610637292265892 max memory_allocated 64877.15673828125 
[2025-03-18 16:36:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 1 loss:0.07145953178405762 norm:0.0025769660715013742 max memory_allocated 64877.15673828125 
[2025-03-18 16:37:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 2 loss:0.05762621760368347 norm:0.0014869843143969774 max memory_allocated 64877.15673828125 
[2025-03-18 16:39:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 3 loss:0.051263757050037384 norm:0.0009893090464174747 max memory_allocated 64877.15673828125 
[2025-03-18 16:41:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 4 loss:0.04802923649549484 norm:0.0007852039998397231 max memory_allocated 64877.15673828125 
[2025-03-18 16:43:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 5 loss:0.04606616124510765 norm:0.0006864277529530227 max memory_allocated 64877.15673828125 
[2025-03-18 16:44:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 6 loss:0.045012567192316055 norm:0.0006292983307503164 max memory_allocated 64877.15673828125 
[2025-03-18 16:46:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 7 loss:0.04426737129688263 norm:0.0005931372288614511 max memory_allocated 64877.15673828125 
[2025-03-18 16:48:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 8 loss:0.04374111443758011 norm:0.0005639969604089856 max memory_allocated 64877.15673828125 
[2025-03-18 16:50:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 9 loss:0.043333061039447784 norm:0.0005420665256679058 max memory_allocated 64877.15673828125 
[2025-03-18 16:51:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 10 loss:0.04300052300095558 norm:0.000526880903635174 max memory_allocated 64877.15673828125 
[2025-03-18 16:53:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 11 loss:0.042779456824064255 norm:0.0005184422479942441 max memory_allocated 64877.15673828125 
[2025-03-18 16:55:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 12 loss:0.04258512333035469 norm:0.0005081557319499552 max memory_allocated 64877.15673828125 
[2025-03-18 16:56:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 13 loss:0.04250955581665039 norm:0.0005075361696071923 max memory_allocated 64877.15673828125 
[2025-03-18 16:58:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 14 loss:0.04238098859786987 norm:0.0005020392127335072 max memory_allocated 64877.15673828125 
[2025-03-18 17:00:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 15 loss:0.04215909168124199 norm:0.00048799969954416156 max memory_allocated 64877.15673828125 
[2025-03-18 17:01:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 16 loss:0.04203168675303459 norm:0.00048716808669269085 max memory_allocated 64877.15673828125 
[2025-03-18 17:03:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 17 loss:0.0419129878282547 norm:0.0004755697154905647 max memory_allocated 64877.15673828125 
[2025-03-18 17:05:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 18 loss:0.04188356176018715 norm:0.00047976194764487445 max memory_allocated 64877.15673828125 
[2025-03-18 17:06:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 7-8 epoch 19 loss:0.041923947632312775 norm:0.000469380320282653 max memory_allocated 64877.15673828125 
[2025-03-18 17:07:16 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 7-8
[2025-03-18 17:07:16 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 8 to 9 ===
[2025-03-18 17:09:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 0 loss:0.18190225958824158 norm:0.01563853956758976 max memory_allocated 64877.34423828125 
[2025-03-18 17:11:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 1 loss:0.08971796929836273 norm:0.0045882766135036945 max memory_allocated 64877.34423828125 
[2025-03-18 17:12:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 2 loss:0.0661463588476181 norm:0.0020788239780813456 max memory_allocated 64877.34423828125 
[2025-03-18 17:14:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 3 loss:0.055972810834646225 norm:0.0013546389527618885 max memory_allocated 64877.34423828125 
[2025-03-18 17:16:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 4 loss:0.05147101730108261 norm:0.001151575706899166 max memory_allocated 64877.34423828125 
[2025-03-18 17:17:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 5 loss:0.04857996851205826 norm:0.00100244814530015 max memory_allocated 64877.34423828125 
[2025-03-18 17:19:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 6 loss:0.04665853828191757 norm:0.0009363351855427027 max memory_allocated 64877.34423828125 
[2025-03-18 17:21:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 7 loss:0.045462414622306824 norm:0.000875552766956389 max memory_allocated 64877.34423828125 
[2025-03-18 17:22:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 8 loss:0.04461744800209999 norm:0.0008421077509410679 max memory_allocated 64877.34423828125 
[2025-03-18 17:24:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 9 loss:0.04398586228489876 norm:0.0008215806446969509 max memory_allocated 64877.34423828125 
[2025-03-18 17:26:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 10 loss:0.04342148080468178 norm:0.0008017504587769508 max memory_allocated 64877.34423828125 
[2025-03-18 17:27:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 11 loss:0.042956627905368805 norm:0.0007958344649523497 max memory_allocated 64877.34423828125 
[2025-03-18 17:29:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 12 loss:0.04254442825913429 norm:0.0007673681247979403 max memory_allocated 64877.34423828125 
[2025-03-18 17:31:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 13 loss:0.04222285747528076 norm:0.0007143190014176071 max memory_allocated 64877.34423828125 
[2025-03-18 17:32:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 14 loss:0.04192967712879181 norm:0.0006950237439014018 max memory_allocated 64877.34423828125 
[2025-03-18 17:34:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 15 loss:0.041631266474723816 norm:0.0006784219876863062 max memory_allocated 64877.34423828125 
[2025-03-18 17:36:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 16 loss:0.0414053350687027 norm:0.0006571803241968155 max memory_allocated 64877.34423828125 
[2025-03-18 17:38:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 17 loss:0.041146375238895416 norm:0.0006382631254382432 max memory_allocated 64877.34423828125 
[2025-03-18 17:39:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 18 loss:0.04088360443711281 norm:0.0006199913914315403 max memory_allocated 64877.34423828125 
[2025-03-18 17:41:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 8-9 epoch 19 loss:0.04067663475871086 norm:0.000618393998593092 max memory_allocated 64877.34423828125 
[2025-03-18 17:42:05 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 8-9
[2025-03-18 17:42:05 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 9 to 10 ===
[2025-03-18 17:44:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 0 loss:0.09088040143251419 norm:0.004449950065463781 max memory_allocated 64877.53173828125 
[2025-03-18 17:45:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 1 loss:0.06448333710432053 norm:0.001872147899121046 max memory_allocated 64877.53173828125 
[2025-03-18 17:47:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 2 loss:0.05332225561141968 norm:0.0010881036287173629 max memory_allocated 64877.53173828125 
[2025-03-18 17:49:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 3 loss:0.04877915233373642 norm:0.0007861927151679993 max memory_allocated 64877.53173828125 
[2025-03-18 17:51:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 4 loss:0.046470049768686295 norm:0.0006426458712667227 max memory_allocated 64877.53173828125 
[2025-03-18 17:52:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 5 loss:0.045152436941862106 norm:0.0005691918777301908 max memory_allocated 64877.53173828125 
[2025-03-18 17:54:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 6 loss:0.04435361921787262 norm:0.0005282786441966891 max memory_allocated 64877.53173828125 
[2025-03-18 17:56:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 7 loss:0.043908387422561646 norm:0.0005089912447147071 max memory_allocated 64877.53173828125 
[2025-03-18 17:57:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 8 loss:0.043722350150346756 norm:0.0004886086680926383 max memory_allocated 64877.53173828125 
[2025-03-18 17:59:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 9 loss:0.043604616075754166 norm:0.0004797822912223637 max memory_allocated 64877.53173828125 
[2025-03-18 18:01:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 10 loss:0.04334569722414017 norm:0.00045611298992298543 max memory_allocated 64877.53173828125 
[2025-03-18 18:02:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 11 loss:0.04318938031792641 norm:0.00044479293865151703 max memory_allocated 64877.53173828125 
[2025-03-18 18:04:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 12 loss:0.043170031160116196 norm:0.00043885924969799817 max memory_allocated 64877.53173828125 
[2025-03-18 18:06:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 13 loss:0.04307589679956436 norm:0.00043042609468102455 max memory_allocated 64877.53173828125 
[2025-03-18 18:07:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 14 loss:0.04284578561782837 norm:0.00041405734373256564 max memory_allocated 64877.53173828125 
[2025-03-18 18:09:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 15 loss:0.0427623987197876 norm:0.0004062498628627509 max memory_allocated 64877.53173828125 
[2025-03-18 18:11:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 16 loss:0.04271221533417702 norm:0.00040419650031253695 max memory_allocated 64877.53173828125 
[2025-03-18 18:12:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 17 loss:0.04259829968214035 norm:0.0004001108754891902 max memory_allocated 64877.53173828125 
[2025-03-18 18:14:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 18 loss:0.042541615664958954 norm:0.00039679990732111037 max memory_allocated 64877.53173828125 
[2025-03-18 18:16:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 9-10 epoch 19 loss:0.04255499318242073 norm:0.00039376196218654513 max memory_allocated 64877.53173828125 
[2025-03-18 18:16:55 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 9-10
[2025-03-18 18:16:56 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 10 to 11 ===
[2025-03-18 18:18:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 0 loss:0.17872489988803864 norm:0.010750572197139263 max memory_allocated 64877.71923828125 
[2025-03-18 18:20:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 1 loss:0.09711316227912903 norm:0.004406954627484083 max memory_allocated 64877.71923828125 
[2025-03-18 18:22:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 2 loss:0.06833787262439728 norm:0.0021974602714180946 max memory_allocated 64877.71923828125 
[2025-03-18 18:24:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 3 loss:0.05698901042342186 norm:0.001291166990995407 max memory_allocated 64877.71923828125 
[2025-03-18 18:25:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 4 loss:0.05134066566824913 norm:0.0009944008197635412 max memory_allocated 64877.71923828125 
[2025-03-18 18:27:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 5 loss:0.04830239713191986 norm:0.0008670005481690168 max memory_allocated 64877.71923828125 
[2025-03-18 18:29:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 6 loss:0.04661250486969948 norm:0.0008072752971202135 max memory_allocated 64877.71923828125 
[2025-03-18 18:30:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 7 loss:0.04554029181599617 norm:0.0007682740688323975 max memory_allocated 64877.71923828125 
[2025-03-18 18:32:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 8 loss:0.04470457136631012 norm:0.0007347974460572004 max memory_allocated 64877.71923828125 
[2025-03-18 18:34:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 9 loss:0.0440739206969738 norm:0.0006983190542086959 max memory_allocated 64877.71923828125 
[2025-03-18 18:36:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 10 loss:0.043606169521808624 norm:0.0006795200170017779 max memory_allocated 64877.71923828125 
[2025-03-18 18:37:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 11 loss:0.04314829036593437 norm:0.0006573026184923947 max memory_allocated 64877.71923828125 
[2025-03-18 18:39:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 12 loss:0.04281351715326309 norm:0.000643391627818346 max memory_allocated 64877.71923828125 
[2025-03-18 18:41:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 13 loss:0.04248427227139473 norm:0.0006313257617875934 max memory_allocated 64877.71923828125 
[2025-03-18 18:42:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 14 loss:0.042231760919094086 norm:0.0006231568986549973 max memory_allocated 64877.71923828125 
[2025-03-18 18:44:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 15 loss:0.04198634997010231 norm:0.0006008981727063656 max memory_allocated 64877.71923828125 
[2025-03-18 18:46:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 16 loss:0.04178842902183533 norm:0.0005902669508941472 max memory_allocated 64877.71923828125 
[2025-03-18 18:47:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 17 loss:0.041597701609134674 norm:0.0005619281437247992 max memory_allocated 64877.71923828125 
[2025-03-18 18:49:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 18 loss:0.04140080511569977 norm:0.0005407468997873366 max memory_allocated 64877.71923828125 
[2025-03-18 18:51:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 10-11 epoch 19 loss:0.04117407277226448 norm:0.0005171187804080546 max memory_allocated 64877.71923828125 
[2025-03-18 18:51:47 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 10-11
[2025-03-18 18:51:48 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 11 to 12 ===
[2025-03-18 18:53:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 0 loss:0.0803360864520073 norm:0.0031005300115793943 max memory_allocated 64877.90673828125 
[2025-03-18 18:55:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 1 loss:0.05795712023973465 norm:0.0012814004439860582 max memory_allocated 64877.90673828125 
[2025-03-18 18:57:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 2 loss:0.047890715301036835 norm:0.0007315423572435975 max memory_allocated 64877.90673828125 
[2025-03-18 18:58:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 3 loss:0.044019583612680435 norm:0.0005237152217887342 max memory_allocated 64877.90673828125 
[2025-03-18 19:00:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 4 loss:0.042129822075366974 norm:0.0004303794994484633 max memory_allocated 64877.90673828125 
[2025-03-18 19:02:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 5 loss:0.04097870737314224 norm:0.0003858670825138688 max memory_allocated 64877.90673828125 
[2025-03-18 19:03:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 6 loss:0.040321532636880875 norm:0.0003645593242254108 max memory_allocated 64877.90673828125 
[2025-03-18 19:05:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 7 loss:0.03983750939369202 norm:0.0003500115417409688 max memory_allocated 64877.90673828125 
[2025-03-18 19:07:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 8 loss:0.039494164288043976 norm:0.00033802111283876 max memory_allocated 64877.90673828125 
[2025-03-18 19:08:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 9 loss:0.03923194110393524 norm:0.00033683751826174557 max memory_allocated 64877.90673828125 
[2025-03-18 19:10:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 10 loss:0.039045535027980804 norm:0.0003291933098807931 max memory_allocated 64877.90673828125 
[2025-03-18 19:12:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 11 loss:0.038874298334121704 norm:0.00033157668076455593 max memory_allocated 64877.90673828125 
[2025-03-18 19:14:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 12 loss:0.038700561970472336 norm:0.0003257477073930204 max memory_allocated 64877.90673828125 
[2025-03-18 19:15:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 13 loss:0.03857274726033211 norm:0.00032071233727037907 max memory_allocated 64877.90673828125 
[2025-03-18 19:17:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 14 loss:0.038458455353975296 norm:0.00031722497078590095 max memory_allocated 64877.90673828125 
[2025-03-18 19:19:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 15 loss:0.03835004195570946 norm:0.00031115105957724154 max memory_allocated 64877.90673828125 
[2025-03-18 19:20:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 16 loss:0.03831607848405838 norm:0.0003110363322775811 max memory_allocated 64877.90673828125 
[2025-03-18 19:22:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 17 loss:0.03824901953339577 norm:0.00030476818210445344 max memory_allocated 64877.90673828125 
[2025-03-18 19:24:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 18 loss:0.03819936886429787 norm:0.0003047579375561327 max memory_allocated 64877.90673828125 
[2025-03-18 19:25:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 11-12 epoch 19 loss:0.038162775337696075 norm:0.0002973513037431985 max memory_allocated 64877.90673828125 
[2025-03-18 19:26:35 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 11-12
[2025-03-18 19:26:35 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 12 to 13 ===
[2025-03-18 19:28:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 0 loss:0.07829569280147552 norm:0.0027262044604867697 max memory_allocated 64878.09423828125 
[2025-03-18 19:30:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 1 loss:0.05813440680503845 norm:0.0012160554761067033 max memory_allocated 64878.09423828125 
[2025-03-18 19:32:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 2 loss:0.04874115437269211 norm:0.0007449495024047792 max memory_allocated 64878.09423828125 
[2025-03-18 19:33:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 3 loss:0.045002806931734085 norm:0.000558671890757978 max memory_allocated 64878.09423828125 
[2025-03-18 19:35:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 4 loss:0.043059881776571274 norm:0.0004705474420916289 max memory_allocated 64878.09423828125 
[2025-03-18 19:37:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 5 loss:0.041792452335357666 norm:0.00042575557017698884 max memory_allocated 64878.09423828125 
[2025-03-18 19:38:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 6 loss:0.04105311259627342 norm:0.00039710747660137713 max memory_allocated 64878.09423828125 
[2025-03-18 19:40:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 7 loss:0.04053125157952309 norm:0.00038276895065791905 max memory_allocated 64878.09423828125 
[2025-03-18 19:42:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 8 loss:0.04016834869980812 norm:0.00037019088631495833 max memory_allocated 64878.09423828125 
[2025-03-18 19:43:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 9 loss:0.03990600258111954 norm:0.00036089998320676386 max memory_allocated 64878.09423828125 
[2025-03-18 19:45:35 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 10 loss:0.0396769717335701 norm:0.00034983365912921727 max memory_allocated 64878.09423828125 
[2025-03-18 19:47:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 11 loss:0.0394805446267128 norm:0.00034212201717309654 max memory_allocated 64878.09423828125 
[2025-03-18 19:48:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 12 loss:0.03932853788137436 norm:0.0003378921246621758 max memory_allocated 64878.09423828125 
[2025-03-18 19:50:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 13 loss:0.039195720106363297 norm:0.00033178538433276117 max memory_allocated 64878.09423828125 
[2025-03-18 19:52:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 14 loss:0.03907432034611702 norm:0.0003246161504648626 max memory_allocated 64878.09423828125 
[2025-03-18 19:53:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 15 loss:0.03899678215384483 norm:0.0003200692299287766 max memory_allocated 64878.09423828125 
[2025-03-18 19:55:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 16 loss:0.038903407752513885 norm:0.0003177359467372298 max memory_allocated 64878.09423828125 
[2025-03-18 19:57:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 17 loss:0.03883664309978485 norm:0.0003167827962897718 max memory_allocated 64878.09423828125 
[2025-03-18 19:58:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 18 loss:0.03878883644938469 norm:0.00031423813197761774 max memory_allocated 64878.09423828125 
[2025-03-18 20:00:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 12-13 epoch 19 loss:0.038739778101444244 norm:0.00031074348953552544 max memory_allocated 64878.09423828125 
[2025-03-18 20:01:22 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 12-13
[2025-03-18 20:01:23 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 13 to 14 ===
[2025-03-18 20:03:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 0 loss:0.08585794270038605 norm:0.00434748362749815 max memory_allocated 64878.28173828125 
[2025-03-18 20:05:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 1 loss:0.06332547217607498 norm:0.002066619461402297 max memory_allocated 64878.28173828125 
[2025-03-18 20:06:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 2 loss:0.05225960910320282 norm:0.001217273180373013 max memory_allocated 64878.28173828125 
[2025-03-18 20:08:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 3 loss:0.047931917011737823 norm:0.0008309251861646771 max memory_allocated 64878.28173828125 
[2025-03-18 20:10:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 4 loss:0.04542273283004761 norm:0.0006215480389073491 max memory_allocated 64878.28173828125 
[2025-03-18 20:11:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 5 loss:0.044109899550676346 norm:0.0005172927631065249 max memory_allocated 64878.28173828125 
[2025-03-18 20:13:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 6 loss:0.043308112770318985 norm:0.0004526455304585397 max memory_allocated 64878.28173828125 
[2025-03-18 20:15:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 7 loss:0.04294423758983612 norm:0.0004235910892020911 max memory_allocated 64878.28173828125 
[2025-03-18 20:16:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 8 loss:0.04257441312074661 norm:0.00039517105324193835 max memory_allocated 64878.28173828125 
[2025-03-18 20:18:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 9 loss:0.0423785038292408 norm:0.0003803781291935593 max memory_allocated 64878.28173828125 
[2025-03-18 20:20:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 10 loss:0.04229511320590973 norm:0.0003725693095475435 max memory_allocated 64878.28173828125 
[2025-03-18 20:22:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 11 loss:0.04219403117895126 norm:0.0003636492183431983 max memory_allocated 64878.28173828125 
[2025-03-18 20:23:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 12 loss:0.04204191640019417 norm:0.0003537893935572356 max memory_allocated 64878.28173828125 
[2025-03-18 20:25:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 13 loss:0.041954949498176575 norm:0.00034532605786807835 max memory_allocated 64878.28173828125 
[2025-03-18 20:27:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 14 loss:0.041942644864320755 norm:0.00034264824353158474 max memory_allocated 64878.28173828125 
[2025-03-18 20:28:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 15 loss:0.04196992889046669 norm:0.0003419283893890679 max memory_allocated 64878.28173828125 
[2025-03-18 20:30:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 16 loss:0.04194643348455429 norm:0.0003403723821975291 max memory_allocated 64878.28173828125 
[2025-03-18 20:32:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 17 loss:0.041871577501297 norm:0.0003387054312042892 max memory_allocated 64878.28173828125 
[2025-03-18 20:33:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 18 loss:0.041807517409324646 norm:0.00033654813887551427 max memory_allocated 64878.28173828125 
[2025-03-18 20:35:35 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 13-14 epoch 19 loss:0.041730377823114395 norm:0.00033364820410497487 max memory_allocated 64878.28173828125 
[2025-03-18 20:36:09 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 13-14
[2025-03-18 20:36:10 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 14 to 15 ===
[2025-03-18 20:38:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 0 loss:0.17911377549171448 norm:0.010843252763152122 max memory_allocated 64878.46923828125 
[2025-03-18 20:40:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 1 loss:0.08309918642044067 norm:0.0031373004894703627 max memory_allocated 64878.46923828125 
[2025-03-18 20:41:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 2 loss:0.06001749634742737 norm:0.0013797340216115117 max memory_allocated 64878.46923828125 
[2025-03-18 20:43:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 3 loss:0.053172215819358826 norm:0.0010509999701753259 max memory_allocated 64878.46923828125 
[2025-03-18 20:45:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 4 loss:0.049322459846735 norm:0.0009271297603845596 max memory_allocated 64878.46923828125 
[2025-03-18 20:46:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 5 loss:0.046859607100486755 norm:0.0008379286737181246 max memory_allocated 64878.46923828125 
[2025-03-18 20:48:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 6 loss:0.04531637579202652 norm:0.0007912507862783968 max memory_allocated 64878.46923828125 
[2025-03-18 20:50:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 7 loss:0.044165078550577164 norm:0.0007278202683664858 max memory_allocated 64878.46923828125 
[2025-03-18 20:51:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 8 loss:0.04338468611240387 norm:0.0006958956364542246 max memory_allocated 64878.46923828125 
[2025-03-18 20:53:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 9 loss:0.04287881404161453 norm:0.0006794973742216825 max memory_allocated 64878.46923828125 
[2025-03-18 20:55:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 10 loss:0.04241160303354263 norm:0.0006500628078356385 max memory_allocated 64878.46923828125 
[2025-03-18 20:56:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 11 loss:0.04199447110295296 norm:0.0006454369286075234 max memory_allocated 64878.46923828125 
[2025-03-18 20:58:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 12 loss:0.04163561388850212 norm:0.000631036760751158 max memory_allocated 64878.46923828125 
[2025-03-18 21:00:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 13 loss:0.04135638102889061 norm:0.0006178697221912444 max memory_allocated 64878.46923828125 
[2025-03-18 21:01:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 14 loss:0.04108940809965134 norm:0.0006027963245287538 max memory_allocated 64878.46923828125 
[2025-03-18 21:03:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 15 loss:0.040879931300878525 norm:0.0005962068680673838 max memory_allocated 64878.46923828125 
[2025-03-18 21:05:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 16 loss:0.0406830869615078 norm:0.0005814764299429953 max memory_allocated 64878.46923828125 
[2025-03-18 21:06:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 17 loss:0.04052324220538139 norm:0.0005668827798217535 max memory_allocated 64878.46923828125 
[2025-03-18 21:08:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 18 loss:0.04036001116037369 norm:0.0005678722518496215 max memory_allocated 64878.46923828125 
[2025-03-18 21:10:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 14-15 epoch 19 loss:0.040202055126428604 norm:0.0005511216586455703 max memory_allocated 64878.46923828125 
[2025-03-18 21:10:58 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 14-15
[2025-03-18 21:10:59 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 15 to 16 ===
[2025-03-18 21:12:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 0 loss:0.0718870684504509 norm:0.0035638681147247553 max memory_allocated 64878.65673828125 
[2025-03-18 21:14:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 1 loss:0.05658438801765442 norm:0.001725323498249054 max memory_allocated 64878.65673828125 
[2025-03-18 21:16:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 2 loss:0.04779157042503357 norm:0.0010150658199563622 max memory_allocated 64878.65673828125 
[2025-03-18 21:18:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 3 loss:0.04440943896770477 norm:0.0007498126360587776 max memory_allocated 64878.65673828125 
[2025-03-18 21:19:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 4 loss:0.04271542280912399 norm:0.0006192151922732592 max memory_allocated 64878.65673828125 
[2025-03-18 21:21:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 5 loss:0.041620370000600815 norm:0.000548062555026263 max memory_allocated 64878.65673828125 
[2025-03-18 21:23:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 6 loss:0.04106494039297104 norm:0.0004937603953294456 max memory_allocated 64878.65673828125 
[2025-03-18 21:24:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 7 loss:0.040827956050634384 norm:0.00047289254143834114 max memory_allocated 64878.65673828125 
[2025-03-18 21:26:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 8 loss:0.04063574969768524 norm:0.0004560936358757317 max memory_allocated 64878.65673828125 
[2025-03-18 21:28:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 9 loss:0.04057592153549194 norm:0.00045279989717528224 max memory_allocated 64878.65673828125 
[2025-03-18 21:30:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 10 loss:0.04056590050458908 norm:0.0004316691483836621 max memory_allocated 64878.65673828125 
[2025-03-18 21:31:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 11 loss:0.04046012461185455 norm:0.0004182877892162651 max memory_allocated 64878.65673828125 
[2025-03-18 21:33:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 12 loss:0.040369100868701935 norm:0.0003979772736784071 max memory_allocated 64878.65673828125 
[2025-03-18 21:35:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 13 loss:0.04027944430708885 norm:0.0003883182362187654 max memory_allocated 64878.65673828125 
[2025-03-18 21:36:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 14 loss:0.04010111466050148 norm:0.00037795447860844433 max memory_allocated 64878.65673828125 
[2025-03-18 21:38:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 15 loss:0.040064431726932526 norm:0.0003772456257138401 max memory_allocated 64878.65673828125 
[2025-03-18 21:40:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 16 loss:0.040084853768348694 norm:0.0003769013565033674 max memory_allocated 64878.65673828125 
[2025-03-18 21:41:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 17 loss:0.04012317955493927 norm:0.0003758098173420876 max memory_allocated 64878.65673828125 
[2025-03-18 21:43:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 18 loss:0.040134549140930176 norm:0.00037519028410315514 max memory_allocated 64878.65673828125 
[2025-03-18 21:45:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 15-16 epoch 19 loss:0.04014674574136734 norm:0.00037454930134117603 max memory_allocated 64878.65673828125 
[2025-03-18 21:45:47 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 15-16
[2025-03-18 21:45:48 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 16 to 17 ===
[2025-03-18 21:47:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 0 loss:0.20768502354621887 norm:0.03359310328960419 max memory_allocated 64878.84423828125 
[2025-03-18 21:49:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 1 loss:0.08865009993314743 norm:0.00897914171218872 max memory_allocated 64878.84423828125 
[2025-03-18 21:51:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 2 loss:0.06046558544039726 norm:0.0024380222894251347 max memory_allocated 64878.84423828125 
[2025-03-18 21:52:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 3 loss:0.05193808302283287 norm:0.0015390978660434484 max memory_allocated 64878.84423828125 
[2025-03-18 21:54:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 4 loss:0.04790215566754341 norm:0.0012426991015672684 max memory_allocated 64878.84423828125 
[2025-03-18 21:56:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 5 loss:0.045291438698768616 norm:0.0010693214135244489 max memory_allocated 64878.84423828125 
[2025-03-18 21:57:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 6 loss:0.04364316910505295 norm:0.0009367653983645141 max memory_allocated 64878.84423828125 
[2025-03-18 21:59:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 7 loss:0.042630165815353394 norm:0.0008760762284509838 max memory_allocated 64878.84423828125 
[2025-03-18 22:01:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 8 loss:0.041865959763526917 norm:0.0008359585190191865 max memory_allocated 64878.84423828125 
[2025-03-18 22:02:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 9 loss:0.04126717522740364 norm:0.0007925335667096078 max memory_allocated 64878.84423828125 
[2025-03-18 22:04:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 10 loss:0.04081223905086517 norm:0.0007759917061775923 max memory_allocated 64878.84423828125 
[2025-03-18 22:06:22 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 11 loss:0.04042422026395798 norm:0.0007399989408440888 max memory_allocated 64878.84423828125 
[2025-03-18 22:07:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 12 loss:0.040102768689394 norm:0.0007222215062938631 max memory_allocated 64878.84423828125 
[2025-03-18 22:09:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 13 loss:0.03981534764170647 norm:0.0006976012955419719 max memory_allocated 64878.84423828125 
[2025-03-18 22:11:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 14 loss:0.03966664522886276 norm:0.0006948306108824909 max memory_allocated 64878.84423828125 
[2025-03-18 22:13:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 15 loss:0.03948746994137764 norm:0.0006807497120462358 max memory_allocated 64878.84423828125 
[2025-03-18 22:14:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 16 loss:0.03925829380750656 norm:0.0006633975426666439 max memory_allocated 64878.84423828125 
[2025-03-18 22:16:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 17 loss:0.039078522473573685 norm:0.0006593838916160166 max memory_allocated 64878.84423828125 
[2025-03-18 22:18:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 18 loss:0.038922153413295746 norm:0.0006400328129529953 max memory_allocated 64878.84423828125 
[2025-03-18 22:19:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 16-17 epoch 19 loss:0.03881159424781799 norm:0.0006319294916465878 max memory_allocated 64878.84423828125 
[2025-03-18 22:20:33 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 16-17
[2025-03-18 22:20:33 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 17 to 18 ===
[2025-03-18 22:22:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 0 loss:0.07131457328796387 norm:0.00448685884475708 max memory_allocated 64879.03173828125 
[2025-03-18 22:24:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 1 loss:0.055156998336315155 norm:0.0020000755321234465 max memory_allocated 64879.03173828125 
[2025-03-18 22:25:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 2 loss:0.046334609389305115 norm:0.0011238532606512308 max memory_allocated 64879.03173828125 
[2025-03-18 22:27:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 3 loss:0.04307669773697853 norm:0.0007611354812979698 max memory_allocated 64879.03173828125 
[2025-03-18 22:29:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 4 loss:0.0412677600979805 norm:0.0005736884195357561 max memory_allocated 64879.03173828125 
[2025-03-18 22:31:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 5 loss:0.040265731513500214 norm:0.00047492666635662317 max memory_allocated 64879.03173828125 
[2025-03-18 22:32:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 6 loss:0.03971546143293381 norm:0.00041838805191218853 max memory_allocated 64879.03173828125 
[2025-03-18 22:34:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 7 loss:0.03939196839928627 norm:0.0003811971109826118 max memory_allocated 64879.03173828125 
[2025-03-18 22:36:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 8 loss:0.03915237635374069 norm:0.00035651950747705996 max memory_allocated 64879.03173828125 
[2025-03-18 22:37:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 9 loss:0.038982540369033813 norm:0.00034122366923838854 max memory_allocated 64879.03173828125 
[2025-03-18 22:39:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 10 loss:0.03886060416698456 norm:0.00033025245647877455 max memory_allocated 64879.03173828125 
[2025-03-18 22:41:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 11 loss:0.03878030925989151 norm:0.00032200547866523266 max memory_allocated 64879.03173828125 
[2025-03-18 22:42:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 12 loss:0.03868182748556137 norm:0.0003170364652760327 max memory_allocated 64879.03173828125 
[2025-03-18 22:44:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 13 loss:0.038591168820858 norm:0.00031259231036528945 max memory_allocated 64879.03173828125 
[2025-03-18 22:46:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 14 loss:0.03855162858963013 norm:0.00030959967989474535 max memory_allocated 64879.03173828125 
[2025-03-18 22:47:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 15 loss:0.03849058970808983 norm:0.00030850470648147166 max memory_allocated 64879.03173828125 
[2025-03-18 22:49:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 16 loss:0.03845752775669098 norm:0.0003069270751439035 max memory_allocated 64879.03173828125 
[2025-03-18 22:51:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 17 loss:0.03840583190321922 norm:0.0003051274688914418 max memory_allocated 64879.03173828125 
[2025-03-18 22:52:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 18 loss:0.038369618356227875 norm:0.0003028390055987984 max memory_allocated 64879.03173828125 
[2025-03-18 22:54:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 17-18 epoch 19 loss:0.038334328681230545 norm:0.00030164822237566113 max memory_allocated 64879.03173828125 
[2025-03-18 22:55:19 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 17-18
[2025-03-18 22:55:20 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 18 to 19 ===
[2025-03-18 22:57:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 0 loss:0.07134626805782318 norm:0.0023687819484621286 max memory_allocated 64879.21923828125 
[2025-03-18 22:58:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 1 loss:0.05691813305020332 norm:0.0010996583150699735 max memory_allocated 64879.21923828125 
[2025-03-18 23:00:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 2 loss:0.04844607785344124 norm:0.0007084828102961183 max memory_allocated 64879.21923828125 
[2025-03-18 23:02:22 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 3 loss:0.045323655009269714 norm:0.0005360356299206614 max memory_allocated 64879.21923828125 
[2025-03-18 23:03:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 4 loss:0.04365716874599457 norm:0.00044402468483895063 max memory_allocated 64879.21923828125 
[2025-03-18 23:05:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 5 loss:0.04275355115532875 norm:0.0003932478139176965 max memory_allocated 64879.21923828125 
[2025-03-18 23:07:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 6 loss:0.042311616241931915 norm:0.0003639701462816447 max memory_allocated 64879.21923828125 
[2025-03-18 23:09:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 7 loss:0.04203968495130539 norm:0.0003487220383249223 max memory_allocated 64879.21923828125 
[2025-03-18 23:10:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 8 loss:0.04182758182287216 norm:0.00033337512286379933 max memory_allocated 64879.21923828125 
[2025-03-18 23:12:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 9 loss:0.04166930913925171 norm:0.00032385176746174693 max memory_allocated 64879.21923828125 
[2025-03-18 23:14:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 10 loss:0.041521839797496796 norm:0.00031774293165653944 max memory_allocated 64879.21923828125 
[2025-03-18 23:15:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 11 loss:0.0414087288081646 norm:0.0003131176927126944 max memory_allocated 64879.21923828125 
[2025-03-18 23:17:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 12 loss:0.041326090693473816 norm:0.0003104505885858089 max memory_allocated 64879.21923828125 
[2025-03-18 23:19:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 13 loss:0.04122807830572128 norm:0.0003095827123615891 max memory_allocated 64879.21923828125 
[2025-03-18 23:20:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 14 loss:0.04117342084646225 norm:0.0003082487965002656 max memory_allocated 64879.21923828125 
[2025-03-18 23:22:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 15 loss:0.041125137358903885 norm:0.0003063359472434968 max memory_allocated 64879.21923828125 
[2025-03-18 23:24:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 16 loss:0.041057929396629333 norm:0.00030521920416504145 max memory_allocated 64879.21923828125 
[2025-03-18 23:26:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 17 loss:0.04100922867655754 norm:0.000306918314890936 max memory_allocated 64879.21923828125 
[2025-03-18 23:27:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 18 loss:0.04097134992480278 norm:0.0003072149702347815 max memory_allocated 64879.21923828125 
[2025-03-18 23:29:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 18-19 epoch 19 loss:0.04092651605606079 norm:0.0003050712402909994 max memory_allocated 64879.21923828125 
[2025-03-18 23:30:04 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 18-19
[2025-03-18 23:30:04 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 19 to 20 ===
[2025-03-18 23:32:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 0 loss:0.07572914659976959 norm:0.0022481780033558607 max memory_allocated 64879.40673828125 
[2025-03-18 23:33:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 1 loss:0.060012586414813995 norm:0.0010974083561450243 max memory_allocated 64879.40673828125 
[2025-03-18 23:35:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 2 loss:0.050828590989112854 norm:0.0006908692303113639 max memory_allocated 64879.40673828125 
[2025-03-18 23:37:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 3 loss:0.04750275984406471 norm:0.000515979714691639 max memory_allocated 64879.40673828125 
[2025-03-18 23:38:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 4 loss:0.04569488391280174 norm:0.0004235698143020272 max memory_allocated 64879.40673828125 
[2025-03-18 23:40:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 5 loss:0.044806599617004395 norm:0.00037507343222387135 max memory_allocated 64879.40673828125 
[2025-03-18 23:42:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 6 loss:0.04432394355535507 norm:0.0003461184096522629 max memory_allocated 64879.40673828125 
[2025-03-18 23:43:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 7 loss:0.04399147257208824 norm:0.0003276733332313597 max memory_allocated 64879.40673828125 
[2025-03-18 23:45:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 8 loss:0.043740276247262955 norm:0.00031606663833372295 max memory_allocated 64879.40673828125 
[2025-03-18 23:47:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 9 loss:0.04356522858142853 norm:0.0003093963023275137 max memory_allocated 64879.40673828125 
[2025-03-18 23:48:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 10 loss:0.04340018332004547 norm:0.0003056645509786904 max memory_allocated 64879.40673828125 
[2025-03-18 23:50:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 11 loss:0.04328956827521324 norm:0.0003011465014424175 max memory_allocated 64879.40673828125 
[2025-03-18 23:52:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 12 loss:0.04316607862710953 norm:0.00029668575734831393 max memory_allocated 64879.40673828125 
[2025-03-18 23:53:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 13 loss:0.04310643672943115 norm:0.0002959243138320744 max memory_allocated 64879.40673828125 
[2025-03-18 23:55:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 14 loss:0.04305870831012726 norm:0.00029452916351146996 max memory_allocated 64879.40673828125 
[2025-03-18 23:57:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 15 loss:0.042992785573005676 norm:0.0002925879380200058 max memory_allocated 64879.40673828125 
[2025-03-18 23:58:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 16 loss:0.04296319931745529 norm:0.0002920850529335439 max memory_allocated 64879.40673828125 
[2025-03-19 00:00:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 17 loss:0.04291220009326935 norm:0.0002910736948251724 max memory_allocated 64879.40673828125 
[2025-03-19 00:02:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 18 loss:0.04287108778953552 norm:0.00028913840651512146 max memory_allocated 64879.40673828125 
[2025-03-19 00:03:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 19-20 epoch 19 loss:0.042853545397520065 norm:0.00028990174178034067 max memory_allocated 64879.40673828125 
[2025-03-19 00:04:50 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 19-20
[2025-03-19 00:04:51 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 20 to 21 ===
[2025-03-19 00:06:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 0 loss:0.08682164549827576 norm:0.003414659295231104 max memory_allocated 64879.59423828125 
[2025-03-19 00:08:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 1 loss:0.06910461187362671 norm:0.0017274238634854555 max memory_allocated 64879.59423828125 
[2025-03-19 00:10:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 2 loss:0.05844418331980705 norm:0.0010574923362582922 max memory_allocated 64879.59423828125 
[2025-03-19 00:11:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 3 loss:0.054772280156612396 norm:0.0007362966425716877 max memory_allocated 64879.59423828125 
[2025-03-19 00:13:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 4 loss:0.05277944356203079 norm:0.0005771049181930721 max memory_allocated 64879.59423828125 
[2025-03-19 00:15:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 5 loss:0.05180395767092705 norm:0.0004890698473900557 max memory_allocated 64879.59423828125 
[2025-03-19 00:16:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 6 loss:0.05121966078877449 norm:0.00044335945858620107 max memory_allocated 64879.59423828125 
[2025-03-19 00:18:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 7 loss:0.05083511024713516 norm:0.0004130020970478654 max memory_allocated 64879.59423828125 
[2025-03-19 00:20:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 8 loss:0.05059666559100151 norm:0.00039329100400209427 max memory_allocated 64879.59423828125 
[2025-03-19 00:22:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 9 loss:0.05037309601902962 norm:0.0003827421460300684 max memory_allocated 64879.59423828125 
[2025-03-19 00:23:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 10 loss:0.05021756514906883 norm:0.0003758418606594205 max memory_allocated 64879.59423828125 
[2025-03-19 00:25:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 11 loss:0.050096675753593445 norm:0.0003697148058563471 max memory_allocated 64879.59423828125 
[2025-03-19 00:27:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 12 loss:0.049978334456682205 norm:0.0003648845013231039 max memory_allocated 64879.59423828125 
[2025-03-19 00:28:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 13 loss:0.0498611256480217 norm:0.000363315106369555 max memory_allocated 64879.59423828125 
[2025-03-19 00:30:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 14 loss:0.049791369587183 norm:0.0003590653359424323 max memory_allocated 64879.59423828125 
[2025-03-19 00:32:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 15 loss:0.04972688853740692 norm:0.0003556318115442991 max memory_allocated 64879.59423828125 
[2025-03-19 00:33:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 16 loss:0.049654264003038406 norm:0.000355543103069067 max memory_allocated 64879.59423828125 
[2025-03-19 00:35:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 17 loss:0.0495963916182518 norm:0.00035316028515808284 max memory_allocated 64879.59423828125 
[2025-03-19 00:37:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 18 loss:0.049565814435482025 norm:0.0003526094078551978 max memory_allocated 64879.59423828125 
[2025-03-19 00:38:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 20-21 epoch 19 loss:0.049523595720529556 norm:0.00035148445749655366 max memory_allocated 64879.59423828125 
[2025-03-19 00:39:20 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 20-21
[2025-03-19 00:39:21 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 21 to 22 ===
[2025-03-19 00:41:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 0 loss:0.1939488649368286 norm:0.013212733902037144 max memory_allocated 64879.78173828125 
[2025-03-19 00:43:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 1 loss:0.08488200604915619 norm:0.002599361352622509 max memory_allocated 64879.78173828125 
[2025-03-19 00:44:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 2 loss:0.06270842254161835 norm:0.00132757518440485 max memory_allocated 64879.78173828125 
[2025-03-19 00:46:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 3 loss:0.056738436222076416 norm:0.0010136422934010625 max memory_allocated 64879.78173828125 
[2025-03-19 00:48:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 4 loss:0.05336758866906166 norm:0.0008718901081010699 max memory_allocated 64879.78173828125 
[2025-03-19 00:50:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 5 loss:0.05111593380570412 norm:0.000797433836851269 max memory_allocated 64879.78173828125 
[2025-03-19 00:51:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 6 loss:0.04983843117952347 norm:0.0007281930302269757 max memory_allocated 64879.78173828125 
[2025-03-19 00:53:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 7 loss:0.04903224855661392 norm:0.0006924148765392601 max memory_allocated 64879.78173828125 
[2025-03-19 00:55:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 8 loss:0.04843723401427269 norm:0.0006539704045280814 max memory_allocated 64879.78173828125 
[2025-03-19 00:56:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 9 loss:0.04793975129723549 norm:0.000631024013273418 max memory_allocated 64879.78173828125 
[2025-03-19 00:58:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 10 loss:0.04754078760743141 norm:0.0006140482728369534 max memory_allocated 64879.78173828125 
[2025-03-19 01:00:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 11 loss:0.047188494354486465 norm:0.0005947586032561958 max memory_allocated 64879.78173828125 
[2025-03-19 01:01:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 12 loss:0.04686106741428375 norm:0.0005791884614154696 max memory_allocated 64879.78173828125 
[2025-03-19 01:03:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 13 loss:0.04656568169593811 norm:0.0005594375543296337 max memory_allocated 64879.78173828125 
[2025-03-19 01:05:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 14 loss:0.046273693442344666 norm:0.0005393382161855698 max memory_allocated 64879.78173828125 
[2025-03-19 01:06:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 15 loss:0.04601573944091797 norm:0.0005370716680772603 max memory_allocated 64879.78173828125 
[2025-03-19 01:08:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 16 loss:0.04578525945544243 norm:0.0005265264771878719 max memory_allocated 64879.78173828125 
[2025-03-19 01:10:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 17 loss:0.04554062336683273 norm:0.0005139984423294663 max memory_allocated 64879.78173828125 
[2025-03-19 01:11:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 18 loss:0.04535118490457535 norm:0.0004977296339347959 max memory_allocated 64879.78173828125 
[2025-03-19 01:13:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 21-22 epoch 19 loss:0.04518427699804306 norm:0.0004925931571051478 max memory_allocated 64879.78173828125 
[2025-03-19 01:14:04 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 21-22
[2025-03-19 01:14:05 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 22 to 23 ===
[2025-03-19 01:16:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 0 loss:0.08140876889228821 norm:0.0029141295235604048 max memory_allocated 64879.96923828125 
[2025-03-19 01:17:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 1 loss:0.06456238031387329 norm:0.0014607985503971577 max memory_allocated 64879.96923828125 
[2025-03-19 01:19:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 2 loss:0.05381031334400177 norm:0.0009186711977235973 max memory_allocated 64879.96923828125 
[2025-03-19 01:21:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 3 loss:0.05025400221347809 norm:0.0006847179029136896 max memory_allocated 64879.96923828125 
[2025-03-19 01:23:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 4 loss:0.04830879718065262 norm:0.0005611445521935821 max memory_allocated 64879.96923828125 
[2025-03-19 01:24:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 5 loss:0.04752061888575554 norm:0.0004950367147102952 max memory_allocated 64879.96923828125 
[2025-03-19 01:26:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 6 loss:0.04703928530216217 norm:0.00045106979086995125 max memory_allocated 64879.96923828125 
[2025-03-19 01:28:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 7 loss:0.04667641222476959 norm:0.00042571008088998497 max memory_allocated 64879.96923828125 
[2025-03-19 01:30:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 8 loss:0.04640958085656166 norm:0.00040772598003968596 max memory_allocated 64879.96923828125 
[2025-03-19 01:31:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 9 loss:0.046208564192056656 norm:0.00039648942765779793 max memory_allocated 64879.96923828125 
[2025-03-19 01:33:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 10 loss:0.0459844246506691 norm:0.0003871791413985193 max memory_allocated 64879.96923828125 
[2025-03-19 01:35:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 11 loss:0.04577784240245819 norm:0.0003795660159084946 max memory_allocated 64879.96923828125 
[2025-03-19 01:36:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 12 loss:0.0456337109208107 norm:0.0003736766811925918 max memory_allocated 64879.96923828125 
[2025-03-19 01:38:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 13 loss:0.04550958424806595 norm:0.00036910054041072726 max memory_allocated 64879.96923828125 
[2025-03-19 01:40:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 14 loss:0.04538901522755623 norm:0.00036519899731501937 max memory_allocated 64879.96923828125 
[2025-03-19 01:41:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 15 loss:0.04528097063302994 norm:0.00036201064358465374 max memory_allocated 64879.96923828125 
[2025-03-19 01:43:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 16 loss:0.04518917202949524 norm:0.00035924697294831276 max memory_allocated 64879.96923828125 
[2025-03-19 01:45:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 17 loss:0.0450674369931221 norm:0.00035222392762079835 max memory_allocated 64879.96923828125 
[2025-03-19 01:46:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 18 loss:0.045015573501586914 norm:0.0003562989877536893 max memory_allocated 64879.96923828125 
[2025-03-19 01:48:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 22-23 epoch 19 loss:0.0449361614882946 norm:0.0003484968328848481 max memory_allocated 64879.96923828125 
[2025-03-19 01:48:57 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 22-23
[2025-03-19 01:48:58 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 23 to 24 ===
[2025-03-19 01:51:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 0 loss:0.10111336410045624 norm:0.004124873783439398 max memory_allocated 64880.15673828125 
[2025-03-19 01:52:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 1 loss:0.0822504311800003 norm:0.002122360747307539 max memory_allocated 64880.15673828125 
[2025-03-19 01:54:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 2 loss:0.0699664056301117 norm:0.001396975014358759 max memory_allocated 64880.15673828125 
[2025-03-19 01:56:22 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 3 loss:0.0656600072979927 norm:0.0010856196749955416 max memory_allocated 64880.15673828125 
[2025-03-19 01:57:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 4 loss:0.06359008699655533 norm:0.0008976793033070862 max memory_allocated 64880.15673828125 
[2025-03-19 01:59:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 5 loss:0.06259414553642273 norm:0.0007755442056804895 max memory_allocated 64880.15673828125 
[2025-03-19 02:01:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 6 loss:0.06198624148964882 norm:0.0007014877628535032 max memory_allocated 64880.15673828125 
[2025-03-19 02:02:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 7 loss:0.06157029792666435 norm:0.0006645820685662329 max memory_allocated 64880.15673828125 
[2025-03-19 02:04:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 8 loss:0.06121939793229103 norm:0.0006282116519287229 max memory_allocated 64880.15673828125 
[2025-03-19 02:06:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 9 loss:0.06112608313560486 norm:0.0006038110004737973 max memory_allocated 64880.15673828125 
[2025-03-19 02:08:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 10 loss:0.060915686190128326 norm:0.0005780948558822274 max memory_allocated 64880.15673828125 
[2025-03-19 02:10:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 11 loss:0.06066922843456268 norm:0.0005522641586139798 max memory_allocated 64880.15673828125 
[2025-03-19 02:11:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 12 loss:0.06046462059020996 norm:0.000542287714779377 max memory_allocated 64880.15673828125 
[2025-03-19 02:13:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 13 loss:0.06029318645596504 norm:0.0005283392383717 max memory_allocated 64880.15673828125 
[2025-03-19 02:15:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 14 loss:0.06010384485125542 norm:0.0005252242553979158 max memory_allocated 64880.15673828125 
[2025-03-19 02:16:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 15 loss:0.059994425624608994 norm:0.0005173975368961692 max memory_allocated 64880.15673828125 
[2025-03-19 02:18:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 16 loss:0.05991671234369278 norm:0.0005160412983968854 max memory_allocated 64880.15673828125 
[2025-03-19 02:20:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 17 loss:0.05986713245511055 norm:0.0005119601846672595 max memory_allocated 64880.15673828125 
[2025-03-19 02:21:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 18 loss:0.05966087803244591 norm:0.0005062275449745357 max memory_allocated 64880.15673828125 
[2025-03-19 02:23:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 23-24 epoch 19 loss:0.059544701129198074 norm:0.0005032382323406637 max memory_allocated 64880.15673828125 
[2025-03-19 02:23:59 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 23-24
[2025-03-19 02:24:00 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 24 to 25 ===
[2025-03-19 02:26:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 0 loss:0.2183995544910431 norm:0.016833998262882233 max memory_allocated 64880.34423828125 
[2025-03-19 02:27:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 1 loss:0.09618739783763885 norm:0.0034785286989063025 max memory_allocated 64880.34423828125 
[2025-03-19 02:29:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 2 loss:0.07032188028097153 norm:0.0014578835107386112 max memory_allocated 64880.34423828125 
[2025-03-19 02:31:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 3 loss:0.06308950483798981 norm:0.0011026825523003936 max memory_allocated 64880.34423828125 
[2025-03-19 02:32:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 4 loss:0.05909370630979538 norm:0.0009881388396024704 max memory_allocated 64880.34423828125 
[2025-03-19 02:34:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 5 loss:0.057099055498838425 norm:0.0008995074895210564 max memory_allocated 64880.34423828125 
[2025-03-19 02:36:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 6 loss:0.05595674365758896 norm:0.0008351158467121422 max memory_allocated 64880.34423828125 
[2025-03-19 02:38:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 7 loss:0.05510867387056351 norm:0.000806668889708817 max memory_allocated 64880.34423828125 
[2025-03-19 02:39:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 8 loss:0.054419975727796555 norm:0.0007824378553777933 max memory_allocated 64880.34423828125 
[2025-03-19 02:41:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 9 loss:0.05376313999295235 norm:0.0007484012749046087 max memory_allocated 64880.34423828125 
[2025-03-19 02:43:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 10 loss:0.05326560139656067 norm:0.0007332238019444048 max memory_allocated 64880.34423828125 
[2025-03-19 02:45:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 11 loss:0.0529361255466938 norm:0.0007132717873901129 max memory_allocated 64880.34423828125 
[2025-03-19 02:46:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 12 loss:0.052486710250377655 norm:0.0006825355812907219 max memory_allocated 64880.34423828125 
[2025-03-19 02:48:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 13 loss:0.05211763456463814 norm:0.0006670948350802064 max memory_allocated 64880.34423828125 
[2025-03-19 02:50:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 14 loss:0.05181225389242172 norm:0.0006540401373058558 max memory_allocated 64880.34423828125 
[2025-03-19 02:51:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 15 loss:0.05154707282781601 norm:0.0006314573111012578 max memory_allocated 64880.34423828125 
[2025-03-19 02:53:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 16 loss:0.0513133630156517 norm:0.0006703517865389585 max memory_allocated 64880.34423828125 
[2025-03-19 02:55:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 17 loss:0.05108882114291191 norm:0.000627110421191901 max memory_allocated 64880.34423828125 
[2025-03-19 02:56:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 18 loss:0.05093367397785187 norm:0.000609743467066437 max memory_allocated 64880.34423828125 
[2025-03-19 02:58:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 24-25 epoch 19 loss:0.05076577886939049 norm:0.0006055535050109029 max memory_allocated 64880.34423828125 
[2025-03-19 02:59:05 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 24-25
[2025-03-19 02:59:05 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 25 to 26 ===
[2025-03-19 03:01:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 0 loss:0.08876281976699829 norm:0.004210676997900009 max memory_allocated 64880.53173828125 
[2025-03-19 03:03:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 1 loss:0.0708240196108818 norm:0.002123995218425989 max memory_allocated 64880.53173828125 
[2025-03-19 03:04:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 2 loss:0.058183081448078156 norm:0.0014553864020854235 max memory_allocated 64880.53173828125 
[2025-03-19 03:06:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 3 loss:0.05393443629145622 norm:0.0011481292312964797 max memory_allocated 64880.53173828125 
[2025-03-19 03:08:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 4 loss:0.051915597170591354 norm:0.0009637056500650942 max memory_allocated 64880.53173828125 
[2025-03-19 03:10:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 5 loss:0.05118480324745178 norm:0.0008504319121129811 max memory_allocated 64880.53173828125 
[2025-03-19 03:11:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 6 loss:0.050675779581069946 norm:0.0007836287841200829 max memory_allocated 64880.53173828125 
[2025-03-19 03:13:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 7 loss:0.0501963347196579 norm:0.0007123868563212454 max memory_allocated 64880.53173828125 
[2025-03-19 03:15:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 8 loss:0.04991331323981285 norm:0.000667847110889852 max memory_allocated 64880.53173828125 
[2025-03-19 03:16:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 9 loss:0.049551237374544144 norm:0.0006209098501130939 max memory_allocated 64880.53173828125 
[2025-03-19 03:18:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 10 loss:0.04922144487500191 norm:0.0005851958994753659 max memory_allocated 64880.53173828125 
[2025-03-19 03:20:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 11 loss:0.04899410530924797 norm:0.000555110746063292 max memory_allocated 64880.53173828125 
[2025-03-19 03:21:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 12 loss:0.04871189594268799 norm:0.000525072100572288 max memory_allocated 64880.53173828125 
[2025-03-19 03:23:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 13 loss:0.04836054891347885 norm:0.0004956384655088186 max memory_allocated 64880.53173828125 
[2025-03-19 03:25:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 14 loss:0.04808366298675537 norm:0.0004805368953384459 max memory_allocated 64880.53173828125 
[2025-03-19 03:26:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 15 loss:0.047948453575372696 norm:0.0004748762003146112 max memory_allocated 64880.53173828125 
[2025-03-19 03:28:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 16 loss:0.047765761613845825 norm:0.0004628426977433264 max memory_allocated 64880.53173828125 
[2025-03-19 03:30:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 17 loss:0.047617994248867035 norm:0.00045523146400228143 max memory_allocated 64880.53173828125 
[2025-03-19 03:32:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 18 loss:0.047432173043489456 norm:0.0004481920332182199 max memory_allocated 64880.53173828125 
[2025-03-19 03:33:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 25-26 epoch 19 loss:0.0473734512925148 norm:0.0004397625452838838 max memory_allocated 64880.53173828125 
[2025-03-19 03:34:17 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 25-26
[2025-03-19 03:34:18 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 26 to 27 ===
[2025-03-19 03:36:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 0 loss:0.23918338119983673 norm:0.025453321635723114 max memory_allocated 64880.71923828125 
[2025-03-19 03:38:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 1 loss:0.1068805456161499 norm:0.00500729912891984 max memory_allocated 64880.71923828125 
[2025-03-19 03:40:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 2 loss:0.07795597612857819 norm:0.0022828378714621067 max memory_allocated 64880.71923828125 
[2025-03-19 03:41:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 3 loss:0.06919560581445694 norm:0.0016947491094470024 max memory_allocated 64880.71923828125 
[2025-03-19 03:43:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 4 loss:0.06455974280834198 norm:0.001429982134141028 max memory_allocated 64880.71923828125 
[2025-03-19 03:45:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 5 loss:0.06261324882507324 norm:0.001256787683814764 max memory_allocated 64880.71923828125 
[2025-03-19 03:46:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 6 loss:0.06155386567115784 norm:0.0011478379601612687 max memory_allocated 64880.71923828125 
[2025-03-19 03:48:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 7 loss:0.06063780188560486 norm:0.0010534704197198153 max memory_allocated 64880.71923828125 
[2025-03-19 03:50:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 8 loss:0.05987225100398064 norm:0.0009963238844648004 max memory_allocated 64880.71923828125 
[2025-03-19 03:51:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 9 loss:0.0593702495098114 norm:0.0009566359221935272 max memory_allocated 64880.71923828125 
[2025-03-19 03:53:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 10 loss:0.05899444967508316 norm:0.0009231116273440421 max memory_allocated 64880.71923828125 
[2025-03-19 03:55:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 11 loss:0.05851145461201668 norm:0.0008883762639015913 max memory_allocated 64880.71923828125 
[2025-03-19 03:57:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 12 loss:0.05809367075562477 norm:0.0008531736093573272 max memory_allocated 64880.71923828125 
[2025-03-19 03:58:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 13 loss:0.057733163237571716 norm:0.000822494737803936 max memory_allocated 64880.71923828125 
[2025-03-19 04:00:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 14 loss:0.05738849937915802 norm:0.0007822559564374387 max memory_allocated 64880.71923828125 
[2025-03-19 04:02:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 15 loss:0.05715435743331909 norm:0.000769183156080544 max memory_allocated 64880.71923828125 
[2025-03-19 04:03:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 16 loss:0.05696533992886543 norm:0.0007451862329617143 max memory_allocated 64880.71923828125 
[2025-03-19 04:05:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 17 loss:0.05669211596250534 norm:0.0007192755001597106 max memory_allocated 64880.71923828125 
[2025-03-19 04:07:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 18 loss:0.05639006942510605 norm:0.0006966629880480468 max memory_allocated 64880.71923828125 
[2025-03-19 04:08:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 26-27 epoch 19 loss:0.056077685207128525 norm:0.0006834479281678796 max memory_allocated 64880.71923828125 
[2025-03-19 04:09:47 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 26-27
[2025-03-19 04:09:48 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 27 to 28 ===
[2025-03-19 04:11:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 0 loss:0.1549501270055771 norm:0.011665417812764645 max memory_allocated 64880.90673828125 
[2025-03-19 04:13:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 1 loss:0.0889105498790741 norm:0.00234469142742455 max memory_allocated 64880.90673828125 
[2025-03-19 04:15:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 2 loss:0.06767811626195908 norm:0.0012781666591763496 max memory_allocated 64880.90673828125 
[2025-03-19 04:16:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 3 loss:0.060974400490522385 norm:0.000998675124719739 max memory_allocated 64880.90673828125 
[2025-03-19 04:18:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 4 loss:0.057637542486190796 norm:0.0008202006574720144 max memory_allocated 64880.90673828125 
[2025-03-19 04:20:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 5 loss:0.05572085082530975 norm:0.0007372396066784859 max memory_allocated 64880.90673828125 
[2025-03-19 04:22:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 6 loss:0.05458004027605057 norm:0.000693551148287952 max memory_allocated 64880.90673828125 
[2025-03-19 04:23:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 7 loss:0.05372670665383339 norm:0.000660654332023114 max memory_allocated 64880.90673828125 
[2025-03-19 04:25:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 8 loss:0.0529683381319046 norm:0.0006313651683740318 max memory_allocated 64880.90673828125 
[2025-03-19 04:27:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 9 loss:0.05234565958380699 norm:0.0006068601505830884 max memory_allocated 64880.90673828125 
[2025-03-19 04:28:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 10 loss:0.05182657763361931 norm:0.0005876533687114716 max memory_allocated 64880.90673828125 
[2025-03-19 04:30:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 11 loss:0.05144864693284035 norm:0.0005839405348524451 max memory_allocated 64880.90673828125 
[2025-03-19 04:32:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 12 loss:0.05112120509147644 norm:0.0005693354760296643 max memory_allocated 64880.90673828125 
[2025-03-19 04:33:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 13 loss:0.05082534998655319 norm:0.0005624498007819057 max memory_allocated 64880.90673828125 
[2025-03-19 04:35:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 14 loss:0.05060790106654167 norm:0.0005512392381206155 max memory_allocated 64880.90673828125 
[2025-03-19 04:37:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 15 loss:0.050361812114715576 norm:0.0005458815721794963 max memory_allocated 64880.90673828125 
[2025-03-19 04:38:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 16 loss:0.0501255989074707 norm:0.0005371492588892579 max memory_allocated 64880.90673828125 
[2025-03-19 04:40:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 17 loss:0.049903977662324905 norm:0.000530912890098989 max memory_allocated 64880.90673828125 
[2025-03-19 04:42:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 18 loss:0.04974633455276489 norm:0.000519908091519028 max memory_allocated 64880.90673828125 
[2025-03-19 04:44:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 27-28 epoch 19 loss:0.049587953835725784 norm:0.0005210650851950049 max memory_allocated 64880.90673828125 
[2025-03-19 04:45:00 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 27-28
[2025-03-19 04:45:00 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 28 to 29 ===
[2025-03-19 04:47:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 0 loss:0.10825155675411224 norm:0.0060044946148991585 max memory_allocated 64881.09423828125 
[2025-03-19 04:48:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 1 loss:0.08511237055063248 norm:0.003184099216014147 max memory_allocated 64881.09423828125 
[2025-03-19 04:50:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 2 loss:0.06858856230974197 norm:0.0020404623355716467 max memory_allocated 64881.09423828125 
[2025-03-19 04:52:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 3 loss:0.06277448683977127 norm:0.0014655919512733817 max memory_allocated 64881.09423828125 
[2025-03-19 04:53:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 4 loss:0.0608622282743454 norm:0.0012183545622974634 max memory_allocated 64881.09423828125 
[2025-03-19 04:55:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 5 loss:0.05985809490084648 norm:0.0010000424226745963 max memory_allocated 64881.09423828125 
[2025-03-19 04:57:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 6 loss:0.0591994933784008 norm:0.000889254966750741 max memory_allocated 64881.09423828125 
[2025-03-19 04:58:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 7 loss:0.05876246467232704 norm:0.00078176538227126 max memory_allocated 64881.09423828125 
[2025-03-19 05:00:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 8 loss:0.05824105069041252 norm:0.0007452421123161912 max memory_allocated 64881.09423828125 
[2025-03-19 05:02:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 9 loss:0.05805967003107071 norm:0.0007021183264441788 max memory_allocated 64881.09423828125 
[2025-03-19 05:03:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 10 loss:0.05753491818904877 norm:0.0006249868310987949 max memory_allocated 64881.09423828125 
[2025-03-19 05:05:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 11 loss:0.05764823406934738 norm:0.0006213211454451084 max memory_allocated 64881.09423828125 
[2025-03-19 05:07:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 12 loss:0.05722572281956673 norm:0.0005914553767070174 max memory_allocated 64881.09423828125 
[2025-03-19 05:09:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 13 loss:0.056848086416721344 norm:0.000554896832909435 max memory_allocated 64881.09423828125 
[2025-03-19 05:11:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 14 loss:0.05684056878089905 norm:0.0005644537159241736 max memory_allocated 64881.09423828125 
[2025-03-19 05:12:41 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 15 loss:0.05635179951786995 norm:0.000517248990945518 max memory_allocated 64881.09423828125 
[2025-03-19 05:14:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 16 loss:0.05633062496781349 norm:0.0005090538179501891 max memory_allocated 64881.09423828125 
[2025-03-19 05:16:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 17 loss:0.056077100336551666 norm:0.0004952900926582515 max memory_allocated 64881.09423828125 
[2025-03-19 05:17:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 18 loss:0.05610848590731621 norm:0.0005026900907978415 max memory_allocated 64881.09423828125 
[2025-03-19 05:19:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 28-29 epoch 19 loss:0.055912479758262634 norm:0.0004975385963916779 max memory_allocated 64881.09423828125 
[2025-03-19 05:20:17 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 28-29
[2025-03-19 05:20:17 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 29 to 30 ===
[2025-03-19 05:22:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 0 loss:0.22556035220623016 norm:0.15105335414409637 max memory_allocated 64881.28173828125 
[2025-03-19 05:23:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 1 loss:0.10926759243011475 norm:0.011981717310845852 max memory_allocated 64881.28173828125 
[2025-03-19 05:25:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 2 loss:0.08088051527738571 norm:0.005775093100965023 max memory_allocated 64881.28173828125 
[2025-03-19 05:27:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 3 loss:0.07230107486248016 norm:0.003349892795085907 max memory_allocated 64881.28173828125 
[2025-03-19 05:29:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 4 loss:0.06856150925159454 norm:0.0031454747077077627 max memory_allocated 64881.28173828125 
[2025-03-19 05:30:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 5 loss:0.06665901094675064 norm:0.0034010503441095352 max memory_allocated 64881.28173828125 
[2025-03-19 05:32:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 6 loss:0.06531001627445221 norm:0.0029106037691235542 max memory_allocated 64881.28173828125 
[2025-03-19 05:34:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 7 loss:0.06431405991315842 norm:0.002618549857288599 max memory_allocated 64881.28173828125 
[2025-03-19 05:36:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 8 loss:0.06365343928337097 norm:0.0024899947457015514 max memory_allocated 64881.28173828125 
[2025-03-19 05:37:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 9 loss:0.06301365792751312 norm:0.002298021921887994 max memory_allocated 64881.28173828125 
[2025-03-19 05:39:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 10 loss:0.06246338412165642 norm:0.0021651999559253454 max memory_allocated 64881.28173828125 
[2025-03-19 05:41:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 11 loss:0.06202623248100281 norm:0.0020235665142536163 max memory_allocated 64881.28173828125 
[2025-03-19 05:42:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 12 loss:0.06163604557514191 norm:0.0019372348906472325 max memory_allocated 64881.28173828125 
[2025-03-19 05:44:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 13 loss:0.061233069747686386 norm:0.0017678657313808799 max memory_allocated 64881.28173828125 
[2025-03-19 05:46:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 14 loss:0.06092078983783722 norm:0.0016639688983559608 max memory_allocated 64881.28173828125 
[2025-03-19 05:47:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 15 loss:0.060638461261987686 norm:0.0016062194481492043 max memory_allocated 64881.28173828125 
[2025-03-19 05:49:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 16 loss:0.06044154614210129 norm:0.001574152265675366 max memory_allocated 64881.28173828125 
[2025-03-19 05:51:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 17 loss:0.06018645316362381 norm:0.0013627762673422694 max memory_allocated 64881.28173828125 
[2025-03-19 05:52:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 18 loss:0.06001219153404236 norm:0.0011963342549279332 max memory_allocated 64881.28173828125 
[2025-03-19 05:54:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 29-30 epoch 19 loss:0.0597725585103035 norm:0.0011896929936483502 max memory_allocated 64881.28173828125 
[2025-03-19 05:55:30 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 29-30
[2025-03-19 05:55:31 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 30 to 31 ===
[2025-03-19 05:57:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 0 loss:0.11492422968149185 norm:0.006652644835412502 max memory_allocated 64881.46923828125 
[2025-03-19 05:59:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 1 loss:0.08760453015565872 norm:0.0031709966715425253 max memory_allocated 64881.46923828125 
[2025-03-19 06:01:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 2 loss:0.07019264250993729 norm:0.0019990301225334406 max memory_allocated 64881.46923828125 
[2025-03-19 06:02:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 3 loss:0.06448963284492493 norm:0.0014611013466492295 max memory_allocated 64881.46923828125 
[2025-03-19 06:04:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 4 loss:0.06213727593421936 norm:0.0011445898562669754 max memory_allocated 64881.46923828125 
[2025-03-19 06:06:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 5 loss:0.060797762125730515 norm:0.0009599040495231748 max memory_allocated 64881.46923828125 
[2025-03-19 06:07:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 6 loss:0.0599796362221241 norm:0.0008667740039527416 max memory_allocated 64881.46923828125 
[2025-03-19 06:09:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 7 loss:0.05925513803958893 norm:0.0007820910541340709 max memory_allocated 64881.46923828125 
[2025-03-19 06:11:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 8 loss:0.058734722435474396 norm:0.000713118992280215 max memory_allocated 64881.46923828125 
[2025-03-19 06:12:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 9 loss:0.058189671486616135 norm:0.0006641755462624133 max memory_allocated 64881.46923828125 
[2025-03-19 06:14:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 10 loss:0.05770796537399292 norm:0.0006254242616705596 max memory_allocated 64881.46923828125 
[2025-03-19 06:16:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 11 loss:0.05734311789274216 norm:0.0006056273705326021 max memory_allocated 64881.46923828125 
[2025-03-19 06:17:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 12 loss:0.05699174851179123 norm:0.0005822918028570712 max memory_allocated 64881.46923828125 
[2025-03-19 06:19:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 13 loss:0.05667147412896156 norm:0.000558200990781188 max memory_allocated 64881.46923828125 
[2025-03-19 06:21:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 14 loss:0.056367725133895874 norm:0.0005383590469136834 max memory_allocated 64881.46923828125 
[2025-03-19 06:23:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 15 loss:0.05617177113890648 norm:0.0005323040531948209 max memory_allocated 64881.46923828125 
[2025-03-19 06:25:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 16 loss:0.05601142346858978 norm:0.000530963996425271 max memory_allocated 64881.46923828125 
[2025-03-19 06:26:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 17 loss:0.05586560070514679 norm:0.0005197923164814711 max memory_allocated 64881.46923828125 
[2025-03-19 06:28:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 18 loss:0.05568843334913254 norm:0.0005150900105945766 max memory_allocated 64881.46923828125 
[2025-03-19 06:30:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 30-31 epoch 19 loss:0.05555851757526398 norm:0.0005171460215933621 max memory_allocated 64881.46923828125 
[2025-03-19 06:30:46 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 30-31
[2025-03-19 06:30:47 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 31 to 32 ===
[2025-03-19 06:32:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 0 loss:0.17304179072380066 norm:0.015906736254692078 max memory_allocated 64881.65673828125 
[2025-03-19 06:34:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 1 loss:0.09982355684041977 norm:0.004236468113958836 max memory_allocated 64881.65673828125 
[2025-03-19 06:36:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 2 loss:0.07662405073642731 norm:0.0024312306195497513 max memory_allocated 64881.65673828125 
[2025-03-19 06:37:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 3 loss:0.069420225918293 norm:0.001674671657383442 max memory_allocated 64881.65673828125 
[2025-03-19 06:39:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 4 loss:0.06631360203027725 norm:0.0012878477573394775 max memory_allocated 64881.65673828125 
[2025-03-19 06:41:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 5 loss:0.0646013468503952 norm:0.0010602427646517754 max memory_allocated 64881.65673828125 
[2025-03-19 06:42:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 6 loss:0.06330130994319916 norm:0.0009033146779984236 max memory_allocated 64881.65673828125 
[2025-03-19 06:44:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 7 loss:0.062430962920188904 norm:0.0008001477108336985 max memory_allocated 64881.65673828125 
[2025-03-19 06:46:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 8 loss:0.06164315715432167 norm:0.0007134307525120676 max memory_allocated 64881.65673828125 
[2025-03-19 06:48:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 9 loss:0.06099686399102211 norm:0.00066067383158952 max memory_allocated 64881.65673828125 
[2025-03-19 06:50:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 10 loss:0.06054651737213135 norm:0.0006184293888509274 max memory_allocated 64881.65673828125 
[2025-03-19 06:51:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 11 loss:0.060170650482177734 norm:0.0005851694731973112 max memory_allocated 64881.65673828125 
[2025-03-19 06:53:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 12 loss:0.05988268926739693 norm:0.000561497756280005 max memory_allocated 64881.65673828125 
[2025-03-19 06:55:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 13 loss:0.05958246812224388 norm:0.0005488302558660507 max memory_allocated 64881.65673828125 
[2025-03-19 06:56:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 14 loss:0.05933057889342308 norm:0.0005314773879945278 max memory_allocated 64881.65673828125 
[2025-03-19 06:58:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 15 loss:0.05916730314493179 norm:0.0005251620896160603 max memory_allocated 64881.65673828125 
[2025-03-19 07:00:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 16 loss:0.059010110795497894 norm:0.000522195128723979 max memory_allocated 64881.65673828125 
[2025-03-19 07:01:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 17 loss:0.05890098586678505 norm:0.0005103086004965007 max memory_allocated 64881.65673828125 
[2025-03-19 07:03:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 18 loss:0.05879012495279312 norm:0.0005103024886921048 max memory_allocated 64881.65673828125 
[2025-03-19 07:05:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 31-32 epoch 19 loss:0.058701593428850174 norm:0.0005044605932198465 max memory_allocated 64881.65673828125 
[2025-03-19 07:05:53 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 31-32
[2025-03-19 07:05:53 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 32 to 33 ===
[2025-03-19 07:07:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 0 loss:0.23048654198646545 norm:1.0038260221481323 max memory_allocated 64881.84423828125 
[2025-03-19 07:09:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 1 loss:0.12556003034114838 norm:0.15847420692443848 max memory_allocated 64881.84423828125 
[2025-03-19 07:11:23 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 2 loss:0.0954514741897583 norm:0.08565764129161835 max memory_allocated 64881.84423828125 
[2025-03-19 07:12:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 3 loss:0.08367538452148438 norm:0.05585794523358345 max memory_allocated 64881.84423828125 
[2025-03-19 07:14:54 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 4 loss:0.0782318264245987 norm:0.039578456431627274 max memory_allocated 64881.84423828125 
[2025-03-19 07:16:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 5 loss:0.07518582046031952 norm:0.02946241945028305 max memory_allocated 64881.84423828125 
[2025-03-19 07:18:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 6 loss:0.07329601794481277 norm:0.02269129455089569 max memory_allocated 64881.84423828125 
[2025-03-19 07:20:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 7 loss:0.07189193367958069 norm:0.017846163362264633 max memory_allocated 64881.84423828125 
[2025-03-19 07:21:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 8 loss:0.07096686214208603 norm:0.014220796525478363 max memory_allocated 64881.84423828125 
[2025-03-19 07:23:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 9 loss:0.07053476572036743 norm:0.011487816460430622 max memory_allocated 64881.84423828125 
[2025-03-19 07:25:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 10 loss:0.07025434076786041 norm:0.00935191847383976 max memory_allocated 64881.84423828125 
[2025-03-19 07:26:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 11 loss:0.06996798515319824 norm:0.007700287736952305 max memory_allocated 64881.84423828125 
[2025-03-19 07:28:20 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 12 loss:0.06972602754831314 norm:0.0063913906924426556 max memory_allocated 64881.84423828125 
[2025-03-19 07:30:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 13 loss:0.06962433457374573 norm:0.005362328141927719 max memory_allocated 64881.84423828125 
[2025-03-19 07:31:52 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 14 loss:0.06950539350509644 norm:0.0045305476523935795 max memory_allocated 64881.84423828125 
[2025-03-19 07:33:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 15 loss:0.06950628012418747 norm:0.0038704639300704002 max memory_allocated 64881.84423828125 
[2025-03-19 07:35:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 16 loss:0.06962007284164429 norm:0.0033425723668187857 max memory_allocated 64881.84423828125 
[2025-03-19 07:36:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 17 loss:0.06968839466571808 norm:0.0029114792123436928 max memory_allocated 64881.84423828125 
[2025-03-19 07:38:34 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 18 loss:0.0699114203453064 norm:0.0025654395576566458 max memory_allocated 64881.84423828125 
[2025-03-19 07:40:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 32-33 epoch 19 loss:0.07012209296226501 norm:0.0022751905489712954 max memory_allocated 64881.84423828125 
[2025-03-19 07:41:03 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 32-33
[2025-03-19 07:41:04 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 33 to 34 ===
[2025-03-19 07:43:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 0 loss:0.16921386122703552 norm:0.01206488162279129 max memory_allocated 64882.03173828125 
[2025-03-19 07:44:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 1 loss:0.11302288621664047 norm:0.0033585710916668177 max memory_allocated 64882.03173828125 
[2025-03-19 07:46:35 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 2 loss:0.0886039286851883 norm:0.002176870359107852 max memory_allocated 64882.03173828125 
[2025-03-19 07:48:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 3 loss:0.08060427010059357 norm:0.001676557818427682 max memory_allocated 64882.03173828125 
[2025-03-19 07:50:08 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 4 loss:0.07790420204401016 norm:0.0014444870175793767 max memory_allocated 64882.03173828125 
[2025-03-19 07:51:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 5 loss:0.0763113722205162 norm:0.001269615488126874 max memory_allocated 64882.03173828125 
[2025-03-19 07:53:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 6 loss:0.07499836385250092 norm:0.0011523384600877762 max memory_allocated 64882.03173828125 
[2025-03-19 07:55:15 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 7 loss:0.07425809651613235 norm:0.001084049348719418 max memory_allocated 64882.03173828125 
[2025-03-19 07:56:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 8 loss:0.07330400496721268 norm:0.000982189434580505 max memory_allocated 64882.03173828125 
[2025-03-19 07:58:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 9 loss:0.07259651273488998 norm:0.000931299407966435 max memory_allocated 64882.03173828125 
[2025-03-19 08:00:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 10 loss:0.07180742919445038 norm:0.0008821136434562504 max memory_allocated 64882.03173828125 
[2025-03-19 08:01:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 11 loss:0.07127904891967773 norm:0.0008371634176000953 max memory_allocated 64882.03173828125 
[2025-03-19 08:03:35 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 12 loss:0.0708450898528099 norm:0.000797986751422286 max memory_allocated 64882.03173828125 
[2025-03-19 08:05:31 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 13 loss:0.0704595148563385 norm:0.00076631223782897 max memory_allocated 64882.03173828125 
[2025-03-19 08:07:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 14 loss:0.06987614929676056 norm:0.0007214354118332267 max memory_allocated 64882.03173828125 
[2025-03-19 08:08:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 15 loss:0.06930692493915558 norm:0.000700759410392493 max memory_allocated 64882.03173828125 
[2025-03-19 08:10:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 16 loss:0.0688810646533966 norm:0.0006830698112025857 max memory_allocated 64882.03173828125 
[2025-03-19 08:12:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 17 loss:0.06855202466249466 norm:0.0006702215760014951 max memory_allocated 64882.03173828125 
[2025-03-19 08:13:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 18 loss:0.06825688481330872 norm:0.000666520674712956 max memory_allocated 64882.03173828125 
[2025-03-19 08:15:44 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 33-34 epoch 19 loss:0.06811799854040146 norm:0.000669993634801358 max memory_allocated 64882.03173828125 
[2025-03-19 08:16:17 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 33-34
[2025-03-19 08:16:17 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 34 to 35 ===
[2025-03-19 08:18:18 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 0 loss:0.2787719964981079 norm:0.02805723436176777 max memory_allocated 64882.21923828125 
[2025-03-19 08:20:16 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 1 loss:0.14388707280158997 norm:0.007303912192583084 max memory_allocated 64882.21923828125 
[2025-03-19 08:21:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 2 loss:0.10507705807685852 norm:0.0038573641795665026 max memory_allocated 64882.21923828125 
[2025-03-19 08:23:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 3 loss:0.09324911236763 norm:0.0026989206671714783 max memory_allocated 64882.21923828125 
[2025-03-19 08:25:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 4 loss:0.08868586272001266 norm:0.002172182546928525 max memory_allocated 64882.21923828125 
[2025-03-19 08:27:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 5 loss:0.08591368049383163 norm:0.0018475805409252644 max memory_allocated 64882.21923828125 
[2025-03-19 08:28:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 6 loss:0.08421559631824493 norm:0.0016352611128240824 max memory_allocated 64882.21923828125 
[2025-03-19 08:30:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 7 loss:0.08285913616418839 norm:0.0014906919095665216 max memory_allocated 64882.21923828125 
[2025-03-19 08:32:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 8 loss:0.0817633718252182 norm:0.0013654862996190786 max memory_allocated 64882.21923828125 
[2025-03-19 08:33:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 9 loss:0.0808233767747879 norm:0.0012431165669113398 max memory_allocated 64882.21923828125 
[2025-03-19 08:35:39 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 10 loss:0.08024924993515015 norm:0.0011927593732252717 max memory_allocated 64882.21923828125 
[2025-03-19 08:37:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 11 loss:0.07957525551319122 norm:0.0011398893548175693 max memory_allocated 64882.21923828125 
[2025-03-19 08:38:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 12 loss:0.07874750345945358 norm:0.0010848824167624116 max memory_allocated 64882.21923828125 
[2025-03-19 08:40:45 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 13 loss:0.07798735052347183 norm:0.0010044184746220708 max memory_allocated 64882.21923828125 
[2025-03-19 08:42:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 14 loss:0.07755349576473236 norm:0.0009977187728509307 max memory_allocated 64882.21923828125 
[2025-03-19 08:43:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 15 loss:0.07705491036176682 norm:0.0009470306104049087 max memory_allocated 64882.21923828125 
[2025-03-19 08:45:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 16 loss:0.07653138786554337 norm:0.0009119524620473385 max memory_allocated 64882.21923828125 
[2025-03-19 08:47:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 17 loss:0.0760047659277916 norm:0.0009229096467606723 max memory_allocated 64882.21923828125 
[2025-03-19 08:49:04 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 18 loss:0.07581891119480133 norm:0.0009318367810919881 max memory_allocated 64882.21923828125 
[2025-03-19 08:51:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 34-35 epoch 19 loss:0.07555600255727768 norm:0.0009033007081598043 max memory_allocated 64882.21923828125 
[2025-03-19 08:51:39 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 34-35
[2025-03-19 08:51:40 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 35 to 36 ===
[2025-03-19 08:53:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 0 loss:0.20455949008464813 norm:0.03390735760331154 max memory_allocated 64882.62353515625 
[2025-03-19 08:55:35 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 1 loss:0.1382400542497635 norm:0.019203515723347664 max memory_allocated 64882.62353515625 
[2025-03-19 08:57:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 2 loss:0.10722187161445618 norm:0.015184266492724419 max memory_allocated 64882.62353515625 
[2025-03-19 08:58:46 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 3 loss:0.09855806827545166 norm:0.01339831855148077 max memory_allocated 64882.62353515625 
[2025-03-19 09:00:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 4 loss:0.09342256933450699 norm:0.011767059564590454 max memory_allocated 64882.62353515625 
[2025-03-19 09:02:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 5 loss:0.09048407524824142 norm:0.010484318248927593 max memory_allocated 64882.62353515625 
[2025-03-19 09:03:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 6 loss:0.08837063610553741 norm:0.009476014412939548 max memory_allocated 64882.62353515625 
[2025-03-19 09:05:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 7 loss:0.08674171566963196 norm:0.008567501790821552 max memory_allocated 64882.62353515625 
[2025-03-19 09:07:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 8 loss:0.08522787690162659 norm:0.007607329171150923 max memory_allocated 64882.62353515625 
[2025-03-19 09:08:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 9 loss:0.08422362804412842 norm:0.006748974788933992 max memory_allocated 64882.62353515625 
[2025-03-19 09:10:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 10 loss:0.08296951651573181 norm:0.005886788945645094 max memory_allocated 64882.62353515625 
[2025-03-19 09:12:24 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 11 loss:0.0821872353553772 norm:0.005199010483920574 max memory_allocated 64882.62353515625 
[2025-03-19 09:13:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 12 loss:0.08155140280723572 norm:0.005070175044238567 max memory_allocated 64882.62353515625 
[2025-03-19 09:15:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 13 loss:0.08098456263542175 norm:0.005003498867154121 max memory_allocated 64882.62353515625 
[2025-03-19 09:17:29 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 14 loss:0.08052635192871094 norm:0.004832487553358078 max memory_allocated 64882.62353515625 
[2025-03-19 09:19:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 15 loss:0.0799454003572464 norm:0.004371828865259886 max memory_allocated 64882.62353515625 
[2025-03-19 09:21:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 16 loss:0.07964392006397247 norm:0.004321990069001913 max memory_allocated 64882.62353515625 
[2025-03-19 09:22:37 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 17 loss:0.0794200450181961 norm:0.0044089425355196 max memory_allocated 64882.62353515625 
[2025-03-19 09:24:11 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 18 loss:0.07915379106998444 norm:0.004601639229804277 max memory_allocated 64882.62353515625 
[2025-03-19 09:26:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 35-36 epoch 19 loss:0.07874003052711487 norm:0.004254091065376997 max memory_allocated 64882.62353515625 
[2025-03-19 09:26:42 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 35-36
[2025-03-19 09:26:43 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 36 to 37 ===
[2025-03-19 09:28:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 0 loss:0.26602083444595337 norm:0.029019776731729507 max memory_allocated 64883.02783203125 
[2025-03-19 09:30:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 1 loss:0.18559831380844116 norm:0.019681045785546303 max memory_allocated 64883.02783203125 
[2025-03-19 09:32:12 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 2 loss:0.14693033695220947 norm:0.015416407026350498 max memory_allocated 64883.02783203125 
[2025-03-19 09:33:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 3 loss:0.13405698537826538 norm:0.012037128210067749 max memory_allocated 64883.02783203125 
[2025-03-19 09:35:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 4 loss:0.1291029453277588 norm:0.010220536962151527 max memory_allocated 64883.02783203125 
[2025-03-19 09:37:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 5 loss:0.12631285190582275 norm:0.009486859664320946 max memory_allocated 64883.02783203125 
[2025-03-19 09:38:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 6 loss:0.12456541508436203 norm:0.009794754907488823 max memory_allocated 64883.02783203125 
[2025-03-19 09:40:48 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 7 loss:0.12301672995090485 norm:0.009294773451983929 max memory_allocated 64883.02783203125 
[2025-03-19 09:42:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 8 loss:0.12190312892198563 norm:0.008841508068144321 max memory_allocated 64883.02783203125 
[2025-03-19 09:43:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 9 loss:0.1213926374912262 norm:0.008312655612826347 max memory_allocated 64883.02783203125 
[2025-03-19 09:45:55 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 10 loss:0.12015214562416077 norm:0.007712395861744881 max memory_allocated 64883.02783203125 
[2025-03-19 09:47:32 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 11 loss:0.1193629652261734 norm:0.007723325397819281 max memory_allocated 64883.02783203125 
[2025-03-19 09:49:06 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 12 loss:0.11842051148414612 norm:0.0071812765672802925 max memory_allocated 64883.02783203125 
[2025-03-19 09:51:03 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 13 loss:0.1177220419049263 norm:0.006795894354581833 max memory_allocated 64883.02783203125 
[2025-03-19 09:52:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 14 loss:0.11729779839515686 norm:0.006739472504705191 max memory_allocated 64883.02783203125 
[2025-03-19 09:54:30 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 15 loss:0.11692509800195694 norm:0.006528254132717848 max memory_allocated 64883.02783203125 
[2025-03-19 09:56:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 16 loss:0.11657120287418365 norm:0.00610332703217864 max memory_allocated 64883.02783203125 
[2025-03-19 09:57:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 17 loss:0.11639110743999481 norm:0.005993359722197056 max memory_allocated 64883.02783203125 
[2025-03-19 09:59:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 18 loss:0.1163020133972168 norm:0.006143998354673386 max memory_allocated 64883.02783203125 
[2025-03-19 10:01:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 36-37 epoch 19 loss:0.11595702916383743 norm:0.006034063640981913 max memory_allocated 64883.02783203125 
[2025-03-19 10:01:53 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 36-37
[2025-03-19 10:01:54 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 37 to 38 ===
[2025-03-19 10:03:53 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 0 loss:0.8056403994560242 norm:0.8671188354492188 max memory_allocated 64883.21533203125 
[2025-03-19 10:05:49 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 1 loss:0.5289068222045898 norm:0.5535872578620911 max memory_allocated 64883.21533203125 
[2025-03-19 10:07:26 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 2 loss:0.4045589864253998 norm:0.3914124071598053 max memory_allocated 64883.21533203125 
[2025-03-19 10:09:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 3 loss:0.35749703645706177 norm:0.3456113636493683 max memory_allocated 64883.21533203125 
[2025-03-19 10:10:57 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 4 loss:0.332242876291275 norm:0.3217047452926636 max memory_allocated 64883.21533203125 
[2025-03-19 10:12:33 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 5 loss:0.2995304763317108 norm:0.25018641352653503 max memory_allocated 64883.21533203125 
[2025-03-19 10:14:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 6 loss:0.2736421525478363 norm:0.20060428977012634 max memory_allocated 64883.21533203125 
[2025-03-19 10:16:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 7 loss:0.27283936738967896 norm:0.2164558619260788 max memory_allocated 64883.21533203125 
[2025-03-19 10:17:38 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 8 loss:0.25472939014434814 norm:0.174237459897995 max memory_allocated 64883.21533203125 
[2025-03-19 10:19:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 9 loss:0.24543878436088562 norm:0.1661074310541153 max memory_allocated 64883.21533203125 
[2025-03-19 10:21:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 10 loss:0.24211806058883667 norm:0.15105213224887848 max memory_allocated 64883.21533203125 
[2025-03-19 10:22:47 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 11 loss:0.23039783537387848 norm:0.12739905714988708 max memory_allocated 64883.21533203125 
[2025-03-19 10:24:42 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 12 loss:0.231539785861969 norm:0.13570114970207214 max memory_allocated 64883.21533203125 
[2025-03-19 10:26:19 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 13 loss:0.2366681545972824 norm:0.16637840867042542 max memory_allocated 64883.21533203125 
[2025-03-19 10:27:56 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 14 loss:0.2465108036994934 norm:0.1773948222398758 max memory_allocated 64883.21533203125 
[2025-03-19 10:29:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 15 loss:0.2337486445903778 norm:0.13485530018806458 max memory_allocated 64883.21533203125 
[2025-03-19 10:31:28 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 16 loss:0.2360401600599289 norm:0.13728053867816925 max memory_allocated 64883.21533203125 
[2025-03-19 10:33:02 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 17 loss:0.2283172756433487 norm:0.12252256274223328 max memory_allocated 64883.21533203125 
[2025-03-19 10:34:59 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 18 loss:0.2312120646238327 norm:0.11476866900920868 max memory_allocated 64883.21533203125 
[2025-03-19 10:36:36 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 37-38 epoch 19 loss:0.22576460242271423 norm:0.10672912746667862 max memory_allocated 64883.21533203125 
[2025-03-19 10:37:09 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 37-38
[2025-03-19 10:37:10 root] (abq_llm_calib_config3_cbq.py 302): INFO === Start quantize layers 38 to 39 ===
[2025-03-19 10:39:10 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 0 loss:1.0197038650512695 norm:0.162439227104187 max memory_allocated 64883.40283203125 
[2025-03-19 10:41:07 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 1 loss:0.7115404605865479 norm:0.10558988153934479 max memory_allocated 64883.40283203125 
[2025-03-19 10:42:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 2 loss:0.5600543022155762 norm:0.07069925963878632 max memory_allocated 64883.40283203125 
[2025-03-19 10:44:40 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 3 loss:0.502221405506134 norm:0.06481729447841644 max memory_allocated 64883.40283203125 
[2025-03-19 10:46:14 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 4 loss:0.47101253271102905 norm:0.06096530705690384 max memory_allocated 64883.40283203125 
[2025-03-19 10:47:51 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 5 loss:0.44735419750213623 norm:0.05616098269820213 max memory_allocated 64883.40283203125 
[2025-03-19 10:49:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 6 loss:0.4304181635379791 norm:0.05434121564030647 max memory_allocated 64883.40283203125 
[2025-03-19 10:51:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 7 loss:0.41790053248405457 norm:0.05221362039446831 max memory_allocated 64883.40283203125 
[2025-03-19 10:53:01 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 8 loss:0.40860337018966675 norm:0.05184158310294151 max memory_allocated 64883.40283203125 
[2025-03-19 10:54:58 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 9 loss:0.40201547741889954 norm:0.05071105808019638 max memory_allocated 64883.40283203125 
[2025-03-19 10:56:35 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 10 loss:0.3928983807563782 norm:0.04849976301193237 max memory_allocated 64883.40283203125 
[2025-03-19 10:58:09 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 11 loss:0.3879317045211792 norm:0.04689650237560272 max memory_allocated 64883.40283203125 
[2025-03-19 11:00:05 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 12 loss:0.38226446509361267 norm:0.04687189310789108 max memory_allocated 64883.40283203125 
[2025-03-19 11:01:43 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 13 loss:0.3814813792705536 norm:0.045118432492017746 max memory_allocated 64883.40283203125 
[2025-03-19 11:03:17 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 14 loss:0.38087427616119385 norm:0.04577891528606415 max memory_allocated 64883.40283203125 
[2025-03-19 11:05:13 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 15 loss:0.373790442943573 norm:0.04402880370616913 max memory_allocated 64883.40283203125 
[2025-03-19 11:06:50 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 16 loss:0.3683170974254608 norm:0.04274270683526993 max memory_allocated 64883.40283203125 
[2025-03-19 11:08:27 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 17 loss:0.36652615666389465 norm:0.046515993773937225 max memory_allocated 64883.40283203125 
[2025-03-19 11:10:25 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 18 loss:0.36267992854118347 norm:0.044056378304958344 max memory_allocated 64883.40283203125 
[2025-03-19 11:12:00 root] (abq_llm_calib_config3_cbq.py 508): INFO layers 38-39 epoch 19 loss:0.36117973923683167 norm:0.04507496953010559 max memory_allocated 64883.40283203125 
[2025-03-19 11:12:36 root] (abq_llm_calib_config3_cbq.py 568): INFO Saving abq_parameters for block 38-39
[2025-03-19 11:12:37 root] (main_calib_config3_cbq.py 376): INFO 81786.42580103874
[2025-03-19 11:12:49 root] (main_calib_config3_cbq.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-19 11:14:26 root] (main_calib_config3_cbq.py 161): INFO wikitext2 : 5.982678413391113
[2025-03-19 11:14:26 root] (main_calib_config3_cbq.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-19 11:16:56 root] (main_calib_config3_cbq.py 161): INFO c4 : 7.693708419799805
[2025-03-19 12:51:24 root] (main_calib_config3_cbq.py 172): INFO {'wikitext2': 5.982678413391113, 'c4': 7.693708419799805, 'results': {'hellaswag': {'acc': 0.5468034256124278, 'acc_stderr': 0.004967872475383275, 'acc_norm': 0.7152957578171679, 'acc_norm_stderr': 0.004503511855050031}, 'boolq': {'acc': 0.6892966360856269, 'acc_stderr': 0.00809410058188262}, 'piqa': {'acc': 0.7731229597388466, 'acc_stderr': 0.009771584259215167, 'acc_norm': 0.7780195865070729, 'acc_norm_stderr': 0.009696120744662026}, 'winogrande': {'acc': 0.6621941594317285, 'acc_stderr': 0.013292583502910883}, 'arc_challenge': {'acc': 0.38310580204778155, 'acc_stderr': 0.014206472661672883, 'acc_norm': 0.4087030716723549, 'acc_norm_stderr': 0.014365750345427008}, 'arc_easy': {'acc': 0.7255892255892256, 'acc_stderr': 0.009156177122244522, 'acc_norm': 0.5656565656565656, 'acc_norm_stderr': 0.010170943451269421}}, 'versions': {'hellaswag': 0, 'boolq': 1, 'piqa': 0, 'winogrande': 0, 'arc_challenge': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-19 12:51:24 root] (main_calib_config3_cbq.py 175): INFO 38.31,72.56,68.93,54.68,77.31,66.22
