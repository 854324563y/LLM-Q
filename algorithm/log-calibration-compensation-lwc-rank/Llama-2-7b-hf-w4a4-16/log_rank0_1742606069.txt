[2025-03-22 01:14:29 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/Llama-2-7b-hf-w4a4-16', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=16)
[2025-03-22 01:17:55 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 01:17:55 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:17:55 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 01:17:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:18:00 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:18:32 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.12565958499908447 norm:0.17540310323238373 max memory_allocated 22564.69970703125 
[2025-03-22 01:19:04 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.04999928176403046 norm:0.08702559769153595 max memory_allocated 22564.69970703125 
[2025-03-22 01:19:37 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.03690991550683975 norm:0.05918539687991142 max memory_allocated 22564.69970703125 
[2025-03-22 01:20:09 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.03218648210167885 norm:0.053780168294906616 max memory_allocated 22564.69970703125 
[2025-03-22 01:20:42 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.030289268121123314 norm:0.05065471678972244 max memory_allocated 22564.69970703125 
[2025-03-22 01:21:14 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.028658980503678322 norm:0.04739741235971451 max memory_allocated 22564.69970703125 
[2025-03-22 01:21:47 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.027008039876818657 norm:0.043141819536685944 max memory_allocated 22564.69970703125 
[2025-03-22 01:22:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.025663459673523903 norm:0.038886189460754395 max memory_allocated 22564.69970703125 
[2025-03-22 01:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.025237543508410454 norm:0.03574958071112633 max memory_allocated 22564.69970703125 
[2025-03-22 01:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.024549098685383797 norm:0.03191791847348213 max memory_allocated 22564.69970703125 
[2025-03-22 01:23:57 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.023622216656804085 norm:0.0283806212246418 max memory_allocated 22564.69970703125 
[2025-03-22 01:24:30 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.022958029061555862 norm:0.025302493944764137 max memory_allocated 22564.69970703125 
[2025-03-22 01:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.02258078008890152 norm:0.0225934237241745 max memory_allocated 22564.69970703125 
[2025-03-22 01:25:35 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.022093093022704124 norm:0.020059019327163696 max memory_allocated 22564.69970703125 
[2025-03-22 01:26:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.021738193929195404 norm:0.01807195506989956 max memory_allocated 22564.69970703125 
[2025-03-22 01:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.021375512704253197 norm:0.016002371907234192 max memory_allocated 22564.69970703125 
[2025-03-22 01:27:12 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.02104315347969532 norm:0.014394551515579224 max memory_allocated 22564.69970703125 
[2025-03-22 01:27:45 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.020753413438796997 norm:0.013319827616214752 max memory_allocated 22564.69970703125 
[2025-03-22 01:28:18 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.020600594580173492 norm:0.012062822468578815 max memory_allocated 22564.69970703125 
[2025-03-22 01:28:50 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.020642727613449097 norm:0.01196383312344551 max memory_allocated 22564.69970703125 
[2025-03-22 01:28:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:29:02 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:29:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.40396469831466675 norm:0.2817794978618622 max memory_allocated 22564.87158203125 
[2025-03-22 01:30:07 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.23362457752227783 norm:0.17109230160713196 max memory_allocated 22564.87158203125 
[2025-03-22 01:30:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.17736788094043732 norm:0.10190130025148392 max memory_allocated 22564.87158203125 
[2025-03-22 01:31:12 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.15817192196846008 norm:0.0844612717628479 max memory_allocated 22564.87158203125 
[2025-03-22 01:31:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.15032528340816498 norm:0.0746319517493248 max memory_allocated 22564.87158203125 
[2025-03-22 01:32:17 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.14282646775245667 norm:0.07236715406179428 max memory_allocated 22564.87158203125 
[2025-03-22 01:32:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.13639070093631744 norm:0.07087095081806183 max memory_allocated 22564.87158203125 
[2025-03-22 01:33:22 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.12952804565429688 norm:0.06242560222744942 max memory_allocated 22564.87158203125 
[2025-03-22 01:33:54 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.1301453858613968 norm:0.06394921243190765 max memory_allocated 22564.87158203125 
[2025-03-22 01:34:27 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.12709686160087585 norm:0.059480004012584686 max memory_allocated 22564.87158203125 
[2025-03-22 01:34:59 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.12191959470510483 norm:0.055576156824827194 max memory_allocated 22564.87158203125 
[2025-03-22 01:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.12058578431606293 norm:0.04951799660921097 max memory_allocated 22564.87158203125 
[2025-03-22 01:36:04 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.1223161369562149 norm:0.05108124762773514 max memory_allocated 22564.87158203125 
[2025-03-22 01:36:37 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.12002059817314148 norm:0.04777289181947708 max memory_allocated 22564.87158203125 
[2025-03-22 01:37:09 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.11965484172105789 norm:0.04522270709276199 max memory_allocated 22564.87158203125 
[2025-03-22 01:37:42 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.11753462255001068 norm:0.04425140842795372 max memory_allocated 22564.87158203125 
[2025-03-22 01:38:15 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.11743883043527603 norm:0.040136195719242096 max memory_allocated 22564.87158203125 
[2025-03-22 01:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.11670278757810593 norm:0.039319537580013275 max memory_allocated 22564.87158203125 
[2025-03-22 01:39:20 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.11624649167060852 norm:0.037760794162750244 max memory_allocated 22564.87158203125 
[2025-03-22 01:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.11750118434429169 norm:0.03796142339706421 max memory_allocated 22564.87158203125 
[2025-03-22 01:40:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:40:05 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:40:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.24133232235908508 norm:0.14654475450515747 max memory_allocated 22565.04345703125 
[2025-03-22 01:41:11 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.1799502670764923 norm:0.11655918508768082 max memory_allocated 22565.04345703125 
[2025-03-22 01:41:43 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.15814244747161865 norm:0.0868566632270813 max memory_allocated 22565.04345703125 
[2025-03-22 01:42:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.1487123668193817 norm:0.06902649253606796 max memory_allocated 22565.04345703125 
[2025-03-22 01:42:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.14457952976226807 norm:0.060871563851833344 max memory_allocated 22565.04345703125 
[2025-03-22 01:43:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.14085736870765686 norm:0.053155310451984406 max memory_allocated 22565.04345703125 
[2025-03-22 01:43:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.13793039321899414 norm:0.04620884358882904 max memory_allocated 22565.04345703125 
[2025-03-22 01:44:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.13571126759052277 norm:0.04022861272096634 max memory_allocated 22565.04345703125 
[2025-03-22 01:45:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.133962020277977 norm:0.03473234549164772 max memory_allocated 22565.04345703125 
[2025-03-22 01:45:33 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.13237327337265015 norm:0.030165396630764008 max memory_allocated 22565.04345703125 
[2025-03-22 01:46:05 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.13106191158294678 norm:0.025696847587823868 max memory_allocated 22565.04345703125 
[2025-03-22 01:46:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.12991352379322052 norm:0.02228706143796444 max memory_allocated 22565.04345703125 
[2025-03-22 01:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.12887568771839142 norm:0.018692128360271454 max memory_allocated 22565.04345703125 
[2025-03-22 01:47:44 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.12795554101467133 norm:0.015453997068107128 max memory_allocated 22565.04345703125 
[2025-03-22 01:48:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.12722404301166534 norm:0.013005687855184078 max memory_allocated 22565.04345703125 
[2025-03-22 01:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.12675079703330994 norm:0.011179342865943909 max memory_allocated 22565.04345703125 
[2025-03-22 01:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.1263587772846222 norm:0.010367766954004765 max memory_allocated 22565.04345703125 
[2025-03-22 01:49:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.12618239223957062 norm:0.009832741692662239 max memory_allocated 22565.04345703125 
[2025-03-22 01:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.12625554203987122 norm:0.009579777717590332 max memory_allocated 22565.04345703125 
[2025-03-22 01:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.12594112753868103 norm:0.00892777182161808 max memory_allocated 22565.04345703125 
[2025-03-22 01:51:09 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 01:51:45 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.22232505679130554 norm:0.021292606368660927 max memory_allocated 22565.04345703125 
[2025-03-22 01:52:17 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.19588813185691833 norm:0.009202374145388603 max memory_allocated 22565.04345703125 
[2025-03-22 01:52:50 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.1750660389661789 norm:0.004051391035318375 max memory_allocated 22565.04345703125 
[2025-03-22 01:53:22 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.1687362939119339 norm:0.003156389808282256 max memory_allocated 22565.04345703125 
[2025-03-22 01:53:54 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.16518856585025787 norm:0.002323034219443798 max memory_allocated 22565.04345703125 
[2025-03-22 01:54:27 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.163483127951622 norm:0.0020503944251686335 max memory_allocated 22565.04345703125 
[2025-03-22 01:54:59 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.16204477846622467 norm:0.0018209338886663318 max memory_allocated 22565.04345703125 
[2025-03-22 01:55:32 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.16087226569652557 norm:0.0016549936262890697 max memory_allocated 22565.04345703125 
[2025-03-22 01:56:04 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.15999622642993927 norm:0.0014989502960816026 max memory_allocated 22565.04345703125 
[2025-03-22 01:56:37 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.15950345993041992 norm:0.0014293128624558449 max memory_allocated 22565.04345703125 
[2025-03-22 01:57:09 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.1595219373703003 norm:0.0013774656690657139 max memory_allocated 22565.04345703125 
[2025-03-22 01:57:42 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.15928630530834198 norm:0.001350542763248086 max memory_allocated 22565.04345703125 
[2025-03-22 01:58:14 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.1592768430709839 norm:0.0013062007492408156 max memory_allocated 22565.04345703125 
[2025-03-22 01:58:47 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.15912604331970215 norm:0.0012662593508139253 max memory_allocated 22565.04345703125 
[2025-03-22 01:59:20 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.15912097692489624 norm:0.001222846214659512 max memory_allocated 22565.04345703125 
[2025-03-22 01:59:52 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.15930896997451782 norm:0.0012349409516900778 max memory_allocated 22565.04345703125 
[2025-03-22 02:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.1599024534225464 norm:0.001297617913223803 max memory_allocated 22565.04345703125 
[2025-03-22 02:00:57 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.15976813435554504 norm:0.0012072835816070437 max memory_allocated 22565.04345703125 
[2025-03-22 02:01:30 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.15944966673851013 norm:0.0011976469540968537 max memory_allocated 22565.04345703125 
[2025-03-22 02:02:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.1595572978258133 norm:0.0011874211486428976 max memory_allocated 22565.04345703125 
[2025-03-22 02:02:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:02:47 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.27933168411254883 norm:0.0372922383248806 max memory_allocated 22565.04345703125 
[2025-03-22 02:03:20 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.23900574445724487 norm:0.01204816997051239 max memory_allocated 22565.04345703125 
[2025-03-22 02:03:52 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.20995521545410156 norm:0.004454689100384712 max memory_allocated 22565.04345703125 
[2025-03-22 02:04:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.20105217397212982 norm:0.002827787771821022 max memory_allocated 22565.04345703125 
[2025-03-22 02:04:57 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.19705989956855774 norm:0.0022621348034590483 max memory_allocated 22565.04345703125 
[2025-03-22 02:05:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.19406357407569885 norm:0.0019179570954293013 max memory_allocated 22565.04345703125 
[2025-03-22 02:06:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.19214311242103577 norm:0.0017960170516744256 max memory_allocated 22565.04345703125 
[2025-03-22 02:06:35 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.19089142978191376 norm:0.001579340430907905 max memory_allocated 22565.04345703125 
[2025-03-22 02:07:07 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.189980149269104 norm:0.001455073244869709 max memory_allocated 22565.04345703125 
[2025-03-22 02:07:40 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.189407080411911 norm:0.0014122927095741034 max memory_allocated 22565.04345703125 
[2025-03-22 02:08:12 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.18879961967468262 norm:0.0013578323414549232 max memory_allocated 22565.04345703125 
[2025-03-22 02:08:45 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.18838916718959808 norm:0.0013012188719585538 max memory_allocated 22565.04345703125 
[2025-03-22 02:09:17 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.18815207481384277 norm:0.0012373985955491662 max memory_allocated 22565.04345703125 
[2025-03-22 02:09:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.18795084953308105 norm:0.0012200942728668451 max memory_allocated 22565.04345703125 
[2025-03-22 02:10:22 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.18783210217952728 norm:0.0012366175651550293 max memory_allocated 22565.04345703125 
[2025-03-22 02:10:55 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.18763922154903412 norm:0.0012480017030611634 max memory_allocated 22565.04345703125 
[2025-03-22 02:11:27 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.18743114173412323 norm:0.0012096595019102097 max memory_allocated 22565.04345703125 
[2025-03-22 02:11:59 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.1873394399881363 norm:0.0012149320682510734 max memory_allocated 22565.04345703125 
[2025-03-22 02:12:32 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.18738126754760742 norm:0.001179629354737699 max memory_allocated 22565.04345703125 
[2025-03-22 02:13:04 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.1873369663953781 norm:0.0011776993051171303 max memory_allocated 22565.04345703125 
[2025-03-22 02:13:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:13:49 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.2998642027378082 norm:0.018899478018283844 max memory_allocated 22565.04345703125 
[2025-03-22 02:14:21 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2691047787666321 norm:0.008728988468647003 max memory_allocated 22565.04345703125 
[2025-03-22 02:14:54 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.2377507984638214 norm:0.003127435687929392 max memory_allocated 22565.04345703125 
[2025-03-22 02:15:26 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.22741049528121948 norm:0.002043326385319233 max memory_allocated 22565.04345703125 
[2025-03-22 02:15:59 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.2227274626493454 norm:0.0017288797535002232 max memory_allocated 22565.04345703125 
[2025-03-22 02:16:31 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.21972179412841797 norm:0.001595387002453208 max memory_allocated 22565.04345703125 
[2025-03-22 02:17:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.21788617968559265 norm:0.0015505950432270765 max memory_allocated 22565.04345703125 
[2025-03-22 02:17:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.21655656397342682 norm:0.0014734980650246143 max memory_allocated 22565.04345703125 
[2025-03-22 02:18:09 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.21562537550926208 norm:0.001441564760170877 max memory_allocated 22565.04345703125 
[2025-03-22 02:18:42 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.21498222649097443 norm:0.0013598272344097495 max memory_allocated 22565.04345703125 
[2025-03-22 02:19:14 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.21440738439559937 norm:0.0018126869108527899 max memory_allocated 22565.04345703125 
[2025-03-22 02:19:47 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.21398885548114777 norm:0.0012194510782137513 max memory_allocated 22565.04345703125 
[2025-03-22 02:20:20 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.21367044746875763 norm:0.001193593256175518 max memory_allocated 22565.04345703125 
[2025-03-22 02:20:52 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.21349658071994781 norm:0.0011591577203944325 max memory_allocated 22565.04345703125 
[2025-03-22 02:21:25 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.21346035599708557 norm:0.001170489238575101 max memory_allocated 22565.04345703125 
[2025-03-22 02:21:58 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.21365657448768616 norm:0.0011838460341095924 max memory_allocated 22565.04345703125 
[2025-03-22 02:22:30 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.21363472938537598 norm:0.0011583270970731974 max memory_allocated 22565.04345703125 
[2025-03-22 02:23:03 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.21348699927330017 norm:0.001171702635474503 max memory_allocated 22565.04345703125 
[2025-03-22 02:23:35 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.21335084736347198 norm:0.001169709488749504 max memory_allocated 22565.04345703125 
[2025-03-22 02:24:08 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.213053360581398 norm:0.0011462726397439837 max memory_allocated 22565.04345703125 
[2025-03-22 02:24:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:24:52 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.3677126169204712 norm:0.03507794439792633 max memory_allocated 22565.04345703125 
[2025-03-22 02:25:24 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.3221985101699829 norm:0.014795207418501377 max memory_allocated 22565.04345703125 
[2025-03-22 02:25:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.2856922149658203 norm:0.0072950697503983974 max memory_allocated 22565.04345703125 
[2025-03-22 02:26:29 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.27142226696014404 norm:0.004736439324915409 max memory_allocated 22565.04345703125 
[2025-03-22 02:27:02 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.26363083720207214 norm:0.0036254588048905134 max memory_allocated 22565.04345703125 
[2025-03-22 02:27:34 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.25911569595336914 norm:0.0030616417061537504 max memory_allocated 22565.04345703125 
[2025-03-22 02:28:06 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.2557389736175537 norm:0.002561833243817091 max memory_allocated 22565.04345703125 
[2025-03-22 02:28:39 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.2536756098270416 norm:0.0022919527254998684 max memory_allocated 22565.04345703125 
[2025-03-22 02:29:11 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.25208672881126404 norm:0.00210520438849926 max memory_allocated 22565.04345703125 
[2025-03-22 02:29:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.25088539719581604 norm:0.0019750655628740788 max memory_allocated 22565.04345703125 
[2025-03-22 02:30:17 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.24981248378753662 norm:0.0018803200218826532 max memory_allocated 22565.04345703125 
[2025-03-22 02:30:49 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.24896258115768433 norm:0.0017635750118643045 max memory_allocated 22565.04345703125 
[2025-03-22 02:31:22 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.24853107333183289 norm:0.001733623561449349 max memory_allocated 22565.04345703125 
[2025-03-22 02:31:54 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.24819640815258026 norm:0.00161712896078825 max memory_allocated 22565.04345703125 
[2025-03-22 02:32:27 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.24761740863323212 norm:0.0015417977701872587 max memory_allocated 22565.04345703125 
[2025-03-22 02:33:00 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.2472103387117386 norm:0.0014961413107812405 max memory_allocated 22565.04345703125 
[2025-03-22 02:33:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.24683113396167755 norm:0.00144405965693295 max memory_allocated 22565.04345703125 
[2025-03-22 02:34:05 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.24667888879776 norm:0.0014105529990047216 max memory_allocated 22565.04345703125 
[2025-03-22 02:34:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.24635641276836395 norm:0.0013917640317231417 max memory_allocated 22565.04345703125 
[2025-03-22 02:35:10 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.24623367190361023 norm:0.0014148493064567447 max memory_allocated 22565.04345703125 
[2025-03-22 02:35:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 02:35:55 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.4028495252132416 norm:0.03680933639407158 max memory_allocated 22565.04345703125 
[2025-03-22 02:36:27 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.344698041677475 norm:0.013870042748749256 max memory_allocated 22565.04345703125 
[2025-03-22 02:37:00 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.30711260437965393 norm:0.0068273902870714664 max memory_allocated 22565.04345703125 
[2025-03-22 02:37:32 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.29356059432029724 norm:0.0045126802287995815 max memory_allocated 22565.04345703125 
[2025-03-22 02:38:05 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.28710266947746277 norm:0.0034940072800964117 max memory_allocated 22565.04345703125 
[2025-03-22 02:38:38 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.28225409984588623 norm:0.0027458355762064457 max memory_allocated 22565.04345703125 
[2025-03-22 02:39:10 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.27902376651763916 norm:0.0024793001357465982 max memory_allocated 22565.04345703125 
[2025-03-22 02:39:43 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.27677565813064575 norm:0.002245152136310935 max memory_allocated 22565.04345703125 
[2025-03-22 02:40:16 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.27524128556251526 norm:0.0021001663990318775 max memory_allocated 22565.04345703125 
[2025-03-22 02:40:48 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.27403607964515686 norm:0.001928364741615951 max memory_allocated 22565.04345703125 
[2025-03-22 02:41:21 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.2730296850204468 norm:0.0018096070270985365 max memory_allocated 22565.04345703125 
[2025-03-22 02:41:53 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.2722136378288269 norm:0.0017236279090866446 max memory_allocated 22565.04345703125 
[2025-03-22 02:42:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.2715710699558258 norm:0.0016299874987453222 max memory_allocated 22565.04345703125 
[2025-03-22 02:42:58 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2710476517677307 norm:0.0016403034096583724 max memory_allocated 22565.04345703125 
[2025-03-22 02:43:31 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.27062857151031494 norm:0.0015328620793297887 max memory_allocated 22565.04345703125 
[2025-03-22 02:44:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.27032172679901123 norm:0.0014827386476099491 max memory_allocated 22565.04345703125 
[2025-03-22 02:44:36 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.2701181173324585 norm:0.0014918969245627522 max memory_allocated 22565.04345703125 
[2025-03-22 02:45:08 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.2700067162513733 norm:0.0014403583481907845 max memory_allocated 22565.04345703125 
[2025-03-22 02:45:41 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.26991063356399536 norm:0.0014617894776165485 max memory_allocated 22565.04345703125 
[2025-03-22 02:46:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.2698894143104553 norm:0.0014727477682754397 max memory_allocated 22565.04345703125 
[2025-03-22 02:46:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 02:46:58 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.39977049827575684 norm:0.025385092943906784 max memory_allocated 22565.04345703125 
[2025-03-22 02:47:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.35694196820259094 norm:0.00969325378537178 max memory_allocated 22565.04345703125 
[2025-03-22 02:48:03 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.3215688169002533 norm:0.004415227100253105 max memory_allocated 22565.04345703125 
[2025-03-22 02:48:35 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.30863961577415466 norm:0.0027916047256439924 max memory_allocated 22565.04345703125 
[2025-03-22 02:49:08 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.30257073044776917 norm:0.0023549527395516634 max memory_allocated 22565.04345703125 
[2025-03-22 02:49:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.2980712056159973 norm:0.0020410611759871244 max memory_allocated 22565.04345703125 
[2025-03-22 02:50:13 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.2951934337615967 norm:0.0018596246372908354 max memory_allocated 22565.04345703125 
[2025-03-22 02:50:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.2928837239742279 norm:0.0016558318166062236 max memory_allocated 22565.04345703125 
[2025-03-22 02:51:18 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.2911965847015381 norm:0.0015681523364037275 max memory_allocated 22565.04345703125 
[2025-03-22 02:51:51 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.2901158630847931 norm:0.0015079171862453222 max memory_allocated 22565.04345703125 
[2025-03-22 02:52:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.28942498564720154 norm:0.0014261100441217422 max memory_allocated 22565.04345703125 
[2025-03-22 02:52:56 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.28879931569099426 norm:0.0013611572794616222 max memory_allocated 22565.04345703125 
[2025-03-22 02:53:29 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.28834566473960876 norm:0.0013337578857317567 max memory_allocated 22565.04345703125 
[2025-03-22 02:54:02 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.2879941165447235 norm:0.001319620292633772 max memory_allocated 22565.04345703125 
[2025-03-22 02:54:34 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.28763478994369507 norm:0.001296686357818544 max memory_allocated 22565.04345703125 
[2025-03-22 02:55:07 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.2873538136482239 norm:0.0012942255707457662 max memory_allocated 22565.04345703125 
[2025-03-22 02:55:40 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.28708934783935547 norm:0.0013059965567663312 max memory_allocated 22565.04345703125 
[2025-03-22 02:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.28686991333961487 norm:0.0012768730521202087 max memory_allocated 22565.04345703125 
[2025-03-22 02:56:45 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.286734402179718 norm:0.0012285370612517 max memory_allocated 22565.04345703125 
[2025-03-22 02:57:17 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.2866816520690918 norm:0.0012386441230773926 max memory_allocated 22565.04345703125 
[2025-03-22 02:57:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 02:58:02 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.44196218252182007 norm:0.039689913392066956 max memory_allocated 22565.04345703125 
[2025-03-22 02:58:34 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.37512093782424927 norm:0.013101745396852493 max memory_allocated 22565.04345703125 
[2025-03-22 02:59:07 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.3353467583656311 norm:0.005324296187609434 max memory_allocated 22565.04345703125 
[2025-03-22 02:59:39 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.32192420959472656 norm:0.003398129716515541 max memory_allocated 22565.04345703125 
[2025-03-22 03:00:12 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.31628432869911194 norm:0.002917506266385317 max memory_allocated 22565.04345703125 
[2025-03-22 03:00:44 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3126264810562134 norm:0.0025997734628617764 max memory_allocated 22565.04345703125 
[2025-03-22 03:01:17 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.3098161518573761 norm:0.002376500517129898 max memory_allocated 22565.04345703125 
[2025-03-22 03:01:49 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.30774468183517456 norm:0.0021555679850280285 max memory_allocated 22565.04345703125 
[2025-03-22 03:02:21 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.306164026260376 norm:0.0019678070675581694 max memory_allocated 22565.04345703125 
[2025-03-22 03:02:54 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3049112856388092 norm:0.0018197509925812483 max memory_allocated 22565.04345703125 
[2025-03-22 03:03:27 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.3039211928844452 norm:0.0016550531145185232 max memory_allocated 22565.04345703125 
[2025-03-22 03:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.3031919598579407 norm:0.0015602732310071588 max memory_allocated 22565.04345703125 
[2025-03-22 03:04:32 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.30254852771759033 norm:0.0014759890036657453 max memory_allocated 22565.04345703125 
[2025-03-22 03:05:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.30203762650489807 norm:0.0013960881624370813 max memory_allocated 22565.04345703125 
[2025-03-22 03:05:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.30171483755111694 norm:0.0013712146319448948 max memory_allocated 22565.04345703125 
[2025-03-22 03:06:10 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.3012622892856598 norm:0.001309349201619625 max memory_allocated 22565.04345703125 
[2025-03-22 03:06:42 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.30090928077697754 norm:0.0012979661114513874 max memory_allocated 22565.04345703125 
[2025-03-22 03:07:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.3006429672241211 norm:0.0013102259254083037 max memory_allocated 22565.04345703125 
[2025-03-22 03:07:48 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.3003707528114319 norm:0.0012809736654162407 max memory_allocated 22565.04345703125 
[2025-03-22 03:08:20 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.300266295671463 norm:0.0012479630531743169 max memory_allocated 22565.04345703125 
[2025-03-22 03:08:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:09:05 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.4195390045642853 norm:0.03098335862159729 max memory_allocated 22565.04345703125 
[2025-03-22 03:09:37 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.3825017511844635 norm:0.014246168546378613 max memory_allocated 22565.04345703125 
[2025-03-22 03:10:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.3477911651134491 norm:0.00638456828892231 max memory_allocated 22565.04345703125 
[2025-03-22 03:10:42 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.33149096369743347 norm:0.002750964602455497 max memory_allocated 22565.04345703125 
[2025-03-22 03:11:15 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.3240974247455597 norm:0.001694546896032989 max memory_allocated 22565.04345703125 
[2025-03-22 03:11:48 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3197300136089325 norm:0.001532950671389699 max memory_allocated 22565.04345703125 
[2025-03-22 03:12:20 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.31663864850997925 norm:0.0014045268762856722 max memory_allocated 22565.04345703125 
[2025-03-22 03:12:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.31453704833984375 norm:0.001337688765488565 max memory_allocated 22565.04345703125 
[2025-03-22 03:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.3127882480621338 norm:0.0012543343473225832 max memory_allocated 22565.04345703125 
[2025-03-22 03:13:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.311515212059021 norm:0.0012185350060462952 max memory_allocated 22565.04345703125 
[2025-03-22 03:14:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.3107430636882782 norm:0.001174726989120245 max memory_allocated 22565.04345703125 
[2025-03-22 03:15:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.31010252237319946 norm:0.0011113357031717896 max memory_allocated 22565.04345703125 
[2025-03-22 03:15:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.30955788493156433 norm:0.0010804085759446025 max memory_allocated 22565.04345703125 
[2025-03-22 03:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3091571033000946 norm:0.0010737128322944045 max memory_allocated 22565.04345703125 
[2025-03-22 03:16:40 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3089331388473511 norm:0.0011032220209017396 max memory_allocated 22565.04345703125 
[2025-03-22 03:17:13 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.3086397349834442 norm:0.0011038633529096842 max memory_allocated 22565.04345703125 
[2025-03-22 03:17:45 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.3083426356315613 norm:0.0010626689763739705 max memory_allocated 22565.04345703125 
[2025-03-22 03:18:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.30810391902923584 norm:0.001040207571350038 max memory_allocated 22565.04345703125 
[2025-03-22 03:18:50 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.30799534916877747 norm:0.0010293469531461596 max memory_allocated 22565.04345703125 
[2025-03-22 03:19:23 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.3078293800354004 norm:0.0009953947737812996 max memory_allocated 22565.04345703125 
[2025-03-22 03:19:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 03:20:07 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.42018306255340576 norm:0.020380796864628792 max memory_allocated 22565.04345703125 
[2025-03-22 03:20:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.3752272427082062 norm:0.008468630723655224 max memory_allocated 22565.04345703125 
[2025-03-22 03:21:13 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.34559860825538635 norm:0.004422151017934084 max memory_allocated 22565.04345703125 
[2025-03-22 03:21:45 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.33244583010673523 norm:0.0024173534475266933 max memory_allocated 22565.04345703125 
[2025-03-22 03:22:18 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.3263654410839081 norm:0.001878645271062851 max memory_allocated 22565.04345703125 
[2025-03-22 03:22:50 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.3225668668746948 norm:0.0016926041571423411 max memory_allocated 22565.04345703125 
[2025-03-22 03:23:23 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.3200569152832031 norm:0.0015703862300142646 max memory_allocated 22565.04345703125 
[2025-03-22 03:23:56 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3182705342769623 norm:0.001423983252607286 max memory_allocated 22565.04345703125 
[2025-03-22 03:24:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.3168594539165497 norm:0.001312033273279667 max memory_allocated 22565.04345703125 
[2025-03-22 03:25:01 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.31586334109306335 norm:0.001234076451510191 max memory_allocated 22565.04345703125 
[2025-03-22 03:25:34 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.3151199519634247 norm:0.0011951667256653309 max memory_allocated 22565.04345703125 
[2025-03-22 03:26:06 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.3144301176071167 norm:0.0011378017952665687 max memory_allocated 22565.04345703125 
[2025-03-22 03:26:39 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3140771985054016 norm:0.0011076022638007998 max memory_allocated 22565.04345703125 
[2025-03-22 03:27:12 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.3136596083641052 norm:0.0010541577357798815 max memory_allocated 22565.04345703125 
[2025-03-22 03:27:44 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3133784830570221 norm:0.0010332440724596381 max memory_allocated 22565.04345703125 
[2025-03-22 03:28:17 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.3130588233470917 norm:0.0010115939658135176 max memory_allocated 22565.04345703125 
[2025-03-22 03:28:50 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.31289076805114746 norm:0.0010061898501589894 max memory_allocated 22565.04345703125 
[2025-03-22 03:29:22 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.31262895464897156 norm:0.000996819930151105 max memory_allocated 22565.04345703125 
[2025-03-22 03:29:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.3124580681324005 norm:0.0009559616446495056 max memory_allocated 22565.04345703125 
[2025-03-22 03:30:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.31224536895751953 norm:0.0009290422312915325 max memory_allocated 22565.04345703125 
[2025-03-22 03:30:37 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 03:31:12 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.4094091057777405 norm:0.012899656780064106 max memory_allocated 22565.04345703125 
[2025-03-22 03:31:44 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.3763394355773926 norm:0.005870100576430559 max memory_allocated 22565.04345703125 
[2025-03-22 03:32:17 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.3500978946685791 norm:0.0031117575708776712 max memory_allocated 22565.04345703125 
[2025-03-22 03:32:49 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.3379251956939697 norm:0.0021070900838822126 max memory_allocated 22565.04345703125 
[2025-03-22 03:33:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.3321394920349121 norm:0.0016074101440608501 max memory_allocated 22565.04345703125 
[2025-03-22 03:33:54 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.3287149667739868 norm:0.0014118640683591366 max memory_allocated 22565.04345703125 
[2025-03-22 03:34:27 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.326125830411911 norm:0.0012383399298414588 max memory_allocated 22565.04345703125 
[2025-03-22 03:34:59 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.32441872358322144 norm:0.0011655696434900165 max memory_allocated 22565.04345703125 
[2025-03-22 03:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.32311299443244934 norm:0.0010971039300784469 max memory_allocated 22565.04345703125 
[2025-03-22 03:36:04 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.32214900851249695 norm:0.0010371513199061155 max memory_allocated 22565.04345703125 
[2025-03-22 03:36:37 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.32145392894744873 norm:0.0009982709307223558 max memory_allocated 22565.04345703125 
[2025-03-22 03:37:09 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.32101303339004517 norm:0.0009927451610565186 max memory_allocated 22565.04345703125 
[2025-03-22 03:37:42 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.320611834526062 norm:0.0009512205724604428 max memory_allocated 22565.04345703125 
[2025-03-22 03:38:14 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.3202345371246338 norm:0.0009307837462984025 max memory_allocated 22565.04345703125 
[2025-03-22 03:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.31996217370033264 norm:0.0009405136224813759 max memory_allocated 22565.04345703125 
[2025-03-22 03:39:20 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.3197593688964844 norm:0.0009193741716444492 max memory_allocated 22565.04345703125 
[2025-03-22 03:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.31945452094078064 norm:0.000886893889401108 max memory_allocated 22565.04345703125 
[2025-03-22 03:40:25 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.31934958696365356 norm:0.0008670854731462896 max memory_allocated 22565.04345703125 
[2025-03-22 03:40:57 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.3192576467990875 norm:0.0008500214316882193 max memory_allocated 22565.04345703125 
[2025-03-22 03:41:30 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.319144606590271 norm:0.0008349194540642202 max memory_allocated 22565.04345703125 
[2025-03-22 03:41:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 03:42:15 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.4423520267009735 norm:0.0446147583425045 max memory_allocated 22565.09033203125 
[2025-03-22 03:42:47 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.3845614194869995 norm:0.015700260177254677 max memory_allocated 22565.09033203125 
[2025-03-22 03:43:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.35177353024482727 norm:0.0074692703783512115 max memory_allocated 22565.09033203125 
[2025-03-22 03:43:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.3392411768436432 norm:0.00430927611887455 max memory_allocated 22565.09033203125 
[2025-03-22 03:44:25 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.33248817920684814 norm:0.0028575898613780737 max memory_allocated 22565.09033203125 
[2025-03-22 03:44:58 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3283650875091553 norm:0.002393079688772559 max memory_allocated 22565.09033203125 
[2025-03-22 03:45:30 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3253574073314667 norm:0.002094732131808996 max memory_allocated 22565.09033203125 
[2025-03-22 03:46:03 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.32317066192626953 norm:0.0019189980812370777 max memory_allocated 22565.09033203125 
[2025-03-22 03:46:35 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.3214731216430664 norm:0.001644772826693952 max memory_allocated 22565.09033203125 
[2025-03-22 03:47:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.32011520862579346 norm:0.0014617160195484757 max memory_allocated 22565.09033203125 
[2025-03-22 03:47:40 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3190810978412628 norm:0.001391274738125503 max memory_allocated 22565.09033203125 
[2025-03-22 03:48:13 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.318242609500885 norm:0.0013074452290311456 max memory_allocated 22565.09033203125 
[2025-03-22 03:48:46 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.31764861941337585 norm:0.0012563790660351515 max memory_allocated 22565.09033203125 
[2025-03-22 03:49:18 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3172396123409271 norm:0.0012138595338910818 max memory_allocated 22565.09033203125 
[2025-03-22 03:49:51 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.31674933433532715 norm:0.0011611831141635776 max memory_allocated 22565.09033203125 
[2025-03-22 03:50:23 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.31639763712882996 norm:0.001128869829699397 max memory_allocated 22565.09033203125 
[2025-03-22 03:50:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.3161126375198364 norm:0.001106594572775066 max memory_allocated 22565.09033203125 
[2025-03-22 03:51:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.31592658162117004 norm:0.0010808815713971853 max memory_allocated 22565.09033203125 
[2025-03-22 03:52:00 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.31567254662513733 norm:0.0010579668451100588 max memory_allocated 22565.09033203125 
[2025-03-22 03:52:33 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.315494179725647 norm:0.0010373902041465044 max memory_allocated 22565.09033203125 
[2025-03-22 03:52:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 03:53:17 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.4042956531047821 norm:0.015195844694972038 max memory_allocated 22565.26220703125 
[2025-03-22 03:53:50 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.37073442339897156 norm:0.006347264163196087 max memory_allocated 22565.26220703125 
[2025-03-22 03:54:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.34675610065460205 norm:0.003010977292433381 max memory_allocated 22565.26220703125 
[2025-03-22 03:54:55 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.3370775282382965 norm:0.0017745174700394273 max memory_allocated 22565.26220703125 
[2025-03-22 03:55:27 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.33256667852401733 norm:0.0014563723234459758 max memory_allocated 22565.26220703125 
[2025-03-22 03:56:00 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3296118974685669 norm:0.001254111179150641 max memory_allocated 22565.26220703125 
[2025-03-22 03:56:32 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.3274269700050354 norm:0.00114660884719342 max memory_allocated 22565.26220703125 
[2025-03-22 03:57:05 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.32586365938186646 norm:0.0010584580013528466 max memory_allocated 22565.26220703125 
[2025-03-22 03:57:37 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.32470351457595825 norm:0.001008856575936079 max memory_allocated 22565.26220703125 
[2025-03-22 03:58:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.32377010583877563 norm:0.000979350646957755 max memory_allocated 22565.26220703125 
[2025-03-22 03:58:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.3231031894683838 norm:0.0009595053852535784 max memory_allocated 22565.26220703125 
[2025-03-22 03:59:15 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.3226875960826874 norm:0.0009361814009025693 max memory_allocated 22565.26220703125 
[2025-03-22 03:59:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.32221969962120056 norm:0.0009019180433824658 max memory_allocated 22565.26220703125 
[2025-03-22 04:00:21 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.3218816816806793 norm:0.0008921492844820023 max memory_allocated 22565.26220703125 
[2025-03-22 04:00:53 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.32158344984054565 norm:0.000887244357727468 max memory_allocated 22565.26220703125 
[2025-03-22 04:01:26 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.32129979133605957 norm:0.0008753766887821257 max memory_allocated 22565.26220703125 
[2025-03-22 04:01:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.32099810242652893 norm:0.0008605442708358169 max memory_allocated 22565.26220703125 
[2025-03-22 04:02:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.3208063840866089 norm:0.0008502614800818264 max memory_allocated 22565.26220703125 
[2025-03-22 04:03:04 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.3205564618110657 norm:0.0008241515606641769 max memory_allocated 22565.26220703125 
[2025-03-22 04:03:36 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.32037943601608276 norm:0.0008083495777100325 max memory_allocated 22565.26220703125 
[2025-03-22 04:03:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 04:04:20 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.4469025135040283 norm:0.04028378427028656 max memory_allocated 22565.43408203125 
[2025-03-22 04:04:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.38993680477142334 norm:0.013553278520703316 max memory_allocated 22565.43408203125 
[2025-03-22 04:05:25 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.35550975799560547 norm:0.005815207026898861 max memory_allocated 22565.43408203125 
[2025-03-22 04:05:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.341826856136322 norm:0.002884628949686885 max memory_allocated 22565.43408203125 
[2025-03-22 04:06:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.3361792266368866 norm:0.00195501372218132 max memory_allocated 22565.43408203125 
[2025-03-22 04:07:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.332660436630249 norm:0.0016495862510055304 max memory_allocated 22565.43408203125 
[2025-03-22 04:07:35 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.33029845356941223 norm:0.0015247963601723313 max memory_allocated 22565.43408203125 
[2025-03-22 04:08:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.3282586336135864 norm:0.0013973740860819817 max memory_allocated 22565.43408203125 
[2025-03-22 04:08:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.3267808258533478 norm:0.0013438003370538354 max memory_allocated 22565.43408203125 
[2025-03-22 04:09:13 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.32566767930984497 norm:0.0012263054959475994 max memory_allocated 22565.43408203125 
[2025-03-22 04:09:45 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.32477253675460815 norm:0.0011459095403552055 max memory_allocated 22565.43408203125 
[2025-03-22 04:10:18 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.32406941056251526 norm:0.0010993327014148235 max memory_allocated 22565.43408203125 
[2025-03-22 04:10:50 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.3234753906726837 norm:0.0010553523898124695 max memory_allocated 22565.43408203125 
[2025-03-22 04:11:23 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.32309627532958984 norm:0.0010145306587219238 max memory_allocated 22565.43408203125 
[2025-03-22 04:11:55 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.32260438799858093 norm:0.0009628859697841108 max memory_allocated 22565.43408203125 
[2025-03-22 04:12:28 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.3222901523113251 norm:0.0009343822603113949 max memory_allocated 22565.43408203125 
[2025-03-22 04:13:01 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.32202133536338806 norm:0.0008868691511452198 max memory_allocated 22565.43408203125 
[2025-03-22 04:13:33 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.32184237241744995 norm:0.0008681922918185592 max memory_allocated 22565.43408203125 
[2025-03-22 04:14:06 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.321652889251709 norm:0.0008698316523805261 max memory_allocated 22565.43408203125 
[2025-03-22 04:14:39 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.3215583264827728 norm:0.0008609003853052855 max memory_allocated 22565.43408203125 
[2025-03-22 04:14:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 04:15:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.45822271704673767 norm:0.04612444341182709 max memory_allocated 22565.60595703125 
[2025-03-22 04:15:56 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.4067443609237671 norm:0.018065746873617172 max memory_allocated 22565.60595703125 
[2025-03-22 04:16:28 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.3684222102165222 norm:0.007422980852425098 max memory_allocated 22565.60595703125 
[2025-03-22 04:17:01 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.3526158034801483 norm:0.0036406097933650017 max memory_allocated 22565.60595703125 
[2025-03-22 04:17:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.34644144773483276 norm:0.002437242539599538 max memory_allocated 22565.60595703125 
[2025-03-22 04:18:06 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.3430604934692383 norm:0.002044132212176919 max memory_allocated 22565.60595703125 
[2025-03-22 04:18:39 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.34063905477523804 norm:0.0018653280567377806 max memory_allocated 22565.60595703125 
[2025-03-22 04:19:11 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.3383924663066864 norm:0.0016289417399093509 max memory_allocated 22565.60595703125 
[2025-03-22 04:19:44 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.3368135988712311 norm:0.0015444664750248194 max memory_allocated 22565.60595703125 
[2025-03-22 04:20:16 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.3354082405567169 norm:0.0014678420266136527 max memory_allocated 22565.60595703125 
[2025-03-22 04:20:49 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.33429092168807983 norm:0.0014337783213704824 max memory_allocated 22565.60595703125 
[2025-03-22 04:21:21 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.3333805501461029 norm:0.0014059697277843952 max memory_allocated 22565.60595703125 
[2025-03-22 04:21:54 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.3327772915363312 norm:0.0013519166968762875 max memory_allocated 22565.60595703125 
[2025-03-22 04:22:26 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.33225202560424805 norm:0.0012985275825485587 max memory_allocated 22565.60595703125 
[2025-03-22 04:22:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.33174413442611694 norm:0.0012316573411226273 max memory_allocated 22565.60595703125 
[2025-03-22 04:23:31 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.33122095465660095 norm:0.0011699434835463762 max memory_allocated 22565.60595703125 
[2025-03-22 04:24:03 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.3309647738933563 norm:0.0011520643020048738 max memory_allocated 22565.60595703125 
[2025-03-22 04:24:36 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.33074530959129333 norm:0.0011245489586144686 max memory_allocated 22565.60595703125 
[2025-03-22 04:25:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.3304826021194458 norm:0.001089700497686863 max memory_allocated 22565.60595703125 
[2025-03-22 04:25:41 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.3302927613258362 norm:0.0010624248534440994 max memory_allocated 22565.60595703125 
[2025-03-22 04:25:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 04:26:25 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.4311932921409607 norm:0.03468933328986168 max memory_allocated 22565.77783203125 
[2025-03-22 04:26:58 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.39303064346313477 norm:0.012558636255562305 max memory_allocated 22565.77783203125 
[2025-03-22 04:27:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.36541709303855896 norm:0.004880501423031092 max memory_allocated 22565.77783203125 
[2025-03-22 04:28:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.35518038272857666 norm:0.0022649846505373716 max memory_allocated 22565.77783203125 
[2025-03-22 04:28:35 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.35158056020736694 norm:0.0018437807448208332 max memory_allocated 22565.77783203125 
[2025-03-22 04:29:08 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.3491641581058502 norm:0.0015994965797290206 max memory_allocated 22565.77783203125 
[2025-03-22 04:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.347306489944458 norm:0.0014805055689066648 max memory_allocated 22565.77783203125 
[2025-03-22 04:30:13 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.34582793712615967 norm:0.001424427144229412 max memory_allocated 22565.77783203125 
[2025-03-22 04:30:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.3446832299232483 norm:0.0013845054199919105 max memory_allocated 22565.77783203125 
[2025-03-22 04:31:18 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.3436644673347473 norm:0.0013327833730727434 max memory_allocated 22565.77783203125 
[2025-03-22 04:31:51 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.34286704659461975 norm:0.0012855923268944025 max memory_allocated 22565.77783203125 
[2025-03-22 04:32:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.3422785699367523 norm:0.001269154017791152 max memory_allocated 22565.77783203125 
[2025-03-22 04:32:56 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.34170812368392944 norm:0.0012135934084653854 max memory_allocated 22565.77783203125 
[2025-03-22 04:33:29 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.34129825234413147 norm:0.0011620491277426481 max memory_allocated 22565.77783203125 
[2025-03-22 04:34:01 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.34091782569885254 norm:0.001131228287704289 max memory_allocated 22565.77783203125 
[2025-03-22 04:34:34 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.3406153917312622 norm:0.0010983715765178204 max memory_allocated 22565.77783203125 
[2025-03-22 04:35:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.340268075466156 norm:0.001057895366102457 max memory_allocated 22565.77783203125 
[2025-03-22 04:35:39 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.3400616943836212 norm:0.0010308964410796762 max memory_allocated 22565.77783203125 
[2025-03-22 04:36:11 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.33991241455078125 norm:0.0010251584462821484 max memory_allocated 22565.77783203125 
[2025-03-22 04:36:44 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.33966290950775146 norm:0.0009940023301169276 max memory_allocated 22565.77783203125 
[2025-03-22 04:36:53 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 04:37:28 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.4533405303955078 norm:0.0455813929438591 max memory_allocated 22565.94970703125 
[2025-03-22 04:38:00 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.4212198257446289 norm:0.017831776291131973 max memory_allocated 22565.94970703125 
[2025-03-22 04:38:33 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.3935030996799469 norm:0.007664091419428587 max memory_allocated 22565.94970703125 
[2025-03-22 04:39:05 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.3799079358577728 norm:0.003647061763331294 max memory_allocated 22565.94970703125 
[2025-03-22 04:39:38 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.37433627247810364 norm:0.0021832247730344534 max memory_allocated 22565.94970703125 
[2025-03-22 04:40:10 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.3713586926460266 norm:0.0015017199330031872 max memory_allocated 22565.94970703125 
[2025-03-22 04:40:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.3694823384284973 norm:0.0014049812452867627 max memory_allocated 22565.94970703125 
[2025-03-22 04:41:15 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.3680031895637512 norm:0.0013592814793810248 max memory_allocated 22565.94970703125 
[2025-03-22 04:41:48 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.366727352142334 norm:0.0013243128778412938 max memory_allocated 22565.94970703125 
[2025-03-22 04:42:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.3656942844390869 norm:0.001290575135499239 max memory_allocated 22565.94970703125 
[2025-03-22 04:42:53 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.364940881729126 norm:0.0012688831193372607 max memory_allocated 22565.94970703125 
[2025-03-22 04:43:25 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.36436617374420166 norm:0.0012121658073738217 max memory_allocated 22565.94970703125 
[2025-03-22 04:43:58 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.36388951539993286 norm:0.0011560821440070868 max memory_allocated 22565.94970703125 
[2025-03-22 04:44:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.3634984493255615 norm:0.0011263071792200208 max memory_allocated 22565.94970703125 
[2025-03-22 04:45:03 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.36314910650253296 norm:0.0010923934169113636 max memory_allocated 22565.94970703125 
[2025-03-22 04:45:36 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.3627987205982208 norm:0.0010899960761889815 max memory_allocated 22565.94970703125 
[2025-03-22 04:46:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.36251944303512573 norm:0.0010612780461087823 max memory_allocated 22565.94970703125 
[2025-03-22 04:46:41 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.3622620105743408 norm:0.0010348984505981207 max memory_allocated 22565.94970703125 
[2025-03-22 04:47:14 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.3621390461921692 norm:0.001024330616928637 max memory_allocated 22565.94970703125 
[2025-03-22 04:47:46 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.36222925782203674 norm:0.001036702306009829 max memory_allocated 22565.94970703125 
[2025-03-22 04:47:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 04:48:31 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.4601564407348633 norm:0.0348968431353569 max memory_allocated 22566.12158203125 
[2025-03-22 04:49:03 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.43704336881637573 norm:0.01620582863688469 max memory_allocated 22566.12158203125 
[2025-03-22 04:49:36 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.4126242399215698 norm:0.0067319986410439014 max memory_allocated 22566.12158203125 
[2025-03-22 04:50:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.40203550457954407 norm:0.0026387805119156837 max memory_allocated 22566.12158203125 
[2025-03-22 04:50:41 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.39842259883880615 norm:0.0011973072541877627 max memory_allocated 22566.12158203125 
[2025-03-22 04:51:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.3964606821537018 norm:0.00109708437230438 max memory_allocated 22566.12158203125 
[2025-03-22 04:51:46 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.39487266540527344 norm:0.001070559723302722 max memory_allocated 22566.12158203125 
[2025-03-22 04:52:18 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.3935736417770386 norm:0.0010323061142116785 max memory_allocated 22566.12158203125 
[2025-03-22 04:52:51 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.39244794845581055 norm:0.0009743880946189165 max memory_allocated 22566.12158203125 
[2025-03-22 04:53:23 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.3916543424129486 norm:0.0009634309681132436 max memory_allocated 22566.12158203125 
[2025-03-22 04:53:55 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.39099007844924927 norm:0.0009329747990705073 max memory_allocated 22566.12158203125 
[2025-03-22 04:54:28 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.39044028520584106 norm:0.0008951373165473342 max memory_allocated 22566.12158203125 
[2025-03-22 04:55:00 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.3900322914123535 norm:0.0008826337871141732 max memory_allocated 22566.12158203125 
[2025-03-22 04:55:33 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.3896637558937073 norm:0.0008620053995400667 max memory_allocated 22566.12158203125 
[2025-03-22 04:56:05 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.38938143849372864 norm:0.000841227185446769 max memory_allocated 22566.12158203125 
[2025-03-22 04:56:38 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.3890819251537323 norm:0.000839662563521415 max memory_allocated 22566.12158203125 
[2025-03-22 04:57:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.3888556659221649 norm:0.0008491993648931384 max memory_allocated 22566.12158203125 
[2025-03-22 04:57:43 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.38871583342552185 norm:0.0008283882052637637 max memory_allocated 22566.12158203125 
[2025-03-22 04:58:16 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.38845187425613403 norm:0.0007986628916114569 max memory_allocated 22566.12158203125 
[2025-03-22 04:58:48 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.3882417678833008 norm:0.0007800004095770419 max memory_allocated 22566.12158203125 
[2025-03-22 04:58:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 04:59:33 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.4992574155330658 norm:0.024543553590774536 max memory_allocated 22566.29345703125 
[2025-03-22 05:00:05 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.47671574354171753 norm:0.012415770441293716 max memory_allocated 22566.29345703125 
[2025-03-22 05:00:38 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.454036682844162 norm:0.005679653026163578 max memory_allocated 22566.29345703125 
[2025-03-22 05:01:10 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.44378000497817993 norm:0.0032873577438294888 max memory_allocated 22566.29345703125 
[2025-03-22 05:01:43 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.440047949552536 norm:0.0025734545197337866 max memory_allocated 22566.29345703125 
[2025-03-22 05:02:16 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.4369334876537323 norm:0.001581186312250793 max memory_allocated 22566.29345703125 
[2025-03-22 05:02:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.4349093437194824 norm:0.0014293028507381678 max memory_allocated 22566.29345703125 
[2025-03-22 05:03:21 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.43333443999290466 norm:0.0013328425120562315 max memory_allocated 22566.29345703125 
[2025-03-22 05:03:53 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.43214938044548035 norm:0.001254891511052847 max memory_allocated 22566.29345703125 
[2025-03-22 05:04:26 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.43111366033554077 norm:0.0012013970408588648 max memory_allocated 22566.29345703125 
[2025-03-22 05:04:58 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.43033677339553833 norm:0.001168679678812623 max memory_allocated 22566.29345703125 
[2025-03-22 05:05:31 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.42974475026130676 norm:0.0011524648871272802 max memory_allocated 22566.29345703125 
[2025-03-22 05:06:03 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.4292004406452179 norm:0.0011448923032730818 max memory_allocated 22566.29345703125 
[2025-03-22 05:06:36 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.42875513434410095 norm:0.0011068705935031176 max memory_allocated 22566.29345703125 
[2025-03-22 05:07:08 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.4283665716648102 norm:0.0010788016952574253 max memory_allocated 22566.29345703125 
[2025-03-22 05:07:41 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.4280310273170471 norm:0.0010768355568870902 max memory_allocated 22566.29345703125 
[2025-03-22 05:08:13 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.4277322590351105 norm:0.0010366361821070313 max memory_allocated 22566.29345703125 
[2025-03-22 05:08:46 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.42741096019744873 norm:0.0010170122841373086 max memory_allocated 22566.29345703125 
[2025-03-22 05:09:18 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.4271436333656311 norm:0.0009738143999129534 max memory_allocated 22566.29345703125 
[2025-03-22 05:09:51 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.4269656240940094 norm:0.0009598139440640807 max memory_allocated 22566.29345703125 
[2025-03-22 05:10:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 05:10:36 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.5179100632667542 norm:0.01685863919556141 max memory_allocated 22566.46533203125 
[2025-03-22 05:11:09 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.5015518665313721 norm:0.007554037030786276 max memory_allocated 22566.46533203125 
[2025-03-22 05:11:41 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.4876474440097809 norm:0.0037313727661967278 max memory_allocated 22566.46533203125 
[2025-03-22 05:12:14 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.4806026518344879 norm:0.001493487972766161 max memory_allocated 22566.46533203125 
[2025-03-22 05:12:46 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.4779529869556427 norm:0.0010389832314103842 max memory_allocated 22566.46533203125 
[2025-03-22 05:13:19 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.4762033522129059 norm:0.0009306118590757251 max memory_allocated 22566.46533203125 
[2025-03-22 05:13:51 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.4746154844760895 norm:0.0008883207337930799 max memory_allocated 22566.46533203125 
[2025-03-22 05:14:24 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.4733147621154785 norm:0.0008480490650981665 max memory_allocated 22566.46533203125 
[2025-03-22 05:14:57 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.47237372398376465 norm:0.0008266849908977747 max memory_allocated 22566.46533203125 
[2025-03-22 05:15:29 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.47163811326026917 norm:0.000799182802438736 max memory_allocated 22566.46533203125 
[2025-03-22 05:16:02 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.47114506363868713 norm:0.0007875866140238941 max memory_allocated 22566.46533203125 
[2025-03-22 05:16:35 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.4706878662109375 norm:0.0007803703774698079 max memory_allocated 22566.46533203125 
[2025-03-22 05:17:07 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.470228910446167 norm:0.0007550718146376312 max memory_allocated 22566.46533203125 
[2025-03-22 05:17:40 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.46987852454185486 norm:0.0007488311384804547 max memory_allocated 22566.46533203125 
[2025-03-22 05:18:12 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.4696809947490692 norm:0.0007475796737708151 max memory_allocated 22566.46533203125 
[2025-03-22 05:18:45 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.4695775806903839 norm:0.0007524005486629903 max memory_allocated 22566.46533203125 
[2025-03-22 05:19:18 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.46939805150032043 norm:0.0007441169582307339 max memory_allocated 22566.46533203125 
[2025-03-22 05:19:50 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.4692327380180359 norm:0.0007316006231121719 max memory_allocated 22566.46533203125 
[2025-03-22 05:20:23 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.46913936734199524 norm:0.000729232095181942 max memory_allocated 22566.46533203125 
[2025-03-22 05:20:55 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.4690142273902893 norm:0.0007214234210550785 max memory_allocated 22566.46533203125 
[2025-03-22 05:21:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 05:21:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:0.5855693817138672 norm:0.01688198745250702 max memory_allocated 22566.63720703125 
[2025-03-22 05:22:12 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:0.5634991526603699 norm:0.005725383758544922 max memory_allocated 22566.63720703125 
[2025-03-22 05:22:44 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:0.5490845441818237 norm:0.003227976616472006 max memory_allocated 22566.63720703125 
[2025-03-22 05:23:17 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:0.5415490865707397 norm:0.001511620357632637 max memory_allocated 22566.63720703125 
[2025-03-22 05:23:49 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:0.5387502908706665 norm:0.0012485241750255227 max memory_allocated 22566.63720703125 
[2025-03-22 05:24:22 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:0.5370157957077026 norm:0.001171694602817297 max memory_allocated 22566.63720703125 
[2025-03-22 05:24:54 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:0.5355109572410583 norm:0.0010887783719226718 max memory_allocated 22566.63720703125 
[2025-03-22 05:25:26 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:0.5342510938644409 norm:0.0010300622088834643 max memory_allocated 22566.63720703125 
[2025-03-22 05:25:59 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:0.533357560634613 norm:0.001090587000362575 max memory_allocated 22566.63720703125 
[2025-03-22 05:26:31 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:0.5325986742973328 norm:0.001004640362225473 max memory_allocated 22566.63720703125 
[2025-03-22 05:27:04 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.531977117061615 norm:0.0009788315510377288 max memory_allocated 22566.63720703125 
[2025-03-22 05:27:36 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.531507134437561 norm:0.0009491330129094422 max memory_allocated 22566.63720703125 
[2025-03-22 05:28:09 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.5308340787887573 norm:0.0009479519794695079 max memory_allocated 22566.63720703125 
[2025-03-22 05:28:42 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.5305163264274597 norm:0.000921626400668174 max memory_allocated 22566.63720703125 
[2025-03-22 05:29:14 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.5304109454154968 norm:0.000900563842151314 max memory_allocated 22566.63720703125 
[2025-03-22 05:29:47 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.5301889181137085 norm:0.0008770230342634022 max memory_allocated 22566.63720703125 
[2025-03-22 05:30:19 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.5300939083099365 norm:0.0008647926733829081 max memory_allocated 22566.63720703125 
[2025-03-22 05:30:52 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.5298800468444824 norm:0.0008404264226555824 max memory_allocated 22566.63720703125 
[2025-03-22 05:31:25 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.5297094583511353 norm:0.0008485910948365927 max memory_allocated 22566.63720703125 
[2025-03-22 05:31:57 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.529437780380249 norm:0.0008203551406040788 max memory_allocated 22566.63720703125 
[2025-03-22 05:32:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 05:32:42 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:0.6332621574401855 norm:0.007964256219565868 max memory_allocated 22566.80908203125 
[2025-03-22 05:33:15 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:0.6191999912261963 norm:0.0038919823709875345 max memory_allocated 22566.80908203125 
[2025-03-22 05:33:47 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:0.6056356430053711 norm:0.002187937032431364 max memory_allocated 22566.80908203125 
[2025-03-22 05:34:20 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:0.6002891659736633 norm:0.0013235185761004686 max memory_allocated 22566.80908203125 
[2025-03-22 05:34:53 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:0.597650945186615 norm:0.0010179359233006835 max memory_allocated 22566.80908203125 
[2025-03-22 05:35:25 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:0.5956679582595825 norm:0.000841215078253299 max memory_allocated 22566.80908203125 
[2025-03-22 05:35:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:0.5940792560577393 norm:0.0007815257413312793 max memory_allocated 22566.80908203125 
[2025-03-22 05:36:30 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:0.5929226279258728 norm:0.0007586975116282701 max memory_allocated 22566.80908203125 
[2025-03-22 05:37:03 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:0.592070460319519 norm:0.0007277902332134545 max memory_allocated 22566.80908203125 
[2025-03-22 05:37:35 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:0.5913385152816772 norm:0.0007124958210624754 max memory_allocated 22566.80908203125 
[2025-03-22 05:38:08 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:0.5908514857292175 norm:0.0006986665539443493 max memory_allocated 22566.80908203125 
[2025-03-22 05:38:40 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:0.5905329585075378 norm:0.0006963774794712663 max memory_allocated 22566.80908203125 
[2025-03-22 05:39:13 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:0.590216875076294 norm:0.000684116268530488 max memory_allocated 22566.80908203125 
[2025-03-22 05:39:45 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:0.5899611711502075 norm:0.0006735745701007545 max memory_allocated 22566.80908203125 
[2025-03-22 05:40:18 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:0.5897126197814941 norm:0.0006726773572154343 max memory_allocated 22566.80908203125 
[2025-03-22 05:40:50 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:0.589520275592804 norm:0.0006580032641068101 max memory_allocated 22566.80908203125 
[2025-03-22 05:41:23 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:0.5893867015838623 norm:0.0006526858196593821 max memory_allocated 22566.80908203125 
[2025-03-22 05:41:55 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:0.5892290472984314 norm:0.0006476452108472586 max memory_allocated 22566.80908203125 
[2025-03-22 05:42:28 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:0.5891212224960327 norm:0.0006391886272467673 max memory_allocated 22566.80908203125 
[2025-03-22 05:43:00 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:0.5890160799026489 norm:0.0006373657379299402 max memory_allocated 22566.80908203125 
[2025-03-22 05:43:10 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 05:43:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:0.7095579504966736 norm:0.013315614312887192 max memory_allocated 22566.98095703125 
[2025-03-22 05:44:18 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:0.6919306516647339 norm:0.005680350586771965 max memory_allocated 22566.98095703125 
[2025-03-22 05:44:50 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:0.676313579082489 norm:0.0029244113247841597 max memory_allocated 22566.98095703125 
[2025-03-22 05:45:23 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:0.6692774295806885 norm:0.0013858057791367173 max memory_allocated 22566.98095703125 
[2025-03-22 05:45:55 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:0.6663168668746948 norm:0.0008753369329497218 max memory_allocated 22566.98095703125 
[2025-03-22 05:46:28 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:0.6641690731048584 norm:0.0008256894652731717 max memory_allocated 22566.98095703125 
[2025-03-22 05:47:00 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:0.6622852087020874 norm:0.0008070921176113188 max memory_allocated 22566.98095703125 
[2025-03-22 05:47:33 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:0.6609221696853638 norm:0.0007901425124146044 max memory_allocated 22566.98095703125 
[2025-03-22 05:48:06 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:0.6599177122116089 norm:0.000778616638854146 max memory_allocated 22566.98095703125 
[2025-03-22 05:48:38 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:0.6592013835906982 norm:0.0007743124151602387 max memory_allocated 22566.98095703125 
[2025-03-22 05:49:11 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:0.6586617827415466 norm:0.0007707225740887225 max memory_allocated 22566.98095703125 
[2025-03-22 05:49:44 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:0.6582024693489075 norm:0.0007611280307173729 max memory_allocated 22566.98095703125 
[2025-03-22 05:50:16 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:0.6579130291938782 norm:0.0007614536443725228 max memory_allocated 22566.98095703125 
[2025-03-22 05:50:49 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:0.6576414108276367 norm:0.0007501664222218096 max memory_allocated 22566.98095703125 
[2025-03-22 05:51:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:0.6574203968048096 norm:0.0007436918676830828 max memory_allocated 22566.98095703125 
[2025-03-22 05:51:54 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:0.6571676731109619 norm:0.0007409456884488463 max memory_allocated 22566.98095703125 
[2025-03-22 05:52:27 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:0.6569945812225342 norm:0.0007307483465410769 max memory_allocated 22566.98095703125 
[2025-03-22 05:52:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:0.6568273305892944 norm:0.0007356920978054404 max memory_allocated 22566.98095703125 
[2025-03-22 05:53:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:0.6566480398178101 norm:0.000736582325771451 max memory_allocated 22566.98095703125 
[2025-03-22 05:54:04 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:0.6564497947692871 norm:0.0007367711514234543 max memory_allocated 22566.98095703125 
[2025-03-22 05:54:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 05:54:48 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:0.799455463886261 norm:0.023440474644303322 max memory_allocated 22567.15283203125 
[2025-03-22 05:55:21 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:0.7841657400131226 norm:0.014283274300396442 max memory_allocated 22567.15283203125 
[2025-03-22 05:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:0.7671018242835999 norm:0.008943462744355202 max memory_allocated 22567.15283203125 
[2025-03-22 05:56:25 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:0.7600026726722717 norm:0.00600847601890564 max memory_allocated 22567.15283203125 
[2025-03-22 05:56:58 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:0.7549625635147095 norm:0.003961695823818445 max memory_allocated 22567.15283203125 
[2025-03-22 05:57:30 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:0.7515894174575806 norm:0.0037844032049179077 max memory_allocated 22567.15283203125 
[2025-03-22 05:58:03 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:0.7497338652610779 norm:0.003540567820891738 max memory_allocated 22567.15283203125 
[2025-03-22 05:58:36 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:0.7469962239265442 norm:0.002700698794797063 max memory_allocated 22567.15283203125 
[2025-03-22 05:59:08 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:0.7464650273323059 norm:0.0028793879318982363 max memory_allocated 22567.15283203125 
[2025-03-22 05:59:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:0.7455713748931885 norm:0.0024230745621025562 max memory_allocated 22567.15283203125 
[2025-03-22 06:00:13 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:0.7437517046928406 norm:0.0020103631541132927 max memory_allocated 22567.15283203125 
[2025-03-22 06:00:46 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:0.7442933917045593 norm:0.0022499803453683853 max memory_allocated 22567.15283203125 
[2025-03-22 06:01:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:0.7427058219909668 norm:0.0016098270425572991 max memory_allocated 22567.15283203125 
[2025-03-22 06:01:51 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:0.7426820397377014 norm:0.0019117542542517185 max memory_allocated 22567.15283203125 
[2025-03-22 06:02:24 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:0.7420458197593689 norm:0.0014808253617957234 max memory_allocated 22567.15283203125 
[2025-03-22 06:02:56 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:0.7415485382080078 norm:0.0015822206623852253 max memory_allocated 22567.15283203125 
[2025-03-22 06:03:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:0.7415270209312439 norm:0.0014378372579813004 max memory_allocated 22567.15283203125 
[2025-03-22 06:04:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:0.7404807209968567 norm:0.0012894528917968273 max memory_allocated 22567.15283203125 
[2025-03-22 06:04:34 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:0.7407771348953247 norm:0.0013103676028549671 max memory_allocated 22567.15283203125 
[2025-03-22 06:05:07 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:0.739820659160614 norm:0.0011799018830060959 max memory_allocated 22567.15283203125 
[2025-03-22 06:05:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 06:05:51 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:0.8896236419677734 norm:0.009952562861144543 max memory_allocated 22567.32470703125 
[2025-03-22 06:06:24 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:0.8705174326896667 norm:0.004758473951369524 max memory_allocated 22567.32470703125 
[2025-03-22 06:06:56 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:0.8512948751449585 norm:0.002093537477776408 max memory_allocated 22567.32470703125 
[2025-03-22 06:07:29 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:0.8435564637184143 norm:0.0012981740292161703 max memory_allocated 22567.32470703125 
[2025-03-22 06:08:01 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:0.8400816917419434 norm:0.0010600890964269638 max memory_allocated 22567.32470703125 
[2025-03-22 06:08:34 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:0.8373501896858215 norm:0.0009428581688553095 max memory_allocated 22567.32470703125 
[2025-03-22 06:09:06 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:0.8351758718490601 norm:0.0008949986076913774 max memory_allocated 22567.32470703125 
[2025-03-22 06:09:39 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:0.833336353302002 norm:0.0008603059104643762 max memory_allocated 22567.32470703125 
[2025-03-22 06:10:11 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:0.8320337533950806 norm:0.0008269724785350263 max memory_allocated 22567.32470703125 
[2025-03-22 06:10:44 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:0.8312090039253235 norm:0.0008186884224414825 max memory_allocated 22567.32470703125 
[2025-03-22 06:11:16 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:0.830508828163147 norm:0.0007863823557272553 max memory_allocated 22567.32470703125 
[2025-03-22 06:11:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:0.8299309611320496 norm:0.0007728434284217656 max memory_allocated 22567.32470703125 
[2025-03-22 06:12:21 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:0.8294416069984436 norm:0.0007597037474624813 max memory_allocated 22567.32470703125 
[2025-03-22 06:12:54 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:0.8290255069732666 norm:0.0007555875345133245 max memory_allocated 22567.32470703125 
[2025-03-22 06:13:26 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:0.8286843299865723 norm:0.0007427997188642621 max memory_allocated 22567.32470703125 
[2025-03-22 06:13:59 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:0.8283868432044983 norm:0.0007499172352254391 max memory_allocated 22567.32470703125 
[2025-03-22 06:14:31 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:0.8280728459358215 norm:0.0007400073809549212 max memory_allocated 22567.32470703125 
[2025-03-22 06:15:04 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:0.8278064131736755 norm:0.000738889561034739 max memory_allocated 22567.32470703125 
[2025-03-22 06:15:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:0.8276888132095337 norm:0.0007502607768401504 max memory_allocated 22567.32470703125 
[2025-03-22 06:16:09 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:0.827508807182312 norm:0.0007635660003870726 max memory_allocated 22567.32470703125 
[2025-03-22 06:16:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 06:16:54 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:0.9951589703559875 norm:0.014596434310078621 max memory_allocated 22567.49658203125 
[2025-03-22 06:17:26 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:0.9740846157073975 norm:0.006591315381228924 max memory_allocated 22567.49658203125 
[2025-03-22 06:17:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:0.9545877575874329 norm:0.003483503358438611 max memory_allocated 22567.49658203125 
[2025-03-22 06:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:0.9463105201721191 norm:0.00227153766900301 max memory_allocated 22567.49658203125 
[2025-03-22 06:19:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:0.9424020648002625 norm:0.0018071446102112532 max memory_allocated 22567.49658203125 
[2025-03-22 06:19:37 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:0.9391236305236816 norm:0.0015054434770718217 max memory_allocated 22567.49658203125 
[2025-03-22 06:20:09 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:0.9364127516746521 norm:0.0012646192917600274 max memory_allocated 22567.49658203125 
[2025-03-22 06:20:42 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:0.9344384670257568 norm:0.0011225752532482147 max memory_allocated 22567.49658203125 
[2025-03-22 06:21:14 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:0.932857871055603 norm:0.0008518287213519216 max memory_allocated 22567.49658203125 
[2025-03-22 06:21:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:0.9318200945854187 norm:0.0007497270125895739 max memory_allocated 22567.49658203125 
[2025-03-22 06:22:19 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:0.9311424493789673 norm:0.0007239349652081728 max memory_allocated 22567.49658203125 
[2025-03-22 06:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:0.930677056312561 norm:0.0007135526975616813 max memory_allocated 22567.49658203125 
[2025-03-22 06:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:0.9302961826324463 norm:0.0006968750967644155 max memory_allocated 22567.49658203125 
[2025-03-22 06:23:57 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:0.9298881888389587 norm:0.0006703614490106702 max memory_allocated 22567.49658203125 
[2025-03-22 06:24:29 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:0.9295716285705566 norm:0.0006653344025835395 max memory_allocated 22567.49658203125 
[2025-03-22 06:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:0.9292975664138794 norm:0.0006594789447262883 max memory_allocated 22567.49658203125 
[2025-03-22 06:25:34 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:0.9290064573287964 norm:0.0006577520398423076 max memory_allocated 22567.49658203125 
[2025-03-22 06:26:07 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:0.9287481307983398 norm:0.0006633645389229059 max memory_allocated 22567.49658203125 
[2025-03-22 06:26:39 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:0.9286746978759766 norm:0.0006590498378500342 max memory_allocated 22567.49658203125 
[2025-03-22 06:27:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:0.9284874200820923 norm:0.0006593018770217896 max memory_allocated 22567.49658203125 
[2025-03-22 06:27:21 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 06:27:24 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:27:56 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:1.148524522781372 norm:0.06126872077584267 max memory_allocated 22569.51220703125 
[2025-03-22 06:28:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:1.1185506582260132 norm:0.04909287765622139 max memory_allocated 22569.51220703125 
[2025-03-22 06:29:02 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:1.0918450355529785 norm:0.034066952764987946 max memory_allocated 22569.51220703125 
[2025-03-22 06:29:34 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:1.0795302391052246 norm:0.026688482612371445 max memory_allocated 22569.51220703125 
[2025-03-22 06:30:07 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:1.0740044116973877 norm:0.022182833403348923 max memory_allocated 22569.51220703125 
[2025-03-22 06:30:40 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:1.0695856809616089 norm:0.018308958038687706 max memory_allocated 22569.51220703125 
[2025-03-22 06:31:13 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:1.0663728713989258 norm:0.015553461387753487 max memory_allocated 22569.51220703125 
[2025-03-22 06:31:45 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:1.0633180141448975 norm:0.013894377276301384 max memory_allocated 22569.51220703125 
[2025-03-22 06:32:18 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:1.0604610443115234 norm:0.011671530082821846 max memory_allocated 22569.51220703125 
[2025-03-22 06:32:51 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:1.0586767196655273 norm:0.010472632944583893 max memory_allocated 22569.51220703125 
[2025-03-22 06:33:24 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:1.0582302808761597 norm:0.009066799655556679 max memory_allocated 22569.51220703125 
[2025-03-22 06:33:57 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:1.0567597150802612 norm:0.00923552643507719 max memory_allocated 22569.51220703125 
[2025-03-22 06:34:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:1.0569021701812744 norm:0.009376944042742252 max memory_allocated 22569.51220703125 
[2025-03-22 06:35:02 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:1.0572593212127686 norm:0.009528610855340958 max memory_allocated 22569.51220703125 
[2025-03-22 06:35:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:1.057471752166748 norm:0.0106770358979702 max memory_allocated 22569.51220703125 
[2025-03-22 06:36:08 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:1.0572433471679688 norm:0.009929106570780277 max memory_allocated 22569.51220703125 
[2025-03-22 06:36:40 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:1.0555341243743896 norm:0.008571460843086243 max memory_allocated 22569.51220703125 
[2025-03-22 06:37:13 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:1.0566343069076538 norm:0.008777095936238766 max memory_allocated 22569.51220703125 
[2025-03-22 06:37:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:1.0527715682983398 norm:0.0066397106274962425 max memory_allocated 22569.51220703125 
[2025-03-22 06:38:18 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:1.0501359701156616 norm:0.005144269205629826 max memory_allocated 22569.51220703125 
[2025-03-22 06:38:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 06:38:30 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:39:03 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:1.3026789426803589 norm:0.058104999363422394 max memory_allocated 22569.68408203125 
[2025-03-22 06:39:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:1.265989065170288 norm:0.04841417446732521 max memory_allocated 22569.68408203125 
[2025-03-22 06:40:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:1.2355672121047974 norm:0.030128788203001022 max memory_allocated 22569.68408203125 
[2025-03-22 06:40:41 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:1.2202942371368408 norm:0.02836487628519535 max memory_allocated 22569.68408203125 
[2025-03-22 06:41:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:1.2122653722763062 norm:0.02403777465224266 max memory_allocated 22569.68408203125 
[2025-03-22 06:41:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:1.2062675952911377 norm:0.02012448012828827 max memory_allocated 22569.68408203125 
[2025-03-22 06:42:18 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:1.2023072242736816 norm:0.01684115268290043 max memory_allocated 22569.68408203125 
[2025-03-22 06:42:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:1.198843002319336 norm:0.01539280079305172 max memory_allocated 22569.68408203125 
[2025-03-22 06:43:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:1.196272611618042 norm:0.014670418575406075 max memory_allocated 22569.68408203125 
[2025-03-22 06:43:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:1.1934752464294434 norm:0.012185313738882542 max memory_allocated 22569.68408203125 
[2025-03-22 06:44:29 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:1.1923238039016724 norm:0.010417269542813301 max memory_allocated 22569.68408203125 
[2025-03-22 06:45:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:1.1897448301315308 norm:0.009317475371062756 max memory_allocated 22569.68408203125 
[2025-03-22 06:45:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:1.1893575191497803 norm:0.00984273012727499 max memory_allocated 22569.68408203125 
[2025-03-22 06:46:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:1.1883829832077026 norm:0.00928441807627678 max memory_allocated 22569.68408203125 
[2025-03-22 06:46:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:1.188033938407898 norm:0.009170175530016422 max memory_allocated 22569.68408203125 
[2025-03-22 06:47:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:1.1873294115066528 norm:0.008687742985785007 max memory_allocated 22569.68408203125 
[2025-03-22 06:47:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:1.1878620386123657 norm:0.009116200730204582 max memory_allocated 22569.68408203125 
[2025-03-22 06:48:19 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:1.1867332458496094 norm:0.008605491369962692 max memory_allocated 22569.68408203125 
[2025-03-22 06:48:52 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:1.185247540473938 norm:0.007697096094489098 max memory_allocated 22569.68408203125 
[2025-03-22 06:49:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:1.1843045949935913 norm:0.007095705717802048 max memory_allocated 22569.68408203125 
[2025-03-22 06:49:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 06:49:36 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:50:09 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:2.905269145965576 norm:0.6499831080436707 max memory_allocated 22569.85595703125 
[2025-03-22 06:50:42 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:3.076782703399658 norm:1.1797914505004883 max memory_allocated 22569.85595703125 
[2025-03-22 06:51:14 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:2.459914207458496 norm:0.4986390471458435 max memory_allocated 22569.85595703125 
[2025-03-22 06:51:47 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:1.9943947792053223 norm:0.3573959171772003 max memory_allocated 22569.85595703125 
[2025-03-22 06:52:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:1.9224594831466675 norm:0.3095143139362335 max memory_allocated 22569.85595703125 
[2025-03-22 06:52:53 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:1.8895996809005737 norm:0.28619521856307983 max memory_allocated 22569.85595703125 
[2025-03-22 06:53:25 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:1.870105504989624 norm:0.25957971811294556 max memory_allocated 22569.85595703125 
[2025-03-22 06:53:58 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:1.861032485961914 norm:0.2459394931793213 max memory_allocated 22569.85595703125 
[2025-03-22 06:54:30 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:1.836240530014038 norm:0.22709505259990692 max memory_allocated 22569.85595703125 
[2025-03-22 06:55:03 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:1.8188378810882568 norm:0.21433429419994354 max memory_allocated 22569.85595703125 
[2025-03-22 06:55:36 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:1.81976318359375 norm:0.18751190602779388 max memory_allocated 22569.85595703125 
[2025-03-22 06:56:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:1.8177196979522705 norm:0.1813095510005951 max memory_allocated 22569.85595703125 
[2025-03-22 06:56:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:1.8242385387420654 norm:0.1810442954301834 max memory_allocated 22569.85595703125 
[2025-03-22 06:57:14 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:1.8165295124053955 norm:0.1728622019290924 max memory_allocated 22569.85595703125 
[2025-03-22 06:57:46 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:1.8049887418746948 norm:0.17078807950019836 max memory_allocated 22569.85595703125 
[2025-03-22 06:58:19 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:1.80485200881958 norm:0.15927955508232117 max memory_allocated 22569.85595703125 
[2025-03-22 06:58:51 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:1.8005361557006836 norm:0.15954601764678955 max memory_allocated 22569.85595703125 
[2025-03-22 06:59:24 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:1.7836058139801025 norm:0.1544794738292694 max memory_allocated 22569.85595703125 
[2025-03-22 06:59:57 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:1.7793786525726318 norm:0.13575278222560883 max memory_allocated 22569.85595703125 
[2025-03-22 07:00:30 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:1.774681568145752 norm:0.13595688343048096 max memory_allocated 22569.85595703125 
[2025-03-22 07:00:38 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 07:00:41 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 07:01:14 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:4.172735214233398 norm:0.6672947406768799 max memory_allocated 22570.02783203125 
[2025-03-22 07:01:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:3.6900458335876465 norm:0.522295355796814 max memory_allocated 22570.02783203125 
[2025-03-22 07:02:19 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:3.3172495365142822 norm:0.4041277766227722 max memory_allocated 22570.02783203125 
[2025-03-22 07:02:52 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:3.187861919403076 norm:0.3328966200351715 max memory_allocated 22570.02783203125 
[2025-03-22 07:03:25 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:3.125995397567749 norm:0.2881656885147095 max memory_allocated 22570.02783203125 
[2025-03-22 07:03:58 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:3.083958387374878 norm:0.2497352510690689 max memory_allocated 22570.02783203125 
[2025-03-22 07:04:30 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:3.0397253036499023 norm:0.20594282448291779 max memory_allocated 22570.02783203125 
[2025-03-22 07:05:03 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:3.011319160461426 norm:0.18586528301239014 max memory_allocated 22570.02783203125 
[2025-03-22 07:05:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:2.9782779216766357 norm:0.1574648767709732 max memory_allocated 22570.02783203125 
[2025-03-22 07:06:09 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:2.9637198448181152 norm:0.14802633225917816 max memory_allocated 22570.02783203125 
[2025-03-22 07:06:42 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:2.938668727874756 norm:0.13345904648303986 max memory_allocated 22570.02783203125 
[2025-03-22 07:07:14 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:2.9209983348846436 norm:0.12759742140769958 max memory_allocated 22570.02783203125 
[2025-03-22 07:07:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:2.903780221939087 norm:0.12588204443454742 max memory_allocated 22570.02783203125 
[2025-03-22 07:08:20 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:2.88218092918396 norm:0.10836374014616013 max memory_allocated 22570.02783203125 
[2025-03-22 07:08:52 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:2.8802437782287598 norm:0.11759346723556519 max memory_allocated 22570.02783203125 
[2025-03-22 07:09:25 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:2.8597958087921143 norm:0.10340908169746399 max memory_allocated 22570.02783203125 
[2025-03-22 07:09:57 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:2.852653980255127 norm:0.1058359295129776 max memory_allocated 22570.02783203125 
[2025-03-22 07:10:30 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:2.846844434738159 norm:0.10761351138353348 max memory_allocated 22570.02783203125 
[2025-03-22 07:11:03 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:2.8277363777160645 norm:0.09526947140693665 max memory_allocated 22570.02783203125 
[2025-03-22 07:11:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:2.8126323223114014 norm:0.0842791423201561 max memory_allocated 22570.02783203125 
[2025-03-22 07:11:44 root] (main_calibration_a.py 369): INFO 21229.786230564117
[2025-03-22 07:11:49 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 07:12:57 root] (main_calibration_a.py 158): INFO wikitext2 : 8.979214668273926
[2025-03-22 07:12:57 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 07:14:42 root] (main_calibration_a.py 158): INFO c4 : 12.573739051818848
[2025-03-22 09:06:22 root] (main_calibration_a.py 169): INFO {'wikitext2': 8.979214668273926, 'c4': 12.573739051818848, 'results': {'arc_easy': {'acc': 0.49326599326599324, 'acc_stderr': 0.010258852980991825, 'acc_norm': 0.40824915824915825, 'acc_norm_stderr': 0.010085566195791254}, 'winogrande': {'acc': 0.5611681136543015, 'acc_stderr': 0.013946933444507032}, 'piqa': {'acc': 0.6702937976060935, 'acc_stderr': 0.010968357083095152, 'acc_norm': 0.6664853101196954, 'acc_norm_stderr': 0.011000139592184573}, 'hellaswag': {'acc': 0.44433379804819756, 'acc_stderr': 0.004958761056959776, 'acc_norm': 0.5877315275841466, 'acc_norm_stderr': 0.00491237002391302}, 'boolq': {'acc': 0.6330275229357798, 'acc_stderr': 0.008429864284269437}, 'arc_challenge': {'acc': 0.2687713310580205, 'acc_stderr': 0.012955065963710679, 'acc_norm': 0.3122866894197952, 'acc_norm_stderr': 0.013542598541688065}}, 'versions': {'arc_easy': 0, 'winogrande': 0, 'piqa': 0, 'hellaswag': 0, 'boolq': 1, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 09:06:22 root] (main_calibration_a.py 172): INFO 26.88,49.33,63.30,44.43,67.03,56.12
