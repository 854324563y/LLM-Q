[2025-03-22 01:14:17 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/Llama-2-7b-hf-w4a4-4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=4)
[2025-03-22 01:17:55 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 01:17:55 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:17:55 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 01:17:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:18:00 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:18:32 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.07309132814407349 norm:0.08989836275577545 max memory_allocated 22562.62548828125 
[2025-03-22 01:19:04 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.037848081439733505 norm:0.04138119891285896 max memory_allocated 22562.62548828125 
[2025-03-22 01:19:37 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.029432330280542374 norm:0.031707268208265305 max memory_allocated 22562.62548828125 
[2025-03-22 01:20:09 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.02559538744390011 norm:0.028005287051200867 max memory_allocated 22562.62548828125 
[2025-03-22 01:20:42 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.0237257182598114 norm:0.02537889964878559 max memory_allocated 22562.62548828125 
[2025-03-22 01:21:14 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.022386107593774796 norm:0.022304076701402664 max memory_allocated 22562.62548828125 
[2025-03-22 01:21:47 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.02147408202290535 norm:0.019178789108991623 max memory_allocated 22562.62548828125 
[2025-03-22 01:22:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.021259557455778122 norm:0.017018847167491913 max memory_allocated 22562.62548828125 
[2025-03-22 01:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.020694248378276825 norm:0.014883787371218204 max memory_allocated 22562.62548828125 
[2025-03-22 01:23:25 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.020306359976530075 norm:0.012901153415441513 max memory_allocated 22562.62548828125 
[2025-03-22 01:23:57 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.019927268847823143 norm:0.011188596487045288 max memory_allocated 22562.62548828125 
[2025-03-22 01:24:30 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.019696764647960663 norm:0.009931632317602634 max memory_allocated 22562.62548828125 
[2025-03-22 01:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.01959734782576561 norm:0.008871016092598438 max memory_allocated 22562.62548828125 
[2025-03-22 01:25:35 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.01942257769405842 norm:0.008217724040150642 max memory_allocated 22562.62548828125 
[2025-03-22 01:26:08 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.019600512459874153 norm:0.008253097534179688 max memory_allocated 22562.62548828125 
[2025-03-22 01:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.019399065524339676 norm:0.0076742954552173615 max memory_allocated 22562.62548828125 
[2025-03-22 01:27:13 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.019443850964307785 norm:0.007360472809523344 max memory_allocated 22562.62548828125 
[2025-03-22 01:27:46 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.01943005807697773 norm:0.007371997926384211 max memory_allocated 22562.62548828125 
[2025-03-22 01:28:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.019298303872346878 norm:0.0066861738450825214 max memory_allocated 22562.62548828125 
[2025-03-22 01:28:51 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.01900731958448887 norm:0.005491855554282665 max memory_allocated 22562.62548828125 
[2025-03-22 01:29:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:29:03 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:29:36 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.2965390086174011 norm:0.15767739713191986 max memory_allocated 22562.79736328125 
[2025-03-22 01:30:08 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.2057589292526245 norm:0.0995723158121109 max memory_allocated 22562.79736328125 
[2025-03-22 01:30:41 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.1625809371471405 norm:0.05868043005466461 max memory_allocated 22562.79736328125 
[2025-03-22 01:31:13 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.14515069127082825 norm:0.05062473192811012 max memory_allocated 22562.79736328125 
[2025-03-22 01:31:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.13845467567443848 norm:0.04638763517141342 max memory_allocated 22562.79736328125 
[2025-03-22 01:32:18 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.13514772057533264 norm:0.04491712898015976 max memory_allocated 22562.79736328125 
[2025-03-22 01:32:51 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.129247784614563 norm:0.041356559842824936 max memory_allocated 22562.79736328125 
[2025-03-22 01:33:23 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.12762504816055298 norm:0.041146714240312576 max memory_allocated 22562.79736328125 
[2025-03-22 01:33:56 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.12288346886634827 norm:0.040222909301519394 max memory_allocated 22562.79736328125 
[2025-03-22 01:34:29 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.12113158404827118 norm:0.03706873953342438 max memory_allocated 22562.79736328125 
[2025-03-22 01:35:01 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.12186222523450851 norm:0.03688451647758484 max memory_allocated 22562.79736328125 
[2025-03-22 01:35:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.12160544097423553 norm:0.03658357262611389 max memory_allocated 22562.79736328125 
[2025-03-22 01:36:06 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.11980707198381424 norm:0.03424271568655968 max memory_allocated 22562.79736328125 
[2025-03-22 01:36:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.11955785751342773 norm:0.033016592264175415 max memory_allocated 22562.79736328125 
[2025-03-22 01:37:11 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.11872845143079758 norm:0.0317404568195343 max memory_allocated 22562.79736328125 
[2025-03-22 01:37:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.11750133335590363 norm:0.032418474555015564 max memory_allocated 22562.79736328125 
[2025-03-22 01:38:17 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.11916332691907883 norm:0.030063923448324203 max memory_allocated 22562.79736328125 
[2025-03-22 01:38:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.11771072447299957 norm:0.030593182891607285 max memory_allocated 22562.79736328125 
[2025-03-22 01:39:22 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.1186235323548317 norm:0.029907679185271263 max memory_allocated 22562.79736328125 
[2025-03-22 01:39:55 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.11685603111982346 norm:0.027606574818491936 max memory_allocated 22562.79736328125 
[2025-03-22 01:40:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:40:06 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:40:40 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.20192676782608032 norm:0.08238822221755981 max memory_allocated 22562.96923828125 
[2025-03-22 01:41:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.16714726388454437 norm:0.06072022765874863 max memory_allocated 22562.96923828125 
[2025-03-22 01:41:46 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.14865681529045105 norm:0.04269396886229515 max memory_allocated 22562.96923828125 
[2025-03-22 01:42:18 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.14077633619308472 norm:0.032639771699905396 max memory_allocated 22562.96923828125 
[2025-03-22 01:42:51 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.13664613664150238 norm:0.02706684172153473 max memory_allocated 22562.96923828125 
[2025-03-22 01:43:24 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.1336265206336975 norm:0.023098265752196312 max memory_allocated 22562.96923828125 
[2025-03-22 01:43:57 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.13144780695438385 norm:0.01905369572341442 max memory_allocated 22562.96923828125 
[2025-03-22 01:44:30 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.129801943898201 norm:0.01581588387489319 max memory_allocated 22562.96923828125 
[2025-03-22 01:45:03 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.1283128708600998 norm:0.012611920945346355 max memory_allocated 22562.96923828125 
[2025-03-22 01:45:36 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.1271536946296692 norm:0.010576543398201466 max memory_allocated 22562.96923828125 
[2025-03-22 01:46:08 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.1260880082845688 norm:0.009025385603308678 max memory_allocated 22562.96923828125 
[2025-03-22 01:46:41 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.12578071653842926 norm:0.008730337955057621 max memory_allocated 22562.96923828125 
[2025-03-22 01:47:14 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.12535345554351807 norm:0.008022136054933071 max memory_allocated 22562.96923828125 
[2025-03-22 01:47:47 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.12503115832805634 norm:0.007336068898439407 max memory_allocated 22562.96923828125 
[2025-03-22 01:48:20 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.12494742125272751 norm:0.007196828722953796 max memory_allocated 22562.96923828125 
[2025-03-22 01:48:53 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.12479794770479202 norm:0.006807577796280384 max memory_allocated 22562.96923828125 
[2025-03-22 01:49:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.12472431361675262 norm:0.006540889386087656 max memory_allocated 22562.96923828125 
[2025-03-22 01:49:58 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.12475069612264633 norm:0.006436198949813843 max memory_allocated 22562.96923828125 
[2025-03-22 01:50:31 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.12494807690382004 norm:0.006371770054101944 max memory_allocated 22562.96923828125 
[2025-03-22 01:51:04 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.12442927062511444 norm:0.005686120130121708 max memory_allocated 22562.96923828125 
[2025-03-22 01:51:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 01:51:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.22758279740810394 norm:0.02340564876794815 max memory_allocated 22562.96923828125 
[2025-03-22 01:52:20 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.19658274948596954 norm:0.008419030345976353 max memory_allocated 22562.96923828125 
[2025-03-22 01:52:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.17659877240657806 norm:0.0042162565514445305 max memory_allocated 22562.96923828125 
[2025-03-22 01:53:25 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.16927559673786163 norm:0.0027543245814740658 max memory_allocated 22562.96923828125 
[2025-03-22 01:53:58 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.16634315252304077 norm:0.002466488629579544 max memory_allocated 22562.96923828125 
[2025-03-22 01:54:30 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.1638682335615158 norm:0.0021229165140539408 max memory_allocated 22562.96923828125 
[2025-03-22 01:55:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.16281016170978546 norm:0.0019650040194392204 max memory_allocated 22562.96923828125 
[2025-03-22 01:55:35 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.16173386573791504 norm:0.0017539644613862038 max memory_allocated 22562.96923828125 
[2025-03-22 01:56:08 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.16094432771205902 norm:0.0016140061197802424 max memory_allocated 22562.96923828125 
[2025-03-22 01:56:40 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.16047802567481995 norm:0.0014862538082525134 max memory_allocated 22562.96923828125 
[2025-03-22 01:57:13 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.16023288667201996 norm:0.0014397448394447565 max memory_allocated 22562.96923828125 
[2025-03-22 01:57:46 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.15985359251499176 norm:0.001372030470520258 max memory_allocated 22562.96923828125 
[2025-03-22 01:58:18 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.16000783443450928 norm:0.0013649777974933386 max memory_allocated 22562.96923828125 
[2025-03-22 01:58:51 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.1598164439201355 norm:0.0013167469296604395 max memory_allocated 22562.96923828125 
[2025-03-22 01:59:24 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.15962280333042145 norm:0.0012514058034867048 max memory_allocated 22562.96923828125 
[2025-03-22 01:59:56 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.1594703197479248 norm:0.0012305149575695395 max memory_allocated 22562.96923828125 
[2025-03-22 02:00:29 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.1594962328672409 norm:0.0011946558952331543 max memory_allocated 22562.96923828125 
[2025-03-22 02:01:02 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.15961183607578278 norm:0.0011874533956870437 max memory_allocated 22562.96923828125 
[2025-03-22 02:01:34 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.1596389263868332 norm:0.0012024627067148685 max memory_allocated 22562.96923828125 
[2025-03-22 02:02:07 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.15995314717292786 norm:0.0012260946677997708 max memory_allocated 22562.96923828125 
[2025-03-22 02:02:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:02:52 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.2880789041519165 norm:0.040852826088666916 max memory_allocated 22562.96923828125 
[2025-03-22 02:03:24 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.24296054244041443 norm:0.012287099845707417 max memory_allocated 22562.96923828125 
[2025-03-22 02:03:57 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.2139386683702469 norm:0.004576900042593479 max memory_allocated 22562.96923828125 
[2025-03-22 02:04:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.20465055108070374 norm:0.002947456669062376 max memory_allocated 22562.96923828125 
[2025-03-22 02:05:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.20040449500083923 norm:0.0025096838362514973 max memory_allocated 22562.96923828125 
[2025-03-22 02:05:35 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.19724631309509277 norm:0.0020642615854740143 max memory_allocated 22562.96923828125 
[2025-03-22 02:06:08 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.1951923966407776 norm:0.0017693450208753347 max memory_allocated 22562.96923828125 
[2025-03-22 02:06:40 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.19373014569282532 norm:0.0015585083747282624 max memory_allocated 22562.96923828125 
[2025-03-22 02:07:13 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.19278980791568756 norm:0.0014502875274047256 max memory_allocated 22562.96923828125 
[2025-03-22 02:07:45 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.19206096231937408 norm:0.0013901228085160255 max memory_allocated 22562.96923828125 
[2025-03-22 02:08:18 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.1914624273777008 norm:0.0013063375372439623 max memory_allocated 22562.96923828125 
[2025-03-22 02:08:51 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.19118624925613403 norm:0.0012653203448280692 max memory_allocated 22562.96923828125 
[2025-03-22 02:09:23 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.19094382226467133 norm:0.001250550732947886 max memory_allocated 22562.96923828125 
[2025-03-22 02:09:56 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.19082245230674744 norm:0.0012290019076317549 max memory_allocated 22562.96923828125 
[2025-03-22 02:10:28 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.1905457079410553 norm:0.0012024720199406147 max memory_allocated 22562.96923828125 
[2025-03-22 02:11:01 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.19043873250484467 norm:0.0012152671115472913 max memory_allocated 22562.96923828125 
[2025-03-22 02:11:33 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.19038085639476776 norm:0.0011987962061539292 max memory_allocated 22562.96923828125 
[2025-03-22 02:12:06 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.1902509331703186 norm:0.0011906033614650369 max memory_allocated 22562.96923828125 
[2025-03-22 02:12:38 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.1902143508195877 norm:0.001193324918858707 max memory_allocated 22562.96923828125 
[2025-03-22 02:13:11 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.19019323587417603 norm:0.0011821239022538066 max memory_allocated 22562.96923828125 
[2025-03-22 02:13:20 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:13:55 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.3060765862464905 norm:0.01907690241932869 max memory_allocated 22563.02392578125 
[2025-03-22 02:14:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2705181837081909 norm:0.007689994294196367 max memory_allocated 22563.02392578125 
[2025-03-22 02:15:00 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.24056410789489746 norm:0.0029150170739740133 max memory_allocated 22563.02392578125 
[2025-03-22 02:15:33 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.23018746078014374 norm:0.0019243452697992325 max memory_allocated 22563.02392578125 
[2025-03-22 02:16:06 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.22519835829734802 norm:0.0016746515175327659 max memory_allocated 22563.02392578125 
[2025-03-22 02:16:38 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.22213171422481537 norm:0.0015223454684019089 max memory_allocated 22563.02392578125 
[2025-03-22 02:17:11 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.22013786435127258 norm:0.0014227339997887611 max memory_allocated 22563.02392578125 
[2025-03-22 02:17:44 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.21899138391017914 norm:0.0013828499941155314 max memory_allocated 22563.02392578125 
[2025-03-22 02:18:17 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.2181955873966217 norm:0.0013512038858607411 max memory_allocated 22563.02392578125 
[2025-03-22 02:18:49 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.2176450490951538 norm:0.001286940649151802 max memory_allocated 22563.02392578125 
[2025-03-22 02:19:22 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.21721072494983673 norm:0.0012702627573162317 max memory_allocated 22563.02392578125 
[2025-03-22 02:19:55 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.21690823137760162 norm:0.001263270154595375 max memory_allocated 22563.02392578125 
[2025-03-22 02:20:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.21658410131931305 norm:0.0012591580161824822 max memory_allocated 22563.02392578125 
[2025-03-22 02:21:00 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.2163671851158142 norm:0.0012175949523225427 max memory_allocated 22563.02392578125 
[2025-03-22 02:21:33 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.21615201234817505 norm:0.0011612738016992807 max memory_allocated 22563.02392578125 
[2025-03-22 02:22:06 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.21590350568294525 norm:0.0011367660481482744 max memory_allocated 22563.02392578125 
[2025-03-22 02:22:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.21586646139621735 norm:0.001125973416492343 max memory_allocated 22563.02392578125 
[2025-03-22 02:23:11 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.21582797169685364 norm:0.0011291713453829288 max memory_allocated 22563.02392578125 
[2025-03-22 02:23:44 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.2157570868730545 norm:0.0011374137829989195 max memory_allocated 22563.02392578125 
[2025-03-22 02:24:17 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.2158062756061554 norm:0.0011324668303132057 max memory_allocated 22563.02392578125 
[2025-03-22 02:24:25 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:25:01 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.37518003582954407 norm:0.038381896913051605 max memory_allocated 22563.19580078125 
[2025-03-22 02:25:33 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.3237786293029785 norm:0.014886236749589443 max memory_allocated 22563.19580078125 
[2025-03-22 02:26:06 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.2845359742641449 norm:0.006899562198668718 max memory_allocated 22563.19580078125 
[2025-03-22 02:26:38 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.26988232135772705 norm:0.004419712815433741 max memory_allocated 22563.19580078125 
[2025-03-22 02:27:11 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.2628006637096405 norm:0.003400876186788082 max memory_allocated 22563.19580078125 
[2025-03-22 02:27:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.2580869793891907 norm:0.002842759247869253 max memory_allocated 22563.19580078125 
[2025-03-22 02:28:16 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.25511646270751953 norm:0.002512500388547778 max memory_allocated 22563.19580078125 
[2025-03-22 02:28:49 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.25286781787872314 norm:0.0021901223808526993 max memory_allocated 22563.19580078125 
[2025-03-22 02:29:21 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.2509945333003998 norm:0.0019883981440216303 max memory_allocated 22563.19580078125 
[2025-03-22 02:29:54 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.24978749454021454 norm:0.001780789578333497 max memory_allocated 22563.19580078125 
[2025-03-22 02:30:27 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.2490634024143219 norm:0.0016861447365954518 max memory_allocated 22563.19580078125 
[2025-03-22 02:30:59 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.24848367273807526 norm:0.0016259080730378628 max memory_allocated 22563.19580078125 
[2025-03-22 02:31:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.24795179069042206 norm:0.0015777596272528172 max memory_allocated 22563.19580078125 
[2025-03-22 02:32:05 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.2476215958595276 norm:0.0014983908040449023 max memory_allocated 22563.19580078125 
[2025-03-22 02:32:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.24719059467315674 norm:0.0014314691070467234 max memory_allocated 22563.19580078125 
[2025-03-22 02:33:10 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.24699033796787262 norm:0.0013942356454208493 max memory_allocated 22563.19580078125 
[2025-03-22 02:33:43 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.24681830406188965 norm:0.0013513338053599 max memory_allocated 22563.19580078125 
[2025-03-22 02:34:15 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.24645349383354187 norm:0.0013190070167183876 max memory_allocated 22563.19580078125 
[2025-03-22 02:34:48 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.246190145611763 norm:0.0012960302410647273 max memory_allocated 22563.19580078125 
[2025-03-22 02:35:21 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.24592769145965576 norm:0.0012737723300233483 max memory_allocated 22563.19580078125 
[2025-03-22 02:35:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 02:36:05 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.41277065873146057 norm:0.04026719182729721 max memory_allocated 22563.36767578125 
[2025-03-22 02:36:38 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.342428058385849 norm:0.012674186378717422 max memory_allocated 22563.36767578125 
[2025-03-22 02:37:11 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.3045139014720917 norm:0.005947374738752842 max memory_allocated 22563.36767578125 
[2025-03-22 02:37:44 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.2910352051258087 norm:0.0038045584224164486 max memory_allocated 22563.36767578125 
[2025-03-22 02:38:16 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.2848573327064514 norm:0.0030209259130060673 max memory_allocated 22563.36767578125 
[2025-03-22 02:38:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.2808533310890198 norm:0.0025901715271174908 max memory_allocated 22563.36767578125 
[2025-03-22 02:39:22 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.27799367904663086 norm:0.002304264809936285 max memory_allocated 22563.36767578125 
[2025-03-22 02:39:55 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.2757752239704132 norm:0.0021479360293596983 max memory_allocated 22563.36767578125 
[2025-03-22 02:40:27 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.2742880582809448 norm:0.0019913860596716404 max memory_allocated 22563.36767578125 
[2025-03-22 02:41:00 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.27305641770362854 norm:0.0018605500226840377 max memory_allocated 22563.36767578125 
[2025-03-22 02:41:33 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.2722265422344208 norm:0.0017189931822940707 max memory_allocated 22563.36767578125 
[2025-03-22 02:42:05 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.27139270305633545 norm:0.0016487485263496637 max memory_allocated 22563.36767578125 
[2025-03-22 02:42:38 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.27098768949508667 norm:0.0016324834432452917 max memory_allocated 22563.36767578125 
[2025-03-22 02:43:11 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.27050119638442993 norm:0.0015482945600524545 max memory_allocated 22563.36767578125 
[2025-03-22 02:43:43 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.27010488510131836 norm:0.0015624812804162502 max memory_allocated 22563.36767578125 
[2025-03-22 02:44:16 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.269747793674469 norm:0.0014835959300398827 max memory_allocated 22563.36767578125 
[2025-03-22 02:44:48 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.26952266693115234 norm:0.001419965410605073 max memory_allocated 22563.36767578125 
[2025-03-22 02:45:21 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.269284725189209 norm:0.001406275201588869 max memory_allocated 22563.36767578125 
[2025-03-22 02:45:54 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.26917600631713867 norm:0.0013893733266741037 max memory_allocated 22563.36767578125 
[2025-03-22 02:46:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.26913487911224365 norm:0.0013716989196836948 max memory_allocated 22563.36767578125 
[2025-03-22 02:46:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 02:47:10 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.40018874406814575 norm:0.024790605530142784 max memory_allocated 22563.53955078125 
[2025-03-22 02:47:43 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.35789600014686584 norm:0.009737343527376652 max memory_allocated 22563.53955078125 
[2025-03-22 02:48:16 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.321903258562088 norm:0.004174599424004555 max memory_allocated 22563.53955078125 
[2025-03-22 02:48:48 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.30809974670410156 norm:0.0025158256758004427 max memory_allocated 22563.53955078125 
[2025-03-22 02:49:21 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.3022056818008423 norm:0.0020834438037127256 max memory_allocated 22563.53955078125 
[2025-03-22 02:49:54 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.2979154586791992 norm:0.0018258052878081799 max memory_allocated 22563.53955078125 
[2025-03-22 02:50:26 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.29511725902557373 norm:0.001652115024626255 max memory_allocated 22563.53955078125 
[2025-03-22 02:50:59 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.2930874824523926 norm:0.001522430800832808 max memory_allocated 22563.53955078125 
[2025-03-22 02:51:32 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.29156312346458435 norm:0.0014352084835991263 max memory_allocated 22563.53955078125 
[2025-03-22 02:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.29065680503845215 norm:0.0013989966828376055 max memory_allocated 22563.53955078125 
[2025-03-22 02:52:38 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.2897540032863617 norm:0.0013371692039072514 max memory_allocated 22563.53955078125 
[2025-03-22 02:53:10 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.2892923951148987 norm:0.0013089196290820837 max memory_allocated 22563.53955078125 
[2025-03-22 02:53:43 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.2887630760669708 norm:0.0012784017017111182 max memory_allocated 22563.53955078125 
[2025-03-22 02:54:16 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.2883842885494232 norm:0.0012557946611195803 max memory_allocated 22563.53955078125 
[2025-03-22 02:54:49 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.2880265414714813 norm:0.0012604413786903024 max memory_allocated 22563.53955078125 
[2025-03-22 02:55:21 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.2878826856613159 norm:0.0012344205752015114 max memory_allocated 22563.53955078125 
[2025-03-22 02:55:54 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.28760650753974915 norm:0.0012144214706495404 max memory_allocated 22563.53955078125 
[2025-03-22 02:56:27 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.28748515248298645 norm:0.0011940405238419771 max memory_allocated 22563.53955078125 
[2025-03-22 02:57:00 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.28737035393714905 norm:0.001178661361336708 max memory_allocated 22563.53955078125 
[2025-03-22 02:57:32 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.2872634530067444 norm:0.001178777078166604 max memory_allocated 22563.53955078125 
[2025-03-22 02:57:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 02:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.44274312257766724 norm:0.03813936188817024 max memory_allocated 22563.71142578125 
[2025-03-22 02:58:49 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.38184255361557007 norm:0.015393033623695374 max memory_allocated 22563.71142578125 
[2025-03-22 02:59:22 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.33598899841308594 norm:0.004898242186754942 max memory_allocated 22563.71142578125 
[2025-03-22 02:59:54 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.32277703285217285 norm:0.0031525674276053905 max memory_allocated 22563.71142578125 
[2025-03-22 03:00:27 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.31691327691078186 norm:0.0027259779162704945 max memory_allocated 22563.71142578125 
[2025-03-22 03:00:59 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.31305718421936035 norm:0.0023734434507787228 max memory_allocated 22563.71142578125 
[2025-03-22 03:01:32 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.31003618240356445 norm:0.002073120791465044 max memory_allocated 22563.71142578125 
[2025-03-22 03:02:05 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.3080247640609741 norm:0.0019250151235610247 max memory_allocated 22563.71142578125 
[2025-03-22 03:02:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.306643009185791 norm:0.0018050660146400332 max memory_allocated 22563.71142578125 
[2025-03-22 03:03:10 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3055356740951538 norm:0.0016782841412350535 max memory_allocated 22563.71142578125 
[2025-03-22 03:03:42 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.30454710125923157 norm:0.0015744779957458377 max memory_allocated 22563.71142578125 
[2025-03-22 03:04:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.30385780334472656 norm:0.0014952804194763303 max memory_allocated 22563.71142578125 
[2025-03-22 03:04:48 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.3032245635986328 norm:0.0013994990149512887 max memory_allocated 22563.71142578125 
[2025-03-22 03:05:21 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.30285346508026123 norm:0.0013627891894429922 max memory_allocated 22563.71142578125 
[2025-03-22 03:05:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.30242979526519775 norm:0.001293472945690155 max memory_allocated 22563.71142578125 
[2025-03-22 03:06:26 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.3021707832813263 norm:0.0012630947167053819 max memory_allocated 22563.71142578125 
[2025-03-22 03:06:59 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.30200058221817017 norm:0.0012096859281882644 max memory_allocated 22563.71142578125 
[2025-03-22 03:07:31 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.3017028868198395 norm:0.001192306401208043 max memory_allocated 22563.71142578125 
[2025-03-22 03:08:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.30146604776382446 norm:0.0011578660923987627 max memory_allocated 22563.71142578125 
[2025-03-22 03:08:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.3011528551578522 norm:0.001141321030445397 max memory_allocated 22563.71142578125 
[2025-03-22 03:08:46 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:09:22 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.4258190989494324 norm:0.03211846202611923 max memory_allocated 22563.88330078125 
[2025-03-22 03:09:54 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.3844375014305115 norm:0.014148300513625145 max memory_allocated 22563.88330078125 
[2025-03-22 03:10:27 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.34963080286979675 norm:0.006321395747363567 max memory_allocated 22563.88330078125 
[2025-03-22 03:11:00 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3327585458755493 norm:0.002523970091715455 max memory_allocated 22563.88330078125 
[2025-03-22 03:11:32 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.325391948223114 norm:0.0016289098421111703 max memory_allocated 22563.88330078125 
[2025-03-22 03:12:05 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3209790885448456 norm:0.0015120487660169601 max memory_allocated 22563.88330078125 
[2025-03-22 03:12:38 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3178207278251648 norm:0.0013822049368172884 max memory_allocated 22563.88330078125 
[2025-03-22 03:13:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.31560444831848145 norm:0.0013014727737754583 max memory_allocated 22563.88330078125 
[2025-03-22 03:13:43 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.31404638290405273 norm:0.0012526791542768478 max memory_allocated 22563.88330078125 
[2025-03-22 03:14:16 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.31298401951789856 norm:0.0012008369667455554 max memory_allocated 22563.88330078125 
[2025-03-22 03:14:48 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.31211650371551514 norm:0.0011310151312500238 max memory_allocated 22563.88330078125 
[2025-03-22 03:15:21 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.3115865886211395 norm:0.0011199975851923227 max memory_allocated 22563.88330078125 
[2025-03-22 03:15:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.3112092614173889 norm:0.0011209931690245867 max memory_allocated 22563.88330078125 
[2025-03-22 03:16:26 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.31075796484947205 norm:0.0010892574209719896 max memory_allocated 22563.88330078125 
[2025-03-22 03:16:59 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3104337751865387 norm:0.0010891801211982965 max memory_allocated 22563.88330078125 
[2025-03-22 03:17:31 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.3102353811264038 norm:0.0010665601585060358 max memory_allocated 22563.88330078125 
[2025-03-22 03:18:04 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.30995863676071167 norm:0.0010450624395161867 max memory_allocated 22563.88330078125 
[2025-03-22 03:18:37 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3097614645957947 norm:0.00101678806822747 max memory_allocated 22563.88330078125 
[2025-03-22 03:19:09 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.30959954857826233 norm:0.0010058796033263206 max memory_allocated 22563.88330078125 
[2025-03-22 03:19:42 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.30959561467170715 norm:0.0009977197041735053 max memory_allocated 22563.88330078125 
[2025-03-22 03:19:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 03:20:26 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.43040308356285095 norm:0.022852441295981407 max memory_allocated 22564.05517578125 
[2025-03-22 03:20:59 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.37900030612945557 norm:0.008862229064106941 max memory_allocated 22564.05517578125 
[2025-03-22 03:21:32 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3473668694496155 norm:0.004416692070662975 max memory_allocated 22564.05517578125 
[2025-03-22 03:22:04 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.3340389132499695 norm:0.0025329405907541513 max memory_allocated 22564.05517578125 
[2025-03-22 03:22:37 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.3275997042655945 norm:0.0017271032556891441 max memory_allocated 22564.05517578125 
[2025-03-22 03:23:10 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.32387715578079224 norm:0.0015542448963969946 max memory_allocated 22564.05517578125 
[2025-03-22 03:23:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.3213791251182556 norm:0.0014449791051447392 max memory_allocated 22564.05517578125 
[2025-03-22 03:24:15 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3195119798183441 norm:0.0013410956598818302 max memory_allocated 22564.05517578125 
[2025-03-22 03:24:48 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.3180728256702423 norm:0.0012536211870610714 max memory_allocated 22564.05517578125 
[2025-03-22 03:25:21 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.3171446919441223 norm:0.0012329714372754097 max memory_allocated 22564.05517578125 
[2025-03-22 03:25:54 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.31640130281448364 norm:0.0011707855155691504 max memory_allocated 22564.05517578125 
[2025-03-22 03:26:26 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.31583738327026367 norm:0.0011326775420457125 max memory_allocated 22564.05517578125 
[2025-03-22 03:26:59 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3154807686805725 norm:0.0010994173353537917 max memory_allocated 22564.05517578125 
[2025-03-22 03:27:32 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.31515470147132874 norm:0.0010627561714500189 max memory_allocated 22564.05517578125 
[2025-03-22 03:28:05 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.31489095091819763 norm:0.001042383722960949 max memory_allocated 22564.05517578125 
[2025-03-22 03:28:38 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.3146298825740814 norm:0.001016729511320591 max memory_allocated 22564.05517578125 
[2025-03-22 03:29:10 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.31441250443458557 norm:0.0009848560439422727 max memory_allocated 22564.05517578125 
[2025-03-22 03:29:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.31429702043533325 norm:0.0009780866093933582 max memory_allocated 22564.05517578125 
[2025-03-22 03:30:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.3142450749874115 norm:0.0009586138185113668 max memory_allocated 22564.05517578125 
[2025-03-22 03:30:48 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.31413382291793823 norm:0.0009524409542791545 max memory_allocated 22564.05517578125 
[2025-03-22 03:30:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 03:31:33 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.41583913564682007 norm:0.013947542756795883 max memory_allocated 22564.22705078125 
[2025-03-22 03:32:05 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.37821513414382935 norm:0.005626874044537544 max memory_allocated 22564.22705078125 
[2025-03-22 03:32:38 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.3497427999973297 norm:0.0026480290107429028 max memory_allocated 22564.22705078125 
[2025-03-22 03:33:11 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.33806538581848145 norm:0.0018196750897914171 max memory_allocated 22564.22705078125 
[2025-03-22 03:33:43 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.3324401080608368 norm:0.0014059771783649921 max memory_allocated 22564.22705078125 
[2025-03-22 03:34:16 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.3291803002357483 norm:0.0012535173445940018 max memory_allocated 22564.22705078125 
[2025-03-22 03:34:48 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.3269203305244446 norm:0.001151663949713111 max memory_allocated 22564.22705078125 
[2025-03-22 03:35:21 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.3253721296787262 norm:0.001095723477192223 max memory_allocated 22564.22705078125 
[2025-03-22 03:35:53 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.3242340087890625 norm:0.0010358490981161594 max memory_allocated 22564.22705078125 
[2025-03-22 03:36:26 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.32331582903862 norm:0.000993075198493898 max memory_allocated 22564.22705078125 
[2025-03-22 03:36:59 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.3227229416370392 norm:0.0009502985049039125 max memory_allocated 22564.22705078125 
[2025-03-22 03:37:31 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.32217636704444885 norm:0.0009350855252705514 max memory_allocated 22564.22705078125 
[2025-03-22 03:38:04 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.3217634856700897 norm:0.0009136101580224931 max memory_allocated 22564.22705078125 
[2025-03-22 03:38:37 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.3214470446109772 norm:0.0008875713683664799 max memory_allocated 22564.22705078125 
[2025-03-22 03:39:09 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.3212202191352844 norm:0.0008651094976812601 max memory_allocated 22564.22705078125 
[2025-03-22 03:39:42 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.3210739493370056 norm:0.0008469319436699152 max memory_allocated 22564.22705078125 
[2025-03-22 03:40:15 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.3209731876850128 norm:0.0008516529924236238 max memory_allocated 22564.22705078125 
[2025-03-22 03:40:48 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.32080379128456116 norm:0.0008280939655378461 max memory_allocated 22564.22705078125 
[2025-03-22 03:41:20 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.3207051455974579 norm:0.0008157634292729199 max memory_allocated 22564.22705078125 
[2025-03-22 03:41:53 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.3205971419811249 norm:0.0008065864676609635 max memory_allocated 22564.22705078125 
[2025-03-22 03:42:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 03:42:38 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.4466049373149872 norm:0.044904157519340515 max memory_allocated 22564.39892578125 
[2025-03-22 03:43:10 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.38986003398895264 norm:0.016709573566913605 max memory_allocated 22564.39892578125 
[2025-03-22 03:43:43 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.35264939069747925 norm:0.007096432149410248 max memory_allocated 22564.39892578125 
[2025-03-22 03:44:16 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.3406127691268921 norm:0.00421410845592618 max memory_allocated 22564.39892578125 
[2025-03-22 03:44:49 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.3338882327079773 norm:0.0027055596001446247 max memory_allocated 22564.39892578125 
[2025-03-22 03:45:21 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3297235369682312 norm:0.0021724896505475044 max memory_allocated 22564.39892578125 
[2025-03-22 03:45:54 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3267115652561188 norm:0.0019328525522723794 max memory_allocated 22564.39892578125 
[2025-03-22 03:46:27 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.32432085275650024 norm:0.0016977303894236684 max memory_allocated 22564.39892578125 
[2025-03-22 03:46:59 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.32254794239997864 norm:0.0014409641735255718 max memory_allocated 22564.39892578125 
[2025-03-22 03:47:32 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.32120856642723083 norm:0.0013319110730662942 max memory_allocated 22564.39892578125 
[2025-03-22 03:48:05 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3202241063117981 norm:0.0012439591810107231 max memory_allocated 22564.39892578125 
[2025-03-22 03:48:37 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.31954526901245117 norm:0.0012292811879888177 max memory_allocated 22564.39892578125 
[2025-03-22 03:49:10 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.3190461993217468 norm:0.0012141258921474218 max memory_allocated 22564.39892578125 
[2025-03-22 03:49:43 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.31845730543136597 norm:0.001163804205134511 max memory_allocated 22564.39892578125 
[2025-03-22 03:50:15 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.3180476427078247 norm:0.0011234042467549443 max memory_allocated 22564.39892578125 
[2025-03-22 03:50:48 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3176981806755066 norm:0.0010727300541475415 max memory_allocated 22564.39892578125 
[2025-03-22 03:51:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.31748682260513306 norm:0.001044761622324586 max memory_allocated 22564.39892578125 
[2025-03-22 03:51:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.317237913608551 norm:0.0009969145758077502 max memory_allocated 22564.39892578125 
[2025-03-22 03:52:26 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.3170209527015686 norm:0.0009868438355624676 max memory_allocated 22564.39892578125 
[2025-03-22 03:52:58 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.3168596923351288 norm:0.0009752369951456785 max memory_allocated 22564.39892578125 
[2025-03-22 03:53:07 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 03:53:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.40884941816329956 norm:0.015433820895850658 max memory_allocated 22564.57080078125 
[2025-03-22 03:54:15 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.37561488151550293 norm:0.006902470253407955 max memory_allocated 22564.57080078125 
[2025-03-22 03:54:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.34886226058006287 norm:0.0029051206074655056 max memory_allocated 22564.57080078125 
[2025-03-22 03:55:20 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.33917319774627686 norm:0.0017493118066340685 max memory_allocated 22564.57080078125 
[2025-03-22 03:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.3345549404621124 norm:0.0014188530622050166 max memory_allocated 22564.57080078125 
[2025-03-22 03:56:26 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3315398097038269 norm:0.001264518708921969 max memory_allocated 22564.57080078125 
[2025-03-22 03:56:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.32939305901527405 norm:0.0011544103035703301 max memory_allocated 22564.57080078125 
[2025-03-22 03:57:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.32780206203460693 norm:0.0010865245712921023 max memory_allocated 22564.57080078125 
[2025-03-22 03:58:04 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3265650272369385 norm:0.001008525025099516 max memory_allocated 22564.57080078125 
[2025-03-22 03:58:37 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.32568275928497314 norm:0.0009713087929412723 max memory_allocated 22564.57080078125 
[2025-03-22 03:59:09 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.3249542713165283 norm:0.0009456687257625163 max memory_allocated 22564.57080078125 
[2025-03-22 03:59:42 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.3243371844291687 norm:0.0009045261540450156 max memory_allocated 22564.57080078125 
[2025-03-22 04:00:15 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.3239113688468933 norm:0.0008774545858614147 max memory_allocated 22564.57080078125 
[2025-03-22 04:00:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.32357171177864075 norm:0.0008580751600675285 max memory_allocated 22564.57080078125 
[2025-03-22 04:01:20 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.3233316242694855 norm:0.0008392700692638755 max memory_allocated 22564.57080078125 
[2025-03-22 04:01:53 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.32309195399284363 norm:0.0008235800196416676 max memory_allocated 22564.57080078125 
[2025-03-22 04:02:26 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.32288074493408203 norm:0.0008252841071225703 max memory_allocated 22564.57080078125 
[2025-03-22 04:02:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.3226984441280365 norm:0.0008209276711568236 max memory_allocated 22564.57080078125 
[2025-03-22 04:03:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.32257866859436035 norm:0.0008168595377355814 max memory_allocated 22564.57080078125 
[2025-03-22 04:04:04 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.3224183917045593 norm:0.0008194750989787281 max memory_allocated 22564.57080078125 
[2025-03-22 04:04:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 04:04:48 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.44826704263687134 norm:0.03877364471554756 max memory_allocated 22564.74267578125 
[2025-03-22 04:05:21 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.39671945571899414 norm:0.014864693395793438 max memory_allocated 22564.74267578125 
[2025-03-22 04:05:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.3569950461387634 norm:0.0055709523148834705 max memory_allocated 22564.74267578125 
[2025-03-22 04:06:26 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.3435385823249817 norm:0.002692923415452242 max memory_allocated 22564.74267578125 
[2025-03-22 04:06:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.33823007345199585 norm:0.0018630314152687788 max memory_allocated 22564.74267578125 
[2025-03-22 04:07:31 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.3348572552204132 norm:0.0017674015834927559 max memory_allocated 22564.74267578125 
[2025-03-22 04:08:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.33232760429382324 norm:0.0016496172174811363 max memory_allocated 22564.74267578125 
[2025-03-22 04:08:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.3304464817047119 norm:0.0015808334574103355 max memory_allocated 22564.74267578125 
[2025-03-22 04:09:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.3291015923023224 norm:0.0015397151000797749 max memory_allocated 22564.74267578125 
[2025-03-22 04:09:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.32804399728775024 norm:0.00143123185262084 max memory_allocated 22564.74267578125 
[2025-03-22 04:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.32709676027297974 norm:0.0013529780553653836 max memory_allocated 22564.74267578125 
[2025-03-22 04:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.32638004422187805 norm:0.0012570428662002087 max memory_allocated 22564.74267578125 
[2025-03-22 04:11:19 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.3258037567138672 norm:0.0011911960318684578 max memory_allocated 22564.74267578125 
[2025-03-22 04:11:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.3253093361854553 norm:0.0011240530293434858 max memory_allocated 22564.74267578125 
[2025-03-22 04:12:24 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.324873149394989 norm:0.001102872658520937 max memory_allocated 22564.74267578125 
[2025-03-22 04:12:57 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.32447683811187744 norm:0.0010546499397605658 max memory_allocated 22564.74267578125 
[2025-03-22 04:13:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.32409876585006714 norm:0.001005930476821959 max memory_allocated 22564.74267578125 
[2025-03-22 04:14:02 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.3238595426082611 norm:0.0009998988825827837 max memory_allocated 22564.74267578125 
[2025-03-22 04:14:35 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.3236236572265625 norm:0.0009740860550664365 max memory_allocated 22564.74267578125 
[2025-03-22 04:15:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.3234357535839081 norm:0.0009591636480763555 max memory_allocated 22564.74267578125 
[2025-03-22 04:15:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 04:15:52 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.45398396253585815 norm:0.042565979063510895 max memory_allocated 22564.91455078125 
[2025-03-22 04:16:25 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.4102233648300171 norm:0.018766533583402634 max memory_allocated 22564.91455078125 
[2025-03-22 04:16:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.37203529477119446 norm:0.00808457937091589 max memory_allocated 22564.91455078125 
[2025-03-22 04:17:30 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.3547472357749939 norm:0.003524288535118103 max memory_allocated 22564.91455078125 
[2025-03-22 04:18:03 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.34887826442718506 norm:0.002378646517172456 max memory_allocated 22564.91455078125 
[2025-03-22 04:18:36 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.34561410546302795 norm:0.0020995757076889277 max memory_allocated 22564.91455078125 
[2025-03-22 04:19:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.3430401682853699 norm:0.0019066223176196218 max memory_allocated 22564.91455078125 
[2025-03-22 04:19:41 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.34091538190841675 norm:0.0017541679553687572 max memory_allocated 22564.91455078125 
[2025-03-22 04:20:14 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.33926692605018616 norm:0.001623361837118864 max memory_allocated 22564.91455078125 
[2025-03-22 04:20:46 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.3378974199295044 norm:0.0015503347385674715 max memory_allocated 22564.91455078125 
[2025-03-22 04:21:19 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.3367611765861511 norm:0.0014965813606977463 max memory_allocated 22564.91455078125 
[2025-03-22 04:21:51 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.3359837532043457 norm:0.001444704830646515 max memory_allocated 22564.91455078125 
[2025-03-22 04:22:24 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.33508870005607605 norm:0.001383633934892714 max memory_allocated 22564.91455078125 
[2025-03-22 04:22:56 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.3343648910522461 norm:0.0013411490945145488 max memory_allocated 22564.91455078125 
[2025-03-22 04:23:29 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.33381375670433044 norm:0.0013021397171542048 max memory_allocated 22564.91455078125 
[2025-03-22 04:24:02 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.3332890272140503 norm:0.0012488337233662605 max memory_allocated 22564.91455078125 
[2025-03-22 04:24:34 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.33277609944343567 norm:0.0011974646477028728 max memory_allocated 22564.91455078125 
[2025-03-22 04:25:07 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.3324531614780426 norm:0.0011426359415054321 max memory_allocated 22564.91455078125 
[2025-03-22 04:25:39 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.33219924569129944 norm:0.0010927676921710372 max memory_allocated 22564.91455078125 
[2025-03-22 04:26:12 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.33188343048095703 norm:0.00103087502066046 max memory_allocated 22564.91455078125 
[2025-03-22 04:26:21 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 04:26:56 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.42998069524765015 norm:0.031789109110832214 max memory_allocated 22565.08642578125 
[2025-03-22 04:27:29 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.4002937972545624 norm:0.015615347772836685 max memory_allocated 22565.08642578125 
[2025-03-22 04:28:02 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.3669520914554596 norm:0.0045919269323349 max memory_allocated 22565.08642578125 
[2025-03-22 04:28:34 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.3568354547023773 norm:0.002058811718598008 max memory_allocated 22565.08642578125 
[2025-03-22 04:29:07 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.3536214828491211 norm:0.0018052125815302134 max memory_allocated 22565.08642578125 
[2025-03-22 04:29:40 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.35123324394226074 norm:0.00164453883189708 max memory_allocated 22565.08642578125 
[2025-03-22 04:30:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.3492557406425476 norm:0.001508174347691238 max memory_allocated 22565.08642578125 
[2025-03-22 04:30:45 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.3477955162525177 norm:0.0014344496885314584 max memory_allocated 22565.08642578125 
[2025-03-22 04:31:18 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.346628874540329 norm:0.00139690563082695 max memory_allocated 22565.08642578125 
[2025-03-22 04:31:51 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.34563058614730835 norm:0.0013516864273697138 max memory_allocated 22565.08642578125 
[2025-03-22 04:32:23 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.34477683901786804 norm:0.0012888641795143485 max memory_allocated 22565.08642578125 
[2025-03-22 04:32:56 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.3441402316093445 norm:0.0012480608420446515 max memory_allocated 22565.08642578125 
[2025-03-22 04:33:29 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.34364840388298035 norm:0.001197217614389956 max memory_allocated 22565.08642578125 
[2025-03-22 04:34:01 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.34321433305740356 norm:0.001153049524873495 max memory_allocated 22565.08642578125 
[2025-03-22 04:34:34 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.3427469730377197 norm:0.001082149799913168 max memory_allocated 22565.08642578125 
[2025-03-22 04:35:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.342460036277771 norm:0.0010708167683333158 max memory_allocated 22565.08642578125 
[2025-03-22 04:35:39 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.34211841225624084 norm:0.0010247748577967286 max memory_allocated 22565.08642578125 
[2025-03-22 04:36:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.34186866879463196 norm:0.0009995389264076948 max memory_allocated 22565.08642578125 
[2025-03-22 04:36:44 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.34162354469299316 norm:0.0009915317641571164 max memory_allocated 22565.08642578125 
[2025-03-22 04:37:17 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.34141698479652405 norm:0.00097610306693241 max memory_allocated 22565.08642578125 
[2025-03-22 04:37:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 04:38:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.4584924280643463 norm:0.04698725789785385 max memory_allocated 22565.25830078125 
[2025-03-22 04:38:33 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.427079975605011 norm:0.019492853432893753 max memory_allocated 22565.25830078125 
[2025-03-22 04:39:06 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.39533311128616333 norm:0.00747937336564064 max memory_allocated 22565.25830078125 
[2025-03-22 04:39:38 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.38143450021743774 norm:0.0033899638801813126 max memory_allocated 22565.25830078125 
[2025-03-22 04:40:11 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.3759647011756897 norm:0.0019261087290942669 max memory_allocated 22565.25830078125 
[2025-03-22 04:40:44 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.37305948138237 norm:0.001375845866277814 max memory_allocated 22565.25830078125 
[2025-03-22 04:41:16 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.3712485432624817 norm:0.0013376041315495968 max memory_allocated 22565.25830078125 
[2025-03-22 04:41:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.3697527050971985 norm:0.0012988211819902062 max memory_allocated 22565.25830078125 
[2025-03-22 04:42:22 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.3684486746788025 norm:0.0012500530574470758 max memory_allocated 22565.25830078125 
[2025-03-22 04:42:54 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.3674914836883545 norm:0.001230372698046267 max memory_allocated 22565.25830078125 
[2025-03-22 04:43:27 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.3666633069515228 norm:0.0012029155623167753 max memory_allocated 22565.25830078125 
[2025-03-22 04:44:00 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.3660683333873749 norm:0.0011772705474868417 max memory_allocated 22565.25830078125 
[2025-03-22 04:44:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.3654642105102539 norm:0.0011164521565660834 max memory_allocated 22565.25830078125 
[2025-03-22 04:45:05 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.3651018738746643 norm:0.0010958238271996379 max memory_allocated 22565.25830078125 
[2025-03-22 04:45:38 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.3648032248020172 norm:0.001056402805261314 max memory_allocated 22565.25830078125 
[2025-03-22 04:46:11 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.36439022421836853 norm:0.0010089698480442166 max memory_allocated 22565.25830078125 
[2025-03-22 04:46:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.36391645669937134 norm:0.0009815582307055593 max memory_allocated 22565.25830078125 
[2025-03-22 04:47:16 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.363646537065506 norm:0.0009728655568324029 max memory_allocated 22565.25830078125 
[2025-03-22 04:47:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.36353540420532227 norm:0.0009577750461176038 max memory_allocated 22565.25830078125 
[2025-03-22 04:48:21 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.3633621335029602 norm:0.0009472054662182927 max memory_allocated 22565.25830078125 
[2025-03-22 04:48:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 04:49:06 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.46440547704696655 norm:0.036996424198150635 max memory_allocated 22565.43017578125 
[2025-03-22 04:49:39 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.4393453598022461 norm:0.016578618437051773 max memory_allocated 22565.43017578125 
[2025-03-22 04:50:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.41481631994247437 norm:0.00733777042478323 max memory_allocated 22565.43017578125 
[2025-03-22 04:50:44 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.40442943572998047 norm:0.0036576406564563513 max memory_allocated 22565.43017578125 
[2025-03-22 04:51:16 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.4001595079898834 norm:0.0012265064287930727 max memory_allocated 22565.43017578125 
[2025-03-22 04:51:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.39811068773269653 norm:0.00104330584872514 max memory_allocated 22565.43017578125 
[2025-03-22 04:52:21 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.39648106694221497 norm:0.0009921231539919972 max memory_allocated 22565.43017578125 
[2025-03-22 04:52:54 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.3950790464878082 norm:0.0009722071117721498 max memory_allocated 22565.43017578125 
[2025-03-22 04:53:27 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.3939800262451172 norm:0.0009373070788569748 max memory_allocated 22565.43017578125 
[2025-03-22 04:53:59 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.3931385576725006 norm:0.0009122998453676701 max memory_allocated 22565.43017578125 
[2025-03-22 04:54:32 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.3924230933189392 norm:0.0008695478900335729 max memory_allocated 22565.43017578125 
[2025-03-22 04:55:04 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.3919163644313812 norm:0.0008655848214402795 max memory_allocated 22565.43017578125 
[2025-03-22 04:55:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.3914664685726166 norm:0.0008464084239676595 max memory_allocated 22565.43017578125 
[2025-03-22 04:56:10 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.3910835385322571 norm:0.0008247147779911757 max memory_allocated 22565.43017578125 
[2025-03-22 04:56:42 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.3908376097679138 norm:0.0008266189834102988 max memory_allocated 22565.43017578125 
[2025-03-22 04:57:15 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.3905414938926697 norm:0.0008103710715658963 max memory_allocated 22565.43017578125 
[2025-03-22 04:57:48 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.39028048515319824 norm:0.0007892494322732091 max memory_allocated 22565.43017578125 
[2025-03-22 04:58:20 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.3900075852870941 norm:0.0007884615333750844 max memory_allocated 22565.43017578125 
[2025-03-22 04:58:53 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.38981834053993225 norm:0.0007918376359157264 max memory_allocated 22565.43017578125 
[2025-03-22 04:59:26 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.3896617591381073 norm:0.0007856361335143447 max memory_allocated 22565.43017578125 
[2025-03-22 04:59:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 05:00:10 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.50336754322052 norm:0.02535783313214779 max memory_allocated 22565.60205078125 
[2025-03-22 05:00:43 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.47873610258102417 norm:0.011984046548604965 max memory_allocated 22565.60205078125 
[2025-03-22 05:01:16 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.45700404047966003 norm:0.0059081572107970715 max memory_allocated 22565.60205078125 
[2025-03-22 05:01:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.4466368854045868 norm:0.0033577047288417816 max memory_allocated 22565.60205078125 
[2025-03-22 05:02:21 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.44292208552360535 norm:0.0025538806803524494 max memory_allocated 22565.60205078125 
[2025-03-22 05:02:54 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.4398380517959595 norm:0.0016213511116802692 max memory_allocated 22565.60205078125 
[2025-03-22 05:03:27 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.43750929832458496 norm:0.0014345240779221058 max memory_allocated 22565.60205078125 
[2025-03-22 05:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.43592649698257446 norm:0.0013315390096977353 max memory_allocated 22565.60205078125 
[2025-03-22 05:04:32 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.43459054827690125 norm:0.0012421725550666451 max memory_allocated 22565.60205078125 
[2025-03-22 05:05:04 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.4336787164211273 norm:0.0011821254156529903 max memory_allocated 22565.60205078125 
[2025-03-22 05:05:37 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.43276068568229675 norm:0.0011067652376368642 max memory_allocated 22565.60205078125 
[2025-03-22 05:06:10 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.43213972449302673 norm:0.0010778633877635002 max memory_allocated 22565.60205078125 
[2025-03-22 05:06:42 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.4316384494304657 norm:0.0010670883348211646 max memory_allocated 22565.60205078125 
[2025-03-22 05:07:15 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.43123766779899597 norm:0.0010735756950452924 max memory_allocated 22565.60205078125 
[2025-03-22 05:07:47 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.43082407116889954 norm:0.0010622397530823946 max memory_allocated 22565.60205078125 
[2025-03-22 05:08:20 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.43044957518577576 norm:0.0010451971320435405 max memory_allocated 22565.60205078125 
[2025-03-22 05:08:53 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.4302140772342682 norm:0.0010514537570998073 max memory_allocated 22565.60205078125 
[2025-03-22 05:09:25 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.4299674928188324 norm:0.0010176963405683637 max memory_allocated 22565.60205078125 
[2025-03-22 05:09:58 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.4296818673610687 norm:0.0009975119028240442 max memory_allocated 22565.60205078125 
[2025-03-22 05:10:30 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.42945292592048645 norm:0.0009833122603595257 max memory_allocated 22565.60205078125 
[2025-03-22 05:10:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 05:11:15 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.521881103515625 norm:0.017538581043481827 max memory_allocated 22565.77392578125 
[2025-03-22 05:11:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.504503071308136 norm:0.00761025957763195 max memory_allocated 22565.77392578125 
[2025-03-22 05:12:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.49046647548675537 norm:0.0037533524446189404 max memory_allocated 22565.77392578125 
[2025-03-22 05:12:52 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.48319682478904724 norm:0.0014432654716074467 max memory_allocated 22565.77392578125 
[2025-03-22 05:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.4804261028766632 norm:0.0009816399542614818 max memory_allocated 22565.77392578125 
[2025-03-22 05:13:58 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.4787256419658661 norm:0.0009309849701821804 max memory_allocated 22565.77392578125 
[2025-03-22 05:14:31 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.477243572473526 norm:0.0009082214091904461 max memory_allocated 22565.77392578125 
[2025-03-22 05:15:03 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.47589993476867676 norm:0.0008618385763838887 max memory_allocated 22565.77392578125 
[2025-03-22 05:15:36 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.47495219111442566 norm:0.0008288492681458592 max memory_allocated 22565.77392578125 
[2025-03-22 05:16:09 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.47419440746307373 norm:0.0008059144602157176 max memory_allocated 22565.77392578125 
[2025-03-22 05:16:41 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.4735841751098633 norm:0.0007807549554854631 max memory_allocated 22565.77392578125 
[2025-03-22 05:17:14 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.47313112020492554 norm:0.0007840714533813298 max memory_allocated 22565.77392578125 
[2025-03-22 05:17:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.47274401783943176 norm:0.000753488449845463 max memory_allocated 22565.77392578125 
[2025-03-22 05:18:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.47241806983947754 norm:0.000734774861484766 max memory_allocated 22565.77392578125 
[2025-03-22 05:18:52 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.47219300270080566 norm:0.0007258764235302806 max memory_allocated 22565.77392578125 
[2025-03-22 05:19:25 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.4719765782356262 norm:0.0007286801701411605 max memory_allocated 22565.77392578125 
[2025-03-22 05:19:57 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.4718446433544159 norm:0.0007325099431909621 max memory_allocated 22565.77392578125 
[2025-03-22 05:20:30 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.47170692682266235 norm:0.000733417458832264 max memory_allocated 22565.77392578125 
[2025-03-22 05:21:03 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.47152525186538696 norm:0.0007260344573296607 max memory_allocated 22565.77392578125 
[2025-03-22 05:21:35 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.47146013379096985 norm:0.0007230885676108301 max memory_allocated 22565.77392578125 
[2025-03-22 05:21:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 05:22:19 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:0.5863926410675049 norm:0.015294460579752922 max memory_allocated 22565.94580078125 
[2025-03-22 05:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:0.5662932991981506 norm:0.005788164213299751 max memory_allocated 22565.94580078125 
[2025-03-22 05:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:0.5508230328559875 norm:0.0028294199146330357 max memory_allocated 22565.94580078125 
[2025-03-22 05:23:57 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:0.5441170334815979 norm:0.00137607054784894 max memory_allocated 22565.94580078125 
[2025-03-22 05:24:30 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:0.5413781404495239 norm:0.001142954919487238 max memory_allocated 22565.94580078125 
[2025-03-22 05:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:0.5396779775619507 norm:0.0011036789510399103 max memory_allocated 22565.94580078125 
[2025-03-22 05:25:35 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:0.5382483601570129 norm:0.0010468489490449429 max memory_allocated 22565.94580078125 
[2025-03-22 05:26:07 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:0.5369327068328857 norm:0.001054111635312438 max memory_allocated 22565.94580078125 
[2025-03-22 05:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:0.5360669493675232 norm:0.0009910153457894921 max memory_allocated 22565.94580078125 
[2025-03-22 05:27:12 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:0.5353201627731323 norm:0.0009918016148731112 max memory_allocated 22565.94580078125 
[2025-03-22 05:27:45 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.5346925854682922 norm:0.0009656602051109076 max memory_allocated 22565.94580078125 
[2025-03-22 05:28:18 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.5341618657112122 norm:0.0009635153692215681 max memory_allocated 22565.94580078125 
[2025-03-22 05:28:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.5337804555892944 norm:0.000923610758036375 max memory_allocated 22565.94580078125 
[2025-03-22 05:29:23 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.5334402322769165 norm:0.0008884575800038874 max memory_allocated 22565.94580078125 
[2025-03-22 05:29:56 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.5330402851104736 norm:0.0008427819120697677 max memory_allocated 22565.94580078125 
[2025-03-22 05:30:28 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.5328409075737 norm:0.0009088423103094101 max memory_allocated 22565.94580078125 
[2025-03-22 05:31:01 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.5328170657157898 norm:0.000855207210406661 max memory_allocated 22565.94580078125 
[2025-03-22 05:31:34 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.5323470830917358 norm:0.000841137021780014 max memory_allocated 22565.94580078125 
[2025-03-22 05:32:06 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.5323848724365234 norm:0.0008914002683013678 max memory_allocated 22565.94580078125 
[2025-03-22 05:32:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.5321585536003113 norm:0.0008811814477667212 max memory_allocated 22565.94580078125 
[2025-03-22 05:32:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 05:33:24 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:0.6356031894683838 norm:0.007766998838633299 max memory_allocated 22566.11767578125 
[2025-03-22 05:33:56 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:0.6218187212944031 norm:0.003890533233061433 max memory_allocated 22566.11767578125 
[2025-03-22 05:34:29 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:0.6080141663551331 norm:0.002158069284632802 max memory_allocated 22566.11767578125 
[2025-03-22 05:35:02 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:0.6027743816375732 norm:0.0012961970642209053 max memory_allocated 22566.11767578125 
[2025-03-22 05:35:34 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:0.6001514196395874 norm:0.0010005043586716056 max memory_allocated 22566.11767578125 
[2025-03-22 05:36:07 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:0.5982261896133423 norm:0.0008517423993907869 max memory_allocated 22566.11767578125 
[2025-03-22 05:36:40 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:0.5966739654541016 norm:0.000786789576523006 max memory_allocated 22566.11767578125 
[2025-03-22 05:37:12 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:0.5954402685165405 norm:0.0007587760919705033 max memory_allocated 22566.11767578125 
[2025-03-22 05:37:45 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:0.5945643186569214 norm:0.0007292605005204678 max memory_allocated 22566.11767578125 
[2025-03-22 05:38:17 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:0.5938920974731445 norm:0.0007068361737765372 max memory_allocated 22566.11767578125 
[2025-03-22 05:38:50 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:0.5934040546417236 norm:0.0006907989154569805 max memory_allocated 22566.11767578125 
[2025-03-22 05:39:22 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:0.5930300951004028 norm:0.0006919358856976032 max memory_allocated 22566.11767578125 
[2025-03-22 05:39:55 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:0.5927140712738037 norm:0.0006770799518562853 max memory_allocated 22566.11767578125 
[2025-03-22 05:40:27 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:0.5924931764602661 norm:0.000685655977576971 max memory_allocated 22566.11767578125 
[2025-03-22 05:41:00 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:0.5922976136207581 norm:0.0006726921419613063 max memory_allocated 22566.11767578125 
[2025-03-22 05:41:33 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:0.5920953750610352 norm:0.0006697014905512333 max memory_allocated 22566.11767578125 
[2025-03-22 05:42:05 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:0.5919056534767151 norm:0.0006546996301040053 max memory_allocated 22566.11767578125 
[2025-03-22 05:42:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:0.5917154550552368 norm:0.0006441905279643834 max memory_allocated 22566.11767578125 
[2025-03-22 05:43:10 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:0.5915194749832153 norm:0.0006384482840076089 max memory_allocated 22566.11767578125 
[2025-03-22 05:43:43 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:0.5914562344551086 norm:0.0006328413728624582 max memory_allocated 22566.11767578125 
[2025-03-22 05:43:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 05:44:27 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:0.7112130522727966 norm:0.012566586956381798 max memory_allocated 22566.28955078125 
[2025-03-22 05:45:00 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:0.6940573453903198 norm:0.005509247072041035 max memory_allocated 22566.28955078125 
[2025-03-22 05:45:33 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:0.6786034107208252 norm:0.0028771283105015755 max memory_allocated 22566.28955078125 
[2025-03-22 05:46:05 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:0.6717715263366699 norm:0.0013495134189724922 max memory_allocated 22566.28955078125 
[2025-03-22 05:46:38 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:0.6687771677970886 norm:0.0008776579052209854 max memory_allocated 22566.28955078125 
[2025-03-22 05:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:0.6666265726089478 norm:0.0008259603637270629 max memory_allocated 22566.28955078125 
[2025-03-22 05:47:43 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:0.664825439453125 norm:0.0008007739088498056 max memory_allocated 22566.28955078125 
[2025-03-22 05:48:16 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:0.6634404063224792 norm:0.0008127369219437242 max memory_allocated 22566.28955078125 
[2025-03-22 05:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:0.6625018119812012 norm:0.0008118078694678843 max memory_allocated 22566.28955078125 
[2025-03-22 05:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:0.6617292165756226 norm:0.0007834485149942338 max memory_allocated 22566.28955078125 
[2025-03-22 05:49:54 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:0.661232590675354 norm:0.000769983627833426 max memory_allocated 22566.28955078125 
[2025-03-22 05:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:0.6607297658920288 norm:0.0007610220345668495 max memory_allocated 22566.28955078125 
[2025-03-22 05:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:0.6603987812995911 norm:0.0007559933001175523 max memory_allocated 22566.28955078125 
[2025-03-22 05:51:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:0.6600908637046814 norm:0.0007525506080128253 max memory_allocated 22566.28955078125 
[2025-03-22 05:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:0.6599302887916565 norm:0.000762281590141356 max memory_allocated 22566.28955078125 
[2025-03-22 05:52:38 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:0.6597000360488892 norm:0.0007548984722234309 max memory_allocated 22566.28955078125 
[2025-03-22 05:53:10 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:0.659461498260498 norm:0.0007476358441635966 max memory_allocated 22566.28955078125 
[2025-03-22 05:53:43 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:0.659301221370697 norm:0.0007436461164616048 max memory_allocated 22566.28955078125 
[2025-03-22 05:54:15 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:0.6591596603393555 norm:0.0007468921248801053 max memory_allocated 22566.28955078125 
[2025-03-22 05:54:48 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:0.6590248942375183 norm:0.0007506386027671397 max memory_allocated 22566.28955078125 
[2025-03-22 05:54:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 05:55:32 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:0.801287055015564 norm:0.022927217185497284 max memory_allocated 22566.46142578125 
[2025-03-22 05:56:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:0.7856861352920532 norm:0.013929881155490875 max memory_allocated 22566.46142578125 
[2025-03-22 05:56:37 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:0.7685189843177795 norm:0.00874344166368246 max memory_allocated 22566.46142578125 
[2025-03-22 05:57:10 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:0.762047290802002 norm:0.006358700804412365 max memory_allocated 22566.46142578125 
[2025-03-22 05:57:42 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:0.7590779662132263 norm:0.004968821071088314 max memory_allocated 22566.46142578125 
[2025-03-22 05:58:15 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:0.7539539337158203 norm:0.0033713560551404953 max memory_allocated 22566.46142578125 
[2025-03-22 05:58:47 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:0.7511880993843079 norm:0.003254355862736702 max memory_allocated 22566.46142578125 
[2025-03-22 05:59:20 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:0.7495638132095337 norm:0.0030103011522442102 max memory_allocated 22566.46142578125 
[2025-03-22 05:59:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:0.7484945058822632 norm:0.0027991251554340124 max memory_allocated 22566.46142578125 
[2025-03-22 06:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:0.7471813559532166 norm:0.0022641136310994625 max memory_allocated 22566.46142578125 
[2025-03-22 06:00:58 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:0.7467114329338074 norm:0.0023749324027448893 max memory_allocated 22566.46142578125 
[2025-03-22 06:01:31 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:0.7456678748130798 norm:0.0019925576634705067 max memory_allocated 22566.46142578125 
[2025-03-22 06:02:03 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:0.7451845407485962 norm:0.0017907239962369204 max memory_allocated 22566.46142578125 
[2025-03-22 06:02:36 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:0.7449907064437866 norm:0.0018833621870726347 max memory_allocated 22566.46142578125 
[2025-03-22 06:03:09 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:0.7439039945602417 norm:0.0015567002119496465 max memory_allocated 22566.46142578125 
[2025-03-22 06:03:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:0.7438363432884216 norm:0.0015571548137813807 max memory_allocated 22566.46142578125 
[2025-03-22 06:04:14 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:0.7435100078582764 norm:0.001424242276698351 max memory_allocated 22566.46142578125 
[2025-03-22 06:04:47 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:0.7428220510482788 norm:0.0013501715147867799 max memory_allocated 22566.46142578125 
[2025-03-22 06:05:19 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:0.7427557706832886 norm:0.0012978382874280214 max memory_allocated 22566.46142578125 
[2025-03-22 06:05:52 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:0.7424729466438293 norm:0.001233193906955421 max memory_allocated 22566.46142578125 
[2025-03-22 06:06:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 06:06:36 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:0.8916448354721069 norm:0.009421628899872303 max memory_allocated 22566.63330078125 
[2025-03-22 06:07:09 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:0.8734602332115173 norm:0.004750035237520933 max memory_allocated 22566.63330078125 
[2025-03-22 06:07:42 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:0.8539066314697266 norm:0.0021338011138141155 max memory_allocated 22566.63330078125 
[2025-03-22 06:08:14 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:0.8460204601287842 norm:0.0012502301251515746 max memory_allocated 22566.63330078125 
[2025-03-22 06:08:47 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:0.8427103757858276 norm:0.0010382678592577577 max memory_allocated 22566.63330078125 
[2025-03-22 06:09:19 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:0.8399389386177063 norm:0.0009700901573523879 max memory_allocated 22566.63330078125 
[2025-03-22 06:09:52 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:0.8376031517982483 norm:0.0009040028089657426 max memory_allocated 22566.63330078125 
[2025-03-22 06:10:24 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:0.8357793092727661 norm:0.0008642497123219073 max memory_allocated 22566.63330078125 
[2025-03-22 06:10:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:0.8345288634300232 norm:0.0008458012016490102 max memory_allocated 22566.63330078125 
[2025-03-22 06:11:29 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:0.8335736989974976 norm:0.000796473934315145 max memory_allocated 22566.63330078125 
[2025-03-22 06:12:02 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:0.8328893184661865 norm:0.0007784119807183743 max memory_allocated 22566.63330078125 
[2025-03-22 06:12:34 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:0.8322984576225281 norm:0.000767776626162231 max memory_allocated 22566.63330078125 
[2025-03-22 06:13:07 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:0.8318390846252441 norm:0.0007744246977381408 max memory_allocated 22566.63330078125 
[2025-03-22 06:13:40 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:0.8313930630683899 norm:0.0007856150041334331 max memory_allocated 22566.63330078125 
[2025-03-22 06:14:12 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:0.8310563564300537 norm:0.0007860767072997987 max memory_allocated 22566.63330078125 
[2025-03-22 06:14:45 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:0.8307657837867737 norm:0.0007805755012668669 max memory_allocated 22566.63330078125 
[2025-03-22 06:15:18 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:0.8305749893188477 norm:0.0007816551951691508 max memory_allocated 22566.63330078125 
[2025-03-22 06:15:50 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:0.8303427696228027 norm:0.0007802594336681068 max memory_allocated 22566.63330078125 
[2025-03-22 06:16:23 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:0.8301241993904114 norm:0.0007815991993993521 max memory_allocated 22566.63330078125 
[2025-03-22 06:16:56 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:0.8299206495285034 norm:0.0007964931428432465 max memory_allocated 22566.63330078125 
[2025-03-22 06:17:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 06:17:40 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:0.9963977336883545 norm:0.014064650051295757 max memory_allocated 22566.80517578125 
[2025-03-22 06:18:13 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:0.9762847423553467 norm:0.006700485944747925 max memory_allocated 22566.80517578125 
[2025-03-22 06:18:45 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:0.9562580585479736 norm:0.0034507005475461483 max memory_allocated 22566.80517578125 
[2025-03-22 06:19:18 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:0.947716474533081 norm:0.00193732266779989 max memory_allocated 22566.80517578125 
[2025-03-22 06:19:51 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:0.9431732296943665 norm:0.0008243739721365273 max memory_allocated 22566.80517578125 
[2025-03-22 06:20:23 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:0.9400826692581177 norm:0.000766033714171499 max memory_allocated 22566.80517578125 
[2025-03-22 06:20:56 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:0.9375287294387817 norm:0.0007281572907231748 max memory_allocated 22566.80517578125 
[2025-03-22 06:21:29 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:0.9356959462165833 norm:0.0007095648325048387 max memory_allocated 22566.80517578125 
[2025-03-22 06:22:01 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:0.9344849586486816 norm:0.0007055443711578846 max memory_allocated 22566.80517578125 
[2025-03-22 06:22:34 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:0.9335843920707703 norm:0.0007009817054495215 max memory_allocated 22566.80517578125 
[2025-03-22 06:23:06 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:0.9329038858413696 norm:0.0006850623758509755 max memory_allocated 22566.80517578125 
[2025-03-22 06:23:39 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:0.9323568940162659 norm:0.0006759538664482534 max memory_allocated 22566.80517578125 
[2025-03-22 06:24:11 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:0.931965708732605 norm:0.0006731674657203257 max memory_allocated 22566.80517578125 
[2025-03-22 06:24:44 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:0.9316096305847168 norm:0.0006713424227200449 max memory_allocated 22566.80517578125 
[2025-03-22 06:25:17 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:0.931329071521759 norm:0.0006558115710504353 max memory_allocated 22566.80517578125 
[2025-03-22 06:25:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:0.9309947490692139 norm:0.0006525169592350721 max memory_allocated 22566.80517578125 
[2025-03-22 06:26:22 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:0.930679976940155 norm:0.0006518139271065593 max memory_allocated 22566.80517578125 
[2025-03-22 06:26:54 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:0.9305441379547119 norm:0.000643159553874284 max memory_allocated 22566.80517578125 
[2025-03-22 06:27:27 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:0.9304199814796448 norm:0.0006383535801433027 max memory_allocated 22566.80517578125 
[2025-03-22 06:27:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:0.9302655458450317 norm:0.0006318435189314187 max memory_allocated 22566.80517578125 
[2025-03-22 06:28:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 06:28:12 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:28:45 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:1.1361441612243652 norm:0.038303472101688385 max memory_allocated 22567.43798828125 
[2025-03-22 06:29:18 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:1.111328125 norm:0.029592089354991913 max memory_allocated 22567.43798828125 
[2025-03-22 06:29:50 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:1.086074948310852 norm:0.020624026656150818 max memory_allocated 22567.43798828125 
[2025-03-22 06:30:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:1.0743141174316406 norm:0.015446910634636879 max memory_allocated 22567.43798828125 
[2025-03-22 06:30:56 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:1.068454384803772 norm:0.012235300615429878 max memory_allocated 22567.43798828125 
[2025-03-22 06:31:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:1.0638231039047241 norm:0.009791303426027298 max memory_allocated 22567.43798828125 
[2025-03-22 06:32:02 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:1.0604119300842285 norm:0.008646419271826744 max memory_allocated 22567.43798828125 
[2025-03-22 06:32:34 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:1.0581194162368774 norm:0.008048555813729763 max memory_allocated 22567.43798828125 
[2025-03-22 06:33:07 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:1.0564157962799072 norm:0.00767201604321599 max memory_allocated 22567.43798828125 
[2025-03-22 06:33:40 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:1.055376410484314 norm:0.00761730782687664 max memory_allocated 22567.43798828125 
[2025-03-22 06:34:13 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:1.0546720027923584 norm:0.007628268096596003 max memory_allocated 22567.43798828125 
[2025-03-22 06:34:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:1.0538561344146729 norm:0.007189282216131687 max memory_allocated 22567.43798828125 
[2025-03-22 06:35:19 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:1.0527229309082031 norm:0.006878886837512255 max memory_allocated 22567.43798828125 
[2025-03-22 06:35:51 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:1.0519492626190186 norm:0.006622841581702232 max memory_allocated 22567.43798828125 
[2025-03-22 06:36:24 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:1.0512267351150513 norm:0.006448562256991863 max memory_allocated 22567.43798828125 
[2025-03-22 06:36:57 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:1.0507996082305908 norm:0.006343526300042868 max memory_allocated 22567.43798828125 
[2025-03-22 06:37:30 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:1.0503461360931396 norm:0.006365213077515364 max memory_allocated 22567.43798828125 
[2025-03-22 06:38:02 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:1.05002760887146 norm:0.0062483842484653 max memory_allocated 22567.43798828125 
[2025-03-22 06:38:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:1.0494613647460938 norm:0.0061753978952765465 max memory_allocated 22567.43798828125 
[2025-03-22 06:39:08 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:1.0492119789123535 norm:0.0058793555945158005 max memory_allocated 22567.43798828125 
[2025-03-22 06:39:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 06:39:20 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:1.2852797508239746 norm:0.039632007479667664 max memory_allocated 22567.60986328125 
[2025-03-22 06:40:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:1.2535274028778076 norm:0.0300951786339283 max memory_allocated 22567.60986328125 
[2025-03-22 06:40:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:1.2243400812149048 norm:0.021440818905830383 max memory_allocated 22567.60986328125 
[2025-03-22 06:41:30 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:1.2090922594070435 norm:0.016089290380477905 max memory_allocated 22567.60986328125 
[2025-03-22 06:42:03 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:1.2002110481262207 norm:0.012282300740480423 max memory_allocated 22567.60986328125 
[2025-03-22 06:42:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:1.1947349309921265 norm:0.010450365021824837 max memory_allocated 22567.60986328125 
[2025-03-22 06:43:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:1.1914963722229004 norm:0.010415889322757721 max memory_allocated 22567.60986328125 
[2025-03-22 06:43:41 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:1.18863844871521 norm:0.009838600642979145 max memory_allocated 22567.60986328125 
[2025-03-22 06:44:14 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:1.186424970626831 norm:0.009128670208156109 max memory_allocated 22567.60986328125 
[2025-03-22 06:44:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:1.1853152513504028 norm:0.00902273878455162 max memory_allocated 22567.60986328125 
[2025-03-22 06:45:19 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:1.184377670288086 norm:0.009227992966771126 max memory_allocated 22567.60986328125 
[2025-03-22 06:45:52 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:1.1836862564086914 norm:0.009043865837156773 max memory_allocated 22567.60986328125 
[2025-03-22 06:46:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:1.1828460693359375 norm:0.008953168988227844 max memory_allocated 22567.60986328125 
[2025-03-22 06:46:58 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:1.1821002960205078 norm:0.00849574338644743 max memory_allocated 22567.60986328125 
[2025-03-22 06:47:30 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:1.181221842765808 norm:0.008166598156094551 max memory_allocated 22567.60986328125 
[2025-03-22 06:48:03 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:1.1806024312973022 norm:0.007894853129982948 max memory_allocated 22567.60986328125 
[2025-03-22 06:48:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:1.1798148155212402 norm:0.007502082735300064 max memory_allocated 22567.60986328125 
[2025-03-22 06:49:09 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:1.179215669631958 norm:0.007210426963865757 max memory_allocated 22567.60986328125 
[2025-03-22 06:49:42 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:1.1792596578598022 norm:0.007834745571017265 max memory_allocated 22567.60986328125 
[2025-03-22 06:50:15 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:1.1787976026535034 norm:0.007359679788351059 max memory_allocated 22567.60986328125 
[2025-03-22 06:50:24 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 06:50:27 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:50:59 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:2.8249897956848145 norm:0.6896499395370483 max memory_allocated 22567.78173828125 
[2025-03-22 06:51:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:2.981668710708618 norm:1.3073604106903076 max memory_allocated 22567.78173828125 
[2025-03-22 06:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:2.3105647563934326 norm:0.5213289260864258 max memory_allocated 22567.78173828125 
[2025-03-22 06:52:38 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:1.927528738975525 norm:0.24548402428627014 max memory_allocated 22567.78173828125 
[2025-03-22 06:53:10 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:1.8493584394454956 norm:0.1969958394765854 max memory_allocated 22567.78173828125 
[2025-03-22 06:53:43 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:1.8310211896896362 norm:0.18551234900951385 max memory_allocated 22567.78173828125 
[2025-03-22 06:54:16 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:1.8190497159957886 norm:0.1711820363998413 max memory_allocated 22567.78173828125 
[2025-03-22 06:54:49 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:1.8119195699691772 norm:0.15532782673835754 max memory_allocated 22567.78173828125 
[2025-03-22 06:55:21 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:1.793951392173767 norm:0.15390752255916595 max memory_allocated 22567.78173828125 
[2025-03-22 06:55:54 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:1.7788044214248657 norm:0.15707767009735107 max memory_allocated 22567.78173828125 
[2025-03-22 06:56:27 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:1.7614786624908447 norm:0.16082076728343964 max memory_allocated 22567.78173828125 
[2025-03-22 06:56:59 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:1.7658467292785645 norm:0.1714080423116684 max memory_allocated 22567.78173828125 
[2025-03-22 06:57:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:1.7564438581466675 norm:0.13834917545318604 max memory_allocated 22567.78173828125 
[2025-03-22 06:58:04 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:1.7470598220825195 norm:0.12917295098304749 max memory_allocated 22567.78173828125 
[2025-03-22 06:58:37 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:1.727952003479004 norm:0.10726899653673172 max memory_allocated 22567.78173828125 
[2025-03-22 06:59:10 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:1.7239105701446533 norm:0.10095220804214478 max memory_allocated 22567.78173828125 
[2025-03-22 06:59:43 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:1.7313684225082397 norm:0.10401313751935959 max memory_allocated 22567.78173828125 
[2025-03-22 07:00:15 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:1.728047490119934 norm:0.1018349826335907 max memory_allocated 22567.78173828125 
[2025-03-22 07:00:48 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:1.732820987701416 norm:0.096154123544693 max memory_allocated 22567.78173828125 
[2025-03-22 07:01:21 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:1.7353180646896362 norm:0.10173829644918442 max memory_allocated 22567.78173828125 
[2025-03-22 07:01:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 07:01:33 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 07:02:05 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:3.801068067550659 norm:0.42849162220954895 max memory_allocated 22567.95361328125 
[2025-03-22 07:02:38 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:3.4465370178222656 norm:0.3238154947757721 max memory_allocated 22567.95361328125 
[2025-03-22 07:03:11 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:3.1369333267211914 norm:0.24269887804985046 max memory_allocated 22567.95361328125 
[2025-03-22 07:03:44 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:3.0344533920288086 norm:0.21768639981746674 max memory_allocated 22567.95361328125 
[2025-03-22 07:04:17 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:2.982311248779297 norm:0.1947978436946869 max memory_allocated 22567.95361328125 
[2025-03-22 07:04:49 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:2.9438464641571045 norm:0.1685532033443451 max memory_allocated 22567.95361328125 
[2025-03-22 07:05:22 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:2.908276081085205 norm:0.151886984705925 max memory_allocated 22567.95361328125 
[2025-03-22 07:05:55 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:2.8716585636138916 norm:0.13470885157585144 max memory_allocated 22567.95361328125 
[2025-03-22 07:06:28 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:2.845851182937622 norm:0.1204356849193573 max memory_allocated 22567.95361328125 
[2025-03-22 07:07:01 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:2.8255574703216553 norm:0.11248050630092621 max memory_allocated 22567.95361328125 
[2025-03-22 07:07:33 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:2.811480760574341 norm:0.10880313068628311 max memory_allocated 22567.95361328125 
[2025-03-22 07:08:06 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:2.7939140796661377 norm:0.10234241932630539 max memory_allocated 22567.95361328125 
[2025-03-22 07:08:39 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:2.7782506942749023 norm:0.09545927494764328 max memory_allocated 22567.95361328125 
[2025-03-22 07:09:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:2.76971697807312 norm:0.09527768194675446 max memory_allocated 22567.95361328125 
[2025-03-22 07:09:44 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:2.7594411373138428 norm:0.09214593470096588 max memory_allocated 22567.95361328125 
[2025-03-22 07:10:17 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:2.750241756439209 norm:0.0869622528553009 max memory_allocated 22567.95361328125 
[2025-03-22 07:10:50 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:2.746563673019409 norm:0.08855269104242325 max memory_allocated 22567.95361328125 
[2025-03-22 07:11:22 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:2.741487503051758 norm:0.09352633357048035 max memory_allocated 22567.95361328125 
[2025-03-22 07:11:55 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:2.736783981323242 norm:0.09145655483007431 max memory_allocated 22567.95361328125 
[2025-03-22 07:12:28 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:2.7340826988220215 norm:0.09042306244373322 max memory_allocated 22567.95361328125 
[2025-03-22 07:12:37 root] (main_calibration_a.py 369): INFO 21281.93227148056
[2025-03-22 07:12:41 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 07:13:49 root] (main_calibration_a.py 158): INFO wikitext2 : 8.727204322814941
[2025-03-22 07:13:49 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 07:15:35 root] (main_calibration_a.py 158): INFO c4 : 12.140763282775879
[2025-03-22 09:10:55 root] (main_calibration_a.py 169): INFO {'wikitext2': 8.727204322814941, 'c4': 12.140763282775879, 'results': {'hellaswag': {'acc': 0.4480183230432185, 'acc_stderr': 0.004962742426849889, 'acc_norm': 0.5943039235212109, 'acc_norm_stderr': 0.004900227226433372}, 'winogrande': {'acc': 0.5761641673243884, 'acc_stderr': 0.013888492389944522}, 'arc_easy': {'acc': 0.5281986531986532, 'acc_stderr': 0.010243454104071787, 'acc_norm': 0.4385521885521885, 'acc_norm_stderr': 0.01018201027547112}, 'boolq': {'acc': 0.6330275229357798, 'acc_stderr': 0.008429864284269442}, 'arc_challenge': {'acc': 0.2738907849829352, 'acc_stderr': 0.013032004972989501, 'acc_norm': 0.3242320819112628, 'acc_norm_stderr': 0.01367881039951882}, 'piqa': {'acc': 0.6882480957562568, 'acc_stderr': 0.010807431424873674, 'acc_norm': 0.6953210010881393, 'acc_norm_stderr': 0.010738889044325161}}, 'versions': {'hellaswag': 0, 'winogrande': 0, 'arc_easy': 0, 'boolq': 1, 'arc_challenge': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 09:10:55 root] (main_calibration_a.py 172): INFO 27.39,52.82,63.30,44.80,68.82,57.62
