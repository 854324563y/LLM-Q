[2025-03-22 14:42:02 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/llama-7b-hf-w4a4-4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=4)
[2025-03-22 14:45:29 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 14:45:29 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:45:30 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 14:45:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 14:45:35 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:46:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.08034640550613403 norm:0.07751531153917313 max memory_allocated 22559.62548828125 
[2025-03-22 14:46:39 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.048909105360507965 norm:0.03720702975988388 max memory_allocated 22559.62548828125 
[2025-03-22 14:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.03945828229188919 norm:0.02789626456797123 max memory_allocated 22559.62548828125 
[2025-03-22 14:47:44 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.03585747629404068 norm:0.024658996611833572 max memory_allocated 22559.62548828125 
[2025-03-22 14:48:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.03360197693109512 norm:0.020609306171536446 max memory_allocated 22559.62548828125 
[2025-03-22 14:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.03229086473584175 norm:0.01787457801401615 max memory_allocated 22559.62548828125 
[2025-03-22 14:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.031363554298877716 norm:0.01533583551645279 max memory_allocated 22559.62548828125 
[2025-03-22 14:49:55 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.030611438676714897 norm:0.013127712532877922 max memory_allocated 22559.62548828125 
[2025-03-22 14:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.0300300270318985 norm:0.011156626045703888 max memory_allocated 22559.62548828125 
[2025-03-22 14:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.029698021709918976 norm:0.009490183554589748 max memory_allocated 22559.62548828125 
[2025-03-22 14:51:33 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.029278332367539406 norm:0.008359737694263458 max memory_allocated 22559.62548828125 
[2025-03-22 14:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.02921493910253048 norm:0.0074629876762628555 max memory_allocated 22559.62548828125 
[2025-03-22 14:52:38 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.02905905619263649 norm:0.006793065927922726 max memory_allocated 22559.62548828125 
[2025-03-22 14:53:11 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.028863362967967987 norm:0.006203771103173494 max memory_allocated 22559.62548828125 
[2025-03-22 14:53:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.028719158843159676 norm:0.0058232685551047325 max memory_allocated 22559.62548828125 
[2025-03-22 14:54:16 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.028638243675231934 norm:0.00540449982509017 max memory_allocated 22559.62548828125 
[2025-03-22 14:54:48 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.028696689754724503 norm:0.005344475153833628 max memory_allocated 22559.62548828125 
[2025-03-22 14:55:21 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.028693821281194687 norm:0.005009140819311142 max memory_allocated 22559.62548828125 
[2025-03-22 14:55:54 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.028651872649788857 norm:0.004882398992776871 max memory_allocated 22559.62548828125 
[2025-03-22 14:56:26 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.028620829805731773 norm:0.004659147001802921 max memory_allocated 22559.62548828125 
[2025-03-22 14:56:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 14:56:38 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:57:10 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.25418621301651 norm:0.176520437002182 max memory_allocated 22559.79736328125 
[2025-03-22 14:57:43 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.14423784613609314 norm:0.07177423685789108 max memory_allocated 22559.79736328125 
[2025-03-22 14:58:15 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.109761081635952 norm:0.03712049871683121 max memory_allocated 22559.79736328125 
[2025-03-22 14:58:48 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.09573211520910263 norm:0.028537720441818237 max memory_allocated 22559.79736328125 
[2025-03-22 14:59:20 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.08857666701078415 norm:0.023010488599538803 max memory_allocated 22559.79736328125 
[2025-03-22 14:59:53 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.08388543128967285 norm:0.019540999084711075 max memory_allocated 22559.79736328125 
[2025-03-22 15:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.08084049820899963 norm:0.016986971721053123 max memory_allocated 22559.79736328125 
[2025-03-22 15:00:58 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.07884366810321808 norm:0.014770681038498878 max memory_allocated 22559.79736328125 
[2025-03-22 15:01:31 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.07739627361297607 norm:0.012861407361924648 max memory_allocated 22559.79736328125 
[2025-03-22 15:02:03 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.0763273686170578 norm:0.011372053064405918 max memory_allocated 22559.79736328125 
[2025-03-22 15:02:36 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.07552729547023773 norm:0.010015852749347687 max memory_allocated 22559.79736328125 
[2025-03-22 15:03:09 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.07478979229927063 norm:0.008828055113554 max memory_allocated 22559.79736328125 
[2025-03-22 15:03:41 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.07441437989473343 norm:0.008303388021886349 max memory_allocated 22559.79736328125 
[2025-03-22 15:04:14 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.07397276908159256 norm:0.007637153845280409 max memory_allocated 22559.79736328125 
[2025-03-22 15:04:47 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.07358624786138535 norm:0.0071685598231852055 max memory_allocated 22559.79736328125 
[2025-03-22 15:05:20 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07322423905134201 norm:0.006778571289032698 max memory_allocated 22559.79736328125 
[2025-03-22 15:05:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.07307513058185577 norm:0.006525086704641581 max memory_allocated 22559.79736328125 
[2025-03-22 15:06:25 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07287438213825226 norm:0.006400132551789284 max memory_allocated 22559.79736328125 
[2025-03-22 15:06:58 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07260365784168243 norm:0.006003412883728743 max memory_allocated 22559.79736328125 
[2025-03-22 15:07:31 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07245032489299774 norm:0.005758513696491718 max memory_allocated 22559.79736328125 
[2025-03-22 15:07:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 15:07:42 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:08:15 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.26004037261009216 norm:0.0457095131278038 max memory_allocated 22559.96923828125 
[2025-03-22 15:08:48 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.2128649652004242 norm:0.027561530470848083 max memory_allocated 22559.96923828125 
[2025-03-22 15:09:21 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.18970739841461182 norm:0.02728007733821869 max memory_allocated 22559.96923828125 
[2025-03-22 15:09:53 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.17750988900661469 norm:0.025464756414294243 max memory_allocated 22559.96923828125 
[2025-03-22 15:10:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.1681472361087799 norm:0.024130839854478836 max memory_allocated 22559.96923828125 
[2025-03-22 15:10:59 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.1626048982143402 norm:0.02271776646375656 max memory_allocated 22559.96923828125 
[2025-03-22 15:11:32 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.1596338301897049 norm:0.022303059697151184 max memory_allocated 22559.96923828125 
[2025-03-22 15:12:04 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.15565478801727295 norm:0.022226568311452866 max memory_allocated 22559.96923828125 
[2025-03-22 15:12:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.15381963551044464 norm:0.02038673125207424 max memory_allocated 22559.96923828125 
[2025-03-22 15:13:10 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.15321891009807587 norm:0.019486669450998306 max memory_allocated 22559.96923828125 
[2025-03-22 15:13:42 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.1513500213623047 norm:0.01982869580388069 max memory_allocated 22559.96923828125 
[2025-03-22 15:14:15 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.15058527886867523 norm:0.019018229097127914 max memory_allocated 22559.96923828125 
[2025-03-22 15:14:48 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.14945587515830994 norm:0.020195502787828445 max memory_allocated 22559.96923828125 
[2025-03-22 15:15:20 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.14967092871665955 norm:0.018555929884314537 max memory_allocated 22559.96923828125 
[2025-03-22 15:15:53 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.14948168396949768 norm:0.01662943884730339 max memory_allocated 22559.96923828125 
[2025-03-22 15:16:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.14843790233135223 norm:0.017122779041528702 max memory_allocated 22559.96923828125 
[2025-03-22 15:16:58 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.14766451716423035 norm:0.016130581498146057 max memory_allocated 22559.96923828125 
[2025-03-22 15:17:31 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.14918696880340576 norm:0.01639937050640583 max memory_allocated 22559.96923828125 
[2025-03-22 15:18:04 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.14900094270706177 norm:0.01581045612692833 max memory_allocated 22559.96923828125 
[2025-03-22 15:18:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.14683878421783447 norm:0.01639436185359955 max memory_allocated 22559.96923828125 
[2025-03-22 15:18:46 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 15:19:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.24566754698753357 norm:0.05287546664476395 max memory_allocated 22559.96923828125 
[2025-03-22 15:19:54 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.1994173675775528 norm:0.014260033145546913 max memory_allocated 22559.96923828125 
[2025-03-22 15:20:26 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.1720406711101532 norm:0.006210672203451395 max memory_allocated 22559.96923828125 
[2025-03-22 15:20:59 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.1635386049747467 norm:0.003529948415234685 max memory_allocated 22559.96923828125 
[2025-03-22 15:21:32 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.15913152694702148 norm:0.0027037039399147034 max memory_allocated 22559.96923828125 
[2025-03-22 15:22:04 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.15647363662719727 norm:0.002275376580655575 max memory_allocated 22559.96923828125 
[2025-03-22 15:22:37 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.15463344752788544 norm:0.0020457280334085226 max memory_allocated 22559.96923828125 
[2025-03-22 15:23:10 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.1532728374004364 norm:0.0018835775554180145 max memory_allocated 22559.96923828125 
[2025-03-22 15:23:42 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.15230987966060638 norm:0.0018050660146400332 max memory_allocated 22559.96923828125 
[2025-03-22 15:24:15 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.15165694057941437 norm:0.001760111772455275 max memory_allocated 22559.96923828125 
[2025-03-22 15:24:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.15134944021701813 norm:0.0017915205098688602 max memory_allocated 22559.96923828125 
[2025-03-22 15:25:20 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.15102867782115936 norm:0.0017154773231595755 max memory_allocated 22559.96923828125 
[2025-03-22 15:25:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.15047091245651245 norm:0.0015770476311445236 max memory_allocated 22559.96923828125 
[2025-03-22 15:26:26 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.15010374784469604 norm:0.0015143322525545955 max memory_allocated 22559.96923828125 
[2025-03-22 15:26:58 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.1501631736755371 norm:0.001536272233352065 max memory_allocated 22559.96923828125 
[2025-03-22 15:27:31 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.15017549693584442 norm:0.0014642482856288552 max memory_allocated 22559.96923828125 
[2025-03-22 15:28:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.15010032057762146 norm:0.0014240216696634889 max memory_allocated 22559.96923828125 
[2025-03-22 15:28:36 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.15008124709129333 norm:0.0013961232034489512 max memory_allocated 22559.96923828125 
[2025-03-22 15:29:08 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.14984506368637085 norm:0.001315892906859517 max memory_allocated 22559.96923828125 
[2025-03-22 15:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.15015345811843872 norm:0.0014258436858654022 max memory_allocated 22559.96923828125 
[2025-03-22 15:29:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 15:30:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.295600026845932 norm:0.04757770523428917 max memory_allocated 22559.96923828125 
[2025-03-22 15:30:58 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.2483869194984436 norm:0.01869170553982258 max memory_allocated 22559.96923828125 
[2025-03-22 15:31:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.21310096979141235 norm:0.00644998624920845 max memory_allocated 22559.96923828125 
[2025-03-22 15:32:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.20094679296016693 norm:0.0030715768225491047 max memory_allocated 22559.96923828125 
[2025-03-22 15:32:35 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.19618678092956543 norm:0.002491749357432127 max memory_allocated 22559.96923828125 
[2025-03-22 15:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.19346778094768524 norm:0.0022216395009309053 max memory_allocated 22559.96923828125 
[2025-03-22 15:33:40 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.19179733097553253 norm:0.0020513630006462336 max memory_allocated 22559.96923828125 
[2025-03-22 15:34:13 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.19096693396568298 norm:0.0020598929841071367 max memory_allocated 22559.96923828125 
[2025-03-22 15:34:45 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.19001901149749756 norm:0.0018634815933182836 max memory_allocated 22559.96923828125 
[2025-03-22 15:35:18 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.18906819820404053 norm:0.001750523573718965 max memory_allocated 22559.96923828125 
[2025-03-22 15:35:51 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.18820887804031372 norm:0.0015006714966148138 max memory_allocated 22559.96923828125 
[2025-03-22 15:36:23 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.1875116378068924 norm:0.001424244837835431 max memory_allocated 22559.96923828125 
[2025-03-22 15:36:56 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.18809284269809723 norm:0.0015466688200831413 max memory_allocated 22559.96923828125 
[2025-03-22 15:37:29 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.1876719743013382 norm:0.001445879228413105 max memory_allocated 22559.96923828125 
[2025-03-22 15:38:01 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.18809273838996887 norm:0.0015671091387048364 max memory_allocated 22559.96923828125 
[2025-03-22 15:38:34 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.18774713575839996 norm:0.0013941130600869656 max memory_allocated 22559.96923828125 
[2025-03-22 15:39:07 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.1876894235610962 norm:0.0013888952089473605 max memory_allocated 22559.96923828125 
[2025-03-22 15:39:39 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.18740656971931458 norm:0.0013023137580603361 max memory_allocated 22559.96923828125 
[2025-03-22 15:40:12 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.1877976953983307 norm:0.0012767298612743616 max memory_allocated 22559.96923828125 
[2025-03-22 15:40:45 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.18741972744464874 norm:0.001223699888214469 max memory_allocated 22559.96923828125 
[2025-03-22 15:40:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 15:41:29 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.3412612974643707 norm:0.06500621140003204 max memory_allocated 22560.02392578125 
[2025-03-22 15:42:01 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2913331687450409 norm:0.02535131201148033 max memory_allocated 22560.02392578125 
[2025-03-22 15:42:34 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.242850661277771 norm:0.007094732020050287 max memory_allocated 22560.02392578125 
[2025-03-22 15:43:07 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.22895127534866333 norm:0.004048360511660576 max memory_allocated 22560.02392578125 
[2025-03-22 15:43:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.2232178896665573 norm:0.003115333616733551 max memory_allocated 22560.02392578125 
[2025-03-22 15:44:12 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.22019921243190765 norm:0.0028817143756896257 max memory_allocated 22560.02392578125 
[2025-03-22 15:44:44 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.21759364008903503 norm:0.002443323377519846 max memory_allocated 22560.02392578125 
[2025-03-22 15:45:17 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.21549612283706665 norm:0.002215565415099263 max memory_allocated 22560.02392578125 
[2025-03-22 15:45:49 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.21423451602458954 norm:0.0020688865333795547 max memory_allocated 22560.02392578125 
[2025-03-22 15:46:22 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.21301987767219543 norm:0.0017981770215556026 max memory_allocated 22560.02392578125 
[2025-03-22 15:46:54 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.2122330218553543 norm:0.0017491518519818783 max memory_allocated 22560.02392578125 
[2025-03-22 15:47:27 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.21164372563362122 norm:0.0016368459910154343 max memory_allocated 22560.02392578125 
[2025-03-22 15:47:59 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.21131929755210876 norm:0.0015081786550581455 max memory_allocated 22560.02392578125 
[2025-03-22 15:48:32 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.2109694629907608 norm:0.0014657917199656367 max memory_allocated 22560.02392578125 
[2025-03-22 15:49:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.21099135279655457 norm:0.0014942078851163387 max memory_allocated 22560.02392578125 
[2025-03-22 15:49:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.21062935888767242 norm:0.0014528152532875538 max memory_allocated 22560.02392578125 
[2025-03-22 15:50:10 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.2104623168706894 norm:0.0013977248454466462 max memory_allocated 22560.02392578125 
[2025-03-22 15:50:42 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.21062301099300385 norm:0.0014136068057268858 max memory_allocated 22560.02392578125 
[2025-03-22 15:51:15 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.21077924966812134 norm:0.0013509138952940702 max memory_allocated 22560.02392578125 
[2025-03-22 15:51:47 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.2106981724500656 norm:0.0013221710687503219 max memory_allocated 22560.02392578125 
[2025-03-22 15:51:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 15:52:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.36947759985923767 norm:0.06996238231658936 max memory_allocated 22560.19580078125 
[2025-03-22 15:53:04 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.31956207752227783 norm:0.028596188873052597 max memory_allocated 22560.19580078125 
[2025-03-22 15:53:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.27323317527770996 norm:0.009327453561127186 max memory_allocated 22560.19580078125 
[2025-03-22 15:54:10 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.25626441836357117 norm:0.004607027862221003 max memory_allocated 22560.19580078125 
[2025-03-22 15:54:42 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.2501832842826843 norm:0.0033585054334253073 max memory_allocated 22560.19580078125 
[2025-03-22 15:55:15 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.246079221367836 norm:0.002699196571484208 max memory_allocated 22560.19580078125 
[2025-03-22 15:55:48 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.24323298037052155 norm:0.002306886250153184 max memory_allocated 22560.19580078125 
[2025-03-22 15:56:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.24150420725345612 norm:0.0020635812543332577 max memory_allocated 22560.19580078125 
[2025-03-22 15:56:53 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.24007821083068848 norm:0.001896898727864027 max memory_allocated 22560.19580078125 
[2025-03-22 15:57:26 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.23922199010849 norm:0.0018777883378788829 max memory_allocated 22560.19580078125 
[2025-03-22 15:57:59 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.23831212520599365 norm:0.001729333191178739 max memory_allocated 22560.19580078125 
[2025-03-22 15:58:31 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.237775057554245 norm:0.0016744185704737902 max memory_allocated 22560.19580078125 
[2025-03-22 15:59:04 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.23760710656642914 norm:0.001696001156233251 max memory_allocated 22560.19580078125 
[2025-03-22 15:59:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.23737271130084991 norm:0.0016165028791874647 max memory_allocated 22560.19580078125 
[2025-03-22 16:00:10 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.23707453906536102 norm:0.0015976213617250323 max memory_allocated 22560.19580078125 
[2025-03-22 16:00:42 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.23686830699443817 norm:0.0015746413264423609 max memory_allocated 22560.19580078125 
[2025-03-22 16:01:15 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.23662178218364716 norm:0.0015194625593721867 max memory_allocated 22560.19580078125 
[2025-03-22 16:01:48 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.23638708889484406 norm:0.0015176597516983747 max memory_allocated 22560.19580078125 
[2025-03-22 16:02:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.23635752499103546 norm:0.0014932823833078146 max memory_allocated 22560.19580078125 
[2025-03-22 16:02:53 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.2364899218082428 norm:0.0014767650282010436 max memory_allocated 22560.19580078125 
[2025-03-22 16:03:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 16:03:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.40470004081726074 norm:0.04823913797736168 max memory_allocated 22560.36767578125 
[2025-03-22 16:04:10 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.3460268974304199 norm:0.019042162224650383 max memory_allocated 22560.36767578125 
[2025-03-22 16:04:42 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.2972544729709625 norm:0.006622996646910906 max memory_allocated 22560.36767578125 
[2025-03-22 16:05:15 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.27935898303985596 norm:0.0032205027528107166 max memory_allocated 22560.36767578125 
[2025-03-22 16:05:47 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.27292412519454956 norm:0.002463324461132288 max memory_allocated 22560.36767578125 
[2025-03-22 16:06:20 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.2690219283103943 norm:0.0021088786888867617 max memory_allocated 22560.36767578125 
[2025-03-22 16:06:52 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.26654720306396484 norm:0.0019203285919502378 max memory_allocated 22560.36767578125 
[2025-03-22 16:07:25 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.2647165358066559 norm:0.0017283606575801969 max memory_allocated 22560.36767578125 
[2025-03-22 16:07:57 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.2634681761264801 norm:0.0015856606187298894 max memory_allocated 22560.36767578125 
[2025-03-22 16:08:30 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.26224851608276367 norm:0.001562400721013546 max memory_allocated 22560.36767578125 
[2025-03-22 16:09:02 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.26169058680534363 norm:0.001467528403736651 max memory_allocated 22560.36767578125 
[2025-03-22 16:09:35 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.2611851692199707 norm:0.0014127115719020367 max memory_allocated 22560.36767578125 
[2025-03-22 16:10:08 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.26080387830734253 norm:0.001332960557192564 max memory_allocated 22560.36767578125 
[2025-03-22 16:10:40 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2606067657470703 norm:0.0012909462675452232 max memory_allocated 22560.36767578125 
[2025-03-22 16:11:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.26065099239349365 norm:0.0013735155807808042 max memory_allocated 22560.36767578125 
[2025-03-22 16:11:46 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.26079586148262024 norm:0.0013385566417127848 max memory_allocated 22560.36767578125 
[2025-03-22 16:12:18 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.26059675216674805 norm:0.0013258315157145262 max memory_allocated 22560.36767578125 
[2025-03-22 16:12:51 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.26067787408828735 norm:0.0013405749341472983 max memory_allocated 22560.36767578125 
[2025-03-22 16:13:24 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.26046109199523926 norm:0.0012672850862145424 max memory_allocated 22560.36767578125 
[2025-03-22 16:13:56 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.26016294956207275 norm:0.0012354959035292268 max memory_allocated 22560.36767578125 
[2025-03-22 16:14:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 16:14:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.41943395137786865 norm:0.036568451672792435 max memory_allocated 22560.53955078125 
[2025-03-22 16:15:14 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.3646750748157501 norm:0.016052868217229843 max memory_allocated 22560.53955078125 
[2025-03-22 16:15:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.31782689690589905 norm:0.006003252696245909 max memory_allocated 22560.53955078125 
[2025-03-22 16:16:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.29879340529441833 norm:0.0025481595657765865 max memory_allocated 22560.53955078125 
[2025-03-22 16:16:52 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.29242777824401855 norm:0.001989086391404271 max memory_allocated 22560.53955078125 
[2025-03-22 16:17:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.28792786598205566 norm:0.001695137470960617 max memory_allocated 22560.53955078125 
[2025-03-22 16:17:57 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.28501343727111816 norm:0.001558292773552239 max memory_allocated 22560.53955078125 
[2025-03-22 16:18:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.2829817533493042 norm:0.0014806673862040043 max memory_allocated 22560.53955078125 
[2025-03-22 16:19:02 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.28155165910720825 norm:0.0013783120084553957 max memory_allocated 22560.53955078125 
[2025-03-22 16:19:35 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.2803783118724823 norm:0.0013327328488230705 max memory_allocated 22560.53955078125 
[2025-03-22 16:20:08 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.2795962691307068 norm:0.0012981672771275043 max memory_allocated 22560.53955078125 
[2025-03-22 16:20:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.2790040373802185 norm:0.001225469633936882 max memory_allocated 22560.53955078125 
[2025-03-22 16:21:13 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.2785468101501465 norm:0.001159939798526466 max memory_allocated 22560.53955078125 
[2025-03-22 16:21:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.27825382351875305 norm:0.0011591814691200852 max memory_allocated 22560.53955078125 
[2025-03-22 16:22:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.2780173122882843 norm:0.0011247133370488882 max memory_allocated 22560.53955078125 
[2025-03-22 16:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.2777519226074219 norm:0.0010898389155045152 max memory_allocated 22560.53955078125 
[2025-03-22 16:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.2776327133178711 norm:0.0011138995178043842 max memory_allocated 22560.53955078125 
[2025-03-22 16:23:57 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.277560830116272 norm:0.0010964712128043175 max memory_allocated 22560.53955078125 
[2025-03-22 16:24:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.2775632441043854 norm:0.0010641207918524742 max memory_allocated 22560.53955078125 
[2025-03-22 16:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.27729716897010803 norm:0.0010330688674002886 max memory_allocated 22560.53955078125 
[2025-03-22 16:25:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 16:25:48 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.45909690856933594 norm:0.04582246392965317 max memory_allocated 22560.71142578125 
[2025-03-22 16:26:20 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.4080808758735657 norm:0.022526811808347702 max memory_allocated 22560.71142578125 
[2025-03-22 16:26:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.3463013470172882 norm:0.007691136095672846 max memory_allocated 22560.71142578125 
[2025-03-22 16:27:26 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.3213373124599457 norm:0.003138323314487934 max memory_allocated 22560.71142578125 
[2025-03-22 16:27:58 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.31387609243392944 norm:0.0024009321350604296 max memory_allocated 22560.71142578125 
[2025-03-22 16:28:31 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.30943360924720764 norm:0.0019487087847664952 max memory_allocated 22560.71142578125 
[2025-03-22 16:29:03 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.30637937784194946 norm:0.001709022093564272 max memory_allocated 22560.71142578125 
[2025-03-22 16:29:36 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.3041594624519348 norm:0.0015655423048883677 max memory_allocated 22560.71142578125 
[2025-03-22 16:30:08 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.3023400604724884 norm:0.0014910685131326318 max memory_allocated 22560.71142578125 
[2025-03-22 16:30:41 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.30125176906585693 norm:0.0013986974954605103 max memory_allocated 22560.71142578125 
[2025-03-22 16:31:13 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.3001794219017029 norm:0.0013161430833861232 max memory_allocated 22560.71142578125 
[2025-03-22 16:31:46 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.2995049059391022 norm:0.001247801585122943 max memory_allocated 22560.71142578125 
[2025-03-22 16:32:19 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.2990832030773163 norm:0.0012421042192727327 max memory_allocated 22560.71142578125 
[2025-03-22 16:32:51 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.2987188696861267 norm:0.0011855491902679205 max memory_allocated 22560.71142578125 
[2025-03-22 16:33:24 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.2983679473400116 norm:0.001091697602532804 max memory_allocated 22560.71142578125 
[2025-03-22 16:33:57 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.29811912775039673 norm:0.0010996991768479347 max memory_allocated 22560.71142578125 
[2025-03-22 16:34:29 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.2980216443538666 norm:0.001091176294721663 max memory_allocated 22560.71142578125 
[2025-03-22 16:35:02 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.29780253767967224 norm:0.0010538611095398664 max memory_allocated 22560.71142578125 
[2025-03-22 16:35:35 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.29802900552749634 norm:0.001106615411117673 max memory_allocated 22560.71142578125 
[2025-03-22 16:36:07 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.2980794608592987 norm:0.0010966429254040122 max memory_allocated 22560.71142578125 
[2025-03-22 16:36:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 16:36:52 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.42402613162994385 norm:0.025414295494556427 max memory_allocated 22560.88330078125 
[2025-03-22 16:37:24 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.3856888711452484 norm:0.01258738525211811 max memory_allocated 22560.88330078125 
[2025-03-22 16:37:57 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.3454006612300873 norm:0.004528087098151445 max memory_allocated 22560.88330078125 
[2025-03-22 16:38:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.33039674162864685 norm:0.0022117653861641884 max memory_allocated 22560.88330078125 
[2025-03-22 16:39:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.3245620131492615 norm:0.0016816919669508934 max memory_allocated 22560.88330078125 
[2025-03-22 16:39:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3215165436267853 norm:0.0015237209154292941 max memory_allocated 22560.88330078125 
[2025-03-22 16:40:08 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.319319486618042 norm:0.0013817467261105776 max memory_allocated 22560.88330078125 
[2025-03-22 16:40:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.31777364015579224 norm:0.0012752669863402843 max memory_allocated 22560.88330078125 
[2025-03-22 16:41:13 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.3167036771774292 norm:0.0011915476061403751 max memory_allocated 22560.88330078125 
[2025-03-22 16:41:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.3158383369445801 norm:0.001110824872739613 max memory_allocated 22560.88330078125 
[2025-03-22 16:42:19 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.31517356634140015 norm:0.0010945103131234646 max memory_allocated 22560.88330078125 
[2025-03-22 16:42:52 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.31454360485076904 norm:0.0010467563988640904 max memory_allocated 22560.88330078125 
[2025-03-22 16:43:24 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.3141116499900818 norm:0.0010243573924526572 max memory_allocated 22560.88330078125 
[2025-03-22 16:43:57 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.31373801827430725 norm:0.0009483867324888706 max memory_allocated 22560.88330078125 
[2025-03-22 16:44:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3135543465614319 norm:0.0009199522901326418 max memory_allocated 22560.88330078125 
[2025-03-22 16:45:02 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.3132542669773102 norm:0.0009047242347151041 max memory_allocated 22560.88330078125 
[2025-03-22 16:45:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.31297290325164795 norm:0.0008828634745441377 max memory_allocated 22560.88330078125 
[2025-03-22 16:46:08 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3128562569618225 norm:0.0008742269710637629 max memory_allocated 22560.88330078125 
[2025-03-22 16:46:40 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.31270086765289307 norm:0.0008553030202165246 max memory_allocated 22560.88330078125 
[2025-03-22 16:47:13 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.3124687373638153 norm:0.0008410164155066013 max memory_allocated 22560.88330078125 
[2025-03-22 16:47:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 16:47:57 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.4486781358718872 norm:0.03484261408448219 max memory_allocated 22561.05517578125 
[2025-03-22 16:48:30 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.4100175201892853 norm:0.017388850450515747 max memory_allocated 22561.05517578125 
[2025-03-22 16:49:02 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3656480014324188 norm:0.006010556593537331 max memory_allocated 22561.05517578125 
[2025-03-22 16:49:35 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.3469429016113281 norm:0.0027674217708408833 max memory_allocated 22561.05517578125 
[2025-03-22 16:50:07 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.3405771553516388 norm:0.0020267581567168236 max memory_allocated 22561.05517578125 
[2025-03-22 16:50:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.3367202877998352 norm:0.001759349717758596 max memory_allocated 22561.05517578125 
[2025-03-22 16:51:12 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.33423784375190735 norm:0.0016089131822809577 max memory_allocated 22561.05517578125 
[2025-03-22 16:51:45 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.332358181476593 norm:0.0015518051804974675 max memory_allocated 22561.05517578125 
[2025-03-22 16:52:17 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.33110395073890686 norm:0.0014456860953941941 max memory_allocated 22561.05517578125 
[2025-03-22 16:52:50 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.32987353205680847 norm:0.001404219539836049 max memory_allocated 22561.05517578125 
[2025-03-22 16:53:23 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.328958660364151 norm:0.0013420649338513613 max memory_allocated 22561.05517578125 
[2025-03-22 16:53:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.32836854457855225 norm:0.001304285367950797 max memory_allocated 22561.05517578125 
[2025-03-22 16:54:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3276715576648712 norm:0.001178915030322969 max memory_allocated 22561.05517578125 
[2025-03-22 16:55:01 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.32728302478790283 norm:0.0011556231183931231 max memory_allocated 22561.05517578125 
[2025-03-22 16:55:33 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3269413709640503 norm:0.0010949502466246486 max memory_allocated 22561.05517578125 
[2025-03-22 16:56:06 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.32658150792121887 norm:0.0010695622768253088 max memory_allocated 22561.05517578125 
[2025-03-22 16:56:39 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.32638484239578247 norm:0.001036025583744049 max memory_allocated 22561.05517578125 
[2025-03-22 16:57:11 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.3261532485485077 norm:0.00102641386911273 max memory_allocated 22561.05517578125 
[2025-03-22 16:57:44 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.3258935809135437 norm:0.0009881528094410896 max memory_allocated 22561.05517578125 
[2025-03-22 16:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.3257136940956116 norm:0.0009477993589825928 max memory_allocated 22561.05517578125 
[2025-03-22 16:58:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 16:59:01 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.44076693058013916 norm:0.02939416468143463 max memory_allocated 22561.22705078125 
[2025-03-22 16:59:34 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.4142644703388214 norm:0.015455000102519989 max memory_allocated 22561.22705078125 
[2025-03-22 17:00:06 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.3799062669277191 norm:0.007321713957935572 max memory_allocated 22561.22705078125 
[2025-03-22 17:00:39 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.3629757761955261 norm:0.0042702388018369675 max memory_allocated 22561.22705078125 
[2025-03-22 17:01:12 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.35665103793144226 norm:0.003242664271965623 max memory_allocated 22561.22705078125 
[2025-03-22 17:01:44 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.3523981273174286 norm:0.0026949045713990927 max memory_allocated 22561.22705078125 
[2025-03-22 17:02:17 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.3490956425666809 norm:0.002284172223880887 max memory_allocated 22561.22705078125 
[2025-03-22 17:02:50 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.34684866666793823 norm:0.0020226393826305866 max memory_allocated 22561.22705078125 
[2025-03-22 17:03:23 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.34510642290115356 norm:0.0018376408843323588 max memory_allocated 22561.22705078125 
[2025-03-22 17:03:55 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.34381628036499023 norm:0.0016993286553770304 max memory_allocated 22561.22705078125 
[2025-03-22 17:04:28 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.34290051460266113 norm:0.0016427808441221714 max memory_allocated 22561.22705078125 
[2025-03-22 17:05:01 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.3419956564903259 norm:0.0015001576393842697 max memory_allocated 22561.22705078125 
[2025-03-22 17:05:34 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.34130099415779114 norm:0.0014071785844862461 max memory_allocated 22561.22705078125 
[2025-03-22 17:06:06 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.340932697057724 norm:0.001349777216091752 max memory_allocated 22561.22705078125 
[2025-03-22 17:06:39 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.3404947817325592 norm:0.0012901730369776487 max memory_allocated 22561.22705078125 
[2025-03-22 17:07:12 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.3401317894458771 norm:0.001204727333970368 max memory_allocated 22561.22705078125 
[2025-03-22 17:07:44 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.33969518542289734 norm:0.0011246176436543465 max memory_allocated 22561.22705078125 
[2025-03-22 17:08:17 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.3394472599029541 norm:0.0010831227991729975 max memory_allocated 22561.22705078125 
[2025-03-22 17:08:50 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.33901065587997437 norm:0.0010312926024198532 max memory_allocated 22561.22705078125 
[2025-03-22 17:09:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.33876466751098633 norm:0.0009963535703718662 max memory_allocated 22561.22705078125 
[2025-03-22 17:09:31 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 17:10:07 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.4464535713195801 norm:0.021845821291208267 max memory_allocated 22561.39892578125 
[2025-03-22 17:10:39 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.41971921920776367 norm:0.012026479467749596 max memory_allocated 22561.39892578125 
[2025-03-22 17:11:12 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.3849923610687256 norm:0.0045747095718979836 max memory_allocated 22561.39892578125 
[2025-03-22 17:11:44 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.3712184727191925 norm:0.0022226045839488506 max memory_allocated 22561.39892578125 
[2025-03-22 17:12:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.3652871549129486 norm:0.0016407421790063381 max memory_allocated 22561.39892578125 
[2025-03-22 17:12:49 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3619629144668579 norm:0.0014648260548710823 max memory_allocated 22561.39892578125 
[2025-03-22 17:13:22 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3597925305366516 norm:0.001352617982774973 max memory_allocated 22561.39892578125 
[2025-03-22 17:13:54 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.3580682575702667 norm:0.0012793178902938962 max memory_allocated 22561.39892578125 
[2025-03-22 17:14:27 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.3567282557487488 norm:0.001198589219711721 max memory_allocated 22561.39892578125 
[2025-03-22 17:15:00 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.355670690536499 norm:0.0011434046318754554 max memory_allocated 22561.39892578125 
[2025-03-22 17:15:32 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3549821078777313 norm:0.001116701285354793 max memory_allocated 22561.39892578125 
[2025-03-22 17:16:05 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.35461685061454773 norm:0.0010998435318470001 max memory_allocated 22561.39892578125 
[2025-03-22 17:16:37 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.3541105389595032 norm:0.0011071667540818453 max memory_allocated 22561.39892578125 
[2025-03-22 17:17:10 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3536885380744934 norm:0.0010963091626763344 max memory_allocated 22561.39892578125 
[2025-03-22 17:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.3532683849334717 norm:0.0010799875017255545 max memory_allocated 22561.39892578125 
[2025-03-22 17:18:15 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3528698980808258 norm:0.001051291124895215 max memory_allocated 22561.39892578125 
[2025-03-22 17:18:48 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.3526713252067566 norm:0.0010025536175817251 max memory_allocated 22561.39892578125 
[2025-03-22 17:19:21 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.3526245355606079 norm:0.0009830238996073604 max memory_allocated 22561.39892578125 
[2025-03-22 17:19:54 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.35253340005874634 norm:0.0009765052818693221 max memory_allocated 22561.39892578125 
[2025-03-22 17:20:26 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.3523579239845276 norm:0.0009352039196528494 max memory_allocated 22561.39892578125 
[2025-03-22 17:20:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 17:21:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.5015951991081238 norm:0.05819340795278549 max memory_allocated 22561.57080078125 
[2025-03-22 17:21:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.467982679605484 norm:0.030271433293819427 max memory_allocated 22561.57080078125 
[2025-03-22 17:22:16 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.4222809076309204 norm:0.012423079460859299 max memory_allocated 22561.57080078125 
[2025-03-22 17:22:49 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.4022975265979767 norm:0.006601271685212851 max memory_allocated 22561.57080078125 
[2025-03-22 17:23:21 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.39495688676834106 norm:0.005193139426410198 max memory_allocated 22561.57080078125 
[2025-03-22 17:23:54 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3903454840183258 norm:0.004274921026080847 max memory_allocated 22561.57080078125 
[2025-03-22 17:24:27 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.3869812786579132 norm:0.0036446095909923315 max memory_allocated 22561.57080078125 
[2025-03-22 17:24:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.38448601961135864 norm:0.0031107498798519373 max memory_allocated 22561.57080078125 
[2025-03-22 17:25:32 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3826088309288025 norm:0.00283258780837059 max memory_allocated 22561.57080078125 
[2025-03-22 17:26:05 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.3811245262622833 norm:0.0024697608314454556 max memory_allocated 22561.57080078125 
[2025-03-22 17:26:37 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.3798861801624298 norm:0.0022363951429724693 max memory_allocated 22561.57080078125 
[2025-03-22 17:27:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.3787720203399658 norm:0.001950777368620038 max memory_allocated 22561.57080078125 
[2025-03-22 17:27:42 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.3779340982437134 norm:0.0017577450489625335 max memory_allocated 22561.57080078125 
[2025-03-22 17:28:15 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.37732240557670593 norm:0.0016115428879857063 max memory_allocated 22561.57080078125 
[2025-03-22 17:28:47 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.3768710792064667 norm:0.0015638420591130853 max memory_allocated 22561.57080078125 
[2025-03-22 17:29:20 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.37633591890335083 norm:0.0014384528622031212 max memory_allocated 22561.57080078125 
[2025-03-22 17:29:52 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.37602418661117554 norm:0.0013512761797755957 max memory_allocated 22561.57080078125 
[2025-03-22 17:30:25 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.37563228607177734 norm:0.0012515460839495063 max memory_allocated 22561.57080078125 
[2025-03-22 17:30:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.37565773725509644 norm:0.0012293425388634205 max memory_allocated 22561.57080078125 
[2025-03-22 17:31:30 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.3754352927207947 norm:0.0011707845842465758 max memory_allocated 22561.57080078125 
[2025-03-22 17:31:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 17:32:15 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.49500253796577454 norm:0.03656584396958351 max memory_allocated 22561.74267578125 
[2025-03-22 17:32:47 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.46635156869888306 norm:0.017151623964309692 max memory_allocated 22561.74267578125 
[2025-03-22 17:33:20 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.4384857416152954 norm:0.007709654048085213 max memory_allocated 22561.74267578125 
[2025-03-22 17:33:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.42359209060668945 norm:0.0037071038968861103 max memory_allocated 22561.74267578125 
[2025-03-22 17:34:25 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.4172210991382599 norm:0.0022277873940765858 max memory_allocated 22561.74267578125 
[2025-03-22 17:34:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.41332367062568665 norm:0.0018570536049082875 max memory_allocated 22561.74267578125 
[2025-03-22 17:35:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.4106064438819885 norm:0.0017956482479348779 max memory_allocated 22561.74267578125 
[2025-03-22 17:36:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.40825897455215454 norm:0.0013550100848078728 max memory_allocated 22561.74267578125 
[2025-03-22 17:36:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.4067927598953247 norm:0.0012732117902487516 max memory_allocated 22561.74267578125 
[2025-03-22 17:37:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.4057973325252533 norm:0.0012078559957444668 max memory_allocated 22561.74267578125 
[2025-03-22 17:37:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.40494078397750854 norm:0.00117275002412498 max memory_allocated 22561.74267578125 
[2025-03-22 17:38:14 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.4042297303676605 norm:0.00115001923404634 max memory_allocated 22561.74267578125 
[2025-03-22 17:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.40391552448272705 norm:0.0011265886714681983 max memory_allocated 22561.74267578125 
[2025-03-22 17:39:19 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.4034194052219391 norm:0.0010943645611405373 max memory_allocated 22561.74267578125 
[2025-03-22 17:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.4029325246810913 norm:0.0010699447011575103 max memory_allocated 22561.74267578125 
[2025-03-22 17:40:25 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.40261152386665344 norm:0.0010549782309681177 max memory_allocated 22561.74267578125 
[2025-03-22 17:40:57 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.40239763259887695 norm:0.0010555519256740808 max memory_allocated 22561.74267578125 
[2025-03-22 17:41:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.4020421802997589 norm:0.0010382778709754348 max memory_allocated 22561.74267578125 
[2025-03-22 17:42:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.40171125531196594 norm:0.0010090600699186325 max memory_allocated 22561.74267578125 
[2025-03-22 17:42:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.4014502167701721 norm:0.0009940022137016058 max memory_allocated 22561.74267578125 
[2025-03-22 17:42:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 17:43:20 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.564656138420105 norm:0.042062900960445404 max memory_allocated 22561.91455078125 
[2025-03-22 17:43:52 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.5277228951454163 norm:0.0217842236161232 max memory_allocated 22561.91455078125 
[2025-03-22 17:44:25 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.48487091064453125 norm:0.009365617297589779 max memory_allocated 22561.91455078125 
[2025-03-22 17:44:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.4655085504055023 norm:0.005233229137957096 max memory_allocated 22561.91455078125 
[2025-03-22 17:45:30 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.457417368888855 norm:0.0032302956096827984 max memory_allocated 22561.91455078125 
[2025-03-22 17:46:03 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.4530448317527771 norm:0.002556141698732972 max memory_allocated 22561.91455078125 
[2025-03-22 17:46:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.45019209384918213 norm:0.0023481319658458233 max memory_allocated 22561.91455078125 
[2025-03-22 17:47:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.447586327791214 norm:0.0019177304347977042 max memory_allocated 22561.91455078125 
[2025-03-22 17:47:40 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.4451603293418884 norm:0.0015438994159922004 max memory_allocated 22561.91455078125 
[2025-03-22 17:48:13 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.44371622800827026 norm:0.0014797942712903023 max memory_allocated 22561.91455078125 
[2025-03-22 17:48:45 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.4426746368408203 norm:0.0014453278854489326 max memory_allocated 22561.91455078125 
[2025-03-22 17:49:18 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.44175809621810913 norm:0.0013722390867769718 max memory_allocated 22561.91455078125 
[2025-03-22 17:49:50 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.4410408139228821 norm:0.0013096185866743326 max memory_allocated 22561.91455078125 
[2025-03-22 17:50:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.44042497873306274 norm:0.0012733297189697623 max memory_allocated 22561.91455078125 
[2025-03-22 17:50:55 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.43996521830558777 norm:0.0012430079514160752 max memory_allocated 22561.91455078125 
[2025-03-22 17:51:28 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.4395994544029236 norm:0.0012206760002300143 max memory_allocated 22561.91455078125 
[2025-03-22 17:52:01 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.4391990303993225 norm:0.0011853453470394015 max memory_allocated 22561.91455078125 
[2025-03-22 17:52:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.4388861358165741 norm:0.0011711153201758862 max memory_allocated 22561.91455078125 
[2025-03-22 17:53:06 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.4386810064315796 norm:0.0011581069556996226 max memory_allocated 22561.91455078125 
[2025-03-22 17:53:39 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.43853047490119934 norm:0.001132441801019013 max memory_allocated 22561.91455078125 
[2025-03-22 17:53:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 17:54:23 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.5963252782821655 norm:0.0388285256922245 max memory_allocated 22562.08642578125 
[2025-03-22 17:54:56 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.5655556917190552 norm:0.01997547596693039 max memory_allocated 22562.08642578125 
[2025-03-22 17:55:28 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.530583381652832 norm:0.008359207771718502 max memory_allocated 22562.08642578125 
[2025-03-22 17:56:01 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.5135300755500793 norm:0.003825210267677903 max memory_allocated 22562.08642578125 
[2025-03-22 17:56:34 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.5064451694488525 norm:0.0025221232790499926 max memory_allocated 22562.08642578125 
[2025-03-22 17:57:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.5024714469909668 norm:0.002153825480490923 max memory_allocated 22562.08642578125 
[2025-03-22 17:57:39 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.499571830034256 norm:0.001963044749572873 max memory_allocated 22562.08642578125 
[2025-03-22 17:58:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.49726951122283936 norm:0.0018243659287691116 max memory_allocated 22562.08642578125 
[2025-03-22 17:58:45 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.4955587089061737 norm:0.0017410137224942446 max memory_allocated 22562.08642578125 
[2025-03-22 17:59:17 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.4942062497138977 norm:0.0016583723481744528 max memory_allocated 22562.08642578125 
[2025-03-22 17:59:50 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.4931584596633911 norm:0.0015876567922532558 max memory_allocated 22562.08642578125 
[2025-03-22 18:00:23 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.49225759506225586 norm:0.0015134542481973767 max memory_allocated 22562.08642578125 
[2025-03-22 18:00:55 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.491715669631958 norm:0.0014851486776024103 max memory_allocated 22562.08642578125 
[2025-03-22 18:01:28 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.49114328622817993 norm:0.0014343531802296638 max memory_allocated 22562.08642578125 
[2025-03-22 18:02:00 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.49057766795158386 norm:0.001373656908981502 max memory_allocated 22562.08642578125 
[2025-03-22 18:02:33 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.4900001287460327 norm:0.001354892272502184 max memory_allocated 22562.08642578125 
[2025-03-22 18:03:05 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.48948416113853455 norm:0.0013380403397604823 max memory_allocated 22562.08642578125 
[2025-03-22 18:03:38 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.48899272084236145 norm:0.0012770425528287888 max memory_allocated 22562.08642578125 
[2025-03-22 18:04:11 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.48858189582824707 norm:0.0012406330788508058 max memory_allocated 22562.08642578125 
[2025-03-22 18:04:43 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.48835107684135437 norm:0.0012042520102113485 max memory_allocated 22562.08642578125 
[2025-03-22 18:04:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 18:05:27 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.6554638743400574 norm:0.040075890719890594 max memory_allocated 22562.25830078125 
[2025-03-22 18:06:00 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.624863862991333 norm:0.019182397052645683 max memory_allocated 22562.25830078125 
[2025-03-22 18:06:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.5918117165565491 norm:0.00853012502193451 max memory_allocated 22562.25830078125 
[2025-03-22 18:07:05 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.5765972137451172 norm:0.00479739997535944 max memory_allocated 22562.25830078125 
[2025-03-22 18:07:37 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.5700392723083496 norm:0.003431331366300583 max memory_allocated 22562.25830078125 
[2025-03-22 18:08:10 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.5659667253494263 norm:0.0024064690805971622 max memory_allocated 22562.25830078125 
[2025-03-22 18:08:42 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.5627612471580505 norm:0.0014094052603468299 max memory_allocated 22562.25830078125 
[2025-03-22 18:09:15 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.5609299540519714 norm:0.001398027059622109 max memory_allocated 22562.25830078125 
[2025-03-22 18:09:48 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.5595231652259827 norm:0.0013829185627400875 max memory_allocated 22562.25830078125 
[2025-03-22 18:10:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.5584468841552734 norm:0.001356253051199019 max memory_allocated 22562.25830078125 
[2025-03-22 18:10:53 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.5575605630874634 norm:0.0013321005972102284 max memory_allocated 22562.25830078125 
[2025-03-22 18:11:26 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.5567926168441772 norm:0.0013032486895099282 max memory_allocated 22562.25830078125 
[2025-03-22 18:11:58 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.5562130212783813 norm:0.0012930806260555983 max memory_allocated 22562.25830078125 
[2025-03-22 18:12:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.5557262301445007 norm:0.0012583312345668674 max memory_allocated 22562.25830078125 
[2025-03-22 18:13:04 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.5553455948829651 norm:0.0012509976513683796 max memory_allocated 22562.25830078125 
[2025-03-22 18:13:36 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.555014967918396 norm:0.0012468096101656556 max memory_allocated 22562.25830078125 
[2025-03-22 18:14:09 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.5547114610671997 norm:0.001240151934325695 max memory_allocated 22562.25830078125 
[2025-03-22 18:14:42 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.5544223785400391 norm:0.0012354461941868067 max memory_allocated 22562.25830078125 
[2025-03-22 18:15:15 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.5540667772293091 norm:0.0012288476573303342 max memory_allocated 22562.25830078125 
[2025-03-22 18:15:47 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.5538628697395325 norm:0.0012164940126240253 max memory_allocated 22562.25830078125 
[2025-03-22 18:15:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 18:16:32 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.7206918597221375 norm:0.02891838178038597 max memory_allocated 22562.43017578125 
[2025-03-22 18:17:04 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.696797251701355 norm:0.015589680522680283 max memory_allocated 22562.43017578125 
[2025-03-22 18:17:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.6719200611114502 norm:0.008440579287707806 max memory_allocated 22562.43017578125 
[2025-03-22 18:18:09 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.6593193411827087 norm:0.005439188797026873 max memory_allocated 22562.43017578125 
[2025-03-22 18:18:42 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.6543656587600708 norm:0.004399906378239393 max memory_allocated 22562.43017578125 
[2025-03-22 18:19:15 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.6509992480278015 norm:0.003623436903581023 max memory_allocated 22562.43017578125 
[2025-03-22 18:19:47 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.6484382152557373 norm:0.003083023941144347 max memory_allocated 22562.43017578125 
[2025-03-22 18:20:20 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.6463499665260315 norm:0.0026841172948479652 max memory_allocated 22562.43017578125 
[2025-03-22 18:20:52 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.6443641781806946 norm:0.0022556371986865997 max memory_allocated 22562.43017578125 
[2025-03-22 18:21:25 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.6430556774139404 norm:0.0020878722425550222 max memory_allocated 22562.43017578125 
[2025-03-22 18:21:57 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.64188551902771 norm:0.0018878362607210875 max memory_allocated 22562.43017578125 
[2025-03-22 18:22:30 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.6405800580978394 norm:0.001471071969717741 max memory_allocated 22562.43017578125 
[2025-03-22 18:23:02 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.6396124362945557 norm:0.001216327422298491 max memory_allocated 22562.43017578125 
[2025-03-22 18:23:35 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.6390587687492371 norm:0.001197504112496972 max memory_allocated 22562.43017578125 
[2025-03-22 18:24:07 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.6385458707809448 norm:0.001183248357847333 max memory_allocated 22562.43017578125 
[2025-03-22 18:24:40 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.6381334662437439 norm:0.0011617335258051753 max memory_allocated 22562.43017578125 
[2025-03-22 18:25:12 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.6379122734069824 norm:0.0011558260302990675 max memory_allocated 22562.43017578125 
[2025-03-22 18:25:45 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.6376489400863647 norm:0.0011479781242087483 max memory_allocated 22562.43017578125 
[2025-03-22 18:26:18 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.6374137997627258 norm:0.001132821780629456 max memory_allocated 22562.43017578125 
[2025-03-22 18:26:50 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.6370131969451904 norm:0.001122158719226718 max memory_allocated 22562.43017578125 
[2025-03-22 18:26:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 18:27:35 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.8486093282699585 norm:0.031334564089775085 max memory_allocated 22562.60205078125 
[2025-03-22 18:28:07 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.8221957683563232 norm:0.017139369621872902 max memory_allocated 22562.60205078125 
[2025-03-22 18:28:40 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.7928934693336487 norm:0.00937204621732235 max memory_allocated 22562.60205078125 
[2025-03-22 18:29:13 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.77585369348526 norm:0.0055936891585588455 max memory_allocated 22562.60205078125 
[2025-03-22 18:29:45 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.7700405716896057 norm:0.004506516270339489 max memory_allocated 22562.60205078125 
[2025-03-22 18:30:18 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.7664755582809448 norm:0.0037981888744980097 max memory_allocated 22562.60205078125 
[2025-03-22 18:30:51 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.7636351585388184 norm:0.0032821332570165396 max memory_allocated 22562.60205078125 
[2025-03-22 18:31:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.761479377746582 norm:0.002962422091513872 max memory_allocated 22562.60205078125 
[2025-03-22 18:31:56 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.7589035630226135 norm:0.0017998437397181988 max memory_allocated 22562.60205078125 
[2025-03-22 18:32:29 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.7575201988220215 norm:0.0016128321876749396 max memory_allocated 22562.60205078125 
[2025-03-22 18:33:02 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.7568221688270569 norm:0.0015989497769623995 max memory_allocated 22562.60205078125 
[2025-03-22 18:33:34 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.7559347748756409 norm:0.001505637657828629 max memory_allocated 22562.60205078125 
[2025-03-22 18:34:07 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.7552369832992554 norm:0.0015069349901750684 max memory_allocated 22562.60205078125 
[2025-03-22 18:34:39 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.7546595931053162 norm:0.0014744673389941454 max memory_allocated 22562.60205078125 
[2025-03-22 18:35:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.7542384266853333 norm:0.001455608056858182 max memory_allocated 22562.60205078125 
[2025-03-22 18:35:45 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.7538917064666748 norm:0.0014379762578755617 max memory_allocated 22562.60205078125 
[2025-03-22 18:36:17 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.7536698579788208 norm:0.0014138242695480585 max memory_allocated 22562.60205078125 
[2025-03-22 18:36:50 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.7535327076911926 norm:0.0013918967451900244 max memory_allocated 22562.60205078125 
[2025-03-22 18:37:22 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.753313422203064 norm:0.0013825952773913741 max memory_allocated 22562.60205078125 
[2025-03-22 18:37:55 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.7531103491783142 norm:0.0013657729141414165 max memory_allocated 22562.60205078125 
[2025-03-22 18:38:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 18:38:39 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.9576029777526855 norm:0.012771094217896461 max memory_allocated 22562.77392578125 
[2025-03-22 18:39:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.9441066384315491 norm:0.008348478004336357 max memory_allocated 22562.77392578125 
[2025-03-22 18:39:44 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.920852780342102 norm:0.004010527394711971 max memory_allocated 22562.77392578125 
[2025-03-22 18:40:16 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.9083775281906128 norm:0.0025967240799218416 max memory_allocated 22562.77392578125 
[2025-03-22 18:40:49 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.902452290058136 norm:0.002099395729601383 max memory_allocated 22562.77392578125 
[2025-03-22 18:41:22 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.8994305729866028 norm:0.0019225706346333027 max memory_allocated 22562.77392578125 
[2025-03-22 18:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.8971781730651855 norm:0.0018764620181173086 max memory_allocated 22562.77392578125 
[2025-03-22 18:42:27 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.8954625129699707 norm:0.0017986109014600515 max memory_allocated 22562.77392578125 
[2025-03-22 18:42:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.8941928148269653 norm:0.0017587941838428378 max memory_allocated 22562.77392578125 
[2025-03-22 18:43:32 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.8931572437286377 norm:0.0017251125536859035 max memory_allocated 22562.77392578125 
[2025-03-22 18:44:05 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.8924058675765991 norm:0.0017170135397464037 max memory_allocated 22562.77392578125 
[2025-03-22 18:44:37 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.8917842507362366 norm:0.0016643846174702048 max memory_allocated 22562.77392578125 
[2025-03-22 18:45:10 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.8911563158035278 norm:0.0016292674699798226 max memory_allocated 22562.77392578125 
[2025-03-22 18:45:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.8907044529914856 norm:0.001621150760911405 max memory_allocated 22562.77392578125 
[2025-03-22 18:46:15 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.8902188539505005 norm:0.0016086671967059374 max memory_allocated 22562.77392578125 
[2025-03-22 18:46:48 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.8899708986282349 norm:0.001570188906043768 max memory_allocated 22562.77392578125 
[2025-03-22 18:47:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.8898336291313171 norm:0.0015843246364966035 max memory_allocated 22562.77392578125 
[2025-03-22 18:47:53 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.8895886540412903 norm:0.0015681734075769782 max memory_allocated 22562.77392578125 
[2025-03-22 18:48:26 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.889060914516449 norm:0.0015594006981700659 max memory_allocated 22562.77392578125 
[2025-03-22 18:48:58 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.8887436389923096 norm:0.0015435279347002506 max memory_allocated 22562.77392578125 
[2025-03-22 18:49:07 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 18:49:44 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:1.0956374406814575 norm:0.01226095762103796 max memory_allocated 22562.94580078125 
[2025-03-22 18:50:17 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:1.075399398803711 norm:0.007066299673169851 max memory_allocated 22562.94580078125 
[2025-03-22 18:50:49 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:1.0528427362442017 norm:0.0038411496207118034 max memory_allocated 22562.94580078125 
[2025-03-22 18:51:22 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:1.0437679290771484 norm:0.002671928610652685 max memory_allocated 22562.94580078125 
[2025-03-22 18:51:55 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:1.0387256145477295 norm:0.002006109105423093 max memory_allocated 22562.94580078125 
[2025-03-22 18:52:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:1.0354140996932983 norm:0.0017624062020331621 max memory_allocated 22562.94580078125 
[2025-03-22 18:53:00 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:1.0328266620635986 norm:0.0015909174690023065 max memory_allocated 22562.94580078125 
[2025-03-22 18:53:33 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:1.0308690071105957 norm:0.0013906513340771198 max memory_allocated 22562.94580078125 
[2025-03-22 18:54:05 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:1.0296236276626587 norm:0.001340286573395133 max memory_allocated 22562.94580078125 
[2025-03-22 18:54:38 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:1.028684139251709 norm:0.0013105423422530293 max memory_allocated 22562.94580078125 
[2025-03-22 18:55:11 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:1.028003215789795 norm:0.0013011550763621926 max memory_allocated 22562.94580078125 
[2025-03-22 18:55:43 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:1.027443766593933 norm:0.0012836111709475517 max memory_allocated 22562.94580078125 
[2025-03-22 18:56:16 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:1.027032732963562 norm:0.0012700462248176336 max memory_allocated 22562.94580078125 
[2025-03-22 18:56:48 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:1.0268162488937378 norm:0.001260586199350655 max memory_allocated 22562.94580078125 
[2025-03-22 18:57:21 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:1.0265226364135742 norm:0.001252568094059825 max memory_allocated 22562.94580078125 
[2025-03-22 18:57:53 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:1.0261907577514648 norm:0.001232682610861957 max memory_allocated 22562.94580078125 
[2025-03-22 18:58:26 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:1.0259547233581543 norm:0.001227526692673564 max memory_allocated 22562.94580078125 
[2025-03-22 18:58:58 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:1.0256952047348022 norm:0.0012231596047058702 max memory_allocated 22562.94580078125 
[2025-03-22 18:59:31 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:1.0255227088928223 norm:0.0012146126246079803 max memory_allocated 22562.94580078125 
[2025-03-22 19:00:03 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:1.0253545045852661 norm:0.0012172420974820852 max memory_allocated 22562.94580078125 
[2025-03-22 19:00:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 19:00:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:1.2806295156478882 norm:0.02095993235707283 max memory_allocated 22563.11767578125 
[2025-03-22 19:01:22 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:1.2565127611160278 norm:0.013420085422694683 max memory_allocated 22563.11767578125 
[2025-03-22 19:01:54 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:1.2289049625396729 norm:0.007308248896151781 max memory_allocated 22563.11767578125 
[2025-03-22 19:02:27 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:1.2152111530303955 norm:0.004219407681375742 max memory_allocated 22563.11767578125 
[2025-03-22 19:02:59 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:1.2086318731307983 norm:0.0032476631458848715 max memory_allocated 22563.11767578125 
[2025-03-22 19:03:32 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:1.2027214765548706 norm:0.001939253299497068 max memory_allocated 22563.11767578125 
[2025-03-22 19:04:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:1.1993184089660645 norm:0.0016189151210710406 max memory_allocated 22563.11767578125 
[2025-03-22 19:04:37 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:1.1972426176071167 norm:0.0015333069022744894 max memory_allocated 22563.11767578125 
[2025-03-22 19:05:10 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:1.1957569122314453 norm:0.001467450405471027 max memory_allocated 22563.11767578125 
[2025-03-22 19:05:42 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:1.1948654651641846 norm:0.0014462770195677876 max memory_allocated 22563.11767578125 
[2025-03-22 19:06:15 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:1.1939646005630493 norm:0.0014174050884321332 max memory_allocated 22563.11767578125 
[2025-03-22 19:06:48 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:1.193405270576477 norm:0.0014024678384885192 max memory_allocated 22563.11767578125 
[2025-03-22 19:07:20 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:1.1930114030838013 norm:0.0014009799342602491 max memory_allocated 22563.11767578125 
[2025-03-22 19:07:53 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:1.1927101612091064 norm:0.0014000327792018652 max memory_allocated 22563.11767578125 
[2025-03-22 19:08:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:1.1924071311950684 norm:0.0013818562729284167 max memory_allocated 22563.11767578125 
[2025-03-22 19:08:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:1.1921402215957642 norm:0.001386062940582633 max memory_allocated 22563.11767578125 
[2025-03-22 19:09:31 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:1.1919224262237549 norm:0.0013854156713932753 max memory_allocated 22563.11767578125 
[2025-03-22 19:10:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:1.1917884349822998 norm:0.0013857078738510609 max memory_allocated 22563.11767578125 
[2025-03-22 19:10:36 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:1.1917256116867065 norm:0.0013841193867847323 max memory_allocated 22563.11767578125 
[2025-03-22 19:11:09 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:1.191760778427124 norm:0.0013709284830838442 max memory_allocated 22563.11767578125 
[2025-03-22 19:11:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 19:11:54 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:1.4441934823989868 norm:0.030542874708771706 max memory_allocated 22563.28955078125 
[2025-03-22 19:12:26 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:1.4244641065597534 norm:0.020931052044034004 max memory_allocated 22563.28955078125 
[2025-03-22 19:12:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:1.3977293968200684 norm:0.013860239647328854 max memory_allocated 22563.28955078125 
[2025-03-22 19:13:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:1.3849678039550781 norm:0.010247581638395786 max memory_allocated 22563.28955078125 
[2025-03-22 19:14:04 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:1.376617431640625 norm:0.007512193638831377 max memory_allocated 22563.28955078125 
[2025-03-22 19:14:37 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:1.3707479238510132 norm:0.006146228406578302 max memory_allocated 22563.28955078125 
[2025-03-22 19:15:10 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:1.367226243019104 norm:0.005342645570635796 max memory_allocated 22563.28955078125 
[2025-03-22 19:15:42 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:1.3646243810653687 norm:0.00469229556620121 max memory_allocated 22563.28955078125 
[2025-03-22 19:16:15 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:1.3624902963638306 norm:0.004183793440461159 max memory_allocated 22563.28955078125 
[2025-03-22 19:16:47 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:1.3609098196029663 norm:0.003778556827455759 max memory_allocated 22563.28955078125 
[2025-03-22 19:17:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:1.3597241640090942 norm:0.003416301216930151 max memory_allocated 22563.28955078125 
[2025-03-22 19:17:52 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:1.358803629875183 norm:0.0031430870294570923 max memory_allocated 22563.28955078125 
[2025-03-22 19:18:25 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:1.3581063747406006 norm:0.0028947012033313513 max memory_allocated 22563.28955078125 
[2025-03-22 19:18:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:1.3574151992797852 norm:0.0026802781503647566 max memory_allocated 22563.28955078125 
[2025-03-22 19:19:30 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:1.356804609298706 norm:0.0024959903676062822 max memory_allocated 22563.28955078125 
[2025-03-22 19:20:02 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:1.3561060428619385 norm:0.0023174849338829517 max memory_allocated 22563.28955078125 
[2025-03-22 19:20:35 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:1.3557220697402954 norm:0.002187448786571622 max memory_allocated 22563.28955078125 
[2025-03-22 19:21:07 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:1.3552368879318237 norm:0.002074537565931678 max memory_allocated 22563.28955078125 
[2025-03-22 19:21:40 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:1.3549327850341797 norm:0.001988248433917761 max memory_allocated 22563.28955078125 
[2025-03-22 19:22:13 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:1.3546767234802246 norm:0.001903943601064384 max memory_allocated 22563.28955078125 
[2025-03-22 19:22:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 19:22:57 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:1.6197105646133423 norm:0.010005840100347996 max memory_allocated 22563.46142578125 
[2025-03-22 19:23:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:1.599166750907898 norm:0.005586718674749136 max memory_allocated 22563.46142578125 
[2025-03-22 19:24:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:1.573500394821167 norm:0.002903472865000367 max memory_allocated 22563.46142578125 
[2025-03-22 19:24:35 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:1.562617301940918 norm:0.0018518920987844467 max memory_allocated 22563.46142578125 
[2025-03-22 19:25:07 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:1.5569716691970825 norm:0.0015377094969153404 max memory_allocated 22563.46142578125 
[2025-03-22 19:25:40 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:1.5527312755584717 norm:0.0014198538847267628 max memory_allocated 22563.46142578125 
[2025-03-22 19:26:13 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:1.5496913194656372 norm:0.0013588971924036741 max memory_allocated 22563.46142578125 
[2025-03-22 19:26:45 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:1.5476455688476562 norm:0.001325588091276586 max memory_allocated 22563.46142578125 
[2025-03-22 19:27:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:1.5461453199386597 norm:0.0012934546684846282 max memory_allocated 22563.46142578125 
[2025-03-22 19:27:51 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:1.5449566841125488 norm:0.0012689094291999936 max memory_allocated 22563.46142578125 
[2025-03-22 19:28:24 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:1.5441569089889526 norm:0.001252055517397821 max memory_allocated 22563.46142578125 
[2025-03-22 19:28:56 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:1.543563723564148 norm:0.001240551588125527 max memory_allocated 22563.46142578125 
[2025-03-22 19:29:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:1.543025255203247 norm:0.0012319417437538505 max memory_allocated 22563.46142578125 
[2025-03-22 19:30:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:1.542393684387207 norm:0.0012252870947122574 max memory_allocated 22563.46142578125 
[2025-03-22 19:30:34 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:1.5418846607208252 norm:0.0012264862889423966 max memory_allocated 22563.46142578125 
[2025-03-22 19:31:07 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:1.541620135307312 norm:0.0012215841561555862 max memory_allocated 22563.46142578125 
[2025-03-22 19:31:40 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:1.5413713455200195 norm:0.0012094298144802451 max memory_allocated 22563.46142578125 
[2025-03-22 19:32:13 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:1.541100263595581 norm:0.0012038860004395247 max memory_allocated 22563.46142578125 
[2025-03-22 19:32:45 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:1.540835976600647 norm:0.001203987980261445 max memory_allocated 22563.46142578125 
[2025-03-22 19:33:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:1.5406776666641235 norm:0.0012031798250973225 max memory_allocated 22563.46142578125 
[2025-03-22 19:33:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 19:34:02 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:1.840259313583374 norm:0.027133004739880562 max memory_allocated 22563.63330078125 
[2025-03-22 19:34:35 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:1.8093291521072388 norm:0.013708332553505898 max memory_allocated 22563.63330078125 
[2025-03-22 19:35:07 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:1.7792882919311523 norm:0.006587722804397345 max memory_allocated 22563.63330078125 
[2025-03-22 19:35:40 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:1.764753818511963 norm:0.003680977039039135 max memory_allocated 22563.63330078125 
[2025-03-22 19:36:12 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:1.7566667795181274 norm:0.0028603540267795324 max memory_allocated 22563.63330078125 
[2025-03-22 19:36:45 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:1.7509678602218628 norm:0.002293917117640376 max memory_allocated 22563.63330078125 
[2025-03-22 19:37:17 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:1.7470072507858276 norm:0.0019512567669153214 max memory_allocated 22563.63330078125 
[2025-03-22 19:37:50 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:1.7442468404769897 norm:0.0017299551982432604 max memory_allocated 22563.63330078125 
[2025-03-22 19:38:23 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:1.7421740293502808 norm:0.001582051278091967 max memory_allocated 22563.63330078125 
[2025-03-22 19:38:55 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:1.7408874034881592 norm:0.0014770857524126768 max memory_allocated 22563.63330078125 
[2025-03-22 19:39:28 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:1.7398864030838013 norm:0.0014010844752192497 max memory_allocated 22563.63330078125 
[2025-03-22 19:40:00 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:1.7391425371170044 norm:0.0013535743346437812 max memory_allocated 22563.63330078125 
[2025-03-22 19:40:33 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:1.7384788990020752 norm:0.0013158172369003296 max memory_allocated 22563.63330078125 
[2025-03-22 19:41:06 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:1.7379193305969238 norm:0.0012929340591654181 max memory_allocated 22563.63330078125 
[2025-03-22 19:41:38 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:1.7374299764633179 norm:0.0012686781119555235 max memory_allocated 22563.63330078125 
[2025-03-22 19:42:11 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:1.7370691299438477 norm:0.001252744346857071 max memory_allocated 22563.63330078125 
[2025-03-22 19:42:44 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:1.736751675605774 norm:0.0012409468181431293 max memory_allocated 22563.63330078125 
[2025-03-22 19:43:16 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:1.7364505529403687 norm:0.0012273334432393312 max memory_allocated 22563.63330078125 
[2025-03-22 19:43:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:1.736229419708252 norm:0.0012237726477906108 max memory_allocated 22563.63330078125 
[2025-03-22 19:44:22 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:1.7361528873443604 norm:0.0012202804209664464 max memory_allocated 22563.63330078125 
[2025-03-22 19:44:31 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 19:45:06 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:2.047555923461914 norm:0.02297094091773033 max memory_allocated 22563.80517578125 
[2025-03-22 19:45:39 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:2.0133073329925537 norm:0.010477648116648197 max memory_allocated 22563.80517578125 
[2025-03-22 19:46:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:1.9819086790084839 norm:0.006146499887108803 max memory_allocated 22563.80517578125 
[2025-03-22 19:46:44 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:1.965073823928833 norm:0.002847095485776663 max memory_allocated 22563.80517578125 
[2025-03-22 19:47:17 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:1.9574239253997803 norm:0.002164646051824093 max memory_allocated 22563.80517578125 
[2025-03-22 19:47:50 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:1.9524013996124268 norm:0.001962637063115835 max memory_allocated 22563.80517578125 
[2025-03-22 19:48:23 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:1.9487024545669556 norm:0.0017794424202293158 max memory_allocated 22563.80517578125 
[2025-03-22 19:48:55 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:1.9462125301361084 norm:0.0017369850538671017 max memory_allocated 22563.80517578125 
[2025-03-22 19:49:28 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:1.9446320533752441 norm:0.00166881806217134 max memory_allocated 22563.80517578125 
[2025-03-22 19:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:1.943228006362915 norm:0.0015811416087672114 max memory_allocated 22563.80517578125 
[2025-03-22 19:50:33 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:1.9421052932739258 norm:0.0015549752861261368 max memory_allocated 22563.80517578125 
[2025-03-22 19:51:06 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:1.9412612915039062 norm:0.0014942488633096218 max memory_allocated 22563.80517578125 
[2025-03-22 19:51:38 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:1.940429449081421 norm:0.0014667558716610074 max memory_allocated 22563.80517578125 
[2025-03-22 19:52:11 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:1.9398643970489502 norm:0.0013978686183691025 max memory_allocated 22563.80517578125 
[2025-03-22 19:52:43 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:1.9392229318618774 norm:0.0013551601441577077 max memory_allocated 22563.80517578125 
[2025-03-22 19:53:16 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:1.9386885166168213 norm:0.001340937800705433 max memory_allocated 22563.80517578125 
[2025-03-22 19:53:48 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:1.9379311800003052 norm:0.0013843948254361749 max memory_allocated 22563.80517578125 
[2025-03-22 19:54:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:1.937114953994751 norm:0.0013632484478875995 max memory_allocated 22563.80517578125 
[2025-03-22 19:54:54 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:1.9367719888687134 norm:0.0013330189976841211 max memory_allocated 22563.80517578125 
[2025-03-22 19:55:26 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:1.9370496273040771 norm:0.0012957303551957011 max memory_allocated 22563.80517578125 
[2025-03-22 19:55:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 19:55:38 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 19:56:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:2.3568387031555176 norm:0.06866823881864548 max memory_allocated 22564.43798828125 
[2025-03-22 19:56:43 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:2.3230135440826416 norm:0.05840856954455376 max memory_allocated 22564.43798828125 
[2025-03-22 19:57:16 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:2.2857987880706787 norm:0.04422600194811821 max memory_allocated 22564.43798828125 
[2025-03-22 19:57:49 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:2.263138771057129 norm:0.03414244204759598 max memory_allocated 22564.43798828125 
[2025-03-22 19:58:22 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:2.2508790493011475 norm:0.027743041515350342 max memory_allocated 22564.43798828125 
[2025-03-22 19:58:54 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:2.2413454055786133 norm:0.02341763861477375 max memory_allocated 22564.43798828125 
[2025-03-22 19:59:27 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:2.2345876693725586 norm:0.021346434950828552 max memory_allocated 22564.43798828125 
[2025-03-22 20:00:00 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:2.229053497314453 norm:0.019350362941622734 max memory_allocated 22564.43798828125 
[2025-03-22 20:00:33 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:2.2254045009613037 norm:0.018896371126174927 max memory_allocated 22564.43798828125 
[2025-03-22 20:01:06 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:2.221871852874756 norm:0.017829719930887222 max memory_allocated 22564.43798828125 
[2025-03-22 20:01:39 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:2.218696117401123 norm:0.01754669100046158 max memory_allocated 22564.43798828125 
[2025-03-22 20:02:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:2.216141700744629 norm:0.016567517071962357 max memory_allocated 22564.43798828125 
[2025-03-22 20:02:44 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:2.213625192642212 norm:0.01612452045083046 max memory_allocated 22564.43798828125 
[2025-03-22 20:03:17 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:2.211362838745117 norm:0.01567540317773819 max memory_allocated 22564.43798828125 
[2025-03-22 20:03:50 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:2.2092041969299316 norm:0.015152026899158955 max memory_allocated 22564.43798828125 
[2025-03-22 20:04:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:2.208035945892334 norm:0.015105227008461952 max memory_allocated 22564.43798828125 
[2025-03-22 20:04:56 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:2.205878257751465 norm:0.014419835060834885 max memory_allocated 22564.43798828125 
[2025-03-22 20:05:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:2.204515218734741 norm:0.01432199776172638 max memory_allocated 22564.43798828125 
[2025-03-22 20:06:01 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:2.2030749320983887 norm:0.013990783132612705 max memory_allocated 22564.43798828125 
[2025-03-22 20:06:34 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:2.202207326889038 norm:0.013935064896941185 max memory_allocated 22564.43798828125 
[2025-03-22 20:06:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 20:06:46 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:07:19 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:2.675153970718384 norm:0.0714704841375351 max memory_allocated 22564.60986328125 
[2025-03-22 20:07:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:2.6287150382995605 norm:0.059401072561740875 max memory_allocated 22564.60986328125 
[2025-03-22 20:08:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:2.5771491527557373 norm:0.04340624436736107 max memory_allocated 22564.60986328125 
[2025-03-22 20:08:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:2.5485422611236572 norm:0.03371672332286835 max memory_allocated 22564.60986328125 
[2025-03-22 20:09:29 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:2.5340065956115723 norm:0.026478946208953857 max memory_allocated 22564.60986328125 
[2025-03-22 20:10:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:2.5239901542663574 norm:0.021255940198898315 max memory_allocated 22564.60986328125 
[2025-03-22 20:10:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:2.5173442363739014 norm:0.018352791666984558 max memory_allocated 22564.60986328125 
[2025-03-22 20:11:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:2.51300048828125 norm:0.01753653958439827 max memory_allocated 22564.60986328125 
[2025-03-22 20:11:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:2.5114681720733643 norm:0.01684887520968914 max memory_allocated 22564.60986328125 
[2025-03-22 20:12:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:2.509040355682373 norm:0.01711386628448963 max memory_allocated 22564.60986328125 
[2025-03-22 20:12:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:2.5096182823181152 norm:0.014178418554365635 max memory_allocated 22564.60986328125 
[2025-03-22 20:13:19 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:2.5049359798431396 norm:0.014952681958675385 max memory_allocated 22564.60986328125 
[2025-03-22 20:13:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:2.5030112266540527 norm:0.013541254214942455 max memory_allocated 22564.60986328125 
[2025-03-22 20:14:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:2.5019590854644775 norm:0.014306768774986267 max memory_allocated 22564.60986328125 
[2025-03-22 20:14:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:2.5012569427490234 norm:0.01371948141604662 max memory_allocated 22564.60986328125 
[2025-03-22 20:15:30 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:2.4999115467071533 norm:0.013605060987174511 max memory_allocated 22564.60986328125 
[2025-03-22 20:16:03 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:2.4994418621063232 norm:0.01278157252818346 max memory_allocated 22564.60986328125 
[2025-03-22 20:16:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:2.4983832836151123 norm:0.013166439719498158 max memory_allocated 22564.60986328125 
[2025-03-22 20:17:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:2.4985713958740234 norm:0.012281801551580429 max memory_allocated 22564.60986328125 
[2025-03-22 20:17:41 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:2.497714042663574 norm:0.01312629971653223 max memory_allocated 22564.60986328125 
[2025-03-22 20:17:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 20:17:53 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:18:26 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:3.529771089553833 norm:0.15309329330921173 max memory_allocated 22564.78173828125 
[2025-03-22 20:18:59 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:3.4159560203552246 norm:0.11215419322252274 max memory_allocated 22564.78173828125 
[2025-03-22 20:19:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:3.2687880992889404 norm:0.0880640298128128 max memory_allocated 22564.78173828125 
[2025-03-22 20:20:04 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:3.2024784088134766 norm:0.07203634083271027 max memory_allocated 22564.78173828125 
[2025-03-22 20:20:37 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:3.1709651947021484 norm:0.06507161259651184 max memory_allocated 22564.78173828125 
[2025-03-22 20:21:10 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:3.149831771850586 norm:0.0612591952085495 max memory_allocated 22564.78173828125 
[2025-03-22 20:21:43 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:3.138355016708374 norm:0.06042569503188133 max memory_allocated 22564.78173828125 
[2025-03-22 20:22:16 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:3.1335465908050537 norm:0.0669323205947876 max memory_allocated 22564.78173828125 
[2025-03-22 20:22:48 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:3.118001699447632 norm:0.06879198551177979 max memory_allocated 22564.78173828125 
[2025-03-22 20:23:21 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:3.1103146076202393 norm:0.06654275208711624 max memory_allocated 22564.78173828125 
[2025-03-22 20:23:54 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:3.106405735015869 norm:0.06705322861671448 max memory_allocated 22564.78173828125 
[2025-03-22 20:24:27 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:3.097001075744629 norm:0.06745050847530365 max memory_allocated 22564.78173828125 
[2025-03-22 20:24:59 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:3.094149351119995 norm:0.06239117681980133 max memory_allocated 22564.78173828125 
[2025-03-22 20:25:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:3.094180107116699 norm:0.06651808321475983 max memory_allocated 22564.78173828125 
[2025-03-22 20:26:05 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:3.087111473083496 norm:0.05937010049819946 max memory_allocated 22564.78173828125 
[2025-03-22 20:26:37 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:3.084455728530884 norm:0.05989770591259003 max memory_allocated 22564.78173828125 
[2025-03-22 20:27:10 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:3.0855770111083984 norm:0.057979121804237366 max memory_allocated 22564.78173828125 
[2025-03-22 20:27:43 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:3.0863802433013916 norm:0.056839849799871445 max memory_allocated 22564.78173828125 
[2025-03-22 20:28:15 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:3.086653470993042 norm:0.06422179192304611 max memory_allocated 22564.78173828125 
[2025-03-22 20:28:48 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:3.0830984115600586 norm:0.06235099956393242 max memory_allocated 22564.78173828125 
[2025-03-22 20:28:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 20:29:00 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:29:33 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:6.57478666305542 norm:0.5807633399963379 max memory_allocated 22564.95361328125 
[2025-03-22 20:30:05 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:6.100169658660889 norm:0.44686296582221985 max memory_allocated 22564.95361328125 
[2025-03-22 20:30:38 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:5.754988670349121 norm:0.37236154079437256 max memory_allocated 22564.95361328125 
[2025-03-22 20:31:11 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:5.544672966003418 norm:0.3378336727619171 max memory_allocated 22564.95361328125 
[2025-03-22 20:31:44 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:5.4357757568359375 norm:0.29989537596702576 max memory_allocated 22564.95361328125 
[2025-03-22 20:32:16 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:5.35330867767334 norm:0.24977125227451324 max memory_allocated 22564.95361328125 
[2025-03-22 20:32:49 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:5.279918193817139 norm:0.23811247944831848 max memory_allocated 22564.95361328125 
[2025-03-22 20:33:22 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:5.237183570861816 norm:0.22960165143013 max memory_allocated 22564.95361328125 
[2025-03-22 20:33:55 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:5.197721004486084 norm:0.2173164188861847 max memory_allocated 22564.95361328125 
[2025-03-22 20:34:28 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:5.170608997344971 norm:0.2115778625011444 max memory_allocated 22564.95361328125 
[2025-03-22 20:35:00 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:5.145503520965576 norm:0.20666024088859558 max memory_allocated 22564.95361328125 
[2025-03-22 20:35:33 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:5.119452476501465 norm:0.19499608874320984 max memory_allocated 22564.95361328125 
[2025-03-22 20:36:06 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:5.10330867767334 norm:0.1935754120349884 max memory_allocated 22564.95361328125 
[2025-03-22 20:36:39 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:5.084677219390869 norm:0.18693950772285461 max memory_allocated 22564.95361328125 
[2025-03-22 20:37:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:5.060666561126709 norm:0.16992418467998505 max memory_allocated 22564.95361328125 
[2025-03-22 20:37:45 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:5.057021141052246 norm:0.17627854645252228 max memory_allocated 22564.95361328125 
[2025-03-22 20:38:17 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:5.037031173706055 norm:0.1598636656999588 max memory_allocated 22564.95361328125 
[2025-03-22 20:38:50 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:5.026889324188232 norm:0.15599018335342407 max memory_allocated 22564.95361328125 
[2025-03-22 20:39:23 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:5.021363735198975 norm:0.1594582349061966 max memory_allocated 22564.95361328125 
[2025-03-22 20:39:56 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:5.022129535675049 norm:0.16495634615421295 max memory_allocated 22564.95361328125 
[2025-03-22 20:40:05 root] (main_calibration_a.py 369): INFO 21275.842851877213
[2025-03-22 20:40:13 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-22 20:41:22 root] (main_calibration_a.py 158): INFO wikitext2 : 8.412346839904785
[2025-03-22 20:41:22 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-22 20:43:07 root] (main_calibration_a.py 158): INFO c4 : 12.391630172729492
[2025-03-22 22:37:13 root] (main_calibration_a.py 169): INFO {'wikitext2': 8.412346839904785, 'c4': 12.391630172729492, 'results': {'hellaswag': {'acc': 0.45050786695877315, 'acc_stderr': 0.004965276587781616, 'acc_norm': 0.5886277633937462, 'acc_norm_stderr': 0.004910767540867415}, 'boolq': {'acc': 0.6403669724770642, 'acc_stderr': 0.008393378084399047}, 'arc_easy': {'acc': 0.5113636363636364, 'acc_stderr': 0.010257133441117111, 'acc_norm': 0.42213804713804715, 'acc_norm_stderr': 0.010134620524592271}, 'winogrande': {'acc': 0.5351223362273086, 'acc_stderr': 0.014017773120881587}, 'piqa': {'acc': 0.6898803046789989, 'acc_stderr': 0.010791876566843057, 'acc_norm': 0.6877040261153428, 'acc_norm_stderr': 0.010812581599154424}, 'arc_challenge': {'acc': 0.28242320819112626, 'acc_stderr': 0.013155456884097224, 'acc_norm': 0.3165529010238908, 'acc_norm_stderr': 0.013592431519068077}}, 'versions': {'hellaswag': 0, 'boolq': 1, 'arc_easy': 0, 'winogrande': 0, 'piqa': 0, 'arc_challenge': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 22:37:13 root] (main_calibration_a.py 172): INFO 28.24,51.14,64.04,45.05,68.99,53.51
