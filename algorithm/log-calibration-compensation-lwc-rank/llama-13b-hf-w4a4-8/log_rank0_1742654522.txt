[2025-03-22 14:42:02 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/llama-13b-hf-w4a4-8', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=8)
[2025-03-22 14:48:36 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 14:48:36 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:48:37 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 14:48:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 14:48:43 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:49:30 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.14228321611881256 norm:0.1479344367980957 max memory_allocated 29269.53759765625 
[2025-03-22 14:50:18 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.06681077182292938 norm:0.06767989695072174 max memory_allocated 29269.53759765625 
[2025-03-22 14:51:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.04995051026344299 norm:0.042241014540195465 max memory_allocated 29269.53759765625 
[2025-03-22 14:51:53 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.043836191296577454 norm:0.03529238700866699 max memory_allocated 29269.53759765625 
[2025-03-22 14:52:41 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.04131535068154335 norm:0.03134399652481079 max memory_allocated 29269.53759765625 
[2025-03-22 14:53:29 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.03924279287457466 norm:0.02878706529736519 max memory_allocated 29269.53759765625 
[2025-03-22 14:54:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.03772586211562157 norm:0.025455210357904434 max memory_allocated 29269.53759765625 
[2025-03-22 14:55:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.03660821542143822 norm:0.02280430868268013 max memory_allocated 29269.53759765625 
[2025-03-22 14:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.035655129700899124 norm:0.02024330571293831 max memory_allocated 29269.53759765625 
[2025-03-22 14:56:40 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.03493424877524376 norm:0.01807284727692604 max memory_allocated 29269.53759765625 
[2025-03-22 14:57:28 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.03431909158825874 norm:0.015927644446492195 max memory_allocated 29269.53759765625 
[2025-03-22 14:58:16 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.03393823280930519 norm:0.014384053647518158 max memory_allocated 29269.53759765625 
[2025-03-22 14:59:03 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.03354775905609131 norm:0.013082641176879406 max memory_allocated 29269.53759765625 
[2025-03-22 14:59:51 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.03316090628504753 norm:0.011797512881457806 max memory_allocated 29269.53759765625 
[2025-03-22 15:00:39 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.03324323520064354 norm:0.01096147671341896 max memory_allocated 29269.53759765625 
[2025-03-22 15:01:27 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.032983481884002686 norm:0.010032989084720612 max memory_allocated 29269.53759765625 
[2025-03-22 15:02:15 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.03281141445040703 norm:0.009462777525186539 max memory_allocated 29269.53759765625 
[2025-03-22 15:03:03 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.032650161534547806 norm:0.008723788894712925 max memory_allocated 29269.53759765625 
[2025-03-22 15:03:51 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.032474368810653687 norm:0.008257372304797173 max memory_allocated 29269.53759765625 
[2025-03-22 15:04:39 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.0323704369366169 norm:0.00796580407768488 max memory_allocated 29269.53759765625 
[2025-03-22 15:04:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 15:04:56 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:05:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.29787978529930115 norm:0.1767502725124359 max memory_allocated 29269.72509765625 
[2025-03-22 15:06:32 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.18335680663585663 norm:0.09504719078540802 max memory_allocated 29269.72509765625 
[2025-03-22 15:07:20 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.13078531622886658 norm:0.04386839643120766 max memory_allocated 29269.72509765625 
[2025-03-22 15:08:08 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.11033459007740021 norm:0.032602909952402115 max memory_allocated 29269.72509765625 
[2025-03-22 15:08:56 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.10030334442853928 norm:0.02594968117773533 max memory_allocated 29269.72509765625 
[2025-03-22 15:09:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.0943717211484909 norm:0.021566664800047874 max memory_allocated 29269.72509765625 
[2025-03-22 15:10:32 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.0904991626739502 norm:0.0186237134039402 max memory_allocated 29269.72509765625 
[2025-03-22 15:11:20 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.08764473348855972 norm:0.016056979075074196 max memory_allocated 29269.72509765625 
[2025-03-22 15:12:08 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.085736483335495 norm:0.013950610533356667 max memory_allocated 29269.72509765625 
[2025-03-22 15:12:55 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.08417894691228867 norm:0.011907367967069149 max memory_allocated 29269.72509765625 
[2025-03-22 15:13:43 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.08308916538953781 norm:0.010434769093990326 max memory_allocated 29269.72509765625 
[2025-03-22 15:14:31 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.08219131082296371 norm:0.009193471632897854 max memory_allocated 29269.72509765625 
[2025-03-22 15:15:19 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.08147035539150238 norm:0.008221108466386795 max memory_allocated 29269.72509765625 
[2025-03-22 15:16:07 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.08091661334037781 norm:0.0075898668728768826 max memory_allocated 29269.72509765625 
[2025-03-22 15:16:55 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.08062171190977097 norm:0.007701556198298931 max memory_allocated 29269.72509765625 
[2025-03-22 15:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.08045012503862381 norm:0.0075678108260035515 max memory_allocated 29269.72509765625 
[2025-03-22 15:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.08027312904596329 norm:0.007456127554178238 max memory_allocated 29269.72509765625 
[2025-03-22 15:19:19 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.08012786507606506 norm:0.0069823539815843105 max memory_allocated 29269.72509765625 
[2025-03-22 15:20:07 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07970792055130005 norm:0.006797648500651121 max memory_allocated 29269.72509765625 
[2025-03-22 15:20:55 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07971929758787155 norm:0.006431118119508028 max memory_allocated 29269.72509765625 
[2025-03-22 15:21:09 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 15:21:13 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:22:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.3171263337135315 norm:0.06196066364645958 max memory_allocated 29269.72509765625 
[2025-03-22 15:22:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.2581999599933624 norm:0.05426642298698425 max memory_allocated 29269.72509765625 
[2025-03-22 15:23:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.22436389327049255 norm:0.04843798279762268 max memory_allocated 29269.72509765625 
[2025-03-22 15:24:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.202309712767601 norm:0.039678484201431274 max memory_allocated 29269.72509765625 
[2025-03-22 15:25:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.19175009429454803 norm:0.04017488658428192 max memory_allocated 29269.72509765625 
[2025-03-22 15:26:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.18245960772037506 norm:0.03793778270483017 max memory_allocated 29269.72509765625 
[2025-03-22 15:26:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.17848090827465057 norm:0.03477976843714714 max memory_allocated 29269.72509765625 
[2025-03-22 15:27:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.1733298897743225 norm:0.03531329333782196 max memory_allocated 29269.72509765625 
[2025-03-22 15:28:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.1695687174797058 norm:0.03424680233001709 max memory_allocated 29269.72509765625 
[2025-03-22 15:29:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.16515864431858063 norm:0.034021563827991486 max memory_allocated 29269.72509765625 
[2025-03-22 15:30:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.16214756667613983 norm:0.03361482918262482 max memory_allocated 29269.72509765625 
[2025-03-22 15:30:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.16022899746894836 norm:0.030640942975878716 max memory_allocated 29269.72509765625 
[2025-03-22 15:31:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.16001875698566437 norm:0.03005940653383732 max memory_allocated 29269.72509765625 
[2025-03-22 15:32:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.157136932015419 norm:0.029993651434779167 max memory_allocated 29269.72509765625 
[2025-03-22 15:33:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.15623421967029572 norm:0.033795394003391266 max memory_allocated 29269.72509765625 
[2025-03-22 15:34:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.15674352645874023 norm:0.02465439960360527 max memory_allocated 29269.72509765625 
[2025-03-22 15:34:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.1537221074104309 norm:0.026056062430143356 max memory_allocated 29269.72509765625 
[2025-03-22 15:35:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.15397310256958008 norm:0.02591746859252453 max memory_allocated 29269.72509765625 
[2025-03-22 15:36:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.15359050035476685 norm:0.025542087852954865 max memory_allocated 29269.72509765625 
[2025-03-22 15:37:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.15174366533756256 norm:0.02337888441979885 max memory_allocated 29269.72509765625 
[2025-03-22 15:37:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 15:38:19 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.25395339727401733 norm:0.03126242011785507 max memory_allocated 29269.72509765625 
[2025-03-22 15:39:07 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.22363850474357605 norm:0.013154802843928337 max memory_allocated 29269.72509765625 
[2025-03-22 15:39:55 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.20208428800106049 norm:0.00778349582105875 max memory_allocated 29269.72509765625 
[2025-03-22 15:40:43 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.19215480983257294 norm:0.005585251376032829 max memory_allocated 29269.72509765625 
[2025-03-22 15:41:31 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.1863684505224228 norm:0.00413049990311265 max memory_allocated 29269.72509765625 
[2025-03-22 15:42:18 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.1836695820093155 norm:0.0035268620122224092 max memory_allocated 29269.72509765625 
[2025-03-22 15:43:06 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.18096576631069183 norm:0.0030348931904882193 max memory_allocated 29269.72509765625 
[2025-03-22 15:43:54 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.1787758320569992 norm:0.002487011719495058 max memory_allocated 29269.72509765625 
[2025-03-22 15:44:42 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.17694081366062164 norm:0.0021176235750317574 max memory_allocated 29269.72509765625 
[2025-03-22 15:45:29 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.17611157894134521 norm:0.0019858418963849545 max memory_allocated 29269.72509765625 
[2025-03-22 15:46:17 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.17514368891716003 norm:0.001868933904916048 max memory_allocated 29269.72509765625 
[2025-03-22 15:47:05 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.17440193891525269 norm:0.0017914099153131247 max memory_allocated 29269.72509765625 
[2025-03-22 15:47:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.17448565363883972 norm:0.001803786726668477 max memory_allocated 29269.72509765625 
[2025-03-22 15:48:41 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.174746572971344 norm:0.001871559419669211 max memory_allocated 29269.72509765625 
[2025-03-22 15:49:28 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.17413456737995148 norm:0.0016953162848949432 max memory_allocated 29269.72509765625 
[2025-03-22 15:50:16 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.17410999536514282 norm:0.0016616396605968475 max memory_allocated 29269.72509765625 
[2025-03-22 15:51:04 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.17387650907039642 norm:0.0016166804125532508 max memory_allocated 29269.72509765625 
[2025-03-22 15:51:52 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.1731877624988556 norm:0.0015088128857314587 max memory_allocated 29269.72509765625 
[2025-03-22 15:52:40 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.17304009199142456 norm:0.0014968449249863625 max memory_allocated 29269.72509765625 
[2025-03-22 15:53:28 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.17309632897377014 norm:0.0015333128394559026 max memory_allocated 29269.72509765625 
[2025-03-22 15:53:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 15:54:34 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.3029657006263733 norm:0.04057599604129791 max memory_allocated 29269.72509765625 
[2025-03-22 15:55:22 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.26900768280029297 norm:0.014422878623008728 max memory_allocated 29269.72509765625 
[2025-03-22 15:56:10 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.2422591745853424 norm:0.006596756633371115 max memory_allocated 29269.72509765625 
[2025-03-22 15:56:58 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.23413054645061493 norm:0.005104553885757923 max memory_allocated 29269.72509765625 
[2025-03-22 15:57:46 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.2299642413854599 norm:0.004167341161519289 max memory_allocated 29269.72509765625 
[2025-03-22 15:58:34 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.22738583385944366 norm:0.0037405528128147125 max memory_allocated 29269.72509765625 
[2025-03-22 15:59:22 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.22509081661701202 norm:0.0032328106462955475 max memory_allocated 29269.72509765625 
[2025-03-22 16:00:10 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.22386561334133148 norm:0.0029709991067647934 max memory_allocated 29269.72509765625 
[2025-03-22 16:00:58 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.22327053546905518 norm:0.0028819451108574867 max memory_allocated 29269.72509765625 
[2025-03-22 16:01:46 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.22200489044189453 norm:0.0025652870535850525 max memory_allocated 29269.72509765625 
[2025-03-22 16:02:34 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.2220502495765686 norm:0.0025159798096865416 max memory_allocated 29269.72509765625 
[2025-03-22 16:03:22 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.22238439321517944 norm:0.0026387651450932026 max memory_allocated 29269.72509765625 
[2025-03-22 16:04:10 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.222188800573349 norm:0.0025714365765452385 max memory_allocated 29269.72509765625 
[2025-03-22 16:04:58 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.2207949161529541 norm:0.002115194220095873 max memory_allocated 29269.72509765625 
[2025-03-22 16:05:46 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.2197904884815216 norm:0.0018771776231005788 max memory_allocated 29269.72509765625 
[2025-03-22 16:06:33 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.21928814053535461 norm:0.001729894895106554 max memory_allocated 29269.72509765625 
[2025-03-22 16:07:21 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.21916766464710236 norm:0.0017724365461617708 max memory_allocated 29269.72509765625 
[2025-03-22 16:08:09 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.2193087637424469 norm:0.001798229874111712 max memory_allocated 29269.72509765625 
[2025-03-22 16:08:57 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.21890509128570557 norm:0.0017182056326419115 max memory_allocated 29269.72509765625 
[2025-03-22 16:09:45 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.21847237646579742 norm:0.0016793018439784646 max memory_allocated 29269.72509765625 
[2025-03-22 16:09:58 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 16:10:51 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.36168038845062256 norm:0.058461450040340424 max memory_allocated 29269.72509765625 
[2025-03-22 16:11:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.3122117817401886 norm:0.017531150951981544 max memory_allocated 29269.72509765625 
[2025-03-22 16:12:27 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.2797447443008423 norm:0.00786083098500967 max memory_allocated 29269.72509765625 
[2025-03-22 16:13:15 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.26832854747772217 norm:0.005487783811986446 max memory_allocated 29269.72509765625 
[2025-03-22 16:14:03 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.26282942295074463 norm:0.004213580396026373 max memory_allocated 29269.72509765625 
[2025-03-22 16:14:51 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.259555846452713 norm:0.0038077104836702347 max memory_allocated 29269.72509765625 
[2025-03-22 16:15:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.2571020722389221 norm:0.0034102085046470165 max memory_allocated 29269.72509765625 
[2025-03-22 16:16:27 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.2554020583629608 norm:0.0031192877795547247 max memory_allocated 29269.72509765625 
[2025-03-22 16:17:15 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.2541705071926117 norm:0.0028907684609293938 max memory_allocated 29269.72509765625 
[2025-03-22 16:18:03 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.25363245606422424 norm:0.0027992944233119488 max memory_allocated 29269.72509765625 
[2025-03-22 16:18:51 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.2532091736793518 norm:0.00273932539857924 max memory_allocated 29269.72509765625 
[2025-03-22 16:19:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.25248390436172485 norm:0.0025576860643923283 max memory_allocated 29269.72509765625 
[2025-03-22 16:20:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.25204119086265564 norm:0.0023372857831418514 max memory_allocated 29269.72509765625 
[2025-03-22 16:21:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.2514050602912903 norm:0.0022006791550666094 max memory_allocated 29269.72509765625 
[2025-03-22 16:22:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.25067079067230225 norm:0.0019821892492473125 max memory_allocated 29269.72509765625 
[2025-03-22 16:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.25005486607551575 norm:0.001853638794273138 max memory_allocated 29269.72509765625 
[2025-03-22 16:23:40 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.24981984496116638 norm:0.001787821645848453 max memory_allocated 29269.72509765625 
[2025-03-22 16:24:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.2490413784980774 norm:0.0016350443474948406 max memory_allocated 29269.72509765625 
[2025-03-22 16:25:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.24868793785572052 norm:0.0015246475813910365 max memory_allocated 29269.72509765625 
[2025-03-22 16:26:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.2485928237438202 norm:0.001553257112391293 max memory_allocated 29269.72509765625 
[2025-03-22 16:26:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 16:27:09 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.4334813058376312 norm:0.016422947868704796 max memory_allocated 29269.72509765625 
[2025-03-22 16:27:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.396709680557251 norm:0.00890401378273964 max memory_allocated 29269.72509765625 
[2025-03-22 16:28:45 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.3648415803909302 norm:0.005636860616505146 max memory_allocated 29269.72509765625 
[2025-03-22 16:29:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.3525592088699341 norm:0.004411325324326754 max memory_allocated 29269.72509765625 
[2025-03-22 16:30:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.34652677178382874 norm:0.004190666601061821 max memory_allocated 29269.72509765625 
[2025-03-22 16:31:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.3444426655769348 norm:0.004558485932648182 max memory_allocated 29269.72509765625 
[2025-03-22 16:31:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.3366275131702423 norm:0.0037404566537588835 max memory_allocated 29269.72509765625 
[2025-03-22 16:32:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.3330921232700348 norm:0.003478085156530142 max memory_allocated 29269.72509765625 
[2025-03-22 16:33:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.3313406705856323 norm:0.003481534542515874 max memory_allocated 29269.72509765625 
[2025-03-22 16:34:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.3294055461883545 norm:0.0034236381761729717 max memory_allocated 29269.72509765625 
[2025-03-22 16:35:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.3277135491371155 norm:0.0033728049602359533 max memory_allocated 29269.72509765625 
[2025-03-22 16:35:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.32641226053237915 norm:0.003360805567353964 max memory_allocated 29269.72509765625 
[2025-03-22 16:36:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.3257475793361664 norm:0.0033806546125561 max memory_allocated 29269.72509765625 
[2025-03-22 16:37:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.3255613148212433 norm:0.0036015468649566174 max memory_allocated 29269.72509765625 
[2025-03-22 16:38:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.32442399859428406 norm:0.0033477640245109797 max memory_allocated 29269.72509765625 
[2025-03-22 16:39:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.32421189546585083 norm:0.003566254395991564 max memory_allocated 29269.72509765625 
[2025-03-22 16:39:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.32525184750556946 norm:0.003551051253452897 max memory_allocated 29269.72509765625 
[2025-03-22 16:40:45 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.3241189122200012 norm:0.003454008372500539 max memory_allocated 29269.72509765625 
[2025-03-22 16:41:33 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.32408827543258667 norm:0.0037538420874625444 max memory_allocated 29269.72509765625 
[2025-03-22 16:42:21 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.322666198015213 norm:0.003666563658043742 max memory_allocated 29269.72509765625 
[2025-03-22 16:42:34 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 16:43:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.4446317255496979 norm:0.019182540476322174 max memory_allocated 29269.72509765625 
[2025-03-22 16:44:14 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.41125673055648804 norm:0.009166083298623562 max memory_allocated 29269.72509765625 
[2025-03-22 16:45:02 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.3742712140083313 norm:0.0038472474552690983 max memory_allocated 29269.72509765625 
[2025-03-22 16:45:50 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.3584829270839691 norm:0.002216098364442587 max memory_allocated 29269.72509765625 
[2025-03-22 16:46:38 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.351917028427124 norm:0.001673487713560462 max memory_allocated 29269.72509765625 
[2025-03-22 16:47:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.3476957678794861 norm:0.0014084669528529048 max memory_allocated 29269.72509765625 
[2025-03-22 16:48:14 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.34457826614379883 norm:0.0012574726715683937 max memory_allocated 29269.72509765625 
[2025-03-22 16:49:02 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.3422410488128662 norm:0.0011614114046096802 max memory_allocated 29269.72509765625 
[2025-03-22 16:49:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.3406423330307007 norm:0.0011042673140764236 max memory_allocated 29269.72509765625 
[2025-03-22 16:50:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.3394790589809418 norm:0.0010740963043645024 max memory_allocated 29269.72509765625 
[2025-03-22 16:51:25 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.3387223482131958 norm:0.0010466236853972077 max memory_allocated 29269.72509765625 
[2025-03-22 16:52:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.33802467584609985 norm:0.0010343046160414815 max memory_allocated 29269.72509765625 
[2025-03-22 16:53:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.33756667375564575 norm:0.001032225671224296 max memory_allocated 29269.72509765625 
[2025-03-22 16:53:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.337185263633728 norm:0.0010275730164721608 max memory_allocated 29269.72509765625 
[2025-03-22 16:54:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.33675992488861084 norm:0.0010301371803507209 max memory_allocated 29269.72509765625 
[2025-03-22 16:55:25 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.33643385767936707 norm:0.001024198834784329 max memory_allocated 29269.72509765625 
[2025-03-22 16:56:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.33616960048675537 norm:0.0010091656586155295 max memory_allocated 29269.72509765625 
[2025-03-22 16:57:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.33599889278411865 norm:0.0010148541769012809 max memory_allocated 29269.72509765625 
[2025-03-22 16:57:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.33597618341445923 norm:0.001001620665192604 max memory_allocated 29269.72509765625 
[2025-03-22 16:58:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.3357674181461334 norm:0.0009764451533555984 max memory_allocated 29269.72509765625 
[2025-03-22 16:58:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 16:59:42 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.46588805317878723 norm:0.018007023259997368 max memory_allocated 29269.88134765625 
[2025-03-22 17:00:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.4331672787666321 norm:0.009413784369826317 max memory_allocated 29269.88134765625 
[2025-03-22 17:01:18 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.39549916982650757 norm:0.004371225368231535 max memory_allocated 29269.88134765625 
[2025-03-22 17:02:06 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.37876126170158386 norm:0.002382435603067279 max memory_allocated 29269.88134765625 
[2025-03-22 17:02:54 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.3720133602619171 norm:0.0017068402376025915 max memory_allocated 29269.88134765625 
[2025-03-22 17:03:42 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.3676404356956482 norm:0.0013900682097300887 max memory_allocated 29269.88134765625 
[2025-03-22 17:04:31 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.36442726850509644 norm:0.0012121417094022036 max memory_allocated 29269.88134765625 
[2025-03-22 17:05:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.3622090816497803 norm:0.0010905055096372962 max memory_allocated 29269.88134765625 
[2025-03-22 17:06:07 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.36068201065063477 norm:0.0010278768604621291 max memory_allocated 29269.88134765625 
[2025-03-22 17:06:55 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.35949844121932983 norm:0.0010067768162116408 max memory_allocated 29269.88134765625 
[2025-03-22 17:07:43 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.35861045122146606 norm:0.0009796986123546958 max memory_allocated 29269.88134765625 
[2025-03-22 17:08:31 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.3579779267311096 norm:0.0009995275177061558 max memory_allocated 29269.88134765625 
[2025-03-22 17:09:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.3574621379375458 norm:0.0009835611563175917 max memory_allocated 29269.88134765625 
[2025-03-22 17:10:07 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.35714051127433777 norm:0.0009534565033391118 max memory_allocated 29269.88134765625 
[2025-03-22 17:10:55 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.35689806938171387 norm:0.0009500769665464759 max memory_allocated 29269.88134765625 
[2025-03-22 17:11:42 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.3566327691078186 norm:0.0009489936055615544 max memory_allocated 29269.88134765625 
[2025-03-22 17:12:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.3563796877861023 norm:0.0009351091575808823 max memory_allocated 29269.88134765625 
[2025-03-22 17:13:18 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.35628536343574524 norm:0.0009468468488194048 max memory_allocated 29269.88134765625 
[2025-03-22 17:14:06 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.35616883635520935 norm:0.0009566877852194011 max memory_allocated 29269.88134765625 
[2025-03-22 17:14:54 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.3560448884963989 norm:0.0009423059527762234 max memory_allocated 29269.88134765625 
[2025-03-22 17:15:07 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 17:16:00 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.4936909079551697 norm:0.017016079276800156 max memory_allocated 29270.06884765625 
[2025-03-22 17:16:48 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.4553259313106537 norm:0.008811111561954021 max memory_allocated 29270.06884765625 
[2025-03-22 17:17:36 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.41003361344337463 norm:0.003480144776403904 max memory_allocated 29270.06884765625 
[2025-03-22 17:18:24 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.3950926959514618 norm:0.0018622911302372813 max memory_allocated 29270.06884765625 
[2025-03-22 17:19:12 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.38920485973358154 norm:0.0013849121751263738 max memory_allocated 29270.06884765625 
[2025-03-22 17:20:01 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.38545361161231995 norm:0.001160112558864057 max memory_allocated 29270.06884765625 
[2025-03-22 17:20:49 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.38274142146110535 norm:0.001073027029633522 max memory_allocated 29270.06884765625 
[2025-03-22 17:21:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.380859375 norm:0.0010063794907182455 max memory_allocated 29270.06884765625 
[2025-03-22 17:22:25 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.3793650269508362 norm:0.0009616492316126823 max memory_allocated 29270.06884765625 
[2025-03-22 17:23:13 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.37829428911209106 norm:0.0009332549525424838 max memory_allocated 29270.06884765625 
[2025-03-22 17:24:01 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.3775279223918915 norm:0.0009207414113916457 max memory_allocated 29270.06884765625 
[2025-03-22 17:24:49 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.3769252598285675 norm:0.0009059898438863456 max memory_allocated 29270.06884765625 
[2025-03-22 17:25:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.3763430416584015 norm:0.0009009162313304842 max memory_allocated 29270.06884765625 
[2025-03-22 17:26:25 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.37587982416152954 norm:0.0009023998281918466 max memory_allocated 29270.06884765625 
[2025-03-22 17:27:13 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.37551742792129517 norm:0.0009021966834552586 max memory_allocated 29270.06884765625 
[2025-03-22 17:28:00 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.37534815073013306 norm:0.0008843103423714638 max memory_allocated 29270.06884765625 
[2025-03-22 17:28:48 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.3750896155834198 norm:0.0008681060280650854 max memory_allocated 29270.06884765625 
[2025-03-22 17:29:36 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.3749009072780609 norm:0.0008629217627458274 max memory_allocated 29270.06884765625 
[2025-03-22 17:30:24 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.37474650144577026 norm:0.0008598611457273364 max memory_allocated 29270.06884765625 
[2025-03-22 17:31:12 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.37468016147613525 norm:0.0008479773532599211 max memory_allocated 29270.06884765625 
[2025-03-22 17:31:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 17:32:17 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.49876338243484497 norm:0.029276590794324875 max memory_allocated 29270.25634765625 
[2025-03-22 17:33:05 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.46864011883735657 norm:0.01251294743269682 max memory_allocated 29270.25634765625 
[2025-03-22 17:33:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.4314199984073639 norm:0.004345699213445187 max memory_allocated 29270.25634765625 
[2025-03-22 17:34:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.4148906171321869 norm:0.002005884191021323 max memory_allocated 29270.25634765625 
[2025-03-22 17:35:29 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.40903031826019287 norm:0.0015032276278361678 max memory_allocated 29270.25634765625 
[2025-03-22 17:36:17 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.4050912857055664 norm:0.0012698398204520345 max memory_allocated 29270.25634765625 
[2025-03-22 17:37:05 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.4026530086994171 norm:0.0011653945548459888 max memory_allocated 29270.25634765625 
[2025-03-22 17:37:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.40083590149879456 norm:0.001106429030187428 max memory_allocated 29270.25634765625 
[2025-03-22 17:38:42 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.39936280250549316 norm:0.0010422011837363243 max memory_allocated 29270.25634765625 
[2025-03-22 17:39:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.39807993173599243 norm:0.0010086275869980454 max memory_allocated 29270.25634765625 
[2025-03-22 17:40:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.397115558385849 norm:0.0009709270088933408 max memory_allocated 29270.25634765625 
[2025-03-22 17:41:06 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.3963751792907715 norm:0.000917820492759347 max memory_allocated 29270.25634765625 
[2025-03-22 17:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.39579448103904724 norm:0.0008675996796227992 max memory_allocated 29270.25634765625 
[2025-03-22 17:42:42 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3954125642776489 norm:0.0008418749202974141 max memory_allocated 29270.25634765625 
[2025-03-22 17:43:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.39534664154052734 norm:0.000853673554956913 max memory_allocated 29270.25634765625 
[2025-03-22 17:44:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.39515623450279236 norm:0.0008489516912959516 max memory_allocated 29270.25634765625 
[2025-03-22 17:45:06 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.39480018615722656 norm:0.0008495395304635167 max memory_allocated 29270.25634765625 
[2025-03-22 17:45:54 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3947644531726837 norm:0.0008408342255279422 max memory_allocated 29270.25634765625 
[2025-03-22 17:46:42 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.394363135099411 norm:0.0008170825894922018 max memory_allocated 29270.25634765625 
[2025-03-22 17:47:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.39421841502189636 norm:0.0008129770867526531 max memory_allocated 29270.25634765625 
[2025-03-22 17:47:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 17:48:35 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.5273435711860657 norm:0.02037891559302807 max memory_allocated 29270.44384765625 
[2025-03-22 17:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.49179011583328247 norm:0.008775106631219387 max memory_allocated 29270.44384765625 
[2025-03-22 17:50:10 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.4521586000919342 norm:0.0034973719157278538 max memory_allocated 29270.44384765625 
[2025-03-22 17:50:58 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.4360472559928894 norm:0.0017087323358282447 max memory_allocated 29270.44384765625 
[2025-03-22 17:51:46 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.4306400418281555 norm:0.001292926725000143 max memory_allocated 29270.44384765625 
[2025-03-22 17:52:34 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.42712464928627014 norm:0.001136028440669179 max memory_allocated 29270.44384765625 
[2025-03-22 17:53:22 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.42452916502952576 norm:0.001020706375129521 max memory_allocated 29270.44384765625 
[2025-03-22 17:54:10 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.4225649833679199 norm:0.000957426440436393 max memory_allocated 29270.44384765625 
[2025-03-22 17:54:58 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.4210592806339264 norm:0.0009134376305155456 max memory_allocated 29270.44384765625 
[2025-03-22 17:55:46 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.4197992980480194 norm:0.0008779425988905132 max memory_allocated 29270.44384765625 
[2025-03-22 17:56:35 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.41898661851882935 norm:0.0008616169798187912 max memory_allocated 29270.44384765625 
[2025-03-22 17:57:23 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.41837888956069946 norm:0.0008436061325483024 max memory_allocated 29270.44384765625 
[2025-03-22 17:58:11 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.41786623001098633 norm:0.0008348912233486772 max memory_allocated 29270.44384765625 
[2025-03-22 17:58:59 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.41742798686027527 norm:0.00083303009159863 max memory_allocated 29270.44384765625 
[2025-03-22 17:59:47 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.4170958399772644 norm:0.0008259295136667788 max memory_allocated 29270.44384765625 
[2025-03-22 18:00:35 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.41677388548851013 norm:0.000812191516160965 max memory_allocated 29270.44384765625 
[2025-03-22 18:01:23 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.41660401225090027 norm:0.0008049908792600036 max memory_allocated 29270.44384765625 
[2025-03-22 18:02:11 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.4165751039981842 norm:0.0008069289033301175 max memory_allocated 29270.44384765625 
[2025-03-22 18:02:59 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.41662460565567017 norm:0.0008206273196265101 max memory_allocated 29270.44384765625 
[2025-03-22 18:03:47 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.4165683388710022 norm:0.0008076339727267623 max memory_allocated 29270.44384765625 
[2025-03-22 18:04:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 18:04:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.5421290397644043 norm:0.02330496907234192 max memory_allocated 29270.63134765625 
[2025-03-22 18:05:39 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.5083910822868347 norm:0.008831936866044998 max memory_allocated 29270.63134765625 
[2025-03-22 18:06:27 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.47535085678100586 norm:0.003852378111332655 max memory_allocated 29270.63134765625 
[2025-03-22 18:07:15 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.4595843255519867 norm:0.0017915837233886123 max memory_allocated 29270.63134765625 
[2025-03-22 18:08:03 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.45358502864837646 norm:0.0013437381712719798 max memory_allocated 29270.63134765625 
[2025-03-22 18:08:51 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.44964128732681274 norm:0.0011552742216736078 max memory_allocated 29270.63134765625 
[2025-03-22 18:09:39 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.4468420147895813 norm:0.0010463998187333345 max memory_allocated 29270.63134765625 
[2025-03-22 18:10:27 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.4447830617427826 norm:0.0009867539629340172 max memory_allocated 29270.63134765625 
[2025-03-22 18:11:15 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.44318830966949463 norm:0.0009476576815359294 max memory_allocated 29270.63134765625 
[2025-03-22 18:12:03 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.4418985843658447 norm:0.0009315707720816135 max memory_allocated 29270.63134765625 
[2025-03-22 18:12:51 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.4408932030200958 norm:0.0009133787825703621 max memory_allocated 29270.63134765625 
[2025-03-22 18:13:39 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.4400135576725006 norm:0.0008957978570833802 max memory_allocated 29270.63134765625 
[2025-03-22 18:14:28 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.4392939805984497 norm:0.0008940966799855232 max memory_allocated 29270.63134765625 
[2025-03-22 18:15:16 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.4389191269874573 norm:0.0008949574548751116 max memory_allocated 29270.63134765625 
[2025-03-22 18:16:04 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.43850821256637573 norm:0.0008758128387853503 max memory_allocated 29270.63134765625 
[2025-03-22 18:16:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.4381198585033417 norm:0.0008830069564282894 max memory_allocated 29270.63134765625 
[2025-03-22 18:17:40 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.4378691017627716 norm:0.0008727621752768755 max memory_allocated 29270.63134765625 
[2025-03-22 18:18:28 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.43763622641563416 norm:0.0008625303744338453 max memory_allocated 29270.63134765625 
[2025-03-22 18:19:16 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.4374660551548004 norm:0.0008514286600984633 max memory_allocated 29270.63134765625 
[2025-03-22 18:20:03 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.43726807832717896 norm:0.0008525699377059937 max memory_allocated 29270.63134765625 
[2025-03-22 18:20:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 18:21:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.545714259147644 norm:0.00862789060920477 max memory_allocated 29270.81884765625 
[2025-03-22 18:21:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.5221652984619141 norm:0.005146520212292671 max memory_allocated 29270.81884765625 
[2025-03-22 18:22:44 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.4929828345775604 norm:0.002513961400836706 max memory_allocated 29270.81884765625 
[2025-03-22 18:23:32 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.48044353723526 norm:0.001534784329123795 max memory_allocated 29270.81884765625 
[2025-03-22 18:24:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.47485724091529846 norm:0.0012449967907741666 max memory_allocated 29270.81884765625 
[2025-03-22 18:25:07 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.4712490737438202 norm:0.001115019666031003 max memory_allocated 29270.81884765625 
[2025-03-22 18:25:55 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.46853554248809814 norm:0.0010599562665447593 max memory_allocated 29270.81884765625 
[2025-03-22 18:26:43 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.4664137065410614 norm:0.0010219758842140436 max memory_allocated 29270.81884765625 
[2025-03-22 18:27:31 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.46473899483680725 norm:0.0009986822260543704 max memory_allocated 29270.81884765625 
[2025-03-22 18:28:19 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.463489830493927 norm:0.00099174736533314 max memory_allocated 29270.81884765625 
[2025-03-22 18:29:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.4625004827976227 norm:0.0009692716994322836 max memory_allocated 29270.81884765625 
[2025-03-22 18:29:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.4618322551250458 norm:0.0009610576671548188 max memory_allocated 29270.81884765625 
[2025-03-22 18:30:44 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.46123889088630676 norm:0.0009509403025731444 max memory_allocated 29270.81884765625 
[2025-03-22 18:31:32 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.46080857515335083 norm:0.000948915199842304 max memory_allocated 29270.81884765625 
[2025-03-22 18:32:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.4603537917137146 norm:0.0009352726629003882 max memory_allocated 29270.81884765625 
[2025-03-22 18:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.4601031541824341 norm:0.0009426852338947356 max memory_allocated 29270.81884765625 
[2025-03-22 18:33:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.4598579406738281 norm:0.0009458930580876768 max memory_allocated 29270.81884765625 
[2025-03-22 18:34:44 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.4594581127166748 norm:0.0009413234074600041 max memory_allocated 29270.81884765625 
[2025-03-22 18:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.4592789113521576 norm:0.0009441265137866139 max memory_allocated 29270.81884765625 
[2025-03-22 18:36:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.45901039242744446 norm:0.0009398575639352202 max memory_allocated 29270.81884765625 
[2025-03-22 18:36:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 18:37:25 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.5988397002220154 norm:0.034001778811216354 max memory_allocated 29271.00634765625 
[2025-03-22 18:38:12 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.5642766356468201 norm:0.01668650284409523 max memory_allocated 29271.00634765625 
[2025-03-22 18:39:00 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.5282245874404907 norm:0.00838311854749918 max memory_allocated 29271.00634765625 
[2025-03-22 18:39:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.5109622478485107 norm:0.004897310398519039 max memory_allocated 29271.00634765625 
[2025-03-22 18:40:35 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.5021125674247742 norm:0.002759763039648533 max memory_allocated 29271.00634765625 
[2025-03-22 18:41:23 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.4970291554927826 norm:0.0018375281943008304 max memory_allocated 29271.00634765625 
[2025-03-22 18:42:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.49383601546287537 norm:0.001601376454345882 max memory_allocated 29271.00634765625 
[2025-03-22 18:42:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.4912849962711334 norm:0.00134646100923419 max memory_allocated 29271.00634765625 
[2025-03-22 18:43:47 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.4893479645252228 norm:0.001187037443742156 max memory_allocated 29271.00634765625 
[2025-03-22 18:44:35 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.48772621154785156 norm:0.0011198509018868208 max memory_allocated 29271.00634765625 
[2025-03-22 18:45:23 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.48649531602859497 norm:0.0010718934936448932 max memory_allocated 29271.00634765625 
[2025-03-22 18:46:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.4854806661605835 norm:0.0010484207887202501 max memory_allocated 29271.00634765625 
[2025-03-22 18:47:00 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.48459458351135254 norm:0.0010378345614299178 max memory_allocated 29271.00634765625 
[2025-03-22 18:47:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.4839508831501007 norm:0.001026608282700181 max memory_allocated 29271.00634765625 
[2025-03-22 18:48:36 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.4834069609642029 norm:0.0010069074342027307 max memory_allocated 29271.00634765625 
[2025-03-22 18:49:24 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.4830015003681183 norm:0.0010099136270582676 max memory_allocated 29271.00634765625 
[2025-03-22 18:50:12 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.48270297050476074 norm:0.0009917274583131075 max memory_allocated 29271.00634765625 
[2025-03-22 18:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.4824185371398926 norm:0.00098328641615808 max memory_allocated 29271.00634765625 
[2025-03-22 18:51:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.4821232557296753 norm:0.0009713546605780721 max memory_allocated 29271.00634765625 
[2025-03-22 18:52:36 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.482027143239975 norm:0.0009687424171715975 max memory_allocated 29271.00634765625 
[2025-03-22 18:52:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 18:53:42 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.6191871166229248 norm:0.06992393732070923 max memory_allocated 29271.19384765625 
[2025-03-22 18:54:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.6021866202354431 norm:0.04053780809044838 max memory_allocated 29271.19384765625 
[2025-03-22 18:55:18 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.5666936039924622 norm:0.015403530560433865 max memory_allocated 29271.19384765625 
[2025-03-22 18:56:06 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.5475497245788574 norm:0.006946738343685865 max memory_allocated 29271.19384765625 
[2025-03-22 18:56:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.5399613976478577 norm:0.004797618370503187 max memory_allocated 29271.19384765625 
[2025-03-22 18:57:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.5345531702041626 norm:0.0030326773412525654 max memory_allocated 29271.19384765625 
[2025-03-22 18:58:29 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.5312119126319885 norm:0.002696249634027481 max memory_allocated 29271.19384765625 
[2025-03-22 18:59:17 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.528944730758667 norm:0.002614517230540514 max memory_allocated 29271.19384765625 
[2025-03-22 19:00:04 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.5263878703117371 norm:0.0017823638627305627 max memory_allocated 29271.19384765625 
[2025-03-22 19:00:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.5245014429092407 norm:0.001516404328867793 max memory_allocated 29271.19384765625 
[2025-03-22 19:01:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.5231842994689941 norm:0.0014873931650072336 max memory_allocated 29271.19384765625 
[2025-03-22 19:02:28 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.5221675634384155 norm:0.001438234350644052 max memory_allocated 29271.19384765625 
[2025-03-22 19:03:16 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.5213192105293274 norm:0.0014014480402693152 max memory_allocated 29271.19384765625 
[2025-03-22 19:04:04 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.5205233693122864 norm:0.0013682625722140074 max memory_allocated 29271.19384765625 
[2025-03-22 19:04:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.5198894739151001 norm:0.001332662533968687 max memory_allocated 29271.19384765625 
[2025-03-22 19:05:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.5194215178489685 norm:0.0013185678981244564 max memory_allocated 29271.19384765625 
[2025-03-22 19:06:28 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.5190261006355286 norm:0.001279938849620521 max memory_allocated 29271.19384765625 
[2025-03-22 19:07:16 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.5184621810913086 norm:0.0012304037809371948 max memory_allocated 29271.19384765625 
[2025-03-22 19:08:04 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.518141508102417 norm:0.001222531427629292 max memory_allocated 29271.19384765625 
[2025-03-22 19:08:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.5179573893547058 norm:0.0012200503842905164 max memory_allocated 29271.19384765625 
[2025-03-22 19:09:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 19:09:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.6307915449142456 norm:0.017073960974812508 max memory_allocated 29271.38134765625 
[2025-03-22 19:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.6053087115287781 norm:0.008716801181435585 max memory_allocated 29271.38134765625 
[2025-03-22 19:11:34 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.5807162523269653 norm:0.004372667986899614 max memory_allocated 29271.38134765625 
[2025-03-22 19:12:22 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.5691123008728027 norm:0.002398064825683832 max memory_allocated 29271.38134765625 
[2025-03-22 19:13:10 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.5643068552017212 norm:0.0017031891038641334 max memory_allocated 29271.38134765625 
[2025-03-22 19:13:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.5612998604774475 norm:0.001448643277399242 max memory_allocated 29271.38134765625 
[2025-03-22 19:14:46 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.5588781237602234 norm:0.001333927852101624 max memory_allocated 29271.38134765625 
[2025-03-22 19:15:34 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.5570046901702881 norm:0.0012623460497707129 max memory_allocated 29271.38134765625 
[2025-03-22 19:16:22 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.5555298328399658 norm:0.0011974978260695934 max memory_allocated 29271.38134765625 
[2025-03-22 19:17:09 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.554294228553772 norm:0.0011627556523308158 max memory_allocated 29271.38134765625 
[2025-03-22 19:17:57 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.553227424621582 norm:0.0011114472290500998 max memory_allocated 29271.38134765625 
[2025-03-22 19:18:45 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.5524306297302246 norm:0.0010587883880361915 max memory_allocated 29271.38134765625 
[2025-03-22 19:19:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.5518282055854797 norm:0.001019071671180427 max memory_allocated 29271.38134765625 
[2025-03-22 19:20:20 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.5511745810508728 norm:0.0009835928212851286 max memory_allocated 29271.38134765625 
[2025-03-22 19:21:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.550646185874939 norm:0.0009674521279521286 max memory_allocated 29271.38134765625 
[2025-03-22 19:21:56 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.5502110719680786 norm:0.0009548994712531567 max memory_allocated 29271.38134765625 
[2025-03-22 19:22:44 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.5498353838920593 norm:0.0009475165861658752 max memory_allocated 29271.38134765625 
[2025-03-22 19:23:32 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.5496339201927185 norm:0.0009504132904112339 max memory_allocated 29271.38134765625 
[2025-03-22 19:24:20 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.5494959950447083 norm:0.0009443360613659024 max memory_allocated 29271.38134765625 
[2025-03-22 19:25:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.5493401885032654 norm:0.0009473091340623796 max memory_allocated 29271.38134765625 
[2025-03-22 19:25:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 19:26:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.6741179823875427 norm:0.013730711303651333 max memory_allocated 29271.56884765625 
[2025-03-22 19:27:02 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.652942419052124 norm:0.007940307259559631 max memory_allocated 29271.56884765625 
[2025-03-22 19:27:50 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.6246979236602783 norm:0.004088848363608122 max memory_allocated 29271.56884765625 
[2025-03-22 19:28:38 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.6123924255371094 norm:0.002424822421744466 max memory_allocated 29271.56884765625 
[2025-03-22 19:29:26 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.6073817610740662 norm:0.0018775698263198137 max memory_allocated 29271.56884765625 
[2025-03-22 19:30:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.6040955185890198 norm:0.001578844734467566 max memory_allocated 29271.56884765625 
[2025-03-22 19:31:02 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.6015169024467468 norm:0.0013733220985159278 max memory_allocated 29271.56884765625 
[2025-03-22 19:31:51 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.5994436740875244 norm:0.0012472247472032905 max memory_allocated 29271.56884765625 
[2025-03-22 19:32:39 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.5979446172714233 norm:0.001206010114401579 max memory_allocated 29271.56884765625 
[2025-03-22 19:33:27 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.5967293381690979 norm:0.0011695502325892448 max memory_allocated 29271.56884765625 
[2025-03-22 19:34:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.5956315994262695 norm:0.0011414651526138186 max memory_allocated 29271.56884765625 
[2025-03-22 19:35:02 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.5947370529174805 norm:0.0011049037566408515 max memory_allocated 29271.56884765625 
[2025-03-22 19:35:50 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.5940853357315063 norm:0.0010938923805952072 max memory_allocated 29271.56884765625 
[2025-03-22 19:36:38 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.593444287776947 norm:0.0010548561112955213 max memory_allocated 29271.56884765625 
[2025-03-22 19:37:26 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.5928004384040833 norm:0.0010219067335128784 max memory_allocated 29271.56884765625 
[2025-03-22 19:38:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.5924012660980225 norm:0.0010050470009446144 max memory_allocated 29271.56884765625 
[2025-03-22 19:39:01 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.592107892036438 norm:0.000996725051663816 max memory_allocated 29271.56884765625 
[2025-03-22 19:39:49 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.5917081236839294 norm:0.0009877110132947564 max memory_allocated 29271.56884765625 
[2025-03-22 19:40:37 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.5914306640625 norm:0.0009806688176468015 max memory_allocated 29271.56884765625 
[2025-03-22 19:41:25 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.5912229418754578 norm:0.0009728633449412882 max memory_allocated 29271.56884765625 
[2025-03-22 19:41:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 19:42:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.7603908181190491 norm:0.057976819574832916 max memory_allocated 29271.75634765625 
[2025-03-22 19:43:19 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.7291411757469177 norm:0.030103471130132675 max memory_allocated 29271.75634765625 
[2025-03-22 19:44:07 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.6984907388687134 norm:0.015341188758611679 max memory_allocated 29271.75634765625 
[2025-03-22 19:44:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.6815984845161438 norm:0.008273404091596603 max memory_allocated 29271.75634765625 
[2025-03-22 19:45:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.6744160056114197 norm:0.005583515390753746 max memory_allocated 29271.75634765625 
[2025-03-22 19:46:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.670379638671875 norm:0.004471988417208195 max memory_allocated 29271.75634765625 
[2025-03-22 19:47:19 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.6669055819511414 norm:0.003523319959640503 max memory_allocated 29271.75634765625 
[2025-03-22 19:48:07 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.6642494797706604 norm:0.003069819649681449 max memory_allocated 29271.75634765625 
[2025-03-22 19:48:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.6617271900177002 norm:0.0023975649382919073 max memory_allocated 29271.75634765625 
[2025-03-22 19:49:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.6602452397346497 norm:0.002379420679062605 max memory_allocated 29271.75634765625 
[2025-03-22 19:50:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.6590696573257446 norm:0.0022551098372787237 max memory_allocated 29271.75634765625 
[2025-03-22 19:51:19 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.6580507159233093 norm:0.002159943338483572 max memory_allocated 29271.75634765625 
[2025-03-22 19:52:07 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.6571998000144958 norm:0.002048714552074671 max memory_allocated 29271.75634765625 
[2025-03-22 19:52:54 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.6565237641334534 norm:0.00197293097153306 max memory_allocated 29271.75634765625 
[2025-03-22 19:53:42 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.6559377312660217 norm:0.001874034060165286 max memory_allocated 29271.75634765625 
[2025-03-22 19:54:30 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.6553335189819336 norm:0.0017894773045554757 max memory_allocated 29271.75634765625 
[2025-03-22 19:55:18 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.6548378467559814 norm:0.0017301100306212902 max memory_allocated 29271.75634765625 
[2025-03-22 19:56:06 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.653922975063324 norm:0.0014372653095051646 max memory_allocated 29271.75634765625 
[2025-03-22 19:56:53 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.6532120704650879 norm:0.0011768367839977145 max memory_allocated 29271.75634765625 
[2025-03-22 19:57:41 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.652844250202179 norm:0.0011610473738983274 max memory_allocated 29271.75634765625 
[2025-03-22 19:57:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 19:58:47 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.79862380027771 norm:0.01774318888783455 max memory_allocated 29271.94384765625 
[2025-03-22 19:59:35 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.7793011665344238 norm:0.009819084778428078 max memory_allocated 29271.94384765625 
[2025-03-22 20:00:23 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.7566837668418884 norm:0.005204855464398861 max memory_allocated 29271.94384765625 
[2025-03-22 20:01:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.745271623134613 norm:0.0030746529810130596 max memory_allocated 29271.94384765625 
[2025-03-22 20:01:59 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.7403383255004883 norm:0.002542068250477314 max memory_allocated 29271.94384765625 
[2025-03-22 20:02:47 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.7371470928192139 norm:0.0021817609667778015 max memory_allocated 29271.94384765625 
[2025-03-22 20:03:35 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.7343968749046326 norm:0.0014221988385543227 max memory_allocated 29271.94384765625 
[2025-03-22 20:04:23 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.732287585735321 norm:0.001362071605399251 max memory_allocated 29271.94384765625 
[2025-03-22 20:05:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.7306490540504456 norm:0.0013315932592377067 max memory_allocated 29271.94384765625 
[2025-03-22 20:05:59 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.7294896245002747 norm:0.0012876145774498582 max memory_allocated 29271.94384765625 
[2025-03-22 20:06:47 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.7286792397499084 norm:0.0012531448155641556 max memory_allocated 29271.94384765625 
[2025-03-22 20:07:35 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.7278184294700623 norm:0.0012316203210502863 max memory_allocated 29271.94384765625 
[2025-03-22 20:08:23 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.7272882461547852 norm:0.0012223469093441963 max memory_allocated 29271.94384765625 
[2025-03-22 20:09:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.7267393469810486 norm:0.0012021597940474749 max memory_allocated 29271.94384765625 
[2025-03-22 20:09:59 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.7263506054878235 norm:0.0012006268370896578 max memory_allocated 29271.94384765625 
[2025-03-22 20:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.7258181571960449 norm:0.0011818140046671033 max memory_allocated 29271.94384765625 
[2025-03-22 20:11:34 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.7253453135490417 norm:0.0011710899416357279 max memory_allocated 29271.94384765625 
[2025-03-22 20:12:22 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.7251235842704773 norm:0.0011669470695778728 max memory_allocated 29271.94384765625 
[2025-03-22 20:13:10 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.7247554063796997 norm:0.0011518775718286633 max memory_allocated 29271.94384765625 
[2025-03-22 20:13:58 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.7245500087738037 norm:0.0011539080878719687 max memory_allocated 29271.94384765625 
[2025-03-22 20:14:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 20:15:03 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.88658207654953 norm:0.030894117429852486 max memory_allocated 29272.13134765625 
[2025-03-22 20:15:51 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.864151120185852 norm:0.01835699751973152 max memory_allocated 29272.13134765625 
[2025-03-22 20:16:39 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.8374125361442566 norm:0.010377851314842701 max memory_allocated 29272.13134765625 
[2025-03-22 20:17:27 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.8233228325843811 norm:0.005552547983825207 max memory_allocated 29272.13134765625 
[2025-03-22 20:18:16 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.8179405331611633 norm:0.0044739097356796265 max memory_allocated 29272.13134765625 
[2025-03-22 20:19:04 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.8142846822738647 norm:0.0035234452225267887 max memory_allocated 29272.13134765625 
[2025-03-22 20:19:52 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.8117436170578003 norm:0.0031773115042597055 max memory_allocated 29272.13134765625 
[2025-03-22 20:20:40 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.8095012903213501 norm:0.00278013595379889 max memory_allocated 29272.13134765625 
[2025-03-22 20:21:28 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.807729959487915 norm:0.0024778968654572964 max memory_allocated 29272.13134765625 
[2025-03-22 20:22:16 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.8063642382621765 norm:0.002278352389112115 max memory_allocated 29272.13134765625 
[2025-03-22 20:23:04 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.8051592111587524 norm:0.0020836153998970985 max memory_allocated 29272.13134765625 
[2025-03-22 20:23:52 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.8041501641273499 norm:0.0019484648946672678 max memory_allocated 29272.13134765625 
[2025-03-22 20:24:39 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.802707850933075 norm:0.0012647106777876616 max memory_allocated 29272.13134765625 
[2025-03-22 20:25:27 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.8020550012588501 norm:0.0012183041544631124 max memory_allocated 29272.13134765625 
[2025-03-22 20:26:15 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.8014469742774963 norm:0.0012090068776160479 max memory_allocated 29272.13134765625 
[2025-03-22 20:27:03 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.8009217381477356 norm:0.0012006277684122324 max memory_allocated 29272.13134765625 
[2025-03-22 20:27:50 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.8005419373512268 norm:0.0011975698871538043 max memory_allocated 29272.13134765625 
[2025-03-22 20:28:38 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.8002766966819763 norm:0.0011864567641168833 max memory_allocated 29272.13134765625 
[2025-03-22 20:29:26 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.7998642325401306 norm:0.0011702838819473982 max memory_allocated 29272.13134765625 
[2025-03-22 20:30:14 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.799536943435669 norm:0.0011617199052125216 max memory_allocated 29272.13134765625 
[2025-03-22 20:30:28 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 20:31:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.9748009443283081 norm:0.010588658042252064 max memory_allocated 29272.31884765625 
[2025-03-22 20:32:08 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.9549810886383057 norm:0.006484213750809431 max memory_allocated 29272.31884765625 
[2025-03-22 20:32:56 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.9299516677856445 norm:0.003208774607628584 max memory_allocated 29272.31884765625 
[2025-03-22 20:33:44 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.9203873872756958 norm:0.0019241204718127847 max memory_allocated 29272.31884765625 
[2025-03-22 20:34:32 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.9161713123321533 norm:0.0016311699291691184 max memory_allocated 29272.31884765625 
[2025-03-22 20:35:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.9133243560791016 norm:0.001543541089631617 max memory_allocated 29272.31884765625 
[2025-03-22 20:36:08 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.9110161662101746 norm:0.001502722967416048 max memory_allocated 29272.31884765625 
[2025-03-22 20:36:56 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.9091179966926575 norm:0.0014637773856520653 max memory_allocated 29272.31884765625 
[2025-03-22 20:37:44 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.9077088236808777 norm:0.0014716596342623234 max memory_allocated 29272.31884765625 
[2025-03-22 20:38:32 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.9063910245895386 norm:0.001417023129761219 max memory_allocated 29272.31884765625 
[2025-03-22 20:39:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.9055122137069702 norm:0.0013985971454530954 max memory_allocated 29272.31884765625 
[2025-03-22 20:40:08 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.9046792387962341 norm:0.0013391179963946342 max memory_allocated 29272.31884765625 
[2025-03-22 20:40:56 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.903857409954071 norm:0.0013014869764447212 max memory_allocated 29272.31884765625 
[2025-03-22 20:41:44 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.903329610824585 norm:0.0012991356197744608 max memory_allocated 29272.31884765625 
[2025-03-22 20:42:31 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.9027745723724365 norm:0.001267456216737628 max memory_allocated 29272.31884765625 
[2025-03-22 20:43:19 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.9023141264915466 norm:0.0012414217926561832 max memory_allocated 29272.31884765625 
[2025-03-22 20:44:07 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.9019656777381897 norm:0.0012390384217724204 max memory_allocated 29272.31884765625 
[2025-03-22 20:44:55 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.9017271399497986 norm:0.0012245732359588146 max memory_allocated 29272.31884765625 
[2025-03-22 20:45:43 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.9015830755233765 norm:0.001221666345372796 max memory_allocated 29272.31884765625 
[2025-03-22 20:46:31 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.9013038277626038 norm:0.0012175638694316149 max memory_allocated 29272.31884765625 
[2025-03-22 20:46:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 20:47:36 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:1.0998618602752686 norm:0.01880372129380703 max memory_allocated 29272.50634765625 
[2025-03-22 20:48:24 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:1.0774509906768799 norm:0.009432196617126465 max memory_allocated 29272.50634765625 
[2025-03-22 20:49:12 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:1.0533397197723389 norm:0.005057173781096935 max memory_allocated 29272.50634765625 
[2025-03-22 20:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:1.0426617860794067 norm:0.0026401265058666468 max memory_allocated 29272.50634765625 
[2025-03-22 20:50:49 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:1.0377873182296753 norm:0.0018258759519085288 max memory_allocated 29272.50634765625 
[2025-03-22 20:51:37 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:1.034519076347351 norm:0.0017003024695441127 max memory_allocated 29272.50634765625 
[2025-03-22 20:52:25 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:1.0321089029312134 norm:0.001614519627764821 max memory_allocated 29272.50634765625 
[2025-03-22 20:53:13 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:1.0303155183792114 norm:0.0015537743456661701 max memory_allocated 29272.50634765625 
[2025-03-22 20:54:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:1.0286707878112793 norm:0.0015030215727165341 max memory_allocated 29272.50634765625 
[2025-03-22 20:54:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:1.027414083480835 norm:0.0014609367353841662 max memory_allocated 29272.50634765625 
[2025-03-22 20:55:38 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:1.0265167951583862 norm:0.0014346122043207288 max memory_allocated 29272.50634765625 
[2025-03-22 20:56:26 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:1.0257833003997803 norm:0.0014286619843915105 max memory_allocated 29272.50634765625 
[2025-03-22 20:57:14 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:1.0251983404159546 norm:0.0014230512315407395 max memory_allocated 29272.50634765625 
[2025-03-22 20:58:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:1.0248795747756958 norm:0.0014177401317283511 max memory_allocated 29272.50634765625 
[2025-03-22 20:58:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:1.0243325233459473 norm:0.0014000308001413941 max memory_allocated 29272.50634765625 
[2025-03-22 20:59:38 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:1.023912787437439 norm:0.0013936614850535989 max memory_allocated 29272.50634765625 
[2025-03-22 21:00:26 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:1.0235669612884521 norm:0.0013791455421596766 max memory_allocated 29272.50634765625 
[2025-03-22 21:01:14 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:1.0232958793640137 norm:0.0013753824168816209 max memory_allocated 29272.50634765625 
[2025-03-22 21:02:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:1.0229527950286865 norm:0.0013699354603886604 max memory_allocated 29272.50634765625 
[2025-03-22 21:02:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:1.0225675106048584 norm:0.0013543623499572277 max memory_allocated 29272.50634765625 
[2025-03-22 21:03:03 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 21:03:56 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:1.240688443183899 norm:0.019654463976621628 max memory_allocated 29272.69384765625 
[2025-03-22 21:04:44 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:1.2220549583435059 norm:0.011642982251942158 max memory_allocated 29272.69384765625 
[2025-03-22 21:05:32 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:1.1999657154083252 norm:0.006217928137630224 max memory_allocated 29272.69384765625 
[2025-03-22 21:06:20 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:1.1902521848678589 norm:0.002447697566822171 max memory_allocated 29272.69384765625 
[2025-03-22 21:07:08 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:1.1860711574554443 norm:0.002043692860752344 max memory_allocated 29272.69384765625 
[2025-03-22 21:07:56 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:1.183065414428711 norm:0.0019283615984022617 max memory_allocated 29272.69384765625 
[2025-03-22 21:08:45 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:1.1806186437606812 norm:0.001856998074799776 max memory_allocated 29272.69384765625 
[2025-03-22 21:09:33 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:1.1784785985946655 norm:0.001791506540030241 max memory_allocated 29272.69384765625 
[2025-03-22 21:10:21 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:1.1769057512283325 norm:0.0017645179759711027 max memory_allocated 29272.69384765625 
[2025-03-22 21:11:09 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:1.175816297531128 norm:0.0017492047045379877 max memory_allocated 29272.69384765625 
[2025-03-22 21:11:57 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:1.1747742891311646 norm:0.0017226128838956356 max memory_allocated 29272.69384765625 
[2025-03-22 21:12:45 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:1.1740248203277588 norm:0.0016999002546072006 max memory_allocated 29272.69384765625 
[2025-03-22 21:13:34 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:1.1733875274658203 norm:0.001675151288509369 max memory_allocated 29272.69384765625 
[2025-03-22 21:14:22 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:1.1728076934814453 norm:0.0016620346577838063 max memory_allocated 29272.69384765625 
[2025-03-22 21:15:10 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:1.1723275184631348 norm:0.0016639633104205132 max memory_allocated 29272.69384765625 
[2025-03-22 21:15:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:1.1717511415481567 norm:0.001645889482460916 max memory_allocated 29272.69384765625 
[2025-03-22 21:16:46 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:1.1712076663970947 norm:0.0016323914751410484 max memory_allocated 29272.69384765625 
[2025-03-22 21:17:35 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:1.1706842184066772 norm:0.0016227156156674027 max memory_allocated 29272.69384765625 
[2025-03-22 21:18:23 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:1.1701511144638062 norm:0.001599776209332049 max memory_allocated 29272.69384765625 
[2025-03-22 21:19:11 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:1.1698553562164307 norm:0.0015936086419969797 max memory_allocated 29272.69384765625 
[2025-03-22 21:19:25 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 21:20:17 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:1.3795690536499023 norm:0.01276165060698986 max memory_allocated 29272.88134765625 
[2025-03-22 21:21:05 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:1.361504077911377 norm:0.007305539213120937 max memory_allocated 29272.88134765625 
[2025-03-22 21:21:53 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:1.342836618423462 norm:0.004872310906648636 max memory_allocated 29272.88134765625 
[2025-03-22 21:22:41 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:1.335774302482605 norm:0.003586411476135254 max memory_allocated 29272.88134765625 
[2025-03-22 21:23:29 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:1.3317744731903076 norm:0.003024578094482422 max memory_allocated 29272.88134765625 
[2025-03-22 21:24:18 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:1.3285260200500488 norm:0.0026928901206701994 max memory_allocated 29272.88134765625 
[2025-03-22 21:25:06 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:1.3258646726608276 norm:0.0023772690910845995 max memory_allocated 29272.88134765625 
[2025-03-22 21:25:54 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:1.3235567808151245 norm:0.0021312604658305645 max memory_allocated 29272.88134765625 
[2025-03-22 21:26:42 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:1.3219486474990845 norm:0.001988289412111044 max memory_allocated 29272.88134765625 
[2025-03-22 21:27:30 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:1.3207277059555054 norm:0.0018681561341509223 max memory_allocated 29272.88134765625 
[2025-03-22 21:28:19 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:1.3196996450424194 norm:0.001783698913641274 max memory_allocated 29272.88134765625 
[2025-03-22 21:29:07 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:1.3189127445220947 norm:0.001703617163002491 max memory_allocated 29272.88134765625 
[2025-03-22 21:29:55 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:1.3181664943695068 norm:0.001652187667787075 max memory_allocated 29272.88134765625 
[2025-03-22 21:30:43 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:1.3175857067108154 norm:0.001608086284250021 max memory_allocated 29272.88134765625 
[2025-03-22 21:31:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:1.3167855739593506 norm:0.0015652617439627647 max memory_allocated 29272.88134765625 
[2025-03-22 21:32:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:1.3161680698394775 norm:0.0015328754670917988 max memory_allocated 29272.88134765625 
[2025-03-22 21:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:1.3156828880310059 norm:0.0015185686061158776 max memory_allocated 29272.88134765625 
[2025-03-22 21:33:56 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:1.3153740167617798 norm:0.001510509755462408 max memory_allocated 29272.88134765625 
[2025-03-22 21:34:44 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:1.3150620460510254 norm:0.0014974039513617754 max memory_allocated 29272.88134765625 
[2025-03-22 21:35:33 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:1.3147971630096436 norm:0.0015030490467324853 max memory_allocated 29272.88134765625 
[2025-03-22 21:35:46 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 21:36:38 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:1.549218773841858 norm:0.019144093617796898 max memory_allocated 29273.06884765625 
[2025-03-22 21:37:26 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:1.52793550491333 norm:0.011030330322682858 max memory_allocated 29273.06884765625 
[2025-03-22 21:38:14 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:1.5068498849868774 norm:0.0064722588285803795 max memory_allocated 29273.06884765625 
[2025-03-22 21:39:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:1.4972097873687744 norm:0.0037657248321920633 max memory_allocated 29273.06884765625 
[2025-03-22 21:39:49 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:1.4920413494110107 norm:0.0018187862588092685 max memory_allocated 29273.06884765625 
[2025-03-22 21:40:37 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:1.4887816905975342 norm:0.00169550278224051 max memory_allocated 29273.06884765625 
[2025-03-22 21:41:25 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:1.4860159158706665 norm:0.0015944447368383408 max memory_allocated 29273.06884765625 
[2025-03-22 21:42:13 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:1.4840564727783203 norm:0.001578222494572401 max memory_allocated 29273.06884765625 
[2025-03-22 21:43:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:1.4825233221054077 norm:0.0015582109335809946 max memory_allocated 29273.06884765625 
[2025-03-22 21:43:50 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:1.4812495708465576 norm:0.0015331755857914686 max memory_allocated 29273.06884765625 
[2025-03-22 21:44:38 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:1.4802093505859375 norm:0.0015201352071017027 max memory_allocated 29273.06884765625 
[2025-03-22 21:45:26 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:1.4792637825012207 norm:0.0014995995443314314 max memory_allocated 29273.06884765625 
[2025-03-22 21:46:14 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:1.4783613681793213 norm:0.0014709621900692582 max memory_allocated 29273.06884765625 
[2025-03-22 21:47:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:1.477858543395996 norm:0.0014691958203911781 max memory_allocated 29273.06884765625 
[2025-03-22 21:47:51 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:1.4775590896606445 norm:0.001466943882405758 max memory_allocated 29273.06884765625 
[2025-03-22 21:48:39 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:1.477066159248352 norm:0.0014646524796262383 max memory_allocated 29273.06884765625 
[2025-03-22 21:49:27 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:1.4766461849212646 norm:0.0014609768986701965 max memory_allocated 29273.06884765625 
[2025-03-22 21:50:15 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:1.4762027263641357 norm:0.0014498635428026319 max memory_allocated 29273.06884765625 
[2025-03-22 21:51:03 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:1.4759913682937622 norm:0.0014480988029390574 max memory_allocated 29273.06884765625 
[2025-03-22 21:51:52 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:1.4757342338562012 norm:0.0014559789560735226 max memory_allocated 29273.06884765625 
[2025-03-22 21:52:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 21:52:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:1.7076447010040283 norm:0.014632998034358025 max memory_allocated 29273.25634765625 
[2025-03-22 21:53:45 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:1.6880525350570679 norm:0.007946212776005268 max memory_allocated 29273.25634765625 
[2025-03-22 21:54:33 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:1.6671817302703857 norm:0.004893803969025612 max memory_allocated 29273.25634765625 
[2025-03-22 21:55:21 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:1.658280372619629 norm:0.0024010685738176107 max memory_allocated 29273.25634765625 
[2025-03-22 21:56:09 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:1.6537498235702515 norm:0.0017918359953910112 max memory_allocated 29273.25634765625 
[2025-03-22 21:56:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:1.6501290798187256 norm:0.0015030010836198926 max memory_allocated 29273.25634765625 
[2025-03-22 21:57:45 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:1.6474298238754272 norm:0.0013996654888615012 max memory_allocated 29273.25634765625 
[2025-03-22 21:58:33 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:1.6454458236694336 norm:0.001389215001836419 max memory_allocated 29273.25634765625 
[2025-03-22 21:59:21 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:1.643967628479004 norm:0.0013782245805487037 max memory_allocated 29273.25634765625 
[2025-03-22 22:00:09 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:1.6427980661392212 norm:0.0013844252098351717 max memory_allocated 29273.25634765625 
[2025-03-22 22:00:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:1.6418962478637695 norm:0.0013761648442596197 max memory_allocated 29273.25634765625 
[2025-03-22 22:01:45 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:1.6412138938903809 norm:0.0013576997444033623 max memory_allocated 29273.25634765625 
[2025-03-22 22:02:33 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:1.640543818473816 norm:0.001349121332168579 max memory_allocated 29273.25634765625 
[2025-03-22 22:03:21 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:1.6399952173233032 norm:0.0013457119930535555 max memory_allocated 29273.25634765625 
[2025-03-22 22:04:09 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:1.6394498348236084 norm:0.0013308689231052995 max memory_allocated 29273.25634765625 
[2025-03-22 22:04:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:1.6390771865844727 norm:0.001332993502728641 max memory_allocated 29273.25634765625 
[2025-03-22 22:05:45 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:1.6388533115386963 norm:0.0013397642178460956 max memory_allocated 29273.25634765625 
[2025-03-22 22:06:33 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:1.6385629177093506 norm:0.0013416269794106483 max memory_allocated 29273.25634765625 
[2025-03-22 22:07:22 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:1.6383129358291626 norm:0.001343368086963892 max memory_allocated 29273.25634765625 
[2025-03-22 22:08:10 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:1.6380152702331543 norm:0.0013396054273471236 max memory_allocated 29273.25634765625 
[2025-03-22 22:08:24 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 22:09:16 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:1.8961786031723022 norm:0.006061401218175888 max memory_allocated 29273.44384765625 
[2025-03-22 22:10:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:1.8778598308563232 norm:0.0036629170645028353 max memory_allocated 29273.44384765625 
[2025-03-22 22:10:52 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:1.8563748598098755 norm:0.0025981671642512083 max memory_allocated 29273.44384765625 
[2025-03-22 22:11:40 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:1.84792160987854 norm:0.00168343645054847 max memory_allocated 29273.44384765625 
[2025-03-22 22:12:28 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:1.843395709991455 norm:0.0015027488116174936 max memory_allocated 29273.44384765625 
[2025-03-22 22:13:16 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:1.8399710655212402 norm:0.001451446209102869 max memory_allocated 29273.44384765625 
[2025-03-22 22:14:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:1.8368724584579468 norm:0.0014314594445750117 max memory_allocated 29273.44384765625 
[2025-03-22 22:14:52 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:1.8346115350723267 norm:0.0014198377029970288 max memory_allocated 29273.44384765625 
[2025-03-22 22:15:40 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:1.832836627960205 norm:0.0013998032081872225 max memory_allocated 29273.44384765625 
[2025-03-22 22:16:28 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:1.831568717956543 norm:0.0013907846296206117 max memory_allocated 29273.44384765625 
[2025-03-22 22:17:16 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:1.8305315971374512 norm:0.0013939929194748402 max memory_allocated 29273.44384765625 
[2025-03-22 22:18:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:1.829776406288147 norm:0.0013943198136985302 max memory_allocated 29273.44384765625 
[2025-03-22 22:18:52 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:1.8289178609848022 norm:0.0014064591377973557 max memory_allocated 29273.44384765625 
[2025-03-22 22:19:40 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:1.8283882141113281 norm:0.0014021418755874038 max memory_allocated 29273.44384765625 
[2025-03-22 22:20:28 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:1.8279359340667725 norm:0.0014110066695138812 max memory_allocated 29273.44384765625 
[2025-03-22 22:21:16 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:1.8275071382522583 norm:0.001401120680384338 max memory_allocated 29273.44384765625 
[2025-03-22 22:22:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:1.8271102905273438 norm:0.001402188092470169 max memory_allocated 29273.44384765625 
[2025-03-22 22:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:1.8267147541046143 norm:0.001403162139467895 max memory_allocated 29273.44384765625 
[2025-03-22 22:23:41 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:1.8264498710632324 norm:0.0014122589491307735 max memory_allocated 29273.44384765625 
[2025-03-22 22:24:29 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:1.8261570930480957 norm:0.0014077817322686315 max memory_allocated 29273.44384765625 
[2025-03-22 22:24:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 22:25:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:2.0910186767578125 norm:0.006395469419658184 max memory_allocated 29273.63134765625 
[2025-03-22 22:26:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:2.0706377029418945 norm:0.003612541826441884 max memory_allocated 29273.63134765625 
[2025-03-22 22:27:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:2.049229621887207 norm:0.002730362117290497 max memory_allocated 29273.63134765625 
[2025-03-22 22:27:59 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:2.0415828227996826 norm:0.002238339511677623 max memory_allocated 29273.63134765625 
[2025-03-22 22:28:47 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:2.0373470783233643 norm:0.0020413559395819902 max memory_allocated 29273.63134765625 
[2025-03-22 22:29:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:2.0339529514312744 norm:0.001930898055434227 max memory_allocated 29273.63134765625 
[2025-03-22 22:30:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:2.03134822845459 norm:0.00182069290895015 max memory_allocated 29273.63134765625 
[2025-03-22 22:31:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:2.028972625732422 norm:0.001803961000405252 max memory_allocated 29273.63134765625 
[2025-03-22 22:31:59 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:2.0271666049957275 norm:0.0017945021390914917 max memory_allocated 29273.63134765625 
[2025-03-22 22:32:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:2.025670051574707 norm:0.001756727579049766 max memory_allocated 29273.63134765625 
[2025-03-22 22:33:34 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:2.0246546268463135 norm:0.001756842713803053 max memory_allocated 29273.63134765625 
[2025-03-22 22:34:22 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:2.023606777191162 norm:0.0016741780564188957 max memory_allocated 29273.63134765625 
[2025-03-22 22:35:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:2.0230002403259277 norm:0.0016612556064501405 max memory_allocated 29273.63134765625 
[2025-03-22 22:35:58 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:2.0222973823547363 norm:0.0016562269302085042 max memory_allocated 29273.63134765625 
[2025-03-22 22:36:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:2.021577835083008 norm:0.0016757444245740771 max memory_allocated 29273.63134765625 
[2025-03-22 22:37:34 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:2.0208137035369873 norm:0.0016592860920354724 max memory_allocated 29273.63134765625 
[2025-03-22 22:38:22 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:2.0202291011810303 norm:0.0017645828193053603 max memory_allocated 29273.63134765625 
[2025-03-22 22:39:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:2.0199341773986816 norm:0.0016442956402897835 max memory_allocated 29273.63134765625 
[2025-03-22 22:39:58 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:2.0193705558776855 norm:0.0016234931536018848 max memory_allocated 29273.63134765625 
[2025-03-22 22:40:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:2.0191469192504883 norm:0.0016594028566032648 max memory_allocated 29273.63134765625 
[2025-03-22 22:41:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 22:41:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:2.27331280708313 norm:0.01140660047531128 max memory_allocated 29273.81884765625 
[2025-03-22 22:42:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:2.2536120414733887 norm:0.00621107779443264 max memory_allocated 29273.81884765625 
[2025-03-22 22:43:28 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:2.230839490890503 norm:0.003699289634823799 max memory_allocated 29273.81884765625 
[2025-03-22 22:44:16 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:2.2224984169006348 norm:0.002594975521788001 max memory_allocated 29273.81884765625 
[2025-03-22 22:45:04 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:2.2174456119537354 norm:0.0016860514879226685 max memory_allocated 29273.81884765625 
[2025-03-22 22:45:52 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:2.2135539054870605 norm:0.001505222637206316 max memory_allocated 29273.81884765625 
[2025-03-22 22:46:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:2.2105751037597656 norm:0.001442649750970304 max memory_allocated 29273.81884765625 
[2025-03-22 22:47:28 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:2.2082157135009766 norm:0.001400034292601049 max memory_allocated 29273.81884765625 
[2025-03-22 22:48:15 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:2.206433057785034 norm:0.0013794050319120288 max memory_allocated 29273.81884765625 
[2025-03-22 22:49:03 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:2.2050909996032715 norm:0.0013418925227597356 max memory_allocated 29273.81884765625 
[2025-03-22 22:49:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:2.204064130783081 norm:0.00132694689091295 max memory_allocated 29273.81884765625 
[2025-03-22 22:50:39 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:2.2033817768096924 norm:0.0013249365147203207 max memory_allocated 29273.81884765625 
[2025-03-22 22:51:27 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:2.2027134895324707 norm:0.001326529891230166 max memory_allocated 29273.81884765625 
[2025-03-22 22:52:14 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:2.2021989822387695 norm:0.0013163973344489932 max memory_allocated 29273.81884765625 
[2025-03-22 22:53:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:2.201735496520996 norm:0.0013115752954035997 max memory_allocated 29273.81884765625 
[2025-03-22 22:53:50 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:2.201434373855591 norm:0.0013111525913700461 max memory_allocated 29273.81884765625 
[2025-03-22 22:54:38 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:2.2010631561279297 norm:0.0013130545848980546 max memory_allocated 29273.81884765625 
[2025-03-22 22:55:26 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:2.200742244720459 norm:0.0013152724131941795 max memory_allocated 29273.81884765625 
[2025-03-22 22:56:14 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:2.2004587650299072 norm:0.0013086405815556645 max memory_allocated 29273.81884765625 
[2025-03-22 22:57:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:2.2001571655273438 norm:0.0013190082972869277 max memory_allocated 29273.81884765625 
[2025-03-22 22:57:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 22:58:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:2.4917049407958984 norm:0.015631472691893578 max memory_allocated 29274.00634765625 
[2025-03-22 22:58:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:2.4721245765686035 norm:0.01095771137624979 max memory_allocated 29274.00634765625 
[2025-03-22 22:59:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:2.448521852493286 norm:0.007837672717869282 max memory_allocated 29274.00634765625 
[2025-03-22 23:00:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:2.439143419265747 norm:0.005969129037111998 max memory_allocated 29274.00634765625 
[2025-03-22 23:01:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:2.4321272373199463 norm:0.004437545780092478 max memory_allocated 29274.00634765625 
[2025-03-22 23:02:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:2.425567626953125 norm:0.0035986867733299732 max memory_allocated 29274.00634765625 
[2025-03-22 23:02:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:2.421847343444824 norm:0.003427547635510564 max memory_allocated 29274.00634765625 
[2025-03-22 23:03:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:2.418837547302246 norm:0.0030675679445266724 max memory_allocated 29274.00634765625 
[2025-03-22 23:04:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:2.4166927337646484 norm:0.0028026390355080366 max memory_allocated 29274.00634765625 
[2025-03-22 23:05:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:2.41487979888916 norm:0.0025626597926020622 max memory_allocated 29274.00634765625 
[2025-03-22 23:06:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:2.4133214950561523 norm:0.002401839941740036 max memory_allocated 29274.00634765625 
[2025-03-22 23:06:55 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:2.4121429920196533 norm:0.002260217908769846 max memory_allocated 29274.00634765625 
[2025-03-22 23:07:43 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:2.4112093448638916 norm:0.002141851931810379 max memory_allocated 29274.00634765625 
[2025-03-22 23:08:31 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:2.4103734493255615 norm:0.0020364702213555574 max memory_allocated 29274.00634765625 
[2025-03-22 23:09:19 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:2.409597635269165 norm:0.0019274991936981678 max memory_allocated 29274.00634765625 
[2025-03-22 23:10:07 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:2.408815383911133 norm:0.0018174622673541307 max memory_allocated 29274.00634765625 
[2025-03-22 23:10:55 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:2.4084908962249756 norm:0.0017980183474719524 max memory_allocated 29274.00634765625 
[2025-03-22 23:11:43 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:2.40795636177063 norm:0.001738987397402525 max memory_allocated 29274.00634765625 
[2025-03-22 23:12:31 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:2.407543897628784 norm:0.0016962670488283038 max memory_allocated 29274.00634765625 
[2025-03-22 23:13:19 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:2.4071223735809326 norm:0.0016431923722848296 max memory_allocated 29274.00634765625 
[2025-03-22 23:13:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 23:14:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:2.6940112113952637 norm:0.006552312523126602 max memory_allocated 29274.19384765625 
[2025-03-22 23:15:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:2.673001289367676 norm:0.003848492866382003 max memory_allocated 29274.19384765625 
[2025-03-22 23:16:00 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:2.6489710807800293 norm:0.0026931767351925373 max memory_allocated 29274.19384765625 
[2025-03-22 23:16:48 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:2.6396634578704834 norm:0.0021552371326833963 max memory_allocated 29274.19384765625 
[2025-03-22 23:17:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:2.6342315673828125 norm:0.001942073693498969 max memory_allocated 29274.19384765625 
[2025-03-22 23:18:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:2.6295411586761475 norm:0.0018203638028353453 max memory_allocated 29274.19384765625 
[2025-03-22 23:19:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:2.6253390312194824 norm:0.001688844640739262 max memory_allocated 29274.19384765625 
[2025-03-22 23:20:00 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:2.6224801540374756 norm:0.0016156334895640612 max memory_allocated 29274.19384765625 
[2025-03-22 23:20:48 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:2.620441198348999 norm:0.0015654159942641854 max memory_allocated 29274.19384765625 
[2025-03-22 23:21:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:2.618948459625244 norm:0.0015373517526313663 max memory_allocated 29274.19384765625 
[2025-03-22 23:22:25 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:2.6179325580596924 norm:0.0015381345292553306 max memory_allocated 29274.19384765625 
[2025-03-22 23:23:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:2.6169259548187256 norm:0.0015195647720247507 max memory_allocated 29274.19384765625 
[2025-03-22 23:24:01 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:2.6161646842956543 norm:0.0015043570892885327 max memory_allocated 29274.19384765625 
[2025-03-22 23:24:49 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:2.615450382232666 norm:0.001504741725511849 max memory_allocated 29274.19384765625 
[2025-03-22 23:25:37 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:2.6147873401641846 norm:0.001489951042458415 max memory_allocated 29274.19384765625 
[2025-03-22 23:26:25 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:2.6142988204956055 norm:0.0014811144210398197 max memory_allocated 29274.19384765625 
[2025-03-22 23:27:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:2.613973617553711 norm:0.0014795366441830993 max memory_allocated 29274.19384765625 
[2025-03-22 23:28:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:2.613629102706909 norm:0.0014885979471728206 max memory_allocated 29274.19384765625 
[2025-03-22 23:28:50 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:2.613219738006592 norm:0.0014996876707300544 max memory_allocated 29274.19384765625 
[2025-03-22 23:29:38 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:2.612884521484375 norm:0.0015013024676591158 max memory_allocated 29274.19384765625 
[2025-03-22 23:29:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 32 ===
[2025-03-22 23:30:43 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 0 loss:2.909069538116455 norm:0.011670952662825584 max memory_allocated 29274.38134765625 
[2025-03-22 23:31:32 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 1 loss:2.8863492012023926 norm:0.007015245500952005 max memory_allocated 29274.38134765625 
[2025-03-22 23:32:20 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 2 loss:2.85943865776062 norm:0.004644588567316532 max memory_allocated 29274.38134765625 
[2025-03-22 23:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 3 loss:2.847820520401001 norm:0.0031712381169199944 max memory_allocated 29274.38134765625 
[2025-03-22 23:33:56 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 4 loss:2.841945171356201 norm:0.0025172154419124126 max memory_allocated 29274.38134765625 
[2025-03-22 23:34:44 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 5 loss:2.8360540866851807 norm:0.0020338601898401976 max memory_allocated 29274.38134765625 
[2025-03-22 23:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 6 loss:2.831439971923828 norm:0.0017769061960279942 max memory_allocated 29274.38134765625 
[2025-03-22 23:36:20 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 7 loss:2.8279364109039307 norm:0.0016557665076106787 max memory_allocated 29274.38134765625 
[2025-03-22 23:37:08 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 8 loss:2.8254482746124268 norm:0.001608754158951342 max memory_allocated 29274.38134765625 
[2025-03-22 23:37:55 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 9 loss:2.8237040042877197 norm:0.0015562871703878045 max memory_allocated 29274.38134765625 
[2025-03-22 23:38:43 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 10 loss:2.8223419189453125 norm:0.001496348064392805 max memory_allocated 29274.38134765625 
[2025-03-22 23:39:31 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 11 loss:2.8211727142333984 norm:0.0014523717109113932 max memory_allocated 29274.38134765625 
[2025-03-22 23:40:19 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 12 loss:2.8203787803649902 norm:0.001434366567991674 max memory_allocated 29274.38134765625 
[2025-03-22 23:41:07 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 13 loss:2.8196120262145996 norm:0.0014150181086733937 max memory_allocated 29274.38134765625 
[2025-03-22 23:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 14 loss:2.818922519683838 norm:0.001396300969645381 max memory_allocated 29274.38134765625 
[2025-03-22 23:42:42 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 15 loss:2.818345546722412 norm:0.0013835136778652668 max memory_allocated 29274.38134765625 
[2025-03-22 23:43:30 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 16 loss:2.8178722858428955 norm:0.0013729925267398357 max memory_allocated 29274.38134765625 
[2025-03-22 23:44:18 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 17 loss:2.8174991607666016 norm:0.0013607291039079428 max memory_allocated 29274.38134765625 
[2025-03-22 23:45:06 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 18 loss:2.817140817642212 norm:0.0013472980353981256 max memory_allocated 29274.38134765625 
[2025-03-22 23:45:54 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 19 loss:2.8167648315429688 norm:0.0013394487323239446 max memory_allocated 29274.38134765625 
[2025-03-22 23:46:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 33 ===
[2025-03-22 23:47:00 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 0 loss:3.151235580444336 norm:0.010681062936782837 max memory_allocated 29274.56884765625 
[2025-03-22 23:47:48 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 1 loss:3.126330852508545 norm:0.0059164767153561115 max memory_allocated 29274.56884765625 
[2025-03-22 23:48:36 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 2 loss:3.097090005874634 norm:0.004008593503385782 max memory_allocated 29274.56884765625 
[2025-03-22 23:49:24 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 3 loss:3.085010528564453 norm:0.00297721941024065 max memory_allocated 29274.56884765625 
[2025-03-22 23:50:12 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 4 loss:3.078061103820801 norm:0.002286625327542424 max memory_allocated 29274.56884765625 
[2025-03-22 23:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 5 loss:3.07218074798584 norm:0.0018957317806780338 max memory_allocated 29274.56884765625 
[2025-03-22 23:51:48 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 6 loss:3.067119836807251 norm:0.0017514106584712863 max memory_allocated 29274.56884765625 
[2025-03-22 23:52:36 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 7 loss:3.0637173652648926 norm:0.0016683845315128565 max memory_allocated 29274.56884765625 
[2025-03-22 23:53:24 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 8 loss:3.06121826171875 norm:0.001614910550415516 max memory_allocated 29274.56884765625 
[2025-03-22 23:54:12 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 9 loss:3.0593206882476807 norm:0.0015570477116852999 max memory_allocated 29274.56884765625 
[2025-03-22 23:55:00 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 10 loss:3.0579490661621094 norm:0.0015234225429594517 max memory_allocated 29274.56884765625 
[2025-03-22 23:55:48 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 11 loss:3.0569376945495605 norm:0.0015102538745850325 max memory_allocated 29274.56884765625 
[2025-03-22 23:56:36 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 12 loss:3.0559377670288086 norm:0.0014827823033556342 max memory_allocated 29274.56884765625 
[2025-03-22 23:57:23 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 13 loss:3.055253505706787 norm:0.00147426116745919 max memory_allocated 29274.56884765625 
[2025-03-22 23:58:11 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 14 loss:3.0545101165771484 norm:0.0014492595801129937 max memory_allocated 29274.56884765625 
[2025-03-22 23:58:59 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 15 loss:3.0537877082824707 norm:0.0014528119936585426 max memory_allocated 29274.56884765625 
[2025-03-22 23:59:47 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 16 loss:3.0531857013702393 norm:0.0014460202073678374 max memory_allocated 29274.56884765625 
[2025-03-23 00:00:35 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 17 loss:3.0528035163879395 norm:0.0014431504532694817 max memory_allocated 29274.56884765625 
[2025-03-23 00:01:23 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 18 loss:3.052346706390381 norm:0.0014370273565873504 max memory_allocated 29274.56884765625 
[2025-03-23 00:02:11 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 19 loss:3.0518438816070557 norm:0.0014397355262190104 max memory_allocated 29274.56884765625 
[2025-03-23 00:02:24 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 34 ===
[2025-03-23 00:03:16 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 0 loss:3.469774007797241 norm:0.033418189734220505 max memory_allocated 29274.75634765625 
[2025-03-23 00:04:04 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 1 loss:3.4367549419403076 norm:0.021925216540694237 max memory_allocated 29274.75634765625 
[2025-03-23 00:04:52 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 2 loss:3.397397994995117 norm:0.01424328237771988 max memory_allocated 29274.75634765625 
[2025-03-23 00:05:40 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 3 loss:3.376155376434326 norm:0.00952061265707016 max memory_allocated 29274.75634765625 
[2025-03-23 00:06:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 4 loss:3.3635716438293457 norm:0.007396923843771219 max memory_allocated 29274.75634765625 
[2025-03-23 00:07:16 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 5 loss:3.355285882949829 norm:0.006215974222868681 max memory_allocated 29274.75634765625 
[2025-03-23 00:08:04 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 6 loss:3.349271059036255 norm:0.005526556167751551 max memory_allocated 29274.75634765625 
[2025-03-23 00:08:52 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 7 loss:3.3434057235717773 norm:0.004364556632936001 max memory_allocated 29274.75634765625 
[2025-03-23 00:09:40 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 8 loss:3.337688684463501 norm:0.0019121025688946247 max memory_allocated 29274.75634765625 
[2025-03-23 00:10:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 9 loss:3.3346734046936035 norm:0.0019158272771164775 max memory_allocated 29274.75634765625 
[2025-03-23 00:11:16 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 10 loss:3.3329336643218994 norm:0.001877176109701395 max memory_allocated 29274.75634765625 
[2025-03-23 00:12:04 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 11 loss:3.3314106464385986 norm:0.0018545696511864662 max memory_allocated 29274.75634765625 
[2025-03-23 00:12:52 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 12 loss:3.3297674655914307 norm:0.0018435562960803509 max memory_allocated 29274.75634765625 
[2025-03-23 00:13:39 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 13 loss:3.32858943939209 norm:0.0018278909847140312 max memory_allocated 29274.75634765625 
[2025-03-23 00:14:27 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 14 loss:3.327719211578369 norm:0.0018132285913452506 max memory_allocated 29274.75634765625 
[2025-03-23 00:15:15 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 15 loss:3.326997756958008 norm:0.0018133108969777822 max memory_allocated 29274.75634765625 
[2025-03-23 00:16:03 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 16 loss:3.3264832496643066 norm:0.0018070783698931336 max memory_allocated 29274.75634765625 
[2025-03-23 00:16:51 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 17 loss:3.326084852218628 norm:0.0017932257615029812 max memory_allocated 29274.75634765625 
[2025-03-23 00:17:39 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 18 loss:3.325578212738037 norm:0.0017830135766416788 max memory_allocated 29274.75634765625 
[2025-03-23 00:18:26 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 19 loss:3.325071096420288 norm:0.001804944360628724 max memory_allocated 29274.75634765625 
[2025-03-23 00:18:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 35 ===
[2025-03-23 00:19:32 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 0 loss:3.761650323867798 norm:0.02226663939654827 max memory_allocated 29274.94384765625 
[2025-03-23 00:20:20 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 1 loss:3.7228140830993652 norm:0.015069843269884586 max memory_allocated 29274.94384765625 
[2025-03-23 00:21:08 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 2 loss:3.680330276489258 norm:0.010255314409732819 max memory_allocated 29274.94384765625 
[2025-03-23 00:21:56 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 3 loss:3.6579222679138184 norm:0.007005937397480011 max memory_allocated 29274.94384765625 
[2025-03-23 00:22:44 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 4 loss:3.645033836364746 norm:0.00563808623701334 max memory_allocated 29274.94384765625 
[2025-03-23 00:23:32 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 5 loss:3.6356072425842285 norm:0.004925872664898634 max memory_allocated 29274.94384765625 
[2025-03-23 00:24:20 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 6 loss:3.628258466720581 norm:0.004287307150661945 max memory_allocated 29274.94384765625 
[2025-03-23 00:25:08 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 7 loss:3.6229374408721924 norm:0.003858930431306362 max memory_allocated 29274.94384765625 
[2025-03-23 00:25:56 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 8 loss:3.618957281112671 norm:0.0035223246086388826 max memory_allocated 29274.94384765625 
[2025-03-23 00:26:44 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 9 loss:3.616039991378784 norm:0.0033147293142974377 max memory_allocated 29274.94384765625 
[2025-03-23 00:27:32 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 10 loss:3.6134209632873535 norm:0.003212001407518983 max memory_allocated 29274.94384765625 
[2025-03-23 00:28:20 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 11 loss:3.610751152038574 norm:0.0029635841492563486 max memory_allocated 29274.94384765625 
[2025-03-23 00:29:08 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 12 loss:3.608776569366455 norm:0.002864220179617405 max memory_allocated 29274.94384765625 
[2025-03-23 00:29:56 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 13 loss:3.6071114540100098 norm:0.0027806826401501894 max memory_allocated 29274.94384765625 
[2025-03-23 00:30:44 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 14 loss:3.605464458465576 norm:0.002712576650083065 max memory_allocated 29274.94384765625 
[2025-03-23 00:31:32 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 15 loss:3.6042606830596924 norm:0.0026341655757278204 max memory_allocated 29274.94384765625 
[2025-03-23 00:32:21 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 16 loss:3.6030759811401367 norm:0.0026346880476921797 max memory_allocated 29274.94384765625 
[2025-03-23 00:33:09 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 17 loss:3.6021018028259277 norm:0.0025971606373786926 max memory_allocated 29274.94384765625 
[2025-03-23 00:33:57 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 18 loss:3.601647138595581 norm:0.0025691911578178406 max memory_allocated 29274.94384765625 
[2025-03-23 00:34:45 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 19 loss:3.601104736328125 norm:0.002588625531643629 max memory_allocated 29274.94384765625 
[2025-03-23 00:34:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 36 ===
[2025-03-23 00:35:02 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 00:35:51 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 0 loss:4.091638565063477 norm:0.06442242115736008 max memory_allocated 29276.28759765625 
[2025-03-23 00:36:39 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 1 loss:4.042348861694336 norm:0.05902956798672676 max memory_allocated 29276.28759765625 
[2025-03-23 00:37:27 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 2 loss:3.988471269607544 norm:0.04679282754659653 max memory_allocated 29276.28759765625 
[2025-03-23 00:38:16 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 3 loss:3.962175130844116 norm:0.03703276067972183 max memory_allocated 29276.28759765625 
[2025-03-23 00:39:04 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 4 loss:3.9469246864318848 norm:0.02973582223057747 max memory_allocated 29276.28759765625 
[2025-03-23 00:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 5 loss:3.9347610473632812 norm:0.025447623804211617 max memory_allocated 29276.28759765625 
[2025-03-23 00:40:40 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 6 loss:3.9241275787353516 norm:0.0225524865090847 max memory_allocated 29276.28759765625 
[2025-03-23 00:41:28 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 7 loss:3.91727352142334 norm:0.021062858402729034 max memory_allocated 29276.28759765625 
[2025-03-23 00:42:16 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 8 loss:3.912050485610962 norm:0.019852392375469208 max memory_allocated 29276.28759765625 
[2025-03-23 00:43:04 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 9 loss:3.9080021381378174 norm:0.019346505403518677 max memory_allocated 29276.28759765625 
[2025-03-23 00:43:52 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 10 loss:3.905247211456299 norm:0.019570359960198402 max memory_allocated 29276.28759765625 
[2025-03-23 00:44:40 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 11 loss:3.9029359817504883 norm:0.019344288855791092 max memory_allocated 29276.28759765625 
[2025-03-23 00:45:28 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 12 loss:3.901775360107422 norm:0.019041407853364944 max memory_allocated 29276.28759765625 
[2025-03-23 00:46:16 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 13 loss:3.8992528915405273 norm:0.018436089158058167 max memory_allocated 29276.28759765625 
[2025-03-23 00:47:04 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 14 loss:3.897152900695801 norm:0.018075229600071907 max memory_allocated 29276.28759765625 
[2025-03-23 00:47:53 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 15 loss:3.8961856365203857 norm:0.01865265518426895 max memory_allocated 29276.28759765625 
[2025-03-23 00:48:41 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 16 loss:3.895575523376465 norm:0.01800483465194702 max memory_allocated 29276.28759765625 
[2025-03-23 00:49:29 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 17 loss:3.8952858448028564 norm:0.018635455518960953 max memory_allocated 29276.28759765625 
[2025-03-23 00:50:17 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 18 loss:3.894650936126709 norm:0.017778411507606506 max memory_allocated 29276.28759765625 
[2025-03-23 00:51:05 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 19 loss:3.892815113067627 norm:0.017630383372306824 max memory_allocated 29276.28759765625 
[2025-03-23 00:51:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 37 ===
[2025-03-23 00:51:23 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 00:52:11 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 0 loss:4.58657693862915 norm:0.10243422538042068 max memory_allocated 29276.47509765625 
[2025-03-23 00:52:59 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 1 loss:4.510060787200928 norm:0.08824041485786438 max memory_allocated 29276.47509765625 
[2025-03-23 00:53:48 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 2 loss:4.433938026428223 norm:0.06685016304254532 max memory_allocated 29276.47509765625 
[2025-03-23 00:54:36 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 3 loss:4.397391319274902 norm:0.05052592605352402 max memory_allocated 29276.47509765625 
[2025-03-23 00:55:24 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 4 loss:4.3754496574401855 norm:0.03950909897685051 max memory_allocated 29276.47509765625 
[2025-03-23 00:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 5 loss:4.360464572906494 norm:0.031492240726947784 max memory_allocated 29276.47509765625 
[2025-03-23 00:57:01 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 6 loss:4.349273681640625 norm:0.0268329456448555 max memory_allocated 29276.47509765625 
[2025-03-23 00:57:49 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 7 loss:4.3397650718688965 norm:0.024720337241888046 max memory_allocated 29276.47509765625 
[2025-03-23 00:58:37 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 8 loss:4.333796977996826 norm:0.023193877190351486 max memory_allocated 29276.47509765625 
[2025-03-23 00:59:25 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 9 loss:4.328697204589844 norm:0.02471526898443699 max memory_allocated 29276.47509765625 
[2025-03-23 01:00:13 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 10 loss:4.327258586883545 norm:0.020653557032346725 max memory_allocated 29276.47509765625 
[2025-03-23 01:01:01 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 11 loss:4.321983814239502 norm:0.017259802669286728 max memory_allocated 29276.47509765625 
[2025-03-23 01:01:49 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 12 loss:4.315497398376465 norm:0.019724946469068527 max memory_allocated 29276.47509765625 
[2025-03-23 01:02:37 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 13 loss:4.313145637512207 norm:0.01990499719977379 max memory_allocated 29276.47509765625 
[2025-03-23 01:03:25 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 14 loss:4.3117756843566895 norm:0.019059766083955765 max memory_allocated 29276.47509765625 
[2025-03-23 01:04:13 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 15 loss:4.309117794036865 norm:0.01908503845334053 max memory_allocated 29276.47509765625 
[2025-03-23 01:05:01 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 16 loss:4.309186935424805 norm:0.018511969596147537 max memory_allocated 29276.47509765625 
[2025-03-23 01:05:49 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 17 loss:4.306380748748779 norm:0.018826596438884735 max memory_allocated 29276.47509765625 
[2025-03-23 01:06:37 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 18 loss:4.305261135101318 norm:0.017238855361938477 max memory_allocated 29276.47509765625 
[2025-03-23 01:07:25 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 19 loss:4.304013729095459 norm:0.019059179350733757 max memory_allocated 29276.47509765625 
[2025-03-23 01:07:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 38 ===
[2025-03-23 01:07:43 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 01:08:31 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 0 loss:6.110428810119629 norm:0.5762455463409424 max memory_allocated 29276.66259765625 
[2025-03-23 01:09:19 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 1 loss:5.794966697692871 norm:0.4129464030265808 max memory_allocated 29276.66259765625 
[2025-03-23 01:10:07 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 2 loss:5.653005599975586 norm:0.33099642395973206 max memory_allocated 29276.66259765625 
[2025-03-23 01:10:55 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 3 loss:5.525229454040527 norm:0.23706288635730743 max memory_allocated 29276.66259765625 
[2025-03-23 01:11:43 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 4 loss:5.435437202453613 norm:0.16330276429653168 max memory_allocated 29276.66259765625 
[2025-03-23 01:12:32 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 5 loss:5.367804527282715 norm:0.1189456507563591 max memory_allocated 29276.66259765625 
[2025-03-23 01:13:20 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 6 loss:5.312160015106201 norm:0.09003004431724548 max memory_allocated 29276.66259765625 
[2025-03-23 01:14:08 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 7 loss:5.27671480178833 norm:0.0797600969672203 max memory_allocated 29276.66259765625 
[2025-03-23 01:14:56 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 8 loss:5.251535415649414 norm:0.07301442325115204 max memory_allocated 29276.66259765625 
[2025-03-23 01:15:44 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 9 loss:5.231226444244385 norm:0.06786499917507172 max memory_allocated 29276.66259765625 
[2025-03-23 01:16:33 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 10 loss:5.2178802490234375 norm:0.0666293054819107 max memory_allocated 29276.66259765625 
[2025-03-23 01:17:21 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 11 loss:5.205552101135254 norm:0.06497989594936371 max memory_allocated 29276.66259765625 
[2025-03-23 01:18:09 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 12 loss:5.197429656982422 norm:0.0653257817029953 max memory_allocated 29276.66259765625 
[2025-03-23 01:18:57 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 13 loss:5.195832252502441 norm:0.06776460260152817 max memory_allocated 29276.66259765625 
[2025-03-23 01:19:46 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 14 loss:5.195133209228516 norm:0.07046039402484894 max memory_allocated 29276.66259765625 
[2025-03-23 01:20:34 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 15 loss:5.19789981842041 norm:0.07283128798007965 max memory_allocated 29276.66259765625 
[2025-03-23 01:21:22 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 16 loss:5.183481693267822 norm:0.06721597909927368 max memory_allocated 29276.66259765625 
[2025-03-23 01:22:11 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 17 loss:5.179171085357666 norm:0.064308300614357 max memory_allocated 29276.66259765625 
[2025-03-23 01:22:59 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 18 loss:5.180429458618164 norm:0.06239486113190651 max memory_allocated 29276.66259765625 
[2025-03-23 01:23:47 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 19 loss:5.173205375671387 norm:0.057695772498846054 max memory_allocated 29276.66259765625 
[2025-03-23 01:24:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 39 ===
[2025-03-23 01:24:05 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 01:24:53 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 0 loss:10.313448905944824 norm:0.8261010646820068 max memory_allocated 29276.85009765625 
[2025-03-23 01:25:41 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 1 loss:9.787601470947266 norm:0.6162195205688477 max memory_allocated 29276.85009765625 
[2025-03-23 01:26:29 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 2 loss:9.326608657836914 norm:0.5044528245925903 max memory_allocated 29276.85009765625 
[2025-03-23 01:27:17 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 3 loss:9.03120231628418 norm:0.4821383059024811 max memory_allocated 29276.85009765625 
[2025-03-23 01:28:04 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 4 loss:8.887569427490234 norm:0.43566685914993286 max memory_allocated 29276.85009765625 
[2025-03-23 01:28:52 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 5 loss:8.78480052947998 norm:0.38989201188087463 max memory_allocated 29276.85009765625 
[2025-03-23 01:29:40 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 6 loss:8.696210861206055 norm:0.3683851361274719 max memory_allocated 29276.85009765625 
[2025-03-23 01:30:28 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 7 loss:8.62427043914795 norm:0.3297611474990845 max memory_allocated 29276.85009765625 
[2025-03-23 01:31:16 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 8 loss:8.581622123718262 norm:0.3318820297718048 max memory_allocated 29276.85009765625 
[2025-03-23 01:32:04 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 9 loss:8.514432907104492 norm:0.3212027847766876 max memory_allocated 29276.85009765625 
[2025-03-23 01:32:52 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 10 loss:8.473310470581055 norm:0.30553296208381653 max memory_allocated 29276.85009765625 
[2025-03-23 01:33:40 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 11 loss:8.443807601928711 norm:0.28021445870399475 max memory_allocated 29276.85009765625 
[2025-03-23 01:34:28 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 12 loss:8.424212455749512 norm:0.2811841666698456 max memory_allocated 29276.85009765625 
[2025-03-23 01:35:17 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 13 loss:8.399463653564453 norm:0.25861355662345886 max memory_allocated 29276.85009765625 
[2025-03-23 01:36:05 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 14 loss:8.386992454528809 norm:0.2586846649646759 max memory_allocated 29276.85009765625 
[2025-03-23 01:36:53 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 15 loss:8.371957778930664 norm:0.2593240737915039 max memory_allocated 29276.85009765625 
[2025-03-23 01:37:42 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 16 loss:8.367375373840332 norm:0.2695018947124481 max memory_allocated 29276.85009765625 
[2025-03-23 01:38:30 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 17 loss:8.337327003479004 norm:0.240922749042511 max memory_allocated 29276.85009765625 
[2025-03-23 01:39:18 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 18 loss:8.326003074645996 norm:0.23351158201694489 max memory_allocated 29276.85009765625 
[2025-03-23 01:40:06 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 19 loss:8.328897476196289 norm:0.2452823370695114 max memory_allocated 29276.85009765625 
[2025-03-23 01:40:20 root] (main_calibration_a.py 369): INFO 39104.02546477318
[2025-03-23 01:40:28 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 01:42:20 root] (main_calibration_a.py 158): INFO wikitext2 : 7.630070686340332
[2025-03-23 01:42:20 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 01:45:12 root] (main_calibration_a.py 158): INFO c4 : 10.846210479736328
[2025-03-23 03:55:08 root] (main_calibration_a.py 169): INFO {'wikitext2': 7.630070686340332, 'c4': 10.846210479736328, 'results': {'boolq': {'acc': 0.636697247706422, 'acc_stderr': 0.008411885836787156}, 'hellaswag': {'acc': 0.4849631547500498, 'acc_stderr': 0.004987524454849701, 'acc_norm': 0.6371240788687512, 'acc_norm_stderr': 0.004798467983635786}, 'arc_challenge': {'acc': 0.2773037542662116, 'acc_stderr': 0.013082095839059374, 'acc_norm': 0.31569965870307165, 'acc_norm_stderr': 0.01358257109581529}, 'winogrande': {'acc': 0.5603788476716653, 'acc_stderr': 0.013949649776015687}, 'arc_easy': {'acc': 0.5286195286195287, 'acc_stderr': 0.010242962617927193, 'acc_norm': 0.43476430976430974, 'acc_norm_stderr': 0.010172083670402786}, 'piqa': {'acc': 0.6708378672470077, 'acc_stderr': 0.0109637504141347, 'acc_norm': 0.6735582154515778, 'acc_norm_stderr': 0.010940467046177306}}, 'versions': {'boolq': 1, 'hellaswag': 0, 'arc_challenge': 0, 'winogrande': 0, 'arc_easy': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 03:55:08 root] (main_calibration_a.py 172): INFO 27.73,52.86,63.67,48.50,67.08,56.04
