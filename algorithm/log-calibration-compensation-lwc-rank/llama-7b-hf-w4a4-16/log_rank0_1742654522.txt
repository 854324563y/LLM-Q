[2025-03-22 14:42:02 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/llama-7b-hf-w4a4-16', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=16)
[2025-03-22 14:45:29 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 14:45:29 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:45:30 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 14:45:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 14:45:35 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:46:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.134061798453331 norm:0.10582264512777328 max memory_allocated 22561.69970703125 
[2025-03-22 14:46:39 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.06210465729236603 norm:0.08203164488077164 max memory_allocated 22561.69970703125 
[2025-03-22 14:47:12 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.047875210642814636 norm:0.05648576468229294 max memory_allocated 22561.69970703125 
[2025-03-22 14:47:44 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.04294870048761368 norm:0.04961852729320526 max memory_allocated 22561.69970703125 
[2025-03-22 14:48:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.040347516536712646 norm:0.044032394886016846 max memory_allocated 22561.69970703125 
[2025-03-22 14:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.0384017638862133 norm:0.03918197751045227 max memory_allocated 22561.69970703125 
[2025-03-22 14:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.03673211485147476 norm:0.03471614420413971 max memory_allocated 22561.69970703125 
[2025-03-22 14:49:54 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.03548194840550423 norm:0.030560791492462158 max memory_allocated 22561.69970703125 
[2025-03-22 14:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.034541599452495575 norm:0.027259398251771927 max memory_allocated 22561.69970703125 
[2025-03-22 14:50:59 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.033387817442417145 norm:0.02380896732211113 max memory_allocated 22561.69970703125 
[2025-03-22 14:51:32 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.0327015295624733 norm:0.021231690421700478 max memory_allocated 22561.69970703125 
[2025-03-22 14:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.03200710937380791 norm:0.0181589238345623 max memory_allocated 22561.69970703125 
[2025-03-22 14:52:37 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.03136012703180313 norm:0.016125859692692757 max memory_allocated 22561.69970703125 
[2025-03-22 14:53:10 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.030993562191724777 norm:0.014398202300071716 max memory_allocated 22561.69970703125 
[2025-03-22 14:53:42 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.03062409721314907 norm:0.01280687190592289 max memory_allocated 22561.69970703125 
[2025-03-22 14:54:15 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.030354347079992294 norm:0.011358572170138359 max memory_allocated 22561.69970703125 
[2025-03-22 14:54:47 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.03016827255487442 norm:0.010364603251218796 max memory_allocated 22561.69970703125 
[2025-03-22 14:55:20 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.030016131699085236 norm:0.009828649461269379 max memory_allocated 22561.69970703125 
[2025-03-22 14:55:52 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.029872708022594452 norm:0.009330352768301964 max memory_allocated 22561.69970703125 
[2025-03-22 14:56:25 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.029665252193808556 norm:0.008691290393471718 max memory_allocated 22561.69970703125 
[2025-03-22 14:56:34 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 14:56:36 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:57:09 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.35307878255844116 norm:0.3058888018131256 max memory_allocated 22561.87158203125 
[2025-03-22 14:57:41 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.17319422960281372 norm:0.1447584182024002 max memory_allocated 22561.87158203125 
[2025-03-22 14:58:14 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.12284093350172043 norm:0.06541189551353455 max memory_allocated 22561.87158203125 
[2025-03-22 14:58:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.10582111030817032 norm:0.049883246421813965 max memory_allocated 22561.87158203125 
[2025-03-22 14:59:19 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.09733197093009949 norm:0.040338486433029175 max memory_allocated 22561.87158203125 
[2025-03-22 14:59:51 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.09214292466640472 norm:0.035192959010601044 max memory_allocated 22561.87158203125 
[2025-03-22 15:00:23 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.08811981230974197 norm:0.029521018266677856 max memory_allocated 22561.87158203125 
[2025-03-22 15:00:56 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.08503782749176025 norm:0.02542758919298649 max memory_allocated 22561.87158203125 
[2025-03-22 15:01:29 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.08301146328449249 norm:0.022435354068875313 max memory_allocated 22561.87158203125 
[2025-03-22 15:02:01 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.08124446868896484 norm:0.020008696243166924 max memory_allocated 22561.87158203125 
[2025-03-22 15:02:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.07988429069519043 norm:0.01778133027255535 max memory_allocated 22561.87158203125 
[2025-03-22 15:03:06 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.07868039608001709 norm:0.015780223533511162 max memory_allocated 22561.87158203125 
[2025-03-22 15:03:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.07790423184633255 norm:0.0141775943338871 max memory_allocated 22561.87158203125 
[2025-03-22 15:04:12 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.07721228897571564 norm:0.012798598036170006 max memory_allocated 22561.87158203125 
[2025-03-22 15:04:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.0766451358795166 norm:0.011625019833445549 max memory_allocated 22561.87158203125 
[2025-03-22 15:05:17 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07626842707395554 norm:0.010844167321920395 max memory_allocated 22561.87158203125 
[2025-03-22 15:05:50 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.07594678550958633 norm:0.010061836801469326 max memory_allocated 22561.87158203125 
[2025-03-22 15:06:22 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07557351142168045 norm:0.009741133078932762 max memory_allocated 22561.87158203125 
[2025-03-22 15:06:55 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07537639141082764 norm:0.009562566876411438 max memory_allocated 22561.87158203125 
[2025-03-22 15:07:28 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07499656826257706 norm:0.008925586007535458 max memory_allocated 22561.87158203125 
[2025-03-22 15:07:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 15:07:39 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:08:12 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.2848352789878845 norm:0.08503634482622147 max memory_allocated 22562.04345703125 
[2025-03-22 15:08:44 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.22954170405864716 norm:0.04857523739337921 max memory_allocated 22562.04345703125 
[2025-03-22 15:09:17 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.2034444659948349 norm:0.052025459706783295 max memory_allocated 22562.04345703125 
[2025-03-22 15:09:50 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.19036366045475006 norm:0.05512506514787674 max memory_allocated 22562.04345703125 
[2025-03-22 15:10:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.17475973069667816 norm:0.04598439112305641 max memory_allocated 22562.04345703125 
[2025-03-22 15:10:55 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.16969507932662964 norm:0.04986640810966492 max memory_allocated 22562.04345703125 
[2025-03-22 15:11:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.16891147196292877 norm:0.04263090342283249 max memory_allocated 22562.04345703125 
[2025-03-22 15:12:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.16579492390155792 norm:0.04069850593805313 max memory_allocated 22562.04345703125 
[2025-03-22 15:12:33 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.16198810935020447 norm:0.037823427468538284 max memory_allocated 22562.04345703125 
[2025-03-22 15:13:05 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.16054832935333252 norm:0.036402419209480286 max memory_allocated 22562.04345703125 
[2025-03-22 15:13:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.15505266189575195 norm:0.0361732617020607 max memory_allocated 22562.04345703125 
[2025-03-22 15:14:10 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.15434318780899048 norm:0.03197968006134033 max memory_allocated 22562.04345703125 
[2025-03-22 15:14:43 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.15375979244709015 norm:0.031961590051651 max memory_allocated 22562.04345703125 
[2025-03-22 15:15:15 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.15223802626132965 norm:0.0326174832880497 max memory_allocated 22562.04345703125 
[2025-03-22 15:15:48 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.15143704414367676 norm:0.02915550209581852 max memory_allocated 22562.04345703125 
[2025-03-22 15:16:21 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.15021759271621704 norm:0.03041820600628853 max memory_allocated 22562.04345703125 
[2025-03-22 15:16:53 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.14943210780620575 norm:0.030120424926280975 max memory_allocated 22562.04345703125 
[2025-03-22 15:17:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.14865300059318542 norm:0.03212784603238106 max memory_allocated 22562.04345703125 
[2025-03-22 15:17:58 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.150441512465477 norm:0.02640286274254322 max memory_allocated 22562.04345703125 
[2025-03-22 15:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.1504446417093277 norm:0.026196539402008057 max memory_allocated 22562.04345703125 
[2025-03-22 15:18:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 15:19:15 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.25921446084976196 norm:0.054504718631505966 max memory_allocated 22562.04345703125 
[2025-03-22 15:19:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.21199633181095123 norm:0.01599644310772419 max memory_allocated 22562.04345703125 
[2025-03-22 15:20:20 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.1827072948217392 norm:0.006457559298723936 max memory_allocated 22562.04345703125 
[2025-03-22 15:20:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.17350535094738007 norm:0.004094475880265236 max memory_allocated 22562.04345703125 
[2025-03-22 15:21:25 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.168934166431427 norm:0.0030681926291435957 max memory_allocated 22562.04345703125 
[2025-03-22 15:21:58 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.16585825383663177 norm:0.002539017703384161 max memory_allocated 22562.04345703125 
[2025-03-22 15:22:30 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.16390381753444672 norm:0.0022613871842622757 max memory_allocated 22562.04345703125 
[2025-03-22 15:23:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.16259829699993134 norm:0.0021266015246510506 max memory_allocated 22562.04345703125 
[2025-03-22 15:23:36 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.16145005822181702 norm:0.001955872168764472 max memory_allocated 22562.04345703125 
[2025-03-22 15:24:08 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.16099178791046143 norm:0.0019563075620681047 max memory_allocated 22562.04345703125 
[2025-03-22 15:24:41 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.16045039892196655 norm:0.0018610258121043444 max memory_allocated 22562.04345703125 
[2025-03-22 15:25:13 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.15989194810390472 norm:0.0017135455273091793 max memory_allocated 22562.04345703125 
[2025-03-22 15:25:46 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.159334197640419 norm:0.00162722438108176 max memory_allocated 22562.04345703125 
[2025-03-22 15:26:18 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.15915542840957642 norm:0.0015860159182921052 max memory_allocated 22562.04345703125 
[2025-03-22 15:26:51 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.15903019905090332 norm:0.0015235875034704804 max memory_allocated 22562.04345703125 
[2025-03-22 15:27:23 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.15910238027572632 norm:0.001480455044656992 max memory_allocated 22562.04345703125 
[2025-03-22 15:27:56 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.15897773206233978 norm:0.001445296686142683 max memory_allocated 22562.04345703125 
[2025-03-22 15:28:28 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.15885451436042786 norm:0.001413044985383749 max memory_allocated 22562.04345703125 
[2025-03-22 15:29:01 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.15884505212306976 norm:0.0014485319843515754 max memory_allocated 22562.04345703125 
[2025-03-22 15:29:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.15861065685749054 norm:0.0013532949378713965 max memory_allocated 22562.04345703125 
[2025-03-22 15:29:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 15:30:17 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.3085615932941437 norm:0.05009765177965164 max memory_allocated 22562.04345703125 
[2025-03-22 15:30:49 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.2586486339569092 norm:0.01957578957080841 max memory_allocated 22562.04345703125 
[2025-03-22 15:31:22 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.22329887747764587 norm:0.006988996174186468 max memory_allocated 22562.04345703125 
[2025-03-22 15:31:54 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.21014243364334106 norm:0.003491378389298916 max memory_allocated 22562.04345703125 
[2025-03-22 15:32:27 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.20529042184352875 norm:0.0028025370556861162 max memory_allocated 22562.04345703125 
[2025-03-22 15:32:59 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.20223590731620789 norm:0.0024491020012646914 max memory_allocated 22562.04345703125 
[2025-03-22 15:33:32 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.20041333138942719 norm:0.0022715232335031033 max memory_allocated 22562.04345703125 
[2025-03-22 15:34:04 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.19905903935432434 norm:0.002123973798006773 max memory_allocated 22562.04345703125 
[2025-03-22 15:34:37 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.19815127551555634 norm:0.002079937607049942 max memory_allocated 22562.04345703125 
[2025-03-22 15:35:09 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.1973763406276703 norm:0.001769335474818945 max memory_allocated 22562.04345703125 
[2025-03-22 15:35:42 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.19676604866981506 norm:0.001685017952695489 max memory_allocated 22562.04345703125 
[2025-03-22 15:36:14 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.1966889202594757 norm:0.0017333897994831204 max memory_allocated 22562.04345703125 
[2025-03-22 15:36:47 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.1964188516139984 norm:0.0016035530716180801 max memory_allocated 22562.04345703125 
[2025-03-22 15:37:19 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.19653944671154022 norm:0.0016559198265895247 max memory_allocated 22562.04345703125 
[2025-03-22 15:37:52 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.19642065465450287 norm:0.001581760821864009 max memory_allocated 22562.04345703125 
[2025-03-22 15:38:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.19621509313583374 norm:0.001552954432554543 max memory_allocated 22562.04345703125 
[2025-03-22 15:38:57 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.19581688940525055 norm:0.0013264528242871165 max memory_allocated 22562.04345703125 
[2025-03-22 15:39:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.19589054584503174 norm:0.0013789189979434013 max memory_allocated 22562.04345703125 
[2025-03-22 15:40:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.1956658661365509 norm:0.0012835883535444736 max memory_allocated 22562.04345703125 
[2025-03-22 15:40:35 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.19575649499893188 norm:0.0012545835925266147 max memory_allocated 22562.04345703125 
[2025-03-22 15:40:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 15:41:19 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.35340628027915955 norm:0.06806968152523041 max memory_allocated 22562.04345703125 
[2025-03-22 15:41:52 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2989119589328766 norm:0.024312298744916916 max memory_allocated 22562.04345703125 
[2025-03-22 15:42:24 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.25095927715301514 norm:0.007282220758497715 max memory_allocated 22562.04345703125 
[2025-03-22 15:42:56 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.2367364764213562 norm:0.004207460209727287 max memory_allocated 22562.04345703125 
[2025-03-22 15:43:29 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.23088449239730835 norm:0.0031744204461574554 max memory_allocated 22562.04345703125 
[2025-03-22 15:44:01 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.22690600156784058 norm:0.0026994773652404547 max memory_allocated 22562.04345703125 
[2025-03-22 15:44:34 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.22469846904277802 norm:0.0025473949499428272 max memory_allocated 22562.04345703125 
[2025-03-22 15:45:06 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.22295916080474854 norm:0.0023405705578625202 max memory_allocated 22562.04345703125 
[2025-03-22 15:45:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.2218475043773651 norm:0.0021767925936728716 max memory_allocated 22562.04345703125 
[2025-03-22 15:46:11 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.22098150849342346 norm:0.001987484283745289 max memory_allocated 22562.04345703125 
[2025-03-22 15:46:43 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.2204047292470932 norm:0.0019112667068839073 max memory_allocated 22562.04345703125 
[2025-03-22 15:47:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.2198447287082672 norm:0.0017942986451089382 max memory_allocated 22562.04345703125 
[2025-03-22 15:47:48 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.2192612737417221 norm:0.0016833455301821232 max memory_allocated 22562.04345703125 
[2025-03-22 15:48:21 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.21902106702327728 norm:0.001663072849623859 max memory_allocated 22562.04345703125 
[2025-03-22 15:48:53 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.21864229440689087 norm:0.0015931871021166444 max memory_allocated 22562.04345703125 
[2025-03-22 15:49:26 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.218190997838974 norm:0.0015329981688410044 max memory_allocated 22562.04345703125 
[2025-03-22 15:49:58 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.21762371063232422 norm:0.0013980044750496745 max memory_allocated 22562.04345703125 
[2025-03-22 15:50:31 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.21754112839698792 norm:0.001348725170828402 max memory_allocated 22562.04345703125 
[2025-03-22 15:51:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.21759594976902008 norm:0.0012908390490338206 max memory_allocated 22562.04345703125 
[2025-03-22 15:51:36 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.21781034767627716 norm:0.0012977338628843427 max memory_allocated 22562.04345703125 
[2025-03-22 15:51:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 15:52:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.3773840367794037 norm:0.06881852447986603 max memory_allocated 22562.04345703125 
[2025-03-22 15:52:53 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.32768332958221436 norm:0.0282426867634058 max memory_allocated 22562.04345703125 
[2025-03-22 15:53:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.27784520387649536 norm:0.007975416257977486 max memory_allocated 22562.04345703125 
[2025-03-22 15:53:58 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.26110392808914185 norm:0.0038052520249038935 max memory_allocated 22562.04345703125 
[2025-03-22 15:54:31 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.25518250465393066 norm:0.002929304027929902 max memory_allocated 22562.04345703125 
[2025-03-22 15:55:03 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.2516266405582428 norm:0.002416461007669568 max memory_allocated 22562.04345703125 
[2025-03-22 15:55:36 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.24920383095741272 norm:0.002201572060585022 max memory_allocated 22562.04345703125 
[2025-03-22 15:56:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.24747903645038605 norm:0.0019848961383104324 max memory_allocated 22562.04345703125 
[2025-03-22 15:56:41 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.24621130526065826 norm:0.0019017148297280073 max memory_allocated 22562.04345703125 
[2025-03-22 15:57:14 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.24527843296527863 norm:0.0018059879075735807 max memory_allocated 22562.04345703125 
[2025-03-22 15:57:46 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.2448519915342331 norm:0.001795207499526441 max memory_allocated 22562.04345703125 
[2025-03-22 15:58:19 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.24455203115940094 norm:0.0017159436829388142 max memory_allocated 22562.04345703125 
[2025-03-22 15:58:51 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.24397686123847961 norm:0.001603185897693038 max memory_allocated 22562.04345703125 
[2025-03-22 15:59:24 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.243757426738739 norm:0.0015888543566688895 max memory_allocated 22562.04345703125 
[2025-03-22 15:59:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.24362120032310486 norm:0.0015287757851183414 max memory_allocated 22562.04345703125 
[2025-03-22 16:00:29 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.24341881275177002 norm:0.0014896400971338153 max memory_allocated 22562.04345703125 
[2025-03-22 16:01:02 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.24322934448719025 norm:0.001476787030696869 max memory_allocated 22562.04345703125 
[2025-03-22 16:01:34 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.2430727630853653 norm:0.001441298401914537 max memory_allocated 22562.04345703125 
[2025-03-22 16:02:07 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.24291135370731354 norm:0.0014168594498187304 max memory_allocated 22562.04345703125 
[2025-03-22 16:02:40 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.242723286151886 norm:0.0014327375683933496 max memory_allocated 22562.04345703125 
[2025-03-22 16:02:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 16:03:24 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.4110874533653259 norm:0.04618611931800842 max memory_allocated 22562.04345703125 
[2025-03-22 16:03:56 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.357088178396225 norm:0.019802549853920937 max memory_allocated 22562.04345703125 
[2025-03-22 16:04:29 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.3041941821575165 norm:0.006558068562299013 max memory_allocated 22562.04345703125 
[2025-03-22 16:05:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.28556495904922485 norm:0.0031328045297414064 max memory_allocated 22562.04345703125 
[2025-03-22 16:05:33 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.2794216573238373 norm:0.002466695848852396 max memory_allocated 22562.04345703125 
[2025-03-22 16:06:06 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.27532336115837097 norm:0.0019960205536335707 max memory_allocated 22562.04345703125 
[2025-03-22 16:06:38 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.27285996079444885 norm:0.0018877596594393253 max memory_allocated 22562.04345703125 
[2025-03-22 16:07:11 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.27111348509788513 norm:0.0017736583249643445 max memory_allocated 22562.04345703125 
[2025-03-22 16:07:43 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.26967838406562805 norm:0.0015857175458222628 max memory_allocated 22562.04345703125 
[2025-03-22 16:08:16 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.26837271451950073 norm:0.0014838584465906024 max memory_allocated 22562.04345703125 
[2025-03-22 16:08:48 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.26775631308555603 norm:0.0013842146145179868 max memory_allocated 22562.04345703125 
[2025-03-22 16:09:21 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.26731136441230774 norm:0.0013547147391363978 max memory_allocated 22562.04345703125 
[2025-03-22 16:09:53 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.267025887966156 norm:0.0013266666792333126 max memory_allocated 22562.04345703125 
[2025-03-22 16:10:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2668516933917999 norm:0.001411973382346332 max memory_allocated 22562.04345703125 
[2025-03-22 16:10:58 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.26656490564346313 norm:0.0013062841026112437 max memory_allocated 22562.04345703125 
[2025-03-22 16:11:31 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.2663749158382416 norm:0.0012760977260768414 max memory_allocated 22562.04345703125 
[2025-03-22 16:12:04 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.26659145951271057 norm:0.0012429091148078442 max memory_allocated 22562.04345703125 
[2025-03-22 16:12:36 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.26646414399147034 norm:0.0011961921118199825 max memory_allocated 22562.04345703125 
[2025-03-22 16:13:09 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.2665711045265198 norm:0.001219131052494049 max memory_allocated 22562.04345703125 
[2025-03-22 16:13:41 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.2663477659225464 norm:0.0011769854463636875 max memory_allocated 22562.04345703125 
[2025-03-22 16:13:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 16:14:26 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.4272458553314209 norm:0.035851672291755676 max memory_allocated 22562.04345703125 
[2025-03-22 16:14:58 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.371990829706192 norm:0.015483486466109753 max memory_allocated 22562.04345703125 
[2025-03-22 16:15:31 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.3227676451206207 norm:0.005621148739010096 max memory_allocated 22562.04345703125 
[2025-03-22 16:16:03 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.3042157292366028 norm:0.002548209158703685 max memory_allocated 22562.04345703125 
[2025-03-22 16:16:36 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.2973012328147888 norm:0.0018403907306492329 max memory_allocated 22562.04345703125 
[2025-03-22 16:17:09 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.29313868284225464 norm:0.0015767768491059542 max memory_allocated 22562.04345703125 
[2025-03-22 16:17:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.29013293981552124 norm:0.0014476116048172116 max memory_allocated 22562.04345703125 
[2025-03-22 16:18:14 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.2883019745349884 norm:0.0014293238054960966 max memory_allocated 22562.04345703125 
[2025-03-22 16:18:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.2870825529098511 norm:0.001396895619109273 max memory_allocated 22562.04345703125 
[2025-03-22 16:19:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.2861393094062805 norm:0.00134373945184052 max memory_allocated 22562.04345703125 
[2025-03-22 16:19:52 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.28518012166023254 norm:0.001297399983741343 max memory_allocated 22562.04345703125 
[2025-03-22 16:20:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.284598708152771 norm:0.0012704806867986917 max memory_allocated 22562.04345703125 
[2025-03-22 16:20:57 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.2840849757194519 norm:0.001158691942691803 max memory_allocated 22562.04345703125 
[2025-03-22 16:21:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.28381362557411194 norm:0.001111004501581192 max memory_allocated 22562.04345703125 
[2025-03-22 16:22:02 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.2835801839828491 norm:0.001099401735700667 max memory_allocated 22562.04345703125 
[2025-03-22 16:22:35 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.2834165692329407 norm:0.0010705889435485005 max memory_allocated 22562.04345703125 
[2025-03-22 16:23:07 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.2834140658378601 norm:0.0010733914095908403 max memory_allocated 22562.04345703125 
[2025-03-22 16:23:40 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.28327128291130066 norm:0.0010447519598528743 max memory_allocated 22562.04345703125 
[2025-03-22 16:24:13 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.2831035256385803 norm:0.0010740762809291482 max memory_allocated 22562.04345703125 
[2025-03-22 16:24:45 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.28290417790412903 norm:0.0010406707879155874 max memory_allocated 22562.04345703125 
[2025-03-22 16:24:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 16:25:29 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.46125972270965576 norm:0.043927643448114395 max memory_allocated 22562.04345703125 
[2025-03-22 16:26:02 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.41274791955947876 norm:0.02170279063284397 max memory_allocated 22562.04345703125 
[2025-03-22 16:26:34 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.34979167580604553 norm:0.0070757376961410046 max memory_allocated 22562.04345703125 
[2025-03-22 16:27:07 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.32557791471481323 norm:0.0029782874044030905 max memory_allocated 22562.04345703125 
[2025-03-22 16:27:39 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.31812578439712524 norm:0.0022981243673712015 max memory_allocated 22562.04345703125 
[2025-03-22 16:28:12 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3139365613460541 norm:0.0019183300901204348 max memory_allocated 22562.04345703125 
[2025-03-22 16:28:44 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.31046074628829956 norm:0.0016560335643589497 max memory_allocated 22562.04345703125 
[2025-03-22 16:29:17 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.3085922300815582 norm:0.001540926517918706 max memory_allocated 22562.04345703125 
[2025-03-22 16:29:49 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.30710506439208984 norm:0.0014340283814817667 max memory_allocated 22562.04345703125 
[2025-03-22 16:30:22 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.30612820386886597 norm:0.0013831969117745757 max memory_allocated 22562.04345703125 
[2025-03-22 16:30:54 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.30495357513427734 norm:0.0012772963382303715 max memory_allocated 22562.04345703125 
[2025-03-22 16:31:27 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.3041840195655823 norm:0.001244185958057642 max memory_allocated 22562.04345703125 
[2025-03-22 16:31:59 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.3037256896495819 norm:0.0011795283062383533 max memory_allocated 22562.04345703125 
[2025-03-22 16:32:32 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.3034855127334595 norm:0.001186260487884283 max memory_allocated 22562.04345703125 
[2025-03-22 16:33:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.3032183349132538 norm:0.0011543112341314554 max memory_allocated 22562.04345703125 
[2025-03-22 16:33:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.30288344621658325 norm:0.0011272599222138524 max memory_allocated 22562.04345703125 
[2025-03-22 16:34:10 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.30266234278678894 norm:0.0010759702417999506 max memory_allocated 22562.04345703125 
[2025-03-22 16:34:42 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.3023356795310974 norm:0.0010791600216180086 max memory_allocated 22562.04345703125 
[2025-03-22 16:35:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.3021763563156128 norm:0.001077474094927311 max memory_allocated 22562.04345703125 
[2025-03-22 16:35:47 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.30204522609710693 norm:0.0010420054895803332 max memory_allocated 22562.04345703125 
[2025-03-22 16:35:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 16:36:32 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.4255445599555969 norm:0.023486550897359848 max memory_allocated 22562.04345703125 
[2025-03-22 16:37:04 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.389328271150589 norm:0.01172717846930027 max memory_allocated 22562.04345703125 
[2025-03-22 16:37:37 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.34889519214630127 norm:0.004056922625750303 max memory_allocated 22562.04345703125 
[2025-03-22 16:38:09 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3340394198894501 norm:0.002002598950639367 max memory_allocated 22562.04345703125 
[2025-03-22 16:38:42 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.3287781774997711 norm:0.0015622882638126612 max memory_allocated 22562.04345703125 
[2025-03-22 16:39:15 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3256702423095703 norm:0.0013972473097965121 max memory_allocated 22562.04345703125 
[2025-03-22 16:39:47 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3234092891216278 norm:0.0012601136695593596 max memory_allocated 22562.04345703125 
[2025-03-22 16:40:20 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.3220396041870117 norm:0.0012181594502180815 max memory_allocated 22562.04345703125 
[2025-03-22 16:40:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.3208836317062378 norm:0.0011672368273139 max memory_allocated 22562.04345703125 
[2025-03-22 16:41:25 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.3200327754020691 norm:0.0011031583417207003 max memory_allocated 22562.04345703125 
[2025-03-22 16:41:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.3192703425884247 norm:0.0010889489203691483 max memory_allocated 22562.04345703125 
[2025-03-22 16:42:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.31852078437805176 norm:0.0010117933852598071 max memory_allocated 22562.04345703125 
[2025-03-22 16:43:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.3178665041923523 norm:0.000954927527345717 max memory_allocated 22562.04345703125 
[2025-03-22 16:43:36 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3176649510860443 norm:0.0009734143386594951 max memory_allocated 22562.04345703125 
[2025-03-22 16:44:08 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3173340857028961 norm:0.0009538912563584745 max memory_allocated 22562.04345703125 
[2025-03-22 16:44:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.31709256768226624 norm:0.0009272681782022119 max memory_allocated 22562.04345703125 
[2025-03-22 16:45:14 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.3166735768318176 norm:0.0009206509566865861 max memory_allocated 22562.04345703125 
[2025-03-22 16:45:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.31665217876434326 norm:0.0008780520875006914 max memory_allocated 22562.04345703125 
[2025-03-22 16:46:19 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.31638872623443604 norm:0.0008578081615269184 max memory_allocated 22562.04345703125 
[2025-03-22 16:46:51 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.31627991795539856 norm:0.0008379943901672959 max memory_allocated 22562.04345703125 
[2025-03-22 16:47:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 16:47:35 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.45214998722076416 norm:0.033977288752794266 max memory_allocated 22562.04345703125 
[2025-03-22 16:48:08 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.4118371307849884 norm:0.016099411994218826 max memory_allocated 22562.04345703125 
[2025-03-22 16:48:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3672366738319397 norm:0.005281570367515087 max memory_allocated 22562.04345703125 
[2025-03-22 16:49:13 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.34990745782852173 norm:0.0025470240507274866 max memory_allocated 22562.04345703125 
[2025-03-22 16:49:45 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.34373098611831665 norm:0.001874649664387107 max memory_allocated 22562.04345703125 
[2025-03-22 16:50:18 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.33966416120529175 norm:0.0015908865025267005 max memory_allocated 22562.04345703125 
[2025-03-22 16:50:50 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.3370402753353119 norm:0.0014666082570329309 max memory_allocated 22562.04345703125 
[2025-03-22 16:51:23 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3350006937980652 norm:0.001349907135590911 max memory_allocated 22562.04345703125 
[2025-03-22 16:51:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.33376091718673706 norm:0.0012921018060296774 max memory_allocated 22562.04345703125 
[2025-03-22 16:52:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.3327766954898834 norm:0.0012025595642626286 max memory_allocated 22562.04345703125 
[2025-03-22 16:53:00 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.332153856754303 norm:0.0011964804725721478 max memory_allocated 22562.04345703125 
[2025-03-22 16:53:33 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.33157411217689514 norm:0.0011406478006392717 max memory_allocated 22562.04345703125 
[2025-03-22 16:54:05 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.33104756474494934 norm:0.0010681883431971073 max memory_allocated 22562.04345703125 
[2025-03-22 16:54:38 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.33058980107307434 norm:0.0010495225433260202 max memory_allocated 22562.04345703125 
[2025-03-22 16:55:10 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3304489254951477 norm:0.0010492911096662283 max memory_allocated 22562.04345703125 
[2025-03-22 16:55:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.33008986711502075 norm:0.0010156973730772734 max memory_allocated 22562.04345703125 
[2025-03-22 16:56:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.3299599289894104 norm:0.0010163786355406046 max memory_allocated 22562.04345703125 
[2025-03-22 16:56:48 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.3297583758831024 norm:0.0010104678804054856 max memory_allocated 22562.04345703125 
[2025-03-22 16:57:21 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.32950615882873535 norm:0.0010016077430918813 max memory_allocated 22562.04345703125 
[2025-03-22 16:57:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.32916688919067383 norm:0.0009453719831071794 max memory_allocated 22562.04345703125 
[2025-03-22 16:58:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 16:58:38 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.44219812750816345 norm:0.028191374614834785 max memory_allocated 22562.04345703125 
[2025-03-22 16:59:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.4182070195674896 norm:0.015025293454527855 max memory_allocated 22562.04345703125 
[2025-03-22 16:59:43 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.38314327597618103 norm:0.006955204531550407 max memory_allocated 22562.04345703125 
[2025-03-22 17:00:15 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.3669048547744751 norm:0.0041282810270786285 max memory_allocated 22562.04345703125 
[2025-03-22 17:00:48 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.3602839708328247 norm:0.003227073699235916 max memory_allocated 22562.04345703125 
[2025-03-22 17:01:21 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.3558770418167114 norm:0.002640733029693365 max memory_allocated 22562.04345703125 
[2025-03-22 17:01:53 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.35236892104148865 norm:0.002172235632315278 max memory_allocated 22562.04345703125 
[2025-03-22 17:02:26 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.3500443994998932 norm:0.0019705554004758596 max memory_allocated 22562.04345703125 
[2025-03-22 17:02:59 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.34849104285240173 norm:0.001811351627111435 max memory_allocated 22562.04345703125 
[2025-03-22 17:03:31 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.3473869562149048 norm:0.0017197325360029936 max memory_allocated 22562.04345703125 
[2025-03-22 17:04:04 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.3464244604110718 norm:0.0015559382736682892 max memory_allocated 22562.04345703125 
[2025-03-22 17:04:37 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.3456905484199524 norm:0.0014689586823806167 max memory_allocated 22562.04345703125 
[2025-03-22 17:05:09 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.3451344668865204 norm:0.0014212740352377295 max memory_allocated 22562.04345703125 
[2025-03-22 17:05:42 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.3444479703903198 norm:0.0012891278602182865 max memory_allocated 22562.04345703125 
[2025-03-22 17:06:14 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.34409695863723755 norm:0.0012652730802074075 max memory_allocated 22562.04345703125 
[2025-03-22 17:06:47 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.34360820055007935 norm:0.0011785265523940325 max memory_allocated 22562.04345703125 
[2025-03-22 17:07:20 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.3432392179965973 norm:0.0011113097425550222 max memory_allocated 22562.04345703125 
[2025-03-22 17:07:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.3428702652454376 norm:0.0010747131891548634 max memory_allocated 22562.04345703125 
[2025-03-22 17:08:25 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.342401385307312 norm:0.0010107698617503047 max memory_allocated 22562.04345703125 
[2025-03-22 17:08:57 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.3420787751674652 norm:0.0009947437793016434 max memory_allocated 22562.04345703125 
[2025-03-22 17:09:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 17:09:42 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.4505341947078705 norm:0.02164006605744362 max memory_allocated 22562.09033203125 
[2025-03-22 17:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.4227234423160553 norm:0.011534563265740871 max memory_allocated 22562.09033203125 
[2025-03-22 17:10:47 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.3891136050224304 norm:0.00458839675411582 max memory_allocated 22562.09033203125 
[2025-03-22 17:11:19 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.3749750256538391 norm:0.0021976428106427193 max memory_allocated 22562.09033203125 
[2025-03-22 17:11:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.36937928199768066 norm:0.001678553642705083 max memory_allocated 22562.09033203125 
[2025-03-22 17:12:24 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3658100664615631 norm:0.0014314660802483559 max memory_allocated 22562.09033203125 
[2025-03-22 17:12:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3635665774345398 norm:0.0013319882564246655 max memory_allocated 22562.09033203125 
[2025-03-22 17:13:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.36195382475852966 norm:0.0012839671690016985 max memory_allocated 22562.09033203125 
[2025-03-22 17:14:01 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.36077773571014404 norm:0.0012502215104177594 max memory_allocated 22562.09033203125 
[2025-03-22 17:14:34 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.35977643728256226 norm:0.0012437582481652498 max memory_allocated 22562.09033203125 
[2025-03-22 17:15:06 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3589555025100708 norm:0.0012149328831583261 max memory_allocated 22562.09033203125 
[2025-03-22 17:15:39 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.35822951793670654 norm:0.0011745679657906294 max memory_allocated 22562.09033203125 
[2025-03-22 17:16:12 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.35766270756721497 norm:0.00116495112888515 max memory_allocated 22562.09033203125 
[2025-03-22 17:16:44 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.35712873935699463 norm:0.0010874334257096052 max memory_allocated 22562.09033203125 
[2025-03-22 17:17:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.35658958554267883 norm:0.001057744724676013 max memory_allocated 22562.09033203125 
[2025-03-22 17:17:49 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3561818599700928 norm:0.0010278647532686591 max memory_allocated 22562.09033203125 
[2025-03-22 17:18:22 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.35567769408226013 norm:0.001002671429887414 max memory_allocated 22562.09033203125 
[2025-03-22 17:18:55 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.35547003149986267 norm:0.000986096216365695 max memory_allocated 22562.09033203125 
[2025-03-22 17:19:27 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.35547760128974915 norm:0.0009746975847519934 max memory_allocated 22562.09033203125 
[2025-03-22 17:20:00 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.3551422953605652 norm:0.0009367655147798359 max memory_allocated 22562.09033203125 
[2025-03-22 17:20:09 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 17:20:45 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.5033963322639465 norm:0.05656319484114647 max memory_allocated 22562.26220703125 
[2025-03-22 17:21:18 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.4693355858325958 norm:0.02908707782626152 max memory_allocated 22562.26220703125 
[2025-03-22 17:21:50 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.4249575138092041 norm:0.012098843231797218 max memory_allocated 22562.26220703125 
[2025-03-22 17:22:23 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.40537452697753906 norm:0.006637383718043566 max memory_allocated 22562.26220703125 
[2025-03-22 17:22:55 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.39808088541030884 norm:0.0052747237496078014 max memory_allocated 22562.26220703125 
[2025-03-22 17:23:28 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3929298520088196 norm:0.004162349738180637 max memory_allocated 22562.26220703125 
[2025-03-22 17:24:01 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.3896358907222748 norm:0.0035998027306050062 max memory_allocated 22562.26220703125 
[2025-03-22 17:24:33 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.3871676027774811 norm:0.0030780122615396976 max memory_allocated 22562.26220703125 
[2025-03-22 17:25:06 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3850628137588501 norm:0.0026560593396425247 max memory_allocated 22562.26220703125 
[2025-03-22 17:25:38 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.383535772562027 norm:0.002319039311259985 max memory_allocated 22562.26220703125 
[2025-03-22 17:26:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.38236814737319946 norm:0.002100402722135186 max memory_allocated 22562.26220703125 
[2025-03-22 17:26:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.381433367729187 norm:0.0019169363658875227 max memory_allocated 22562.26220703125 
[2025-03-22 17:27:16 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.3807353377342224 norm:0.0017772710416465998 max memory_allocated 22562.26220703125 
[2025-03-22 17:27:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.3800835609436035 norm:0.001639467547647655 max memory_allocated 22562.26220703125 
[2025-03-22 17:28:21 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.3796079754829407 norm:0.0015997674781829119 max memory_allocated 22562.26220703125 
[2025-03-22 17:28:53 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.37908101081848145 norm:0.0014577097026631236 max memory_allocated 22562.26220703125 
[2025-03-22 17:29:26 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.3786054849624634 norm:0.0013976373011246324 max memory_allocated 22562.26220703125 
[2025-03-22 17:29:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.37813955545425415 norm:0.0013078245101496577 max memory_allocated 22562.26220703125 
[2025-03-22 17:30:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.3778490126132965 norm:0.0012882869923487306 max memory_allocated 22562.26220703125 
[2025-03-22 17:31:03 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.37751320004463196 norm:0.0012974643614143133 max memory_allocated 22562.26220703125 
[2025-03-22 17:31:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 17:31:47 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.4952181279659271 norm:0.03473789989948273 max memory_allocated 22562.43408203125 
[2025-03-22 17:32:20 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.46826016902923584 norm:0.01656639762222767 max memory_allocated 22562.43408203125 
[2025-03-22 17:32:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.4435712397098541 norm:0.008460716344416142 max memory_allocated 22562.43408203125 
[2025-03-22 17:33:25 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.4265785813331604 norm:0.003644940908998251 max memory_allocated 22562.43408203125 
[2025-03-22 17:33:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.4194152355194092 norm:0.002238301560282707 max memory_allocated 22562.43408203125 
[2025-03-22 17:34:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.41524073481559753 norm:0.0017622446175664663 max memory_allocated 22562.43408203125 
[2025-03-22 17:35:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.41241639852523804 norm:0.001682629925198853 max memory_allocated 22562.43408203125 
[2025-03-22 17:35:35 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.41010987758636475 norm:0.0012862151488661766 max memory_allocated 22562.43408203125 
[2025-03-22 17:36:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.4087202250957489 norm:0.0012170270783826709 max memory_allocated 22562.43408203125 
[2025-03-22 17:36:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.40756309032440186 norm:0.0011558587430045009 max memory_allocated 22562.43408203125 
[2025-03-22 17:37:13 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.40668952465057373 norm:0.00112389016430825 max memory_allocated 22562.43408203125 
[2025-03-22 17:37:46 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.4060639441013336 norm:0.0011005998821929097 max memory_allocated 22562.43408203125 
[2025-03-22 17:38:18 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.4055224657058716 norm:0.0010723505401983857 max memory_allocated 22562.43408203125 
[2025-03-22 17:38:51 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.4052067995071411 norm:0.0010880905902013183 max memory_allocated 22562.43408203125 
[2025-03-22 17:39:24 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.4047701358795166 norm:0.0010702824220061302 max memory_allocated 22562.43408203125 
[2025-03-22 17:39:56 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.4043954312801361 norm:0.0010649958858266473 max memory_allocated 22562.43408203125 
[2025-03-22 17:40:29 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.40401414036750793 norm:0.001032853266224265 max memory_allocated 22562.43408203125 
[2025-03-22 17:41:02 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.403608113527298 norm:0.001036071334965527 max memory_allocated 22562.43408203125 
[2025-03-22 17:41:34 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.40338656306266785 norm:0.001031473046168685 max memory_allocated 22562.43408203125 
[2025-03-22 17:42:07 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.4031746983528137 norm:0.001015576533973217 max memory_allocated 22562.43408203125 
[2025-03-22 17:42:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 17:42:51 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.5646175742149353 norm:0.041186295449733734 max memory_allocated 22562.60595703125 
[2025-03-22 17:43:24 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.5278027653694153 norm:0.0205489881336689 max memory_allocated 22562.60595703125 
[2025-03-22 17:43:56 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.4868347942829132 norm:0.009481272660195827 max memory_allocated 22562.60595703125 
[2025-03-22 17:44:29 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.46674519777297974 norm:0.004998482763767242 max memory_allocated 22562.60595703125 
[2025-03-22 17:45:01 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.45899951457977295 norm:0.003246406791731715 max memory_allocated 22562.60595703125 
[2025-03-22 17:45:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.45444363355636597 norm:0.0026055367197841406 max memory_allocated 22562.60595703125 
[2025-03-22 17:46:06 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.45114150643348694 norm:0.002160155912861228 max memory_allocated 22562.60595703125 
[2025-03-22 17:46:38 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.4488069415092468 norm:0.0019838653970509768 max memory_allocated 22562.60595703125 
[2025-03-22 17:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.4469565749168396 norm:0.0018035920802503824 max memory_allocated 22562.60595703125 
[2025-03-22 17:47:43 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.4456080198287964 norm:0.001690241857431829 max memory_allocated 22562.60595703125 
[2025-03-22 17:48:16 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.44452497363090515 norm:0.0016153395408764482 max memory_allocated 22562.60595703125 
[2025-03-22 17:48:48 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.44366684556007385 norm:0.001554482034407556 max memory_allocated 22562.60595703125 
[2025-03-22 17:49:20 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.4429192543029785 norm:0.0015206540701910853 max memory_allocated 22562.60595703125 
[2025-03-22 17:49:53 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.44185012578964233 norm:0.0012415804667398334 max memory_allocated 22562.60595703125 
[2025-03-22 17:50:25 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.4414516091346741 norm:0.0012334195198491216 max memory_allocated 22562.60595703125 
[2025-03-22 17:50:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.44099918007850647 norm:0.0012058892752975225 max memory_allocated 22562.60595703125 
[2025-03-22 17:51:31 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.4405622184276581 norm:0.001223583472892642 max memory_allocated 22562.60595703125 
[2025-03-22 17:52:03 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.44022336602211 norm:0.0012123340275138617 max memory_allocated 22562.60595703125 
[2025-03-22 17:52:36 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.43989354372024536 norm:0.001199247082695365 max memory_allocated 22562.60595703125 
[2025-03-22 17:53:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.4395115375518799 norm:0.0011534304358065128 max memory_allocated 22562.60595703125 
[2025-03-22 17:53:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 17:53:53 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.5958123803138733 norm:0.03743801638484001 max memory_allocated 22562.77783203125 
[2025-03-22 17:54:25 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.565707802772522 norm:0.019190555438399315 max memory_allocated 22562.77783203125 
[2025-03-22 17:54:58 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.5334620475769043 norm:0.00857757031917572 max memory_allocated 22562.77783203125 
[2025-03-22 17:55:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.5151408314704895 norm:0.0037074030842632055 max memory_allocated 22562.77783203125 
[2025-03-22 17:56:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.5076319575309753 norm:0.0023659192956984043 max memory_allocated 22562.77783203125 
[2025-03-22 17:56:35 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.5035346746444702 norm:0.0019462067866697907 max memory_allocated 22562.77783203125 
[2025-03-22 17:57:08 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.5006531476974487 norm:0.0017776370514184237 max memory_allocated 22562.77783203125 
[2025-03-22 17:57:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.498710572719574 norm:0.0016951863653957844 max memory_allocated 22562.77783203125 
[2025-03-22 17:58:13 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.4970141351222992 norm:0.001610723789781332 max memory_allocated 22562.77783203125 
[2025-03-22 17:58:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.49581798911094666 norm:0.0015874613309279084 max memory_allocated 22562.77783203125 
[2025-03-22 17:59:19 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.49485403299331665 norm:0.0015210205456241965 max memory_allocated 22562.77783203125 
[2025-03-22 17:59:51 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.4940122961997986 norm:0.0014621262671425939 max memory_allocated 22562.77783203125 
[2025-03-22 18:00:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.4929967224597931 norm:0.0013994164764881134 max memory_allocated 22562.77783203125 
[2025-03-22 18:00:56 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.4922487437725067 norm:0.00133464892860502 max memory_allocated 22562.77783203125 
[2025-03-22 18:01:29 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.4915693402290344 norm:0.001281781936995685 max memory_allocated 22562.77783203125 
[2025-03-22 18:02:01 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.4910871088504791 norm:0.001270001521334052 max memory_allocated 22562.77783203125 
[2025-03-22 18:02:34 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.49074867367744446 norm:0.0012756551150232553 max memory_allocated 22562.77783203125 
[2025-03-22 18:03:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.490471750497818 norm:0.0012832123320549726 max memory_allocated 22562.77783203125 
[2025-03-22 18:03:39 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.49005448818206787 norm:0.0012404619483277202 max memory_allocated 22562.77783203125 
[2025-03-22 18:04:11 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.4896768629550934 norm:0.0012077776482328773 max memory_allocated 22562.77783203125 
[2025-03-22 18:04:20 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 18:04:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.6531287431716919 norm:0.0376407764852047 max memory_allocated 22562.94970703125 
[2025-03-22 18:05:28 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.6255865097045898 norm:0.01899001933634281 max memory_allocated 22562.94970703125 
[2025-03-22 18:06:00 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.5929149985313416 norm:0.00849518459290266 max memory_allocated 22562.94970703125 
[2025-03-22 18:06:33 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.5773531794548035 norm:0.004556944128125906 max memory_allocated 22562.94970703125 
[2025-03-22 18:07:05 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.5710322856903076 norm:0.003271058900281787 max memory_allocated 22562.94970703125 
[2025-03-22 18:07:38 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.5672953724861145 norm:0.0027912326622754335 max memory_allocated 22562.94970703125 
[2025-03-22 18:08:10 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.5648972392082214 norm:0.0026321550831198692 max memory_allocated 22562.94970703125 
[2025-03-22 18:08:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.5629518032073975 norm:0.0023995735682547092 max memory_allocated 22562.94970703125 
[2025-03-22 18:09:15 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.5614995956420898 norm:0.0022469963878393173 max memory_allocated 22562.94970703125 
[2025-03-22 18:09:48 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.5603582859039307 norm:0.0020451198797672987 max memory_allocated 22562.94970703125 
[2025-03-22 18:10:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.559116005897522 norm:0.0017002560198307037 max memory_allocated 22562.94970703125 
[2025-03-22 18:10:53 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.5579352378845215 norm:0.001206228625960648 max memory_allocated 22562.94970703125 
[2025-03-22 18:11:25 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.5573834180831909 norm:0.0011940950062125921 max memory_allocated 22562.94970703125 
[2025-03-22 18:11:58 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.556818962097168 norm:0.0011714146239683032 max memory_allocated 22562.94970703125 
[2025-03-22 18:12:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.5565488338470459 norm:0.0011939797550439835 max memory_allocated 22562.94970703125 
[2025-03-22 18:13:03 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.5562315583229065 norm:0.001175313489511609 max memory_allocated 22562.94970703125 
[2025-03-22 18:13:36 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.5559926629066467 norm:0.0011805135291069746 max memory_allocated 22562.94970703125 
[2025-03-22 18:14:09 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.5556215047836304 norm:0.001176929916255176 max memory_allocated 22562.94970703125 
[2025-03-22 18:14:41 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.5551523566246033 norm:0.0011632173554971814 max memory_allocated 22562.94970703125 
[2025-03-22 18:15:14 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.5549280643463135 norm:0.0011457018554210663 max memory_allocated 22562.94970703125 
[2025-03-22 18:15:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 18:15:58 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.7199661135673523 norm:0.02771134115755558 max memory_allocated 22563.12158203125 
[2025-03-22 18:16:31 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.6971675753593445 norm:0.015353839844465256 max memory_allocated 22563.12158203125 
[2025-03-22 18:17:03 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.6741523742675781 norm:0.008763889782130718 max memory_allocated 22563.12158203125 
[2025-03-22 18:17:36 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.6615636944770813 norm:0.00578156067058444 max memory_allocated 22563.12158203125 
[2025-03-22 18:18:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.6560827493667603 norm:0.004474423360079527 max memory_allocated 22563.12158203125 
[2025-03-22 18:18:41 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.652509868144989 norm:0.0036197758745402098 max memory_allocated 22563.12158203125 
[2025-03-22 18:19:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.6497623324394226 norm:0.0030453968793153763 max memory_allocated 22563.12158203125 
[2025-03-22 18:19:45 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.6476603746414185 norm:0.0026223950553685427 max memory_allocated 22563.12158203125 
[2025-03-22 18:20:18 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.645842432975769 norm:0.0022466289810836315 max memory_allocated 22563.12158203125 
[2025-03-22 18:20:50 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.6445047855377197 norm:0.0020411107689142227 max memory_allocated 22563.12158203125 
[2025-03-22 18:21:23 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.6432697176933289 norm:0.0018204712541773915 max memory_allocated 22563.12158203125 
[2025-03-22 18:21:55 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.6421103477478027 norm:0.0015668943524360657 max memory_allocated 22563.12158203125 
[2025-03-22 18:22:28 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.6408554911613464 norm:0.0011708621168509126 max memory_allocated 22563.12158203125 
[2025-03-22 18:23:00 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.6404288411140442 norm:0.0011897383956238627 max memory_allocated 22563.12158203125 
[2025-03-22 18:23:32 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.6399084329605103 norm:0.001151902019046247 max memory_allocated 22563.12158203125 
[2025-03-22 18:24:05 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.6393945813179016 norm:0.0011556701501831412 max memory_allocated 22563.12158203125 
[2025-03-22 18:24:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.6390140056610107 norm:0.0011747076641768217 max memory_allocated 22563.12158203125 
[2025-03-22 18:25:10 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.6386222839355469 norm:0.0011624330654740334 max memory_allocated 22563.12158203125 
[2025-03-22 18:25:42 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.6383407115936279 norm:0.001144508016295731 max memory_allocated 22563.12158203125 
[2025-03-22 18:26:15 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.6381573677062988 norm:0.0011356251779943705 max memory_allocated 22563.12158203125 
[2025-03-22 18:26:24 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 18:26:59 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.8467007875442505 norm:0.029740577563643456 max memory_allocated 22563.29345703125 
[2025-03-22 18:27:32 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.8223905563354492 norm:0.016802914440631866 max memory_allocated 22563.29345703125 
[2025-03-22 18:28:04 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.7934348583221436 norm:0.009235743433237076 max memory_allocated 22563.29345703125 
[2025-03-22 18:28:37 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.7775568962097168 norm:0.005645254161208868 max memory_allocated 22563.29345703125 
[2025-03-22 18:29:09 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.7714811563491821 norm:0.004371413495391607 max memory_allocated 22563.29345703125 
[2025-03-22 18:29:42 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.7674685120582581 norm:0.00370239932090044 max memory_allocated 22563.29345703125 
[2025-03-22 18:30:15 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.7645933032035828 norm:0.0031843609176576138 max memory_allocated 22563.29345703125 
[2025-03-22 18:30:47 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.7629079222679138 norm:0.0029482953250408173 max memory_allocated 22563.29345703125 
[2025-03-22 18:31:20 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.7606467008590698 norm:0.0016855821013450623 max memory_allocated 22563.29345703125 
[2025-03-22 18:31:52 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.758919358253479 norm:0.0014342598151415586 max memory_allocated 22563.29345703125 
[2025-03-22 18:32:25 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.7578343152999878 norm:0.001446535112336278 max memory_allocated 22563.29345703125 
[2025-03-22 18:32:58 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.7573133111000061 norm:0.0014704179484397173 max memory_allocated 22563.29345703125 
[2025-03-22 18:33:30 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.7566899061203003 norm:0.001441270112991333 max memory_allocated 22563.29345703125 
[2025-03-22 18:34:03 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.7563489079475403 norm:0.0014590260107070208 max memory_allocated 22563.29345703125 
[2025-03-22 18:34:35 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.7557914853096008 norm:0.001371618709526956 max memory_allocated 22563.29345703125 
[2025-03-22 18:35:08 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.7552608847618103 norm:0.001357448287308216 max memory_allocated 22563.29345703125 
[2025-03-22 18:35:40 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.7550635933876038 norm:0.0013837430160492659 max memory_allocated 22563.29345703125 
[2025-03-22 18:36:13 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.7547789812088013 norm:0.0013592898612841964 max memory_allocated 22563.29345703125 
[2025-03-22 18:36:45 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.754714846611023 norm:0.0013467483222484589 max memory_allocated 22563.29345703125 
[2025-03-22 18:37:17 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.7545647621154785 norm:0.0013393450062721968 max memory_allocated 22563.29345703125 
[2025-03-22 18:37:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 18:38:01 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.9587318897247314 norm:0.012650476768612862 max memory_allocated 22563.46533203125 
[2025-03-22 18:38:34 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.9460724592208862 norm:0.008292053826153278 max memory_allocated 22563.46533203125 
[2025-03-22 18:39:06 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.9237337112426758 norm:0.004185931291431189 max memory_allocated 22563.46533203125 
[2025-03-22 18:39:39 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.9101755023002625 norm:0.0027203778736293316 max memory_allocated 22563.46533203125 
[2025-03-22 18:40:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.9043864011764526 norm:0.002061201725155115 max memory_allocated 22563.46533203125 
[2025-03-22 18:40:44 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.9011328816413879 norm:0.001876226393505931 max memory_allocated 22563.46533203125 
[2025-03-22 18:41:16 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.898766040802002 norm:0.0018043620511889458 max memory_allocated 22563.46533203125 
[2025-03-22 18:41:49 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.8971617817878723 norm:0.001782887033186853 max memory_allocated 22563.46533203125 
[2025-03-22 18:42:21 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.8959864377975464 norm:0.0017679909942671657 max memory_allocated 22563.46533203125 
[2025-03-22 18:42:54 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.8948979377746582 norm:0.001735930796712637 max memory_allocated 22563.46533203125 
[2025-03-22 18:43:26 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.8941727876663208 norm:0.0017351899296045303 max memory_allocated 22563.46533203125 
[2025-03-22 18:43:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.893757164478302 norm:0.0017304873326793313 max memory_allocated 22563.46533203125 
[2025-03-22 18:44:31 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.8933305740356445 norm:0.0017059281235560775 max memory_allocated 22563.46533203125 
[2025-03-22 18:45:04 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.8926231265068054 norm:0.001662190305069089 max memory_allocated 22563.46533203125 
[2025-03-22 18:45:37 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.8922533988952637 norm:0.0016448673559352756 max memory_allocated 22563.46533203125 
[2025-03-22 18:46:09 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.89178067445755 norm:0.0016095290193334222 max memory_allocated 22563.46533203125 
[2025-03-22 18:46:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.8913657665252686 norm:0.0015620404155924916 max memory_allocated 22563.46533203125 
[2025-03-22 18:47:14 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.8912991881370544 norm:0.0015666403342038393 max memory_allocated 22563.46533203125 
[2025-03-22 18:47:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.8908097743988037 norm:0.0015485993353649974 max memory_allocated 22563.46533203125 
[2025-03-22 18:48:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.8905609250068665 norm:0.0015114633133634925 max memory_allocated 22563.46533203125 
[2025-03-22 18:48:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 18:49:04 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:1.0979177951812744 norm:0.012644880451261997 max memory_allocated 22563.63720703125 
[2025-03-22 18:49:37 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:1.0775469541549683 norm:0.007404865697026253 max memory_allocated 22563.63720703125 
[2025-03-22 18:50:09 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:1.0545170307159424 norm:0.003949235193431377 max memory_allocated 22563.63720703125 
[2025-03-22 18:50:42 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:1.0453671216964722 norm:0.0027034820523113012 max memory_allocated 22563.63720703125 
[2025-03-22 18:51:14 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:1.0404775142669678 norm:0.00203472631983459 max memory_allocated 22563.63720703125 
[2025-03-22 18:51:47 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:1.0370618104934692 norm:0.0017852052114903927 max memory_allocated 22563.63720703125 
[2025-03-22 18:52:19 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:1.0343225002288818 norm:0.0015825226437300444 max memory_allocated 22563.63720703125 
[2025-03-22 18:52:52 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:1.0323714017868042 norm:0.0013904481893405318 max memory_allocated 22563.63720703125 
[2025-03-22 18:53:25 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:1.031125545501709 norm:0.0013606963912025094 max memory_allocated 22563.63720703125 
[2025-03-22 18:53:57 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:1.0302150249481201 norm:0.0013298280537128448 max memory_allocated 22563.63720703125 
[2025-03-22 18:54:30 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:1.0294363498687744 norm:0.0013130633160471916 max memory_allocated 22563.63720703125 
[2025-03-22 18:55:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:1.0291059017181396 norm:0.0013010394759476185 max memory_allocated 22563.63720703125 
[2025-03-22 18:55:35 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:1.0287755727767944 norm:0.001283815479837358 max memory_allocated 22563.63720703125 
[2025-03-22 18:56:07 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:1.02847158908844 norm:0.0012769183376803994 max memory_allocated 22563.63720703125 
[2025-03-22 18:56:40 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:1.0282591581344604 norm:0.0012691577430814505 max memory_allocated 22563.63720703125 
[2025-03-22 18:57:12 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:1.0281424522399902 norm:0.0012455274118110538 max memory_allocated 22563.63720703125 
[2025-03-22 18:57:45 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:1.0278477668762207 norm:0.0012392911594361067 max memory_allocated 22563.63720703125 
[2025-03-22 18:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:1.0275665521621704 norm:0.0012291248422116041 max memory_allocated 22563.63720703125 
[2025-03-22 18:58:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:1.027543544769287 norm:0.0012308708392083645 max memory_allocated 22563.63720703125 
[2025-03-22 18:59:22 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:1.0275405645370483 norm:0.0012399419210851192 max memory_allocated 22563.63720703125 
[2025-03-22 18:59:31 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 19:00:06 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:1.283911108970642 norm:0.02140023745596409 max memory_allocated 22563.80908203125 
[2025-03-22 19:00:39 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:1.2606630325317383 norm:0.013910080306231976 max memory_allocated 22563.80908203125 
[2025-03-22 19:01:11 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:1.2316563129425049 norm:0.007512453477829695 max memory_allocated 22563.80908203125 
[2025-03-22 19:01:43 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:1.2171844244003296 norm:0.004315848462283611 max memory_allocated 22563.80908203125 
[2025-03-22 19:02:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:1.210493564605713 norm:0.00332073075696826 max memory_allocated 22563.80908203125 
[2025-03-22 19:02:48 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:1.2055851221084595 norm:0.002507953206077218 max memory_allocated 22563.80908203125 
[2025-03-22 19:03:21 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:1.201973795890808 norm:0.0020283630583435297 max memory_allocated 22563.80908203125 
[2025-03-22 19:03:54 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:1.1995517015457153 norm:0.001817760756239295 max memory_allocated 22563.80908203125 
[2025-03-22 19:04:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:1.1975440979003906 norm:0.0014958946267142892 max memory_allocated 22563.80908203125 
[2025-03-22 19:04:59 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:1.1963481903076172 norm:0.0014603284653276205 max memory_allocated 22563.80908203125 
[2025-03-22 19:05:31 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:1.195488452911377 norm:0.0014503981219604611 max memory_allocated 22563.80908203125 
[2025-03-22 19:06:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:1.1948449611663818 norm:0.001426247414201498 max memory_allocated 22563.80908203125 
[2025-03-22 19:06:36 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:1.1946194171905518 norm:0.0014317788882181048 max memory_allocated 22563.80908203125 
[2025-03-22 19:07:09 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:1.194322109222412 norm:0.0014238512376323342 max memory_allocated 22563.80908203125 
[2025-03-22 19:07:42 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:1.1940233707427979 norm:0.0014056299114599824 max memory_allocated 22563.80908203125 
[2025-03-22 19:08:14 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:1.193600058555603 norm:0.0013894208241254091 max memory_allocated 22563.80908203125 
[2025-03-22 19:08:47 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:1.1935076713562012 norm:0.0013871727278456092 max memory_allocated 22563.80908203125 
[2025-03-22 19:09:19 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:1.1934385299682617 norm:0.0013882821658626199 max memory_allocated 22563.80908203125 
[2025-03-22 19:09:52 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:1.1931841373443604 norm:0.001375339343212545 max memory_allocated 22563.80908203125 
[2025-03-22 19:10:25 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:1.192906141281128 norm:0.0013719467679038644 max memory_allocated 22563.80908203125 
[2025-03-22 19:10:34 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 19:11:09 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:1.4469809532165527 norm:0.030640801414847374 max memory_allocated 22563.98095703125 
[2025-03-22 19:11:42 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:1.4270063638687134 norm:0.0209885835647583 max memory_allocated 22563.98095703125 
[2025-03-22 19:12:14 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:1.3998134136199951 norm:0.013842343352735043 max memory_allocated 22563.98095703125 
[2025-03-22 19:12:47 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:1.3863292932510376 norm:0.010145770385861397 max memory_allocated 22563.98095703125 
[2025-03-22 19:13:19 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:1.3780021667480469 norm:0.00753678847104311 max memory_allocated 22563.98095703125 
[2025-03-22 19:13:52 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:1.3719482421875 norm:0.006072103977203369 max memory_allocated 22563.98095703125 
[2025-03-22 19:14:24 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:1.3681371212005615 norm:0.005270080640912056 max memory_allocated 22563.98095703125 
[2025-03-22 19:14:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:1.3656904697418213 norm:0.004691626410931349 max memory_allocated 22563.98095703125 
[2025-03-22 19:15:29 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:1.3639328479766846 norm:0.00423963088542223 max memory_allocated 22563.98095703125 
[2025-03-22 19:16:02 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:1.3624893426895142 norm:0.0037836770061403513 max memory_allocated 22563.98095703125 
[2025-03-22 19:16:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:1.3613274097442627 norm:0.0034171428997069597 max memory_allocated 22563.98095703125 
[2025-03-22 19:17:07 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:1.3602378368377686 norm:0.003147832816466689 max memory_allocated 22563.98095703125 
[2025-03-22 19:17:39 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:1.3593013286590576 norm:0.0029005995020270348 max memory_allocated 22563.98095703125 
[2025-03-22 19:18:12 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:1.3583526611328125 norm:0.0026754895225167274 max memory_allocated 22563.98095703125 
[2025-03-22 19:18:44 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:1.3577312231063843 norm:0.0024741091765463352 max memory_allocated 22563.98095703125 
[2025-03-22 19:19:17 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:1.3568452596664429 norm:0.002175775822252035 max memory_allocated 22563.98095703125 
[2025-03-22 19:19:49 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:1.3562432527542114 norm:0.0020363260991871357 max memory_allocated 22563.98095703125 
[2025-03-22 19:20:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:1.3560669422149658 norm:0.0019574847538024187 max memory_allocated 22563.98095703125 
[2025-03-22 19:20:54 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:1.3558037281036377 norm:0.0018782437546178699 max memory_allocated 22563.98095703125 
[2025-03-22 19:21:27 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:1.3555089235305786 norm:0.0018114580307155848 max memory_allocated 22563.98095703125 
[2025-03-22 19:21:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 19:22:11 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:1.6197035312652588 norm:0.009929953143000603 max memory_allocated 22564.15283203125 
[2025-03-22 19:22:43 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:1.6003235578536987 norm:0.005892063025385141 max memory_allocated 22564.15283203125 
[2025-03-22 19:23:16 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:1.5739800930023193 norm:0.003006712533533573 max memory_allocated 22564.15283203125 
[2025-03-22 19:23:48 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:1.5629193782806396 norm:0.0018424906302243471 max memory_allocated 22564.15283203125 
[2025-03-22 19:24:21 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:1.5574066638946533 norm:0.00156135275028646 max memory_allocated 22564.15283203125 
[2025-03-22 19:24:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:1.5531949996948242 norm:0.0014199558645486832 max memory_allocated 22564.15283203125 
[2025-03-22 19:25:26 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:1.5501623153686523 norm:0.001363203627988696 max memory_allocated 22564.15283203125 
[2025-03-22 19:25:59 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:1.5481359958648682 norm:0.0013215363724157214 max memory_allocated 22564.15283203125 
[2025-03-22 19:26:31 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:1.546652913093567 norm:0.0012950730742886662 max memory_allocated 22564.15283203125 
[2025-03-22 19:27:04 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:1.5455149412155151 norm:0.0012781398836523294 max memory_allocated 22564.15283203125 
[2025-03-22 19:27:37 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:1.5446727275848389 norm:0.0012574769789353013 max memory_allocated 22564.15283203125 
[2025-03-22 19:28:09 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:1.5440142154693604 norm:0.0012434201780706644 max memory_allocated 22564.15283203125 
[2025-03-22 19:28:42 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:1.543430209159851 norm:0.001238246913999319 max memory_allocated 22564.15283203125 
[2025-03-22 19:29:14 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:1.5428978204727173 norm:0.0012305359123274684 max memory_allocated 22564.15283203125 
[2025-03-22 19:29:47 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:1.54245126247406 norm:0.0012272866442799568 max memory_allocated 22564.15283203125 
[2025-03-22 19:30:20 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:1.542054533958435 norm:0.001215994474478066 max memory_allocated 22564.15283203125 
[2025-03-22 19:30:52 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:1.541770100593567 norm:0.0012114583514630795 max memory_allocated 22564.15283203125 
[2025-03-22 19:31:25 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:1.5414668321609497 norm:0.0012121973559260368 max memory_allocated 22564.15283203125 
[2025-03-22 19:31:58 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:1.5412169694900513 norm:0.0012084969785064459 max memory_allocated 22564.15283203125 
[2025-03-22 19:32:30 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:1.5410172939300537 norm:0.0012067292118445039 max memory_allocated 22564.15283203125 
[2025-03-22 19:32:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 19:33:14 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:1.8411500453948975 norm:0.027495084330439568 max memory_allocated 22564.32470703125 
[2025-03-22 19:33:47 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:1.8104435205459595 norm:0.013967698439955711 max memory_allocated 22564.32470703125 
[2025-03-22 19:34:19 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:1.7795524597167969 norm:0.006734536495059729 max memory_allocated 22564.32470703125 
[2025-03-22 19:34:52 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:1.765023946762085 norm:0.003774241078644991 max memory_allocated 22564.32470703125 
[2025-03-22 19:35:24 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:1.7572665214538574 norm:0.002929084934294224 max memory_allocated 22564.32470703125 
[2025-03-22 19:35:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:1.751448392868042 norm:0.0023563543800264597 max memory_allocated 22564.32470703125 
[2025-03-22 19:36:29 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:1.7474204301834106 norm:0.0019975053146481514 max memory_allocated 22564.32470703125 
[2025-03-22 19:37:01 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:1.7447175979614258 norm:0.0017661498859524727 max memory_allocated 22564.32470703125 
[2025-03-22 19:37:34 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:1.7427548170089722 norm:0.0016052311984822154 max memory_allocated 22564.32470703125 
[2025-03-22 19:38:06 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:1.7414544820785522 norm:0.0015037598786875606 max memory_allocated 22564.32470703125 
[2025-03-22 19:38:39 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:1.7404286861419678 norm:0.0014216741546988487 max memory_allocated 22564.32470703125 
[2025-03-22 19:39:11 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:1.73965322971344 norm:0.0013650560285896063 max memory_allocated 22564.32470703125 
[2025-03-22 19:39:44 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:1.7390060424804688 norm:0.0013279588893055916 max memory_allocated 22564.32470703125 
[2025-03-22 19:40:16 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:1.738329291343689 norm:0.0012941801687702537 max memory_allocated 22564.32470703125 
[2025-03-22 19:40:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:1.7376610040664673 norm:0.001269486965611577 max memory_allocated 22564.32470703125 
[2025-03-22 19:41:21 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:1.7372883558273315 norm:0.001249500666745007 max memory_allocated 22564.32470703125 
[2025-03-22 19:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:1.7369887828826904 norm:0.0012415905948728323 max memory_allocated 22564.32470703125 
[2025-03-22 19:42:27 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:1.736770749092102 norm:0.0012339986860752106 max memory_allocated 22564.32470703125 
[2025-03-22 19:42:59 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:1.7366163730621338 norm:0.0012236718321219087 max memory_allocated 22564.32470703125 
[2025-03-22 19:43:32 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:1.7364251613616943 norm:0.001215086318552494 max memory_allocated 22564.32470703125 
[2025-03-22 19:43:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 19:44:16 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:2.048704147338867 norm:0.023277975618839264 max memory_allocated 22564.49658203125 
[2025-03-22 19:44:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:2.0142757892608643 norm:0.01056029461324215 max memory_allocated 22564.49658203125 
[2025-03-22 19:45:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:1.9826831817626953 norm:0.006392213050276041 max memory_allocated 22564.49658203125 
[2025-03-22 19:45:54 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:1.9648956060409546 norm:0.0027950427029281855 max memory_allocated 22564.49658203125 
[2025-03-22 19:46:26 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:1.9576613903045654 norm:0.002176400041207671 max memory_allocated 22564.49658203125 
[2025-03-22 19:46:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:1.9522700309753418 norm:0.0019209935562685132 max memory_allocated 22564.49658203125 
[2025-03-22 19:47:32 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:1.9486501216888428 norm:0.001749789691530168 max memory_allocated 22564.49658203125 
[2025-03-22 19:48:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:1.9462584257125854 norm:0.0016043736832216382 max memory_allocated 22564.49658203125 
[2025-03-22 19:48:37 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:1.9447332620620728 norm:0.001571384258568287 max memory_allocated 22564.49658203125 
[2025-03-22 19:49:09 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:1.9437302350997925 norm:0.0015363791026175022 max memory_allocated 22564.49658203125 
[2025-03-22 19:49:42 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:1.9427275657653809 norm:0.0015267417766153812 max memory_allocated 22564.49658203125 
[2025-03-22 19:50:14 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:1.941824197769165 norm:0.0014435574412345886 max memory_allocated 22564.49658203125 
[2025-03-22 19:50:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:1.941123366355896 norm:0.0014538782415911555 max memory_allocated 22564.49658203125 
[2025-03-22 19:51:19 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:1.9407660961151123 norm:0.0014134389348328114 max memory_allocated 22564.49658203125 
[2025-03-22 19:51:52 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:1.940166711807251 norm:0.0013589368900284171 max memory_allocated 22564.49658203125 
[2025-03-22 19:52:24 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:1.9397395849227905 norm:0.0013426110381260514 max memory_allocated 22564.49658203125 
[2025-03-22 19:52:57 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:1.939100742340088 norm:0.001334276283159852 max memory_allocated 22564.49658203125 
[2025-03-22 19:53:29 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:1.93905508518219 norm:0.0013388125225901604 max memory_allocated 22564.49658203125 
[2025-03-22 19:54:02 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:1.938759446144104 norm:0.0013429918326437473 max memory_allocated 22564.49658203125 
[2025-03-22 19:54:34 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:1.9376671314239502 norm:0.0015706248814240098 max memory_allocated 22564.49658203125 
[2025-03-22 19:54:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 19:54:46 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 19:55:18 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:2.3817827701568604 norm:0.10163189470767975 max memory_allocated 22566.51220703125 
[2025-03-22 19:55:51 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:2.3412132263183594 norm:0.08615634590387344 max memory_allocated 22566.51220703125 
[2025-03-22 19:56:24 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:2.304030656814575 norm:0.06925294548273087 max memory_allocated 22566.51220703125 
[2025-03-22 19:56:56 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:2.2807457447052 norm:0.05478856712579727 max memory_allocated 22566.51220703125 
[2025-03-22 19:57:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:2.268136501312256 norm:0.04458578675985336 max memory_allocated 22566.51220703125 
[2025-03-22 19:58:02 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:2.257976770401001 norm:0.03703287988901138 max memory_allocated 22566.51220703125 
[2025-03-22 19:58:34 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:2.2484118938446045 norm:0.03023068793118 max memory_allocated 22566.51220703125 
[2025-03-22 19:59:07 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:2.241489887237549 norm:0.025859178975224495 max memory_allocated 22566.51220703125 
[2025-03-22 19:59:40 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:2.2362613677978516 norm:0.023131143301725388 max memory_allocated 22566.51220703125 
[2025-03-22 20:00:13 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:2.2325310707092285 norm:0.021177031099796295 max memory_allocated 22566.51220703125 
[2025-03-22 20:00:45 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:2.228792190551758 norm:0.021481983363628387 max memory_allocated 22566.51220703125 
[2025-03-22 20:01:18 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:2.2262954711914062 norm:0.016581205651164055 max memory_allocated 22566.51220703125 
[2025-03-22 20:01:51 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:2.226874828338623 norm:0.01975853368639946 max memory_allocated 22566.51220703125 
[2025-03-22 20:02:24 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:2.2233963012695312 norm:0.015845034271478653 max memory_allocated 22566.51220703125 
[2025-03-22 20:02:57 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:2.2182974815368652 norm:0.016033660620450974 max memory_allocated 22566.51220703125 
[2025-03-22 20:03:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:2.2144408226013184 norm:0.014624972827732563 max memory_allocated 22566.51220703125 
[2025-03-22 20:04:02 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:2.212815046310425 norm:0.014876997098326683 max memory_allocated 22566.51220703125 
[2025-03-22 20:04:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:2.212211847305298 norm:0.015138749964535236 max memory_allocated 22566.51220703125 
[2025-03-22 20:05:08 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:2.2127695083618164 norm:0.01608465611934662 max memory_allocated 22566.51220703125 
[2025-03-22 20:05:40 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:2.2166736125946045 norm:0.016624709591269493 max memory_allocated 22566.51220703125 
[2025-03-22 20:05:49 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 20:05:52 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:06:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:2.7227227687835693 norm:0.11526782065629959 max memory_allocated 22566.68408203125 
[2025-03-22 20:06:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:2.666712760925293 norm:0.10208578407764435 max memory_allocated 22566.68408203125 
[2025-03-22 20:07:30 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:2.6130285263061523 norm:0.07877194881439209 max memory_allocated 22566.68408203125 
[2025-03-22 20:08:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:2.5806751251220703 norm:0.06146091967821121 max memory_allocated 22566.68408203125 
[2025-03-22 20:08:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:2.5654423236846924 norm:0.04849480837583542 max memory_allocated 22566.68408203125 
[2025-03-22 20:09:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:2.5540850162506104 norm:0.04064439237117767 max memory_allocated 22566.68408203125 
[2025-03-22 20:09:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:2.5454254150390625 norm:0.03374425321817398 max memory_allocated 22566.68408203125 
[2025-03-22 20:10:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:2.537426710128784 norm:0.028133505955338478 max memory_allocated 22566.68408203125 
[2025-03-22 20:10:45 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:2.5322701930999756 norm:0.024391382932662964 max memory_allocated 22566.68408203125 
[2025-03-22 20:11:18 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:2.5290613174438477 norm:0.022750146687030792 max memory_allocated 22566.68408203125 
[2025-03-22 20:11:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:2.526548385620117 norm:0.020948156714439392 max memory_allocated 22566.68408203125 
[2025-03-22 20:12:23 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:2.5237436294555664 norm:0.02046346664428711 max memory_allocated 22566.68408203125 
[2025-03-22 20:12:56 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:2.5227742195129395 norm:0.0185179952532053 max memory_allocated 22566.68408203125 
[2025-03-22 20:13:29 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:2.5223217010498047 norm:0.01939370669424534 max memory_allocated 22566.68408203125 
[2025-03-22 20:14:01 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:2.5202717781066895 norm:0.01691330410540104 max memory_allocated 22566.68408203125 
[2025-03-22 20:14:34 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:2.5187735557556152 norm:0.01708865351974964 max memory_allocated 22566.68408203125 
[2025-03-22 20:15:07 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:2.521566867828369 norm:0.016434408724308014 max memory_allocated 22566.68408203125 
[2025-03-22 20:15:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:2.5199151039123535 norm:0.016277767717838287 max memory_allocated 22566.68408203125 
[2025-03-22 20:16:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:2.514352798461914 norm:0.01359505020081997 max memory_allocated 22566.68408203125 
[2025-03-22 20:16:45 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:2.5138297080993652 norm:0.014815565198659897 max memory_allocated 22566.68408203125 
[2025-03-22 20:16:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 20:16:57 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:17:30 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:3.623389720916748 norm:0.2646184265613556 max memory_allocated 22566.85595703125 
[2025-03-22 20:18:03 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:3.4965133666992188 norm:0.20420308411121368 max memory_allocated 22566.85595703125 
[2025-03-22 20:18:35 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:3.342806100845337 norm:0.22028708457946777 max memory_allocated 22566.85595703125 
[2025-03-22 20:19:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:3.2608580589294434 norm:0.1813078075647354 max memory_allocated 22566.85595703125 
[2025-03-22 20:19:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:3.218174695968628 norm:0.1514483541250229 max memory_allocated 22566.85595703125 
[2025-03-22 20:20:14 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:3.19242525100708 norm:0.14953140914440155 max memory_allocated 22566.85595703125 
[2025-03-22 20:20:46 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:3.170159339904785 norm:0.12761317193508148 max memory_allocated 22566.85595703125 
[2025-03-22 20:21:19 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:3.1596193313598633 norm:0.12029320746660233 max memory_allocated 22566.85595703125 
[2025-03-22 20:21:52 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:3.151597261428833 norm:0.11271006613969803 max memory_allocated 22566.85595703125 
[2025-03-22 20:22:24 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:3.1384758949279785 norm:0.1009296253323555 max memory_allocated 22566.85595703125 
[2025-03-22 20:22:57 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:3.128647565841675 norm:0.09176908433437347 max memory_allocated 22566.85595703125 
[2025-03-22 20:23:30 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:3.1228950023651123 norm:0.09277481585741043 max memory_allocated 22566.85595703125 
[2025-03-22 20:24:02 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:3.115544319152832 norm:0.08947522193193436 max memory_allocated 22566.85595703125 
[2025-03-22 20:24:35 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:3.1153976917266846 norm:0.09600149095058441 max memory_allocated 22566.85595703125 
[2025-03-22 20:25:07 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:3.1103146076202393 norm:0.09804918617010117 max memory_allocated 22566.85595703125 
[2025-03-22 20:25:40 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:3.1023662090301514 norm:0.08536967635154724 max memory_allocated 22566.85595703125 
[2025-03-22 20:26:13 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:3.100170850753784 norm:0.08273174613714218 max memory_allocated 22566.85595703125 
[2025-03-22 20:26:45 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:3.1052379608154297 norm:0.09228213876485825 max memory_allocated 22566.85595703125 
[2025-03-22 20:27:18 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:3.1004111766815186 norm:0.09253166615962982 max memory_allocated 22566.85595703125 
[2025-03-22 20:27:51 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:3.0950870513916016 norm:0.08580807596445084 max memory_allocated 22566.85595703125 
[2025-03-22 20:27:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 20:28:02 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:28:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:6.988929748535156 norm:0.9133809804916382 max memory_allocated 22567.02783203125 
[2025-03-22 20:29:08 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:6.358264923095703 norm:0.6650835275650024 max memory_allocated 22567.02783203125 
[2025-03-22 20:29:40 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:5.996665000915527 norm:0.5553044080734253 max memory_allocated 22567.02783203125 
[2025-03-22 20:30:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:5.7397589683532715 norm:0.47436094284057617 max memory_allocated 22567.02783203125 
[2025-03-22 20:30:46 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:5.595064163208008 norm:0.3676247000694275 max memory_allocated 22567.02783203125 
[2025-03-22 20:31:18 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:5.502913475036621 norm:0.3107025623321533 max memory_allocated 22567.02783203125 
[2025-03-22 20:31:51 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:5.430451393127441 norm:0.2868804931640625 max memory_allocated 22567.02783203125 
[2025-03-22 20:32:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:5.377830982208252 norm:0.2589401602745056 max memory_allocated 22567.02783203125 
[2025-03-22 20:32:56 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:5.328479766845703 norm:0.23208066821098328 max memory_allocated 22567.02783203125 
[2025-03-22 20:33:29 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:5.28652286529541 norm:0.20704564452171326 max memory_allocated 22567.02783203125 
[2025-03-22 20:34:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:5.279171943664551 norm:0.22609633207321167 max memory_allocated 22567.02783203125 
[2025-03-22 20:34:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:5.270829200744629 norm:0.2383844256401062 max memory_allocated 22567.02783203125 
[2025-03-22 20:35:07 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:5.232913494110107 norm:0.21187740564346313 max memory_allocated 22567.02783203125 
[2025-03-22 20:35:40 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:5.20734977722168 norm:0.19551566243171692 max memory_allocated 22567.02783203125 
[2025-03-22 20:36:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:5.192314147949219 norm:0.19470249116420746 max memory_allocated 22567.02783203125 
[2025-03-22 20:36:46 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:5.174774646759033 norm:0.1860419511795044 max memory_allocated 22567.02783203125 
[2025-03-22 20:37:19 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:5.1576948165893555 norm:0.17830488085746765 max memory_allocated 22567.02783203125 
[2025-03-22 20:37:51 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:5.159491062164307 norm:0.1827089786529541 max memory_allocated 22567.02783203125 
[2025-03-22 20:38:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:5.143746852874756 norm:0.17486807703971863 max memory_allocated 22567.02783203125 
[2025-03-22 20:38:57 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:5.121712684631348 norm:0.16271285712718964 max memory_allocated 22567.02783203125 
[2025-03-22 20:39:06 root] (main_calibration_a.py 369): INFO 21216.7244784832
[2025-03-22 20:39:11 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-22 20:40:19 root] (main_calibration_a.py 158): INFO wikitext2 : 8.421010971069336
[2025-03-22 20:40:19 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-22 20:42:04 root] (main_calibration_a.py 158): INFO c4 : 14.04535961151123
[2025-03-22 22:32:07 root] (main_calibration_a.py 169): INFO {'wikitext2': 8.421010971069336, 'c4': 14.04535961151123, 'results': {'piqa': {'acc': 0.661588683351469, 'acc_stderr': 0.011039817512986824, 'acc_norm': 0.6523394994559304, 'acc_norm_stderr': 0.011111173661393733}, 'arc_challenge': {'acc': 0.27559726962457337, 'acc_stderr': 0.013057169655761843, 'acc_norm': 0.30119453924914674, 'acc_norm_stderr': 0.013406741767847615}, 'boolq': {'acc': 0.6318042813455658, 'acc_stderr': 0.008435740064578371}, 'winogrande': {'acc': 0.5367008681925809, 'acc_stderr': 0.01401457845884326}, 'arc_easy': {'acc': 0.484006734006734, 'acc_stderr': 0.010254533589288182, 'acc_norm': 0.4128787878787879, 'acc_norm_stderr': 0.010102837421104658}, 'hellaswag': {'acc': 0.4404501095399323, 'acc_stderr': 0.0049542655953734695, 'acc_norm': 0.569308902609042, 'acc_norm_stderr': 0.004941609820763593}}, 'versions': {'piqa': 0, 'arc_challenge': 0, 'boolq': 1, 'winogrande': 0, 'arc_easy': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 22:32:07 root] (main_calibration_a.py 172): INFO 27.56,48.40,63.18,44.05,66.16,53.67
