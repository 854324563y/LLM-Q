[2025-03-22 14:42:02 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/llama-7b-hf-w4a4-2', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=2)
[2025-03-22 14:45:29 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 14:45:29 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:45:30 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 14:45:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 14:45:35 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:46:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.07180771231651306 norm:0.05531911924481392 max memory_allocated 22559.27978515625 
[2025-03-22 14:46:39 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.04609012231230736 norm:0.02800997719168663 max memory_allocated 22559.27978515625 
[2025-03-22 14:47:12 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.03754652291536331 norm:0.020500894635915756 max memory_allocated 22559.27978515625 
[2025-03-22 14:47:44 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.03393886238336563 norm:0.017769010737538338 max memory_allocated 22559.27978515625 
[2025-03-22 14:48:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.03228768706321716 norm:0.0150523092597723 max memory_allocated 22559.27978515625 
[2025-03-22 14:48:50 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.03109162673354149 norm:0.012782257050275803 max memory_allocated 22559.27978515625 
[2025-03-22 14:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.030232343822717667 norm:0.010706627741456032 max memory_allocated 22559.27978515625 
[2025-03-22 14:49:55 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.029727300629019737 norm:0.009197061881422997 max memory_allocated 22559.27978515625 
[2025-03-22 14:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.02941000461578369 norm:0.007920521311461926 max memory_allocated 22559.27978515625 
[2025-03-22 14:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.02919730916619301 norm:0.0069420915096998215 max memory_allocated 22559.27978515625 
[2025-03-22 14:51:33 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.02894413098692894 norm:0.0060854507610201836 max memory_allocated 22559.27978515625 
[2025-03-22 14:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.02879507467150688 norm:0.00533483037725091 max memory_allocated 22559.27978515625 
[2025-03-22 14:52:38 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.02866484597325325 norm:0.004858325235545635 max memory_allocated 22559.27978515625 
[2025-03-22 14:53:10 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.028588756918907166 norm:0.004662034101784229 max memory_allocated 22559.27978515625 
[2025-03-22 14:53:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.028483940288424492 norm:0.004410341382026672 max memory_allocated 22559.27978515625 
[2025-03-22 14:54:15 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.02841821312904358 norm:0.004197885747998953 max memory_allocated 22559.27978515625 
[2025-03-22 14:54:48 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.028398074209690094 norm:0.004017767962068319 max memory_allocated 22559.27978515625 
[2025-03-22 14:55:21 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.02837965451180935 norm:0.0038139214739203453 max memory_allocated 22559.27978515625 
[2025-03-22 14:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.028439585119485855 norm:0.0038176067173480988 max memory_allocated 22559.27978515625 
[2025-03-22 14:56:26 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.02838912419974804 norm:0.0036440654657781124 max memory_allocated 22559.27978515625 
[2025-03-22 14:56:34 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 14:56:37 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:57:10 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.24683192372322083 norm:0.146968275308609 max memory_allocated 22559.45166015625 
[2025-03-22 14:57:42 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.1413743644952774 norm:0.05144686624407768 max memory_allocated 22559.45166015625 
[2025-03-22 14:58:14 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.10906341671943665 norm:0.028616491705179214 max memory_allocated 22559.45166015625 
[2025-03-22 14:58:47 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.09527885913848877 norm:0.02277756854891777 max memory_allocated 22559.45166015625 
[2025-03-22 14:59:19 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.08796554803848267 norm:0.019004080444574356 max memory_allocated 22559.45166015625 
[2025-03-22 14:59:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.08345635235309601 norm:0.01610228605568409 max memory_allocated 22559.45166015625 
[2025-03-22 15:00:24 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.08069037646055222 norm:0.014166847802698612 max memory_allocated 22559.45166015625 
[2025-03-22 15:00:57 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.07889606058597565 norm:0.012519693933427334 max memory_allocated 22559.45166015625 
[2025-03-22 15:01:30 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.0774528905749321 norm:0.011201733723282814 max memory_allocated 22559.45166015625 
[2025-03-22 15:02:02 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.07647348940372467 norm:0.009823203086853027 max memory_allocated 22559.45166015625 
[2025-03-22 15:02:35 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.07558305561542511 norm:0.008703740313649178 max memory_allocated 22559.45166015625 
[2025-03-22 15:03:07 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.07494959980249405 norm:0.0079262163490057 max memory_allocated 22559.45166015625 
[2025-03-22 15:03:40 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.0744529515504837 norm:0.007284895516932011 max memory_allocated 22559.45166015625 
[2025-03-22 15:04:13 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.07408317178487778 norm:0.006869675125926733 max memory_allocated 22559.45166015625 
[2025-03-22 15:04:45 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.07380025088787079 norm:0.0065336949191987514 max memory_allocated 22559.45166015625 
[2025-03-22 15:05:18 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07350672036409378 norm:0.006462695077061653 max memory_allocated 22559.45166015625 
[2025-03-22 15:05:51 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.07330134510993958 norm:0.006018561776727438 max memory_allocated 22559.45166015625 
[2025-03-22 15:06:23 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07298848778009415 norm:0.0057936767116189 max memory_allocated 22559.45166015625 
[2025-03-22 15:06:56 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07283775508403778 norm:0.005552574060857296 max memory_allocated 22559.45166015625 
[2025-03-22 15:07:29 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07263151556253433 norm:0.005478959530591965 max memory_allocated 22559.45166015625 
[2025-03-22 15:07:38 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 15:07:41 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:08:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.2632954716682434 norm:0.04314705356955528 max memory_allocated 22559.62353515625 
[2025-03-22 15:08:46 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.2174808233976364 norm:0.03360135853290558 max memory_allocated 22559.62353515625 
[2025-03-22 15:09:19 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.18897032737731934 norm:0.019569184631109238 max memory_allocated 22559.62353515625 
[2025-03-22 15:09:51 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.17423279583454132 norm:0.017575249075889587 max memory_allocated 22559.62353515625 
[2025-03-22 15:10:24 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.16635920107364655 norm:0.016275521367788315 max memory_allocated 22559.62353515625 
[2025-03-22 15:10:56 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.1630243957042694 norm:0.0166696198284626 max memory_allocated 22559.62353515625 
[2025-03-22 15:11:29 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.15866519510746002 norm:0.016328951343894005 max memory_allocated 22559.62353515625 
[2025-03-22 15:12:02 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.15408997237682343 norm:0.015582620166242123 max memory_allocated 22559.62353515625 
[2025-03-22 15:12:34 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.15482257306575775 norm:0.015081437304615974 max memory_allocated 22559.62353515625 
[2025-03-22 15:13:07 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.15157562494277954 norm:0.015545057132840157 max memory_allocated 22559.62353515625 
[2025-03-22 15:13:40 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.14806918799877167 norm:0.014256303198635578 max memory_allocated 22559.62353515625 
[2025-03-22 15:14:12 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.1501060426235199 norm:0.01396782137453556 max memory_allocated 22559.62353515625 
[2025-03-22 15:14:45 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.14542950689792633 norm:0.014067661948502064 max memory_allocated 22559.62353515625 
[2025-03-22 15:15:17 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.1471937596797943 norm:0.012984024360775948 max memory_allocated 22559.62353515625 
[2025-03-22 15:15:50 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.14732038974761963 norm:0.012815184891223907 max memory_allocated 22559.62353515625 
[2025-03-22 15:16:23 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.14662820100784302 norm:0.01306862011551857 max memory_allocated 22559.62353515625 
[2025-03-22 15:16:55 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.1477385312318802 norm:0.013197675347328186 max memory_allocated 22559.62353515625 
[2025-03-22 15:17:28 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.1438925713300705 norm:0.013441281393170357 max memory_allocated 22559.62353515625 
[2025-03-22 15:18:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.14710098505020142 norm:0.012842145748436451 max memory_allocated 22559.62353515625 
[2025-03-22 15:18:33 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.14419835805892944 norm:0.012927364557981491 max memory_allocated 22559.62353515625 
[2025-03-22 15:18:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 15:19:18 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.24909983575344086 norm:0.05251830443739891 max memory_allocated 22559.62353515625 
[2025-03-22 15:19:50 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.20177966356277466 norm:0.014194026589393616 max memory_allocated 22559.62353515625 
[2025-03-22 15:20:23 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.17383931577205658 norm:0.005880864802747965 max memory_allocated 22559.62353515625 
[2025-03-22 15:20:55 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.1640843003988266 norm:0.0035752004478126764 max memory_allocated 22559.62353515625 
[2025-03-22 15:21:28 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.15961718559265137 norm:0.0026889569126069546 max memory_allocated 22559.62353515625 
[2025-03-22 15:22:00 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.1568344235420227 norm:0.0022733109071850777 max memory_allocated 22559.62353515625 
[2025-03-22 15:22:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.15499506890773773 norm:0.0020324736833572388 max memory_allocated 22559.62353515625 
[2025-03-22 15:23:05 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.1536114513874054 norm:0.0018341878894716501 max memory_allocated 22559.62353515625 
[2025-03-22 15:23:38 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.1526612490415573 norm:0.0017127336468547583 max memory_allocated 22559.62353515625 
[2025-03-22 15:24:11 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.1521441787481308 norm:0.0016746800392866135 max memory_allocated 22559.62353515625 
[2025-03-22 15:24:43 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.15164528787136078 norm:0.0016667407471686602 max memory_allocated 22559.62353515625 
[2025-03-22 15:25:16 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.1513349711894989 norm:0.0016124239191412926 max memory_allocated 22559.62353515625 
[2025-03-22 15:25:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.15067830681800842 norm:0.0014795830938965082 max memory_allocated 22559.62353515625 
[2025-03-22 15:26:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.15018655359745026 norm:0.00142959738150239 max memory_allocated 22559.62353515625 
[2025-03-22 15:26:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.15015240013599396 norm:0.0013840064639225602 max memory_allocated 22559.62353515625 
[2025-03-22 15:27:26 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.14998477697372437 norm:0.0013053870061412454 max memory_allocated 22559.62353515625 
[2025-03-22 15:27:58 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.14960303902626038 norm:0.0012629544362425804 max memory_allocated 22559.62353515625 
[2025-03-22 15:28:31 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.14951804280281067 norm:0.0012225337559357285 max memory_allocated 22559.62353515625 
[2025-03-22 15:29:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.14952851831912994 norm:0.0012192726135253906 max memory_allocated 22559.62353515625 
[2025-03-22 15:29:36 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.14938710629940033 norm:0.0011937262024730444 max memory_allocated 22559.62353515625 
[2025-03-22 15:29:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 15:30:20 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.29713577032089233 norm:0.04632319509983063 max memory_allocated 22559.73681640625 
[2025-03-22 15:30:52 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.24832768738269806 norm:0.01811498962342739 max memory_allocated 22559.73681640625 
[2025-03-22 15:31:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.21431384980678558 norm:0.006614084355533123 max memory_allocated 22559.73681640625 
[2025-03-22 15:31:57 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.20136737823486328 norm:0.0031408921349793673 max memory_allocated 22559.73681640625 
[2025-03-22 15:32:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.19660694897174835 norm:0.002520992886275053 max memory_allocated 22559.73681640625 
[2025-03-22 15:33:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.19357453286647797 norm:0.002269348129630089 max memory_allocated 22559.73681640625 
[2025-03-22 15:33:35 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.19149281084537506 norm:0.0019557042978703976 max memory_allocated 22559.73681640625 
[2025-03-22 15:34:07 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.1902424842119217 norm:0.0018379352986812592 max memory_allocated 22559.73681640625 
[2025-03-22 15:34:40 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.18928122520446777 norm:0.0017795683816075325 max memory_allocated 22559.73681640625 
[2025-03-22 15:35:12 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.18844307959079742 norm:0.00168136996217072 max memory_allocated 22559.73681640625 
[2025-03-22 15:35:45 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.1881478875875473 norm:0.0015903483144938946 max memory_allocated 22559.73681640625 
[2025-03-22 15:36:18 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.18784096837043762 norm:0.0015474651008844376 max memory_allocated 22559.73681640625 
[2025-03-22 15:36:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.18760313093662262 norm:0.0014708067756146193 max memory_allocated 22559.73681640625 
[2025-03-22 15:37:23 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.18745793402194977 norm:0.001436418853700161 max memory_allocated 22559.73681640625 
[2025-03-22 15:37:55 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.18766850233078003 norm:0.0014635826228186488 max memory_allocated 22559.73681640625 
[2025-03-22 15:38:28 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.1873997300863266 norm:0.0014219068689271808 max memory_allocated 22559.73681640625 
[2025-03-22 15:39:01 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.18702514469623566 norm:0.0013441935880109668 max memory_allocated 22559.73681640625 
[2025-03-22 15:39:33 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.18696965277194977 norm:0.0013452655402943492 max memory_allocated 22559.73681640625 
[2025-03-22 15:40:06 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.18718864023685455 norm:0.0013804093468934298 max memory_allocated 22559.73681640625 
[2025-03-22 15:40:38 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.18717443943023682 norm:0.0013362939935177565 max memory_allocated 22559.73681640625 
[2025-03-22 15:40:47 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 15:41:23 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.3483772575855255 norm:0.06948888301849365 max memory_allocated 22559.90869140625 
[2025-03-22 15:41:55 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.29217153787612915 norm:0.023732012137770653 max memory_allocated 22559.90869140625 
[2025-03-22 15:42:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.24204084277153015 norm:0.006465244106948376 max memory_allocated 22559.90869140625 
[2025-03-22 15:43:00 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.227696031332016 norm:0.0035963975824415684 max memory_allocated 22559.90869140625 
[2025-03-22 15:43:33 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.22201023995876312 norm:0.0029386496171355247 max memory_allocated 22559.90869140625 
[2025-03-22 15:44:05 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.2180304229259491 norm:0.002513261279091239 max memory_allocated 22559.90869140625 
[2025-03-22 15:44:38 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.2155432254076004 norm:0.00234505464322865 max memory_allocated 22559.90869140625 
[2025-03-22 15:45:10 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.21375608444213867 norm:0.0020970292389392853 max memory_allocated 22559.90869140625 
[2025-03-22 15:45:43 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.21243995428085327 norm:0.0018861660500988364 max memory_allocated 22559.90869140625 
[2025-03-22 15:46:15 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.21170039474964142 norm:0.001811215770430863 max memory_allocated 22559.90869140625 
[2025-03-22 15:46:48 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.21090289950370789 norm:0.0016937586478888988 max memory_allocated 22559.90869140625 
[2025-03-22 15:47:20 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.21027317643165588 norm:0.0016116797924041748 max memory_allocated 22559.90869140625 
[2025-03-22 15:47:53 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.21035540103912354 norm:0.001638487447053194 max memory_allocated 22559.90869140625 
[2025-03-22 15:48:25 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.21031823754310608 norm:0.0015734252519905567 max memory_allocated 22559.90869140625 
[2025-03-22 15:48:58 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.2098393738269806 norm:0.0015062057645991445 max memory_allocated 22559.90869140625 
[2025-03-22 15:49:30 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.20947927236557007 norm:0.0014075884828343987 max memory_allocated 22559.90869140625 
[2025-03-22 15:50:03 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.20938843488693237 norm:0.0013864085776731372 max memory_allocated 22559.90869140625 
[2025-03-22 15:50:35 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.2087806612253189 norm:0.0013496887404471636 max memory_allocated 22559.90869140625 
[2025-03-22 15:51:08 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.20827634632587433 norm:0.001274685375392437 max memory_allocated 22559.90869140625 
[2025-03-22 15:51:41 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.2082219421863556 norm:0.0012578369351103902 max memory_allocated 22559.90869140625 
[2025-03-22 15:51:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 15:52:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.3577078878879547 norm:0.05968427658081055 max memory_allocated 22560.08056640625 
[2025-03-22 15:52:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.317066490650177 norm:0.027178287506103516 max memory_allocated 22560.08056640625 
[2025-03-22 15:53:30 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.26709023118019104 norm:0.0073213945142924786 max memory_allocated 22560.08056640625 
[2025-03-22 15:54:03 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.24964778125286102 norm:0.0032086351420730352 max memory_allocated 22560.08056640625 
[2025-03-22 15:54:35 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.2442019283771515 norm:0.002576362807303667 max memory_allocated 22560.08056640625 
[2025-03-22 15:55:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.24065834283828735 norm:0.002144522499293089 max memory_allocated 22560.08056640625 
[2025-03-22 15:55:40 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.23811158537864685 norm:0.0018469463102519512 max memory_allocated 22560.08056640625 
[2025-03-22 15:56:13 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.23628149926662445 norm:0.0016522170044481754 max memory_allocated 22560.08056640625 
[2025-03-22 15:56:46 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.23498335480690002 norm:0.0015808604657649994 max memory_allocated 22560.08056640625 
[2025-03-22 15:57:18 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.23437559604644775 norm:0.0015351050533354282 max memory_allocated 22560.08056640625 
[2025-03-22 15:57:51 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.2339569628238678 norm:0.0014904842246323824 max memory_allocated 22560.08056640625 
[2025-03-22 15:58:24 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.23321320116519928 norm:0.0013449826510623097 max memory_allocated 22560.08056640625 
[2025-03-22 15:58:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.23272472620010376 norm:0.0013000278268009424 max memory_allocated 22560.08056640625 
[2025-03-22 15:59:29 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.23259831964969635 norm:0.0012941015884280205 max memory_allocated 22560.08056640625 
[2025-03-22 16:00:01 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.23243673145771027 norm:0.0012828486505895853 max memory_allocated 22560.08056640625 
[2025-03-22 16:00:34 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.23229266703128815 norm:0.001274391426704824 max memory_allocated 22560.08056640625 
[2025-03-22 16:01:07 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.23216122388839722 norm:0.0012475426774471998 max memory_allocated 22560.08056640625 
[2025-03-22 16:01:39 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.23186136782169342 norm:0.0012252958258613944 max memory_allocated 22560.08056640625 
[2025-03-22 16:02:12 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.23175543546676636 norm:0.0012233706656843424 max memory_allocated 22560.08056640625 
[2025-03-22 16:02:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.23183563351631165 norm:0.001234081806614995 max memory_allocated 22560.08056640625 
[2025-03-22 16:02:53 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 16:03:29 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.3935256004333496 norm:0.042023975402116776 max memory_allocated 22560.25244140625 
[2025-03-22 16:04:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.34736377000808716 norm:0.02004176378250122 max memory_allocated 22560.25244140625 
[2025-03-22 16:04:34 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.2933383584022522 norm:0.006341180764138699 max memory_allocated 22560.25244140625 
[2025-03-22 16:05:06 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.27492058277130127 norm:0.002871544798836112 max memory_allocated 22560.25244140625 
[2025-03-22 16:05:39 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.2683471441268921 norm:0.002167717320844531 max memory_allocated 22560.25244140625 
[2025-03-22 16:06:11 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.26430362462997437 norm:0.0018611917039379478 max memory_allocated 22560.25244140625 
[2025-03-22 16:06:44 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.2616044282913208 norm:0.0017042404506355524 max memory_allocated 22560.25244140625 
[2025-03-22 16:07:16 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.2598140239715576 norm:0.001553007634356618 max memory_allocated 22560.25244140625 
[2025-03-22 16:07:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.2587175667285919 norm:0.0013991731684654951 max memory_allocated 22560.25244140625 
[2025-03-22 16:08:21 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.2578738331794739 norm:0.0013643635902553797 max memory_allocated 22560.25244140625 
[2025-03-22 16:08:54 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.25726327300071716 norm:0.001299500116147101 max memory_allocated 22560.25244140625 
[2025-03-22 16:09:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.2569751739501953 norm:0.0012677611084654927 max memory_allocated 22560.25244140625 
[2025-03-22 16:09:59 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.2562176585197449 norm:0.0011911541223526 max memory_allocated 22560.25244140625 
[2025-03-22 16:10:32 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2560332417488098 norm:0.0012391916243359447 max memory_allocated 22560.25244140625 
[2025-03-22 16:11:04 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.25581565499305725 norm:0.0012419979320839047 max memory_allocated 22560.25244140625 
[2025-03-22 16:11:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.25570163130760193 norm:0.0012098895385861397 max memory_allocated 22560.25244140625 
[2025-03-22 16:12:09 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.2556917369365692 norm:0.0011656585847958922 max memory_allocated 22560.25244140625 
[2025-03-22 16:12:42 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.2557997405529022 norm:0.0011419591028243303 max memory_allocated 22560.25244140625 
[2025-03-22 16:13:15 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.25581595301628113 norm:0.0011463066330179572 max memory_allocated 22560.25244140625 
[2025-03-22 16:13:47 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.2559581696987152 norm:0.0011151648359373212 max memory_allocated 22560.25244140625 
[2025-03-22 16:13:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 16:14:32 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.40962788462638855 norm:0.032053280621767044 max memory_allocated 22560.42431640625 
[2025-03-22 16:15:04 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.36072349548339844 norm:0.01479305513203144 max memory_allocated 22560.42431640625 
[2025-03-22 16:15:37 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.31311628222465515 norm:0.0056427037343382835 max memory_allocated 22560.42431640625 
[2025-03-22 16:16:09 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.29506972432136536 norm:0.00260048545897007 max memory_allocated 22560.42431640625 
[2025-03-22 16:16:42 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.28806886076927185 norm:0.001951829413883388 max memory_allocated 22560.42431640625 
[2025-03-22 16:17:15 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.2840225398540497 norm:0.0017667075153440237 max memory_allocated 22560.42431640625 
[2025-03-22 16:17:47 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.2809280455112457 norm:0.0016075694002211094 max memory_allocated 22560.42431640625 
[2025-03-22 16:18:20 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.27875688672065735 norm:0.0014838404022157192 max memory_allocated 22560.42431640625 
[2025-03-22 16:18:53 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.27739599347114563 norm:0.001369929639622569 max memory_allocated 22560.42431640625 
[2025-03-22 16:19:25 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.2763805389404297 norm:0.0012971549294888973 max memory_allocated 22560.42431640625 
[2025-03-22 16:19:58 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.2756005525588989 norm:0.001252794056199491 max memory_allocated 22560.42431640625 
[2025-03-22 16:20:31 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.275007963180542 norm:0.0012481613084673882 max memory_allocated 22560.42431640625 
[2025-03-22 16:21:03 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.274767130613327 norm:0.0012590157566592097 max memory_allocated 22560.42431640625 
[2025-03-22 16:21:36 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.27434685826301575 norm:0.0011992740910500288 max memory_allocated 22560.42431640625 
[2025-03-22 16:22:09 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.27411949634552 norm:0.0011582052102312446 max memory_allocated 22560.42431640625 
[2025-03-22 16:22:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.27386996150016785 norm:0.0011603961465880275 max memory_allocated 22560.42431640625 
[2025-03-22 16:23:14 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.2737962603569031 norm:0.0011657930444926023 max memory_allocated 22560.42431640625 
[2025-03-22 16:23:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.2735855281352997 norm:0.0011855714255943894 max memory_allocated 22560.42431640625 
[2025-03-22 16:24:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.27328771352767944 norm:0.0011209292570129037 max memory_allocated 22560.42431640625 
[2025-03-22 16:24:52 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.2732832133769989 norm:0.0010551890591159463 max memory_allocated 22560.42431640625 
[2025-03-22 16:25:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 16:25:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.4458179771900177 norm:0.03989802300930023 max memory_allocated 22560.59619140625 
[2025-03-22 16:26:10 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.40058833360671997 norm:0.02059425786137581 max memory_allocated 22560.59619140625 
[2025-03-22 16:26:42 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.3403920829296112 norm:0.0072081382386386395 max memory_allocated 22560.59619140625 
[2025-03-22 16:27:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.3168557584285736 norm:0.003121722722426057 max memory_allocated 22560.59619140625 
[2025-03-22 16:27:47 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.30935394763946533 norm:0.0023697447031736374 max memory_allocated 22560.59619140625 
[2025-03-22 16:28:20 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3048616051673889 norm:0.0019815179985016584 max memory_allocated 22560.59619140625 
[2025-03-22 16:28:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.3017686903476715 norm:0.0017990507185459137 max memory_allocated 22560.59619140625 
[2025-03-22 16:29:25 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.29941877722740173 norm:0.0015232667792588472 max memory_allocated 22560.59619140625 
[2025-03-22 16:29:57 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.2974947392940521 norm:0.0014040020760148764 max memory_allocated 22560.59619140625 
[2025-03-22 16:30:30 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.29645219445228577 norm:0.0013324503088369966 max memory_allocated 22560.59619140625 
[2025-03-22 16:31:02 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.295664519071579 norm:0.0012903683818876743 max memory_allocated 22560.59619140625 
[2025-03-22 16:31:35 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.2950829863548279 norm:0.0012662580702453852 max memory_allocated 22560.59619140625 
[2025-03-22 16:32:07 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.2945217788219452 norm:0.001241413876414299 max memory_allocated 22560.59619140625 
[2025-03-22 16:32:40 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.2942020893096924 norm:0.0012546142097562551 max memory_allocated 22560.59619140625 
[2025-03-22 16:33:12 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.2939683496952057 norm:0.0012138000456616282 max memory_allocated 22560.59619140625 
[2025-03-22 16:33:45 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.29346194863319397 norm:0.0011834693141281605 max memory_allocated 22560.59619140625 
[2025-03-22 16:34:18 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.2931053042411804 norm:0.001163413398899138 max memory_allocated 22560.59619140625 
[2025-03-22 16:34:50 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.29298049211502075 norm:0.001072408864274621 max memory_allocated 22560.59619140625 
[2025-03-22 16:35:23 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.29275965690612793 norm:0.0010524527169764042 max memory_allocated 22560.59619140625 
[2025-03-22 16:35:55 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.292717844247818 norm:0.0010086932452395558 max memory_allocated 22560.59619140625 
[2025-03-22 16:36:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 16:36:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.41227033734321594 norm:0.02151579037308693 max memory_allocated 22560.76806640625 
[2025-03-22 16:37:13 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.38055485486984253 norm:0.011904345825314522 max memory_allocated 22560.76806640625 
[2025-03-22 16:37:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.34007352590560913 norm:0.004179417621344328 max memory_allocated 22560.76806640625 
[2025-03-22 16:38:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.32512080669403076 norm:0.0020376723259687424 max memory_allocated 22560.76806640625 
[2025-03-22 16:38:51 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.31983089447021484 norm:0.0016539280768483877 max memory_allocated 22560.76806640625 
[2025-03-22 16:39:24 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3167578876018524 norm:0.0014798453776165843 max memory_allocated 22560.76806640625 
[2025-03-22 16:39:56 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.31455957889556885 norm:0.0013510440476238728 max memory_allocated 22560.76806640625 
[2025-03-22 16:40:29 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.31279855966567993 norm:0.001263757236301899 max memory_allocated 22560.76806640625 
[2025-03-22 16:41:02 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.3115042448043823 norm:0.0011749004479497671 max memory_allocated 22560.76806640625 
[2025-03-22 16:41:34 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.31072068214416504 norm:0.0011307154782116413 max memory_allocated 22560.76806640625 
[2025-03-22 16:42:07 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.31008586287498474 norm:0.0010975286131724715 max memory_allocated 22560.76806640625 
[2025-03-22 16:42:39 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.3096129894256592 norm:0.0010641204426065087 max memory_allocated 22560.76806640625 
[2025-03-22 16:43:12 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.30927228927612305 norm:0.0010525116231292486 max memory_allocated 22560.76806640625 
[2025-03-22 16:43:45 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.30890440940856934 norm:0.0010605637216940522 max memory_allocated 22560.76806640625 
[2025-03-22 16:44:17 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3085126578807831 norm:0.0009938864968717098 max memory_allocated 22560.76806640625 
[2025-03-22 16:44:50 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.30811142921447754 norm:0.0009772889316082 max memory_allocated 22560.76806640625 
[2025-03-22 16:45:23 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.30793100595474243 norm:0.0009443455492146313 max memory_allocated 22560.76806640625 
[2025-03-22 16:45:55 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3078705072402954 norm:0.0009065645281225443 max memory_allocated 22560.76806640625 
[2025-03-22 16:46:28 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.30756956338882446 norm:0.0008884877897799015 max memory_allocated 22560.76806640625 
[2025-03-22 16:47:01 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.3074307441711426 norm:0.0008599305292591453 max memory_allocated 22560.76806640625 
[2025-03-22 16:47:09 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 16:47:45 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.43943867087364197 norm:0.03270701691508293 max memory_allocated 22560.93994140625 
[2025-03-22 16:48:17 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.4046080410480499 norm:0.016753721982240677 max memory_allocated 22560.93994140625 
[2025-03-22 16:48:50 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3596595525741577 norm:0.005716500338166952 max memory_allocated 22560.93994140625 
[2025-03-22 16:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.34115898609161377 norm:0.0025231943000108004 max memory_allocated 22560.93994140625 
[2025-03-22 16:49:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.33508607745170593 norm:0.0019537426996976137 max memory_allocated 22560.93994140625 
[2025-03-22 16:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.3313167691230774 norm:0.0016733186785131693 max memory_allocated 22560.93994140625 
[2025-03-22 16:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.32868507504463196 norm:0.0015505891060456634 max memory_allocated 22560.93994140625 
[2025-03-22 16:51:32 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3267955183982849 norm:0.0014188694767653942 max memory_allocated 22560.93994140625 
[2025-03-22 16:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.3254636824131012 norm:0.0013275681994855404 max memory_allocated 22560.93994140625 
[2025-03-22 16:52:37 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.32460951805114746 norm:0.0012763218255713582 max memory_allocated 22560.93994140625 
[2025-03-22 16:53:10 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.3238319456577301 norm:0.0012256477493792772 max memory_allocated 22560.93994140625 
[2025-03-22 16:53:42 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.323264479637146 norm:0.001222546910867095 max memory_allocated 22560.93994140625 
[2025-03-22 16:54:15 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3227492570877075 norm:0.001175748067907989 max memory_allocated 22560.93994140625 
[2025-03-22 16:54:48 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.32239001989364624 norm:0.001177608035504818 max memory_allocated 22560.93994140625 
[2025-03-22 16:55:20 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3220444917678833 norm:0.0011386261321604252 max memory_allocated 22560.93994140625 
[2025-03-22 16:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.32152289152145386 norm:0.0011004635598510504 max memory_allocated 22560.93994140625 
[2025-03-22 16:56:25 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.3211907148361206 norm:0.001023203949443996 max memory_allocated 22560.93994140625 
[2025-03-22 16:56:58 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.32092782855033875 norm:0.0010073671583086252 max memory_allocated 22560.93994140625 
[2025-03-22 16:57:31 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.32067224383354187 norm:0.0009867188055068254 max memory_allocated 22560.93994140625 
[2025-03-22 16:58:03 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.3204387426376343 norm:0.0009656965266913176 max memory_allocated 22560.93994140625 
[2025-03-22 16:58:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 16:58:48 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.4301052987575531 norm:0.026578214019536972 max memory_allocated 22561.11181640625 
[2025-03-22 16:59:20 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.4070829153060913 norm:0.014704684726893902 max memory_allocated 22561.11181640625 
[2025-03-22 16:59:53 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.3764035701751709 norm:0.007493163459002972 max memory_allocated 22561.11181640625 
[2025-03-22 17:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.35919511318206787 norm:0.0044360775500535965 max memory_allocated 22561.11181640625 
[2025-03-22 17:00:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.3522959053516388 norm:0.003306412370875478 max memory_allocated 22561.11181640625 
[2025-03-22 17:01:30 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.3476226329803467 norm:0.002675354713574052 max memory_allocated 22561.11181640625 
[2025-03-22 17:02:03 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.34453263878822327 norm:0.002308635273948312 max memory_allocated 22561.11181640625 
[2025-03-22 17:02:36 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.3422870635986328 norm:0.0019842637702822685 max memory_allocated 22561.11181640625 
[2025-03-22 17:03:08 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.34063100814819336 norm:0.0018067567143589258 max memory_allocated 22561.11181640625 
[2025-03-22 17:03:41 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.33929523825645447 norm:0.0016342486487701535 max memory_allocated 22561.11181640625 
[2025-03-22 17:04:14 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.3383447825908661 norm:0.0015975652495399117 max memory_allocated 22561.11181640625 
[2025-03-22 17:04:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.33758577704429626 norm:0.0015081000747159123 max memory_allocated 22561.11181640625 
[2025-03-22 17:05:19 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.33699876070022583 norm:0.0014405858237296343 max memory_allocated 22561.11181640625 
[2025-03-22 17:05:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.3364085257053375 norm:0.0013366336934268475 max memory_allocated 22561.11181640625 
[2025-03-22 17:06:24 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.33579927682876587 norm:0.0012292799074202776 max memory_allocated 22561.11181640625 
[2025-03-22 17:06:57 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.33538946509361267 norm:0.0011999596608802676 max memory_allocated 22561.11181640625 
[2025-03-22 17:07:30 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.33495306968688965 norm:0.001136471750214696 max memory_allocated 22561.11181640625 
[2025-03-22 17:08:02 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.3345925807952881 norm:0.0010717902332544327 max memory_allocated 22561.11181640625 
[2025-03-22 17:08:35 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.33422330021858215 norm:0.0010218138340860605 max memory_allocated 22561.11181640625 
[2025-03-22 17:09:07 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.33403605222702026 norm:0.0009953982662409544 max memory_allocated 22561.11181640625 
[2025-03-22 17:09:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 17:09:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.4380783438682556 norm:0.01910983957350254 max memory_allocated 22561.28369140625 
[2025-03-22 17:10:24 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.4120364785194397 norm:0.010377668775618076 max memory_allocated 22561.28369140625 
[2025-03-22 17:10:57 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.3821982443332672 norm:0.004813662730157375 max memory_allocated 22561.28369140625 
[2025-03-22 17:11:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.36781662702560425 norm:0.002320659114047885 max memory_allocated 22561.28369140625 
[2025-03-22 17:12:02 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.36189359426498413 norm:0.0016793531831353903 max memory_allocated 22561.28369140625 
[2025-03-22 17:12:34 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3584567606449127 norm:0.001455335645005107 max memory_allocated 22561.28369140625 
[2025-03-22 17:13:07 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.35626477003097534 norm:0.001375188585370779 max memory_allocated 22561.28369140625 
[2025-03-22 17:13:39 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.35462555289268494 norm:0.0013183793053030968 max memory_allocated 22561.28369140625 
[2025-03-22 17:14:12 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.3535046875476837 norm:0.0013010097900405526 max memory_allocated 22561.28369140625 
[2025-03-22 17:14:44 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.3524032235145569 norm:0.0012763012200593948 max memory_allocated 22561.28369140625 
[2025-03-22 17:15:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3512997031211853 norm:0.0012376999948173761 max memory_allocated 22561.28369140625 
[2025-03-22 17:15:50 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.3504818379878998 norm:0.0011848045978695154 max memory_allocated 22561.28369140625 
[2025-03-22 17:16:22 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.3498903512954712 norm:0.001117632375098765 max memory_allocated 22561.28369140625 
[2025-03-22 17:16:55 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.34917303919792175 norm:0.0010510679567232728 max memory_allocated 22561.28369140625 
[2025-03-22 17:17:27 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.3489239811897278 norm:0.0010419925674796104 max memory_allocated 22561.28369140625 
[2025-03-22 17:18:00 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3485458791255951 norm:0.0010062563233077526 max memory_allocated 22561.28369140625 
[2025-03-22 17:18:33 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.3481137156486511 norm:0.0009715565829537809 max memory_allocated 22561.28369140625 
[2025-03-22 17:19:05 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.3478162884712219 norm:0.000952313479501754 max memory_allocated 22561.28369140625 
[2025-03-22 17:19:38 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.34760192036628723 norm:0.0009422022849321365 max memory_allocated 22561.28369140625 
[2025-03-22 17:20:11 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.34731078147888184 norm:0.0009169923141598701 max memory_allocated 22561.28369140625 
[2025-03-22 17:20:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 17:20:55 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.4908263385295868 norm:0.05353262275457382 max memory_allocated 22561.45556640625 
[2025-03-22 17:21:27 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.4612641930580139 norm:0.0292355939745903 max memory_allocated 22561.45556640625 
[2025-03-22 17:22:00 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.4175763726234436 norm:0.012385597452521324 max memory_allocated 22561.45556640625 
[2025-03-22 17:22:33 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.39849650859832764 norm:0.006849499419331551 max memory_allocated 22561.45556640625 
[2025-03-22 17:23:05 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.3905009925365448 norm:0.005168527830392122 max memory_allocated 22561.45556640625 
[2025-03-22 17:23:38 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3860093355178833 norm:0.0043535493314266205 max memory_allocated 22561.45556640625 
[2025-03-22 17:24:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.38269200921058655 norm:0.003665943630039692 max memory_allocated 22561.45556640625 
[2025-03-22 17:24:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.3803742527961731 norm:0.003184011671692133 max memory_allocated 22561.45556640625 
[2025-03-22 17:25:16 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3784133493900299 norm:0.00278974836692214 max memory_allocated 22561.45556640625 
[2025-03-22 17:25:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.37691473960876465 norm:0.002459202893078327 max memory_allocated 22561.45556640625 
[2025-03-22 17:26:21 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.37554115056991577 norm:0.002182752825319767 max memory_allocated 22561.45556640625 
[2025-03-22 17:26:53 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.3745444416999817 norm:0.001981578068807721 max memory_allocated 22561.45556640625 
[2025-03-22 17:27:26 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.3739258944988251 norm:0.001840183394961059 max memory_allocated 22561.45556640625 
[2025-03-22 17:27:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.37324729561805725 norm:0.0016695385565981269 max memory_allocated 22561.45556640625 
[2025-03-22 17:28:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.3727744221687317 norm:0.0016184798441827297 max memory_allocated 22561.45556640625 
[2025-03-22 17:29:03 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.37233567237854004 norm:0.001550592016428709 max memory_allocated 22561.45556640625 
[2025-03-22 17:29:36 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.3716140389442444 norm:0.0014039418892934918 max memory_allocated 22561.45556640625 
[2025-03-22 17:30:09 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.3711852431297302 norm:0.001362028531730175 max memory_allocated 22561.45556640625 
[2025-03-22 17:30:41 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.3707837462425232 norm:0.0013207891024649143 max memory_allocated 22561.45556640625 
[2025-03-22 17:31:14 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.37041470408439636 norm:0.0012524572666734457 max memory_allocated 22561.45556640625 
[2025-03-22 17:31:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 17:31:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.48853370547294617 norm:0.0354793407022953 max memory_allocated 22561.62744140625 
[2025-03-22 17:32:31 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.46374937891960144 norm:0.01816267892718315 max memory_allocated 22561.62744140625 
[2025-03-22 17:33:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.43563076853752136 norm:0.00803968496620655 max memory_allocated 22561.62744140625 
[2025-03-22 17:33:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.41991835832595825 norm:0.004069000016897917 max memory_allocated 22561.62744140625 
[2025-03-22 17:34:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.4135255515575409 norm:0.0025813637766987085 max memory_allocated 22561.62744140625 
[2025-03-22 17:34:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.4092101454734802 norm:0.0020035263150930405 max memory_allocated 22561.62744140625 
[2025-03-22 17:35:13 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.40618234872817993 norm:0.0017762414645403624 max memory_allocated 22561.62744140625 
[2025-03-22 17:35:46 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.4040664732456207 norm:0.001695640035904944 max memory_allocated 22561.62744140625 
[2025-03-22 17:36:19 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.4023148715496063 norm:0.0012914995895698667 max memory_allocated 22561.62744140625 
[2025-03-22 17:36:51 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.4010947346687317 norm:0.001227963948622346 max memory_allocated 22561.62744140625 
[2025-03-22 17:37:24 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.40023064613342285 norm:0.0011854278855025768 max memory_allocated 22561.62744140625 
[2025-03-22 17:37:57 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.3996797800064087 norm:0.0011731541017070413 max memory_allocated 22561.62744140625 
[2025-03-22 17:38:29 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.39917925000190735 norm:0.001143618836067617 max memory_allocated 22561.62744140625 
[2025-03-22 17:39:02 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.3987392485141754 norm:0.0011017017532140017 max memory_allocated 22561.62744140625 
[2025-03-22 17:39:35 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.3980991840362549 norm:0.0010818187147378922 max memory_allocated 22561.62744140625 
[2025-03-22 17:40:07 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.39778390526771545 norm:0.001079679117538035 max memory_allocated 22561.62744140625 
[2025-03-22 17:40:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.3974490761756897 norm:0.001056258101016283 max memory_allocated 22561.62744140625 
[2025-03-22 17:41:13 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.39718595147132874 norm:0.0010619349777698517 max memory_allocated 22561.62744140625 
[2025-03-22 17:41:45 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.39689409732818604 norm:0.0010472070425748825 max memory_allocated 22561.62744140625 
[2025-03-22 17:42:18 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.39657750725746155 norm:0.0010221605189144611 max memory_allocated 22561.62744140625 
[2025-03-22 17:42:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 17:43:02 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.5568388104438782 norm:0.039310526102781296 max memory_allocated 22561.79931640625 
[2025-03-22 17:43:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.5235849022865295 norm:0.02097294293344021 max memory_allocated 22561.79931640625 
[2025-03-22 17:44:07 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.4820602536201477 norm:0.009532114490866661 max memory_allocated 22561.79931640625 
[2025-03-22 17:44:40 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.4606148302555084 norm:0.005105288699269295 max memory_allocated 22561.79931640625 
[2025-03-22 17:45:12 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.4522683024406433 norm:0.0030435859225690365 max memory_allocated 22561.79931640625 
[2025-03-22 17:45:45 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.44797539710998535 norm:0.0023836942855268717 max memory_allocated 22561.79931640625 
[2025-03-22 17:46:17 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.4450104534626007 norm:0.0021614297293126583 max memory_allocated 22561.79931640625 
[2025-03-22 17:46:50 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.4426872730255127 norm:0.002020979765802622 max memory_allocated 22561.79931640625 
[2025-03-22 17:47:22 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.44105929136276245 norm:0.001897974405437708 max memory_allocated 22561.79931640625 
[2025-03-22 17:47:55 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.4395398497581482 norm:0.001744051929563284 max memory_allocated 22561.79931640625 
[2025-03-22 17:48:27 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.43835633993148804 norm:0.0016524859238415956 max memory_allocated 22561.79931640625 
[2025-03-22 17:49:00 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.43737781047821045 norm:0.0015823144931346178 max memory_allocated 22561.79931640625 
[2025-03-22 17:49:32 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.43662765622138977 norm:0.0015206849202513695 max memory_allocated 22561.79931640625 
[2025-03-22 17:50:05 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.43590766191482544 norm:0.0013501441571861506 max memory_allocated 22561.79931640625 
[2025-03-22 17:50:37 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.4353203773498535 norm:0.0012208361877128482 max memory_allocated 22561.79931640625 
[2025-03-22 17:51:10 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.43480849266052246 norm:0.0011995650129392743 max memory_allocated 22561.79931640625 
[2025-03-22 17:51:42 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.43455544114112854 norm:0.00119244575034827 max memory_allocated 22561.79931640625 
[2025-03-22 17:52:15 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.43435168266296387 norm:0.001190774142742157 max memory_allocated 22561.79931640625 
[2025-03-22 17:52:48 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.4339442253112793 norm:0.0011932682245969772 max memory_allocated 22561.79931640625 
[2025-03-22 17:53:20 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.4336690306663513 norm:0.0011740282643586397 max memory_allocated 22561.79931640625 
[2025-03-22 17:53:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 17:54:05 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.5875453352928162 norm:0.03475619852542877 max memory_allocated 22561.97119140625 
[2025-03-22 17:54:37 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.5592368245124817 norm:0.018528729677200317 max memory_allocated 22561.97119140625 
[2025-03-22 17:55:10 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.5270100235939026 norm:0.008559665642678738 max memory_allocated 22561.97119140625 
[2025-03-22 17:55:42 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.509416937828064 norm:0.0038836710155010223 max memory_allocated 22561.97119140625 
[2025-03-22 17:56:15 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.5018572211265564 norm:0.0025113862939178944 max memory_allocated 22561.97119140625 
[2025-03-22 17:56:48 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.4975843131542206 norm:0.002004388952627778 max memory_allocated 22561.97119140625 
[2025-03-22 17:57:20 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.4949774742126465 norm:0.0018747231224551797 max memory_allocated 22561.97119140625 
[2025-03-22 17:57:53 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.49269041419029236 norm:0.0017029085429385304 max memory_allocated 22561.97119140625 
[2025-03-22 17:58:25 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.49091631174087524 norm:0.0016096922336146235 max memory_allocated 22561.97119140625 
[2025-03-22 17:58:58 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.4896095395088196 norm:0.0015454229433089495 max memory_allocated 22561.97119140625 
[2025-03-22 17:59:31 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.4885917901992798 norm:0.0014759076293557882 max memory_allocated 22561.97119140625 
[2025-03-22 18:00:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.4877651035785675 norm:0.001439346233382821 max memory_allocated 22561.97119140625 
[2025-03-22 18:00:36 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.4868320822715759 norm:0.0013833064585924149 max memory_allocated 22561.97119140625 
[2025-03-22 18:01:08 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.4861427843570709 norm:0.0013096577022224665 max memory_allocated 22561.97119140625 
[2025-03-22 18:01:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.4855935573577881 norm:0.0012896588305011392 max memory_allocated 22561.97119140625 
[2025-03-22 18:02:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.4851441979408264 norm:0.0012529705418273807 max memory_allocated 22561.97119140625 
[2025-03-22 18:02:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.4848383069038391 norm:0.0012369382893666625 max memory_allocated 22561.97119140625 
[2025-03-22 18:03:19 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.48454686999320984 norm:0.0012274411274120212 max memory_allocated 22561.97119140625 
[2025-03-22 18:03:51 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.48420509696006775 norm:0.0012106127105653286 max memory_allocated 22561.97119140625 
[2025-03-22 18:04:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.48386386036872864 norm:0.0011986354365944862 max memory_allocated 22561.97119140625 
[2025-03-22 18:04:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 18:05:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.6478626132011414 norm:0.03661462664604187 max memory_allocated 22562.14306640625 
[2025-03-22 18:05:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.6197315454483032 norm:0.018830647692084312 max memory_allocated 22562.14306640625 
[2025-03-22 18:06:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.5876559615135193 norm:0.008667238056659698 max memory_allocated 22562.14306640625 
[2025-03-22 18:06:45 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.5722148418426514 norm:0.004796254448592663 max memory_allocated 22562.14306640625 
[2025-03-22 18:07:18 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.5658222436904907 norm:0.0035190044436603785 max memory_allocated 22562.14306640625 
[2025-03-22 18:07:50 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.5615769624710083 norm:0.002851244993507862 max memory_allocated 22562.14306640625 
[2025-03-22 18:08:23 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.5591214895248413 norm:0.002661406761035323 max memory_allocated 22562.14306640625 
[2025-03-22 18:08:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.5570956468582153 norm:0.002451010514050722 max memory_allocated 22562.14306640625 
[2025-03-22 18:09:28 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.5555113554000854 norm:0.0022499647457152605 max memory_allocated 22562.14306640625 
[2025-03-22 18:10:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.5544291734695435 norm:0.002135392976924777 max memory_allocated 22562.14306640625 
[2025-03-22 18:10:33 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.5535107254981995 norm:0.001982090063393116 max memory_allocated 22562.14306640625 
[2025-03-22 18:11:06 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.5522069334983826 norm:0.0015698233619332314 max memory_allocated 22562.14306640625 
[2025-03-22 18:11:38 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.5513030290603638 norm:0.0011653457768261433 max memory_allocated 22562.14306640625 
[2025-03-22 18:12:11 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.5508487224578857 norm:0.0011636489070951939 max memory_allocated 22562.14306640625 
[2025-03-22 18:12:44 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.5505325794219971 norm:0.0011657732538878918 max memory_allocated 22562.14306640625 
[2025-03-22 18:13:16 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.5502468347549438 norm:0.0011865742271766067 max memory_allocated 22562.14306640625 
[2025-03-22 18:13:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.5498775243759155 norm:0.0011545016895979643 max memory_allocated 22562.14306640625 
[2025-03-22 18:14:22 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.5494469404220581 norm:0.001130804419517517 max memory_allocated 22562.14306640625 
[2025-03-22 18:14:54 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.5492620468139648 norm:0.001125773531384766 max memory_allocated 22562.14306640625 
[2025-03-22 18:15:27 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.5491000413894653 norm:0.001113429549150169 max memory_allocated 22562.14306640625 
[2025-03-22 18:15:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 18:16:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.7126969695091248 norm:0.02693745121359825 max memory_allocated 22562.31494140625 
[2025-03-22 18:16:44 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.6910327672958374 norm:0.015211434103548527 max memory_allocated 22562.31494140625 
[2025-03-22 18:17:16 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.6666924953460693 norm:0.008535695262253284 max memory_allocated 22562.31494140625 
[2025-03-22 18:17:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.6550202369689941 norm:0.005755212157964706 max memory_allocated 22562.31494140625 
[2025-03-22 18:18:21 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.6502037644386292 norm:0.004587158095091581 max memory_allocated 22562.31494140625 
[2025-03-22 18:18:54 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.6466657519340515 norm:0.003730323165655136 max memory_allocated 22562.31494140625 
[2025-03-22 18:19:26 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.6437562704086304 norm:0.0030922689475119114 max memory_allocated 22562.31494140625 
[2025-03-22 18:19:59 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.641338586807251 norm:0.0025888467207551003 max memory_allocated 22562.31494140625 
[2025-03-22 18:20:31 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.6396134495735168 norm:0.0022669746540486813 max memory_allocated 22562.31494140625 
[2025-03-22 18:21:04 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.6381645798683167 norm:0.0020021828822791576 max memory_allocated 22562.31494140625 
[2025-03-22 18:21:36 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.6369373202323914 norm:0.0018231167923659086 max memory_allocated 22562.31494140625 
[2025-03-22 18:22:09 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.6354911923408508 norm:0.0013200902612879872 max memory_allocated 22562.31494140625 
[2025-03-22 18:22:41 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.6346243023872375 norm:0.001153855468146503 max memory_allocated 22562.31494140625 
[2025-03-22 18:23:14 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.6340463161468506 norm:0.001156780170276761 max memory_allocated 22562.31494140625 
[2025-03-22 18:23:46 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.6334532499313354 norm:0.0011496772058308125 max memory_allocated 22562.31494140625 
[2025-03-22 18:24:19 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.6330912709236145 norm:0.001149793155491352 max memory_allocated 22562.31494140625 
[2025-03-22 18:24:51 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.6327755451202393 norm:0.0011434666812419891 max memory_allocated 22562.31494140625 
[2025-03-22 18:25:24 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.6324669122695923 norm:0.0011415802873671055 max memory_allocated 22562.31494140625 
[2025-03-22 18:25:57 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.6321098804473877 norm:0.0011262153275310993 max memory_allocated 22562.31494140625 
[2025-03-22 18:26:29 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.6319531202316284 norm:0.0011255115969106555 max memory_allocated 22562.31494140625 
[2025-03-22 18:26:38 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 18:27:14 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.8391953706741333 norm:0.02849012240767479 max memory_allocated 22562.48681640625 
[2025-03-22 18:27:46 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.8150508403778076 norm:0.016320327296853065 max memory_allocated 22562.48681640625 
[2025-03-22 18:28:19 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.7864322066307068 norm:0.009090611711144447 max memory_allocated 22562.48681640625 
[2025-03-22 18:28:51 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.7699778079986572 norm:0.005559276789426804 max memory_allocated 22562.48681640625 
[2025-03-22 18:29:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.7646946907043457 norm:0.004370892886072397 max memory_allocated 22562.48681640625 
[2025-03-22 18:29:56 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.7611348628997803 norm:0.003725352231413126 max memory_allocated 22562.48681640625 
[2025-03-22 18:30:29 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.7580064535140991 norm:0.0031715508084744215 max memory_allocated 22562.48681640625 
[2025-03-22 18:31:02 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.7559616565704346 norm:0.002933115465566516 max memory_allocated 22562.48681640625 
[2025-03-22 18:31:34 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.753583550453186 norm:0.001710875192657113 max memory_allocated 22562.48681640625 
[2025-03-22 18:32:07 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.7524293065071106 norm:0.0014951251214370131 max memory_allocated 22562.48681640625 
[2025-03-22 18:32:40 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.7516341209411621 norm:0.0014600401045754552 max memory_allocated 22562.48681640625 
[2025-03-22 18:33:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.7508566379547119 norm:0.0014436490600928664 max memory_allocated 22562.48681640625 
[2025-03-22 18:33:45 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.7500729560852051 norm:0.001405289163812995 max memory_allocated 22562.48681640625 
[2025-03-22 18:34:17 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.749679684638977 norm:0.0014125755988061428 max memory_allocated 22562.48681640625 
[2025-03-22 18:34:50 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.7493726015090942 norm:0.0013945659156888723 max memory_allocated 22562.48681640625 
[2025-03-22 18:35:22 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.7489582300186157 norm:0.0013700987910851836 max memory_allocated 22562.48681640625 
[2025-03-22 18:35:55 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.7486293911933899 norm:0.001362489303573966 max memory_allocated 22562.48681640625 
[2025-03-22 18:36:27 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.7482913732528687 norm:0.0013587744906544685 max memory_allocated 22562.48681640625 
[2025-03-22 18:37:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.7481197118759155 norm:0.0013388694496825337 max memory_allocated 22562.48681640625 
[2025-03-22 18:37:33 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.7479341626167297 norm:0.0013271891511976719 max memory_allocated 22562.48681640625 
[2025-03-22 18:37:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 18:38:17 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.9487040042877197 norm:0.011323971673846245 max memory_allocated 22562.65869140625 
[2025-03-22 18:38:49 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.9368587732315063 norm:0.007530622184276581 max memory_allocated 22562.65869140625 
[2025-03-22 18:39:22 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.9147932529449463 norm:0.0037744673900306225 max memory_allocated 22562.65869140625 
[2025-03-22 18:39:54 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.9030909538269043 norm:0.0026658042334020138 max memory_allocated 22562.65869140625 
[2025-03-22 18:40:27 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.896703839302063 norm:0.001932188170030713 max memory_allocated 22562.65869140625 
[2025-03-22 18:40:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.8935503959655762 norm:0.0017955275252461433 max memory_allocated 22562.65869140625 
[2025-03-22 18:41:32 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.8914127349853516 norm:0.0017808054108172655 max memory_allocated 22562.65869140625 
[2025-03-22 18:42:04 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.8899459838867188 norm:0.0017325363587588072 max memory_allocated 22562.65869140625 
[2025-03-22 18:42:37 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.8887426853179932 norm:0.0017053834162652493 max memory_allocated 22562.65869140625 
[2025-03-22 18:43:09 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.8877696990966797 norm:0.0016783007886260748 max memory_allocated 22562.65869140625 
[2025-03-22 18:43:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.8869727849960327 norm:0.0016653335187584162 max memory_allocated 22562.65869140625 
[2025-03-22 18:44:15 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.8863548636436462 norm:0.0016256826929748058 max memory_allocated 22562.65869140625 
[2025-03-22 18:44:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.8861643671989441 norm:0.0016042852075770497 max memory_allocated 22562.65869140625 
[2025-03-22 18:45:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.8853107690811157 norm:0.0015464113093912601 max memory_allocated 22562.65869140625 
[2025-03-22 18:45:52 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.8850527405738831 norm:0.0015557121951133013 max memory_allocated 22562.65869140625 
[2025-03-22 18:46:25 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.8846428394317627 norm:0.0015234129969030619 max memory_allocated 22562.65869140625 
[2025-03-22 18:46:58 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.8841171264648438 norm:0.0015566081274300814 max memory_allocated 22562.65869140625 
[2025-03-22 18:47:30 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.8838744759559631 norm:0.0015241160290315747 max memory_allocated 22562.65869140625 
[2025-03-22 18:48:03 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.8833887577056885 norm:0.0015260898508131504 max memory_allocated 22562.65869140625 
[2025-03-22 18:48:36 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.883427083492279 norm:0.0015389244072139263 max memory_allocated 22562.65869140625 
[2025-03-22 18:48:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 18:49:20 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:1.0885659456253052 norm:0.012113917618989944 max memory_allocated 22562.83056640625 
[2025-03-22 18:49:52 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:1.0690619945526123 norm:0.0070574404671788216 max memory_allocated 22562.83056640625 
[2025-03-22 18:50:25 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:1.046491265296936 norm:0.0038261788431555033 max memory_allocated 22562.83056640625 
[2025-03-22 18:50:58 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:1.0376290082931519 norm:0.0026789880357682705 max memory_allocated 22562.83056640625 
[2025-03-22 18:51:30 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:1.0327168703079224 norm:0.0019928738474845886 max memory_allocated 22562.83056640625 
[2025-03-22 18:52:03 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:1.0292878150939941 norm:0.001740738982334733 max memory_allocated 22562.83056640625 
[2025-03-22 18:52:36 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:1.0266497135162354 norm:0.0015688387211412191 max memory_allocated 22562.83056640625 
[2025-03-22 18:53:08 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:1.0246347188949585 norm:0.0013812422985211015 max memory_allocated 22562.83056640625 
[2025-03-22 18:53:41 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:1.023359775543213 norm:0.001341980998404324 max memory_allocated 22562.83056640625 
[2025-03-22 18:54:13 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:1.0223255157470703 norm:0.0013144853292033076 max memory_allocated 22562.83056640625 
[2025-03-22 18:54:46 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:1.0214909315109253 norm:0.001296960050240159 max memory_allocated 22562.83056640625 
[2025-03-22 18:55:19 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:1.0209522247314453 norm:0.0012805042788386345 max memory_allocated 22562.83056640625 
[2025-03-22 18:55:51 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:1.020573377609253 norm:0.0012603293871507049 max memory_allocated 22562.83056640625 
[2025-03-22 18:56:24 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:1.020222544670105 norm:0.0012481480371206999 max memory_allocated 22562.83056640625 
[2025-03-22 18:56:56 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:1.0200337171554565 norm:0.0012411397183313966 max memory_allocated 22562.83056640625 
[2025-03-22 18:57:29 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:1.0197585821151733 norm:0.001222030958160758 max memory_allocated 22562.83056640625 
[2025-03-22 18:58:01 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:1.019568681716919 norm:0.0012114576529711485 max memory_allocated 22562.83056640625 
[2025-03-22 18:58:34 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:1.0192749500274658 norm:0.001208471367135644 max memory_allocated 22562.83056640625 
[2025-03-22 18:59:06 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:1.0189692974090576 norm:0.0012050102232024074 max memory_allocated 22562.83056640625 
[2025-03-22 18:59:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:1.0188666582107544 norm:0.0012065388727933168 max memory_allocated 22562.83056640625 
[2025-03-22 18:59:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 19:00:23 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:1.2745952606201172 norm:0.02134624868631363 max memory_allocated 22563.00244140625 
[2025-03-22 19:00:55 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:1.2512155771255493 norm:0.013806571252644062 max memory_allocated 22563.00244140625 
[2025-03-22 19:01:28 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:1.223243236541748 norm:0.007452900521457195 max memory_allocated 22563.00244140625 
[2025-03-22 19:02:00 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:1.2088152170181274 norm:0.004234531428664923 max memory_allocated 22563.00244140625 
[2025-03-22 19:02:33 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:1.2023762464523315 norm:0.0032508079893887043 max memory_allocated 22563.00244140625 
[2025-03-22 19:03:06 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:1.1964443922042847 norm:0.0020119990222156048 max memory_allocated 22563.00244140625 
[2025-03-22 19:03:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:1.192929983139038 norm:0.0015991778345778584 max memory_allocated 22563.00244140625 
[2025-03-22 19:04:11 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:1.1907919645309448 norm:0.0015371732879430056 max memory_allocated 22563.00244140625 
[2025-03-22 19:04:43 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:1.189371109008789 norm:0.001492994255386293 max memory_allocated 22563.00244140625 
[2025-03-22 19:05:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:1.1881496906280518 norm:0.0014535988448187709 max memory_allocated 22563.00244140625 
[2025-03-22 19:05:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:1.187292218208313 norm:0.0014422960812225938 max memory_allocated 22563.00244140625 
[2025-03-22 19:06:21 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:1.1866722106933594 norm:0.0014357391046360135 max memory_allocated 22563.00244140625 
[2025-03-22 19:06:54 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:1.1862528324127197 norm:0.0014150126371532679 max memory_allocated 22563.00244140625 
[2025-03-22 19:07:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:1.1860156059265137 norm:0.0014160318532958627 max memory_allocated 22563.00244140625 
[2025-03-22 19:07:59 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:1.1857367753982544 norm:0.001413618796505034 max memory_allocated 22563.00244140625 
[2025-03-22 19:08:32 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:1.1854438781738281 norm:0.0014034495688974857 max memory_allocated 22563.00244140625 
[2025-03-22 19:09:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:1.1851251125335693 norm:0.0013962555676698685 max memory_allocated 22563.00244140625 
[2025-03-22 19:09:37 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:1.18503737449646 norm:0.0014017142821103334 max memory_allocated 22563.00244140625 
[2025-03-22 19:10:09 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:1.1848745346069336 norm:0.0013795812847092748 max memory_allocated 22563.00244140625 
[2025-03-22 19:10:42 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:1.1845976114273071 norm:0.00137520267162472 max memory_allocated 22563.00244140625 
[2025-03-22 19:10:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 19:11:26 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:1.4368574619293213 norm:0.030347466468811035 max memory_allocated 22563.17431640625 
[2025-03-22 19:11:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:1.417128086090088 norm:0.020747900009155273 max memory_allocated 22563.17431640625 
[2025-03-22 19:12:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:1.3900405168533325 norm:0.013625064864754677 max memory_allocated 22563.17431640625 
[2025-03-22 19:13:04 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:1.3773627281188965 norm:0.010398472659289837 max memory_allocated 22563.17431640625 
[2025-03-22 19:13:37 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:1.3691208362579346 norm:0.007721340283751488 max memory_allocated 22563.17431640625 
[2025-03-22 19:14:09 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:1.3627970218658447 norm:0.006038682535290718 max memory_allocated 22563.17431640625 
[2025-03-22 19:14:42 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:1.3590264320373535 norm:0.005281487014144659 max memory_allocated 22563.17431640625 
[2025-03-22 19:15:15 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:1.3563573360443115 norm:0.004647864494472742 max memory_allocated 22563.17431640625 
[2025-03-22 19:15:47 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:1.3546229600906372 norm:0.004198660608381033 max memory_allocated 22563.17431640625 
[2025-03-22 19:16:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:1.3532745838165283 norm:0.0037510767579078674 max memory_allocated 22563.17431640625 
[2025-03-22 19:16:52 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:1.352108120918274 norm:0.003426458453759551 max memory_allocated 22563.17431640625 
[2025-03-22 19:17:25 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:1.3510640859603882 norm:0.003120675217360258 max memory_allocated 22563.17431640625 
[2025-03-22 19:17:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:1.3502261638641357 norm:0.0028786673210561275 max memory_allocated 22563.17431640625 
[2025-03-22 19:18:30 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:1.3494384288787842 norm:0.002652540570124984 max memory_allocated 22563.17431640625 
[2025-03-22 19:19:02 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:1.3486820459365845 norm:0.002459139097481966 max memory_allocated 22563.17431640625 
[2025-03-22 19:19:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:1.3480360507965088 norm:0.0022977532353252172 max memory_allocated 22563.17431640625 
[2025-03-22 19:20:07 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:1.347529649734497 norm:0.0021580758038908243 max memory_allocated 22563.17431640625 
[2025-03-22 19:20:40 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:1.346687912940979 norm:0.0019112017471343279 max memory_allocated 22563.17431640625 
[2025-03-22 19:21:12 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:1.3462663888931274 norm:0.0018352004699409008 max memory_allocated 22563.17431640625 
[2025-03-22 19:21:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:1.3459768295288086 norm:0.0017757316818460822 max memory_allocated 22563.17431640625 
[2025-03-22 19:21:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 19:22:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:1.609650731086731 norm:0.0094725601375103 max memory_allocated 22563.34619140625 
[2025-03-22 19:23:01 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:1.5897679328918457 norm:0.005349849816411734 max memory_allocated 22563.34619140625 
[2025-03-22 19:23:34 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:1.5639262199401855 norm:0.002834056504070759 max memory_allocated 22563.34619140625 
[2025-03-22 19:24:07 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:1.5527139902114868 norm:0.0017859538784250617 max memory_allocated 22563.34619140625 
[2025-03-22 19:24:39 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:1.547148585319519 norm:0.0015277897473424673 max memory_allocated 22563.34619140625 
[2025-03-22 19:25:12 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:1.5430140495300293 norm:0.0013982163509353995 max memory_allocated 22563.34619140625 
[2025-03-22 19:25:44 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:1.5400992631912231 norm:0.001340222079306841 max memory_allocated 22563.34619140625 
[2025-03-22 19:26:17 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:1.5380432605743408 norm:0.0013032068964093924 max memory_allocated 22563.34619140625 
[2025-03-22 19:26:50 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:1.5365029573440552 norm:0.001277535455301404 max memory_allocated 22563.34619140625 
[2025-03-22 19:27:22 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:1.5353248119354248 norm:0.001258166041225195 max memory_allocated 22563.34619140625 
[2025-03-22 19:27:55 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:1.5344280004501343 norm:0.0012434831587597728 max memory_allocated 22563.34619140625 
[2025-03-22 19:28:27 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:1.5338059663772583 norm:0.0012383704306557775 max memory_allocated 22563.34619140625 
[2025-03-22 19:29:00 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:1.5331777334213257 norm:0.001224856823682785 max memory_allocated 22563.34619140625 
[2025-03-22 19:29:33 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:1.532677412033081 norm:0.0012184205697849393 max memory_allocated 22563.34619140625 
[2025-03-22 19:30:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:1.5322554111480713 norm:0.0012084082700312138 max memory_allocated 22563.34619140625 
[2025-03-22 19:30:38 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:1.53189218044281 norm:0.0012066634371876717 max memory_allocated 22563.34619140625 
[2025-03-22 19:31:11 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:1.5316452980041504 norm:0.0011947164312005043 max memory_allocated 22563.34619140625 
[2025-03-22 19:31:43 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:1.531335473060608 norm:0.001190616050735116 max memory_allocated 22563.34619140625 
[2025-03-22 19:32:16 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:1.5311369895935059 norm:0.0011866962304338813 max memory_allocated 22563.34619140625 
[2025-03-22 19:32:48 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:1.531093716621399 norm:0.0011933505302295089 max memory_allocated 22563.34619140625 
[2025-03-22 19:32:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 19:33:33 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:1.8298308849334717 norm:0.026682794094085693 max memory_allocated 22563.51806640625 
[2025-03-22 19:34:05 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:1.7995672225952148 norm:0.013535910286009312 max memory_allocated 22563.51806640625 
[2025-03-22 19:34:38 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:1.7681432962417603 norm:0.0062882499769330025 max memory_allocated 22563.51806640625 
[2025-03-22 19:35:10 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:1.7546252012252808 norm:0.0036651890259236097 max memory_allocated 22563.51806640625 
[2025-03-22 19:35:43 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:1.7468903064727783 norm:0.0028600236400961876 max memory_allocated 22563.51806640625 
[2025-03-22 19:36:15 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:1.7411407232284546 norm:0.002303109737113118 max memory_allocated 22563.51806640625 
[2025-03-22 19:36:48 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:1.7373493909835815 norm:0.0019615197088569403 max memory_allocated 22563.51806640625 
[2025-03-22 19:37:20 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:1.7347055673599243 norm:0.0017356413882225752 max memory_allocated 22563.51806640625 
[2025-03-22 19:37:53 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:1.732844352722168 norm:0.0015868842601776123 max memory_allocated 22563.51806640625 
[2025-03-22 19:38:25 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:1.731501579284668 norm:0.0014737098244950175 max memory_allocated 22563.51806640625 
[2025-03-22 19:38:58 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:1.7304761409759521 norm:0.0013958640629425645 max memory_allocated 22563.51806640625 
[2025-03-22 19:39:30 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:1.7296090126037598 norm:0.0013437788002192974 max memory_allocated 22563.51806640625 
[2025-03-22 19:40:03 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:1.7289366722106934 norm:0.0013096841285005212 max memory_allocated 22563.51806640625 
[2025-03-22 19:40:36 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:1.7283906936645508 norm:0.00128549849614501 max memory_allocated 22563.51806640625 
[2025-03-22 19:41:08 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:1.727827787399292 norm:0.0012603660579770803 max memory_allocated 22563.51806640625 
[2025-03-22 19:41:41 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:1.7273616790771484 norm:0.0012386429589241743 max memory_allocated 22563.51806640625 
[2025-03-22 19:42:13 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:1.7269576787948608 norm:0.0012325318530201912 max memory_allocated 22563.51806640625 
[2025-03-22 19:42:46 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:1.726633906364441 norm:0.0012220299104228616 max memory_allocated 22563.51806640625 
[2025-03-22 19:43:19 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:1.7264636754989624 norm:0.001218763180077076 max memory_allocated 22563.51806640625 
[2025-03-22 19:43:51 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:1.7261980772018433 norm:0.0012191599234938622 max memory_allocated 22563.51806640625 
[2025-03-22 19:44:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 19:44:36 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:2.037407636642456 norm:0.02267792448401451 max memory_allocated 22563.68994140625 
[2025-03-22 19:45:08 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:2.002997875213623 norm:0.010279182344675064 max memory_allocated 22563.68994140625 
[2025-03-22 19:45:41 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:1.9714930057525635 norm:0.006330007221549749 max memory_allocated 22563.68994140625 
[2025-03-22 19:46:13 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:1.9540560245513916 norm:0.0027371197938919067 max memory_allocated 22563.68994140625 
[2025-03-22 19:46:46 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:1.9467216730117798 norm:0.002157148439437151 max memory_allocated 22563.68994140625 
[2025-03-22 19:47:19 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:1.9415382146835327 norm:0.0018789332825690508 max memory_allocated 22563.68994140625 
[2025-03-22 19:47:51 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:1.9376081228256226 norm:0.0016958820633590221 max memory_allocated 22563.68994140625 
[2025-03-22 19:48:24 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:1.935091257095337 norm:0.0015553859993815422 max memory_allocated 22563.68994140625 
[2025-03-22 19:48:56 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:1.9335155487060547 norm:0.0015088736545294523 max memory_allocated 22563.68994140625 
[2025-03-22 19:49:29 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:1.932250738143921 norm:0.0014826073311269283 max memory_allocated 22563.68994140625 
[2025-03-22 19:50:02 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:1.9310733079910278 norm:0.0014175643445923924 max memory_allocated 22563.68994140625 
[2025-03-22 19:50:34 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:1.930151343345642 norm:0.001485779182985425 max memory_allocated 22563.68994140625 
[2025-03-22 19:51:07 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:1.9296549558639526 norm:0.0013767783530056477 max memory_allocated 22563.68994140625 
[2025-03-22 19:51:39 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:1.9287158250808716 norm:0.0014910607133060694 max memory_allocated 22563.68994140625 
[2025-03-22 19:52:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:1.9277561902999878 norm:0.0013723971787840128 max memory_allocated 22563.68994140625 
[2025-03-22 19:52:44 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:1.927865743637085 norm:0.0013282920699566603 max memory_allocated 22563.68994140625 
[2025-03-22 19:53:17 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:1.9276944398880005 norm:0.0013142614625394344 max memory_allocated 22563.68994140625 
[2025-03-22 19:53:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:1.9270745515823364 norm:0.0013130808947607875 max memory_allocated 22563.68994140625 
[2025-03-22 19:54:22 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:1.9268481731414795 norm:0.0012884042225778103 max memory_allocated 22563.68994140625 
[2025-03-22 19:54:54 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:1.9265433549880981 norm:0.001291737426072359 max memory_allocated 22563.68994140625 
[2025-03-22 19:55:03 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 19:55:06 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 19:55:39 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:2.341603994369507 norm:0.06042574718594551 max memory_allocated 22564.09228515625 
[2025-03-22 19:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:2.3077683448791504 norm:0.052804503589868546 max memory_allocated 22564.09228515625 
[2025-03-22 19:56:44 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:2.2713277339935303 norm:0.041233569383621216 max memory_allocated 22564.09228515625 
[2025-03-22 19:57:17 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:2.2501039505004883 norm:0.033523935824632645 max memory_allocated 22564.09228515625 
[2025-03-22 19:57:50 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:2.238417863845825 norm:0.02823958918452263 max memory_allocated 22564.09228515625 
[2025-03-22 19:58:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:2.229351282119751 norm:0.023695874959230423 max memory_allocated 22564.09228515625 
[2025-03-22 19:58:55 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:2.2225003242492676 norm:0.02095857262611389 max memory_allocated 22564.09228515625 
[2025-03-22 19:59:28 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:2.2169172763824463 norm:0.019337907433509827 max memory_allocated 22564.09228515625 
[2025-03-22 20:00:01 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:2.212862968444824 norm:0.017890824005007744 max memory_allocated 22564.09228515625 
[2025-03-22 20:00:33 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:2.2097742557525635 norm:0.01712767593562603 max memory_allocated 22564.09228515625 
[2025-03-22 20:01:06 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:2.206404209136963 norm:0.016934754326939583 max memory_allocated 22564.09228515625 
[2025-03-22 20:01:39 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:2.204350471496582 norm:0.016709234565496445 max memory_allocated 22564.09228515625 
[2025-03-22 20:02:12 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:2.2015721797943115 norm:0.016846297308802605 max memory_allocated 22564.09228515625 
[2025-03-22 20:02:45 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:2.199277400970459 norm:0.016033727675676346 max memory_allocated 22564.09228515625 
[2025-03-22 20:03:17 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:2.1962409019470215 norm:0.014935722574591637 max memory_allocated 22564.09228515625 
[2025-03-22 20:03:50 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:2.1944828033447266 norm:0.01446162536740303 max memory_allocated 22564.09228515625 
[2025-03-22 20:04:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:2.192713737487793 norm:0.01410470437258482 max memory_allocated 22564.09228515625 
[2025-03-22 20:04:56 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:2.1914961338043213 norm:0.014095000922679901 max memory_allocated 22564.09228515625 
[2025-03-22 20:05:28 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:2.189997434616089 norm:0.014084286987781525 max memory_allocated 22564.09228515625 
[2025-03-22 20:06:01 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:2.189276695251465 norm:0.01415493618696928 max memory_allocated 22564.09228515625 
[2025-03-22 20:06:10 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 20:06:13 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:06:45 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:2.654221534729004 norm:0.061755843460559845 max memory_allocated 22564.26416015625 
[2025-03-22 20:07:18 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:2.608966112136841 norm:0.051058828830718994 max memory_allocated 22564.26416015625 
[2025-03-22 20:07:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:2.5578243732452393 norm:0.03839436545968056 max memory_allocated 22564.26416015625 
[2025-03-22 20:08:23 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:2.530379056930542 norm:0.030620288103818893 max memory_allocated 22564.26416015625 
[2025-03-22 20:08:56 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:2.5165040493011475 norm:0.02473355643451214 max memory_allocated 22564.26416015625 
[2025-03-22 20:09:29 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:2.5066535472869873 norm:0.020248450338840485 max memory_allocated 22564.26416015625 
[2025-03-22 20:10:01 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:2.500352621078491 norm:0.01749148964881897 max memory_allocated 22564.26416015625 
[2025-03-22 20:10:34 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:2.4961395263671875 norm:0.015626396983861923 max memory_allocated 22564.26416015625 
[2025-03-22 20:11:07 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:2.493405342102051 norm:0.015134626068174839 max memory_allocated 22564.26416015625 
[2025-03-22 20:11:39 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:2.4905519485473633 norm:0.014354236423969269 max memory_allocated 22564.26416015625 
[2025-03-22 20:12:12 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:2.4891281127929688 norm:0.014376605860888958 max memory_allocated 22564.26416015625 
[2025-03-22 20:12:45 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:2.487525463104248 norm:0.014456630684435368 max memory_allocated 22564.26416015625 
[2025-03-22 20:13:17 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:2.486963987350464 norm:0.014357423409819603 max memory_allocated 22564.26416015625 
[2025-03-22 20:13:50 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:2.4860756397247314 norm:0.01480251643806696 max memory_allocated 22564.26416015625 
[2025-03-22 20:14:23 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:2.4852287769317627 norm:0.01425088383257389 max memory_allocated 22564.26416015625 
[2025-03-22 20:14:56 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:2.483330249786377 norm:0.013334083370864391 max memory_allocated 22564.26416015625 
[2025-03-22 20:15:28 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:2.4825305938720703 norm:0.01309618167579174 max memory_allocated 22564.26416015625 
[2025-03-22 20:16:01 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:2.4817893505096436 norm:0.013001706451177597 max memory_allocated 22564.26416015625 
[2025-03-22 20:16:34 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:2.4810614585876465 norm:0.012662901543080807 max memory_allocated 22564.26416015625 
[2025-03-22 20:17:07 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:2.4805893898010254 norm:0.01263511274009943 max memory_allocated 22564.26416015625 
[2025-03-22 20:17:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 20:17:18 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:17:51 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:3.4961538314819336 norm:0.1266118884086609 max memory_allocated 22564.43603515625 
[2025-03-22 20:18:24 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:3.3968212604522705 norm:0.09279844164848328 max memory_allocated 22564.43603515625 
[2025-03-22 20:18:57 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:3.257974147796631 norm:0.0666741356253624 max memory_allocated 22564.43603515625 
[2025-03-22 20:19:29 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:3.184903860092163 norm:0.05303872004151344 max memory_allocated 22564.43603515625 
[2025-03-22 20:20:02 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:3.1548824310302734 norm:0.04654068872332573 max memory_allocated 22564.43603515625 
[2025-03-22 20:20:35 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:3.134821891784668 norm:0.04566498100757599 max memory_allocated 22564.43603515625 
[2025-03-22 20:21:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:3.1214113235473633 norm:0.04706627130508423 max memory_allocated 22564.43603515625 
[2025-03-22 20:21:40 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:3.1084253787994385 norm:0.04732349514961243 max memory_allocated 22564.43603515625 
[2025-03-22 20:22:13 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:3.1017372608184814 norm:0.05155819654464722 max memory_allocated 22564.43603515625 
[2025-03-22 20:22:46 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:3.0957729816436768 norm:0.052588652819395065 max memory_allocated 22564.43603515625 
[2025-03-22 20:23:18 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:3.0889065265655518 norm:0.053735725581645966 max memory_allocated 22564.43603515625 
[2025-03-22 20:23:51 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:3.0856754779815674 norm:0.05315642058849335 max memory_allocated 22564.43603515625 
[2025-03-22 20:24:24 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:3.079193115234375 norm:0.0516246035695076 max memory_allocated 22564.43603515625 
[2025-03-22 20:24:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:3.076852560043335 norm:0.05237191542983055 max memory_allocated 22564.43603515625 
[2025-03-22 20:25:29 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:3.071200370788574 norm:0.051989685744047165 max memory_allocated 22564.43603515625 
[2025-03-22 20:26:02 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:3.065277099609375 norm:0.047371167689561844 max memory_allocated 22564.43603515625 
[2025-03-22 20:26:34 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:3.066039562225342 norm:0.04737413302063942 max memory_allocated 22564.43603515625 
[2025-03-22 20:27:07 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:3.067807912826538 norm:0.04709017276763916 max memory_allocated 22564.43603515625 
[2025-03-22 20:27:40 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:3.06154203414917 norm:0.047762371599674225 max memory_allocated 22564.43603515625 
[2025-03-22 20:28:12 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:3.0627360343933105 norm:0.04531276971101761 max memory_allocated 22564.43603515625 
[2025-03-22 20:28:21 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 20:28:24 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:28:57 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:6.549492835998535 norm:0.5031402111053467 max memory_allocated 22564.60791015625 
[2025-03-22 20:29:29 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:6.066298484802246 norm:0.38714420795440674 max memory_allocated 22564.60791015625 
[2025-03-22 20:30:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:5.732799530029297 norm:0.32767635583877563 max memory_allocated 22564.60791015625 
[2025-03-22 20:30:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:5.5345563888549805 norm:0.3184646964073181 max memory_allocated 22564.60791015625 
[2025-03-22 20:31:07 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:5.432512283325195 norm:0.2901206910610199 max memory_allocated 22564.60791015625 
[2025-03-22 20:31:40 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:5.366590976715088 norm:0.26916587352752686 max memory_allocated 22564.60791015625 
[2025-03-22 20:32:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:5.314549446105957 norm:0.24848388135433197 max memory_allocated 22564.60791015625 
[2025-03-22 20:32:46 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:5.267630100250244 norm:0.22745759785175323 max memory_allocated 22564.60791015625 
[2025-03-22 20:33:18 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:5.226542949676514 norm:0.2130427360534668 max memory_allocated 22564.60791015625 
[2025-03-22 20:33:51 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:5.1895341873168945 norm:0.20037959516048431 max memory_allocated 22564.60791015625 
[2025-03-22 20:34:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:5.167662143707275 norm:0.20126616954803467 max memory_allocated 22564.60791015625 
[2025-03-22 20:34:57 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:5.144105434417725 norm:0.19399279356002808 max memory_allocated 22564.60791015625 
[2025-03-22 20:35:29 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:5.132845878601074 norm:0.19972103834152222 max memory_allocated 22564.60791015625 
[2025-03-22 20:36:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:5.122678756713867 norm:0.1986357718706131 max memory_allocated 22564.60791015625 
[2025-03-22 20:36:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:5.1056036949157715 norm:0.19362202286720276 max memory_allocated 22564.60791015625 
[2025-03-22 20:37:08 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:5.088059425354004 norm:0.18159040808677673 max memory_allocated 22564.60791015625 
[2025-03-22 20:37:40 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:5.0773749351501465 norm:0.17489640414714813 max memory_allocated 22564.60791015625 
[2025-03-22 20:38:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:5.066936016082764 norm:0.1708359569311142 max memory_allocated 22564.60791015625 
[2025-03-22 20:38:46 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:5.066122531890869 norm:0.17861953377723694 max memory_allocated 22564.60791015625 
[2025-03-22 20:39:18 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:5.0651535987854 norm:0.17994043231010437 max memory_allocated 22564.60791015625 
[2025-03-22 20:39:28 root] (main_calibration_a.py 369): INFO 21238.560463428497
[2025-03-22 20:39:32 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-22 20:40:40 root] (main_calibration_a.py 158): INFO wikitext2 : 8.345358848571777
[2025-03-22 20:40:40 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-22 20:42:25 root] (main_calibration_a.py 158): INFO c4 : 11.716571807861328
[2025-03-22 20:42:35 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/piqa/6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011 (last modified on Tue Feb 18 03:13:08 2025) since it couldn't be found locally at piqa., or remotely on the Hugging Face Hub.
[2025-03-22 22:36:57 root] (main_calibration_a.py 169): INFO {'wikitext2': 8.345358848571777, 'c4': 11.716571807861328, 'results': {'piqa': {'acc': 0.7040261153427638, 'acc_stderr': 0.010650414317148115, 'acc_norm': 0.7040261153427638, 'acc_norm_stderr': 0.010650414317148126}, 'hellaswag': {'acc': 0.45339573790081655, 'acc_stderr': 0.004968058944472163, 'acc_norm': 0.6015733917546305, 'acc_norm_stderr': 0.0048857359633469}, 'winogrande': {'acc': 0.5556432517758485, 'acc_stderr': 0.013965196769083556}, 'arc_challenge': {'acc': 0.3267918088737201, 'acc_stderr': 0.013706665975587342, 'acc_norm': 0.33532423208191126, 'acc_norm_stderr': 0.013796182947785562}, 'boolq': {'acc': 0.6470948012232416, 'acc_stderr': 0.008358060743875658}, 'arc_easy': {'acc': 0.5744949494949495, 'acc_stderr': 0.01014527118259102, 'acc_norm': 0.4659090909090909, 'acc_norm_stderr': 0.010235908103438687}}, 'versions': {'piqa': 0, 'hellaswag': 0, 'winogrande': 0, 'arc_challenge': 0, 'boolq': 1, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 22:36:57 root] (main_calibration_a.py 172): INFO 32.68,57.45,64.71,45.34,70.40,55.56
