[2025-03-22 01:14:23 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/Llama-2-7b-hf-w4a4-8', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=8)
[2025-03-22 01:17:55 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 01:17:55 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:17:55 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 01:17:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:18:00 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:18:32 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.09577421844005585 norm:0.10465800762176514 max memory_allocated 22563.31689453125 
[2025-03-22 01:19:04 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.04516758769750595 norm:0.06549373269081116 max memory_allocated 22563.31689453125 
[2025-03-22 01:19:37 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.033474400639534 norm:0.043191876262426376 max memory_allocated 22563.31689453125 
[2025-03-22 01:20:09 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.028887737542390823 norm:0.038510777056217194 max memory_allocated 22563.31689453125 
[2025-03-22 01:20:41 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.026896489784121513 norm:0.03485265001654625 max memory_allocated 22563.31689453125 
[2025-03-22 01:21:14 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.025566348806023598 norm:0.03303204104304314 max memory_allocated 22563.31689453125 
[2025-03-22 01:21:46 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.02392072230577469 norm:0.028684262186288834 max memory_allocated 22563.31689453125 
[2025-03-22 01:22:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.02306358702480793 norm:0.0256938636302948 max memory_allocated 22563.31689453125 
[2025-03-22 01:22:51 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.02235601842403412 norm:0.02237059734761715 max memory_allocated 22563.31689453125 
[2025-03-22 01:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.021952161565423012 norm:0.0201470497995615 max memory_allocated 22563.31689453125 
[2025-03-22 01:23:56 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.02134757488965988 norm:0.017723795026540756 max memory_allocated 22563.31689453125 
[2025-03-22 01:24:29 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.02119584009051323 norm:0.01594005897641182 max memory_allocated 22563.31689453125 
[2025-03-22 01:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.020898031070828438 norm:0.014295765198767185 max memory_allocated 22563.31689453125 
[2025-03-22 01:25:34 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.020825985819101334 norm:0.012795236892998219 max memory_allocated 22563.31689453125 
[2025-03-22 01:26:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.02044493332505226 norm:0.01137841958552599 max memory_allocated 22563.31689453125 
[2025-03-22 01:26:39 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.02014796994626522 norm:0.010348986834287643 max memory_allocated 22563.31689453125 
[2025-03-22 01:27:12 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.019986119121313095 norm:0.009521237574517727 max memory_allocated 22563.31689453125 
[2025-03-22 01:27:44 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.019792776554822922 norm:0.009080386720597744 max memory_allocated 22563.31689453125 
[2025-03-22 01:28:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.019770849496126175 norm:0.008520776405930519 max memory_allocated 22563.31689453125 
[2025-03-22 01:28:49 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.019643984735012054 norm:0.007909288629889488 max memory_allocated 22563.31689453125 
[2025-03-22 01:28:58 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:29:01 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:29:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.33246487379074097 norm:0.21074004471302032 max memory_allocated 22563.48876953125 
[2025-03-22 01:30:06 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.22047986090183258 norm:0.12777523696422577 max memory_allocated 22563.48876953125 
[2025-03-22 01:30:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.173599511384964 norm:0.07450980693101883 max memory_allocated 22563.48876953125 
[2025-03-22 01:31:11 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.15411338210105896 norm:0.05934707447886467 max memory_allocated 22563.48876953125 
[2025-03-22 01:31:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.14728416502475739 norm:0.05726172402501106 max memory_allocated 22563.48876953125 
[2025-03-22 01:32:16 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.13984018564224243 norm:0.055503666400909424 max memory_allocated 22563.48876953125 
[2025-03-22 01:32:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.13322791457176208 norm:0.05234405770897865 max memory_allocated 22563.48876953125 
[2025-03-22 01:33:21 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.13123637437820435 norm:0.05193495750427246 max memory_allocated 22563.48876953125 
[2025-03-22 01:33:54 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.12744928896427155 norm:0.046495094895362854 max memory_allocated 22563.48876953125 
[2025-03-22 01:34:26 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.1279560923576355 norm:0.04671908915042877 max memory_allocated 22563.48876953125 
[2025-03-22 01:34:59 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.12390254437923431 norm:0.043280038982629776 max memory_allocated 22563.48876953125 
[2025-03-22 01:35:31 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.12258848547935486 norm:0.041208431124687195 max memory_allocated 22563.48876953125 
[2025-03-22 01:36:04 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.12206467241048813 norm:0.03916278854012489 max memory_allocated 22563.48876953125 
[2025-03-22 01:36:36 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.12172912061214447 norm:0.04126400500535965 max memory_allocated 22563.48876953125 
[2025-03-22 01:37:09 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.12161172926425934 norm:0.033772680908441544 max memory_allocated 22563.48876953125 
[2025-03-22 01:37:41 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.11913291364908218 norm:0.034472305327653885 max memory_allocated 22563.48876953125 
[2025-03-22 01:38:14 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.11871860176324844 norm:0.03435265272855759 max memory_allocated 22563.48876953125 
[2025-03-22 01:38:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.11852574348449707 norm:0.0319158174097538 max memory_allocated 22563.48876953125 
[2025-03-22 01:39:19 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.1200374886393547 norm:0.03261526674032211 max memory_allocated 22563.48876953125 
[2025-03-22 01:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.11656232178211212 norm:0.03201206028461456 max memory_allocated 22563.48876953125 
[2025-03-22 01:40:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:40:05 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:40:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.21346257627010345 norm:0.09559714049100876 max memory_allocated 22563.66064453125 
[2025-03-22 01:41:11 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.17724454402923584 norm:0.08667910844087601 max memory_allocated 22563.66064453125 
[2025-03-22 01:41:43 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.15729787945747375 norm:0.058725059032440186 max memory_allocated 22563.66064453125 
[2025-03-22 01:42:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.1490602046251297 norm:0.04569973051548004 max memory_allocated 22563.66064453125 
[2025-03-22 01:42:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.1442539393901825 norm:0.038740843534469604 max memory_allocated 22563.66064453125 
[2025-03-22 01:43:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.14151933789253235 norm:0.03338461369276047 max memory_allocated 22563.66064453125 
[2025-03-22 01:43:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.1392524689435959 norm:0.027407942339777946 max memory_allocated 22563.66064453125 
[2025-03-22 01:44:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.13675926625728607 norm:0.021566012874245644 max memory_allocated 22563.66064453125 
[2025-03-22 01:45:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.13469265401363373 norm:0.0176494549959898 max memory_allocated 22563.66064453125 
[2025-03-22 01:45:33 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.1332794576883316 norm:0.014146314933896065 max memory_allocated 22563.66064453125 
[2025-03-22 01:46:05 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.1322496086359024 norm:0.011788935400545597 max memory_allocated 22563.66064453125 
[2025-03-22 01:46:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.1318058967590332 norm:0.011232603341341019 max memory_allocated 22563.66064453125 
[2025-03-22 01:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.13127350807189941 norm:0.010191929526627064 max memory_allocated 22563.66064453125 
[2025-03-22 01:47:44 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.13095137476921082 norm:0.009935982525348663 max memory_allocated 22563.66064453125 
[2025-03-22 01:48:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.13027891516685486 norm:0.008542975410819054 max memory_allocated 22563.66064453125 
[2025-03-22 01:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.1303829848766327 norm:0.008711690083146095 max memory_allocated 22563.66064453125 
[2025-03-22 01:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.1305873692035675 norm:0.009079045616090298 max memory_allocated 22563.66064453125 
[2025-03-22 01:49:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.13018541038036346 norm:0.008568168617784977 max memory_allocated 22563.66064453125 
[2025-03-22 01:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.12992501258850098 norm:0.007851448841392994 max memory_allocated 22563.66064453125 
[2025-03-22 01:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.13048644363880157 norm:0.008008002303540707 max memory_allocated 22563.66064453125 
[2025-03-22 01:51:09 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 01:51:45 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.2346312403678894 norm:0.02546948939561844 max memory_allocated 22563.66064453125 
[2025-03-22 01:52:17 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.20369771122932434 norm:0.009208126924932003 max memory_allocated 22563.66064453125 
[2025-03-22 01:52:50 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.18209412693977356 norm:0.0042886557057499886 max memory_allocated 22563.66064453125 
[2025-03-22 01:53:22 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.1750180423259735 norm:0.0029243051540106535 max memory_allocated 22563.66064453125 
[2025-03-22 01:53:54 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.17230001091957092 norm:0.0027732602320611477 max memory_allocated 22563.66064453125 
[2025-03-22 01:54:27 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.1700783520936966 norm:0.0022144061513245106 max memory_allocated 22563.66064453125 
[2025-03-22 01:54:59 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.16860078275203705 norm:0.001958369743078947 max memory_allocated 22563.66064453125 
[2025-03-22 01:55:32 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.16759389638900757 norm:0.0017633411334827542 max memory_allocated 22563.66064453125 
[2025-03-22 01:56:04 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.166982039809227 norm:0.00160650082398206 max memory_allocated 22563.66064453125 
[2025-03-22 01:56:37 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.16647866368293762 norm:0.0015321852406486869 max memory_allocated 22563.66064453125 
[2025-03-22 01:57:09 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.16609260439872742 norm:0.0014579023700207472 max memory_allocated 22563.66064453125 
[2025-03-22 01:57:42 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.16618776321411133 norm:0.0014507501618936658 max memory_allocated 22563.66064453125 
[2025-03-22 01:58:15 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.16588249802589417 norm:0.0013822945766150951 max memory_allocated 22563.66064453125 
[2025-03-22 01:58:47 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.16584551334381104 norm:0.0013725365279242396 max memory_allocated 22563.66064453125 
[2025-03-22 01:59:20 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.16594785451889038 norm:0.0013549062423408031 max memory_allocated 22563.66064453125 
[2025-03-22 01:59:52 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.1656646877527237 norm:0.0012961591128259897 max memory_allocated 22563.66064453125 
[2025-03-22 02:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.1657772660255432 norm:0.0013015931472182274 max memory_allocated 22563.66064453125 
[2025-03-22 02:00:57 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.1662808507680893 norm:0.001362329930998385 max memory_allocated 22563.66064453125 
[2025-03-22 02:01:30 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.16648009419441223 norm:0.0012673066230490804 max memory_allocated 22563.66064453125 
[2025-03-22 02:02:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.16612479090690613 norm:0.0012678031343966722 max memory_allocated 22563.66064453125 
[2025-03-22 02:02:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:02:47 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.28766945004463196 norm:0.03897104412317276 max memory_allocated 22563.66064453125 
[2025-03-22 02:03:20 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.24825657904148102 norm:0.012954170815646648 max memory_allocated 22563.66064453125 
[2025-03-22 02:03:52 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.21759237349033356 norm:0.004618850536644459 max memory_allocated 22563.66064453125 
[2025-03-22 02:04:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.20856156945228577 norm:0.0030370403546839952 max memory_allocated 22563.66064453125 
[2025-03-22 02:04:57 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.20396509766578674 norm:0.0022643411066383123 max memory_allocated 22563.66064453125 
[2025-03-22 02:05:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.20102806389331818 norm:0.0019331690855324268 max memory_allocated 22563.66064453125 
[2025-03-22 02:06:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.19901388883590698 norm:0.0017002747626975179 max memory_allocated 22563.66064453125 
[2025-03-22 02:06:35 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.19788317382335663 norm:0.0015650000423192978 max memory_allocated 22563.66064453125 
[2025-03-22 02:07:07 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.19708457589149475 norm:0.0014434640761464834 max memory_allocated 22563.66064453125 
[2025-03-22 02:07:40 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.19643810391426086 norm:0.0013268054462969303 max memory_allocated 22563.66064453125 
[2025-03-22 02:08:13 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.19600838422775269 norm:0.00129667017608881 max memory_allocated 22563.66064453125 
[2025-03-22 02:08:45 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.1956881582736969 norm:0.0012824072036892176 max memory_allocated 22563.66064453125 
[2025-03-22 02:09:17 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.19538560509681702 norm:0.0012542097829282284 max memory_allocated 22563.66064453125 
[2025-03-22 02:09:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.19515937566757202 norm:0.0012514663394540548 max memory_allocated 22563.66064453125 
[2025-03-22 02:10:22 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.19493751227855682 norm:0.0012075933627784252 max memory_allocated 22563.66064453125 
[2025-03-22 02:10:55 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.1947917491197586 norm:0.0012068122159689665 max memory_allocated 22563.66064453125 
[2025-03-22 02:11:27 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.19466358423233032 norm:0.001942364266142249 max memory_allocated 22563.66064453125 
[2025-03-22 02:12:00 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.19447638094425201 norm:0.0011822069063782692 max memory_allocated 22563.66064453125 
[2025-03-22 02:12:32 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.1944824755191803 norm:0.0011655059643089771 max memory_allocated 22563.66064453125 
[2025-03-22 02:13:05 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.19455914199352264 norm:0.0011410671286284924 max memory_allocated 22563.66064453125 
[2025-03-22 02:13:14 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:13:49 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.30828315019607544 norm:0.019532881677150726 max memory_allocated 22563.66064453125 
[2025-03-22 02:14:22 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2754107713699341 norm:0.008521100506186485 max memory_allocated 22563.66064453125 
[2025-03-22 02:14:54 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.24324238300323486 norm:0.0031669926829636097 max memory_allocated 22563.66064453125 
[2025-03-22 02:15:27 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.23265764117240906 norm:0.0021346500143408775 max memory_allocated 22563.66064453125 
[2025-03-22 02:15:59 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.22791947424411774 norm:0.0018438613042235374 max memory_allocated 22563.66064453125 
[2025-03-22 02:16:32 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.2249031364917755 norm:0.001673404243774712 max memory_allocated 22563.66064453125 
[2025-03-22 02:17:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.22310759127140045 norm:0.0015438752016052604 max memory_allocated 22563.66064453125 
[2025-03-22 02:17:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.2218341827392578 norm:0.0014488049782812595 max memory_allocated 22563.66064453125 
[2025-03-22 02:18:10 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.22099868953227997 norm:0.0014140272978693247 max memory_allocated 22563.66064453125 
[2025-03-22 02:18:42 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.22033970057964325 norm:0.0013482520589604974 max memory_allocated 22563.66064453125 
[2025-03-22 02:19:15 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.2197623997926712 norm:0.00131902820430696 max memory_allocated 22563.66064453125 
[2025-03-22 02:19:47 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.21937568485736847 norm:0.0012965796049684286 max memory_allocated 22563.66064453125 
[2025-03-22 02:20:20 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.21910949051380157 norm:0.0012739297235384583 max memory_allocated 22563.66064453125 
[2025-03-22 02:20:53 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.21887370944023132 norm:0.0012316086795181036 max memory_allocated 22563.66064453125 
[2025-03-22 02:21:25 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.21863991022109985 norm:0.0011998382396996021 max memory_allocated 22563.66064453125 
[2025-03-22 02:21:58 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.2185867428779602 norm:0.0021472976077347994 max memory_allocated 22563.66064453125 
[2025-03-22 02:22:31 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.2184918224811554 norm:0.0011754321167245507 max memory_allocated 22563.66064453125 
[2025-03-22 02:23:03 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.21860355138778687 norm:0.0012058167485520244 max memory_allocated 22563.66064453125 
[2025-03-22 02:23:36 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.2186724692583084 norm:0.0012337267398834229 max memory_allocated 22563.66064453125 
[2025-03-22 02:24:08 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.21856729686260223 norm:0.0011859892401844263 max memory_allocated 22563.66064453125 
[2025-03-22 02:24:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:24:53 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.3708406388759613 norm:0.03543063998222351 max memory_allocated 22563.66064453125 
[2025-03-22 02:25:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.32200562953948975 norm:0.014034630730748177 max memory_allocated 22563.66064453125 
[2025-03-22 02:25:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.2835896611213684 norm:0.006302478723227978 max memory_allocated 22563.66064453125 
[2025-03-22 02:26:30 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.2695528566837311 norm:0.004111978225409985 max memory_allocated 22563.66064453125 
[2025-03-22 02:27:02 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.2626088857650757 norm:0.003120612818747759 max memory_allocated 22563.66064453125 
[2025-03-22 02:27:35 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.2584548592567444 norm:0.0025988759007304907 max memory_allocated 22563.66064453125 
[2025-03-22 02:28:07 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.2555033564567566 norm:0.0022202078253030777 max memory_allocated 22563.66064453125 
[2025-03-22 02:28:40 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.2534533441066742 norm:0.002029722323641181 max memory_allocated 22563.66064453125 
[2025-03-22 02:29:12 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.2519834339618683 norm:0.0017937560332939029 max memory_allocated 22563.66064453125 
[2025-03-22 02:29:45 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.2508544325828552 norm:0.0016799562145024538 max memory_allocated 22563.66064453125 
[2025-03-22 02:30:17 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.2499740719795227 norm:0.0015811817720532417 max memory_allocated 22563.66064453125 
[2025-03-22 02:30:50 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.24983547627925873 norm:0.0015389473410323262 max memory_allocated 22563.66064453125 
[2025-03-22 02:31:23 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.24919655919075012 norm:0.0014036325737833977 max memory_allocated 22563.66064453125 
[2025-03-22 02:31:55 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.2485901117324829 norm:0.0013849921524524689 max memory_allocated 22563.66064453125 
[2025-03-22 02:32:28 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.2481413185596466 norm:0.001337744644843042 max memory_allocated 22563.66064453125 
[2025-03-22 02:33:00 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.2478187382221222 norm:0.001286042737774551 max memory_allocated 22563.66064453125 
[2025-03-22 02:33:33 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.24761703610420227 norm:0.0012456804979592562 max memory_allocated 22563.66064453125 
[2025-03-22 02:34:06 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.24747538566589355 norm:0.0012291341554373503 max memory_allocated 22563.66064453125 
[2025-03-22 02:34:38 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.2474890649318695 norm:0.0012325075222179294 max memory_allocated 22563.66064453125 
[2025-03-22 02:35:11 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.24730752408504486 norm:0.0012118929298594594 max memory_allocated 22563.66064453125 
[2025-03-22 02:35:20 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 02:35:55 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.40808767080307007 norm:0.038113340735435486 max memory_allocated 22563.66064453125 
[2025-03-22 02:36:28 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.3428928256034851 norm:0.013013381510972977 max memory_allocated 22563.66064453125 
[2025-03-22 02:37:00 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.3030247688293457 norm:0.005672870669513941 max memory_allocated 22563.66064453125 
[2025-03-22 02:37:33 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.2898985743522644 norm:0.003605796955525875 max memory_allocated 22563.66064453125 
[2025-03-22 02:38:06 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.28434813022613525 norm:0.0029942644760012627 max memory_allocated 22563.66064453125 
[2025-03-22 02:38:38 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.2803586721420288 norm:0.002445139456540346 max memory_allocated 22563.66064453125 
[2025-03-22 02:39:11 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.2773832082748413 norm:0.0021847705356776714 max memory_allocated 22563.66064453125 
[2025-03-22 02:39:43 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.27551454305648804 norm:0.0020352518185973167 max memory_allocated 22563.66064453125 
[2025-03-22 02:40:16 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.27378329634666443 norm:0.00175709486939013 max memory_allocated 22563.66064453125 
[2025-03-22 02:40:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.2726761996746063 norm:0.0016336941625922918 max memory_allocated 22563.66064453125 
[2025-03-22 02:41:21 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.27152740955352783 norm:0.0015487294876948 max memory_allocated 22563.66064453125 
[2025-03-22 02:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.27090829610824585 norm:0.001526428502984345 max memory_allocated 22563.66064453125 
[2025-03-22 02:42:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.27029678225517273 norm:0.0015094757545739412 max memory_allocated 22563.66064453125 
[2025-03-22 02:42:59 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2697823643684387 norm:0.001497289165854454 max memory_allocated 22563.66064453125 
[2025-03-22 02:43:31 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.26940953731536865 norm:0.001423613983206451 max memory_allocated 22563.66064453125 
[2025-03-22 02:44:04 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.2691740393638611 norm:0.0013704702723771334 max memory_allocated 22563.66064453125 
[2025-03-22 02:44:36 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.26909682154655457 norm:0.001391450292430818 max memory_allocated 22563.66064453125 
[2025-03-22 02:45:09 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.2688758671283722 norm:0.0013727827463299036 max memory_allocated 22563.66064453125 
[2025-03-22 02:45:41 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.26879391074180603 norm:0.0013268438633531332 max memory_allocated 22563.66064453125 
[2025-03-22 02:46:14 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.26868608593940735 norm:0.0012623982038348913 max memory_allocated 22563.66064453125 
[2025-03-22 02:46:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 02:46:58 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.3958454132080078 norm:0.023652415722608566 max memory_allocated 22563.77001953125 
[2025-03-22 02:47:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.3530316650867462 norm:0.00880500115454197 max memory_allocated 22563.77001953125 
[2025-03-22 02:48:03 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.31751689314842224 norm:0.0036881943233311176 max memory_allocated 22563.77001953125 
[2025-03-22 02:48:35 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.3051210045814514 norm:0.0022922849748283625 max memory_allocated 22563.77001953125 
[2025-03-22 02:49:08 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.2994856536388397 norm:0.0019063168438151479 max memory_allocated 22563.77001953125 
[2025-03-22 02:49:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.2955174446105957 norm:0.0017190168146044016 max memory_allocated 22563.77001953125 
[2025-03-22 02:50:13 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.292850136756897 norm:0.0015888864872977138 max memory_allocated 22563.77001953125 
[2025-03-22 02:50:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.290842205286026 norm:0.0014694869751110673 max memory_allocated 22563.77001953125 
[2025-03-22 02:51:18 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.28949403762817383 norm:0.0013864078791812062 max memory_allocated 22563.77001953125 
[2025-03-22 02:51:51 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.28855741024017334 norm:0.0013481427449733019 max memory_allocated 22563.77001953125 
[2025-03-22 02:52:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.2878726124763489 norm:0.0012647050898522139 max memory_allocated 22563.77001953125 
[2025-03-22 02:52:56 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.28733882308006287 norm:0.0012306788703426719 max memory_allocated 22563.77001953125 
[2025-03-22 02:53:29 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.286908894777298 norm:0.00119629199616611 max memory_allocated 22563.77001953125 
[2025-03-22 02:54:02 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.28663912415504456 norm:0.0011684491764754057 max memory_allocated 22563.77001953125 
[2025-03-22 02:54:34 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.28632432222366333 norm:0.0011788704432547092 max memory_allocated 22563.77001953125 
[2025-03-22 02:55:07 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.28616252541542053 norm:0.0011496141087263823 max memory_allocated 22563.77001953125 
[2025-03-22 02:55:39 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.2859507203102112 norm:0.0011139045236632228 max memory_allocated 22563.77001953125 
[2025-03-22 02:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.28574198484420776 norm:0.001121937995776534 max memory_allocated 22563.77001953125 
[2025-03-22 02:56:45 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.2856445014476776 norm:0.0011223319452255964 max memory_allocated 22563.77001953125 
[2025-03-22 02:57:17 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.285502165555954 norm:0.0011097658425569534 max memory_allocated 22563.77001953125 
[2025-03-22 02:57:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 02:58:01 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.44063204526901245 norm:0.03984132781624794 max memory_allocated 22563.94189453125 
[2025-03-22 02:58:34 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.37471288442611694 norm:0.013595165684819221 max memory_allocated 22563.94189453125 
[2025-03-22 02:59:06 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.3314821422100067 norm:0.004526110831648111 max memory_allocated 22563.94189453125 
[2025-03-22 02:59:39 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.31912124156951904 norm:0.002897202270105481 max memory_allocated 22563.94189453125 
[2025-03-22 03:00:11 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.31370076537132263 norm:0.002476007677614689 max memory_allocated 22563.94189453125 
[2025-03-22 03:00:44 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3100874722003937 norm:0.0021693611051887274 max memory_allocated 22563.94189453125 
[2025-03-22 03:01:16 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.30734992027282715 norm:0.0019682627171278 max memory_allocated 22563.94189453125 
[2025-03-22 03:01:49 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.30539971590042114 norm:0.0018201415659859776 max memory_allocated 22563.94189453125 
[2025-03-22 03:02:21 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.30391961336135864 norm:0.0016838307492434978 max memory_allocated 22563.94189453125 
[2025-03-22 03:02:54 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3029457926750183 norm:0.001560111646540463 max memory_allocated 22563.94189453125 
[2025-03-22 03:03:27 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.3021338880062103 norm:0.0014853491447865963 max memory_allocated 22563.94189453125 
[2025-03-22 03:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.30149951577186584 norm:0.001413155347108841 max memory_allocated 22563.94189453125 
[2025-03-22 03:04:32 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.3009723424911499 norm:0.0013346548657864332 max memory_allocated 22563.94189453125 
[2025-03-22 03:05:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.3006010949611664 norm:0.0012635730672627687 max memory_allocated 22563.94189453125 
[2025-03-22 03:05:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.30019694566726685 norm:0.0011902196565642953 max memory_allocated 22563.94189453125 
[2025-03-22 03:06:10 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.29988399147987366 norm:0.001140938838943839 max memory_allocated 22563.94189453125 
[2025-03-22 03:06:42 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.2996647357940674 norm:0.0010921311331912875 max memory_allocated 22563.94189453125 
[2025-03-22 03:07:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.29945576190948486 norm:0.0010747368214651942 max memory_allocated 22563.94189453125 
[2025-03-22 03:07:48 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.29926174879074097 norm:0.0010900873458012938 max memory_allocated 22563.94189453125 
[2025-03-22 03:08:20 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.2990288734436035 norm:0.0011013736948370934 max memory_allocated 22563.94189453125 
[2025-03-22 03:08:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:09:05 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.4198874235153198 norm:0.03160155192017555 max memory_allocated 22564.11376953125 
[2025-03-22 03:09:37 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.38095563650131226 norm:0.014329523779451847 max memory_allocated 22564.11376953125 
[2025-03-22 03:10:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.34563151001930237 norm:0.005918566137552261 max memory_allocated 22564.11376953125 
[2025-03-22 03:10:42 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3300149738788605 norm:0.0024424486327916384 max memory_allocated 22564.11376953125 
[2025-03-22 03:11:15 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.32277530431747437 norm:0.0014987956965342164 max memory_allocated 22564.11376953125 
[2025-03-22 03:11:48 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3184022903442383 norm:0.0013472221326082945 max memory_allocated 22564.11376953125 
[2025-03-22 03:12:20 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3154681622982025 norm:0.001258363015949726 max memory_allocated 22564.11376953125 
[2025-03-22 03:12:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.31331390142440796 norm:0.0011814411263912916 max memory_allocated 22564.11376953125 
[2025-03-22 03:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.31181976199150085 norm:0.0011119998525828123 max memory_allocated 22564.11376953125 
[2025-03-22 03:13:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.3107110261917114 norm:0.0010884146904572845 max memory_allocated 22564.11376953125 
[2025-03-22 03:14:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.30992358922958374 norm:0.0010472279973328114 max memory_allocated 22564.11376953125 
[2025-03-22 03:15:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.3092808127403259 norm:0.000998462550342083 max memory_allocated 22564.11376953125 
[2025-03-22 03:15:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.3088269531726837 norm:0.0009895509574562311 max memory_allocated 22564.11376953125 
[2025-03-22 03:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3084944188594818 norm:0.0009879269637167454 max memory_allocated 22564.11376953125 
[2025-03-22 03:16:40 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3081852197647095 norm:0.0009906094055622816 max memory_allocated 22564.11376953125 
[2025-03-22 03:17:13 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.3080318868160248 norm:0.0009695166372694075 max memory_allocated 22564.11376953125 
[2025-03-22 03:17:45 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.3077879250049591 norm:0.0009676610352471471 max memory_allocated 22564.11376953125 
[2025-03-22 03:18:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3075631260871887 norm:0.0009618915501050651 max memory_allocated 22564.11376953125 
[2025-03-22 03:18:50 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.30743077397346497 norm:0.0009623164078220725 max memory_allocated 22564.11376953125 
[2025-03-22 03:19:23 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.3072727620601654 norm:0.0009709768928587437 max memory_allocated 22564.11376953125 
[2025-03-22 03:19:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 03:20:07 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.4243718683719635 norm:0.022304534912109375 max memory_allocated 22564.28564453125 
[2025-03-22 03:20:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.37669235467910767 norm:0.008932546712458134 max memory_allocated 22564.28564453125 
[2025-03-22 03:21:13 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3441043794155121 norm:0.004207613877952099 max memory_allocated 22564.28564453125 
[2025-03-22 03:21:45 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.33073097467422485 norm:0.0021100062876939774 max memory_allocated 22564.28564453125 
[2025-03-22 03:22:18 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.32478460669517517 norm:0.0016093633603304625 max memory_allocated 22564.28564453125 
[2025-03-22 03:22:50 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.3212125599384308 norm:0.0014831437729299068 max memory_allocated 22564.28564453125 
[2025-03-22 03:23:23 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.3187314569950104 norm:0.0013763431925326586 max memory_allocated 22564.28564453125 
[2025-03-22 03:23:56 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3169120252132416 norm:0.0012705681147053838 max memory_allocated 22564.28564453125 
[2025-03-22 03:24:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.31557661294937134 norm:0.0012174324365332723 max memory_allocated 22564.28564453125 
[2025-03-22 03:25:01 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.3147304356098175 norm:0.0011666793143376708 max memory_allocated 22564.28564453125 
[2025-03-22 03:25:34 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.3140791654586792 norm:0.0011168495984748006 max memory_allocated 22564.28564453125 
[2025-03-22 03:26:06 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.31351709365844727 norm:0.001077548717148602 max memory_allocated 22564.28564453125 
[2025-03-22 03:26:39 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.31304606795310974 norm:0.0010499587515369058 max memory_allocated 22564.28564453125 
[2025-03-22 03:27:12 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.31277284026145935 norm:0.0010243903379887342 max memory_allocated 22564.28564453125 
[2025-03-22 03:27:44 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.31254929304122925 norm:0.000989945256151259 max memory_allocated 22564.28564453125 
[2025-03-22 03:28:17 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.31237152218818665 norm:0.000952055910602212 max memory_allocated 22564.28564453125 
[2025-03-22 03:28:50 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.31217750906944275 norm:0.0009408683981746435 max memory_allocated 22564.28564453125 
[2025-03-22 03:29:22 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.3120078146457672 norm:0.0009353612549602985 max memory_allocated 22564.28564453125 
[2025-03-22 03:29:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.3119480311870575 norm:0.0009397921385243535 max memory_allocated 22564.28564453125 
[2025-03-22 03:30:27 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.31184473633766174 norm:0.0009228113922290504 max memory_allocated 22564.28564453125 
[2025-03-22 03:30:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 03:31:12 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.4116319417953491 norm:0.013656570576131344 max memory_allocated 22564.45751953125 
[2025-03-22 03:31:44 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.3747416138648987 norm:0.005394952837377787 max memory_allocated 22564.45751953125 
[2025-03-22 03:32:17 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.34783750772476196 norm:0.002579732332378626 max memory_allocated 22564.45751953125 
[2025-03-22 03:32:49 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.33611759543418884 norm:0.0017588259652256966 max memory_allocated 22564.45751953125 
[2025-03-22 03:33:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.3308367133140564 norm:0.0013593684416264296 max memory_allocated 22564.45751953125 
[2025-03-22 03:33:54 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.32742342352867126 norm:0.0011918930104002357 max memory_allocated 22564.45751953125 
[2025-03-22 03:34:27 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.32507506012916565 norm:0.0010629324242472649 max memory_allocated 22564.45751953125 
[2025-03-22 03:34:59 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.32347819209098816 norm:0.000993521185591817 max memory_allocated 22564.45751953125 
[2025-03-22 03:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.32236647605895996 norm:0.0009538623271510005 max memory_allocated 22564.45751953125 
[2025-03-22 03:36:04 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.3215334117412567 norm:0.000939361285418272 max memory_allocated 22564.45751953125 
[2025-03-22 03:36:37 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.32081371545791626 norm:0.0008993935189209878 max memory_allocated 22564.45751953125 
[2025-03-22 03:37:09 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.3202548027038574 norm:0.0008804603712633252 max memory_allocated 22564.45751953125 
[2025-03-22 03:37:42 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.31991857290267944 norm:0.0008359724888578057 max memory_allocated 22564.45751953125 
[2025-03-22 03:38:14 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.31956520676612854 norm:0.0008167056948877871 max memory_allocated 22564.45751953125 
[2025-03-22 03:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.31937581300735474 norm:0.000811759615316987 max memory_allocated 22564.45751953125 
[2025-03-22 03:39:20 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.3192009925842285 norm:0.0007937774062156677 max memory_allocated 22564.45751953125 
[2025-03-22 03:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.31906548142433167 norm:0.0007995349005796015 max memory_allocated 22564.45751953125 
[2025-03-22 03:40:25 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.31896400451660156 norm:0.0008125929161906242 max memory_allocated 22564.45751953125 
[2025-03-22 03:40:57 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.3188668191432953 norm:0.0008259794558398426 max memory_allocated 22564.45751953125 
[2025-03-22 03:41:30 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.3187918961048126 norm:0.0008236829307861626 max memory_allocated 22564.45751953125 
[2025-03-22 03:41:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 03:42:14 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.44248512387275696 norm:0.045584917068481445 max memory_allocated 22564.62939453125 
[2025-03-22 03:42:47 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.38481396436691284 norm:0.015969276428222656 max memory_allocated 22564.62939453125 
[2025-03-22 03:43:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.35014578700065613 norm:0.006906900554895401 max memory_allocated 22564.62939453125 
[2025-03-22 03:43:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.3380662500858307 norm:0.004053925164043903 max memory_allocated 22564.62939453125 
[2025-03-22 03:44:25 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.3314281105995178 norm:0.002549155382439494 max memory_allocated 22564.62939453125 
[2025-03-22 03:44:58 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3275867998600006 norm:0.0021622255444526672 max memory_allocated 22564.62939453125 
[2025-03-22 03:45:30 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3244907557964325 norm:0.0018516244599595666 max memory_allocated 22564.62939453125 
[2025-03-22 03:46:03 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.32236334681510925 norm:0.0017259123269468546 max memory_allocated 22564.62939453125 
[2025-03-22 03:46:35 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.32082054018974304 norm:0.0015249934513121843 max memory_allocated 22564.62939453125 
[2025-03-22 03:47:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.3193739652633667 norm:0.0012690059375017881 max memory_allocated 22564.62939453125 
[2025-03-22 03:47:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.31841179728507996 norm:0.001186301582492888 max memory_allocated 22564.62939453125 
[2025-03-22 03:48:13 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.31766456365585327 norm:0.0011418787762522697 max memory_allocated 22564.62939453125 
[2025-03-22 03:48:46 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.31716907024383545 norm:0.0011311112903058529 max memory_allocated 22564.62939453125 
[2025-03-22 03:49:18 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3166874349117279 norm:0.00109808926936239 max memory_allocated 22564.62939453125 
[2025-03-22 03:49:51 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.3163047432899475 norm:0.0010537734488025308 max memory_allocated 22564.62939453125 
[2025-03-22 03:50:23 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3159259557723999 norm:0.0010160334641113877 max memory_allocated 22564.62939453125 
[2025-03-22 03:50:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.31562691926956177 norm:0.0010074615711346269 max memory_allocated 22564.62939453125 
[2025-03-22 03:51:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.31535062193870544 norm:0.0009814039804041386 max memory_allocated 22564.62939453125 
[2025-03-22 03:52:01 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.31516915559768677 norm:0.0009432123624719679 max memory_allocated 22564.62939453125 
[2025-03-22 03:52:33 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.31498318910598755 norm:0.0009522323962301016 max memory_allocated 22564.62939453125 
[2025-03-22 03:52:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 03:53:17 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.4079139232635498 norm:0.016152378171682358 max memory_allocated 22564.80126953125 
[2025-03-22 03:53:50 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.37296611070632935 norm:0.00661767553538084 max memory_allocated 22564.80126953125 
[2025-03-22 03:54:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.3466421663761139 norm:0.002767126075923443 max memory_allocated 22564.80126953125 
[2025-03-22 03:54:55 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.3368593156337738 norm:0.0015348331071436405 max memory_allocated 22564.80126953125 
[2025-03-22 03:55:27 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.3324766755104065 norm:0.0012514539994299412 max memory_allocated 22564.80126953125 
[2025-03-22 03:56:00 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3295029103755951 norm:0.0011435151100158691 max memory_allocated 22564.80126953125 
[2025-03-22 03:56:33 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.32745617628097534 norm:0.0010520400246605277 max memory_allocated 22564.80126953125 
[2025-03-22 03:57:05 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.32589995861053467 norm:0.0009654542664065957 max memory_allocated 22564.80126953125 
[2025-03-22 03:57:38 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3248229920864105 norm:0.0009420840069651604 max memory_allocated 22564.80126953125 
[2025-03-22 03:58:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.32393693923950195 norm:0.000889010087121278 max memory_allocated 22564.80126953125 
[2025-03-22 03:58:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.3232000470161438 norm:0.0008550542988814414 max memory_allocated 22564.80126953125 
[2025-03-22 03:59:16 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.32269662618637085 norm:0.0008497313247062266 max memory_allocated 22564.80126953125 
[2025-03-22 03:59:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.3222518563270569 norm:0.0008442599792033434 max memory_allocated 22564.80126953125 
[2025-03-22 04:00:21 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.3219267725944519 norm:0.0008229223312810063 max memory_allocated 22564.80126953125 
[2025-03-22 04:00:53 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.32164037227630615 norm:0.0008069568430073559 max memory_allocated 22564.80126953125 
[2025-03-22 04:01:26 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.3213787078857422 norm:0.0008041972178034484 max memory_allocated 22564.80126953125 
[2025-03-22 04:01:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.32117602229118347 norm:0.000820558809209615 max memory_allocated 22564.80126953125 
[2025-03-22 04:02:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.3209463953971863 norm:0.0008058494422584772 max memory_allocated 22564.80126953125 
[2025-03-22 04:03:04 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.32080429792404175 norm:0.000785946671385318 max memory_allocated 22564.80126953125 
[2025-03-22 04:03:37 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.32067692279815674 norm:0.0007784909103065729 max memory_allocated 22564.80126953125 
[2025-03-22 04:03:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 04:04:21 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.45088833570480347 norm:0.04161461442708969 max memory_allocated 22564.97314453125 
[2025-03-22 04:04:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.39169907569885254 norm:0.013608885928988457 max memory_allocated 22564.97314453125 
[2025-03-22 04:05:26 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.35445737838745117 norm:0.005374481901526451 max memory_allocated 22564.97314453125 
[2025-03-22 04:05:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.3414272665977478 norm:0.0025472017005085945 max memory_allocated 22564.97314453125 
[2025-03-22 04:06:31 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.3359350860118866 norm:0.001620435155928135 max memory_allocated 22564.97314453125 
[2025-03-22 04:07:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.33269748091697693 norm:0.0014862804673612118 max memory_allocated 22564.97314453125 
[2025-03-22 04:07:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.3302268385887146 norm:0.0013948215637356043 max memory_allocated 22564.97314453125 
[2025-03-22 04:08:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.3284439742565155 norm:0.0013620394747704268 max memory_allocated 22564.97314453125 
[2025-03-22 04:08:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.327151358127594 norm:0.001317585352808237 max memory_allocated 22564.97314453125 
[2025-03-22 04:09:13 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.32608020305633545 norm:0.0012581674382090569 max memory_allocated 22564.97314453125 
[2025-03-22 04:09:46 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.3252497911453247 norm:0.0012014892417937517 max memory_allocated 22564.97314453125 
[2025-03-22 04:10:18 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.3245662450790405 norm:0.0011711122933775187 max memory_allocated 22564.97314453125 
[2025-03-22 04:10:51 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.3240866959095001 norm:0.001144652720540762 max memory_allocated 22564.97314453125 
[2025-03-22 04:11:23 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.32362470030784607 norm:0.0010740087600424886 max memory_allocated 22564.97314453125 
[2025-03-22 04:11:56 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.3232859671115875 norm:0.001035063760355115 max memory_allocated 22564.97314453125 
[2025-03-22 04:12:28 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.32302364706993103 norm:0.0009948633378371596 max memory_allocated 22564.97314453125 
[2025-03-22 04:13:01 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.32273223996162415 norm:0.0009610914858058095 max memory_allocated 22564.97314453125 
[2025-03-22 04:13:34 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.322483628988266 norm:0.0009204935631714761 max memory_allocated 22564.97314453125 
[2025-03-22 04:14:06 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.32231199741363525 norm:0.0009179756743833423 max memory_allocated 22564.97314453125 
[2025-03-22 04:14:39 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.3221409320831299 norm:0.0009112890693359077 max memory_allocated 22564.97314453125 
[2025-03-22 04:14:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 04:15:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.4545198976993561 norm:0.04350638762116432 max memory_allocated 22565.14501953125 
[2025-03-22 04:15:56 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.40769875049591064 norm:0.017926834523677826 max memory_allocated 22565.14501953125 
[2025-03-22 04:16:28 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.3696056008338928 norm:0.007612968795001507 max memory_allocated 22565.14501953125 
[2025-03-22 04:17:01 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.3527683913707733 norm:0.003421717556193471 max memory_allocated 22565.14501953125 
[2025-03-22 04:17:34 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.34713107347488403 norm:0.0022563107777386904 max memory_allocated 22565.14501953125 
[2025-03-22 04:18:06 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.34387698769569397 norm:0.001968852709978819 max memory_allocated 22565.14501953125 
[2025-03-22 04:18:39 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.3412763476371765 norm:0.001745574758388102 max memory_allocated 22565.14501953125 
[2025-03-22 04:19:11 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.3392792046070099 norm:0.0016207259614020586 max memory_allocated 22565.14501953125 
[2025-03-22 04:19:44 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.33757972717285156 norm:0.0014845597324892879 max memory_allocated 22565.14501953125 
[2025-03-22 04:20:16 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.33631622791290283 norm:0.0014347935793921351 max memory_allocated 22565.14501953125 
[2025-03-22 04:20:49 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.335338830947876 norm:0.0013402990298345685 max memory_allocated 22565.14501953125 
[2025-03-22 04:21:21 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.33440113067626953 norm:0.0012876467080786824 max memory_allocated 22565.14501953125 
[2025-03-22 04:21:54 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.3337375819683075 norm:0.001267453539185226 max memory_allocated 22565.14501953125 
[2025-03-22 04:22:26 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.33309200406074524 norm:0.001236498705111444 max memory_allocated 22565.14501953125 
[2025-03-22 04:22:59 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.3324938118457794 norm:0.00122576835565269 max memory_allocated 22565.14501953125 
[2025-03-22 04:23:31 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.3322320878505707 norm:0.0012138887541368604 max memory_allocated 22565.14501953125 
[2025-03-22 04:24:04 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.3317891061306 norm:0.0011682591866701841 max memory_allocated 22565.14501953125 
[2025-03-22 04:24:36 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.3314564824104309 norm:0.0011468629818409681 max memory_allocated 22565.14501953125 
[2025-03-22 04:25:09 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.33122551441192627 norm:0.001097960746847093 max memory_allocated 22565.14501953125 
[2025-03-22 04:25:41 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.3310071527957916 norm:0.0010898783802986145 max memory_allocated 22565.14501953125 
[2025-03-22 04:25:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 04:26:26 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.4288971424102783 norm:0.031852640211582184 max memory_allocated 22565.31689453125 
[2025-03-22 04:26:58 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.39800965785980225 norm:0.014502601698040962 max memory_allocated 22565.31689453125 
[2025-03-22 04:27:31 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.3662220239639282 norm:0.004626335576176643 max memory_allocated 22565.31689453125 
[2025-03-22 04:28:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.3556351363658905 norm:0.0018738366197794676 max memory_allocated 22565.31689453125 
[2025-03-22 04:28:36 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.35205090045928955 norm:0.0015465683536604047 max memory_allocated 22565.31689453125 
[2025-03-22 04:29:09 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.3498142957687378 norm:0.0013861370971426368 max memory_allocated 22565.31689453125 
[2025-03-22 04:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.34806618094444275 norm:0.0013211226323619485 max memory_allocated 22565.31689453125 
[2025-03-22 04:30:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.3466280400753021 norm:0.0013350428780540824 max memory_allocated 22565.31689453125 
[2025-03-22 04:30:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.345536470413208 norm:0.0012874763924628496 max memory_allocated 22565.31689453125 
[2025-03-22 04:31:19 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.3445996940135956 norm:0.0012842940632253885 max memory_allocated 22565.31689453125 
[2025-03-22 04:31:52 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.3438456952571869 norm:0.0012480003060773015 max memory_allocated 22565.31689453125 
[2025-03-22 04:32:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.3431726396083832 norm:0.0011937792878597975 max memory_allocated 22565.31689453125 
[2025-03-22 04:32:57 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.34263044595718384 norm:0.0011411155574023724 max memory_allocated 22565.31689453125 
[2025-03-22 04:33:29 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.3421652913093567 norm:0.0010611612815409899 max memory_allocated 22565.31689453125 
[2025-03-22 04:34:02 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.3417665660381317 norm:0.0010198840172961354 max memory_allocated 22565.31689453125 
[2025-03-22 04:34:35 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.34148433804512024 norm:0.001016335911117494 max memory_allocated 22565.31689453125 
[2025-03-22 04:35:07 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.34127435088157654 norm:0.0009939854498952627 max memory_allocated 22565.31689453125 
[2025-03-22 04:35:40 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.34102317690849304 norm:0.000984165002591908 max memory_allocated 22565.31689453125 
[2025-03-22 04:36:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.34081801772117615 norm:0.0009633175213821232 max memory_allocated 22565.31689453125 
[2025-03-22 04:36:45 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.34062814712524414 norm:0.0009418574045412242 max memory_allocated 22565.31689453125 
[2025-03-22 04:36:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 04:37:29 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.4539024233818054 norm:0.04456299543380737 max memory_allocated 22565.48876953125 
[2025-03-22 04:38:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.426054984331131 norm:0.02001654915511608 max memory_allocated 22565.48876953125 
[2025-03-22 04:38:34 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.3952638506889343 norm:0.00779064791277051 max memory_allocated 22565.48876953125 
[2025-03-22 04:39:06 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.38064175844192505 norm:0.0033970503136515617 max memory_allocated 22565.48876953125 
[2025-03-22 04:39:39 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.37522444128990173 norm:0.0019800893496721983 max memory_allocated 22565.48876953125 
[2025-03-22 04:40:11 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.3723528981208801 norm:0.0013896605232730508 max memory_allocated 22565.48876953125 
[2025-03-22 04:40:44 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.3704322278499603 norm:0.0012807449093088508 max memory_allocated 22565.48876953125 
[2025-03-22 04:41:16 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.3688206374645233 norm:0.0012266295962035656 max memory_allocated 22565.48876953125 
[2025-03-22 04:41:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.367708683013916 norm:0.0012242747470736504 max memory_allocated 22565.48876953125 
[2025-03-22 04:42:21 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.3668094575405121 norm:0.0012134878197684884 max memory_allocated 22565.48876953125 
[2025-03-22 04:42:54 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.36606618762016296 norm:0.0011760741472244263 max memory_allocated 22565.48876953125 
[2025-03-22 04:43:26 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.36546415090560913 norm:0.0011292955605313182 max memory_allocated 22565.48876953125 
[2025-03-22 04:43:59 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.36492547392845154 norm:0.0010952039156109095 max memory_allocated 22565.48876953125 
[2025-03-22 04:44:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.3644042909145355 norm:0.001031909603625536 max memory_allocated 22565.48876953125 
[2025-03-22 04:45:04 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.3639843463897705 norm:0.001003712648525834 max memory_allocated 22565.48876953125 
[2025-03-22 04:45:37 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.36370667815208435 norm:0.0009996594162657857 max memory_allocated 22565.48876953125 
[2025-03-22 04:46:09 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.3635081350803375 norm:0.0009939343435689807 max memory_allocated 22565.48876953125 
[2025-03-22 04:46:42 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.36319226026535034 norm:0.00097148452186957 max memory_allocated 22565.48876953125 
[2025-03-22 04:47:15 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.3632051348686218 norm:0.0009784094290807843 max memory_allocated 22565.48876953125 
[2025-03-22 04:47:47 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.36313095688819885 norm:0.0009791742777451873 max memory_allocated 22565.48876953125 
[2025-03-22 04:47:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 04:48:32 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.4662370979785919 norm:0.03952767327427864 max memory_allocated 22565.66064453125 
[2025-03-22 04:49:04 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.4409627914428711 norm:0.017619188874959946 max memory_allocated 22565.66064453125 
[2025-03-22 04:49:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.41534656286239624 norm:0.007662329822778702 max memory_allocated 22565.66064453125 
[2025-03-22 04:50:09 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.40321046113967896 norm:0.002362392842769623 max memory_allocated 22565.66064453125 
[2025-03-22 04:50:41 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.39956042170524597 norm:0.0011707437224686146 max memory_allocated 22565.66064453125 
[2025-03-22 04:51:14 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.3976776599884033 norm:0.0010911328718066216 max memory_allocated 22565.66064453125 
[2025-03-22 04:51:46 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.39604681730270386 norm:0.0010276418179273605 max memory_allocated 22565.66064453125 
[2025-03-22 04:52:19 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.3945898711681366 norm:0.0009462265879847109 max memory_allocated 22565.66064453125 
[2025-03-22 04:52:51 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.3935072124004364 norm:0.0009352480992674828 max memory_allocated 22565.66064453125 
[2025-03-22 04:53:24 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.3926345407962799 norm:0.0009050742955878377 max memory_allocated 22565.66064453125 
[2025-03-22 04:53:56 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.39193516969680786 norm:0.0008559901616536081 max memory_allocated 22565.66064453125 
[2025-03-22 04:54:29 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.39146673679351807 norm:0.0008583316230215132 max memory_allocated 22565.66064453125 
[2025-03-22 04:55:01 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.3910512924194336 norm:0.0008376289624720812 max memory_allocated 22565.66064453125 
[2025-03-22 04:55:34 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.39064621925354004 norm:0.0008178632124327123 max memory_allocated 22565.66064453125 
[2025-03-22 04:56:06 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.390356183052063 norm:0.0008078344981186092 max memory_allocated 22565.66064453125 
[2025-03-22 04:56:39 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.3900700509548187 norm:0.0007876867894083261 max memory_allocated 22565.66064453125 
[2025-03-22 04:57:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.3897812068462372 norm:0.0007799681043252349 max memory_allocated 22565.66064453125 
[2025-03-22 04:57:44 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.3896227478981018 norm:0.0007778176222927868 max memory_allocated 22565.66064453125 
[2025-03-22 04:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.38943684101104736 norm:0.0007572214235551655 max memory_allocated 22565.66064453125 
[2025-03-22 04:58:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.38930851221084595 norm:0.0007544383988715708 max memory_allocated 22565.66064453125 
[2025-03-22 04:58:58 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 04:59:34 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.4983721077442169 norm:0.023098211735486984 max memory_allocated 22565.83251953125 
[2025-03-22 05:00:06 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.4782899022102356 norm:0.012380378320813179 max memory_allocated 22565.83251953125 
[2025-03-22 05:00:39 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.45551300048828125 norm:0.005711342208087444 max memory_allocated 22565.83251953125 
[2025-03-22 05:01:11 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.44535672664642334 norm:0.0033660195767879486 max memory_allocated 22565.83251953125 
[2025-03-22 05:01:44 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.44168299436569214 norm:0.002701940480619669 max memory_allocated 22565.83251953125 
[2025-03-22 05:02:16 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.43849363923072815 norm:0.0015696871560066938 max memory_allocated 22565.83251953125 
[2025-03-22 05:02:49 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.4364631175994873 norm:0.0013874233700335026 max memory_allocated 22565.83251953125 
[2025-03-22 05:03:22 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.4348544180393219 norm:0.0012855230597779155 max memory_allocated 22565.83251953125 
[2025-03-22 05:03:54 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.43368613719940186 norm:0.0012064258335158229 max memory_allocated 22565.83251953125 
[2025-03-22 05:04:27 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.43264758586883545 norm:0.0011578946141526103 max memory_allocated 22565.83251953125 
[2025-03-22 05:04:59 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.43191301822662354 norm:0.0010981999803334475 max memory_allocated 22565.83251953125 
[2025-03-22 05:05:32 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.43120288848876953 norm:0.0010479288175702095 max memory_allocated 22565.83251953125 
[2025-03-22 05:06:04 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.43069037795066833 norm:0.001042164396494627 max memory_allocated 22565.83251953125 
[2025-03-22 05:06:37 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.4302312433719635 norm:0.0010316552361473441 max memory_allocated 22565.83251953125 
[2025-03-22 05:07:09 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.4299744963645935 norm:0.0010264381999149919 max memory_allocated 22565.83251953125 
[2025-03-22 05:07:42 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.4296921491622925 norm:0.0010174940107390285 max memory_allocated 22565.83251953125 
[2025-03-22 05:08:14 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.4293906092643738 norm:0.0009838403202593327 max memory_allocated 22565.83251953125 
[2025-03-22 05:08:47 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.4291676878929138 norm:0.0009805774316191673 max memory_allocated 22565.83251953125 
[2025-03-22 05:09:19 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.4288412630558014 norm:0.0009660983341746032 max memory_allocated 22565.83251953125 
[2025-03-22 05:09:52 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.42869898676872253 norm:0.0009630719432607293 max memory_allocated 22565.83251953125 
[2025-03-22 05:10:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 05:10:38 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.5212801694869995 norm:0.018097596243023872 max memory_allocated 22566.00439453125 
[2025-03-22 05:11:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.5035730004310608 norm:0.00783436931669712 max memory_allocated 22566.00439453125 
[2025-03-22 05:11:43 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.4895610809326172 norm:0.003942269366234541 max memory_allocated 22566.00439453125 
[2025-03-22 05:12:16 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.482119083404541 norm:0.0014884676784276962 max memory_allocated 22566.00439453125 
[2025-03-22 05:12:48 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.47928717732429504 norm:0.0010067167459055781 max memory_allocated 22566.00439453125 
[2025-03-22 05:13:21 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.47762325406074524 norm:0.0009394604712724686 max memory_allocated 22566.00439453125 
[2025-03-22 05:13:53 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.4761590361595154 norm:0.0009114598506130278 max memory_allocated 22566.00439453125 
[2025-03-22 05:14:26 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.4748682379722595 norm:0.0008720421465113759 max memory_allocated 22566.00439453125 
[2025-03-22 05:14:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.47392797470092773 norm:0.0008478218223899603 max memory_allocated 22566.00439453125 
[2025-03-22 05:15:31 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.47316938638687134 norm:0.0008112473879009485 max memory_allocated 22566.00439453125 
[2025-03-22 05:16:04 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.47259068489074707 norm:0.0007945640245452523 max memory_allocated 22566.00439453125 
[2025-03-22 05:16:36 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.4721846282482147 norm:0.0007794115808792412 max memory_allocated 22566.00439453125 
[2025-03-22 05:17:09 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.4718139171600342 norm:0.0007598472875542939 max memory_allocated 22566.00439453125 
[2025-03-22 05:17:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.4714967906475067 norm:0.0007514557219110429 max memory_allocated 22566.00439453125 
[2025-03-22 05:18:14 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.47121766209602356 norm:0.0007491415599361062 max memory_allocated 22566.00439453125 
[2025-03-22 05:18:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.4710389971733093 norm:0.0007510474533773959 max memory_allocated 22566.00439453125 
[2025-03-22 05:19:19 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.4708530902862549 norm:0.000746416044421494 max memory_allocated 22566.00439453125 
[2025-03-22 05:19:52 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.47075292468070984 norm:0.000730755040422082 max memory_allocated 22566.00439453125 
[2025-03-22 05:20:24 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.4706796407699585 norm:0.0007221567793749273 max memory_allocated 22566.00439453125 
[2025-03-22 05:20:57 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.4705970287322998 norm:0.0007188214221969247 max memory_allocated 22566.00439453125 
[2025-03-22 05:21:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 05:21:42 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:0.5871880650520325 norm:0.016787081956863403 max memory_allocated 22566.17626953125 
[2025-03-22 05:22:15 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:0.5664482116699219 norm:0.006399641744792461 max memory_allocated 22566.17626953125 
[2025-03-22 05:22:47 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:0.5503344535827637 norm:0.003114035353064537 max memory_allocated 22566.17626953125 
[2025-03-22 05:23:19 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:0.5432689189910889 norm:0.00155634933616966 max memory_allocated 22566.17626953125 
[2025-03-22 05:23:52 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:0.5406268239021301 norm:0.0012738327495753765 max memory_allocated 22566.17626953125 
[2025-03-22 05:24:24 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:0.5388225317001343 norm:0.0011274359421804547 max memory_allocated 22566.17626953125 
[2025-03-22 05:24:57 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:0.5373910665512085 norm:0.001099639805033803 max memory_allocated 22566.17626953125 
[2025-03-22 05:25:29 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:0.5360385179519653 norm:0.001129279611632228 max memory_allocated 22566.17626953125 
[2025-03-22 05:26:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:0.5352013111114502 norm:0.0010775619884952903 max memory_allocated 22566.17626953125 
[2025-03-22 05:26:34 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:0.5344619750976562 norm:0.0010919948108494282 max memory_allocated 22566.17626953125 
[2025-03-22 05:27:07 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.5337781310081482 norm:0.001044317614287138 max memory_allocated 22566.17626953125 
[2025-03-22 05:27:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.5332577228546143 norm:0.0010314877144992352 max memory_allocated 22566.17626953125 
[2025-03-22 05:28:12 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.532861053943634 norm:0.001000783871859312 max memory_allocated 22566.17626953125 
[2025-03-22 05:28:44 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.5325104594230652 norm:0.0009654694003984332 max memory_allocated 22566.17626953125 
[2025-03-22 05:29:17 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.5320017337799072 norm:0.0009434468811377883 max memory_allocated 22566.17626953125 
[2025-03-22 05:29:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.5317264199256897 norm:0.0009528337395749986 max memory_allocated 22566.17626953125 
[2025-03-22 05:30:22 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.5317747592926025 norm:0.0009400084963999689 max memory_allocated 22566.17626953125 
[2025-03-22 05:30:55 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.5316594243049622 norm:0.0009045030456036329 max memory_allocated 22566.17626953125 
[2025-03-22 05:31:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.5312613248825073 norm:0.0008910151664167643 max memory_allocated 22566.17626953125 
[2025-03-22 05:32:00 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.5312770009040833 norm:0.0008985883323475718 max memory_allocated 22566.17626953125 
[2025-03-22 05:32:10 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 05:32:45 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:0.6351864337921143 norm:0.008380218409001827 max memory_allocated 22566.34814453125 
[2025-03-22 05:33:18 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:0.6215232610702515 norm:0.004249913152307272 max memory_allocated 22566.34814453125 
[2025-03-22 05:33:50 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:0.6073781251907349 norm:0.0023167699109762907 max memory_allocated 22566.34814453125 
[2025-03-22 05:34:23 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:0.6018666625022888 norm:0.0014333045110106468 max memory_allocated 22566.34814453125 
[2025-03-22 05:34:56 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:0.5989980697631836 norm:0.0010360029991716146 max memory_allocated 22566.34814453125 
[2025-03-22 05:35:28 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:0.5970536470413208 norm:0.0008802285301499069 max memory_allocated 22566.34814453125 
[2025-03-22 05:36:01 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:0.5954179167747498 norm:0.0008057199302129447 max memory_allocated 22566.34814453125 
[2025-03-22 05:36:33 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:0.594227135181427 norm:0.0007698786212131381 max memory_allocated 22566.34814453125 
[2025-03-22 05:37:06 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:0.5933924913406372 norm:0.0007362766191363335 max memory_allocated 22566.34814453125 
[2025-03-22 05:37:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:0.5927066206932068 norm:0.0007242632564157248 max memory_allocated 22566.34814453125 
[2025-03-22 05:38:11 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:0.592227041721344 norm:0.0006997878663241863 max memory_allocated 22566.34814453125 
[2025-03-22 05:38:43 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:0.5918450355529785 norm:0.0006908450159244239 max memory_allocated 22566.34814453125 
[2025-03-22 05:39:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:0.5915161967277527 norm:0.0006842396105639637 max memory_allocated 22566.34814453125 
[2025-03-22 05:39:48 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:0.5912299752235413 norm:0.000680228229612112 max memory_allocated 22566.34814453125 
[2025-03-22 05:40:21 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:0.5910484790802002 norm:0.0006777240196242929 max memory_allocated 22566.34814453125 
[2025-03-22 05:40:53 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:0.5908588171005249 norm:0.0006682372186332941 max memory_allocated 22566.34814453125 
[2025-03-22 05:41:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:0.5906863212585449 norm:0.0006648984854109585 max memory_allocated 22566.34814453125 
[2025-03-22 05:41:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:0.5905123949050903 norm:0.0006593494908884168 max memory_allocated 22566.34814453125 
[2025-03-22 05:42:31 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:0.590389609336853 norm:0.000647595152258873 max memory_allocated 22566.34814453125 
[2025-03-22 05:43:03 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:0.5902609825134277 norm:0.0006422527367249131 max memory_allocated 22566.34814453125 
[2025-03-22 05:43:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 05:43:47 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:0.710639238357544 norm:0.013370898552238941 max memory_allocated 22566.52001953125 
[2025-03-22 05:44:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:0.6934210658073425 norm:0.005967647768557072 max memory_allocated 22566.52001953125 
[2025-03-22 05:44:52 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:0.6773909330368042 norm:0.0029808622784912586 max memory_allocated 22566.52001953125 
[2025-03-22 05:45:25 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:0.6703649163246155 norm:0.0013996697962284088 max memory_allocated 22566.52001953125 
[2025-03-22 05:45:58 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:0.6673102378845215 norm:0.0009064520709216595 max memory_allocated 22566.52001953125 
[2025-03-22 05:46:30 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:0.6651321649551392 norm:0.0008615416591055691 max memory_allocated 22566.52001953125 
[2025-03-22 05:47:03 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:0.6633373498916626 norm:0.000854308542329818 max memory_allocated 22566.52001953125 
[2025-03-22 05:47:35 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:0.6619663238525391 norm:0.0008426931453868747 max memory_allocated 22566.52001953125 
[2025-03-22 05:48:08 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:0.6610222458839417 norm:0.0008105619344860315 max memory_allocated 22566.52001953125 
[2025-03-22 05:48:41 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:0.6602967381477356 norm:0.0007922077202238142 max memory_allocated 22566.52001953125 
[2025-03-22 05:49:13 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:0.6597431898117065 norm:0.0007886140374466777 max memory_allocated 22566.52001953125 
[2025-03-22 05:49:46 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:0.6593331098556519 norm:0.000772604369558394 max memory_allocated 22566.52001953125 
[2025-03-22 05:50:19 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:0.6589921712875366 norm:0.000764741562306881 max memory_allocated 22566.52001953125 
[2025-03-22 05:50:51 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:0.6586889028549194 norm:0.0007436855230480433 max memory_allocated 22566.52001953125 
[2025-03-22 05:51:24 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:0.6584445238113403 norm:0.0007518201600760221 max memory_allocated 22566.52001953125 
[2025-03-22 05:51:56 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:0.6581530570983887 norm:0.0007510820287279785 max memory_allocated 22566.52001953125 
[2025-03-22 05:52:29 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:0.6580318212509155 norm:0.0007537518395110965 max memory_allocated 22566.52001953125 
[2025-03-22 05:53:01 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:0.6578842401504517 norm:0.0007448513060808182 max memory_allocated 22566.52001953125 
[2025-03-22 05:53:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:0.6577683687210083 norm:0.0007394611602649093 max memory_allocated 22566.52001953125 
[2025-03-22 05:54:06 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:0.657570481300354 norm:0.0007302764570340514 max memory_allocated 22566.52001953125 
[2025-03-22 05:54:15 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 05:54:51 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:0.8018662929534912 norm:0.024532495066523552 max memory_allocated 22566.69189453125 
[2025-03-22 05:55:23 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:0.7861555814743042 norm:0.014887253753840923 max memory_allocated 22566.69189453125 
[2025-03-22 05:55:55 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:0.7684034109115601 norm:0.009144308045506477 max memory_allocated 22566.69189453125 
[2025-03-22 05:56:28 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:0.7615657448768616 norm:0.006517481990158558 max memory_allocated 22566.69189453125 
[2025-03-22 05:57:00 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:0.7583366632461548 norm:0.00511923898011446 max memory_allocated 22566.69189453125 
[2025-03-22 05:57:33 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:0.7531156539916992 norm:0.003478855127468705 max memory_allocated 22566.69189453125 
[2025-03-22 05:58:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:0.750232994556427 norm:0.003347009187564254 max memory_allocated 22566.69189453125 
[2025-03-22 05:58:38 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:0.7485675811767578 norm:0.0031169631984084845 max memory_allocated 22566.69189453125 
[2025-03-22 05:59:10 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:0.7474505305290222 norm:0.002886650152504444 max memory_allocated 22566.69189453125 
[2025-03-22 05:59:43 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:0.7459315657615662 norm:0.002230382990092039 max memory_allocated 22566.69189453125 
[2025-03-22 06:00:16 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:0.7453931570053101 norm:0.0024468181654810905 max memory_allocated 22566.69189453125 
[2025-03-22 06:00:48 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:0.7445134520530701 norm:0.0018845514860004187 max memory_allocated 22566.69189453125 
[2025-03-22 06:01:21 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:0.7441168427467346 norm:0.0020771189592778683 max memory_allocated 22566.69189453125 
[2025-03-22 06:01:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:0.7434431314468384 norm:0.00160027458332479 max memory_allocated 22566.69189453125 
[2025-03-22 06:02:26 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:0.7431194186210632 norm:0.0018023495795205235 max memory_allocated 22566.69189453125 
[2025-03-22 06:02:59 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:0.7425428628921509 norm:0.0014245646307244897 max memory_allocated 22566.69189453125 
[2025-03-22 06:03:31 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:0.7421435713768005 norm:0.0015523461624979973 max memory_allocated 22566.69189453125 
[2025-03-22 06:04:04 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:0.7415936589241028 norm:0.0013189183082431555 max memory_allocated 22566.69189453125 
[2025-03-22 06:04:37 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:0.7414735555648804 norm:0.0013351812958717346 max memory_allocated 22566.69189453125 
[2025-03-22 06:05:09 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:0.7411664724349976 norm:0.001280535594560206 max memory_allocated 22566.69189453125 
[2025-03-22 06:05:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 06:05:53 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:0.8910025358200073 norm:0.010052146390080452 max memory_allocated 22566.86376953125 
[2025-03-22 06:06:26 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:0.8722952008247375 norm:0.004994628485292196 max memory_allocated 22566.86376953125 
[2025-03-22 06:06:59 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:0.8525099158287048 norm:0.0021463821176439524 max memory_allocated 22566.86376953125 
[2025-03-22 06:07:31 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:0.8442999124526978 norm:0.001272514695301652 max memory_allocated 22566.86376953125 
[2025-03-22 06:08:04 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:0.8408904075622559 norm:0.00105209369212389 max memory_allocated 22566.86376953125 
[2025-03-22 06:08:36 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:0.8381404876708984 norm:0.0009582050843164325 max memory_allocated 22566.86376953125 
[2025-03-22 06:09:09 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:0.8358530402183533 norm:0.0009003180311992764 max memory_allocated 22566.86376953125 
[2025-03-22 06:09:41 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:0.8342046737670898 norm:0.0008632960962131619 max memory_allocated 22566.86376953125 
[2025-03-22 06:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:0.832895040512085 norm:0.0008422437822446227 max memory_allocated 22566.86376953125 
[2025-03-22 06:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:0.8319212198257446 norm:0.0007928632549010217 max memory_allocated 22566.86376953125 
[2025-03-22 06:11:19 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:0.8312026858329773 norm:0.000769629143178463 max memory_allocated 22566.86376953125 
[2025-03-22 06:11:51 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:0.830647349357605 norm:0.0007600179524160922 max memory_allocated 22566.86376953125 
[2025-03-22 06:12:24 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:0.8302599787712097 norm:0.0007533558527939022 max memory_allocated 22566.86376953125 
[2025-03-22 06:12:56 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:0.8298662304878235 norm:0.0007573885377496481 max memory_allocated 22566.86376953125 
[2025-03-22 06:13:29 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:0.82948899269104 norm:0.0007444475777447224 max memory_allocated 22566.86376953125 
[2025-03-22 06:14:01 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:0.8291652202606201 norm:0.0007433593855239451 max memory_allocated 22566.86376953125 
[2025-03-22 06:14:34 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:0.8288809061050415 norm:0.0007424274226650596 max memory_allocated 22566.86376953125 
[2025-03-22 06:15:07 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:0.8286200165748596 norm:0.0007441937341354787 max memory_allocated 22566.86376953125 
[2025-03-22 06:15:39 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:0.8283442258834839 norm:0.0007502194494009018 max memory_allocated 22566.86376953125 
[2025-03-22 06:16:12 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:0.828176736831665 norm:0.0007593204500153661 max memory_allocated 22566.86376953125 
[2025-03-22 06:16:21 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 06:16:56 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:0.9958891868591309 norm:0.014896967448294163 max memory_allocated 22567.03564453125 
[2025-03-22 06:17:29 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:0.9750816822052002 norm:0.0070124175399541855 max memory_allocated 22567.03564453125 
[2025-03-22 06:18:01 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:0.9548896551132202 norm:0.003591869492083788 max memory_allocated 22567.03564453125 
[2025-03-22 06:18:34 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:0.9464673399925232 norm:0.002273994730785489 max memory_allocated 22567.03564453125 
[2025-03-22 06:19:07 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:0.9415829181671143 norm:0.0009572951239533722 max memory_allocated 22567.03564453125 
[2025-03-22 06:19:39 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:0.9384167194366455 norm:0.0008156330441124737 max memory_allocated 22567.03564453125 
[2025-03-22 06:20:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:0.9358280897140503 norm:0.0007572761387564242 max memory_allocated 22567.03564453125 
[2025-03-22 06:20:44 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:0.9339957237243652 norm:0.0007434063008986413 max memory_allocated 22567.03564453125 
[2025-03-22 06:21:17 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:0.9327700138092041 norm:0.0007442119531333447 max memory_allocated 22567.03564453125 
[2025-03-22 06:21:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:0.9318247437477112 norm:0.0007169210584834218 max memory_allocated 22567.03564453125 
[2025-03-22 06:22:22 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:0.9311358332633972 norm:0.0007006747182458639 max memory_allocated 22567.03564453125 
[2025-03-22 06:22:55 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:0.9306164979934692 norm:0.0006940088351257145 max memory_allocated 22567.03564453125 
[2025-03-22 06:23:27 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:0.9302149415016174 norm:0.0006818489637225866 max memory_allocated 22567.03564453125 
[2025-03-22 06:24:00 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:0.9298362731933594 norm:0.0006683329702354968 max memory_allocated 22567.03564453125 
[2025-03-22 06:24:32 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:0.9295077323913574 norm:0.0006617045728489757 max memory_allocated 22567.03564453125 
[2025-03-22 06:25:05 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:0.9292576313018799 norm:0.0006584947113879025 max memory_allocated 22567.03564453125 
[2025-03-22 06:25:37 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:0.9290872812271118 norm:0.0006548610981553793 max memory_allocated 22567.03564453125 
[2025-03-22 06:26:10 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:0.9288773536682129 norm:0.0006584624643437564 max memory_allocated 22567.03564453125 
[2025-03-22 06:26:42 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:0.9285880327224731 norm:0.000651457579806447 max memory_allocated 22567.03564453125 
[2025-03-22 06:27:15 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:0.9283890724182129 norm:0.0006487016216851771 max memory_allocated 22567.03564453125 
[2025-03-22 06:27:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 06:27:26 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:27:59 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:1.1400742530822754 norm:0.04417215287685394 max memory_allocated 22568.12939453125 
[2025-03-22 06:28:32 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:1.113791823387146 norm:0.036072298884391785 max memory_allocated 22568.12939453125 
[2025-03-22 06:29:04 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:1.0871241092681885 norm:0.02423308975994587 max memory_allocated 22568.12939453125 
[2025-03-22 06:29:37 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:1.074832797050476 norm:0.018367618322372437 max memory_allocated 22568.12939453125 
[2025-03-22 06:30:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:1.0689481496810913 norm:0.014410926960408688 max memory_allocated 22568.12939453125 
[2025-03-22 06:30:42 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:1.0638409852981567 norm:0.011413953267037868 max memory_allocated 22568.12939453125 
[2025-03-22 06:31:15 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:1.0600597858428955 norm:0.00942748412489891 max memory_allocated 22568.12939453125 
[2025-03-22 06:31:48 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:1.0578808784484863 norm:0.009145822376012802 max memory_allocated 22568.12939453125 
[2025-03-22 06:32:21 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:1.0560590028762817 norm:0.008450613357126713 max memory_allocated 22568.12939453125 
[2025-03-22 06:32:53 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:1.0551217794418335 norm:0.008217536844313145 max memory_allocated 22568.12939453125 
[2025-03-22 06:33:26 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:1.0542104244232178 norm:0.008043275214731693 max memory_allocated 22568.12939453125 
[2025-03-22 06:33:59 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:1.0543479919433594 norm:0.006896052975207567 max memory_allocated 22568.12939453125 
[2025-03-22 06:34:32 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:1.0532197952270508 norm:0.007381260395050049 max memory_allocated 22568.12939453125 
[2025-03-22 06:35:05 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:1.0522024631500244 norm:0.005387434735894203 max memory_allocated 22568.12939453125 
[2025-03-22 06:35:37 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:1.0505443811416626 norm:0.005749526899307966 max memory_allocated 22568.12939453125 
[2025-03-22 06:36:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:1.0510687828063965 norm:0.006057098973542452 max memory_allocated 22568.12939453125 
[2025-03-22 06:36:43 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:1.0510755777359009 norm:0.006328527815639973 max memory_allocated 22568.12939453125 
[2025-03-22 06:37:15 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:1.0505340099334717 norm:0.005417514592409134 max memory_allocated 22568.12939453125 
[2025-03-22 06:37:48 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:1.0493242740631104 norm:0.0055942414328455925 max memory_allocated 22568.12939453125 
[2025-03-22 06:38:21 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:1.0482724905014038 norm:0.005304476711899042 max memory_allocated 22568.12939453125 
[2025-03-22 06:38:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 06:38:32 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:39:05 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:1.2938346862792969 norm:0.04998767375946045 max memory_allocated 22568.30126953125 
[2025-03-22 06:39:38 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:1.259548306465149 norm:0.03770424425601959 max memory_allocated 22568.30126953125 
[2025-03-22 06:40:10 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:1.2293988466262817 norm:0.027554724365472794 max memory_allocated 22568.30126953125 
[2025-03-22 06:40:43 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:1.212962031364441 norm:0.021112782880663872 max memory_allocated 22568.30126953125 
[2025-03-22 06:41:15 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:1.2043788433074951 norm:0.016183847561478615 max memory_allocated 22568.30126953125 
[2025-03-22 06:41:48 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:1.1977269649505615 norm:0.01261109858751297 max memory_allocated 22568.30126953125 
[2025-03-22 06:42:21 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:1.1938685178756714 norm:0.01135452650487423 max memory_allocated 22568.30126953125 
[2025-03-22 06:42:53 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:1.1909372806549072 norm:0.010561083443462849 max memory_allocated 22568.30126953125 
[2025-03-22 06:43:26 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:1.1886157989501953 norm:0.009424913674592972 max memory_allocated 22568.30126953125 
[2025-03-22 06:43:59 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:1.1873157024383545 norm:0.009450382553040981 max memory_allocated 22568.30126953125 
[2025-03-22 06:44:31 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:1.1863985061645508 norm:0.00901772826910019 max memory_allocated 22568.30126953125 
[2025-03-22 06:45:04 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:1.1855250597000122 norm:0.009020848199725151 max memory_allocated 22568.30126953125 
[2025-03-22 06:45:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:1.1843633651733398 norm:0.008311163634061813 max memory_allocated 22568.30126953125 
[2025-03-22 06:46:10 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:1.1837434768676758 norm:0.00842191930860281 max memory_allocated 22568.30126953125 
[2025-03-22 06:46:42 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:1.1831638813018799 norm:0.008157949894666672 max memory_allocated 22568.30126953125 
[2025-03-22 06:47:15 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:1.1825979948043823 norm:0.0077326479367911816 max memory_allocated 22568.30126953125 
[2025-03-22 06:47:48 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:1.1815598011016846 norm:0.007648891303688288 max memory_allocated 22568.30126953125 
[2025-03-22 06:48:21 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:1.1818163394927979 norm:0.007241384591907263 max memory_allocated 22568.30126953125 
[2025-03-22 06:48:54 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:1.1809941530227661 norm:0.007415893021970987 max memory_allocated 22568.30126953125 
[2025-03-22 06:49:26 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:1.1812655925750732 norm:0.007778380997478962 max memory_allocated 22568.30126953125 
[2025-03-22 06:49:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 06:49:38 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:50:11 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:2.7849695682525635 norm:0.48549604415893555 max memory_allocated 22568.47314453125 
[2025-03-22 06:50:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:2.8841147422790527 norm:0.915113091468811 max memory_allocated 22568.47314453125 
[2025-03-22 06:51:16 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:2.3042149543762207 norm:0.39584487676620483 max memory_allocated 22568.47314453125 
[2025-03-22 06:51:49 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:1.905051350593567 norm:0.23524224758148193 max memory_allocated 22568.47314453125 
[2025-03-22 06:52:22 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:1.8586000204086304 norm:0.24156454205513 max memory_allocated 22568.47314453125 
[2025-03-22 06:52:54 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:1.8319453001022339 norm:0.2359636127948761 max memory_allocated 22568.47314453125 
[2025-03-22 06:53:27 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:1.8125746250152588 norm:0.2229044884443283 max memory_allocated 22568.47314453125 
[2025-03-22 06:53:59 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:1.8031662702560425 norm:0.20665691792964935 max memory_allocated 22568.47314453125 
[2025-03-22 06:54:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:1.7973850965499878 norm:0.19776800274848938 max memory_allocated 22568.47314453125 
[2025-03-22 06:55:05 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:1.7965902090072632 norm:0.18160146474838257 max memory_allocated 22568.47314453125 
[2025-03-22 06:55:37 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:1.7781727313995361 norm:0.18408147990703583 max memory_allocated 22568.47314453125 
[2025-03-22 06:56:10 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:1.7590243816375732 norm:0.16042804718017578 max memory_allocated 22568.47314453125 
[2025-03-22 06:56:43 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:1.755934715270996 norm:0.15820454061031342 max memory_allocated 22568.47314453125 
[2025-03-22 06:57:15 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:1.756478190422058 norm:0.17323794960975647 max memory_allocated 22568.47314453125 
[2025-03-22 06:57:48 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:1.7509915828704834 norm:0.14457423985004425 max memory_allocated 22568.47314453125 
[2025-03-22 06:58:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:1.7428711652755737 norm:0.14367051422595978 max memory_allocated 22568.47314453125 
[2025-03-22 06:58:53 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:1.7326059341430664 norm:0.1291058361530304 max memory_allocated 22568.47314453125 
[2025-03-22 06:59:26 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:1.7066782712936401 norm:0.11521470546722412 max memory_allocated 22568.47314453125 
[2025-03-22 06:59:58 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:1.692490577697754 norm:0.09392083436250687 max memory_allocated 22568.47314453125 
[2025-03-22 07:00:31 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:1.6674014329910278 norm:0.09642953425645828 max memory_allocated 22568.47314453125 
[2025-03-22 07:00:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 07:00:43 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 07:01:15 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:3.9853310585021973 norm:0.5438740849494934 max memory_allocated 22568.64501953125 
[2025-03-22 07:01:48 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:3.5755629539489746 norm:0.4210156202316284 max memory_allocated 22568.64501953125 
[2025-03-22 07:02:21 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:3.1991918087005615 norm:0.3040980100631714 max memory_allocated 22568.64501953125 
[2025-03-22 07:02:53 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:3.0768589973449707 norm:0.25544166564941406 max memory_allocated 22568.64501953125 
[2025-03-22 07:03:26 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:3.018876075744629 norm:0.2280137836933136 max memory_allocated 22568.64501953125 
[2025-03-22 07:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:2.980297088623047 norm:0.20388098061084747 max memory_allocated 22568.64501953125 
[2025-03-22 07:04:31 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:2.9440269470214844 norm:0.1688786894083023 max memory_allocated 22568.64501953125 
[2025-03-22 07:05:04 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:2.918776512145996 norm:0.1567266285419464 max memory_allocated 22568.64501953125 
[2025-03-22 07:05:37 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:2.894073963165283 norm:0.13520333170890808 max memory_allocated 22568.64501953125 
[2025-03-22 07:06:10 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:2.87115478515625 norm:0.1260969191789627 max memory_allocated 22568.64501953125 
[2025-03-22 07:06:42 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:2.8535923957824707 norm:0.12107723951339722 max memory_allocated 22568.64501953125 
[2025-03-22 07:07:15 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:2.8352417945861816 norm:0.10662626475095749 max memory_allocated 22568.64501953125 
[2025-03-22 07:07:48 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:2.824946880340576 norm:0.11130072176456451 max memory_allocated 22568.64501953125 
[2025-03-22 07:08:20 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:2.817647695541382 norm:0.11320570856332779 max memory_allocated 22568.64501953125 
[2025-03-22 07:08:53 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:2.792424201965332 norm:0.10491196811199188 max memory_allocated 22568.64501953125 
[2025-03-22 07:09:26 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:2.77411150932312 norm:0.09431803971529007 max memory_allocated 22568.64501953125 
[2025-03-22 07:09:58 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:2.776581048965454 norm:0.10585827380418777 max memory_allocated 22568.64501953125 
[2025-03-22 07:10:31 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:2.7619099617004395 norm:0.10054294019937515 max memory_allocated 22568.64501953125 
[2025-03-22 07:11:03 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:2.7455906867980957 norm:0.08606591075658798 max memory_allocated 22568.64501953125 
[2025-03-22 07:11:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:2.7429308891296387 norm:0.09193105250597 max memory_allocated 22568.64501953125 
[2025-03-22 07:11:45 root] (main_calibration_a.py 369): INFO 21230.388278484344
[2025-03-22 07:11:49 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 07:12:57 root] (main_calibration_a.py 158): INFO wikitext2 : 8.984498023986816
[2025-03-22 07:12:57 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 07:14:43 root] (main_calibration_a.py 158): INFO c4 : 12.548008918762207
[2025-03-22 09:08:45 root] (main_calibration_a.py 169): INFO {'wikitext2': 8.984498023986816, 'c4': 12.548008918762207, 'results': {'hellaswag': {'acc': 0.4331806413065126, 'acc_stderr': 0.0049450236570322765, 'acc_norm': 0.5753833897629954, 'acc_norm_stderr': 0.004932745013072705}, 'boolq': {'acc': 0.6406727828746177, 'acc_stderr': 0.008391811770406732}, 'winogrande': {'acc': 0.5674822415153907, 'acc_stderr': 0.013923911578623823}, 'piqa': {'acc': 0.6866158868335147, 'acc_stderr': 0.010822829929195487, 'acc_norm': 0.7023939064200218, 'acc_norm_stderr': 0.01066735379238821}, 'arc_challenge': {'acc': 0.29948805460750855, 'acc_stderr': 0.01338502163731357, 'acc_norm': 0.32764505119453924, 'acc_norm_stderr': 0.01371584794071934}, 'arc_easy': {'acc': 0.5467171717171717, 'acc_stderr': 0.01021490151673162, 'acc_norm': 0.44907407407407407, 'acc_norm_stderr': 0.010206428316323363}}, 'versions': {'hellaswag': 0, 'boolq': 1, 'winogrande': 0, 'piqa': 0, 'arc_challenge': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 09:08:45 root] (main_calibration_a.py 172): INFO 29.95,54.67,64.07,43.32,68.66,56.75
