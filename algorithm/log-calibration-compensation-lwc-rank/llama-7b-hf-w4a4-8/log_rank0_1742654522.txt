[2025-03-22 14:42:02 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/llama-7b-hf-w4a4-8', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=8)
[2025-03-22 14:45:29 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 14:45:29 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:45:30 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 14:45:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 14:45:35 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:46:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.09905096888542175 norm:0.10778922587633133 max memory_allocated 22560.31689453125 
[2025-03-22 14:46:39 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.054871175438165665 norm:0.0560966357588768 max memory_allocated 22560.31689453125 
[2025-03-22 14:47:12 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.043089572340250015 norm:0.03924933075904846 max memory_allocated 22560.31689453125 
[2025-03-22 14:47:44 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.03871075436472893 norm:0.0341319665312767 max memory_allocated 22560.31689453125 
[2025-03-22 14:48:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.03653116524219513 norm:0.030089300125837326 max memory_allocated 22560.31689453125 
[2025-03-22 14:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.0346309058368206 norm:0.02623703144490719 max memory_allocated 22560.31689453125 
[2025-03-22 14:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.03330179303884506 norm:0.023107178509235382 max memory_allocated 22560.31689453125 
[2025-03-22 14:49:54 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.03242111578583717 norm:0.02030348964035511 max memory_allocated 22560.31689453125 
[2025-03-22 14:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.03164093568921089 norm:0.017413312569260597 max memory_allocated 22560.31689453125 
[2025-03-22 14:50:59 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.030937805771827698 norm:0.015199759975075722 max memory_allocated 22560.31689453125 
[2025-03-22 14:51:32 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.03044687584042549 norm:0.013267304748296738 max memory_allocated 22560.31689453125 
[2025-03-22 14:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.030078187584877014 norm:0.01154685765504837 max memory_allocated 22560.31689453125 
[2025-03-22 14:52:37 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.029719112440943718 norm:0.010132228024303913 max memory_allocated 22560.31689453125 
[2025-03-22 14:53:10 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.0295846089720726 norm:0.009122585877776146 max memory_allocated 22560.31689453125 
[2025-03-22 14:53:42 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.02939249761402607 norm:0.008028198964893818 max memory_allocated 22560.31689453125 
[2025-03-22 14:54:15 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.02918645739555359 norm:0.007459748070687056 max memory_allocated 22560.31689453125 
[2025-03-22 14:54:47 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.02904672920703888 norm:0.006973220966756344 max memory_allocated 22560.31689453125 
[2025-03-22 14:55:20 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.028798095881938934 norm:0.006478291004896164 max memory_allocated 22560.31689453125 
[2025-03-22 14:55:52 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.02882065623998642 norm:0.00635477714240551 max memory_allocated 22560.31689453125 
[2025-03-22 14:56:25 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.028860662132501602 norm:0.006159087643027306 max memory_allocated 22560.31689453125 
[2025-03-22 14:56:34 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 14:56:36 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:57:09 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.2945268154144287 norm:0.2326926440000534 max memory_allocated 22560.48876953125 
[2025-03-22 14:57:41 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.15902307629585266 norm:0.10608339309692383 max memory_allocated 22560.48876953125 
[2025-03-22 14:58:14 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.11706140637397766 norm:0.04979570582509041 max memory_allocated 22560.48876953125 
[2025-03-22 14:58:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.10107702016830444 norm:0.03646446019411087 max memory_allocated 22560.48876953125 
[2025-03-22 14:59:19 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.09292550384998322 norm:0.029630504548549652 max memory_allocated 22560.48876953125 
[2025-03-22 14:59:51 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.08767366409301758 norm:0.024881580844521523 max memory_allocated 22560.48876953125 
[2025-03-22 15:00:24 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.08440334349870682 norm:0.021545490249991417 max memory_allocated 22560.48876953125 
[2025-03-22 15:00:56 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.08204151690006256 norm:0.019276730716228485 max memory_allocated 22560.48876953125 
[2025-03-22 15:01:29 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.0802067294716835 norm:0.016735825687646866 max memory_allocated 22560.48876953125 
[2025-03-22 15:02:01 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.07893512398004532 norm:0.01461686473339796 max memory_allocated 22560.48876953125 
[2025-03-22 15:02:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.07792481780052185 norm:0.012921588495373726 max memory_allocated 22560.48876953125 
[2025-03-22 15:03:06 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.07706964761018753 norm:0.011351659893989563 max memory_allocated 22560.48876953125 
[2025-03-22 15:03:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.07634582370519638 norm:0.010354500263929367 max memory_allocated 22560.48876953125 
[2025-03-22 15:04:12 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.07580167800188065 norm:0.009516886435449123 max memory_allocated 22560.48876953125 
[2025-03-22 15:04:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.07540829479694366 norm:0.008926784619688988 max memory_allocated 22560.48876953125 
[2025-03-22 15:05:17 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07512946426868439 norm:0.008520042523741722 max memory_allocated 22560.48876953125 
[2025-03-22 15:05:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.0747622400522232 norm:0.007937004789710045 max memory_allocated 22560.48876953125 
[2025-03-22 15:06:22 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07452519237995148 norm:0.00792186614125967 max memory_allocated 22560.48876953125 
[2025-03-22 15:06:55 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07419692724943161 norm:0.0074571240693330765 max memory_allocated 22560.48876953125 
[2025-03-22 15:07:27 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07395607233047485 norm:0.00678542023524642 max memory_allocated 22560.48876953125 
[2025-03-22 15:07:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 15:07:39 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:08:12 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.2609611749649048 norm:0.06342925876379013 max memory_allocated 22560.66064453125 
[2025-03-22 15:08:44 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.21074910461902618 norm:0.035415228456258774 max memory_allocated 22560.66064453125 
[2025-03-22 15:09:17 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.18981093168258667 norm:0.03270353376865387 max memory_allocated 22560.66064453125 
[2025-03-22 15:09:50 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.17944689095020294 norm:0.03158000856637955 max memory_allocated 22560.66064453125 
[2025-03-22 15:10:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.16896101832389832 norm:0.034539222717285156 max memory_allocated 22560.66064453125 
[2025-03-22 15:10:55 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.16294464468955994 norm:0.03372760862112045 max memory_allocated 22560.66064453125 
[2025-03-22 15:11:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.15929925441741943 norm:0.030975742265582085 max memory_allocated 22560.66064453125 
[2025-03-22 15:12:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.15827438235282898 norm:0.027862265706062317 max memory_allocated 22560.66064453125 
[2025-03-22 15:12:33 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.15390056371688843 norm:0.028182487934827805 max memory_allocated 22560.66064453125 
[2025-03-22 15:13:05 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.15556058287620544 norm:0.026992106810212135 max memory_allocated 22560.66064453125 
[2025-03-22 15:13:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.15095654129981995 norm:0.025920405983924866 max memory_allocated 22560.66064453125 
[2025-03-22 15:14:10 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.14988288283348083 norm:0.02497667632997036 max memory_allocated 22560.66064453125 
[2025-03-22 15:14:43 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.1504708081483841 norm:0.022806163877248764 max memory_allocated 22560.66064453125 
[2025-03-22 15:15:15 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.1481553465127945 norm:0.025231091305613518 max memory_allocated 22560.66064453125 
[2025-03-22 15:15:48 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.14983412623405457 norm:0.021353308111429214 max memory_allocated 22560.66064453125 
[2025-03-22 15:16:21 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.14908669888973236 norm:0.021415837109088898 max memory_allocated 22560.66064453125 
[2025-03-22 15:16:53 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.14598721265792847 norm:0.022424260154366493 max memory_allocated 22560.66064453125 
[2025-03-22 15:17:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.1471683233976364 norm:0.020608579739928246 max memory_allocated 22560.66064453125 
[2025-03-22 15:17:59 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.14416442811489105 norm:0.019424065947532654 max memory_allocated 22560.66064453125 
[2025-03-22 15:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.14594127237796783 norm:0.019254615530371666 max memory_allocated 22560.66064453125 
[2025-03-22 15:18:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 15:19:15 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.25079095363616943 norm:0.04974507540464401 max memory_allocated 22560.66064453125 
[2025-03-22 15:19:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.20730255544185638 norm:0.01516795251518488 max memory_allocated 22560.66064453125 
[2025-03-22 15:20:20 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.1798076331615448 norm:0.00663702841848135 max memory_allocated 22560.66064453125 
[2025-03-22 15:20:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.17084656655788422 norm:0.0037649665027856827 max memory_allocated 22560.66064453125 
[2025-03-22 15:21:25 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.16648370027542114 norm:0.002869260963052511 max memory_allocated 22560.66064453125 
[2025-03-22 15:21:58 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.16345523297786713 norm:0.0023200088180601597 max memory_allocated 22560.66064453125 
[2025-03-22 15:22:30 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.16152074933052063 norm:0.0020826710388064384 max memory_allocated 22560.66064453125 
[2025-03-22 15:23:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.16014985740184784 norm:0.0018931971862912178 max memory_allocated 22560.66064453125 
[2025-03-22 15:23:36 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.1593044102191925 norm:0.0018202024511992931 max memory_allocated 22560.66064453125 
[2025-03-22 15:24:08 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.1588381677865982 norm:0.001799910212866962 max memory_allocated 22560.66064453125 
[2025-03-22 15:24:41 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.15822190046310425 norm:0.0016595970373600721 max memory_allocated 22560.66064453125 
[2025-03-22 15:25:13 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.1576274335384369 norm:0.0015700875082984567 max memory_allocated 22560.66064453125 
[2025-03-22 15:25:46 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.15690428018569946 norm:0.001439419575035572 max memory_allocated 22560.66064453125 
[2025-03-22 15:26:18 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.1566380113363266 norm:0.0013852646807208657 max memory_allocated 22560.66064453125 
[2025-03-22 15:26:51 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.15660825371742249 norm:0.0013619507662951946 max memory_allocated 22560.66064453125 
[2025-03-22 15:27:23 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.15655630826950073 norm:0.0013224631547927856 max memory_allocated 22560.66064453125 
[2025-03-22 15:27:56 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.15666747093200684 norm:0.0013231202028691769 max memory_allocated 22560.66064453125 
[2025-03-22 15:28:28 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.15682150423526764 norm:0.0013177875662222505 max memory_allocated 22560.66064453125 
[2025-03-22 15:29:01 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.1565612107515335 norm:0.0012806870508939028 max memory_allocated 22560.66064453125 
[2025-03-22 15:29:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.15687812864780426 norm:0.0013287246692925692 max memory_allocated 22560.66064453125 
[2025-03-22 15:29:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 15:30:17 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.3038017749786377 norm:0.04757029563188553 max memory_allocated 22560.66064453125 
[2025-03-22 15:30:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.25669389963150024 norm:0.018972596153616905 max memory_allocated 22560.66064453125 
[2025-03-22 15:31:22 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.2210141122341156 norm:0.006668594665825367 max memory_allocated 22560.66064453125 
[2025-03-22 15:31:55 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.20855873823165894 norm:0.003394557163119316 max memory_allocated 22560.66064453125 
[2025-03-22 15:32:27 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.2035091668367386 norm:0.002580429194495082 max memory_allocated 22560.66064453125 
[2025-03-22 15:32:59 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.20055481791496277 norm:0.002247149357572198 max memory_allocated 22560.66064453125 
[2025-03-22 15:33:32 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.19859322905540466 norm:0.0020924087148159742 max memory_allocated 22560.66064453125 
[2025-03-22 15:34:04 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.19732724130153656 norm:0.002071801573038101 max memory_allocated 22560.66064453125 
[2025-03-22 15:34:37 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.19611914455890656 norm:0.0018677922198548913 max memory_allocated 22560.66064453125 
[2025-03-22 15:35:10 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.1953626126050949 norm:0.0017118531977757812 max memory_allocated 22560.66064453125 
[2025-03-22 15:35:42 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.19472736120224 norm:0.0015748448204249144 max memory_allocated 22560.66064453125 
[2025-03-22 15:36:15 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.19417452812194824 norm:0.0015303646214306355 max memory_allocated 22560.66064453125 
[2025-03-22 15:36:47 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.19402958452701569 norm:0.001412264653481543 max memory_allocated 22560.66064453125 
[2025-03-22 15:37:20 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.19411388039588928 norm:0.0014867038698866963 max memory_allocated 22560.66064453125 
[2025-03-22 15:37:52 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.1938835233449936 norm:0.001428677118383348 max memory_allocated 22560.66064453125 
[2025-03-22 15:38:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.1937040239572525 norm:0.001324319513514638 max memory_allocated 22560.66064453125 
[2025-03-22 15:38:58 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.1933492124080658 norm:0.001338193193078041 max memory_allocated 22560.66064453125 
[2025-03-22 15:39:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.19328680634498596 norm:0.00127477350179106 max memory_allocated 22560.66064453125 
[2025-03-22 15:40:03 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.19342918694019318 norm:0.0012288272846490145 max memory_allocated 22560.66064453125 
[2025-03-22 15:40:35 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.1934226006269455 norm:0.0012206552783027291 max memory_allocated 22560.66064453125 
[2025-03-22 15:40:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 15:41:20 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.34724047780036926 norm:0.062404781579971313 max memory_allocated 22560.66064453125 
[2025-03-22 15:41:52 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2924840450286865 norm:0.021067297086119652 max memory_allocated 22560.66064453125 
[2025-03-22 15:42:25 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.2492859810590744 norm:0.0069894324988126755 max memory_allocated 22560.66064453125 
[2025-03-22 15:42:57 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.2350659817457199 norm:0.003930267412215471 max memory_allocated 22560.66064453125 
[2025-03-22 15:43:30 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.2294921576976776 norm:0.003177817678079009 max memory_allocated 22560.66064453125 
[2025-03-22 15:44:02 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.22547906637191772 norm:0.0026196567341685295 max memory_allocated 22560.66064453125 
[2025-03-22 15:44:35 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.2231733202934265 norm:0.0024049985222518444 max memory_allocated 22560.66064453125 
[2025-03-22 15:45:07 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.22147749364376068 norm:0.0021601191256195307 max memory_allocated 22560.66064453125 
[2025-03-22 15:45:40 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.22042949497699738 norm:0.0020025537814944983 max memory_allocated 22560.66064453125 
[2025-03-22 15:46:12 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.21959561109542847 norm:0.0019583809189498425 max memory_allocated 22560.66064453125 
[2025-03-22 15:46:45 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.21904373168945312 norm:0.0018614137079566717 max memory_allocated 22560.66064453125 
[2025-03-22 15:47:17 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.21872425079345703 norm:0.001725896610878408 max memory_allocated 22560.66064453125 
[2025-03-22 15:47:50 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.21805527806282043 norm:0.0016618294175714254 max memory_allocated 22560.66064453125 
[2025-03-22 15:48:22 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.21757367253303528 norm:0.0015973608242347836 max memory_allocated 22560.66064453125 
[2025-03-22 15:48:55 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.2168978750705719 norm:0.0013982938835397363 max memory_allocated 22560.66064453125 
[2025-03-22 15:49:27 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.2165292501449585 norm:0.0013261784333735704 max memory_allocated 22560.66064453125 
[2025-03-22 15:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.2162034809589386 norm:0.0013029346009716392 max memory_allocated 22560.66064453125 
[2025-03-22 15:50:32 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.21621741354465485 norm:0.0012616224121302366 max memory_allocated 22560.66064453125 
[2025-03-22 15:51:05 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.21663914620876312 norm:0.0012996089644730091 max memory_allocated 22560.66064453125 
[2025-03-22 15:51:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.21654018759727478 norm:0.0012325800489634275 max memory_allocated 22560.66064453125 
[2025-03-22 15:51:46 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 15:52:22 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.3756275475025177 norm:0.0657425969839096 max memory_allocated 22560.66064453125 
[2025-03-22 15:52:54 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.32649439573287964 norm:0.026986053213477135 max memory_allocated 22560.66064453125 
[2025-03-22 15:53:27 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.2761029601097107 norm:0.007046348415315151 max memory_allocated 22560.66064453125 
[2025-03-22 15:53:59 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.2595530152320862 norm:0.003429325995966792 max memory_allocated 22560.66064453125 
[2025-03-22 15:54:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.2536769211292267 norm:0.0027653316501528025 max memory_allocated 22560.66064453125 
[2025-03-22 15:55:05 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.24995699524879456 norm:0.002260747831314802 max memory_allocated 22560.66064453125 
[2025-03-22 15:55:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.24789853394031525 norm:0.002036851132288575 max memory_allocated 22560.66064453125 
[2025-03-22 15:56:10 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.2462451010942459 norm:0.0018757401267066598 max memory_allocated 22560.66064453125 
[2025-03-22 15:56:42 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.2449212372303009 norm:0.0016904613003134727 max memory_allocated 22560.66064453125 
[2025-03-22 15:57:15 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.24383224546909332 norm:0.0016120483633130789 max memory_allocated 22560.66064453125 
[2025-03-22 15:57:48 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.2432752400636673 norm:0.0015843387227505445 max memory_allocated 22560.66064453125 
[2025-03-22 15:58:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.24310657382011414 norm:0.0015828321920707822 max memory_allocated 22560.66064453125 
[2025-03-22 15:58:53 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.24265949428081512 norm:0.0014900955138728023 max memory_allocated 22560.66064453125 
[2025-03-22 15:59:26 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.24219465255737305 norm:0.0013971829321235418 max memory_allocated 22560.66064453125 
[2025-03-22 15:59:58 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.24196626245975494 norm:0.0013825490605086088 max memory_allocated 22560.66064453125 
[2025-03-22 16:00:31 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.242216557264328 norm:0.0014279214665293694 max memory_allocated 22560.66064453125 
[2025-03-22 16:01:04 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.24191470444202423 norm:0.0013793406542390585 max memory_allocated 22560.66064453125 
[2025-03-22 16:01:36 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.24152356386184692 norm:0.0013018547324463725 max memory_allocated 22560.66064453125 
[2025-03-22 16:02:09 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.2413674294948578 norm:0.0012682495871558785 max memory_allocated 22560.66064453125 
[2025-03-22 16:02:41 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.24113553762435913 norm:0.001246049883775413 max memory_allocated 22560.66064453125 
[2025-03-22 16:02:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 16:03:25 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.4076283276081085 norm:0.04390084743499756 max memory_allocated 22560.66064453125 
[2025-03-22 16:03:58 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.35403746366500854 norm:0.018681926652789116 max memory_allocated 22560.66064453125 
[2025-03-22 16:04:30 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.3008919358253479 norm:0.005583306308835745 max memory_allocated 22560.66064453125 
[2025-03-22 16:05:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.28420132398605347 norm:0.0028800521977245808 max memory_allocated 22560.66064453125 
[2025-03-22 16:05:35 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.2781900465488434 norm:0.0022015334106981754 max memory_allocated 22560.66064453125 
[2025-03-22 16:06:08 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.27441754937171936 norm:0.001997952116653323 max memory_allocated 22560.66064453125 
[2025-03-22 16:06:40 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.27214527130126953 norm:0.0018263892270624638 max memory_allocated 22560.66064453125 
[2025-03-22 16:07:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.27034828066825867 norm:0.001630761893466115 max memory_allocated 22560.66064453125 
[2025-03-22 16:07:45 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.26886895298957825 norm:0.0015179375186562538 max memory_allocated 22560.66064453125 
[2025-03-22 16:08:18 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.267818421125412 norm:0.001429706229828298 max memory_allocated 22560.66064453125 
[2025-03-22 16:08:50 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.26696401834487915 norm:0.0013232572237029672 max memory_allocated 22560.66064453125 
[2025-03-22 16:09:23 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.2667221426963806 norm:0.0013129336293786764 max memory_allocated 22560.66064453125 
[2025-03-22 16:09:56 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.26644691824913025 norm:0.001261569675989449 max memory_allocated 22560.66064453125 
[2025-03-22 16:10:28 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2662978768348694 norm:0.0012848335318267345 max memory_allocated 22560.66064453125 
[2025-03-22 16:11:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.26622891426086426 norm:0.0013209778117015958 max memory_allocated 22560.66064453125 
[2025-03-22 16:11:33 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.2658257484436035 norm:0.001215506810694933 max memory_allocated 22560.66064453125 
[2025-03-22 16:12:06 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.26574522256851196 norm:0.0012046684278175235 max memory_allocated 22560.66064453125 
[2025-03-22 16:12:39 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.26591628789901733 norm:0.0012287249555811286 max memory_allocated 22560.66064453125 
[2025-03-22 16:13:11 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.2660176753997803 norm:0.0012136122677475214 max memory_allocated 22560.66064453125 
[2025-03-22 16:13:44 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.26592421531677246 norm:0.0011900212848559022 max memory_allocated 22560.66064453125 
[2025-03-22 16:13:53 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 16:14:28 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.42061465978622437 norm:0.03313856199383736 max memory_allocated 22560.77001953125 
[2025-03-22 16:15:01 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.370947927236557 norm:0.015035690739750862 max memory_allocated 22560.77001953125 
[2025-03-22 16:15:33 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.31967025995254517 norm:0.004745993763208389 max memory_allocated 22560.77001953125 
[2025-03-22 16:16:06 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.30294865369796753 norm:0.002303660847246647 max memory_allocated 22560.77001953125 
[2025-03-22 16:16:39 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.29671525955200195 norm:0.0017887455178424716 max memory_allocated 22560.77001953125 
[2025-03-22 16:17:11 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.29296430945396423 norm:0.0015829161275178194 max memory_allocated 22560.77001953125 
[2025-03-22 16:17:44 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.29011276364326477 norm:0.0014324979856610298 max memory_allocated 22560.77001953125 
[2025-03-22 16:18:16 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.28828343749046326 norm:0.0013240959960967302 max memory_allocated 22560.77001953125 
[2025-03-22 16:18:49 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.2869854271411896 norm:0.0012960063759237528 max memory_allocated 22560.77001953125 
[2025-03-22 16:19:22 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.2860191762447357 norm:0.001287800376303494 max memory_allocated 22560.77001953125 
[2025-03-22 16:19:54 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.28493213653564453 norm:0.0012185937957838178 max memory_allocated 22560.77001953125 
[2025-03-22 16:20:27 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.2843111753463745 norm:0.0011717411689460278 max memory_allocated 22560.77001953125 
[2025-03-22 16:21:00 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.2837907671928406 norm:0.00108165736310184 max memory_allocated 22560.77001953125 
[2025-03-22 16:21:32 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.28353482484817505 norm:0.0011172706726938486 max memory_allocated 22560.77001953125 
[2025-03-22 16:22:05 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.28319641947746277 norm:0.001094575971364975 max memory_allocated 22560.77001953125 
[2025-03-22 16:22:38 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.2830909192562103 norm:0.0010824258206412196 max memory_allocated 22560.77001953125 
[2025-03-22 16:23:10 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.28295591473579407 norm:0.001010892796330154 max memory_allocated 22560.77001953125 
[2025-03-22 16:23:43 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.28301873803138733 norm:0.0010167906293645501 max memory_allocated 22560.77001953125 
[2025-03-22 16:24:16 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.28288307785987854 norm:0.0010016062296926975 max memory_allocated 22560.77001953125 
[2025-03-22 16:24:48 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.2827969193458557 norm:0.0010472422000020742 max memory_allocated 22560.77001953125 
[2025-03-22 16:24:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 16:25:33 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.4560178816318512 norm:0.040643755346536636 max memory_allocated 22560.94189453125 
[2025-03-22 16:26:05 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.4118749797344208 norm:0.02104901522397995 max memory_allocated 22560.94189453125 
[2025-03-22 16:26:38 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.3482895493507385 norm:0.006476941052824259 max memory_allocated 22560.94189453125 
[2025-03-22 16:27:10 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.3249015510082245 norm:0.002733275294303894 max memory_allocated 22560.94189453125 
[2025-03-22 16:27:43 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.31770139932632446 norm:0.0020920955576002598 max memory_allocated 22560.94189453125 
[2025-03-22 16:28:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.31323447823524475 norm:0.001738084596581757 max memory_allocated 22560.94189453125 
[2025-03-22 16:28:48 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.31013035774230957 norm:0.0015484660398215055 max memory_allocated 22560.94189453125 
[2025-03-22 16:29:20 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.3083541691303253 norm:0.0014425796689465642 max memory_allocated 22560.94189453125 
[2025-03-22 16:29:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.30674201250076294 norm:0.0013086266117170453 max memory_allocated 22560.94189453125 
[2025-03-22 16:30:25 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3057468831539154 norm:0.0012547055957838893 max memory_allocated 22560.94189453125 
[2025-03-22 16:30:58 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.3047315776348114 norm:0.001209617592394352 max memory_allocated 22560.94189453125 
[2025-03-22 16:31:30 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.3039079010486603 norm:0.001160339219495654 max memory_allocated 22560.94189453125 
[2025-03-22 16:32:03 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.30361080169677734 norm:0.0011343881487846375 max memory_allocated 22560.94189453125 
[2025-03-22 16:32:35 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.30326926708221436 norm:0.0011049194727092981 max memory_allocated 22560.94189453125 
[2025-03-22 16:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.30296915769577026 norm:0.0010621374240145087 max memory_allocated 22560.94189453125 
[2025-03-22 16:33:41 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.30277279019355774 norm:0.0010649490868672729 max memory_allocated 22560.94189453125 
[2025-03-22 16:34:13 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.30272752046585083 norm:0.0010543172247707844 max memory_allocated 22560.94189453125 
[2025-03-22 16:34:46 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.30252060294151306 norm:0.0010268192272633314 max memory_allocated 22560.94189453125 
[2025-03-22 16:35:18 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.30255207419395447 norm:0.0010424524080008268 max memory_allocated 22560.94189453125 
[2025-03-22 16:35:51 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.3023667335510254 norm:0.0010310643119737506 max memory_allocated 22560.94189453125 
[2025-03-22 16:36:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 16:36:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.4227520823478699 norm:0.021950630471110344 max memory_allocated 22561.11376953125 
[2025-03-22 16:37:08 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.3881036043167114 norm:0.011255648918449879 max memory_allocated 22561.11376953125 
[2025-03-22 16:37:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.3478923738002777 norm:0.0037636638153344393 max memory_allocated 22561.11376953125 
[2025-03-22 16:38:13 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3334054946899414 norm:0.0018015882233157754 max memory_allocated 22561.11376953125 
[2025-03-22 16:38:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.328303724527359 norm:0.0014745654771104455 max memory_allocated 22561.11376953125 
[2025-03-22 16:39:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3250747323036194 norm:0.001335585257038474 max memory_allocated 22561.11376953125 
[2025-03-22 16:39:51 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3230883479118347 norm:0.0012311474420130253 max memory_allocated 22561.11376953125 
[2025-03-22 16:40:24 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.3216738700866699 norm:0.0011988983023911715 max memory_allocated 22561.11376953125 
[2025-03-22 16:40:56 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.3206048011779785 norm:0.0011333235306665301 max memory_allocated 22561.11376953125 
[2025-03-22 16:41:29 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.31980252265930176 norm:0.001073532272130251 max memory_allocated 22561.11376953125 
[2025-03-22 16:42:02 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.3190698027610779 norm:0.0010104491375386715 max memory_allocated 22561.11376953125 
[2025-03-22 16:42:34 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.31841838359832764 norm:0.0009648244013078511 max memory_allocated 22561.11376953125 
[2025-03-22 16:43:07 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.31804123520851135 norm:0.0009338930249214172 max memory_allocated 22561.11376953125 
[2025-03-22 16:43:40 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.31768879294395447 norm:0.0009260293445549905 max memory_allocated 22561.11376953125 
[2025-03-22 16:44:12 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.31748682260513306 norm:0.0009233615128323436 max memory_allocated 22561.11376953125 
[2025-03-22 16:44:45 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.3171842098236084 norm:0.0009018411510623991 max memory_allocated 22561.11376953125 
[2025-03-22 16:45:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.31692275404930115 norm:0.0008566336473450065 max memory_allocated 22561.11376953125 
[2025-03-22 16:45:50 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3167803883552551 norm:0.0008391532464884222 max memory_allocated 22561.11376953125 
[2025-03-22 16:46:23 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.31663787364959717 norm:0.0008178827119991183 max memory_allocated 22561.11376953125 
[2025-03-22 16:46:55 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.31648316979408264 norm:0.0008178707212209702 max memory_allocated 22561.11376953125 
[2025-03-22 16:47:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 16:47:41 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.450716108083725 norm:0.03315114974975586 max memory_allocated 22561.28564453125 
[2025-03-22 16:48:13 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.4103464186191559 norm:0.015490940771996975 max memory_allocated 22561.28564453125 
[2025-03-22 16:48:46 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.36754345893859863 norm:0.005388964898884296 max memory_allocated 22561.28564453125 
[2025-03-22 16:49:18 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.3498319387435913 norm:0.0024425298906862736 max memory_allocated 22561.28564453125 
[2025-03-22 16:49:51 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.3434393107891083 norm:0.0018423888832330704 max memory_allocated 22561.28564453125 
[2025-03-22 16:50:23 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.3396070897579193 norm:0.0016163976397365332 max memory_allocated 22561.28564453125 
[2025-03-22 16:50:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.3369189202785492 norm:0.0014705521753057837 max memory_allocated 22561.28564453125 
[2025-03-22 16:51:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.33501848578453064 norm:0.0013930299319326878 max memory_allocated 22561.28564453125 
[2025-03-22 16:52:01 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.33379894495010376 norm:0.0013407871592789888 max memory_allocated 22561.28564453125 
[2025-03-22 16:52:33 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.3327976167201996 norm:0.0012655934551730752 max memory_allocated 22561.28564453125 
[2025-03-22 16:53:06 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.332297682762146 norm:0.0012466175248846412 max memory_allocated 22561.28564453125 
[2025-03-22 16:53:38 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.33171766996383667 norm:0.0011748949764296412 max memory_allocated 22561.28564453125 
[2025-03-22 16:54:11 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3312220275402069 norm:0.0011325012892484665 max memory_allocated 22561.28564453125 
[2025-03-22 16:54:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.33086568117141724 norm:0.001091409008949995 max memory_allocated 22561.28564453125 
[2025-03-22 16:55:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3305940628051758 norm:0.0011178774293512106 max memory_allocated 22561.28564453125 
[2025-03-22 16:55:49 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.33032652735710144 norm:0.0010727463522925973 max memory_allocated 22561.28564453125 
[2025-03-22 16:56:21 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.3300405740737915 norm:0.0010160505771636963 max memory_allocated 22561.28564453125 
[2025-03-22 16:56:54 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.3298051953315735 norm:0.0010193977504968643 max memory_allocated 22561.28564453125 
[2025-03-22 16:57:27 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.32949933409690857 norm:0.0010046667885035276 max memory_allocated 22561.28564453125 
[2025-03-22 16:57:59 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.329147070646286 norm:0.0009535945137031376 max memory_allocated 22561.28564453125 
[2025-03-22 16:58:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 16:58:44 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.4396923780441284 norm:0.027393896132707596 max memory_allocated 22561.45751953125 
[2025-03-22 16:59:16 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.4153529107570648 norm:0.014452338218688965 max memory_allocated 22561.45751953125 
[2025-03-22 16:59:49 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.3826656639575958 norm:0.007011811248958111 max memory_allocated 22561.45751953125 
[2025-03-22 17:00:21 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.36567801237106323 norm:0.0039990912191569805 max memory_allocated 22561.45751953125 
[2025-03-22 17:00:54 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.35925909876823425 norm:0.0030041979625821114 max memory_allocated 22561.45751953125 
[2025-03-22 17:01:27 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.35489052534103394 norm:0.002554780337959528 max memory_allocated 22561.45751953125 
[2025-03-22 17:01:59 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.3516172766685486 norm:0.0021282071247696877 max memory_allocated 22561.45751953125 
[2025-03-22 17:02:32 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.3495684564113617 norm:0.0019503411604091525 max memory_allocated 22561.45751953125 
[2025-03-22 17:03:04 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.3479269742965698 norm:0.0017660678131505847 max memory_allocated 22561.45751953125 
[2025-03-22 17:03:37 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.3466574251651764 norm:0.0016066627576947212 max memory_allocated 22561.45751953125 
[2025-03-22 17:04:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.34566497802734375 norm:0.0014993221266195178 max memory_allocated 22561.45751953125 
[2025-03-22 17:04:42 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.34507954120635986 norm:0.0014631678350269794 max memory_allocated 22561.45751953125 
[2025-03-22 17:05:15 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.3444334864616394 norm:0.0013485680101439357 max memory_allocated 22561.45751953125 
[2025-03-22 17:05:48 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.34400179982185364 norm:0.0013085035607218742 max memory_allocated 22561.45751953125 
[2025-03-22 17:06:20 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.3436166048049927 norm:0.0012429580092430115 max memory_allocated 22561.45751953125 
[2025-03-22 17:06:53 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.34317439794540405 norm:0.0011683503398671746 max memory_allocated 22561.45751953125 
[2025-03-22 17:07:26 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.3430277407169342 norm:0.0011117965914309025 max memory_allocated 22561.45751953125 
[2025-03-22 17:07:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.3426809012889862 norm:0.0010627855081111193 max memory_allocated 22561.45751953125 
[2025-03-22 17:08:31 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.3424730896949768 norm:0.0010425043292343616 max memory_allocated 22561.45751953125 
[2025-03-22 17:09:03 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.3421995937824249 norm:0.0010004147188737988 max memory_allocated 22561.45751953125 
[2025-03-22 17:09:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 17:09:48 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.446866512298584 norm:0.01946689374744892 max memory_allocated 22561.62939453125 
[2025-03-22 17:10:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.41960352659225464 norm:0.010363382287323475 max memory_allocated 22561.62939453125 
[2025-03-22 17:10:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.38905438780784607 norm:0.004550505429506302 max memory_allocated 22561.62939453125 
[2025-03-22 17:11:25 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.37491923570632935 norm:0.002175187459215522 max memory_allocated 22561.62939453125 
[2025-03-22 17:11:58 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.3690943121910095 norm:0.0016241312259808183 max memory_allocated 22561.62939453125 
[2025-03-22 17:12:30 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.36570557951927185 norm:0.001416345126926899 max memory_allocated 22561.62939453125 
[2025-03-22 17:13:03 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3632335066795349 norm:0.0012989237438887358 max memory_allocated 22561.62939453125 
[2025-03-22 17:13:35 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.3613843321800232 norm:0.0012371536577120423 max memory_allocated 22561.62939453125 
[2025-03-22 17:14:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.3600461184978485 norm:0.0012347606243565679 max memory_allocated 22561.62939453125 
[2025-03-22 17:14:40 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.3592052161693573 norm:0.0012276727939024568 max memory_allocated 22561.62939453125 
[2025-03-22 17:15:13 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3583196997642517 norm:0.0011774885933846235 max memory_allocated 22561.62939453125 
[2025-03-22 17:15:45 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.35761886835098267 norm:0.0011306987144052982 max memory_allocated 22561.62939453125 
[2025-03-22 17:16:18 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.35715773701667786 norm:0.001097654807381332 max memory_allocated 22561.62939453125 
[2025-03-22 17:16:50 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3568195402622223 norm:0.0010983225656673312 max memory_allocated 22561.62939453125 
[2025-03-22 17:17:23 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.3564682602882385 norm:0.0010548143181949854 max memory_allocated 22561.62939453125 
[2025-03-22 17:17:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3559860587120056 norm:0.0009754380444064736 max memory_allocated 22561.62939453125 
[2025-03-22 17:18:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.3560972213745117 norm:0.000986762810498476 max memory_allocated 22561.62939453125 
[2025-03-22 17:19:01 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.35578641295433044 norm:0.0009621676872484386 max memory_allocated 22561.62939453125 
[2025-03-22 17:19:34 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.35549163818359375 norm:0.0009361077100038528 max memory_allocated 22561.62939453125 
[2025-03-22 17:20:06 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.3552784323692322 norm:0.0009197227191179991 max memory_allocated 22561.62939453125 
[2025-03-22 17:20:15 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 17:20:51 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.5001639127731323 norm:0.0541556254029274 max memory_allocated 22561.80126953125 
[2025-03-22 17:21:23 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.46878767013549805 norm:0.028568634763360023 max memory_allocated 22561.80126953125 
[2025-03-22 17:21:56 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.4240061938762665 norm:0.011633629910647869 max memory_allocated 22561.80126953125 
[2025-03-22 17:22:28 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.40488940477371216 norm:0.006340100429952145 max memory_allocated 22561.80126953125 
[2025-03-22 17:23:01 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.39739012718200684 norm:0.004849553573876619 max memory_allocated 22561.80126953125 
[2025-03-22 17:23:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.39291059970855713 norm:0.004041227512061596 max memory_allocated 22561.80126953125 
[2025-03-22 17:24:06 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.38952556252479553 norm:0.003415812272578478 max memory_allocated 22561.80126953125 
[2025-03-22 17:24:39 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.38723573088645935 norm:0.0030595792923122644 max memory_allocated 22561.80126953125 
[2025-03-22 17:25:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3853979706764221 norm:0.0026644677855074406 max memory_allocated 22561.80126953125 
[2025-03-22 17:25:44 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.38377857208251953 norm:0.00234282691963017 max memory_allocated 22561.80126953125 
[2025-03-22 17:26:16 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.38264259696006775 norm:0.0020832386799156666 max memory_allocated 22561.80126953125 
[2025-03-22 17:26:49 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.3817102909088135 norm:0.0019320400897413492 max memory_allocated 22561.80126953125 
[2025-03-22 17:27:21 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.38101688027381897 norm:0.0017362426733598113 max memory_allocated 22561.80126953125 
[2025-03-22 17:27:54 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.3804001212120056 norm:0.0015966971404850483 max memory_allocated 22561.80126953125 
[2025-03-22 17:28:26 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.3798670470714569 norm:0.0015330772148445249 max memory_allocated 22561.80126953125 
[2025-03-22 17:28:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.37950944900512695 norm:0.0014548287726938725 max memory_allocated 22561.80126953125 
[2025-03-22 17:29:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.37923067808151245 norm:0.0013844131026417017 max memory_allocated 22561.80126953125 
[2025-03-22 17:30:04 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.3789270222187042 norm:0.001293774927034974 max memory_allocated 22561.80126953125 
[2025-03-22 17:30:37 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.37838470935821533 norm:0.0012328014709055424 max memory_allocated 22561.80126953125 
[2025-03-22 17:31:09 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.3780059814453125 norm:0.0011815010802820325 max memory_allocated 22561.80126953125 
[2025-03-22 17:31:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 17:31:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.4938676357269287 norm:0.03286495432257652 max memory_allocated 22561.97314453125 
[2025-03-22 17:32:26 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.468309223651886 norm:0.01574394665658474 max memory_allocated 22561.97314453125 
[2025-03-22 17:32:59 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.4410437047481537 norm:0.007378591224551201 max memory_allocated 22561.97314453125 
[2025-03-22 17:33:31 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.42604756355285645 norm:0.0033882628194987774 max memory_allocated 22561.97314453125 
[2025-03-22 17:34:04 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.41955527663230896 norm:0.0020733687561005354 max memory_allocated 22561.97314453125 
[2025-03-22 17:34:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.41561228036880493 norm:0.001771177863702178 max memory_allocated 22561.97314453125 
[2025-03-22 17:35:09 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.4128638207912445 norm:0.0015221633948385715 max memory_allocated 22561.97314453125 
[2025-03-22 17:35:42 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.4107896387577057 norm:0.0012819727417081594 max memory_allocated 22561.97314453125 
[2025-03-22 17:36:14 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.4092976450920105 norm:0.0012105058412998915 max memory_allocated 22561.97314453125 
[2025-03-22 17:36:47 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.4082293212413788 norm:0.0011897204676643014 max memory_allocated 22561.97314453125 
[2025-03-22 17:37:19 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.4074428677558899 norm:0.001172265037894249 max memory_allocated 22561.97314453125 
[2025-03-22 17:37:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.40691572427749634 norm:0.00115648889914155 max memory_allocated 22561.97314453125 
[2025-03-22 17:38:25 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.4064110517501831 norm:0.0011406696867197752 max memory_allocated 22561.97314453125 
[2025-03-22 17:38:57 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.4059094786643982 norm:0.001100291614420712 max memory_allocated 22561.97314453125 
[2025-03-22 17:39:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.40538233518600464 norm:0.001079689129255712 max memory_allocated 22561.97314453125 
[2025-03-22 17:40:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.4049857556819916 norm:0.0010783628094941378 max memory_allocated 22561.97314453125 
[2025-03-22 17:40:35 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.4046836197376251 norm:0.0010469849221408367 max memory_allocated 22561.97314453125 
[2025-03-22 17:41:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.4044034779071808 norm:0.0010155695490539074 max memory_allocated 22561.97314453125 
[2025-03-22 17:41:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.40413743257522583 norm:0.0009982611518353224 max memory_allocated 22561.97314453125 
[2025-03-22 17:42:13 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.4039788246154785 norm:0.0010063303634524345 max memory_allocated 22561.97314453125 
[2025-03-22 17:42:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 17:42:57 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.5634723901748657 norm:0.039476849138736725 max memory_allocated 22562.14501953125 
[2025-03-22 17:43:30 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.5291714072227478 norm:0.020622700452804565 max memory_allocated 22562.14501953125 
[2025-03-22 17:44:02 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.4871651232242584 norm:0.009033380076289177 max memory_allocated 22562.14501953125 
[2025-03-22 17:44:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.4676946997642517 norm:0.004937487188726664 max memory_allocated 22562.14501953125 
[2025-03-22 17:45:07 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.459952175617218 norm:0.0031930343247950077 max memory_allocated 22562.14501953125 
[2025-03-22 17:45:40 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.455478698015213 norm:0.002499029040336609 max memory_allocated 22562.14501953125 
[2025-03-22 17:46:12 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.45247882604599 norm:0.0022418033331632614 max memory_allocated 22562.14501953125 
[2025-03-22 17:46:45 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.4502222537994385 norm:0.002059713238850236 max memory_allocated 22562.14501953125 
[2025-03-22 17:47:17 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.448383629322052 norm:0.0019310828065499663 max memory_allocated 22562.14501953125 
[2025-03-22 17:47:50 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.4467625021934509 norm:0.0017812784062698483 max memory_allocated 22562.14501953125 
[2025-03-22 17:48:22 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.44565442204475403 norm:0.0016470729606226087 max memory_allocated 22562.14501953125 
[2025-03-22 17:48:55 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.44450512528419495 norm:0.0013404118362814188 max memory_allocated 22562.14501953125 
[2025-03-22 17:49:27 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.44370442628860474 norm:0.0013326656771823764 max memory_allocated 22562.14501953125 
[2025-03-22 17:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.44301044940948486 norm:0.0012828235048800707 max memory_allocated 22562.14501953125 
[2025-03-22 17:50:32 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.4424794018268585 norm:0.0012939533917233348 max memory_allocated 22562.14501953125 
[2025-03-22 17:51:05 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.44193214178085327 norm:0.0012306172866374254 max memory_allocated 22562.14501953125 
[2025-03-22 17:51:37 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.4415612518787384 norm:0.0011978064430877566 max memory_allocated 22562.14501953125 
[2025-03-22 17:52:10 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.44129467010498047 norm:0.001186911016702652 max memory_allocated 22562.14501953125 
[2025-03-22 17:52:43 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.44104063510894775 norm:0.0011528050526976585 max memory_allocated 22562.14501953125 
[2025-03-22 17:53:15 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.4407353103160858 norm:0.0011371808359399438 max memory_allocated 22562.14501953125 
[2025-03-22 17:53:24 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 17:54:00 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.5946044325828552 norm:0.03555775433778763 max memory_allocated 22562.31689453125 
[2025-03-22 17:54:32 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.5671250224113464 norm:0.01911521516740322 max memory_allocated 22562.31689453125 
[2025-03-22 17:55:05 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.5335974097251892 norm:0.008080016821622849 max memory_allocated 22562.31689453125 
[2025-03-22 17:55:37 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.5161330699920654 norm:0.0037327436730265617 max memory_allocated 22562.31689453125 
[2025-03-22 17:56:10 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.5091677904129028 norm:0.002524532377719879 max memory_allocated 22562.31689453125 
[2025-03-22 17:56:42 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.5050640106201172 norm:0.0020945339929312468 max memory_allocated 22562.31689453125 
[2025-03-22 17:57:15 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.5021963715553284 norm:0.0019187028519809246 max memory_allocated 22562.31689453125 
[2025-03-22 17:57:48 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.49994969367980957 norm:0.001746433088555932 max memory_allocated 22562.31689453125 
[2025-03-22 17:58:20 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.498182475566864 norm:0.0016571921296417713 max memory_allocated 22562.31689453125 
[2025-03-22 17:58:53 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.49688151478767395 norm:0.0015955877024680376 max memory_allocated 22562.31689453125 
[2025-03-22 17:59:26 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.49577632546424866 norm:0.0015310178278014064 max memory_allocated 22562.31689453125 
[2025-03-22 17:59:58 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.49484747648239136 norm:0.001475190743803978 max memory_allocated 22562.31689453125 
[2025-03-22 18:00:31 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.49431973695755005 norm:0.001460900530219078 max memory_allocated 22562.31689453125 
[2025-03-22 18:01:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.4935442805290222 norm:0.0014154498931020498 max memory_allocated 22562.31689453125 
[2025-03-22 18:01:36 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.4929770231246948 norm:0.0014133926015347242 max memory_allocated 22562.31689453125 
[2025-03-22 18:02:08 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.49239152669906616 norm:0.0013693032087758183 max memory_allocated 22562.31689453125 
[2025-03-22 18:02:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.491837739944458 norm:0.0013049960834905505 max memory_allocated 22562.31689453125 
[2025-03-22 18:03:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.4913974404335022 norm:0.0012657574843615294 max memory_allocated 22562.31689453125 
[2025-03-22 18:03:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.49114346504211426 norm:0.0012517333962023258 max memory_allocated 22562.31689453125 
[2025-03-22 18:04:19 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.491006076335907 norm:0.001222481019794941 max memory_allocated 22562.31689453125 
[2025-03-22 18:04:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 18:05:03 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.6551089286804199 norm:0.03750355914235115 max memory_allocated 22562.48876953125 
[2025-03-22 18:05:35 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.6270464658737183 norm:0.018797732889652252 max memory_allocated 22562.48876953125 
[2025-03-22 18:06:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.5948489904403687 norm:0.008576124906539917 max memory_allocated 22562.48876953125 
[2025-03-22 18:06:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.5793942213058472 norm:0.004656387958675623 max memory_allocated 22562.48876953125 
[2025-03-22 18:07:12 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.5733835697174072 norm:0.0036324062384665012 max memory_allocated 22562.48876953125 
[2025-03-22 18:07:45 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.5693146586418152 norm:0.0026380126364529133 max memory_allocated 22562.48876953125 
[2025-03-22 18:08:17 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.5655964612960815 norm:0.0013795605627819896 max memory_allocated 22562.48876953125 
[2025-03-22 18:08:50 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.563615083694458 norm:0.0012904934119433165 max memory_allocated 22562.48876953125 
[2025-03-22 18:09:23 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.5620630979537964 norm:0.0012817725073546171 max memory_allocated 22562.48876953125 
[2025-03-22 18:09:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.5608842968940735 norm:0.001269177533686161 max memory_allocated 22562.48876953125 
[2025-03-22 18:10:28 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.5601418018341064 norm:0.0012785461731255054 max memory_allocated 22562.48876953125 
[2025-03-22 18:11:00 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.5594848394393921 norm:0.0012639123015105724 max memory_allocated 22562.48876953125 
[2025-03-22 18:11:33 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.5590157508850098 norm:0.0012574965367093682 max memory_allocated 22562.48876953125 
[2025-03-22 18:12:06 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.558627188205719 norm:0.0012627590913325548 max memory_allocated 22562.48876953125 
[2025-03-22 18:12:38 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.5582918524742126 norm:0.001263156533241272 max memory_allocated 22562.48876953125 
[2025-03-22 18:13:11 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.5579749941825867 norm:0.001248268294148147 max memory_allocated 22562.48876953125 
[2025-03-22 18:13:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.5576468706130981 norm:0.0012349957833066583 max memory_allocated 22562.48876953125 
[2025-03-22 18:14:16 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.5570599436759949 norm:0.0012086168862879276 max memory_allocated 22562.48876953125 
[2025-03-22 18:14:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.5567952394485474 norm:0.0011928578605875373 max memory_allocated 22562.48876953125 
[2025-03-22 18:15:21 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.556386411190033 norm:0.0011705210199579597 max memory_allocated 22562.48876953125 
[2025-03-22 18:15:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 18:16:06 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.7215606570243835 norm:0.027883978560566902 max memory_allocated 22562.66064453125 
[2025-03-22 18:16:38 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.6986576318740845 norm:0.015280288644134998 max memory_allocated 22562.66064453125 
[2025-03-22 18:17:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.6749659776687622 norm:0.00874337088316679 max memory_allocated 22562.66064453125 
[2025-03-22 18:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.662852942943573 norm:0.005777249112725258 max memory_allocated 22562.66064453125 
[2025-03-22 18:18:16 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.6576160192489624 norm:0.0045511177740991116 max memory_allocated 22562.66064453125 
[2025-03-22 18:18:48 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.6539186239242554 norm:0.0036695420276373625 max memory_allocated 22562.66064453125 
[2025-03-22 18:19:21 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.6512498259544373 norm:0.0030815491918474436 max memory_allocated 22562.66064453125 
[2025-03-22 18:19:53 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.6489403247833252 norm:0.0025849342346191406 max memory_allocated 22562.66064453125 
[2025-03-22 18:20:25 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.6471550464630127 norm:0.0022557699121534824 max memory_allocated 22562.66064453125 
[2025-03-22 18:20:58 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.6459164619445801 norm:0.002045760862529278 max memory_allocated 22562.66064453125 
[2025-03-22 18:21:30 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.6445431113243103 norm:0.0017840424552559853 max memory_allocated 22562.66064453125 
[2025-03-22 18:22:03 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.6433253884315491 norm:0.001429217867553234 max memory_allocated 22562.66064453125 
[2025-03-22 18:22:35 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.6423324346542358 norm:0.0011915252543985844 max memory_allocated 22562.66064453125 
[2025-03-22 18:23:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.6417766809463501 norm:0.0011868051951751113 max memory_allocated 22562.66064453125 
[2025-03-22 18:23:40 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.6412206888198853 norm:0.0011550227645784616 max memory_allocated 22562.66064453125 
[2025-03-22 18:24:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.640667736530304 norm:0.0011529868934303522 max memory_allocated 22562.66064453125 
[2025-03-22 18:24:45 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.6403104066848755 norm:0.0011482344707474113 max memory_allocated 22562.66064453125 
[2025-03-22 18:25:18 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.6399231553077698 norm:0.0011173202656209469 max memory_allocated 22562.66064453125 
[2025-03-22 18:25:50 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.6396634578704834 norm:0.0010995916090905666 max memory_allocated 22562.66064453125 
[2025-03-22 18:26:23 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.6394380927085876 norm:0.0010975011391565204 max memory_allocated 22562.66064453125 
[2025-03-22 18:26:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 18:27:07 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.8486506342887878 norm:0.02957899309694767 max memory_allocated 22562.83251953125 
[2025-03-22 18:27:40 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.8239573240280151 norm:0.01651648059487343 max memory_allocated 22562.83251953125 
[2025-03-22 18:28:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.795127272605896 norm:0.00906501617282629 max memory_allocated 22562.83251953125 
[2025-03-22 18:28:45 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.778244137763977 norm:0.00555049255490303 max memory_allocated 22562.83251953125 
[2025-03-22 18:29:17 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.7727725505828857 norm:0.0044520762749016285 max memory_allocated 22562.83251953125 
[2025-03-22 18:29:50 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.7689071893692017 norm:0.0036840266548097134 max memory_allocated 22562.83251953125 
[2025-03-22 18:30:23 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.7660838961601257 norm:0.0031824971083551645 max memory_allocated 22562.83251953125 
[2025-03-22 18:30:55 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.7641979455947876 norm:0.002931224415078759 max memory_allocated 22562.83251953125 
[2025-03-22 18:31:28 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.7617502808570862 norm:0.0016054855659604073 max memory_allocated 22562.83251953125 
[2025-03-22 18:32:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.7602818012237549 norm:0.001464451546780765 max memory_allocated 22562.83251953125 
[2025-03-22 18:32:33 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.7596027851104736 norm:0.0014940488617867231 max memory_allocated 22562.83251953125 
[2025-03-22 18:33:06 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.7589686512947083 norm:0.0014638763386756182 max memory_allocated 22562.83251953125 
[2025-03-22 18:33:38 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.75822913646698 norm:0.0014371832367032766 max memory_allocated 22562.83251953125 
[2025-03-22 18:34:11 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.7578275203704834 norm:0.0014461165992543101 max memory_allocated 22562.83251953125 
[2025-03-22 18:34:43 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.7575430870056152 norm:0.0013995777117088437 max memory_allocated 22562.83251953125 
[2025-03-22 18:35:16 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.7570289373397827 norm:0.0013860482722520828 max memory_allocated 22562.83251953125 
[2025-03-22 18:35:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.756646990776062 norm:0.0013980218209326267 max memory_allocated 22562.83251953125 
[2025-03-22 18:36:21 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.7565591335296631 norm:0.0013872047420591116 max memory_allocated 22562.83251953125 
[2025-03-22 18:36:53 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.7561778426170349 norm:0.0013697799295186996 max memory_allocated 22562.83251953125 
[2025-03-22 18:37:26 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.7560021877288818 norm:0.001370623242110014 max memory_allocated 22562.83251953125 
[2025-03-22 18:37:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 18:38:10 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.9599092602729797 norm:0.012043401598930359 max memory_allocated 22563.00439453125 
[2025-03-22 18:38:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.9471046924591064 norm:0.007970673963427544 max memory_allocated 22563.00439453125 
[2025-03-22 18:39:15 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.9247400164604187 norm:0.003916105721145868 max memory_allocated 22563.00439453125 
[2025-03-22 18:39:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.9119790196418762 norm:0.002714909380301833 max memory_allocated 22563.00439453125 
[2025-03-22 18:40:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.9061790704727173 norm:0.0019771393854171038 max memory_allocated 22563.00439453125 
[2025-03-22 18:40:52 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.9029322266578674 norm:0.0018398566171526909 max memory_allocated 22563.00439453125 
[2025-03-22 18:41:25 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.9006692171096802 norm:0.0018063277238979936 max memory_allocated 22563.00439453125 
[2025-03-22 18:41:57 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.8989958167076111 norm:0.001800262602046132 max memory_allocated 22563.00439453125 
[2025-03-22 18:42:30 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.8977407813072205 norm:0.0017715569119900465 max memory_allocated 22563.00439453125 
[2025-03-22 18:43:02 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.8967908024787903 norm:0.0017428728751838207 max memory_allocated 22563.00439453125 
[2025-03-22 18:43:35 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.8960375785827637 norm:0.001693048281595111 max memory_allocated 22563.00439453125 
[2025-03-22 18:44:08 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.8956384062767029 norm:0.0017319085309281945 max memory_allocated 22563.00439453125 
[2025-03-22 18:44:40 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.8951225876808167 norm:0.0016834866255521774 max memory_allocated 22563.00439453125 
[2025-03-22 18:45:13 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.894477128982544 norm:0.0016876691952347755 max memory_allocated 22563.00439453125 
[2025-03-22 18:45:45 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.8941043019294739 norm:0.001613637083210051 max memory_allocated 22563.00439453125 
[2025-03-22 18:46:18 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.8937318921089172 norm:0.0015693532768636942 max memory_allocated 22563.00439453125 
[2025-03-22 18:46:51 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.8934308290481567 norm:0.0015749017475172877 max memory_allocated 22563.00439453125 
[2025-03-22 18:47:23 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.8930564522743225 norm:0.0015495261177420616 max memory_allocated 22563.00439453125 
[2025-03-22 18:47:56 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.8926038146018982 norm:0.0015398792456835508 max memory_allocated 22563.00439453125 
[2025-03-22 18:48:28 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.8923659324645996 norm:0.0015239990316331387 max memory_allocated 22563.00439453125 
[2025-03-22 18:48:37 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 18:49:13 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:1.0990808010101318 norm:0.012226555496454239 max memory_allocated 22563.17626953125 
[2025-03-22 18:49:45 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:1.078851342201233 norm:0.007097652647644281 max memory_allocated 22563.17626953125 
[2025-03-22 18:50:18 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:1.0561120510101318 norm:0.003816478420048952 max memory_allocated 22563.17626953125 
[2025-03-22 18:50:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:1.0470248460769653 norm:0.002697554649785161 max memory_allocated 22563.17626953125 
[2025-03-22 18:51:23 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:1.0419328212738037 norm:0.00198673689737916 max memory_allocated 22563.17626953125 
[2025-03-22 18:51:56 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:1.0385864973068237 norm:0.0017452896572649479 max memory_allocated 22563.17626953125 
[2025-03-22 18:52:28 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:1.0360069274902344 norm:0.0015577400336042047 max memory_allocated 22563.17626953125 
[2025-03-22 18:53:01 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:1.0341932773590088 norm:0.001388842472806573 max memory_allocated 22563.17626953125 
[2025-03-22 18:53:33 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:1.0330238342285156 norm:0.001351261860691011 max memory_allocated 22563.17626953125 
[2025-03-22 18:54:06 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:1.0320302248001099 norm:0.0013279221020638943 max memory_allocated 22563.17626953125 
[2025-03-22 18:54:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:1.0312469005584717 norm:0.0012907171621918678 max memory_allocated 22563.17626953125 
[2025-03-22 18:55:11 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:1.0306320190429688 norm:0.001267818035557866 max memory_allocated 22563.17626953125 
[2025-03-22 18:55:44 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:1.030109167098999 norm:0.0012583989882841706 max memory_allocated 22563.17626953125 
[2025-03-22 18:56:16 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:1.0299707651138306 norm:0.0012633082224056125 max memory_allocated 22563.17626953125 
[2025-03-22 18:56:49 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:1.0296335220336914 norm:0.0012480251025408506 max memory_allocated 22563.17626953125 
[2025-03-22 18:57:21 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:1.0293699502944946 norm:0.001231229631230235 max memory_allocated 22563.17626953125 
[2025-03-22 18:57:54 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:1.029164433479309 norm:0.0012215141905471683 max memory_allocated 22563.17626953125 
[2025-03-22 18:58:26 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:1.0289435386657715 norm:0.0012220023199915886 max memory_allocated 22563.17626953125 
[2025-03-22 18:58:59 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:1.0286643505096436 norm:0.0012196721509099007 max memory_allocated 22563.17626953125 
[2025-03-22 18:59:31 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:1.028515100479126 norm:0.0012131972471252084 max memory_allocated 22563.17626953125 
[2025-03-22 18:59:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 19:00:15 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:1.285402774810791 norm:0.02141396515071392 max memory_allocated 22563.34814453125 
[2025-03-22 19:00:48 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:1.262380838394165 norm:0.013935891911387444 max memory_allocated 22563.34814453125 
[2025-03-22 19:01:20 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:1.233527660369873 norm:0.0074973683804273605 max memory_allocated 22563.34814453125 
[2025-03-22 19:01:53 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:1.2195100784301758 norm:0.004277793690562248 max memory_allocated 22563.34814453125 
[2025-03-22 19:02:25 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:1.2125924825668335 norm:0.0031926732044667006 max memory_allocated 22563.34814453125 
[2025-03-22 19:02:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:1.2080161571502686 norm:0.0027433994691818953 max memory_allocated 22563.34814453125 
[2025-03-22 19:03:30 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:1.2033131122589111 norm:0.0016945003299042583 max memory_allocated 22563.34814453125 
[2025-03-22 19:04:03 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:1.2011711597442627 norm:0.0015432881191372871 max memory_allocated 22563.34814453125 
[2025-03-22 19:04:36 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:1.1999222040176392 norm:0.0014935985673218966 max memory_allocated 22563.34814453125 
[2025-03-22 19:05:08 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:1.1986943483352661 norm:0.001454402576200664 max memory_allocated 22563.34814453125 
[2025-03-22 19:05:41 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:1.197847843170166 norm:0.001421233988367021 max memory_allocated 22563.34814453125 
[2025-03-22 19:06:13 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:1.1972200870513916 norm:0.0014235347043722868 max memory_allocated 22563.34814453125 
[2025-03-22 19:06:46 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:1.1966609954833984 norm:0.0014046956785023212 max memory_allocated 22563.34814453125 
[2025-03-22 19:07:19 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:1.1963852643966675 norm:0.0014025070704519749 max memory_allocated 22563.34814453125 
[2025-03-22 19:07:51 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:1.1959818601608276 norm:0.0013963909586891532 max memory_allocated 22563.34814453125 
[2025-03-22 19:08:24 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:1.1957308053970337 norm:0.0013890028931200504 max memory_allocated 22563.34814453125 
[2025-03-22 19:08:56 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:1.1954940557479858 norm:0.0013868029927834868 max memory_allocated 22563.34814453125 
[2025-03-22 19:09:29 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:1.195347547531128 norm:0.0013856784207746387 max memory_allocated 22563.34814453125 
[2025-03-22 19:10:02 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:1.1951825618743896 norm:0.0013805945636704564 max memory_allocated 22563.34814453125 
[2025-03-22 19:10:34 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:1.1951066255569458 norm:0.0013778435532003641 max memory_allocated 22563.34814453125 
[2025-03-22 19:10:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 19:11:19 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:1.4482839107513428 norm:0.030624568462371826 max memory_allocated 22563.52001953125 
[2025-03-22 19:11:51 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:1.4285935163497925 norm:0.021066151559352875 max memory_allocated 22563.52001953125 
[2025-03-22 19:12:24 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:1.4017510414123535 norm:0.013975986279547215 max memory_allocated 22563.52001953125 
[2025-03-22 19:12:56 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:1.3889366388320923 norm:0.010311117395758629 max memory_allocated 22563.52001953125 
[2025-03-22 19:13:29 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:1.380699634552002 norm:0.007668070495128632 max memory_allocated 22563.52001953125 
[2025-03-22 19:14:02 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:1.3743433952331543 norm:0.006072930991649628 max memory_allocated 22563.52001953125 
[2025-03-22 19:14:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:1.3705863952636719 norm:0.0052495719864964485 max memory_allocated 22563.52001953125 
[2025-03-22 19:15:07 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:1.3679163455963135 norm:0.004670485854148865 max memory_allocated 22563.52001953125 
[2025-03-22 19:15:39 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:1.3658629655838013 norm:0.004171956796199083 max memory_allocated 22563.52001953125 
[2025-03-22 19:16:12 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:1.3643709421157837 norm:0.0037691339384764433 max memory_allocated 22563.52001953125 
[2025-03-22 19:16:44 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:1.3632991313934326 norm:0.0034288910683244467 max memory_allocated 22563.52001953125 
[2025-03-22 19:17:17 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:1.3622599840164185 norm:0.0031466963700950146 max memory_allocated 22563.52001953125 
[2025-03-22 19:17:49 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:1.3613569736480713 norm:0.0028866336215287447 max memory_allocated 22563.52001953125 
[2025-03-22 19:18:21 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:1.3605432510375977 norm:0.002646439475938678 max memory_allocated 22563.52001953125 
[2025-03-22 19:18:54 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:1.3598496913909912 norm:0.002457971451804042 max memory_allocated 22563.52001953125 
[2025-03-22 19:19:26 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:1.3588584661483765 norm:0.0021560925524681807 max memory_allocated 22563.52001953125 
[2025-03-22 19:19:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:1.3582123517990112 norm:0.0020279460586607456 max memory_allocated 22563.52001953125 
[2025-03-22 19:20:31 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:1.3578739166259766 norm:0.0019424293423071504 max memory_allocated 22563.52001953125 
[2025-03-22 19:21:04 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:1.3574763536453247 norm:0.0018626225646585226 max memory_allocated 22563.52001953125 
[2025-03-22 19:21:36 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:1.3571239709854126 norm:0.0018030217615887523 max memory_allocated 22563.52001953125 
[2025-03-22 19:21:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 19:22:21 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:1.6238677501678467 norm:0.010122914798557758 max memory_allocated 22563.69189453125 
[2025-03-22 19:22:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:1.6036580801010132 norm:0.005869775079190731 max memory_allocated 22563.69189453125 
[2025-03-22 19:23:26 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:1.5770474672317505 norm:0.002940807491540909 max memory_allocated 22563.69189453125 
[2025-03-22 19:23:58 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:1.5658471584320068 norm:0.0018649329431355 max memory_allocated 22563.69189453125 
[2025-03-22 19:24:31 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:1.5604279041290283 norm:0.0015845524612814188 max memory_allocated 22563.69189453125 
[2025-03-22 19:25:04 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:1.5561842918395996 norm:0.0014613615348935127 max memory_allocated 22563.69189453125 
[2025-03-22 19:25:36 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:1.5530143976211548 norm:0.0013721393188461661 max memory_allocated 22563.69189453125 
[2025-03-22 19:26:09 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:1.5508928298950195 norm:0.0013403450138866901 max memory_allocated 22563.69189453125 
[2025-03-22 19:26:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:1.5492238998413086 norm:0.0013118237257003784 max memory_allocated 22563.69189453125 
[2025-03-22 19:27:14 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:1.5480870008468628 norm:0.0012947324430570006 max memory_allocated 22563.69189453125 
[2025-03-22 19:27:46 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:1.547302484512329 norm:0.001279292395338416 max memory_allocated 22563.69189453125 
[2025-03-22 19:28:19 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:1.546608328819275 norm:0.0012676117476075888 max memory_allocated 22563.69189453125 
[2025-03-22 19:28:52 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:1.5460479259490967 norm:0.0012611999409273267 max memory_allocated 22563.69189453125 
[2025-03-22 19:29:24 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:1.54554283618927 norm:0.001250654924660921 max memory_allocated 22563.69189453125 
[2025-03-22 19:29:57 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:1.5450949668884277 norm:0.001235544797964394 max memory_allocated 22563.69189453125 
[2025-03-22 19:30:30 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:1.544691801071167 norm:0.0012334233615547419 max memory_allocated 22563.69189453125 
[2025-03-22 19:31:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:1.5444648265838623 norm:0.0012345401337370276 max memory_allocated 22563.69189453125 
[2025-03-22 19:31:35 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:1.544081211090088 norm:0.0012266701087355614 max memory_allocated 22563.69189453125 
[2025-03-22 19:32:07 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:1.5438880920410156 norm:0.0012270987499505281 max memory_allocated 22563.69189453125 
[2025-03-22 19:32:40 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:1.5437431335449219 norm:0.0012229408603161573 max memory_allocated 22563.69189453125 
[2025-03-22 19:32:49 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 19:33:24 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:1.8446561098098755 norm:0.027168555185198784 max memory_allocated 22563.86376953125 
[2025-03-22 19:33:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:1.8135623931884766 norm:0.013757891952991486 max memory_allocated 22563.86376953125 
[2025-03-22 19:34:29 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:1.7826611995697021 norm:0.006516898050904274 max memory_allocated 22563.86376953125 
[2025-03-22 19:35:02 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:1.7684334516525269 norm:0.003694605315104127 max memory_allocated 22563.86376953125 
[2025-03-22 19:35:34 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:1.7607008218765259 norm:0.002876867540180683 max memory_allocated 22563.86376953125 
[2025-03-22 19:36:07 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:1.7548185586929321 norm:0.002321709878742695 max memory_allocated 22563.86376953125 
[2025-03-22 19:36:39 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:1.7508037090301514 norm:0.0019835324492305517 max memory_allocated 22563.86376953125 
[2025-03-22 19:37:12 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:1.7479603290557861 norm:0.0017541245324537158 max memory_allocated 22563.86376953125 
[2025-03-22 19:37:44 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:1.7461116313934326 norm:0.001596052199602127 max memory_allocated 22563.86376953125 
[2025-03-22 19:38:17 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:1.7447993755340576 norm:0.0014878337970003486 max memory_allocated 22563.86376953125 
[2025-03-22 19:38:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:1.7437816858291626 norm:0.0014141328865662217 max memory_allocated 22563.86376953125 
[2025-03-22 19:39:22 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:1.7429661750793457 norm:0.001362537732347846 max memory_allocated 22563.86376953125 
[2025-03-22 19:39:54 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:1.7422668933868408 norm:0.0013267010217532516 max memory_allocated 22563.86376953125 
[2025-03-22 19:40:27 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:1.7416166067123413 norm:0.0012922557070851326 max memory_allocated 22563.86376953125 
[2025-03-22 19:40:59 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:1.7411600351333618 norm:0.0012716585770249367 max memory_allocated 22563.86376953125 
[2025-03-22 19:41:32 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:1.7408064603805542 norm:0.0012547088554129004 max memory_allocated 22563.86376953125 
[2025-03-22 19:42:04 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:1.7404935359954834 norm:0.0012458174023777246 max memory_allocated 22563.86376953125 
[2025-03-22 19:42:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:1.7402431964874268 norm:0.001238578581251204 max memory_allocated 22563.86376953125 
[2025-03-22 19:43:10 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:1.7399195432662964 norm:0.0012307566357776523 max memory_allocated 22563.86376953125 
[2025-03-22 19:43:42 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:1.7397515773773193 norm:0.0012263183016330004 max memory_allocated 22563.86376953125 
[2025-03-22 19:43:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 19:44:27 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:2.0522301197052 norm:0.022944767028093338 max memory_allocated 22564.03564453125 
[2025-03-22 19:44:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:2.0172524452209473 norm:0.010453546419739723 max memory_allocated 22564.03564453125 
[2025-03-22 19:45:32 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:1.9860728979110718 norm:0.006219938397407532 max memory_allocated 22564.03564453125 
[2025-03-22 19:46:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:1.9686819314956665 norm:0.0028430880047380924 max memory_allocated 22564.03564453125 
[2025-03-22 19:46:37 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:1.9608339071273804 norm:0.0021556548308581114 max memory_allocated 22564.03564453125 
[2025-03-22 19:47:10 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:1.9559078216552734 norm:0.0018922867020592093 max memory_allocated 22564.03564453125 
[2025-03-22 19:47:42 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:1.9522204399108887 norm:0.0017293053679168224 max memory_allocated 22564.03564453125 
[2025-03-22 19:48:15 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:1.949899673461914 norm:0.0016080851200968027 max memory_allocated 22564.03564453125 
[2025-03-22 19:48:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:1.948230504989624 norm:0.0015799588290974498 max memory_allocated 22564.03564453125 
[2025-03-22 19:49:20 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:1.946749210357666 norm:0.0015142700867727399 max memory_allocated 22564.03564453125 
[2025-03-22 19:49:53 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:1.94571852684021 norm:0.0014845668338239193 max memory_allocated 22564.03564453125 
[2025-03-22 19:50:25 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:1.9447872638702393 norm:0.0014346673851832747 max memory_allocated 22564.03564453125 
[2025-03-22 19:50:58 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:1.9439946413040161 norm:0.0013858155580237508 max memory_allocated 22564.03564453125 
[2025-03-22 19:51:30 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:1.9433625936508179 norm:0.001372741418890655 max memory_allocated 22564.03564453125 
[2025-03-22 19:52:03 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:1.9429523944854736 norm:0.0013548493152484298 max memory_allocated 22564.03564453125 
[2025-03-22 19:52:35 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:1.9421147108078003 norm:0.0014445458073168993 max memory_allocated 22564.03564453125 
[2025-03-22 19:53:08 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:1.9413578510284424 norm:0.0014001074014231563 max memory_allocated 22564.03564453125 
[2025-03-22 19:53:40 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:1.9419426918029785 norm:0.0013429761165753007 max memory_allocated 22564.03564453125 
[2025-03-22 19:54:13 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:1.9415167570114136 norm:0.001329892547801137 max memory_allocated 22564.03564453125 
[2025-03-22 19:54:45 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:1.9411776065826416 norm:0.0013134963810443878 max memory_allocated 22564.03564453125 
[2025-03-22 19:54:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 19:54:57 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 19:55:30 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:2.37261962890625 norm:0.08018247038125992 max memory_allocated 22565.12939453125 
[2025-03-22 19:56:02 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:2.3359169960021973 norm:0.06690196692943573 max memory_allocated 22565.12939453125 
[2025-03-22 19:56:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:2.296581745147705 norm:0.05036075785756111 max memory_allocated 22565.12939453125 
[2025-03-22 19:57:08 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:2.272976875305176 norm:0.03833095729351044 max memory_allocated 22565.12939453125 
[2025-03-22 19:57:40 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:2.260174036026001 norm:0.030050059780478477 max memory_allocated 22565.12939453125 
[2025-03-22 19:58:13 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:2.250530242919922 norm:0.025314103811979294 max memory_allocated 22565.12939453125 
[2025-03-22 19:58:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:2.2433784008026123 norm:0.02264520712196827 max memory_allocated 22565.12939453125 
[2025-03-22 19:59:18 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:2.2379961013793945 norm:0.020859938114881516 max memory_allocated 22565.12939453125 
[2025-03-22 19:59:51 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:2.2360339164733887 norm:0.020272178575396538 max memory_allocated 22565.12939453125 
[2025-03-22 20:00:24 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:2.230393886566162 norm:0.01918814890086651 max memory_allocated 22565.12939453125 
[2025-03-22 20:00:57 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:2.2269034385681152 norm:0.017146626487374306 max memory_allocated 22565.12939453125 
[2025-03-22 20:01:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:2.224506139755249 norm:0.017150184139609337 max memory_allocated 22565.12939453125 
[2025-03-22 20:02:02 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:2.2220561504364014 norm:0.016246788203716278 max memory_allocated 22565.12939453125 
[2025-03-22 20:02:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:2.22029709815979 norm:0.01635836437344551 max memory_allocated 22565.12939453125 
[2025-03-22 20:03:08 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:2.217741012573242 norm:0.015078802593052387 max memory_allocated 22565.12939453125 
[2025-03-22 20:03:41 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:2.2161850929260254 norm:0.015439050272107124 max memory_allocated 22565.12939453125 
[2025-03-22 20:04:13 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:2.216732978820801 norm:0.013556107878684998 max memory_allocated 22565.12939453125 
[2025-03-22 20:04:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:2.216106414794922 norm:0.016359303146600723 max memory_allocated 22565.12939453125 
[2025-03-22 20:05:19 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:2.2156708240509033 norm:0.010897990316152573 max memory_allocated 22565.12939453125 
[2025-03-22 20:05:51 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:2.2135186195373535 norm:0.013018443249166012 max memory_allocated 22565.12939453125 
[2025-03-22 20:06:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 20:06:03 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:06:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:2.692862033843994 norm:0.08354852348566055 max memory_allocated 22565.30126953125 
[2025-03-22 20:07:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:2.6443119049072266 norm:0.06922326982021332 max memory_allocated 22565.30126953125 
[2025-03-22 20:07:41 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:2.590290069580078 norm:0.05142439901828766 max memory_allocated 22565.30126953125 
[2025-03-22 20:08:14 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:2.559828281402588 norm:0.03783321753144264 max memory_allocated 22565.30126953125 
[2025-03-22 20:08:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:2.544612407684326 norm:0.02897457405924797 max memory_allocated 22565.30126953125 
[2025-03-22 20:09:19 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:2.5347280502319336 norm:0.0233855452388525 max memory_allocated 22565.30126953125 
[2025-03-22 20:09:52 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:2.5277886390686035 norm:0.020800648257136345 max memory_allocated 22565.30126953125 
[2025-03-22 20:10:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:2.5229508876800537 norm:0.018253084272146225 max memory_allocated 22565.30126953125 
[2025-03-22 20:10:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:2.51998233795166 norm:0.01782337948679924 max memory_allocated 22565.30126953125 
[2025-03-22 20:11:30 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:2.5171613693237305 norm:0.016107909381389618 max memory_allocated 22565.30126953125 
[2025-03-22 20:12:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:2.5150341987609863 norm:0.016091063618659973 max memory_allocated 22565.30126953125 
[2025-03-22 20:12:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:2.512982130050659 norm:0.014391611330211163 max memory_allocated 22565.30126953125 
[2025-03-22 20:13:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:2.511571168899536 norm:0.01456491183489561 max memory_allocated 22565.30126953125 
[2025-03-22 20:13:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:2.5110020637512207 norm:0.014984611421823502 max memory_allocated 22565.30126953125 
[2025-03-22 20:14:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:2.5106282234191895 norm:0.013899213634431362 max memory_allocated 22565.30126953125 
[2025-03-22 20:14:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:2.5101699829101562 norm:0.01462461892515421 max memory_allocated 22565.30126953125 
[2025-03-22 20:15:19 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:2.5089681148529053 norm:0.013775942847132683 max memory_allocated 22565.30126953125 
[2025-03-22 20:15:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:2.5088565349578857 norm:0.013426070101559162 max memory_allocated 22565.30126953125 
[2025-03-22 20:16:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:2.508671998977661 norm:0.013316627591848373 max memory_allocated 22565.30126953125 
[2025-03-22 20:16:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:2.5088024139404297 norm:0.013799848034977913 max memory_allocated 22565.30126953125 
[2025-03-22 20:17:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 20:17:09 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:17:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:3.6199872493743896 norm:0.19145697355270386 max memory_allocated 22565.47314453125 
[2025-03-22 20:18:14 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:3.5051002502441406 norm:0.15540999174118042 max memory_allocated 22565.47314453125 
[2025-03-22 20:18:47 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:3.3530778884887695 norm:0.13240255415439606 max memory_allocated 22565.47314453125 
[2025-03-22 20:19:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:3.274768352508545 norm:0.1017647385597229 max memory_allocated 22565.47314453125 
[2025-03-22 20:19:52 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:3.242473602294922 norm:0.10074539482593536 max memory_allocated 22565.47314453125 
[2025-03-22 20:20:25 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:3.2200284004211426 norm:0.09142862260341644 max memory_allocated 22565.47314453125 
[2025-03-22 20:20:58 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:3.199406862258911 norm:0.08892805129289627 max memory_allocated 22565.47314453125 
[2025-03-22 20:21:31 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:3.1887667179107666 norm:0.08360309898853302 max memory_allocated 22565.47314453125 
[2025-03-22 20:22:03 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:3.1794071197509766 norm:0.07748870551586151 max memory_allocated 22565.47314453125 
[2025-03-22 20:22:36 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:3.1814117431640625 norm:0.08600759506225586 max memory_allocated 22565.47314453125 
[2025-03-22 20:23:09 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:3.1693317890167236 norm:0.08125830441713333 max memory_allocated 22565.47314453125 
[2025-03-22 20:23:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:3.1553120613098145 norm:0.06995898485183716 max memory_allocated 22565.47314453125 
[2025-03-22 20:24:14 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:3.151549816131592 norm:0.06420902907848358 max memory_allocated 22565.47314453125 
[2025-03-22 20:24:46 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:3.154987335205078 norm:0.06775877624750137 max memory_allocated 22565.47314453125 
[2025-03-22 20:25:19 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:3.153535842895508 norm:0.07220900803804398 max memory_allocated 22565.47314453125 
[2025-03-22 20:25:52 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:3.1584136486053467 norm:0.0760834664106369 max memory_allocated 22565.47314453125 
[2025-03-22 20:26:24 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:3.145172595977783 norm:0.0738835483789444 max memory_allocated 22565.47314453125 
[2025-03-22 20:26:57 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:3.138185977935791 norm:0.05874188244342804 max memory_allocated 22565.47314453125 
[2025-03-22 20:27:30 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:3.1397757530212402 norm:0.058290932327508926 max memory_allocated 22565.47314453125 
[2025-03-22 20:28:02 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:3.14150071144104 norm:0.06135540455579758 max memory_allocated 22565.47314453125 
[2025-03-22 20:28:11 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 20:28:14 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 20:28:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:7.030322074890137 norm:0.7254128456115723 max memory_allocated 22565.64501953125 
[2025-03-22 20:29:19 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:6.438002586364746 norm:0.5362930297851562 max memory_allocated 22565.64501953125 
[2025-03-22 20:29:52 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:6.06218957901001 norm:0.44104304909706116 max memory_allocated 22565.64501953125 
[2025-03-22 20:30:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:5.819952964782715 norm:0.3816272020339966 max memory_allocated 22565.64501953125 
[2025-03-22 20:30:57 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:5.703167915344238 norm:0.3244313597679138 max memory_allocated 22565.64501953125 
[2025-03-22 20:31:30 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:5.622760772705078 norm:0.2739323377609253 max memory_allocated 22565.64501953125 
[2025-03-22 20:32:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:5.568315505981445 norm:0.24212899804115295 max memory_allocated 22565.64501953125 
[2025-03-22 20:32:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:5.524194240570068 norm:0.2320782095193863 max memory_allocated 22565.64501953125 
[2025-03-22 20:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:5.483599662780762 norm:0.2231736183166504 max memory_allocated 22565.64501953125 
[2025-03-22 20:33:41 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:5.442486763000488 norm:0.2185671627521515 max memory_allocated 22565.64501953125 
[2025-03-22 20:34:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:5.430236339569092 norm:0.22470031678676605 max memory_allocated 22565.64501953125 
[2025-03-22 20:34:46 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:5.404215335845947 norm:0.22264836728572845 max memory_allocated 22565.64501953125 
[2025-03-22 20:35:19 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:5.384687423706055 norm:0.20426933467388153 max memory_allocated 22565.64501953125 
[2025-03-22 20:35:52 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:5.367928981781006 norm:0.20108142495155334 max memory_allocated 22565.64501953125 
[2025-03-22 20:36:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:5.347611427307129 norm:0.18708519637584686 max memory_allocated 22565.64501953125 
[2025-03-22 20:36:57 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:5.339675426483154 norm:0.18286126852035522 max memory_allocated 22565.64501953125 
[2025-03-22 20:37:30 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:5.328300476074219 norm:0.17957395315170288 max memory_allocated 22565.64501953125 
[2025-03-22 20:38:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:5.31497049331665 norm:0.1722400188446045 max memory_allocated 22565.64501953125 
[2025-03-22 20:38:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:5.314632892608643 norm:0.17735189199447632 max memory_allocated 22565.64501953125 
[2025-03-22 20:39:08 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:5.302110195159912 norm:0.16463495790958405 max memory_allocated 22565.64501953125 
[2025-03-22 20:39:17 root] (main_calibration_a.py 369): INFO 21228.01754307747
[2025-03-22 20:39:21 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-22 20:40:30 root] (main_calibration_a.py 158): INFO wikitext2 : 8.32187557220459
[2025-03-22 20:40:30 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-22 20:42:15 root] (main_calibration_a.py 158): INFO c4 : 11.89075756072998
[2025-03-22 22:36:54 root] (main_calibration_a.py 169): INFO {'wikitext2': 8.32187557220459, 'c4': 11.89075756072998, 'results': {'arc_challenge': {'acc': 0.30716723549488056, 'acc_stderr': 0.013481034054980945, 'acc_norm': 0.3191126279863481, 'acc_norm_stderr': 0.013621696119173302}, 'hellaswag': {'acc': 0.4534953196574388, 'acc_stderr': 0.0049681518782110495, 'acc_norm': 0.6009759012148974, 'acc_norm_stderr': 0.00488696926694428}, 'arc_easy': {'acc': 0.5488215488215489, 'acc_stderr': 0.010210757101073479, 'acc_norm': 0.45454545454545453, 'acc_norm_stderr': 0.01021729976270943}, 'winogrande': {'acc': 0.5509076558800315, 'acc_stderr': 0.01397945938914084}, 'boolq': {'acc': 0.6296636085626911, 'acc_stderr': 0.008445882436783668}, 'piqa': {'acc': 0.6953210010881393, 'acc_stderr': 0.010738889044325161, 'acc_norm': 0.6920565832426551, 'acc_norm_stderr': 0.010770892367463676}}, 'versions': {'arc_challenge': 0, 'hellaswag': 0, 'arc_easy': 0, 'winogrande': 0, 'boolq': 1, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 22:36:54 root] (main_calibration_a.py 172): INFO 30.72,54.88,62.97,45.35,69.53,55.09
