[2025-03-22 14:42:02 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/llama-13b-hf-w4a4-4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=4)
[2025-03-22 14:48:36 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 14:48:36 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:48:37 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 14:48:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 14:48:43 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:49:30 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.11636515706777573 norm:0.10633543133735657 max memory_allocated 29268.67041015625 
[2025-03-22 14:50:18 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.06034751236438751 norm:0.04827672988176346 max memory_allocated 29268.67041015625 
[2025-03-22 14:51:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.046110183000564575 norm:0.03046967275440693 max memory_allocated 29268.67041015625 
[2025-03-22 14:51:53 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.041041504591703415 norm:0.025693770498037338 max memory_allocated 29268.67041015625 
[2025-03-22 14:52:41 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.03853278607130051 norm:0.02268405444920063 max memory_allocated 29268.67041015625 
[2025-03-22 14:53:28 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.03676864504814148 norm:0.02004329115152359 max memory_allocated 29268.67041015625 
[2025-03-22 14:54:16 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.03552662581205368 norm:0.017891209572553635 max memory_allocated 29268.67041015625 
[2025-03-22 14:55:03 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.034653495997190475 norm:0.01587124727666378 max memory_allocated 29268.67041015625 
[2025-03-22 14:55:51 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.03401470184326172 norm:0.014126218855381012 max memory_allocated 29268.67041015625 
[2025-03-22 14:56:38 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.03353886306285858 norm:0.012522184289991856 max memory_allocated 29268.67041015625 
[2025-03-22 14:57:26 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.03307509422302246 norm:0.011008528992533684 max memory_allocated 29268.67041015625 
[2025-03-22 14:58:14 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.03276972845196724 norm:0.010010451078414917 max memory_allocated 29268.67041015625 
[2025-03-22 14:59:01 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.032541919499635696 norm:0.009090542793273926 max memory_allocated 29268.67041015625 
[2025-03-22 14:59:49 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.032468825578689575 norm:0.008349691517651081 max memory_allocated 29268.67041015625 
[2025-03-22 15:00:36 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.03236584737896919 norm:0.007691383361816406 max memory_allocated 29268.67041015625 
[2025-03-22 15:01:24 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.03228750079870224 norm:0.007127079647034407 max memory_allocated 29268.67041015625 
[2025-03-22 15:02:11 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.03223048523068428 norm:0.006825902499258518 max memory_allocated 29268.67041015625 
[2025-03-22 15:02:59 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.03218089044094086 norm:0.00647569727152586 max memory_allocated 29268.67041015625 
[2025-03-22 15:03:47 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.03228084743022919 norm:0.0063064005225896835 max memory_allocated 29268.67041015625 
[2025-03-22 15:04:35 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.032124049961566925 norm:0.005887082312256098 max memory_allocated 29268.67041015625 
[2025-03-22 15:04:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 15:04:52 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:05:40 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.29816484451293945 norm:0.1480473279953003 max memory_allocated 29268.67041015625 
[2025-03-22 15:06:27 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.1827027052640915 norm:0.07594365626573563 max memory_allocated 29268.67041015625 
[2025-03-22 15:07:15 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.13006946444511414 norm:0.03505873307585716 max memory_allocated 29268.67041015625 
[2025-03-22 15:08:03 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.10975100845098495 norm:0.025665370747447014 max memory_allocated 29268.67041015625 
[2025-03-22 15:08:51 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.09971435368061066 norm:0.020913580432534218 max memory_allocated 29268.67041015625 
[2025-03-22 15:09:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.09406886994838715 norm:0.017310453578829765 max memory_allocated 29268.67041015625 
[2025-03-22 15:10:26 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.09027106314897537 norm:0.015003136359155178 max memory_allocated 29268.67041015625 
[2025-03-22 15:11:14 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.08749739825725555 norm:0.013098200783133507 max memory_allocated 29268.67041015625 
[2025-03-22 15:12:02 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.08564648032188416 norm:0.011440918780863285 max memory_allocated 29268.67041015625 
[2025-03-22 15:12:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.08423023670911789 norm:0.01011255569756031 max memory_allocated 29268.67041015625 
[2025-03-22 15:13:37 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.08308329433202744 norm:0.009001480415463448 max memory_allocated 29268.67041015625 
[2025-03-22 15:14:25 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.08237800002098083 norm:0.008087215945124626 max memory_allocated 29268.67041015625 
[2025-03-22 15:15:12 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.08168421685695648 norm:0.007406232412904501 max memory_allocated 29268.67041015625 
[2025-03-22 15:16:00 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.08120007067918777 norm:0.007073012180626392 max memory_allocated 29268.67041015625 
[2025-03-22 15:16:48 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.0807788074016571 norm:0.006936343386769295 max memory_allocated 29268.67041015625 
[2025-03-22 15:17:36 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.0805879607796669 norm:0.007013029418885708 max memory_allocated 29268.67041015625 
[2025-03-22 15:18:23 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.08031240105628967 norm:0.0066933343186974525 max memory_allocated 29268.67041015625 
[2025-03-22 15:19:11 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.08010931313037872 norm:0.006273183040320873 max memory_allocated 29268.67041015625 
[2025-03-22 15:19:59 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07969976216554642 norm:0.005942956544458866 max memory_allocated 29268.67041015625 
[2025-03-22 15:20:47 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07968585938215256 norm:0.005711527541279793 max memory_allocated 29268.67041015625 
[2025-03-22 15:21:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 15:21:04 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:21:52 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.31185320019721985 norm:0.05741926282644272 max memory_allocated 29268.67041015625 
[2025-03-22 15:22:40 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.24578186869621277 norm:0.04432257264852524 max memory_allocated 29268.67041015625 
[2025-03-22 15:23:28 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.21569286286830902 norm:0.03727695345878601 max memory_allocated 29268.67041015625 
[2025-03-22 15:24:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.19576041400432587 norm:0.03265774995088577 max memory_allocated 29268.67041015625 
[2025-03-22 15:25:04 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.18275195360183716 norm:0.0323844775557518 max memory_allocated 29268.67041015625 
[2025-03-22 15:25:52 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.17680419981479645 norm:0.03079361654818058 max memory_allocated 29268.67041015625 
[2025-03-22 15:26:39 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.1718151569366455 norm:0.027881179004907608 max memory_allocated 29268.67041015625 
[2025-03-22 15:27:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.1691906452178955 norm:0.02715184912085533 max memory_allocated 29268.67041015625 
[2025-03-22 15:28:15 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.16280895471572876 norm:0.025632133707404137 max memory_allocated 29268.67041015625 
[2025-03-22 15:29:03 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.15941113233566284 norm:0.024422328919172287 max memory_allocated 29268.67041015625 
[2025-03-22 15:29:51 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.15605708956718445 norm:0.025191962718963623 max memory_allocated 29268.67041015625 
[2025-03-22 15:30:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.15455113351345062 norm:0.023134218528866768 max memory_allocated 29268.67041015625 
[2025-03-22 15:31:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.15386629104614258 norm:0.02289522811770439 max memory_allocated 29268.67041015625 
[2025-03-22 15:32:14 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.15029999613761902 norm:0.02176724374294281 max memory_allocated 29268.67041015625 
[2025-03-22 15:33:02 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.1515873372554779 norm:0.022106550633907318 max memory_allocated 29268.67041015625 
[2025-03-22 15:33:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.14971551299095154 norm:0.022763468325138092 max memory_allocated 29268.67041015625 
[2025-03-22 15:34:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.14513428509235382 norm:0.019222119823098183 max memory_allocated 29268.67041015625 
[2025-03-22 15:35:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.14842119812965393 norm:0.02060362696647644 max memory_allocated 29268.67041015625 
[2025-03-22 15:36:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.14625221490859985 norm:0.01849329099059105 max memory_allocated 29268.67041015625 
[2025-03-22 15:37:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.14651748538017273 norm:0.018751204013824463 max memory_allocated 29268.67041015625 
[2025-03-22 15:37:15 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 15:38:06 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.254457950592041 norm:0.0318860299885273 max memory_allocated 29268.67041015625 
[2025-03-22 15:38:54 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.2268093079328537 norm:0.015276135876774788 max memory_allocated 29268.67041015625 
[2025-03-22 15:39:42 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.19996358454227448 norm:0.007518043275922537 max memory_allocated 29268.67041015625 
[2025-03-22 15:40:30 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.1886463463306427 norm:0.004777172580361366 max memory_allocated 29268.67041015625 
[2025-03-22 15:41:17 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.18365173041820526 norm:0.0037950780242681503 max memory_allocated 29268.67041015625 
[2025-03-22 15:42:05 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.18012690544128418 norm:0.003060729242861271 max memory_allocated 29268.67041015625 
[2025-03-22 15:42:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.17757050693035126 norm:0.0026562651619315147 max memory_allocated 29268.67041015625 
[2025-03-22 15:43:40 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.1756778508424759 norm:0.002392910420894623 max memory_allocated 29268.67041015625 
[2025-03-22 15:44:28 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.17499853670597076 norm:0.0022334149107337 max memory_allocated 29268.67041015625 
[2025-03-22 15:45:15 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.173912912607193 norm:0.00198085093870759 max memory_allocated 29268.67041015625 
[2025-03-22 15:46:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.1726764738559723 norm:0.0017951155314221978 max memory_allocated 29268.67041015625 
[2025-03-22 15:46:51 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.17243999242782593 norm:0.0017741795163601637 max memory_allocated 29268.67041015625 
[2025-03-22 15:47:38 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.17241019010543823 norm:0.001804420375265181 max memory_allocated 29268.67041015625 
[2025-03-22 15:48:26 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.17191873490810394 norm:0.001764186890795827 max memory_allocated 29268.67041015625 
[2025-03-22 15:49:13 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.17153126001358032 norm:0.0016296636313199997 max memory_allocated 29268.67041015625 
[2025-03-22 15:50:01 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.1714974343776703 norm:0.001574631780385971 max memory_allocated 29268.67041015625 
[2025-03-22 15:50:49 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.17118984460830688 norm:0.0015237157931551337 max memory_allocated 29268.67041015625 
[2025-03-22 15:51:37 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.1710636019706726 norm:0.001528957742266357 max memory_allocated 29268.67041015625 
[2025-03-22 15:52:25 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.171027272939682 norm:0.001512211631052196 max memory_allocated 29268.67041015625 
[2025-03-22 15:53:12 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.17084971070289612 norm:0.0014749455731362104 max memory_allocated 29268.67041015625 
[2025-03-22 15:53:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 15:54:17 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.3061022460460663 norm:0.04973051697015762 max memory_allocated 29268.84228515625 
[2025-03-22 15:55:05 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.26772189140319824 norm:0.014721892774105072 max memory_allocated 29268.84228515625 
[2025-03-22 15:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.23921331763267517 norm:0.006463998928666115 max memory_allocated 29268.84228515625 
[2025-03-22 15:56:41 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.23017610609531403 norm:0.004688159096986055 max memory_allocated 29268.84228515625 
[2025-03-22 15:57:29 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.22569791972637177 norm:0.003781579900532961 max memory_allocated 29268.84228515625 
[2025-03-22 15:58:16 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.2226254791021347 norm:0.0032931806053966284 max memory_allocated 29268.84228515625 
[2025-03-22 15:59:04 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.22043952345848083 norm:0.0028855353593826294 max memory_allocated 29268.84228515625 
[2025-03-22 15:59:52 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.21923409402370453 norm:0.0027177496813237667 max memory_allocated 29268.84228515625 
[2025-03-22 16:00:40 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.2179878056049347 norm:0.0024758693762123585 max memory_allocated 29268.84228515625 
[2025-03-22 16:01:28 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.2172674834728241 norm:0.002336849458515644 max memory_allocated 29268.84228515625 
[2025-03-22 16:02:15 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.21696236729621887 norm:0.002325352281332016 max memory_allocated 29268.84228515625 
[2025-03-22 16:03:03 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.2164703607559204 norm:0.002144906437024474 max memory_allocated 29268.84228515625 
[2025-03-22 16:03:51 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.21574757993221283 norm:0.0019038361497223377 max memory_allocated 29268.84228515625 
[2025-03-22 16:04:38 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.21573731303215027 norm:0.0019306848989799619 max memory_allocated 29268.84228515625 
[2025-03-22 16:05:26 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.21585281193256378 norm:0.00193190632853657 max memory_allocated 29268.84228515625 
[2025-03-22 16:06:14 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.2161143720149994 norm:0.0019832730758935213 max memory_allocated 29268.84228515625 
[2025-03-22 16:07:01 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.21632680296897888 norm:0.0020291144028306007 max memory_allocated 29268.84228515625 
[2025-03-22 16:07:49 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.21533749997615814 norm:0.0017514246283099055 max memory_allocated 29268.84228515625 
[2025-03-22 16:08:37 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.21488924324512482 norm:0.001650013029575348 max memory_allocated 29268.84228515625 
[2025-03-22 16:09:24 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.21476471424102783 norm:0.0016011868137866259 max memory_allocated 29268.84228515625 
[2025-03-22 16:09:38 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 16:10:29 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.36236801743507385 norm:0.0661560669541359 max memory_allocated 29269.02978515625 
[2025-03-22 16:11:17 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.3155677318572998 norm:0.022316452115774155 max memory_allocated 29269.02978515625 
[2025-03-22 16:12:05 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.27575787901878357 norm:0.007378516253083944 max memory_allocated 29269.02978515625 
[2025-03-22 16:12:53 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.26346561312675476 norm:0.004908136557787657 max memory_allocated 29269.02978515625 
[2025-03-22 16:13:41 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.2576783299446106 norm:0.00378694711253047 max memory_allocated 29269.02978515625 
[2025-03-22 16:14:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.2541160583496094 norm:0.0032185749150812626 max memory_allocated 29269.02978515625 
[2025-03-22 16:15:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.25153449177742004 norm:0.0028868631925433874 max memory_allocated 29269.02978515625 
[2025-03-22 16:16:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.24955737590789795 norm:0.002551875775679946 max memory_allocated 29269.02978515625 
[2025-03-22 16:16:52 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.2483387440443039 norm:0.0024053044617176056 max memory_allocated 29269.02978515625 
[2025-03-22 16:17:40 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.24748770892620087 norm:0.0023381297942250967 max memory_allocated 29269.02978515625 
[2025-03-22 16:18:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.24689415097236633 norm:0.0022841503378003836 max memory_allocated 29269.02978515625 
[2025-03-22 16:19:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.24650715291500092 norm:0.0021380309481173754 max memory_allocated 29269.02978515625 
[2025-03-22 16:20:03 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.24626095592975616 norm:0.0021079867146909237 max memory_allocated 29269.02978515625 
[2025-03-22 16:20:51 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.24566411972045898 norm:0.001987162744626403 max memory_allocated 29269.02978515625 
[2025-03-22 16:21:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.24558115005493164 norm:0.001970203360542655 max memory_allocated 29269.02978515625 
[2025-03-22 16:22:27 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.2453385889530182 norm:0.0019127636915072799 max memory_allocated 29269.02978515625 
[2025-03-22 16:23:15 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.2452956885099411 norm:0.001889678300358355 max memory_allocated 29269.02978515625 
[2025-03-22 16:24:03 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.24511849880218506 norm:0.0018646008102223277 max memory_allocated 29269.02978515625 
[2025-03-22 16:24:51 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.24456241726875305 norm:0.0016978367930278182 max memory_allocated 29269.02978515625 
[2025-03-22 16:25:38 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.24420690536499023 norm:0.0015985684003680944 max memory_allocated 29269.02978515625 
[2025-03-22 16:25:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 16:26:43 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.4287141263484955 norm:0.015382172539830208 max memory_allocated 29269.21728515625 
[2025-03-22 16:27:31 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.39572909474372864 norm:0.008460940793156624 max memory_allocated 29269.21728515625 
[2025-03-22 16:28:18 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.36282628774642944 norm:0.005235828924924135 max memory_allocated 29269.21728515625 
[2025-03-22 16:29:06 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.3512713611125946 norm:0.0037116599269211292 max memory_allocated 29269.21728515625 
[2025-03-22 16:29:53 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.3443455696105957 norm:0.0034926363732665777 max memory_allocated 29269.21728515625 
[2025-03-22 16:30:41 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.33775782585144043 norm:0.003194312332198024 max memory_allocated 29269.21728515625 
[2025-03-22 16:31:29 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.3345808982849121 norm:0.003122166497632861 max memory_allocated 29269.21728515625 
[2025-03-22 16:32:16 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.3315238058567047 norm:0.0030671078711748123 max memory_allocated 29269.21728515625 
[2025-03-22 16:33:04 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.33044135570526123 norm:0.002778031164780259 max memory_allocated 29269.21728515625 
[2025-03-22 16:33:52 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.3329853415489197 norm:0.004666181746870279 max memory_allocated 29269.21728515625 
[2025-03-22 16:34:40 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.32609981298446655 norm:0.002863815752789378 max memory_allocated 29269.21728515625 
[2025-03-22 16:35:28 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.3256291151046753 norm:0.0029109313618391752 max memory_allocated 29269.21728515625 
[2025-03-22 16:36:16 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.32496723532676697 norm:0.003065842669457197 max memory_allocated 29269.21728515625 
[2025-03-22 16:37:03 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.3245452046394348 norm:0.0030965397600084543 max memory_allocated 29269.21728515625 
[2025-03-22 16:37:51 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.3237236440181732 norm:0.003304828191176057 max memory_allocated 29269.21728515625 
[2025-03-22 16:38:39 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.3238905966281891 norm:0.003201871644705534 max memory_allocated 29269.21728515625 
[2025-03-22 16:39:27 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.32347607612609863 norm:0.0032646534964442253 max memory_allocated 29269.21728515625 
[2025-03-22 16:40:15 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.3233472406864166 norm:0.003360837697982788 max memory_allocated 29269.21728515625 
[2025-03-22 16:41:03 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.3231884241104126 norm:0.0033790559973567724 max memory_allocated 29269.21728515625 
[2025-03-22 16:41:50 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.32328152656555176 norm:0.0033623320050537586 max memory_allocated 29269.21728515625 
[2025-03-22 16:42:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 16:42:56 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.45471665263175964 norm:0.021657755598425865 max memory_allocated 29269.40478515625 
[2025-03-22 16:43:44 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.4204624891281128 norm:0.010776016861200333 max memory_allocated 29269.40478515625 
[2025-03-22 16:44:32 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.37928083539009094 norm:0.004394809249788523 max memory_allocated 29269.40478515625 
[2025-03-22 16:45:19 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.36254268884658813 norm:0.002442043973132968 max memory_allocated 29269.40478515625 
[2025-03-22 16:46:07 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.3555416762828827 norm:0.0017954722279682755 max memory_allocated 29269.40478515625 
[2025-03-22 16:46:55 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.35122615098953247 norm:0.0014925174182280898 max memory_allocated 29269.40478515625 
[2025-03-22 16:47:42 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.3478231430053711 norm:0.001281869481317699 max memory_allocated 29269.40478515625 
[2025-03-22 16:48:30 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.3455272316932678 norm:0.0011751485290005803 max memory_allocated 29269.40478515625 
[2025-03-22 16:49:18 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.34374305605888367 norm:0.0011035094503313303 max memory_allocated 29269.40478515625 
[2025-03-22 16:50:05 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.3425619900226593 norm:0.0010775293922051787 max memory_allocated 29269.40478515625 
[2025-03-22 16:50:53 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.34168267250061035 norm:0.0010614355560392141 max memory_allocated 29269.40478515625 
[2025-03-22 16:51:41 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.3411475121974945 norm:0.001079924521036446 max memory_allocated 29269.40478515625 
[2025-03-22 16:52:28 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.3405170440673828 norm:0.0010559854563325644 max memory_allocated 29269.40478515625 
[2025-03-22 16:53:16 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.3400880694389343 norm:0.0010596802458167076 max memory_allocated 29269.40478515625 
[2025-03-22 16:54:04 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.3396618962287903 norm:0.0010275674285367131 max memory_allocated 29269.40478515625 
[2025-03-22 16:54:52 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.339460551738739 norm:0.0010202781995758414 max memory_allocated 29269.40478515625 
[2025-03-22 16:55:39 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.33927595615386963 norm:0.0010061305947601795 max memory_allocated 29269.40478515625 
[2025-03-22 16:56:27 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.3391096591949463 norm:0.0010052149882540107 max memory_allocated 29269.40478515625 
[2025-03-22 16:57:15 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.33882951736450195 norm:0.0009807416936382651 max memory_allocated 29269.40478515625 
[2025-03-22 16:58:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.33860448002815247 norm:0.0009762542322278023 max memory_allocated 29269.40478515625 
[2025-03-22 16:58:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 16:59:08 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.4756174087524414 norm:0.01981346495449543 max memory_allocated 29269.59228515625 
[2025-03-22 16:59:56 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.44371283054351807 norm:0.011055486276745796 max memory_allocated 29269.59228515625 
[2025-03-22 17:00:44 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.400391548871994 norm:0.004520931746810675 max memory_allocated 29269.59228515625 
[2025-03-22 17:01:31 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.38180670142173767 norm:0.0023113382048904896 max memory_allocated 29269.59228515625 
[2025-03-22 17:02:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.37542685866355896 norm:0.0018247513798996806 max memory_allocated 29269.59228515625 
[2025-03-22 17:03:07 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.37131622433662415 norm:0.0014704957138746977 max memory_allocated 29269.59228515625 
[2025-03-22 17:03:55 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.36805543303489685 norm:0.0012098978040739894 max memory_allocated 29269.59228515625 
[2025-03-22 17:04:43 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.3658931255340576 norm:0.001112663303501904 max memory_allocated 29269.59228515625 
[2025-03-22 17:05:31 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.36438560485839844 norm:0.0010456567397341132 max memory_allocated 29269.59228515625 
[2025-03-22 17:06:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.36313095688819885 norm:0.001044290023855865 max memory_allocated 29269.59228515625 
[2025-03-22 17:07:07 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.36231136322021484 norm:0.0010086044203490019 max memory_allocated 29269.59228515625 
[2025-03-22 17:07:55 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.3616672456264496 norm:0.000980120967142284 max memory_allocated 29269.59228515625 
[2025-03-22 17:08:42 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.3611550033092499 norm:0.000943587685469538 max memory_allocated 29269.59228515625 
[2025-03-22 17:09:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.36076807975769043 norm:0.0009479247964918613 max memory_allocated 29269.59228515625 
[2025-03-22 17:10:18 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.3605547547340393 norm:0.0009474381804466248 max memory_allocated 29269.59228515625 
[2025-03-22 17:11:05 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.3602527678012848 norm:0.0009427422191947699 max memory_allocated 29269.59228515625 
[2025-03-22 17:11:53 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.3600339889526367 norm:0.0009355823858641088 max memory_allocated 29269.59228515625 
[2025-03-22 17:12:40 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.3598995804786682 norm:0.000938973855227232 max memory_allocated 29269.59228515625 
[2025-03-22 17:13:28 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.35952258110046387 norm:0.0009276324417442083 max memory_allocated 29269.59228515625 
[2025-03-22 17:14:16 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.35939985513687134 norm:0.0009135465952567756 max memory_allocated 29269.59228515625 
[2025-03-22 17:14:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 17:15:21 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.5072306394577026 norm:0.020494753494858742 max memory_allocated 29269.77978515625 
[2025-03-22 17:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.46561431884765625 norm:0.010476971045136452 max memory_allocated 29269.77978515625 
[2025-03-22 17:16:56 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.4159907102584839 norm:0.003756715217605233 max memory_allocated 29269.77978515625 
[2025-03-22 17:17:44 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.39950576424598694 norm:0.0019301573047414422 max memory_allocated 29269.77978515625 
[2025-03-22 17:18:32 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.3936902582645416 norm:0.0014666980132460594 max memory_allocated 29269.77978515625 
[2025-03-22 17:19:20 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3898589611053467 norm:0.0012227499391883612 max memory_allocated 29269.77978515625 
[2025-03-22 17:20:07 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.38712000846862793 norm:0.0010948129929602146 max memory_allocated 29269.77978515625 
[2025-03-22 17:20:55 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.38515281677246094 norm:0.0010327065829187632 max memory_allocated 29269.77978515625 
[2025-03-22 17:21:43 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.3836548924446106 norm:0.0009954633424058557 max memory_allocated 29269.77978515625 
[2025-03-22 17:22:31 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3825162351131439 norm:0.0009613852016627789 max memory_allocated 29269.77978515625 
[2025-03-22 17:23:19 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.38173699378967285 norm:0.000943825813010335 max memory_allocated 29269.77978515625 
[2025-03-22 17:24:07 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.38113933801651 norm:0.0009258906356990337 max memory_allocated 29269.77978515625 
[2025-03-22 17:24:54 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.38072413206100464 norm:0.0009150468395091593 max memory_allocated 29269.77978515625 
[2025-03-22 17:25:42 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.3803904950618744 norm:0.0009033884853124619 max memory_allocated 29269.77978515625 
[2025-03-22 17:26:30 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.38006582856178284 norm:0.0008894655620679259 max memory_allocated 29269.77978515625 
[2025-03-22 17:27:17 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.37967172265052795 norm:0.000872353557497263 max memory_allocated 29269.77978515625 
[2025-03-22 17:28:05 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.37950199842453003 norm:0.0008719550096429884 max memory_allocated 29269.77978515625 
[2025-03-22 17:28:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.3793644607067108 norm:0.0008708753157407045 max memory_allocated 29269.77978515625 
[2025-03-22 17:29:40 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.3792674243450165 norm:0.0008813866879791021 max memory_allocated 29269.77978515625 
[2025-03-22 17:30:28 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.3790971040725708 norm:0.0008705774671398103 max memory_allocated 29269.77978515625 
[2025-03-22 17:30:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 17:31:33 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.5081316232681274 norm:0.029681896790862083 max memory_allocated 29269.96728515625 
[2025-03-22 17:32:21 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.4777286648750305 norm:0.013358601368963718 max memory_allocated 29269.96728515625 
[2025-03-22 17:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.43892112374305725 norm:0.004895596764981747 max memory_allocated 29269.96728515625 
[2025-03-22 17:33:56 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.42090415954589844 norm:0.0022300162818282843 max memory_allocated 29269.96728515625 
[2025-03-22 17:34:44 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.41426214575767517 norm:0.0015844289446249604 max memory_allocated 29269.96728515625 
[2025-03-22 17:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.4104923903942108 norm:0.0013534987810999155 max memory_allocated 29269.96728515625 
[2025-03-22 17:36:19 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.4077581763267517 norm:0.0012388189788907766 max memory_allocated 29269.96728515625 
[2025-03-22 17:37:07 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.4057663381099701 norm:0.0011465174611657858 max memory_allocated 29269.96728515625 
[2025-03-22 17:37:55 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.40416207909584045 norm:0.001058145659044385 max memory_allocated 29269.96728515625 
[2025-03-22 17:38:43 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.4030284881591797 norm:0.0010013695573434234 max memory_allocated 29269.96728515625 
[2025-03-22 17:39:31 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.4021049439907074 norm:0.0009638812043704093 max memory_allocated 29269.96728515625 
[2025-03-22 17:40:19 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.40157467126846313 norm:0.000956874166149646 max memory_allocated 29269.96728515625 
[2025-03-22 17:41:07 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.40117764472961426 norm:0.0009349051979370415 max memory_allocated 29269.96728515625 
[2025-03-22 17:41:55 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.40062496066093445 norm:0.0009093224653042853 max memory_allocated 29269.96728515625 
[2025-03-22 17:42:43 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.4002329409122467 norm:0.0008825070690363646 max memory_allocated 29269.96728515625 
[2025-03-22 17:43:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.39999955892562866 norm:0.0008802809752523899 max memory_allocated 29269.96728515625 
[2025-03-22 17:44:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.39976346492767334 norm:0.0008648991934023798 max memory_allocated 29269.96728515625 
[2025-03-22 17:45:06 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3994971215724945 norm:0.0008434437913820148 max memory_allocated 29269.96728515625 
[2025-03-22 17:45:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.3993365466594696 norm:0.0008200575248338282 max memory_allocated 29269.96728515625 
[2025-03-22 17:46:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.39897069334983826 norm:0.0008076103404164314 max memory_allocated 29269.96728515625 
[2025-03-22 17:46:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 17:47:46 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.5410728454589844 norm:0.022399963811039925 max memory_allocated 29270.15478515625 
[2025-03-22 17:48:33 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.5070303678512573 norm:0.010475421324372292 max memory_allocated 29270.15478515625 
[2025-03-22 17:49:21 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.4616488516330719 norm:0.004196521360427141 max memory_allocated 29270.15478515625 
[2025-03-22 17:50:09 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.44268670678138733 norm:0.0019211454782634974 max memory_allocated 29270.15478515625 
[2025-03-22 17:50:56 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.43654701113700867 norm:0.0013846149668097496 max memory_allocated 29270.15478515625 
[2025-03-22 17:51:44 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.4327148199081421 norm:0.00116306624840945 max memory_allocated 29270.15478515625 
[2025-03-22 17:52:32 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.4298612177371979 norm:0.0010379814775660634 max memory_allocated 29270.15478515625 
[2025-03-22 17:53:20 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.42787492275238037 norm:0.0009751382167451084 max memory_allocated 29270.15478515625 
[2025-03-22 17:54:08 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.4264546036720276 norm:0.0009418762638233602 max memory_allocated 29270.15478515625 
[2025-03-22 17:54:56 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.42531290650367737 norm:0.0009063853067345917 max memory_allocated 29270.15478515625 
[2025-03-22 17:55:44 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.424341082572937 norm:0.0008705795626156032 max memory_allocated 29270.15478515625 
[2025-03-22 17:56:31 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.42367929220199585 norm:0.0008540687267668545 max memory_allocated 29270.15478515625 
[2025-03-22 17:57:19 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.42318493127822876 norm:0.0008415004122070968 max memory_allocated 29270.15478515625 
[2025-03-22 17:58:07 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.4228594899177551 norm:0.000839770829770714 max memory_allocated 29270.15478515625 
[2025-03-22 17:58:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.4225025177001953 norm:0.0008334033191204071 max memory_allocated 29270.15478515625 
[2025-03-22 17:59:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.4222753942012787 norm:0.0008237209403887391 max memory_allocated 29270.15478515625 
[2025-03-22 18:00:31 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.42214205861091614 norm:0.0008175023249350488 max memory_allocated 29270.15478515625 
[2025-03-22 18:01:19 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.42207449674606323 norm:0.0008280084584839642 max memory_allocated 29270.15478515625 
[2025-03-22 18:02:06 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.42191848158836365 norm:0.00082432507770136 max memory_allocated 29270.15478515625 
[2025-03-22 18:02:54 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.4218656122684479 norm:0.0008282477501779795 max memory_allocated 29270.15478515625 
[2025-03-22 18:03:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 18:04:00 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.552434504032135 norm:0.024214759469032288 max memory_allocated 29270.34228515625 
[2025-03-22 18:04:47 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.5196921229362488 norm:0.009914989583194256 max memory_allocated 29270.34228515625 
[2025-03-22 18:05:35 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.48324325680732727 norm:0.004133209586143494 max memory_allocated 29270.34228515625 
[2025-03-22 18:06:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.46624481678009033 norm:0.0019744322635233402 max memory_allocated 29270.34228515625 
[2025-03-22 18:07:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.45982271432876587 norm:0.0014127056347206235 max memory_allocated 29270.34228515625 
[2025-03-22 18:07:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.45565998554229736 norm:0.0012152588460594416 max memory_allocated 29270.34228515625 
[2025-03-22 18:08:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.4526607394218445 norm:0.0010886163217946887 max memory_allocated 29270.34228515625 
[2025-03-22 18:09:33 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.450443297624588 norm:0.0010243862634524703 max memory_allocated 29270.34228515625 
[2025-03-22 18:10:21 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.448853462934494 norm:0.0009980564936995506 max memory_allocated 29270.34228515625 
[2025-03-22 18:11:09 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.44743603467941284 norm:0.0009635366732254624 max memory_allocated 29270.34228515625 
[2025-03-22 18:11:57 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.44644954800605774 norm:0.0009543451014906168 max memory_allocated 29270.34228515625 
[2025-03-22 18:12:45 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.44554856419563293 norm:0.0009232540614902973 max memory_allocated 29270.34228515625 
[2025-03-22 18:13:33 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.4448990225791931 norm:0.0009195942548103631 max memory_allocated 29270.34228515625 
[2025-03-22 18:14:21 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.4443584978580475 norm:0.000907398178242147 max memory_allocated 29270.34228515625 
[2025-03-22 18:15:09 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.44404688477516174 norm:0.0008996528340503573 max memory_allocated 29270.34228515625 
[2025-03-22 18:15:57 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.44377052783966064 norm:0.0008910938631743193 max memory_allocated 29270.34228515625 
[2025-03-22 18:16:45 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.4435020685195923 norm:0.0008844395633786917 max memory_allocated 29270.34228515625 
[2025-03-22 18:17:32 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.44313040375709534 norm:0.0008788554696366191 max memory_allocated 29270.34228515625 
[2025-03-22 18:18:20 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.4429318308830261 norm:0.0008885383140295744 max memory_allocated 29270.34228515625 
[2025-03-22 18:19:08 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.44270628690719604 norm:0.0008737497264519334 max memory_allocated 29270.34228515625 
[2025-03-22 18:19:21 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 18:20:12 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.5545221567153931 norm:0.009192053228616714 max memory_allocated 29270.52978515625 
[2025-03-22 18:21:00 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.5318492650985718 norm:0.005893024615943432 max memory_allocated 29270.52978515625 
[2025-03-22 18:21:48 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.5023465752601624 norm:0.003111108671873808 max memory_allocated 29270.52978515625 
[2025-03-22 18:22:35 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.4874093234539032 norm:0.0017081063706427813 max memory_allocated 29270.52978515625 
[2025-03-22 18:23:23 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.4811360538005829 norm:0.0013281445717439055 max memory_allocated 29270.52978515625 
[2025-03-22 18:24:10 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.47735899686813354 norm:0.0011842924868687987 max memory_allocated 29270.52978515625 
[2025-03-22 18:24:58 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.4746285676956177 norm:0.0011068119201809168 max memory_allocated 29270.52978515625 
[2025-03-22 18:25:46 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.47258004546165466 norm:0.001065006828866899 max memory_allocated 29270.52978515625 
[2025-03-22 18:26:34 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.4708007872104645 norm:0.0010186954168602824 max memory_allocated 29270.52978515625 
[2025-03-22 18:27:22 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.4694163203239441 norm:0.0009956029243767262 max memory_allocated 29270.52978515625 
[2025-03-22 18:28:09 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.46838170289993286 norm:0.0009650065330788493 max memory_allocated 29270.52978515625 
[2025-03-22 18:28:57 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.4676450490951538 norm:0.000956340110860765 max memory_allocated 29270.52978515625 
[2025-03-22 18:29:45 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.467056006193161 norm:0.0009466626215726137 max memory_allocated 29270.52978515625 
[2025-03-22 18:30:33 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.4666610360145569 norm:0.0009478252031840384 max memory_allocated 29270.52978515625 
[2025-03-22 18:31:21 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.4662652611732483 norm:0.0009491164819337428 max memory_allocated 29270.52978515625 
[2025-03-22 18:32:09 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.46578964591026306 norm:0.0009382364223711193 max memory_allocated 29270.52978515625 
[2025-03-22 18:32:57 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.4655436873435974 norm:0.0009421826689504087 max memory_allocated 29270.52978515625 
[2025-03-22 18:33:45 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.4651983976364136 norm:0.0009334144415333867 max memory_allocated 29270.52978515625 
[2025-03-22 18:34:32 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.46501681208610535 norm:0.0009371612104587257 max memory_allocated 29270.52978515625 
[2025-03-22 18:35:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.4648400545120239 norm:0.000943458522669971 max memory_allocated 29270.52978515625 
[2025-03-22 18:35:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 18:36:25 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.610538899898529 norm:0.03663749620318413 max memory_allocated 29270.71728515625 
[2025-03-22 18:37:12 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.5776829719543457 norm:0.01889065094292164 max memory_allocated 29270.71728515625 
[2025-03-22 18:38:00 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.5392072796821594 norm:0.00962231308221817 max memory_allocated 29270.71728515625 
[2025-03-22 18:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.5192283391952515 norm:0.005463897716253996 max memory_allocated 29270.71728515625 
[2025-03-22 18:39:35 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.5094431638717651 norm:0.0033676177263259888 max memory_allocated 29270.71728515625 
[2025-03-22 18:40:23 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.5040228366851807 norm:0.002072440693154931 max memory_allocated 29270.71728515625 
[2025-03-22 18:41:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.5004096031188965 norm:0.0017001660307869315 max memory_allocated 29270.71728515625 
[2025-03-22 18:41:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.49772942066192627 norm:0.0015070299850776792 max memory_allocated 29270.71728515625 
[2025-03-22 18:42:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.4953162968158722 norm:0.0012217109324410558 max memory_allocated 29270.71728515625 
[2025-03-22 18:43:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.4935917854309082 norm:0.0011544690933078527 max memory_allocated 29270.71728515625 
[2025-03-22 18:44:21 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.4924420714378357 norm:0.0011254813289269805 max memory_allocated 29270.71728515625 
[2025-03-22 18:45:09 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.4914512038230896 norm:0.0010846536606550217 max memory_allocated 29270.71728515625 
[2025-03-22 18:45:57 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.49058133363723755 norm:0.0010580223752185702 max memory_allocated 29270.71728515625 
[2025-03-22 18:46:45 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.4899015724658966 norm:0.0010392111726105213 max memory_allocated 29270.71728515625 
[2025-03-22 18:47:33 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.4893454313278198 norm:0.00100985961034894 max memory_allocated 29270.71728515625 
[2025-03-22 18:48:21 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.4889306426048279 norm:0.0010159950470551848 max memory_allocated 29270.71728515625 
[2025-03-22 18:49:09 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.4886542856693268 norm:0.001001225784420967 max memory_allocated 29270.71728515625 
[2025-03-22 18:49:57 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.48837554454803467 norm:0.000987697159871459 max memory_allocated 29270.71728515625 
[2025-03-22 18:50:44 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.488140732049942 norm:0.000985747086815536 max memory_allocated 29270.71728515625 
[2025-03-22 18:51:32 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.4880310893058777 norm:0.0009852349758148193 max memory_allocated 29270.71728515625 
[2025-03-22 18:51:46 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 18:52:38 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.6347854137420654 norm:0.07956381142139435 max memory_allocated 29270.90478515625 
[2025-03-22 18:53:25 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.613934338092804 norm:0.04495665431022644 max memory_allocated 29270.90478515625 
[2025-03-22 18:54:13 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.5816251039505005 norm:0.02129380591213703 max memory_allocated 29270.90478515625 
[2025-03-22 18:55:01 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.557607114315033 norm:0.007830310612916946 max memory_allocated 29270.90478515625 
[2025-03-22 18:55:49 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.548454999923706 norm:0.005208455957472324 max memory_allocated 29270.90478515625 
[2025-03-22 18:56:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.5432298183441162 norm:0.0037314752116799355 max memory_allocated 29270.90478515625 
[2025-03-22 18:57:24 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.5391996502876282 norm:0.0029499386437237263 max memory_allocated 29270.90478515625 
[2025-03-22 18:58:11 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.5364841222763062 norm:0.0027177261654287577 max memory_allocated 29270.90478515625 
[2025-03-22 18:58:59 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.5343844294548035 norm:0.0025760598946362734 max memory_allocated 29270.90478515625 
[2025-03-22 18:59:47 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.5325795412063599 norm:0.0024330085143446922 max memory_allocated 29270.90478515625 
[2025-03-22 19:00:34 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.5305156111717224 norm:0.0017544585280120373 max memory_allocated 29270.90478515625 
[2025-03-22 19:01:22 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.5289847254753113 norm:0.0014767084503546357 max memory_allocated 29270.90478515625 
[2025-03-22 19:02:10 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.5282635688781738 norm:0.0014877587091177702 max memory_allocated 29270.90478515625 
[2025-03-22 19:02:57 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.5274404287338257 norm:0.001430046046152711 max memory_allocated 29270.90478515625 
[2025-03-22 19:03:45 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.5267738699913025 norm:0.0013862686464563012 max memory_allocated 29270.90478515625 
[2025-03-22 19:04:33 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.5261961221694946 norm:0.0013462253846228123 max memory_allocated 29270.90478515625 
[2025-03-22 19:05:21 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.5257132649421692 norm:0.001324872369877994 max memory_allocated 29270.90478515625 
[2025-03-22 19:06:09 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.5252393484115601 norm:0.001283322460949421 max memory_allocated 29270.90478515625 
[2025-03-22 19:06:56 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.525026798248291 norm:0.0012687583221122622 max memory_allocated 29270.90478515625 
[2025-03-22 19:07:44 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.5247945785522461 norm:0.001239720731973648 max memory_allocated 29270.90478515625 
[2025-03-22 19:07:58 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 19:08:49 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.6434342861175537 norm:0.019877174869179726 max memory_allocated 29271.09228515625 
[2025-03-22 19:09:37 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.6198451519012451 norm:0.011462240479886532 max memory_allocated 29271.09228515625 
[2025-03-22 19:10:25 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.5904362797737122 norm:0.005218320060521364 max memory_allocated 29271.09228515625 
[2025-03-22 19:11:13 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.5771045088768005 norm:0.0026761717163026333 max memory_allocated 29271.09228515625 
[2025-03-22 19:12:01 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.5715205669403076 norm:0.001847876701503992 max memory_allocated 29271.09228515625 
[2025-03-22 19:12:49 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.5683284401893616 norm:0.001555088791064918 max memory_allocated 29271.09228515625 
[2025-03-22 19:13:37 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.5658718347549438 norm:0.0014128582552075386 max memory_allocated 29271.09228515625 
[2025-03-22 19:14:24 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.5639451146125793 norm:0.001311927568167448 max memory_allocated 29271.09228515625 
[2025-03-22 19:15:12 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.5624589323997498 norm:0.0012427782639861107 max memory_allocated 29271.09228515625 
[2025-03-22 19:16:00 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.5611558556556702 norm:0.0011708269594237208 max memory_allocated 29271.09228515625 
[2025-03-22 19:16:47 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.5601434707641602 norm:0.0011236892314627767 max memory_allocated 29271.09228515625 
[2025-03-22 19:17:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.5592124462127686 norm:0.001077817170880735 max memory_allocated 29271.09228515625 
[2025-03-22 19:18:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.558462917804718 norm:0.0010450202971696854 max memory_allocated 29271.09228515625 
[2025-03-22 19:19:10 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.5578716397285461 norm:0.001005343277938664 max memory_allocated 29271.09228515625 
[2025-03-22 19:19:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.5574790835380554 norm:0.0009918740252032876 max memory_allocated 29271.09228515625 
[2025-03-22 19:20:46 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.5571580529212952 norm:0.0009870147332549095 max memory_allocated 29271.09228515625 
[2025-03-22 19:21:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.5568832159042358 norm:0.0009846922475844622 max memory_allocated 29271.09228515625 
[2025-03-22 19:22:21 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.5565593838691711 norm:0.0009684108081273735 max memory_allocated 29271.09228515625 
[2025-03-22 19:23:09 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.5562450885772705 norm:0.0009617803152650595 max memory_allocated 29271.09228515625 
[2025-03-22 19:23:57 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.5560932755470276 norm:0.0009675431647337973 max memory_allocated 29271.09228515625 
[2025-03-22 19:24:10 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 19:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.6854016780853271 norm:0.01501680538058281 max memory_allocated 29271.27978515625 
[2025-03-22 19:25:50 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.6631662845611572 norm:0.008745920844376087 max memory_allocated 29271.27978515625 
[2025-03-22 19:26:37 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.6338967680931091 norm:0.004605894908308983 max memory_allocated 29271.27978515625 
[2025-03-22 19:27:25 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.6205004453659058 norm:0.002699362114071846 max memory_allocated 29271.27978515625 
[2025-03-22 19:28:13 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.6152125000953674 norm:0.002034857403486967 max memory_allocated 29271.27978515625 
[2025-03-22 19:29:01 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.6117266416549683 norm:0.0016755465185269713 max memory_allocated 29271.27978515625 
[2025-03-22 19:29:49 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.608906626701355 norm:0.0014471516478806734 max memory_allocated 29271.27978515625 
[2025-03-22 19:30:37 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.6068639755249023 norm:0.0013246650341898203 max memory_allocated 29271.27978515625 
[2025-03-22 19:31:25 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.6051948666572571 norm:0.0012632719008252025 max memory_allocated 29271.27978515625 
[2025-03-22 19:32:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.6039016246795654 norm:0.0012198961339890957 max memory_allocated 29271.27978515625 
[2025-03-22 19:33:00 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.6027374863624573 norm:0.0011692497646436095 max memory_allocated 29271.27978515625 
[2025-03-22 19:33:48 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.6018896102905273 norm:0.0011234191479161382 max memory_allocated 29271.27978515625 
[2025-03-22 19:34:36 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.6012201905250549 norm:0.0010947955306619406 max memory_allocated 29271.27978515625 
[2025-03-22 19:35:23 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.6004171967506409 norm:0.0010331026278436184 max memory_allocated 29271.27978515625 
[2025-03-22 19:36:11 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.5999329090118408 norm:0.001009757979772985 max memory_allocated 29271.27978515625 
[2025-03-22 19:36:59 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.5995959043502808 norm:0.0010017757304012775 max memory_allocated 29271.27978515625 
[2025-03-22 19:37:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.5992095470428467 norm:0.0009861512808129191 max memory_allocated 29271.27978515625 
[2025-03-22 19:38:34 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.598881185054779 norm:0.0009854184463620186 max memory_allocated 29271.27978515625 
[2025-03-22 19:39:22 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.5986884832382202 norm:0.0009816548554226756 max memory_allocated 29271.27978515625 
[2025-03-22 19:40:09 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.5984455347061157 norm:0.0009773551719263196 max memory_allocated 29271.27978515625 
[2025-03-22 19:40:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 19:41:14 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.7771124839782715 norm:0.06411197781562805 max memory_allocated 29271.46728515625 
[2025-03-22 19:42:02 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.7474237084388733 norm:0.03563646972179413 max memory_allocated 29271.46728515625 
[2025-03-22 19:42:50 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.709492564201355 norm:0.0168601106852293 max memory_allocated 29271.46728515625 
[2025-03-22 19:43:38 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.6897815465927124 norm:0.008859907276928425 max memory_allocated 29271.46728515625 
[2025-03-22 19:44:25 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.6822242736816406 norm:0.005962656810879707 max memory_allocated 29271.46728515625 
[2025-03-22 19:45:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.6776926517486572 norm:0.004526637960225344 max memory_allocated 29271.46728515625 
[2025-03-22 19:46:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.6746107339859009 norm:0.003990492317825556 max memory_allocated 29271.46728515625 
[2025-03-22 19:46:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.6716688275337219 norm:0.0031596478074789047 max memory_allocated 29271.46728515625 
[2025-03-22 19:47:37 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.6692798137664795 norm:0.0025985122192651033 max memory_allocated 29271.46728515625 
[2025-03-22 19:48:25 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.6678225994110107 norm:0.002566362265497446 max memory_allocated 29271.46728515625 
[2025-03-22 19:49:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.6667925715446472 norm:0.0024977957364171743 max memory_allocated 29271.46728515625 
[2025-03-22 19:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.6656871438026428 norm:0.002325617941096425 max memory_allocated 29271.46728515625 
[2025-03-22 19:50:48 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.664804220199585 norm:0.002194775966927409 max memory_allocated 29271.46728515625 
[2025-03-22 19:51:36 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.6641847491264343 norm:0.002114835660904646 max memory_allocated 29271.46728515625 
[2025-03-22 19:52:23 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.6634700298309326 norm:0.0020079282112419605 max memory_allocated 29271.46728515625 
[2025-03-22 19:53:11 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.6627473831176758 norm:0.0018860402051359415 max memory_allocated 29271.46728515625 
[2025-03-22 19:53:58 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.6622834205627441 norm:0.0019056092714890838 max memory_allocated 29271.46728515625 
[2025-03-22 19:54:46 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.6610620021820068 norm:0.0012924704933539033 max memory_allocated 29271.46728515625 
[2025-03-22 19:55:34 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.6604490876197815 norm:0.0012369905598461628 max memory_allocated 29271.46728515625 
[2025-03-22 19:56:21 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.6600370407104492 norm:0.0012225651880726218 max memory_allocated 29271.46728515625 
[2025-03-22 19:56:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 19:57:26 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.8102222084999084 norm:0.020383721217513084 max memory_allocated 29271.65478515625 
[2025-03-22 19:58:14 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.7896878719329834 norm:0.011191780678927898 max memory_allocated 29271.65478515625 
[2025-03-22 19:59:02 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.7659935355186462 norm:0.006134584546089172 max memory_allocated 29271.65478515625 
[2025-03-22 19:59:50 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.7532551884651184 norm:0.0034481999464333057 max memory_allocated 29271.65478515625 
[2025-03-22 20:00:38 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.7478506565093994 norm:0.0027368308510631323 max memory_allocated 29271.65478515625 
[2025-03-22 20:01:26 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.7445873022079468 norm:0.0024348204024136066 max memory_allocated 29271.65478515625 
[2025-03-22 20:02:14 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.7415453791618347 norm:0.0015631156275048852 max memory_allocated 29271.65478515625 
[2025-03-22 20:03:01 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.7395610809326172 norm:0.0014362011570483446 max memory_allocated 29271.65478515625 
[2025-03-22 20:03:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.7378805875778198 norm:0.001386305782943964 max memory_allocated 29271.65478515625 
[2025-03-22 20:04:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.7365164160728455 norm:0.001333683030679822 max memory_allocated 29271.65478515625 
[2025-03-22 20:05:25 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.7356967329978943 norm:0.001316495705395937 max memory_allocated 29271.65478515625 
[2025-03-22 20:06:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.7348265051841736 norm:0.0012839355040341616 max memory_allocated 29271.65478515625 
[2025-03-22 20:07:00 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.7341008186340332 norm:0.0012530408566817641 max memory_allocated 29271.65478515625 
[2025-03-22 20:07:48 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.7335948348045349 norm:0.0012431973591446877 max memory_allocated 29271.65478515625 
[2025-03-22 20:08:36 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.7330135107040405 norm:0.0012261131778359413 max memory_allocated 29271.65478515625 
[2025-03-22 20:09:23 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.7324321866035461 norm:0.0012110673123970628 max memory_allocated 29271.65478515625 
[2025-03-22 20:10:11 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.7319111824035645 norm:0.001201256294734776 max memory_allocated 29271.65478515625 
[2025-03-22 20:10:58 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.7315929532051086 norm:0.0011850761948153377 max memory_allocated 29271.65478515625 
[2025-03-22 20:11:46 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.7312452793121338 norm:0.001164544140920043 max memory_allocated 29271.65478515625 
[2025-03-22 20:12:34 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.7310401201248169 norm:0.0011605231557041407 max memory_allocated 29271.65478515625 
[2025-03-22 20:12:47 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 20:13:39 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.9011884331703186 norm:0.03587818518280983 max memory_allocated 29271.84228515625 
[2025-03-22 20:14:27 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.8759151697158813 norm:0.02086745947599411 max memory_allocated 29271.84228515625 
[2025-03-22 20:15:14 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.8472837209701538 norm:0.011966975405812263 max memory_allocated 29271.84228515625 
[2025-03-22 20:16:02 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.8315552473068237 norm:0.006107486318796873 max memory_allocated 29271.84228515625 
[2025-03-22 20:16:50 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.825577974319458 norm:0.004796791821718216 max memory_allocated 29271.84228515625 
[2025-03-22 20:17:38 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.8217889070510864 norm:0.004027824383229017 max memory_allocated 29271.84228515625 
[2025-03-22 20:18:26 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.8189841508865356 norm:0.0033586060162633657 max memory_allocated 29271.84228515625 
[2025-03-22 20:19:14 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.8168495297431946 norm:0.0030798902735114098 max memory_allocated 29271.84228515625 
[2025-03-22 20:20:01 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.8150933980941772 norm:0.0027531394734978676 max memory_allocated 29271.84228515625 
[2025-03-22 20:20:49 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.8134161233901978 norm:0.0023730145767331123 max memory_allocated 29271.84228515625 
[2025-03-22 20:21:37 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.8121193051338196 norm:0.002194140339270234 max memory_allocated 29271.84228515625 
[2025-03-22 20:22:25 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.8108761310577393 norm:0.0020370071288198233 max memory_allocated 29271.84228515625 
[2025-03-22 20:23:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.8099997043609619 norm:0.001986391842365265 max memory_allocated 29271.84228515625 
[2025-03-22 20:24:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.8086601495742798 norm:0.001494572963565588 max memory_allocated 29271.84228515625 
[2025-03-22 20:24:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.8078039288520813 norm:0.001241460908204317 max memory_allocated 29271.84228515625 
[2025-03-22 20:25:35 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.8073446154594421 norm:0.0012311192695051432 max memory_allocated 29271.84228515625 
[2025-03-22 20:26:23 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.8070576786994934 norm:0.0012108830269426107 max memory_allocated 29271.84228515625 
[2025-03-22 20:27:11 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.8067901134490967 norm:0.001191615010611713 max memory_allocated 29271.84228515625 
[2025-03-22 20:27:58 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.806547999382019 norm:0.0012004961026832461 max memory_allocated 29271.84228515625 
[2025-03-22 20:28:46 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.8063457608222961 norm:0.0011935430811718106 max memory_allocated 29271.84228515625 
[2025-03-22 20:28:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 20:29:51 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.9858039617538452 norm:0.01273525133728981 max memory_allocated 29272.02978515625 
[2025-03-22 20:30:39 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.9635665416717529 norm:0.007346931379288435 max memory_allocated 29272.02978515625 
[2025-03-22 20:31:26 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.937169075012207 norm:0.00366180669516325 max memory_allocated 29272.02978515625 
[2025-03-22 20:32:14 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.9275497198104858 norm:0.0021258483175188303 max memory_allocated 29272.02978515625 
[2025-03-22 20:33:02 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.9232038259506226 norm:0.0017776242457330227 max memory_allocated 29272.02978515625 
[2025-03-22 20:33:50 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.9201740622520447 norm:0.0016234532231464982 max memory_allocated 29272.02978515625 
[2025-03-22 20:34:38 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.9177879095077515 norm:0.0015823416179046035 max memory_allocated 29272.02978515625 
[2025-03-22 20:35:26 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.9159462451934814 norm:0.0015565373469144106 max memory_allocated 29272.02978515625 
[2025-03-22 20:36:14 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.9144325256347656 norm:0.0015044787432998419 max memory_allocated 29272.02978515625 
[2025-03-22 20:37:02 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.9131343960762024 norm:0.0014681480824947357 max memory_allocated 29272.02978515625 
[2025-03-22 20:37:49 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.9121579527854919 norm:0.0014463833067566156 max memory_allocated 29272.02978515625 
[2025-03-22 20:38:37 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.9113650918006897 norm:0.0014054663479328156 max memory_allocated 29272.02978515625 
[2025-03-22 20:39:25 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.9105847477912903 norm:0.0013810705859214067 max memory_allocated 29272.02978515625 
[2025-03-22 20:40:13 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.9100151658058167 norm:0.001337303314357996 max memory_allocated 29272.02978515625 
[2025-03-22 20:41:00 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.9095884561538696 norm:0.0013229058822616935 max memory_allocated 29272.02978515625 
[2025-03-22 20:41:48 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.9090718030929565 norm:0.0013051423011347651 max memory_allocated 29272.02978515625 
[2025-03-22 20:42:35 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.9087087512016296 norm:0.0012946572387591004 max memory_allocated 29272.02978515625 
[2025-03-22 20:43:23 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.9083662629127502 norm:0.0012715240009129047 max memory_allocated 29272.02978515625 
[2025-03-22 20:44:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.9081037044525146 norm:0.0012563326163217425 max memory_allocated 29272.02978515625 
[2025-03-22 20:44:58 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.9076977968215942 norm:0.0012460374273359776 max memory_allocated 29272.02978515625 
[2025-03-22 20:45:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 20:46:04 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:1.10904061794281 norm:0.019650105386972427 max memory_allocated 29272.21728515625 
[2025-03-22 20:46:51 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:1.086262822151184 norm:0.010114502161741257 max memory_allocated 29272.21728515625 
[2025-03-22 20:47:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:1.0607435703277588 norm:0.005491512827575207 max memory_allocated 29272.21728515625 
[2025-03-22 20:48:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:1.0491235256195068 norm:0.0027303537353873253 max memory_allocated 29272.21728515625 
[2025-03-22 20:49:15 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:1.0442144870758057 norm:0.0019394905539229512 max memory_allocated 29272.21728515625 
[2025-03-22 20:50:03 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:1.0412105321884155 norm:0.001818415243178606 max memory_allocated 29272.21728515625 
[2025-03-22 20:50:51 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:1.0386662483215332 norm:0.0017199760768562555 max memory_allocated 29272.21728515625 
[2025-03-22 20:51:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:1.0365874767303467 norm:0.0016431969124823809 max memory_allocated 29272.21728515625 
[2025-03-22 20:52:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:1.0350220203399658 norm:0.001557768089696765 max memory_allocated 29272.21728515625 
[2025-03-22 20:53:14 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:1.0337117910385132 norm:0.001540389726869762 max memory_allocated 29272.21728515625 
[2025-03-22 20:54:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:1.0327353477478027 norm:0.0015204385854303837 max memory_allocated 29272.21728515625 
[2025-03-22 20:54:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:1.0319066047668457 norm:0.0014858139911666512 max memory_allocated 29272.21728515625 
[2025-03-22 20:55:38 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:1.0312674045562744 norm:0.0014679802116006613 max memory_allocated 29272.21728515625 
[2025-03-22 20:56:26 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:1.03070068359375 norm:0.001453409087844193 max memory_allocated 29272.21728515625 
[2025-03-22 20:57:14 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:1.0302541255950928 norm:0.0014294900465756655 max memory_allocated 29272.21728515625 
[2025-03-22 20:58:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:1.0298428535461426 norm:0.001423695939593017 max memory_allocated 29272.21728515625 
[2025-03-22 20:58:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:1.029431700706482 norm:0.0014051573816686869 max memory_allocated 29272.21728515625 
[2025-03-22 20:59:38 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:1.0291409492492676 norm:0.0013998850481584668 max memory_allocated 29272.21728515625 
[2025-03-22 21:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:1.0288426876068115 norm:0.0013890608679503202 max memory_allocated 29272.21728515625 
[2025-03-22 21:01:13 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:1.028538703918457 norm:0.0013762323651462793 max memory_allocated 29272.21728515625 
[2025-03-22 21:01:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 21:02:18 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:1.2472400665283203 norm:0.019633116200566292 max memory_allocated 29272.40478515625 
[2025-03-22 21:03:06 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:1.227716326713562 norm:0.011559880338609219 max memory_allocated 29272.40478515625 
[2025-03-22 21:03:53 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:1.2061067819595337 norm:0.0061964355409145355 max memory_allocated 29272.40478515625 
[2025-03-22 21:04:41 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:1.1957898139953613 norm:0.0024154321290552616 max memory_allocated 29272.40478515625 
[2025-03-22 21:05:29 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:1.1914719343185425 norm:0.00205399957485497 max memory_allocated 29272.40478515625 
[2025-03-22 21:06:17 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:1.188409447669983 norm:0.0019396953284740448 max memory_allocated 29272.40478515625 
[2025-03-22 21:07:05 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:1.1860445737838745 norm:0.0018868783954530954 max memory_allocated 29272.40478515625 
[2025-03-22 21:07:53 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:1.1842131614685059 norm:0.0018201242201030254 max memory_allocated 29272.40478515625 
[2025-03-22 21:08:41 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:1.1825029850006104 norm:0.001777615980245173 max memory_allocated 29272.40478515625 
[2025-03-22 21:09:29 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:1.1811283826828003 norm:0.001743717584758997 max memory_allocated 29272.40478515625 
[2025-03-22 21:10:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:1.1801526546478271 norm:0.001717696082778275 max memory_allocated 29272.40478515625 
[2025-03-22 21:11:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:1.1792958974838257 norm:0.0016969023272395134 max memory_allocated 29272.40478515625 
[2025-03-22 21:11:52 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:1.1785290241241455 norm:0.0016832036199048162 max memory_allocated 29272.40478515625 
[2025-03-22 21:12:40 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:1.177980661392212 norm:0.0016710605705156922 max memory_allocated 29272.40478515625 
[2025-03-22 21:13:28 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:1.1774120330810547 norm:0.0016524933744221926 max memory_allocated 29272.40478515625 
[2025-03-22 21:14:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:1.1768288612365723 norm:0.001630665035918355 max memory_allocated 29272.40478515625 
[2025-03-22 21:15:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:1.1764106750488281 norm:0.0016202314291149378 max memory_allocated 29272.40478515625 
[2025-03-22 21:15:52 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:1.1759296655654907 norm:0.0016146067064255476 max memory_allocated 29272.40478515625 
[2025-03-22 21:16:40 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:1.1756455898284912 norm:0.001626653247512877 max memory_allocated 29272.40478515625 
[2025-03-22 21:17:28 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:1.175335168838501 norm:0.0016027032397687435 max memory_allocated 29272.40478515625 
[2025-03-22 21:17:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 21:18:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:1.3851897716522217 norm:0.012942974455654621 max memory_allocated 29272.59228515625 
[2025-03-22 21:19:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:1.3663852214813232 norm:0.007446653209626675 max memory_allocated 29272.59228515625 
[2025-03-22 21:20:10 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:1.347953200340271 norm:0.004941757768392563 max memory_allocated 29272.59228515625 
[2025-03-22 21:20:58 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:1.3407706022262573 norm:0.0035754835698753595 max memory_allocated 29272.59228515625 
[2025-03-22 21:21:46 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:1.33688223361969 norm:0.0030609192326664925 max memory_allocated 29272.59228515625 
[2025-03-22 21:22:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:1.3336161375045776 norm:0.0026964345015585423 max memory_allocated 29272.59228515625 
[2025-03-22 21:23:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:1.330754041671753 norm:0.002375416923314333 max memory_allocated 29272.59228515625 
[2025-03-22 21:24:10 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:1.3285309076309204 norm:0.002169162267819047 max memory_allocated 29272.59228515625 
[2025-03-22 21:24:58 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:1.326756477355957 norm:0.002007904928177595 max memory_allocated 29272.59228515625 
[2025-03-22 21:25:46 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:1.3254084587097168 norm:0.0018774670315906405 max memory_allocated 29272.59228515625 
[2025-03-22 21:26:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:1.3242347240447998 norm:0.001786760170944035 max memory_allocated 29272.59228515625 
[2025-03-22 21:27:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:1.3233298063278198 norm:0.0017028490547090769 max memory_allocated 29272.59228515625 
[2025-03-22 21:28:10 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:1.3225767612457275 norm:0.0016379386652261019 max memory_allocated 29272.59228515625 
[2025-03-22 21:28:58 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:1.3219529390335083 norm:0.0016130302101373672 max memory_allocated 29272.59228515625 
[2025-03-22 21:29:46 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:1.321385145187378 norm:0.0015898244455456734 max memory_allocated 29272.59228515625 
[2025-03-22 21:30:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:1.3208422660827637 norm:0.001555745373480022 max memory_allocated 29272.59228515625 
[2025-03-22 21:31:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:1.3204106092453003 norm:0.0015326952561736107 max memory_allocated 29272.59228515625 
[2025-03-22 21:32:10 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:1.3201584815979004 norm:0.0015057484852150083 max memory_allocated 29272.59228515625 
[2025-03-22 21:32:58 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:1.3198671340942383 norm:0.0014845009427517653 max memory_allocated 29272.59228515625 
[2025-03-22 21:33:46 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:1.3196451663970947 norm:0.0014728331007063389 max memory_allocated 29272.59228515625 
[2025-03-22 21:33:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 21:34:51 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:1.5545549392700195 norm:0.019534289836883545 max memory_allocated 29272.77978515625 
[2025-03-22 21:35:39 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:1.5336918830871582 norm:0.011427344754338264 max memory_allocated 29272.77978515625 
[2025-03-22 21:36:26 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:1.512157678604126 norm:0.006747835781425238 max memory_allocated 29272.77978515625 
[2025-03-22 21:37:14 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:1.5025150775909424 norm:0.004090889357030392 max memory_allocated 29272.77978515625 
[2025-03-22 21:38:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:1.4967825412750244 norm:0.0018968787044286728 max memory_allocated 29272.77978515625 
[2025-03-22 21:38:49 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:1.4930474758148193 norm:0.0016828327206894755 max memory_allocated 29272.77978515625 
[2025-03-22 21:39:37 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:1.49038827419281 norm:0.0016127793351188302 max memory_allocated 29272.77978515625 
[2025-03-22 21:40:25 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:1.4882341623306274 norm:0.001571124535985291 max memory_allocated 29272.77978515625 
[2025-03-22 21:41:12 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:1.4866784811019897 norm:0.0015697672497481108 max memory_allocated 29272.77978515625 
[2025-03-22 21:42:00 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:1.4855940341949463 norm:0.0015571024268865585 max memory_allocated 29272.77978515625 
[2025-03-22 21:42:48 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:1.484651803970337 norm:0.001539499731734395 max memory_allocated 29272.77978515625 
[2025-03-22 21:43:36 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:1.4837579727172852 norm:0.0015177729073911905 max memory_allocated 29272.77978515625 
[2025-03-22 21:44:24 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:1.4830543994903564 norm:0.001500954618677497 max memory_allocated 29272.77978515625 
[2025-03-22 21:45:12 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:1.482496738433838 norm:0.0014926474541425705 max memory_allocated 29272.77978515625 
[2025-03-22 21:46:00 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:1.481880784034729 norm:0.0014846441335976124 max memory_allocated 29272.77978515625 
[2025-03-22 21:46:48 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:1.4814658164978027 norm:0.001477154204621911 max memory_allocated 29272.77978515625 
[2025-03-22 21:47:36 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:1.4813352823257446 norm:0.0014761029742658138 max memory_allocated 29272.77978515625 
[2025-03-22 21:48:24 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:1.4811452627182007 norm:0.001474403659813106 max memory_allocated 29272.77978515625 
[2025-03-22 21:49:12 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:1.4809224605560303 norm:0.00146797439083457 max memory_allocated 29272.77978515625 
[2025-03-22 21:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:1.480626106262207 norm:0.0014602429000660777 max memory_allocated 29272.77978515625 
[2025-03-22 21:50:14 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 21:51:06 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:1.7129567861557007 norm:0.014830555766820908 max memory_allocated 29272.96728515625 
[2025-03-22 21:51:54 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:1.6932024955749512 norm:0.008051854558289051 max memory_allocated 29272.96728515625 
[2025-03-22 21:52:42 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:1.6718429327011108 norm:0.004945625551044941 max memory_allocated 29272.96728515625 
[2025-03-22 21:53:30 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:1.6630710363388062 norm:0.002405567094683647 max memory_allocated 29272.96728515625 
[2025-03-22 21:54:18 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:1.6585355997085571 norm:0.0017033735057339072 max memory_allocated 29272.96728515625 
[2025-03-22 21:55:06 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:1.6550695896148682 norm:0.0015229058917611837 max memory_allocated 29272.96728515625 
[2025-03-22 21:55:54 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:1.6523081064224243 norm:0.0014243570622056723 max memory_allocated 29272.96728515625 
[2025-03-22 21:56:41 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:1.6503046751022339 norm:0.001403767499141395 max memory_allocated 29272.96728515625 
[2025-03-22 21:57:29 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:1.6488685607910156 norm:0.0013940349454060197 max memory_allocated 29272.96728515625 
[2025-03-22 21:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:1.647779941558838 norm:0.0013765156036242843 max memory_allocated 29272.96728515625 
[2025-03-22 21:59:05 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:1.646904468536377 norm:0.001386704738251865 max memory_allocated 29272.96728515625 
[2025-03-22 21:59:52 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:1.6461870670318604 norm:0.0013891810085624456 max memory_allocated 29272.96728515625 
[2025-03-22 22:00:40 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:1.6455111503601074 norm:0.0013855563011020422 max memory_allocated 29272.96728515625 
[2025-03-22 22:01:28 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:1.6450008153915405 norm:0.0013795592822134495 max memory_allocated 29272.96728515625 
[2025-03-22 22:02:16 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:1.6444631814956665 norm:0.0013675600057467818 max memory_allocated 29272.96728515625 
[2025-03-22 22:03:04 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:1.6440708637237549 norm:0.001365582225844264 max memory_allocated 29272.96728515625 
[2025-03-22 22:03:52 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:1.6437307596206665 norm:0.0013674844522029161 max memory_allocated 29272.96728515625 
[2025-03-22 22:04:40 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:1.6434712409973145 norm:0.0013668547617271543 max memory_allocated 29272.96728515625 
[2025-03-22 22:05:28 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:1.6432013511657715 norm:0.0013685044832527637 max memory_allocated 29272.96728515625 
[2025-03-22 22:06:16 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:1.6430637836456299 norm:0.001367220189422369 max memory_allocated 29272.96728515625 
[2025-03-22 22:06:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 22:07:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:1.9007799625396729 norm:0.006044638343155384 max memory_allocated 29273.15478515625 
[2025-03-22 22:08:09 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:1.8826037645339966 norm:0.00373777374625206 max memory_allocated 29273.15478515625 
[2025-03-22 22:08:57 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:1.8606529235839844 norm:0.002630912698805332 max memory_allocated 29273.15478515625 
[2025-03-22 22:09:45 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:1.8520941734313965 norm:0.001714628655463457 max memory_allocated 29273.15478515625 
[2025-03-22 22:10:33 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:1.847460150718689 norm:0.0015184273943305016 max memory_allocated 29273.15478515625 
[2025-03-22 22:11:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:1.843883752822876 norm:0.0014426816487684846 max memory_allocated 29273.15478515625 
[2025-03-22 22:12:09 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:1.8410108089447021 norm:0.0014188027707859874 max memory_allocated 29273.15478515625 
[2025-03-22 22:12:57 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:1.8387702703475952 norm:0.0014052358455955982 max memory_allocated 29273.15478515625 
[2025-03-22 22:13:44 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:1.8369985818862915 norm:0.001396820181980729 max memory_allocated 29273.15478515625 
[2025-03-22 22:14:32 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:1.8356611728668213 norm:0.001390085439197719 max memory_allocated 29273.15478515625 
[2025-03-22 22:15:20 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:1.8345698118209839 norm:0.0013896087184548378 max memory_allocated 29273.15478515625 
[2025-03-22 22:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:1.8336260318756104 norm:0.0013842484913766384 max memory_allocated 29273.15478515625 
[2025-03-22 22:16:55 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:1.8329863548278809 norm:0.001389967859722674 max memory_allocated 29273.15478515625 
[2025-03-22 22:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:1.8324271440505981 norm:0.0013975552283227444 max memory_allocated 29273.15478515625 
[2025-03-22 22:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:1.8319576978683472 norm:0.001396824256516993 max memory_allocated 29273.15478515625 
[2025-03-22 22:19:19 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:1.8316502571105957 norm:0.0013986103003844619 max memory_allocated 29273.15478515625 
[2025-03-22 22:20:07 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:1.8313688039779663 norm:0.0014073287602514029 max memory_allocated 29273.15478515625 
[2025-03-22 22:20:54 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:1.8311747312545776 norm:0.0014097185339778662 max memory_allocated 29273.15478515625 
[2025-03-22 22:21:42 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:1.830923080444336 norm:0.0014095025835558772 max memory_allocated 29273.15478515625 
[2025-03-22 22:22:30 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:1.8306955099105835 norm:0.0014069507597014308 max memory_allocated 29273.15478515625 
[2025-03-22 22:22:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 22:23:36 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:2.098574638366699 norm:0.00742217805236578 max memory_allocated 29273.34228515625 
[2025-03-22 22:24:24 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:2.0769989490509033 norm:0.004225288517773151 max memory_allocated 29273.34228515625 
[2025-03-22 22:25:12 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:2.0551795959472656 norm:0.0031628001015633345 max memory_allocated 29273.34228515625 
[2025-03-22 22:26:00 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:2.047837972640991 norm:0.0027047814801335335 max memory_allocated 29273.34228515625 
[2025-03-22 22:26:47 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:2.0431647300720215 norm:0.0022500702179968357 max memory_allocated 29273.34228515625 
[2025-03-22 22:27:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:2.039466381072998 norm:0.0021094500552862883 max memory_allocated 29273.34228515625 
[2025-03-22 22:28:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:2.0363214015960693 norm:0.0019890815019607544 max memory_allocated 29273.34228515625 
[2025-03-22 22:29:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:2.034067392349243 norm:0.0019425756763666868 max memory_allocated 29273.34228515625 
[2025-03-22 22:29:59 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:2.032383918762207 norm:0.0018534284317865968 max memory_allocated 29273.34228515625 
[2025-03-22 22:30:47 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:2.0310373306274414 norm:0.0018169902032241225 max memory_allocated 29273.34228515625 
[2025-03-22 22:31:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:2.030005931854248 norm:0.0017680751625448465 max memory_allocated 29273.34228515625 
[2025-03-22 22:32:22 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:2.029078960418701 norm:0.0017253514379262924 max memory_allocated 29273.34228515625 
[2025-03-22 22:33:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:2.028589963912964 norm:0.0017348849214613438 max memory_allocated 29273.34228515625 
[2025-03-22 22:33:58 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:2.027947187423706 norm:0.0017455486813560128 max memory_allocated 29273.34228515625 
[2025-03-22 22:34:45 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:2.0271804332733154 norm:0.0017140146810561419 max memory_allocated 29273.34228515625 
[2025-03-22 22:35:33 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:2.026627540588379 norm:0.0016898480243980885 max memory_allocated 29273.34228515625 
[2025-03-22 22:36:21 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:2.0261521339416504 norm:0.0016741130966693163 max memory_allocated 29273.34228515625 
[2025-03-22 22:37:09 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:2.0256614685058594 norm:0.001651796861551702 max memory_allocated 29273.34228515625 
[2025-03-22 22:37:56 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:2.025041341781616 norm:0.0016525391256436706 max memory_allocated 29273.34228515625 
[2025-03-22 22:38:44 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:2.024749755859375 norm:0.0016654308419674635 max memory_allocated 29273.34228515625 
[2025-03-22 22:38:58 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 22:39:50 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:2.28003191947937 norm:0.011605053208768368 max memory_allocated 29273.52978515625 
[2025-03-22 22:40:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:2.260143518447876 norm:0.006372692529112101 max memory_allocated 29273.52978515625 
[2025-03-22 22:41:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:2.2367665767669678 norm:0.0037682473193854094 max memory_allocated 29273.52978515625 
[2025-03-22 22:42:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:2.2280142307281494 norm:0.0026466501876711845 max memory_allocated 29273.52978515625 
[2025-03-22 22:43:01 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:2.2226953506469727 norm:0.0017001795349642634 max memory_allocated 29273.52978515625 
[2025-03-22 22:43:49 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:2.2188189029693604 norm:0.0015001744031906128 max memory_allocated 29273.52978515625 
[2025-03-22 22:44:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:2.2155938148498535 norm:0.0014429612783715129 max memory_allocated 29273.52978515625 
[2025-03-22 22:45:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:2.2131500244140625 norm:0.0014010689919814467 max memory_allocated 29273.52978515625 
[2025-03-22 22:46:12 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:2.211421012878418 norm:0.0013661381090059876 max memory_allocated 29273.52978515625 
[2025-03-22 22:47:00 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:2.2100589275360107 norm:0.0013579099904745817 max memory_allocated 29273.52978515625 
[2025-03-22 22:47:48 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:2.209120512008667 norm:0.0013520619831979275 max memory_allocated 29273.52978515625 
[2025-03-22 22:48:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:2.2084083557128906 norm:0.0013526304392144084 max memory_allocated 29273.52978515625 
[2025-03-22 22:49:23 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:2.207796812057495 norm:0.0013255310477688909 max memory_allocated 29273.52978515625 
[2025-03-22 22:50:10 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:2.2072839736938477 norm:0.001328185317106545 max memory_allocated 29273.52978515625 
[2025-03-22 22:50:58 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:2.20686936378479 norm:0.0013349107466638088 max memory_allocated 29273.52978515625 
[2025-03-22 22:51:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:2.206480026245117 norm:0.0013370474334806204 max memory_allocated 29273.52978515625 
[2025-03-22 22:52:33 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:2.20617938041687 norm:0.0013361740857362747 max memory_allocated 29273.52978515625 
[2025-03-22 22:53:21 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:2.2058980464935303 norm:0.001333086285740137 max memory_allocated 29273.52978515625 
[2025-03-22 22:54:09 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:2.205712080001831 norm:0.0013280822895467281 max memory_allocated 29273.52978515625 
[2025-03-22 22:54:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:2.2054502964019775 norm:0.001322511350736022 max memory_allocated 29273.52978515625 
[2025-03-22 22:55:11 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 22:56:02 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:2.4986047744750977 norm:0.015882140025496483 max memory_allocated 29273.71728515625 
[2025-03-22 22:56:50 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:2.4788975715637207 norm:0.011176977306604385 max memory_allocated 29273.71728515625 
[2025-03-22 22:57:38 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:2.4549689292907715 norm:0.008005723357200623 max memory_allocated 29273.71728515625 
[2025-03-22 22:58:26 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:2.4454660415649414 norm:0.006176173686981201 max memory_allocated 29273.71728515625 
[2025-03-22 22:59:13 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:2.4387059211730957 norm:0.004739692434668541 max memory_allocated 29273.71728515625 
[2025-03-22 23:00:01 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:2.4317562580108643 norm:0.0036349401343613863 max memory_allocated 29273.71728515625 
[2025-03-22 23:00:49 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:2.4275856018066406 norm:0.0035385112278163433 max memory_allocated 29273.71728515625 
[2025-03-22 23:01:37 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:2.424731969833374 norm:0.0031942541245371103 max memory_allocated 29273.71728515625 
[2025-03-22 23:02:25 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:2.422506332397461 norm:0.0029332884587347507 max memory_allocated 29273.71728515625 
[2025-03-22 23:03:13 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:2.4207797050476074 norm:0.002706375904381275 max memory_allocated 29273.71728515625 
[2025-03-22 23:04:01 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:2.419346570968628 norm:0.002541277091950178 max memory_allocated 29273.71728515625 
[2025-03-22 23:04:48 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:2.418144464492798 norm:0.002372015966102481 max memory_allocated 29273.71728515625 
[2025-03-22 23:05:36 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:2.4172654151916504 norm:0.0022266656160354614 max memory_allocated 29273.71728515625 
[2025-03-22 23:06:24 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:2.416443347930908 norm:0.0021034418605268 max memory_allocated 29273.71728515625 
[2025-03-22 23:07:11 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:2.4156856536865234 norm:0.00198898883536458 max memory_allocated 29273.71728515625 
[2025-03-22 23:07:59 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:2.415060520172119 norm:0.0019119430799037218 max memory_allocated 29273.71728515625 
[2025-03-22 23:08:46 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:2.414358615875244 norm:0.0018306704005226493 max memory_allocated 29273.71728515625 
[2025-03-22 23:09:34 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:2.4138407707214355 norm:0.001791799091733992 max memory_allocated 29273.71728515625 
[2025-03-22 23:10:22 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:2.413407802581787 norm:0.0017731825355440378 max memory_allocated 29273.71728515625 
[2025-03-22 23:11:10 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:2.412844657897949 norm:0.001716469880193472 max memory_allocated 29273.71728515625 
[2025-03-22 23:11:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 23:12:15 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:2.700791597366333 norm:0.0065742796286940575 max memory_allocated 29273.90478515625 
[2025-03-22 23:13:03 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:2.6791861057281494 norm:0.0038944846019148827 max memory_allocated 29273.90478515625 
[2025-03-22 23:13:50 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:2.6551132202148438 norm:0.002797225955873728 max memory_allocated 29273.90478515625 
[2025-03-22 23:14:38 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:2.6457128524780273 norm:0.0023141452111303806 max memory_allocated 29273.90478515625 
[2025-03-22 23:15:26 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:2.639927387237549 norm:0.0020862414967268705 max memory_allocated 29273.90478515625 
[2025-03-22 23:16:14 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:2.635087251663208 norm:0.001997284824028611 max memory_allocated 29273.90478515625 
[2025-03-22 23:17:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:2.6311888694763184 norm:0.0019229024183005095 max memory_allocated 29273.90478515625 
[2025-03-22 23:17:50 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:2.6281919479370117 norm:0.0018161414191126823 max memory_allocated 29273.90478515625 
[2025-03-22 23:18:37 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:2.6261725425720215 norm:0.0017563028959557414 max memory_allocated 29273.90478515625 
[2025-03-22 23:19:25 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:2.6245851516723633 norm:0.001721069449558854 max memory_allocated 29273.90478515625 
[2025-03-22 23:20:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:2.623406410217285 norm:0.0016741103027015924 max memory_allocated 29273.90478515625 
[2025-03-22 23:21:01 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:2.6224441528320312 norm:0.001669213641434908 max memory_allocated 29273.90478515625 
[2025-03-22 23:21:49 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:2.6216509342193604 norm:0.001657606102526188 max memory_allocated 29273.90478515625 
[2025-03-22 23:22:37 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:2.6210575103759766 norm:0.0016273042419925332 max memory_allocated 29273.90478515625 
[2025-03-22 23:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:2.620582342147827 norm:0.0016198891680687666 max memory_allocated 29273.90478515625 
[2025-03-22 23:24:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:2.6201095581054688 norm:0.0016058895271271467 max memory_allocated 29273.90478515625 
[2025-03-22 23:25:00 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:2.619642734527588 norm:0.0015845660818740726 max memory_allocated 29273.90478515625 
[2025-03-22 23:25:48 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:2.6192760467529297 norm:0.0015781832626089454 max memory_allocated 29273.90478515625 
[2025-03-22 23:26:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:2.618962049484253 norm:0.0015775581123307347 max memory_allocated 29273.90478515625 
[2025-03-22 23:27:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:2.618720054626465 norm:0.0015684225363656878 max memory_allocated 29273.90478515625 
[2025-03-22 23:27:37 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 32 ===
[2025-03-22 23:28:29 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 0 loss:2.915574550628662 norm:0.011943826451897621 max memory_allocated 29274.09228515625 
[2025-03-22 23:29:17 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 1 loss:2.8926732540130615 norm:0.0071996352635324 max memory_allocated 29274.09228515625 
[2025-03-22 23:30:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 2 loss:2.865222692489624 norm:0.004599708132445812 max memory_allocated 29274.09228515625 
[2025-03-22 23:30:52 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 3 loss:2.8531858921051025 norm:0.003125215182080865 max memory_allocated 29274.09228515625 
[2025-03-22 23:31:40 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 4 loss:2.8474602699279785 norm:0.0024530249647796154 max memory_allocated 29274.09228515625 
[2025-03-22 23:32:28 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 5 loss:2.842273712158203 norm:0.0021264327224344015 max memory_allocated 29274.09228515625 
[2025-03-22 23:33:16 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 6 loss:2.8377552032470703 norm:0.0018988021183758974 max memory_allocated 29274.09228515625 
[2025-03-22 23:34:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 7 loss:2.8342981338500977 norm:0.0017619545105844736 max memory_allocated 29274.09228515625 
[2025-03-22 23:34:51 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 8 loss:2.831878900527954 norm:0.001668228767812252 max memory_allocated 29274.09228515625 
[2025-03-22 23:35:39 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 9 loss:2.830172061920166 norm:0.001616175752133131 max memory_allocated 29274.09228515625 
[2025-03-22 23:36:27 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 10 loss:2.8286430835723877 norm:0.001536525902338326 max memory_allocated 29274.09228515625 
[2025-03-22 23:37:14 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 11 loss:2.8275842666625977 norm:0.0015000093262642622 max memory_allocated 29274.09228515625 
[2025-03-22 23:38:02 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 12 loss:2.826813220977783 norm:0.0014842052478343248 max memory_allocated 29274.09228515625 
[2025-03-22 23:38:50 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 13 loss:2.826007127761841 norm:0.001460256869904697 max memory_allocated 29274.09228515625 
[2025-03-22 23:39:37 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 14 loss:2.8253273963928223 norm:0.0014415652258321643 max memory_allocated 29274.09228515625 
[2025-03-22 23:40:25 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 15 loss:2.8249905109405518 norm:0.0014252477558329701 max memory_allocated 29274.09228515625 
[2025-03-22 23:41:13 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 16 loss:2.824601888656616 norm:0.0014166529290378094 max memory_allocated 29274.09228515625 
[2025-03-22 23:42:00 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 17 loss:2.8241798877716064 norm:0.0014043045230209827 max memory_allocated 29274.09228515625 
[2025-03-22 23:42:48 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 18 loss:2.8237597942352295 norm:0.0013913005823269486 max memory_allocated 29274.09228515625 
[2025-03-22 23:43:36 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 19 loss:2.8233723640441895 norm:0.0013877088204026222 max memory_allocated 29274.09228515625 
[2025-03-22 23:43:49 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 33 ===
[2025-03-22 23:44:41 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 0 loss:3.15956974029541 norm:0.010902108624577522 max memory_allocated 29274.27978515625 
[2025-03-22 23:45:29 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 1 loss:3.1345224380493164 norm:0.006027192808687687 max memory_allocated 29274.27978515625 
[2025-03-22 23:46:16 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 2 loss:3.1049880981445312 norm:0.004072091542184353 max memory_allocated 29274.27978515625 
[2025-03-22 23:47:04 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 3 loss:3.0923893451690674 norm:0.002903313608840108 max memory_allocated 29274.27978515625 
[2025-03-22 23:47:52 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 4 loss:3.0852909088134766 norm:0.0023202525917440653 max memory_allocated 29274.27978515625 
[2025-03-22 23:48:40 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 5 loss:3.079240322113037 norm:0.001957896165549755 max memory_allocated 29274.27978515625 
[2025-03-22 23:49:28 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 6 loss:3.0743908882141113 norm:0.0018351627513766289 max memory_allocated 29274.27978515625 
[2025-03-22 23:50:15 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 7 loss:3.0705442428588867 norm:0.0017537298845127225 max memory_allocated 29274.27978515625 
[2025-03-22 23:51:03 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 8 loss:3.0682425498962402 norm:0.0016945370007306337 max memory_allocated 29274.27978515625 
[2025-03-22 23:51:51 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 9 loss:3.066311836242676 norm:0.0016378732398152351 max memory_allocated 29274.27978515625 
[2025-03-22 23:52:39 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 10 loss:3.0649099349975586 norm:0.0015935056144371629 max memory_allocated 29274.27978515625 
[2025-03-22 23:53:27 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 11 loss:3.0638949871063232 norm:0.0015641265781596303 max memory_allocated 29274.27978515625 
[2025-03-22 23:54:14 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 12 loss:3.06278395652771 norm:0.0015448657795786858 max memory_allocated 29274.27978515625 
[2025-03-22 23:55:02 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 13 loss:3.061791181564331 norm:0.0015106737846508622 max memory_allocated 29274.27978515625 
[2025-03-22 23:55:49 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 14 loss:3.0609660148620605 norm:0.0014894853811711073 max memory_allocated 29274.27978515625 
[2025-03-22 23:56:37 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 15 loss:3.0602610111236572 norm:0.0014691052492707968 max memory_allocated 29274.27978515625 
[2025-03-22 23:57:25 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 16 loss:3.0597214698791504 norm:0.0014540267875418067 max memory_allocated 29274.27978515625 
[2025-03-22 23:58:12 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 17 loss:3.059321641921997 norm:0.001453002798371017 max memory_allocated 29274.27978515625 
[2025-03-22 23:59:00 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 18 loss:3.058924913406372 norm:0.0014671809040009975 max memory_allocated 29274.27978515625 
[2025-03-22 23:59:48 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 19 loss:3.058581829071045 norm:0.0014726186636835337 max memory_allocated 29274.27978515625 
[2025-03-23 00:00:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 34 ===
[2025-03-23 00:00:54 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 0 loss:3.4807305335998535 norm:0.034350164234638214 max memory_allocated 29274.46728515625 
[2025-03-23 00:01:42 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 1 loss:3.4467756748199463 norm:0.022705383598804474 max memory_allocated 29274.46728515625 
[2025-03-23 00:02:29 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 2 loss:3.4066245555877686 norm:0.014965113252401352 max memory_allocated 29274.46728515625 
[2025-03-23 00:03:17 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 3 loss:3.3851804733276367 norm:0.009980151429772377 max memory_allocated 29274.46728515625 
[2025-03-23 00:04:05 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 4 loss:3.372303009033203 norm:0.007435898296535015 max memory_allocated 29274.46728515625 
[2025-03-23 00:04:53 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 5 loss:3.3639466762542725 norm:0.006369187496602535 max memory_allocated 29274.46728515625 
[2025-03-23 00:05:41 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 6 loss:3.358032464981079 norm:0.005535291042178869 max memory_allocated 29274.46728515625 
[2025-03-23 00:06:29 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 7 loss:3.352938175201416 norm:0.004892466589808464 max memory_allocated 29274.46728515625 
[2025-03-23 00:07:16 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 8 loss:3.3485288619995117 norm:0.003964610863476992 max memory_allocated 29274.46728515625 
[2025-03-23 00:08:04 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 9 loss:3.3438758850097656 norm:0.0019064015941694379 max memory_allocated 29274.46728515625 
[2025-03-23 00:08:52 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 10 loss:3.3412771224975586 norm:0.0019219114910811186 max memory_allocated 29274.46728515625 
[2025-03-23 00:09:40 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 11 loss:3.3392581939697266 norm:0.0019058807520195842 max memory_allocated 29274.46728515625 
[2025-03-23 00:10:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 12 loss:3.3384461402893066 norm:0.0018962572794407606 max memory_allocated 29274.46728515625 
[2025-03-23 00:11:15 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 13 loss:3.3380026817321777 norm:0.0018486955668777227 max memory_allocated 29274.46728515625 
[2025-03-23 00:12:03 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 14 loss:3.3367700576782227 norm:0.001837554736994207 max memory_allocated 29274.46728515625 
[2025-03-23 00:12:51 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 15 loss:3.335784435272217 norm:0.0018416333477944136 max memory_allocated 29274.46728515625 
[2025-03-23 00:13:38 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 16 loss:3.334986686706543 norm:0.0018335104687139392 max memory_allocated 29274.46728515625 
[2025-03-23 00:14:26 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 17 loss:3.334124803543091 norm:0.0018274071626365185 max memory_allocated 29274.46728515625 
[2025-03-23 00:15:13 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 18 loss:3.3336434364318848 norm:0.0018317745998501778 max memory_allocated 29274.46728515625 
[2025-03-23 00:16:01 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 19 loss:3.3333911895751953 norm:0.001815393683500588 max memory_allocated 29274.46728515625 
[2025-03-23 00:16:15 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 35 ===
[2025-03-23 00:17:06 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 0 loss:3.7751526832580566 norm:0.022735334932804108 max memory_allocated 29274.65478515625 
[2025-03-23 00:17:54 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 1 loss:3.736006736755371 norm:0.015339169651269913 max memory_allocated 29274.65478515625 
[2025-03-23 00:18:42 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 2 loss:3.6916697025299072 norm:0.010552888736128807 max memory_allocated 29274.65478515625 
[2025-03-23 00:19:29 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 3 loss:3.669334888458252 norm:0.00750049389898777 max memory_allocated 29274.65478515625 
[2025-03-23 00:20:17 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 4 loss:3.6561012268066406 norm:0.005720242392271757 max memory_allocated 29274.65478515625 
[2025-03-23 00:21:05 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 5 loss:3.646782636642456 norm:0.005066127981990576 max memory_allocated 29274.65478515625 
[2025-03-23 00:21:53 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 6 loss:3.63907527923584 norm:0.004389074631035328 max memory_allocated 29274.65478515625 
[2025-03-23 00:22:40 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 7 loss:3.6339187622070312 norm:0.003992373123764992 max memory_allocated 29274.65478515625 
[2025-03-23 00:23:28 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 8 loss:3.6295156478881836 norm:0.0036618972662836313 max memory_allocated 29274.65478515625 
[2025-03-23 00:24:16 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 9 loss:3.626591682434082 norm:0.003394728759303689 max memory_allocated 29274.65478515625 
[2025-03-23 00:25:04 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 10 loss:3.624152183532715 norm:0.00323107885196805 max memory_allocated 29274.65478515625 
[2025-03-23 00:25:52 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 11 loss:3.6220340728759766 norm:0.003046785481274128 max memory_allocated 29274.65478515625 
[2025-03-23 00:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 12 loss:3.6203465461730957 norm:0.0029392880387604237 max memory_allocated 29274.65478515625 
[2025-03-23 00:27:28 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 13 loss:3.6187996864318848 norm:0.0029619389679282904 max memory_allocated 29274.65478515625 
[2025-03-23 00:28:15 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 14 loss:3.617304801940918 norm:0.0030061157885938883 max memory_allocated 29274.65478515625 
[2025-03-23 00:29:03 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 15 loss:3.616546154022217 norm:0.002996761817485094 max memory_allocated 29274.65478515625 
[2025-03-23 00:29:51 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 16 loss:3.6153273582458496 norm:0.0029500462114810944 max memory_allocated 29274.65478515625 
[2025-03-23 00:30:39 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 17 loss:3.6138641834259033 norm:0.002862410619854927 max memory_allocated 29274.65478515625 
[2025-03-23 00:31:27 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 18 loss:3.613018751144409 norm:0.002976016141474247 max memory_allocated 29274.65478515625 
[2025-03-23 00:32:15 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 19 loss:3.6123030185699463 norm:0.0030207543168216944 max memory_allocated 29274.65478515625 
[2025-03-23 00:32:28 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 36 ===
[2025-03-23 00:32:32 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 00:33:20 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 0 loss:4.096463680267334 norm:0.05612852796912193 max memory_allocated 29275.42041015625 
[2025-03-23 00:34:08 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 1 loss:4.046751976013184 norm:0.04990816488862038 max memory_allocated 29275.42041015625 
[2025-03-23 00:34:56 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 2 loss:3.992619752883911 norm:0.03976409137248993 max memory_allocated 29275.42041015625 
[2025-03-23 00:35:44 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 3 loss:3.9678049087524414 norm:0.032862886786460876 max memory_allocated 29275.42041015625 
[2025-03-23 00:36:32 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 4 loss:3.953809976577759 norm:0.027785133570432663 max memory_allocated 29275.42041015625 
[2025-03-23 00:37:20 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 5 loss:3.942431688308716 norm:0.024635344743728638 max memory_allocated 29275.42041015625 
[2025-03-23 00:38:08 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 6 loss:3.9319448471069336 norm:0.022220127284526825 max memory_allocated 29275.42041015625 
[2025-03-23 00:38:56 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 7 loss:3.9249532222747803 norm:0.02063959650695324 max memory_allocated 29275.42041015625 
[2025-03-23 00:39:44 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 8 loss:3.919923782348633 norm:0.01923360675573349 max memory_allocated 29275.42041015625 
[2025-03-23 00:40:32 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 9 loss:3.9157028198242188 norm:0.018413309007883072 max memory_allocated 29275.42041015625 
[2025-03-23 00:41:20 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 10 loss:3.9125847816467285 norm:0.017956873401999474 max memory_allocated 29275.42041015625 
[2025-03-23 00:42:08 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 11 loss:3.9102041721343994 norm:0.01706642657518387 max memory_allocated 29275.42041015625 
[2025-03-23 00:42:56 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 12 loss:3.90848445892334 norm:0.01665131375193596 max memory_allocated 29275.42041015625 
[2025-03-23 00:43:44 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 13 loss:3.9072751998901367 norm:0.016644520685076714 max memory_allocated 29275.42041015625 
[2025-03-23 00:44:32 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 14 loss:3.905280351638794 norm:0.016401996836066246 max memory_allocated 29275.42041015625 
[2025-03-23 00:45:19 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 15 loss:3.9036598205566406 norm:0.016331220045685768 max memory_allocated 29275.42041015625 
[2025-03-23 00:46:07 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 16 loss:3.902597665786743 norm:0.016033757477998734 max memory_allocated 29275.42041015625 
[2025-03-23 00:46:55 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 17 loss:3.901684522628784 norm:0.015812410041689873 max memory_allocated 29275.42041015625 
[2025-03-23 00:47:43 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 18 loss:3.9009854793548584 norm:0.015850527212023735 max memory_allocated 29275.42041015625 
[2025-03-23 00:48:31 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 19 loss:3.900205373764038 norm:0.015868481248617172 max memory_allocated 29275.42041015625 
[2025-03-23 00:48:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 37 ===
[2025-03-23 00:48:48 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 00:49:36 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 0 loss:4.576813697814941 norm:0.08845168352127075 max memory_allocated 29275.60791015625 
[2025-03-23 00:50:24 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 1 loss:4.501611232757568 norm:0.06912337988615036 max memory_allocated 29275.60791015625 
[2025-03-23 00:51:12 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 2 loss:4.42554235458374 norm:0.05068020895123482 max memory_allocated 29275.60791015625 
[2025-03-23 00:52:00 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 3 loss:4.38905143737793 norm:0.036691371351480484 max memory_allocated 29275.60791015625 
[2025-03-23 00:52:48 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 4 loss:4.3677978515625 norm:0.0278718713670969 max memory_allocated 29275.60791015625 
[2025-03-23 00:53:36 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 5 loss:4.353488922119141 norm:0.023966921493411064 max memory_allocated 29275.60791015625 
[2025-03-23 00:54:24 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 6 loss:4.343041896820068 norm:0.023829687386751175 max memory_allocated 29275.60791015625 
[2025-03-23 00:55:12 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 7 loss:4.336667537689209 norm:0.02332967147231102 max memory_allocated 29275.60791015625 
[2025-03-23 00:56:00 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 8 loss:4.330605983734131 norm:0.02300339564681053 max memory_allocated 29275.60791015625 
[2025-03-23 00:56:48 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 9 loss:4.326635360717773 norm:0.022655565291643143 max memory_allocated 29275.60791015625 
[2025-03-23 00:57:36 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 10 loss:4.320425033569336 norm:0.0197878647595644 max memory_allocated 29275.60791015625 
[2025-03-23 00:58:24 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 11 loss:4.317263603210449 norm:0.0197908878326416 max memory_allocated 29275.60791015625 
[2025-03-23 00:59:12 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 12 loss:4.315840721130371 norm:0.019164487719535828 max memory_allocated 29275.60791015625 
[2025-03-23 01:00:00 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 13 loss:4.314908981323242 norm:0.02003558538854122 max memory_allocated 29275.60791015625 
[2025-03-23 01:00:48 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 14 loss:4.312058448791504 norm:0.019636869430541992 max memory_allocated 29275.60791015625 
[2025-03-23 01:01:36 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 15 loss:4.310087203979492 norm:0.019555076956748962 max memory_allocated 29275.60791015625 
[2025-03-23 01:02:23 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 16 loss:4.3078813552856445 norm:0.01907467097043991 max memory_allocated 29275.60791015625 
[2025-03-23 01:03:11 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 17 loss:4.307156562805176 norm:0.019770393148064613 max memory_allocated 29275.60791015625 
[2025-03-23 01:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 18 loss:4.305235862731934 norm:0.019790250808000565 max memory_allocated 29275.60791015625 
[2025-03-23 01:04:47 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 19 loss:4.304081916809082 norm:0.019807830452919006 max memory_allocated 29275.60791015625 
[2025-03-23 01:05:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 38 ===
[2025-03-23 01:05:06 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 01:05:54 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 0 loss:6.08938455581665 norm:0.5187114477157593 max memory_allocated 29275.79541015625 
[2025-03-23 01:06:42 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 1 loss:5.759889125823975 norm:0.3790751099586487 max memory_allocated 29275.79541015625 
[2025-03-23 01:07:30 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 2 loss:5.650102615356445 norm:0.3196834921836853 max memory_allocated 29275.79541015625 
[2025-03-23 01:08:18 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 3 loss:5.5401740074157715 norm:0.23407086730003357 max memory_allocated 29275.79541015625 
[2025-03-23 01:09:06 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 4 loss:5.456137180328369 norm:0.17183391749858856 max memory_allocated 29275.79541015625 
[2025-03-23 01:09:54 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 5 loss:5.3779616355896 norm:0.1233111172914505 max memory_allocated 29275.79541015625 
[2025-03-23 01:10:42 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 6 loss:5.327648639678955 norm:0.10673758387565613 max memory_allocated 29275.79541015625 
[2025-03-23 01:11:29 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 7 loss:5.2955827713012695 norm:0.08941727876663208 max memory_allocated 29275.79541015625 
[2025-03-23 01:12:17 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 8 loss:5.270287990570068 norm:0.08597544580698013 max memory_allocated 29275.79541015625 
[2025-03-23 01:13:05 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 9 loss:5.249663352966309 norm:0.08079347014427185 max memory_allocated 29275.79541015625 
[2025-03-23 01:13:53 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 10 loss:5.235143184661865 norm:0.07694804668426514 max memory_allocated 29275.79541015625 
[2025-03-23 01:14:41 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 11 loss:5.223232746124268 norm:0.07429838925600052 max memory_allocated 29275.79541015625 
[2025-03-23 01:15:30 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 12 loss:5.212441444396973 norm:0.06853154301643372 max memory_allocated 29275.79541015625 
[2025-03-23 01:16:18 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 13 loss:5.202292442321777 norm:0.06408049166202545 max memory_allocated 29275.79541015625 
[2025-03-23 01:17:06 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 14 loss:5.195804119110107 norm:0.06131725013256073 max memory_allocated 29275.79541015625 
[2025-03-23 01:17:54 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 15 loss:5.187126159667969 norm:0.056924741715192795 max memory_allocated 29275.79541015625 
[2025-03-23 01:18:42 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 16 loss:5.182962417602539 norm:0.05607444792985916 max memory_allocated 29275.79541015625 
[2025-03-23 01:19:30 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 17 loss:5.1801652908325195 norm:0.05783975496888161 max memory_allocated 29275.79541015625 
[2025-03-23 01:20:18 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 18 loss:5.176018714904785 norm:0.055143680423498154 max memory_allocated 29275.79541015625 
[2025-03-23 01:21:06 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 19 loss:5.172404766082764 norm:0.05329392850399017 max memory_allocated 29275.79541015625 
[2025-03-23 01:21:20 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 39 ===
[2025-03-23 01:21:24 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 01:22:12 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 0 loss:10.01268196105957 norm:0.6557026505470276 max memory_allocated 29275.98291015625 
[2025-03-23 01:23:00 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 1 loss:9.567815780639648 norm:0.5017364621162415 max memory_allocated 29275.98291015625 
[2025-03-23 01:23:47 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 2 loss:9.137201309204102 norm:0.4135306477546692 max memory_allocated 29275.98291015625 
[2025-03-23 01:24:35 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 3 loss:8.860629081726074 norm:0.40396854281425476 max memory_allocated 29275.98291015625 
[2025-03-23 01:25:23 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 4 loss:8.723310470581055 norm:0.36625775694847107 max memory_allocated 29275.98291015625 
[2025-03-23 01:26:11 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 5 loss:8.631003379821777 norm:0.3339998126029968 max memory_allocated 29275.98291015625 
[2025-03-23 01:26:59 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 6 loss:8.561150550842285 norm:0.3253481388092041 max memory_allocated 29275.98291015625 
[2025-03-23 01:27:46 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 7 loss:8.492901802062988 norm:0.3118225634098053 max memory_allocated 29275.98291015625 
[2025-03-23 01:28:34 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 8 loss:8.419743537902832 norm:0.307686448097229 max memory_allocated 29275.98291015625 
[2025-03-23 01:29:22 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 9 loss:8.371402740478516 norm:0.2851264774799347 max memory_allocated 29275.98291015625 
[2025-03-23 01:30:10 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 10 loss:8.3419828414917 norm:0.2748139500617981 max memory_allocated 29275.98291015625 
[2025-03-23 01:30:57 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 11 loss:8.32472038269043 norm:0.27434736490249634 max memory_allocated 29275.98291015625 
[2025-03-23 01:31:45 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 12 loss:8.308650970458984 norm:0.27181506156921387 max memory_allocated 29275.98291015625 
[2025-03-23 01:32:33 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 13 loss:8.274081230163574 norm:0.2513465881347656 max memory_allocated 29275.98291015625 
[2025-03-23 01:33:21 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 14 loss:8.24796199798584 norm:0.23548433184623718 max memory_allocated 29275.98291015625 
[2025-03-23 01:34:09 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 15 loss:8.234219551086426 norm:0.22074024379253387 max memory_allocated 29275.98291015625 
[2025-03-23 01:34:57 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 16 loss:8.22672176361084 norm:0.22408145666122437 max memory_allocated 29275.98291015625 
[2025-03-23 01:35:45 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 17 loss:8.215703964233398 norm:0.22602587938308716 max memory_allocated 29275.98291015625 
[2025-03-23 01:36:33 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 18 loss:8.215604782104492 norm:0.2329159826040268 max memory_allocated 29275.98291015625 
[2025-03-23 01:37:21 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 19 loss:8.228299140930176 norm:0.24723058938980103 max memory_allocated 29275.98291015625 
[2025-03-23 01:37:35 root] (main_calibration_a.py 369): INFO 38938.78698039055
[2025-03-23 01:37:43 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 01:39:34 root] (main_calibration_a.py 158): INFO wikitext2 : 7.503870964050293
[2025-03-23 01:39:34 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 01:42:26 root] (main_calibration_a.py 158): INFO c4 : 10.585162162780762
[2025-03-23 03:52:45 root] (main_calibration_a.py 169): INFO {'wikitext2': 7.503870964050293, 'c4': 10.585162162780762, 'results': {'arc_challenge': {'acc': 0.2909556313993174, 'acc_stderr': 0.013273077865907583, 'acc_norm': 0.3293515358361775, 'acc_norm_stderr': 0.013734057652635474}, 'piqa': {'acc': 0.6686615886833515, 'acc_stderr': 0.010982077458957351, 'acc_norm': 0.6664853101196954, 'acc_norm_stderr': 0.011000139592184573}, 'arc_easy': {'acc': 0.5231481481481481, 'acc_stderr': 0.010248782484554471, 'acc_norm': 0.42803030303030304, 'acc_norm_stderr': 0.01015294331642626}, 'winogrande': {'acc': 0.5895816890292028, 'acc_stderr': 0.01382510712003587}, 'hellaswag': {'acc': 0.4881497709619598, 'acc_stderr': 0.004988379805261158, 'acc_norm': 0.6405098585939056, 'acc_norm_stderr': 0.004788703173474767}, 'boolq': {'acc': 0.6247706422018349, 'acc_stderr': 0.008468397820914284}}, 'versions': {'arc_challenge': 0, 'piqa': 0, 'arc_easy': 0, 'winogrande': 0, 'hellaswag': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 03:52:45 root] (main_calibration_a.py 172): INFO 29.10,52.31,62.48,48.81,66.87,58.96
