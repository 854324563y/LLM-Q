[2025-03-22 01:15:09 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/Llama-2-13b-hf-w4a4-16', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=16)
[2025-03-22 01:15:11 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 01:15:11 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:15:11 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 01:15:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:15:21 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.16470111906528473 norm:0.20575498044490814 max memory_allocated 29274.27197265625 
[2025-03-22 01:16:56 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.06374560296535492 norm:0.08821453154087067 max memory_allocated 29274.27197265625 
[2025-03-22 01:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.04752111807465553 norm:0.06664551794528961 max memory_allocated 29274.27197265625 
[2025-03-22 01:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.03925766795873642 norm:0.056275125592947006 max memory_allocated 29274.27197265625 
[2025-03-22 01:19:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.03586982190608978 norm:0.05042238160967827 max memory_allocated 29274.27197265625 
[2025-03-22 01:20:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.033565472811460495 norm:0.046254828572273254 max memory_allocated 29274.27197265625 
[2025-03-22 01:20:55 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.031381357461214066 norm:0.041671738028526306 max memory_allocated 29274.27197265625 
[2025-03-22 01:21:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.030016131699085236 norm:0.039526354521512985 max memory_allocated 29274.27197265625 
[2025-03-22 01:22:31 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.02898266538977623 norm:0.03517044708132744 max memory_allocated 29274.27197265625 
[2025-03-22 01:23:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.027941100299358368 norm:0.03186410292983055 max memory_allocated 29274.27197265625 
[2025-03-22 01:24:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.027042826637625694 norm:0.02922259271144867 max memory_allocated 29274.27197265625 
[2025-03-22 01:24:55 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.026180913671851158 norm:0.02602781541645527 max memory_allocated 29274.27197265625 
[2025-03-22 01:25:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.025184299796819687 norm:0.023326443508267403 max memory_allocated 29274.27197265625 
[2025-03-22 01:26:31 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.02459423616528511 norm:0.02087024413049221 max memory_allocated 29274.27197265625 
[2025-03-22 01:27:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.02404898591339588 norm:0.019093196839094162 max memory_allocated 29274.27197265625 
[2025-03-22 01:28:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.0235928725451231 norm:0.01738152652978897 max memory_allocated 29274.27197265625 
[2025-03-22 01:28:55 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.023329628631472588 norm:0.01654004119336605 max memory_allocated 29274.27197265625 
[2025-03-22 01:29:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.02302117832005024 norm:0.014749499037861824 max memory_allocated 29274.27197265625 
[2025-03-22 01:30:31 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.022813986986875534 norm:0.013845705427229404 max memory_allocated 29274.27197265625 
[2025-03-22 01:31:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.022622890770435333 norm:0.012976613827049732 max memory_allocated 29274.27197265625 
[2025-03-22 01:31:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:31:41 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:32:29 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.1864936649799347 norm:0.10910992324352264 max memory_allocated 29274.27197265625 
[2025-03-22 01:33:17 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.1195848286151886 norm:0.0845869705080986 max memory_allocated 29274.27197265625 
[2025-03-22 01:34:04 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.10022243112325668 norm:0.06276731193065643 max memory_allocated 29274.27197265625 
[2025-03-22 01:34:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.09222442656755447 norm:0.051639363169670105 max memory_allocated 29274.27197265625 
[2025-03-22 01:35:40 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.08778620511293411 norm:0.04494451731443405 max memory_allocated 29274.27197265625 
[2025-03-22 01:36:28 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.08470035344362259 norm:0.04027824476361275 max memory_allocated 29274.27197265625 
[2025-03-22 01:37:16 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.08247974514961243 norm:0.03676380589604378 max memory_allocated 29274.27197265625 
[2025-03-22 01:38:04 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.08074775338172913 norm:0.03345470875501633 max memory_allocated 29274.27197265625 
[2025-03-22 01:38:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.07938387244939804 norm:0.029469488188624382 max memory_allocated 29274.27197265625 
[2025-03-22 01:39:40 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.07821597903966904 norm:0.02645522728562355 max memory_allocated 29274.27197265625 
[2025-03-22 01:40:28 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.07722847163677216 norm:0.024338413029909134 max memory_allocated 29274.27197265625 
[2025-03-22 01:41:17 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.07655829191207886 norm:0.021914102137088776 max memory_allocated 29274.27197265625 
[2025-03-22 01:42:05 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.0758659765124321 norm:0.01924726366996765 max memory_allocated 29274.27197265625 
[2025-03-22 01:42:53 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.07533980160951614 norm:0.017054855823516846 max memory_allocated 29274.27197265625 
[2025-03-22 01:43:41 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.07470158487558365 norm:0.01511422824114561 max memory_allocated 29274.27197265625 
[2025-03-22 01:44:29 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07427054643630981 norm:0.01317522581666708 max memory_allocated 29274.27197265625 
[2025-03-22 01:45:17 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.07387242466211319 norm:0.011609758250415325 max memory_allocated 29274.27197265625 
[2025-03-22 01:46:05 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07368726283311844 norm:0.010596263222396374 max memory_allocated 29274.27197265625 
[2025-03-22 01:46:54 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.0734778568148613 norm:0.009693378582596779 max memory_allocated 29274.27197265625 
[2025-03-22 01:47:42 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07317513227462769 norm:0.008761150762438774 max memory_allocated 29274.27197265625 
[2025-03-22 01:47:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:48:02 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:48:50 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.19067199528217316 norm:0.07823484390974045 max memory_allocated 29274.64697265625 
[2025-03-22 01:49:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.14423927664756775 norm:0.05669902637600899 max memory_allocated 29274.64697265625 
[2025-03-22 01:50:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.13132889568805695 norm:0.05047045648097992 max memory_allocated 29274.64697265625 
[2025-03-22 01:51:14 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.12461914867162704 norm:0.042322367429733276 max memory_allocated 29274.64697265625 
[2025-03-22 01:52:02 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.12060178816318512 norm:0.03709597885608673 max memory_allocated 29274.64697265625 
[2025-03-22 01:52:50 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.11788000911474228 norm:0.03299542888998985 max memory_allocated 29274.64697265625 
[2025-03-22 01:53:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.11599204689264297 norm:0.030259234830737114 max memory_allocated 29274.64697265625 
[2025-03-22 01:54:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.11452259123325348 norm:0.027494125068187714 max memory_allocated 29274.64697265625 
[2025-03-22 01:55:14 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.11368928849697113 norm:0.025522807613015175 max memory_allocated 29274.64697265625 
[2025-03-22 01:56:02 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.11286400258541107 norm:0.021966539323329926 max memory_allocated 29274.64697265625 
[2025-03-22 01:56:50 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.1119127869606018 norm:0.0190692488104105 max memory_allocated 29274.64697265625 
[2025-03-22 01:57:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.11148415505886078 norm:0.018006959930062294 max memory_allocated 29274.64697265625 
[2025-03-22 01:58:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.1108342707157135 norm:0.015810508280992508 max memory_allocated 29274.64697265625 
[2025-03-22 01:59:15 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.1103430986404419 norm:0.01335769984871149 max memory_allocated 29274.64697265625 
[2025-03-22 02:00:03 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.10969133675098419 norm:0.010941818356513977 max memory_allocated 29274.64697265625 
[2025-03-22 02:00:51 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.10919113457202911 norm:0.008980900049209595 max memory_allocated 29274.64697265625 
[2025-03-22 02:01:39 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.10870201885700226 norm:0.007488288916647434 max memory_allocated 29274.64697265625 
[2025-03-22 02:02:28 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.10872607678174973 norm:0.007022676523774862 max memory_allocated 29274.64697265625 
[2025-03-22 02:03:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.10888850688934326 norm:0.006641266401857138 max memory_allocated 29274.64697265625 
[2025-03-22 02:04:04 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.10867439210414886 norm:0.006184232421219349 max memory_allocated 29274.64697265625 
[2025-03-22 02:04:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 02:05:11 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.4917089641094208 norm:0.18693184852600098 max memory_allocated 29274.64697265625 
[2025-03-22 02:05:59 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.38371771574020386 norm:0.05200774967670441 max memory_allocated 29274.64697265625 
[2025-03-22 02:06:47 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.3389124274253845 norm:0.02577398344874382 max memory_allocated 29274.64697265625 
[2025-03-22 02:07:35 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.30757784843444824 norm:0.02464732713997364 max memory_allocated 29274.64697265625 
[2025-03-22 02:08:22 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.2916240692138672 norm:0.02397068962454796 max memory_allocated 29274.64697265625 
[2025-03-22 02:09:10 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.27835795283317566 norm:0.02251124009490013 max memory_allocated 29274.64697265625 
[2025-03-22 02:09:58 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.265522837638855 norm:0.018736321479082108 max memory_allocated 29274.64697265625 
[2025-03-22 02:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.2583431899547577 norm:0.018604489043354988 max memory_allocated 29274.64697265625 
[2025-03-22 02:11:34 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.2568880319595337 norm:0.01890832930803299 max memory_allocated 29274.64697265625 
[2025-03-22 02:12:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.25198498368263245 norm:0.016291983425617218 max memory_allocated 29274.64697265625 
[2025-03-22 02:13:09 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.2483855038881302 norm:0.015537753701210022 max memory_allocated 29274.64697265625 
[2025-03-22 02:13:57 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.2476588785648346 norm:0.015739627182483673 max memory_allocated 29274.64697265625 
[2025-03-22 02:14:45 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.24377624690532684 norm:0.015345961786806583 max memory_allocated 29274.64697265625 
[2025-03-22 02:15:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.23644214868545532 norm:0.015169454738497734 max memory_allocated 29274.64697265625 
[2025-03-22 02:16:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.23442292213439941 norm:0.014810245484113693 max memory_allocated 29274.64697265625 
[2025-03-22 02:17:09 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.23203402757644653 norm:0.015140348114073277 max memory_allocated 29274.64697265625 
[2025-03-22 02:17:57 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.2349786013364792 norm:0.017366133630275726 max memory_allocated 29274.64697265625 
[2025-03-22 02:18:45 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.23719698190689087 norm:0.017404353246092796 max memory_allocated 29274.64697265625 
[2025-03-22 02:19:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.23985333740711212 norm:0.016976136714220047 max memory_allocated 29274.64697265625 
[2025-03-22 02:20:22 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.2377767115831375 norm:0.01658073626458645 max memory_allocated 29274.64697265625 
[2025-03-22 02:20:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:21:27 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.27174416184425354 norm:0.010412449948489666 max memory_allocated 29274.64697265625 
[2025-03-22 02:22:15 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.25255292654037476 norm:0.005241987761110067 max memory_allocated 29274.64697265625 
[2025-03-22 02:23:03 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.23947346210479736 norm:0.003416708204895258 max memory_allocated 29274.64697265625 
[2025-03-22 02:23:51 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.2329624891281128 norm:0.002283245325088501 max memory_allocated 29274.64697265625 
[2025-03-22 02:24:39 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.22910574078559875 norm:0.0018108552321791649 max memory_allocated 29274.64697265625 
[2025-03-22 02:25:27 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.22720611095428467 norm:0.0015785458963364363 max memory_allocated 29274.64697265625 
[2025-03-22 02:26:14 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.22639700770378113 norm:0.0015282424865290523 max memory_allocated 29274.64697265625 
[2025-03-22 02:27:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.22562696039676666 norm:0.0014750506961718202 max memory_allocated 29274.64697265625 
[2025-03-22 02:27:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.22455433011054993 norm:0.0013494380982592702 max memory_allocated 29274.64697265625 
[2025-03-22 02:28:38 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.22351869940757751 norm:0.001223600353114307 max memory_allocated 29274.64697265625 
[2025-03-22 02:29:26 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.2228800356388092 norm:0.0012434672098606825 max memory_allocated 29274.64697265625 
[2025-03-22 02:30:14 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.2225053310394287 norm:0.0011485551949590445 max memory_allocated 29274.64697265625 
[2025-03-22 02:31:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.22214320302009583 norm:0.001127260155044496 max memory_allocated 29274.64697265625 
[2025-03-22 02:31:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.22178158164024353 norm:0.0010924118105322123 max memory_allocated 29274.64697265625 
[2025-03-22 02:32:38 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.2213539332151413 norm:0.0010578050278127193 max memory_allocated 29274.64697265625 
[2025-03-22 02:33:26 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.22120401263237 norm:0.001057168236002326 max memory_allocated 29274.64697265625 
[2025-03-22 02:34:14 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.2210080474615097 norm:0.001058475929312408 max memory_allocated 29274.64697265625 
[2025-03-22 02:35:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.22094613313674927 norm:0.0010605606948956847 max memory_allocated 29274.64697265625 
[2025-03-22 02:35:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.22098521888256073 norm:0.001071438891813159 max memory_allocated 29274.64697265625 
[2025-03-22 02:36:39 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.22100858390331268 norm:0.0010616147192195058 max memory_allocated 29274.64697265625 
[2025-03-22 02:36:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:37:44 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.30173608660697937 norm:0.013405834324657917 max memory_allocated 29274.64697265625 
[2025-03-22 02:38:32 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.27859610319137573 norm:0.00565949734300375 max memory_allocated 29274.64697265625 
[2025-03-22 02:39:20 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.26293739676475525 norm:0.0035028685815632343 max memory_allocated 29274.64697265625 
[2025-03-22 02:40:08 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.2564363181591034 norm:0.0023921430110931396 max memory_allocated 29274.64697265625 
[2025-03-22 02:40:56 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.25333037972450256 norm:0.0019220178946852684 max memory_allocated 29274.64697265625 
[2025-03-22 02:41:44 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.2510453462600708 norm:0.0016762127634137869 max memory_allocated 29274.64697265625 
[2025-03-22 02:42:32 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.24939802289009094 norm:0.001517968950793147 max memory_allocated 29274.64697265625 
[2025-03-22 02:43:20 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.2482399046421051 norm:0.0013596650678664446 max memory_allocated 29274.64697265625 
[2025-03-22 02:44:07 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.24734827876091003 norm:0.0013473550789058208 max memory_allocated 29274.64697265625 
[2025-03-22 02:44:55 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.24646344780921936 norm:0.0012046603951603174 max memory_allocated 29274.64697265625 
[2025-03-22 02:45:43 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.24616411328315735 norm:0.0012240473879501224 max memory_allocated 29274.64697265625 
[2025-03-22 02:46:31 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.24570904672145844 norm:0.0011757750762626529 max memory_allocated 29274.64697265625 
[2025-03-22 02:47:19 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.24547302722930908 norm:0.0011532537173479795 max memory_allocated 29274.64697265625 
[2025-03-22 02:48:07 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.24525956809520721 norm:0.001104529481381178 max memory_allocated 29274.64697265625 
[2025-03-22 02:48:55 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.2452375292778015 norm:0.0011973899090662599 max memory_allocated 29274.64697265625 
[2025-03-22 02:49:43 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.2448311746120453 norm:0.001031932421028614 max memory_allocated 29274.64697265625 
[2025-03-22 02:50:31 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.2445501685142517 norm:0.00105279718991369 max memory_allocated 29274.64697265625 
[2025-03-22 02:51:19 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.24442802369594574 norm:0.0010723598534241319 max memory_allocated 29274.64697265625 
[2025-03-22 02:52:07 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.2441617250442505 norm:0.0009811996715143323 max memory_allocated 29274.64697265625 
[2025-03-22 02:52:56 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.24403046071529388 norm:0.0009882226586341858 max memory_allocated 29274.64697265625 
[2025-03-22 02:53:09 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:54:01 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.33953455090522766 norm:0.024177711457014084 max memory_allocated 29274.64697265625 
[2025-03-22 02:54:49 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.31082409620285034 norm:0.012029917910695076 max memory_allocated 29274.64697265625 
[2025-03-22 02:55:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.2853492498397827 norm:0.0035991098266094923 max memory_allocated 29274.64697265625 
[2025-03-22 02:56:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.27774834632873535 norm:0.0017726212972775102 max memory_allocated 29274.64697265625 
[2025-03-22 02:57:13 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.27394309639930725 norm:0.0014982637949287891 max memory_allocated 29274.64697265625 
[2025-03-22 02:58:01 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.2712850868701935 norm:0.0013925249222666025 max memory_allocated 29274.64697265625 
[2025-03-22 02:58:49 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.2694520354270935 norm:0.001302778022363782 max memory_allocated 29274.64697265625 
[2025-03-22 02:59:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.267932265996933 norm:0.0012244215467944741 max memory_allocated 29274.64697265625 
[2025-03-22 03:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.2666676342487335 norm:0.0011735574807971716 max memory_allocated 29274.64697265625 
[2025-03-22 03:01:13 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.2659470736980438 norm:0.0011287081288173795 max memory_allocated 29274.64697265625 
[2025-03-22 03:02:00 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.26537296175956726 norm:0.0011004767147824168 max memory_allocated 29274.64697265625 
[2025-03-22 03:02:48 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.2649073004722595 norm:0.0010820570169016719 max memory_allocated 29274.64697265625 
[2025-03-22 03:03:36 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.26453015208244324 norm:0.001070401631295681 max memory_allocated 29274.64697265625 
[2025-03-22 03:04:24 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.26419728994369507 norm:0.0010716886026784778 max memory_allocated 29274.64697265625 
[2025-03-22 03:05:12 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.264011412858963 norm:0.0010487252147868276 max memory_allocated 29274.64697265625 
[2025-03-22 03:06:01 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.2638198733329773 norm:0.0010512664448469877 max memory_allocated 29274.64697265625 
[2025-03-22 03:06:49 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.2637205719947815 norm:0.0010489880805835128 max memory_allocated 29274.64697265625 
[2025-03-22 03:07:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.2636202573776245 norm:0.001042426098138094 max memory_allocated 29274.64697265625 
[2025-03-22 03:08:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.26351261138916016 norm:0.0010216633090749383 max memory_allocated 29274.64697265625 
[2025-03-22 03:09:13 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.2634599506855011 norm:0.001016777940094471 max memory_allocated 29274.64697265625 
[2025-03-22 03:09:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 03:10:18 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.3920489549636841 norm:0.01825578883290291 max memory_allocated 29274.64697265625 
[2025-03-22 03:11:06 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.35005295276641846 norm:0.007424851879477501 max memory_allocated 29274.64697265625 
[2025-03-22 03:11:54 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.31711345911026 norm:0.0029133085627108812 max memory_allocated 29274.64697265625 
[2025-03-22 03:12:42 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.3072760999202728 norm:0.001953218597918749 max memory_allocated 29274.64697265625 
[2025-03-22 03:13:30 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.3029753267765045 norm:0.0016203244449570775 max memory_allocated 29274.64697265625 
[2025-03-22 03:14:18 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.2995901107788086 norm:0.0014606757322326303 max memory_allocated 29274.64697265625 
[2025-03-22 03:15:06 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.29736778140068054 norm:0.001412870711646974 max memory_allocated 29274.64697265625 
[2025-03-22 03:15:54 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.2958109974861145 norm:0.0013879085890948772 max memory_allocated 29274.64697265625 
[2025-03-22 03:16:42 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.2944658696651459 norm:0.001385966781526804 max memory_allocated 29274.64697265625 
[2025-03-22 03:17:29 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.29351842403411865 norm:0.0013201445108279586 max memory_allocated 29274.64697265625 
[2025-03-22 03:18:17 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.2931261658668518 norm:0.0012942948378622532 max memory_allocated 29274.64697265625 
[2025-03-22 03:19:05 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.29262545704841614 norm:0.0012741198297590017 max memory_allocated 29274.64697265625 
[2025-03-22 03:19:53 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.29247087240219116 norm:0.0012469952926039696 max memory_allocated 29274.64697265625 
[2025-03-22 03:20:41 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2922455072402954 norm:0.0012374158250167966 max memory_allocated 29274.64697265625 
[2025-03-22 03:21:30 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.2921030521392822 norm:0.0012193468865007162 max memory_allocated 29274.64697265625 
[2025-03-22 03:22:18 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.29187044501304626 norm:0.0011894000926986337 max memory_allocated 29274.64697265625 
[2025-03-22 03:23:06 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.2917041778564453 norm:0.0011736787855625153 max memory_allocated 29274.64697265625 
[2025-03-22 03:23:54 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.2915268540382385 norm:0.0011661989847198129 max memory_allocated 29274.64697265625 
[2025-03-22 03:24:42 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.2913861572742462 norm:0.0011636224808171391 max memory_allocated 29274.64697265625 
[2025-03-22 03:25:30 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.2911333441734314 norm:0.0011389806168153882 max memory_allocated 29274.64697265625 
[2025-03-22 03:25:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 03:26:36 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.41861605644226074 norm:0.033539097756147385 max memory_allocated 29274.64697265625 
[2025-03-22 03:27:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.3873804211616516 norm:0.02161998115479946 max memory_allocated 29274.64697265625 
[2025-03-22 03:28:12 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.3491077423095703 norm:0.013325624167919159 max memory_allocated 29274.64697265625 
[2025-03-22 03:29:00 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.33439815044403076 norm:0.008649565279483795 max memory_allocated 29274.64697265625 
[2025-03-22 03:29:48 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.3260856568813324 norm:0.004140208940953016 max memory_allocated 29274.64697265625 
[2025-03-22 03:30:36 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.3206123411655426 norm:0.0017692290712147951 max memory_allocated 29274.64697265625 
[2025-03-22 03:31:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.3174010217189789 norm:0.001567343482747674 max memory_allocated 29274.64697265625 
[2025-03-22 03:32:12 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.3152497708797455 norm:0.0014776380266994238 max memory_allocated 29274.64697265625 
[2025-03-22 03:33:00 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.31383687257766724 norm:0.0014111204072833061 max memory_allocated 29274.64697265625 
[2025-03-22 03:33:47 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.3127231001853943 norm:0.0013445933582261205 max memory_allocated 29274.64697265625 
[2025-03-22 03:34:35 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.3117353022098541 norm:0.0013006498338654637 max memory_allocated 29274.64697265625 
[2025-03-22 03:35:23 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.31095293164253235 norm:0.0012304551200941205 max memory_allocated 29274.64697265625 
[2025-03-22 03:36:11 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.31035226583480835 norm:0.0011784968664869666 max memory_allocated 29274.64697265625 
[2025-03-22 03:36:59 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.30996429920196533 norm:0.0011600892757996917 max memory_allocated 29274.64697265625 
[2025-03-22 03:37:47 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.30970993638038635 norm:0.001140177482739091 max memory_allocated 29274.64697265625 
[2025-03-22 03:38:35 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.30947789549827576 norm:0.0011022042017430067 max memory_allocated 29274.64697265625 
[2025-03-22 03:39:23 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.3092597723007202 norm:0.001083696959540248 max memory_allocated 29274.64697265625 
[2025-03-22 03:40:11 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.3090640604496002 norm:0.0010783928446471691 max memory_allocated 29274.64697265625 
[2025-03-22 03:40:59 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.30893474817276 norm:0.0010693180374801159 max memory_allocated 29274.64697265625 
[2025-03-22 03:41:47 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.3087419867515564 norm:0.0010523101082071662 max memory_allocated 29274.64697265625 
[2025-03-22 03:42:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 03:42:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.49237629771232605 norm:0.041814208030700684 max memory_allocated 29274.64697265625 
[2025-03-22 03:43:41 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.437893807888031 norm:0.020237982273101807 max memory_allocated 29274.64697265625 
[2025-03-22 03:44:29 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.378534197807312 norm:0.007964752614498138 max memory_allocated 29274.64697265625 
[2025-03-22 03:45:17 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.35599789023399353 norm:0.004015085753053427 max memory_allocated 29274.64697265625 
[2025-03-22 03:46:05 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.34822532534599304 norm:0.0029102719854563475 max memory_allocated 29274.64697265625 
[2025-03-22 03:46:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3436298370361328 norm:0.002495227614417672 max memory_allocated 29274.64697265625 
[2025-03-22 03:47:41 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.34040796756744385 norm:0.0021298336796462536 max memory_allocated 29274.64697265625 
[2025-03-22 03:48:29 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.33824723958969116 norm:0.001975346589460969 max memory_allocated 29274.64697265625 
[2025-03-22 03:49:17 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.33635225892066956 norm:0.0018139679450541735 max memory_allocated 29274.64697265625 
[2025-03-22 03:50:05 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3347550928592682 norm:0.0016503429505974054 max memory_allocated 29274.64697265625 
[2025-03-22 03:50:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.33372175693511963 norm:0.0015619631158187985 max memory_allocated 29274.64697265625 
[2025-03-22 03:51:40 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.33270978927612305 norm:0.0014335284940898418 max memory_allocated 29274.64697265625 
[2025-03-22 03:52:28 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.33184072375297546 norm:0.0013469925615936518 max memory_allocated 29274.64697265625 
[2025-03-22 03:53:16 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.3313044607639313 norm:0.0012870701029896736 max memory_allocated 29274.64697265625 
[2025-03-22 03:54:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.330981969833374 norm:0.001243620179593563 max memory_allocated 29274.64697265625 
[2025-03-22 03:54:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.3305896520614624 norm:0.0012004057643935084 max memory_allocated 29274.64697265625 
[2025-03-22 03:55:40 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.3302398920059204 norm:0.0011885503772646189 max memory_allocated 29274.64697265625 
[2025-03-22 03:56:28 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.32995498180389404 norm:0.001167661976069212 max memory_allocated 29274.64697265625 
[2025-03-22 03:57:16 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.3296710252761841 norm:0.0011201470624655485 max memory_allocated 29274.64697265625 
[2025-03-22 03:58:05 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.3293669819831848 norm:0.0010877905879169703 max memory_allocated 29274.64697265625 
[2025-03-22 03:58:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:59:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.4686891436576843 norm:0.02746923454105854 max memory_allocated 29274.64697265625 
[2025-03-22 03:59:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.4227724075317383 norm:0.0119758490473032 max memory_allocated 29274.64697265625 
[2025-03-22 04:00:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.3825470209121704 norm:0.004621066153049469 max memory_allocated 29274.64697265625 
[2025-03-22 04:01:34 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3671610653400421 norm:0.002488617319613695 max memory_allocated 29274.64697265625 
[2025-03-22 04:02:22 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.36124539375305176 norm:0.0020153687801212072 max memory_allocated 29274.64697265625 
[2025-03-22 04:03:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.35751765966415405 norm:0.001733327517285943 max memory_allocated 29274.64697265625 
[2025-03-22 04:03:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3548602759838104 norm:0.0015188073739409447 max memory_allocated 29274.64697265625 
[2025-03-22 04:04:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.35299766063690186 norm:0.0014019118389114738 max memory_allocated 29274.64697265625 
[2025-03-22 04:05:34 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.351548969745636 norm:0.0012993153650313616 max memory_allocated 29274.64697265625 
[2025-03-22 04:06:22 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.35043102502822876 norm:0.0012066472554579377 max memory_allocated 29274.64697265625 
[2025-03-22 04:07:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.3496722877025604 norm:0.001138398190960288 max memory_allocated 29274.64697265625 
[2025-03-22 04:07:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.349042147397995 norm:0.0010970879811793566 max memory_allocated 29274.64697265625 
[2025-03-22 04:08:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.3485957384109497 norm:0.0010933534940704703 max memory_allocated 29274.64697265625 
[2025-03-22 04:09:34 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3479602336883545 norm:0.0010502206860110164 max memory_allocated 29274.64697265625 
[2025-03-22 04:10:22 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.34755173325538635 norm:0.001001010532490909 max memory_allocated 29274.64697265625 
[2025-03-22 04:11:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.34731632471084595 norm:0.0009934111731126904 max memory_allocated 29274.64697265625 
[2025-03-22 04:11:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.3470704257488251 norm:0.0009847756009548903 max memory_allocated 29274.64697265625 
[2025-03-22 04:12:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3468729853630066 norm:0.0009419625857844949 max memory_allocated 29274.64697265625 
[2025-03-22 04:13:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.3466554880142212 norm:0.0009254002361558378 max memory_allocated 29274.64697265625 
[2025-03-22 04:14:23 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.3464585840702057 norm:0.000917108089197427 max memory_allocated 29274.64697265625 
[2025-03-22 04:14:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 04:15:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.476546049118042 norm:0.03311377018690109 max memory_allocated 29274.64697265625 
[2025-03-22 04:16:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.43751955032348633 norm:0.013596785254776478 max memory_allocated 29274.64697265625 
[2025-03-22 04:17:04 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3965921998023987 norm:0.004867791663855314 max memory_allocated 29274.64697265625 
[2025-03-22 04:17:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.3823971748352051 norm:0.0024194829165935516 max memory_allocated 29274.64697265625 
[2025-03-22 04:18:41 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.3765827417373657 norm:0.00144939124584198 max memory_allocated 29274.64697265625 
[2025-03-22 04:19:29 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.37299373745918274 norm:0.0012989600654691458 max memory_allocated 29274.64697265625 
[2025-03-22 04:20:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.3702940344810486 norm:0.0011769410921260715 max memory_allocated 29274.64697265625 
[2025-03-22 04:21:04 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3682279884815216 norm:0.0010843579657375813 max memory_allocated 29274.64697265625 
[2025-03-22 04:21:52 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.366850882768631 norm:0.0010254316730424762 max memory_allocated 29274.64697265625 
[2025-03-22 04:22:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.36584949493408203 norm:0.0009777951054275036 max memory_allocated 29274.64697265625 
[2025-03-22 04:23:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.3650386333465576 norm:0.0009286071872338653 max memory_allocated 29274.64697265625 
[2025-03-22 04:24:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.36449429392814636 norm:0.0008931203628890216 max memory_allocated 29274.64697265625 
[2025-03-22 04:25:04 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3640070855617523 norm:0.0008658992010168731 max memory_allocated 29274.64697265625 
[2025-03-22 04:25:52 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.3635893762111664 norm:0.0008359123603440821 max memory_allocated 29274.64697265625 
[2025-03-22 04:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3633764982223511 norm:0.0008313430589623749 max memory_allocated 29274.64697265625 
[2025-03-22 04:27:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.36320236325263977 norm:0.0008228773367591202 max memory_allocated 29274.64697265625 
[2025-03-22 04:28:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.36301472783088684 norm:0.0008178025018423796 max memory_allocated 29274.64697265625 
[2025-03-22 04:29:05 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.36277323961257935 norm:0.0008136896649375558 max memory_allocated 29274.64697265625 
[2025-03-22 04:29:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.36253586411476135 norm:0.000834947160910815 max memory_allocated 29274.64697265625 
[2025-03-22 04:30:41 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.36231786012649536 norm:0.0008269856916740537 max memory_allocated 29274.64697265625 
[2025-03-22 04:30:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 04:31:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.49595457315444946 norm:0.02111067809164524 max memory_allocated 29274.64697265625 
[2025-03-22 04:32:34 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.4465269148349762 norm:0.009336567483842373 max memory_allocated 29274.64697265625 
[2025-03-22 04:33:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.4040411710739136 norm:0.003638874739408493 max memory_allocated 29274.64697265625 
[2025-03-22 04:34:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.38983646035194397 norm:0.002112586982548237 max memory_allocated 29274.64697265625 
[2025-03-22 04:34:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.3843734562397003 norm:0.0016533774323761463 max memory_allocated 29274.64697265625 
[2025-03-22 04:35:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.38089755177497864 norm:0.0013977509224787354 max memory_allocated 29274.64697265625 
[2025-03-22 04:36:34 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.378478467464447 norm:0.0012837561080232263 max memory_allocated 29274.64697265625 
[2025-03-22 04:37:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.37651142477989197 norm:0.0012042252346873283 max memory_allocated 29274.64697265625 
[2025-03-22 04:38:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.3750958740711212 norm:0.0011196816340088844 max memory_allocated 29274.64697265625 
[2025-03-22 04:38:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.37413501739501953 norm:0.00107143959030509 max memory_allocated 29274.64697265625 
[2025-03-22 04:39:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.3733192980289459 norm:0.000956670381128788 max memory_allocated 29274.64697265625 
[2025-03-22 04:40:34 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.3726332187652588 norm:0.0009138699970208108 max memory_allocated 29274.64697265625 
[2025-03-22 04:41:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.372175931930542 norm:0.0008980723796412349 max memory_allocated 29274.64697265625 
[2025-03-22 04:42:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.37180086970329285 norm:0.0008893259218893945 max memory_allocated 29274.64697265625 
[2025-03-22 04:42:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.3714914321899414 norm:0.0008736888412386179 max memory_allocated 29274.64697265625 
[2025-03-22 04:43:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.3711441457271576 norm:0.0008578443084843457 max memory_allocated 29274.64697265625 
[2025-03-22 04:44:35 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.37084951996803284 norm:0.0008528424659743905 max memory_allocated 29274.64697265625 
[2025-03-22 04:45:23 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.37066853046417236 norm:0.0008509535691700876 max memory_allocated 29274.64697265625 
[2025-03-22 04:46:11 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.3704717755317688 norm:0.0008258646121248603 max memory_allocated 29274.64697265625 
[2025-03-22 04:46:59 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.37026509642601013 norm:0.0008146198815666139 max memory_allocated 29274.64697265625 
[2025-03-22 04:47:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 04:48:04 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.49279943108558655 norm:0.02636943757534027 max memory_allocated 29274.64697265625 
[2025-03-22 04:48:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.4498523473739624 norm:0.011855412274599075 max memory_allocated 29274.64697265625 
[2025-03-22 04:49:40 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.4135083258152008 norm:0.005430017132312059 max memory_allocated 29274.64697265625 
[2025-03-22 04:50:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.39904695749282837 norm:0.0022906134836375713 max memory_allocated 29274.64697265625 
[2025-03-22 04:51:16 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.3933573067188263 norm:0.0015572470147162676 max memory_allocated 29274.64697265625 
[2025-03-22 04:52:04 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.38990601897239685 norm:0.0013446336379274726 max memory_allocated 29274.64697265625 
[2025-03-22 04:52:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3873330354690552 norm:0.0012091576354578137 max memory_allocated 29274.64697265625 
[2025-03-22 04:53:40 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.3855447769165039 norm:0.0011334004811942577 max memory_allocated 29274.64697265625 
[2025-03-22 04:54:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.38408777117729187 norm:0.0010632029734551907 max memory_allocated 29274.64697265625 
[2025-03-22 04:55:16 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.3830134868621826 norm:0.0010139024816453457 max memory_allocated 29274.64697265625 
[2025-03-22 04:56:04 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3821655213832855 norm:0.000969180662650615 max memory_allocated 29274.64697265625 
[2025-03-22 04:56:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.3814937472343445 norm:0.0009540835744701326 max memory_allocated 29274.64697265625 
[2025-03-22 04:57:40 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.3809725046157837 norm:0.0009251087321899831 max memory_allocated 29274.64697265625 
[2025-03-22 04:58:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3805799186229706 norm:0.0009082723408937454 max memory_allocated 29274.64697265625 
[2025-03-22 04:59:16 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.38032880425453186 norm:0.0008905126014724374 max memory_allocated 29274.64697265625 
[2025-03-22 05:00:04 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3802448511123657 norm:0.0008898747037164867 max memory_allocated 29274.64697265625 
[2025-03-22 05:00:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.37999171018600464 norm:0.000864118046592921 max memory_allocated 29274.64697265625 
[2025-03-22 05:01:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.37976306676864624 norm:0.0008401349768973887 max memory_allocated 29274.64697265625 
[2025-03-22 05:02:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.3795081377029419 norm:0.0008224478224292397 max memory_allocated 29274.64697265625 
[2025-03-22 05:03:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.37927350401878357 norm:0.0008206781931221485 max memory_allocated 29274.64697265625 
[2025-03-22 05:03:31 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 05:04:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.49137696623802185 norm:0.020660167559981346 max memory_allocated 29274.64697265625 
[2025-03-22 05:05:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.4546484053134918 norm:0.010495088994503021 max memory_allocated 29274.64697265625 
[2025-03-22 05:05:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.42376989126205444 norm:0.005616756621748209 max memory_allocated 29274.64697265625 
[2025-03-22 05:06:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.41127637028694153 norm:0.0036288935225456953 max memory_allocated 29274.64697265625 
[2025-03-22 05:07:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.40560382604599 norm:0.002594932448118925 max memory_allocated 29274.64697265625 
[2025-03-22 05:08:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.40209317207336426 norm:0.00205313041806221 max memory_allocated 29274.64697265625 
[2025-03-22 05:09:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.39946725964546204 norm:0.0017028057482093573 max memory_allocated 29274.64697265625 
[2025-03-22 05:09:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.397340327501297 norm:0.0013549579307436943 max memory_allocated 29274.64697265625 
[2025-03-22 05:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3958686590194702 norm:0.0011623312020674348 max memory_allocated 29274.64697265625 
[2025-03-22 05:11:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.39488685131073 norm:0.0010728391353040934 max memory_allocated 29274.64697265625 
[2025-03-22 05:12:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.39408087730407715 norm:0.0010492344154044986 max memory_allocated 29274.64697265625 
[2025-03-22 05:13:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.39353349804878235 norm:0.0010095771867781878 max memory_allocated 29274.64697265625 
[2025-03-22 05:13:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.3930012881755829 norm:0.0009437929838895798 max memory_allocated 29274.64697265625 
[2025-03-22 05:14:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.3925386965274811 norm:0.0008943456341512501 max memory_allocated 29274.64697265625 
[2025-03-22 05:15:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.3922181725502014 norm:0.0008603582973591983 max memory_allocated 29274.64697265625 
[2025-03-22 05:16:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.3919280469417572 norm:0.0008320281631313264 max memory_allocated 29274.64697265625 
[2025-03-22 05:17:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.39171102643013 norm:0.0008274796418845654 max memory_allocated 29274.64697265625 
[2025-03-22 05:17:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.39150166511535645 norm:0.0008121744031086564 max memory_allocated 29274.64697265625 
[2025-03-22 05:18:47 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.3913421332836151 norm:0.0008074716897681355 max memory_allocated 29274.64697265625 
[2025-03-22 05:19:35 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.3911602795124054 norm:0.0008041485561989248 max memory_allocated 29274.64697265625 
[2025-03-22 05:19:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 05:20:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.4847245514392853 norm:0.016193099319934845 max memory_allocated 29274.64697265625 
[2025-03-22 05:21:28 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.4571588933467865 norm:0.009204667061567307 max memory_allocated 29274.64697265625 
[2025-03-22 05:22:16 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.42878448963165283 norm:0.00483955442905426 max memory_allocated 29274.64697265625 
[2025-03-22 05:23:04 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.41407737135887146 norm:0.002448013285174966 max memory_allocated 29274.64697265625 
[2025-03-22 05:23:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.40829867124557495 norm:0.0016506824176758528 max memory_allocated 29274.64697265625 
[2025-03-22 05:24:39 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.4049839377403259 norm:0.0012833287473767996 max memory_allocated 29274.64697265625 
[2025-03-22 05:25:27 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.402643620967865 norm:0.0011489642783999443 max memory_allocated 29274.64697265625 
[2025-03-22 05:26:15 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.4009285867214203 norm:0.0010786197381094098 max memory_allocated 29274.64697265625 
[2025-03-22 05:27:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.39952602982521057 norm:0.0009922613389790058 max memory_allocated 29274.64697265625 
[2025-03-22 05:27:51 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.39831751585006714 norm:0.000912425690330565 max memory_allocated 29274.64697265625 
[2025-03-22 05:28:39 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.39754822850227356 norm:0.0008841961971484125 max memory_allocated 29274.64697265625 
[2025-03-22 05:29:27 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.3970192074775696 norm:0.0008585099712945521 max memory_allocated 29274.64697265625 
[2025-03-22 05:30:16 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.3964952230453491 norm:0.0008475743234157562 max memory_allocated 29274.64697265625 
[2025-03-22 05:31:04 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.3960138261318207 norm:0.0008405779371969402 max memory_allocated 29274.64697265625 
[2025-03-22 05:31:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.3955044746398926 norm:0.000817186082713306 max memory_allocated 29274.64697265625 
[2025-03-22 05:32:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.39531171321868896 norm:0.0008135693497024477 max memory_allocated 29274.64697265625 
[2025-03-22 05:33:28 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.39510348439216614 norm:0.0007987806457094848 max memory_allocated 29274.64697265625 
[2025-03-22 05:34:16 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.39488959312438965 norm:0.0007953835884109139 max memory_allocated 29274.64697265625 
[2025-03-22 05:35:04 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.3947068452835083 norm:0.0007888152613304555 max memory_allocated 29274.64697265625 
[2025-03-22 05:35:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.3945935070514679 norm:0.0007797058206051588 max memory_allocated 29274.64697265625 
[2025-03-22 05:36:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 05:36:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.5099323987960815 norm:0.03984493762254715 max memory_allocated 29274.64697265625 
[2025-03-22 05:37:45 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.47893285751342773 norm:0.019686490297317505 max memory_allocated 29274.64697265625 
[2025-03-22 05:38:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.4452507197856903 norm:0.008790213614702225 max memory_allocated 29274.64697265625 
[2025-03-22 05:39:21 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.43001633882522583 norm:0.004953411407768726 max memory_allocated 29274.64697265625 
[2025-03-22 05:40:09 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.4240853786468506 norm:0.0032855242025107145 max memory_allocated 29274.64697265625 
[2025-03-22 05:40:57 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.42052048444747925 norm:0.00260357279330492 max memory_allocated 29274.64697265625 
[2025-03-22 05:41:45 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.41764411330223083 norm:0.0021987478248775005 max memory_allocated 29274.64697265625 
[2025-03-22 05:42:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.41534754633903503 norm:0.0017150241183117032 max memory_allocated 29274.64697265625 
[2025-03-22 05:43:21 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.41358014941215515 norm:0.0015079258009791374 max memory_allocated 29274.64697265625 
[2025-03-22 05:44:09 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.41231703758239746 norm:0.0014386782422661781 max memory_allocated 29274.64697265625 
[2025-03-22 05:44:57 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.41136279702186584 norm:0.0013699070550501347 max memory_allocated 29274.64697265625 
[2025-03-22 05:45:45 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.41066473722457886 norm:0.0013004860375076532 max memory_allocated 29274.64697265625 
[2025-03-22 05:46:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.41002869606018066 norm:0.0012501375749707222 max memory_allocated 29274.64697265625 
[2025-03-22 05:47:21 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.4094439148902893 norm:0.0012113973498344421 max memory_allocated 29274.64697265625 
[2025-03-22 05:48:09 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.40894457697868347 norm:0.0011712827254086733 max memory_allocated 29274.64697265625 
[2025-03-22 05:48:57 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.40862900018692017 norm:0.0011301969643682241 max memory_allocated 29274.64697265625 
[2025-03-22 05:49:46 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.40830737352371216 norm:0.0010930021526291966 max memory_allocated 29274.64697265625 
[2025-03-22 05:50:34 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.4080982506275177 norm:0.0010676321107894182 max memory_allocated 29274.64697265625 
[2025-03-22 05:51:22 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.40781041979789734 norm:0.0010276157408952713 max memory_allocated 29274.64697265625 
[2025-03-22 05:52:10 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.4076458215713501 norm:0.0010112473974004388 max memory_allocated 29274.64697265625 
[2025-03-22 05:52:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 05:53:15 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.511500895023346 norm:0.0411490760743618 max memory_allocated 29275.14697265625 
[2025-03-22 05:54:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.48325008153915405 norm:0.021440010517835617 max memory_allocated 29275.14697265625 
[2025-03-22 05:54:51 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.4505375325679779 norm:0.008021804504096508 max memory_allocated 29275.14697265625 
[2025-03-22 05:55:39 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.4369542896747589 norm:0.003745428519323468 max memory_allocated 29275.14697265625 
[2025-03-22 05:56:26 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.4320039749145508 norm:0.0027204244397580624 max memory_allocated 29275.14697265625 
[2025-03-22 05:57:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.42898881435394287 norm:0.0023227627389132977 max memory_allocated 29275.14697265625 
[2025-03-22 05:58:02 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.4265756905078888 norm:0.0020209667272865772 max memory_allocated 29275.14697265625 
[2025-03-22 05:58:50 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.4248444139957428 norm:0.0018678870983421803 max memory_allocated 29275.14697265625 
[2025-03-22 05:59:38 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.4233032464981079 norm:0.0016678868560120463 max memory_allocated 29275.14697265625 
[2025-03-22 06:00:26 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.4218898117542267 norm:0.0014489958994090557 max memory_allocated 29275.14697265625 
[2025-03-22 06:01:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.42086586356163025 norm:0.001387045718729496 max memory_allocated 29275.14697265625 
[2025-03-22 06:02:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.4199824929237366 norm:0.001267358660697937 max memory_allocated 29275.14697265625 
[2025-03-22 06:02:51 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.4193645417690277 norm:0.0012597255408763885 max memory_allocated 29275.14697265625 
[2025-03-22 06:03:39 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.4186508059501648 norm:0.0011758606415241957 max memory_allocated 29275.14697265625 
[2025-03-22 06:04:27 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.41805699467658997 norm:0.001129864831455052 max memory_allocated 29275.14697265625 
[2025-03-22 06:05:15 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.41763657331466675 norm:0.001119866268709302 max memory_allocated 29275.14697265625 
[2025-03-22 06:06:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.4172310531139374 norm:0.0010636156657710671 max memory_allocated 29275.14697265625 
[2025-03-22 06:06:51 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.4170284867286682 norm:0.0010697967372834682 max memory_allocated 29275.14697265625 
[2025-03-22 06:07:39 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.41679129004478455 norm:0.0010439512552693486 max memory_allocated 29275.14697265625 
[2025-03-22 06:08:27 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.4165356755256653 norm:0.0010236268863081932 max memory_allocated 29275.14697265625 
[2025-03-22 06:08:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 06:09:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.5120950937271118 norm:0.041750356554985046 max memory_allocated 29275.33447265625 
[2025-03-22 06:10:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.4846174120903015 norm:0.020634764805436134 max memory_allocated 29275.33447265625 
[2025-03-22 06:11:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.45937708020210266 norm:0.009880478493869305 max memory_allocated 29275.33447265625 
[2025-03-22 06:11:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.4468122720718384 norm:0.004689543973654509 max memory_allocated 29275.33447265625 
[2025-03-22 06:12:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.4413527250289917 norm:0.00290537322871387 max memory_allocated 29275.33447265625 
[2025-03-22 06:13:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.4383958578109741 norm:0.0022524180822074413 max memory_allocated 29275.33447265625 
[2025-03-22 06:14:19 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.43654000759124756 norm:0.0020163776353001595 max memory_allocated 29275.33447265625 
[2025-03-22 06:15:07 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.43525785207748413 norm:0.0019188313744962215 max memory_allocated 29275.33447265625 
[2025-03-22 06:15:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.43401533365249634 norm:0.0017802028451114893 max memory_allocated 29275.33447265625 
[2025-03-22 06:16:44 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.4329223930835724 norm:0.0016206956934183836 max memory_allocated 29275.33447265625 
[2025-03-22 06:17:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.4319704473018646 norm:0.0014935851795598865 max memory_allocated 29275.33447265625 
[2025-03-22 06:18:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.4310016930103302 norm:0.0013876036973670125 max memory_allocated 29275.33447265625 
[2025-03-22 06:19:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.4302809238433838 norm:0.001300518517382443 max memory_allocated 29275.33447265625 
[2025-03-22 06:19:56 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.4295002818107605 norm:0.0011602460872381926 max memory_allocated 29275.33447265625 
[2025-03-22 06:20:44 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.42885804176330566 norm:0.001081677619367838 max memory_allocated 29275.33447265625 
[2025-03-22 06:21:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.42842328548431396 norm:0.001074351603165269 max memory_allocated 29275.33447265625 
[2025-03-22 06:22:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.42802315950393677 norm:0.001039600232616067 max memory_allocated 29275.33447265625 
[2025-03-22 06:23:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.42771387100219727 norm:0.0010246390011161566 max memory_allocated 29275.33447265625 
[2025-03-22 06:23:56 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.4275181293487549 norm:0.001009313273243606 max memory_allocated 29275.33447265625 
[2025-03-22 06:24:44 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.42735493183135986 norm:0.0010210559703409672 max memory_allocated 29275.33447265625 
[2025-03-22 06:24:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 06:25:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.5197596549987793 norm:0.028835266828536987 max memory_allocated 29275.52197265625 
[2025-03-22 06:26:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.4980868101119995 norm:0.014922295697033405 max memory_allocated 29275.52197265625 
[2025-03-22 06:27:25 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.4750921428203583 norm:0.006936510093510151 max memory_allocated 29275.52197265625 
[2025-03-22 06:28:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.4636809527873993 norm:0.003418516367673874 max memory_allocated 29275.52197265625 
[2025-03-22 06:29:01 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.459489643573761 norm:0.002454581903293729 max memory_allocated 29275.52197265625 
[2025-03-22 06:29:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.45689475536346436 norm:0.00203468999825418 max memory_allocated 29275.52197265625 
[2025-03-22 06:30:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.45499706268310547 norm:0.0017854912439361215 max memory_allocated 29275.52197265625 
[2025-03-22 06:31:25 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.4534948468208313 norm:0.001618879148736596 max memory_allocated 29275.52197265625 
[2025-03-22 06:32:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.45229724049568176 norm:0.001529061235487461 max memory_allocated 29275.52197265625 
[2025-03-22 06:33:01 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.4512382745742798 norm:0.0014249070081859827 max memory_allocated 29275.52197265625 
[2025-03-22 06:33:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.4500815272331238 norm:0.0012493240647017956 max memory_allocated 29275.52197265625 
[2025-03-22 06:34:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.4492570161819458 norm:0.0011862354585900903 max memory_allocated 29275.52197265625 
[2025-03-22 06:35:26 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.4486749768257141 norm:0.0011459680972620845 max memory_allocated 29275.52197265625 
[2025-03-22 06:36:14 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.4482711851596832 norm:0.0011352382134646177 max memory_allocated 29275.52197265625 
[2025-03-22 06:37:02 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.4477909505367279 norm:0.0011004245607182384 max memory_allocated 29275.52197265625 
[2025-03-22 06:37:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.44744765758514404 norm:0.0010993547039106488 max memory_allocated 29275.52197265625 
[2025-03-22 06:38:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.4471103847026825 norm:0.0010563971009105444 max memory_allocated 29275.52197265625 
[2025-03-22 06:39:25 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.4467978775501251 norm:0.0010348487412557006 max memory_allocated 29275.52197265625 
[2025-03-22 06:40:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.44660717248916626 norm:0.0010260059498250484 max memory_allocated 29275.52197265625 
[2025-03-22 06:41:01 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.44632643461227417 norm:0.0009876543190330267 max memory_allocated 29275.52197265625 
[2025-03-22 06:41:14 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 06:42:06 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.5386916995048523 norm:0.028920290991663933 max memory_allocated 29275.70947265625 
[2025-03-22 06:42:54 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.5177719593048096 norm:0.014783572405576706 max memory_allocated 29275.70947265625 
[2025-03-22 06:43:42 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.49666276574134827 norm:0.0067445882596075535 max memory_allocated 29275.70947265625 
[2025-03-22 06:44:30 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.4856676757335663 norm:0.003221099730581045 max memory_allocated 29275.70947265625 
[2025-03-22 06:45:18 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.48156246542930603 norm:0.0022384801413863897 max memory_allocated 29275.70947265625 
[2025-03-22 06:46:06 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.4793945550918579 norm:0.00195374246686697 max memory_allocated 29275.70947265625 
[2025-03-22 06:46:54 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.4776768684387207 norm:0.0017616855911910534 max memory_allocated 29275.70947265625 
[2025-03-22 06:47:42 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.476320743560791 norm:0.001622940064407885 max memory_allocated 29275.70947265625 
[2025-03-22 06:48:30 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.47521746158599854 norm:0.0015063929604366422 max memory_allocated 29275.70947265625 
[2025-03-22 06:49:18 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.4740803837776184 norm:0.0013308944180607796 max memory_allocated 29275.70947265625 
[2025-03-22 06:50:06 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.4731484651565552 norm:0.0012556653236970305 max memory_allocated 29275.70947265625 
[2025-03-22 06:50:55 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.4725055992603302 norm:0.0012087742798030376 max memory_allocated 29275.70947265625 
[2025-03-22 06:51:43 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.4719054400920868 norm:0.0011463246773928404 max memory_allocated 29275.70947265625 
[2025-03-22 06:52:31 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.47115376591682434 norm:0.001021274016238749 max memory_allocated 29275.70947265625 
[2025-03-22 06:53:19 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.4706903398036957 norm:0.0009834578959271312 max memory_allocated 29275.70947265625 
[2025-03-22 06:54:07 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.47032755613327026 norm:0.0009781151311472058 max memory_allocated 29275.70947265625 
[2025-03-22 06:54:54 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.47012436389923096 norm:0.0009706310229375958 max memory_allocated 29275.70947265625 
[2025-03-22 06:55:42 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.4698494076728821 norm:0.0009605882223695517 max memory_allocated 29275.70947265625 
[2025-03-22 06:56:30 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.46962279081344604 norm:0.0009551853872835636 max memory_allocated 29275.70947265625 
[2025-03-22 06:57:18 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.4694684147834778 norm:0.0009499050211161375 max memory_allocated 29275.70947265625 
[2025-03-22 06:57:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 06:58:23 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.5763615369796753 norm:0.027597635984420776 max memory_allocated 29275.89697265625 
[2025-03-22 06:59:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.5569316148757935 norm:0.014305589720606804 max memory_allocated 29275.89697265625 
[2025-03-22 06:59:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.5358548760414124 norm:0.007068404462188482 max memory_allocated 29275.89697265625 
[2025-03-22 07:00:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.5241078734397888 norm:0.0036655182484537363 max memory_allocated 29275.89697265625 
[2025-03-22 07:01:35 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.5192493796348572 norm:0.002509450074285269 max memory_allocated 29275.89697265625 
[2025-03-22 07:02:23 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.5168132781982422 norm:0.0021055489778518677 max memory_allocated 29275.89697265625 
[2025-03-22 07:03:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.515097439289093 norm:0.0019113575108349323 max memory_allocated 29275.89697265625 
[2025-03-22 07:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.5135871171951294 norm:0.0017583799781277776 max memory_allocated 29275.89697265625 
[2025-03-22 07:04:48 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.5123487114906311 norm:0.001634376123547554 max memory_allocated 29275.89697265625 
[2025-03-22 07:05:36 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.5112634897232056 norm:0.0015196842141449451 max memory_allocated 29275.89697265625 
[2025-03-22 07:06:24 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.5103987455368042 norm:0.0014424906112253666 max memory_allocated 29275.89697265625 
[2025-03-22 07:07:12 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.5096585750579834 norm:0.0013697280082851648 max memory_allocated 29275.89697265625 
[2025-03-22 07:08:00 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.5089439153671265 norm:0.001297985203564167 max memory_allocated 29275.89697265625 
[2025-03-22 07:08:48 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.5083293914794922 norm:0.0012239047791808844 max memory_allocated 29275.89697265625 
[2025-03-22 07:09:36 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.5077824592590332 norm:0.0011674148263409734 max memory_allocated 29275.89697265625 
[2025-03-22 07:10:24 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.5073796510696411 norm:0.0011425705160945654 max memory_allocated 29275.89697265625 
[2025-03-22 07:11:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.5069781541824341 norm:0.0011157961562275887 max memory_allocated 29275.89697265625 
[2025-03-22 07:11:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.5066234469413757 norm:0.0010968346614390612 max memory_allocated 29275.89697265625 
[2025-03-22 07:12:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.5063484907150269 norm:0.001080551533959806 max memory_allocated 29275.89697265625 
[2025-03-22 07:13:35 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.5061048269271851 norm:0.0010598767548799515 max memory_allocated 29275.89697265625 
[2025-03-22 07:13:49 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 07:14:40 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:0.594229519367218 norm:0.014718802645802498 max memory_allocated 29276.08447265625 
[2025-03-22 07:15:28 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:0.5794323086738586 norm:0.008596813306212425 max memory_allocated 29276.08447265625 
[2025-03-22 07:16:16 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:0.5653136372566223 norm:0.005205749534070492 max memory_allocated 29276.08447265625 
[2025-03-22 07:17:04 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:0.5576198101043701 norm:0.002950432477518916 max memory_allocated 29276.08447265625 
[2025-03-22 07:17:52 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:0.5540276169776917 norm:0.0021679173223674297 max memory_allocated 29276.08447265625 
[2025-03-22 07:18:40 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:0.5517295002937317 norm:0.0017050481401383877 max memory_allocated 29276.08447265625 
[2025-03-22 07:19:29 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:0.5501392483711243 norm:0.0015235577011480927 max memory_allocated 29276.08447265625 
[2025-03-22 07:20:17 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:0.5488549470901489 norm:0.0014154546661302447 max memory_allocated 29276.08447265625 
[2025-03-22 07:21:05 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:0.5476481318473816 norm:0.0012776635121554136 max memory_allocated 29276.08447265625 
[2025-03-22 07:21:53 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:0.5466812252998352 norm:0.0011959073599427938 max memory_allocated 29276.08447265625 
[2025-03-22 07:22:41 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.5458880662918091 norm:0.0011624465696513653 max memory_allocated 29276.08447265625 
[2025-03-22 07:23:30 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.545264482498169 norm:0.001097105792723596 max memory_allocated 29276.08447265625 
[2025-03-22 07:24:18 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.5447685122489929 norm:0.0010602610418573022 max memory_allocated 29276.08447265625 
[2025-03-22 07:25:06 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.5443762540817261 norm:0.001048112753778696 max memory_allocated 29276.08447265625 
[2025-03-22 07:25:54 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.5440219640731812 norm:0.0010111244628205895 max memory_allocated 29276.08447265625 
[2025-03-22 07:26:42 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.543677568435669 norm:0.0009878372075036168 max memory_allocated 29276.08447265625 
[2025-03-22 07:27:30 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.543329656124115 norm:0.000967918080277741 max memory_allocated 29276.08447265625 
[2025-03-22 07:28:18 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.5429856181144714 norm:0.0009557289886288345 max memory_allocated 29276.08447265625 
[2025-03-22 07:29:06 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.5426006317138672 norm:0.0009238572092726827 max memory_allocated 29276.08447265625 
[2025-03-22 07:29:54 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.5423296689987183 norm:0.0009087075595743954 max memory_allocated 29276.08447265625 
[2025-03-22 07:30:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 07:31:00 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:0.6394123435020447 norm:0.026883676648139954 max memory_allocated 29276.27197265625 
[2025-03-22 07:31:48 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:0.6268234252929688 norm:0.016086256131529808 max memory_allocated 29276.27197265625 
[2025-03-22 07:32:36 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:0.6134013533592224 norm:0.009848752990365028 max memory_allocated 29276.27197265625 
[2025-03-22 07:33:24 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:0.6067861914634705 norm:0.0068146721459925175 max memory_allocated 29276.27197265625 
[2025-03-22 07:34:12 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:0.6030529737472534 norm:0.004974387586116791 max memory_allocated 29276.27197265625 
[2025-03-22 07:35:01 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:0.6003928780555725 norm:0.0038079603109508753 max memory_allocated 29276.27197265625 
[2025-03-22 07:35:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:0.598298966884613 norm:0.003182596294209361 max memory_allocated 29276.27197265625 
[2025-03-22 07:36:37 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:0.5965409874916077 norm:0.0027309462893754244 max memory_allocated 29276.27197265625 
[2025-03-22 07:37:25 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:0.5951378345489502 norm:0.002404410857707262 max memory_allocated 29276.27197265625 
[2025-03-22 07:38:14 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:0.5940051674842834 norm:0.0021225756499916315 max memory_allocated 29276.27197265625 
[2025-03-22 07:39:02 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:0.5930901169776917 norm:0.0019202148541808128 max memory_allocated 29276.27197265625 
[2025-03-22 07:39:50 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:0.5922591090202332 norm:0.001749740680679679 max memory_allocated 29276.27197265625 
[2025-03-22 07:40:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:0.5916734337806702 norm:0.0016537080518901348 max memory_allocated 29276.27197265625 
[2025-03-22 07:41:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:0.5910887718200684 norm:0.0015513899270445108 max memory_allocated 29276.27197265625 
[2025-03-22 07:42:14 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:0.5906092524528503 norm:0.001433046068996191 max memory_allocated 29276.27197265625 
[2025-03-22 07:43:02 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:0.5901689529418945 norm:0.0013444280484691262 max memory_allocated 29276.27197265625 
[2025-03-22 07:43:50 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:0.5897738933563232 norm:0.0012779913377016783 max memory_allocated 29276.27197265625 
[2025-03-22 07:44:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:0.5894600749015808 norm:0.0012303588446229696 max memory_allocated 29276.27197265625 
[2025-03-22 07:45:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:0.5891232490539551 norm:0.0010732667287811637 max memory_allocated 29276.27197265625 
[2025-03-22 07:46:14 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:0.5886166095733643 norm:0.0008381436346098781 max memory_allocated 29276.27197265625 
[2025-03-22 07:46:28 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 07:47:19 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:0.6797248721122742 norm:0.013950853608548641 max memory_allocated 29276.45947265625 
[2025-03-22 07:48:08 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:0.6705183386802673 norm:0.00957680307328701 max memory_allocated 29276.45947265625 
[2025-03-22 07:48:56 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:0.6591029167175293 norm:0.006152685731649399 max memory_allocated 29276.45947265625 
[2025-03-22 07:49:44 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:0.6531969308853149 norm:0.003975228872150183 max memory_allocated 29276.45947265625 
[2025-03-22 07:50:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:0.6499033570289612 norm:0.003025921294465661 max memory_allocated 29276.45947265625 
[2025-03-22 07:51:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:0.6476577520370483 norm:0.002555935410782695 max memory_allocated 29276.45947265625 
[2025-03-22 07:52:08 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:0.6457979083061218 norm:0.002254219027236104 max memory_allocated 29276.45947265625 
[2025-03-22 07:52:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:0.644218385219574 norm:0.002084361622110009 max memory_allocated 29276.45947265625 
[2025-03-22 07:53:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:0.6425949335098267 norm:0.0018367128213867545 max memory_allocated 29276.45947265625 
[2025-03-22 07:54:33 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:0.6414999961853027 norm:0.0016719723353162408 max memory_allocated 29276.45947265625 
[2025-03-22 07:55:21 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:0.6405988335609436 norm:0.0015441457508131862 max memory_allocated 29276.45947265625 
[2025-03-22 07:56:09 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:0.6398447751998901 norm:0.0014491331530734897 max memory_allocated 29276.45947265625 
[2025-03-22 07:56:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:0.6392109394073486 norm:0.0013686707243323326 max memory_allocated 29276.45947265625 
[2025-03-22 07:57:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:0.6387596726417542 norm:0.0013036002637818456 max memory_allocated 29276.45947265625 
[2025-03-22 07:58:33 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:0.6384245157241821 norm:0.001272936468012631 max memory_allocated 29276.45947265625 
[2025-03-22 07:59:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:0.638012707233429 norm:0.0012183027574792504 max memory_allocated 29276.45947265625 
[2025-03-22 08:00:10 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:0.6377617716789246 norm:0.0011624618200585246 max memory_allocated 29276.45947265625 
[2025-03-22 08:00:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:0.6375423669815063 norm:0.001144564594142139 max memory_allocated 29276.45947265625 
[2025-03-22 08:01:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:0.6373502016067505 norm:0.001093690050765872 max memory_allocated 29276.45947265625 
[2025-03-22 08:02:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:0.637104868888855 norm:0.0010496448958292603 max memory_allocated 29276.45947265625 
[2025-03-22 08:02:47 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 08:03:39 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:0.7314687371253967 norm:0.008794186636805534 max memory_allocated 29276.64697265625 
[2025-03-22 08:04:27 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:0.7218126654624939 norm:0.00534039968624711 max memory_allocated 29276.64697265625 
[2025-03-22 08:05:15 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:0.7110017538070679 norm:0.003451382042840123 max memory_allocated 29276.64697265625 
[2025-03-22 08:06:04 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:0.7065556049346924 norm:0.00243482762016356 max memory_allocated 29276.64697265625 
[2025-03-22 08:06:52 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:0.7039254903793335 norm:0.0017916037468239665 max memory_allocated 29276.64697265625 
[2025-03-22 08:07:40 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:0.7021019458770752 norm:0.0014917338266968727 max memory_allocated 29276.64697265625 
[2025-03-22 08:08:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:0.7005566954612732 norm:0.0013401010073721409 max memory_allocated 29276.64697265625 
[2025-03-22 08:09:17 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:0.6991889476776123 norm:0.0012264820979908109 max memory_allocated 29276.64697265625 
[2025-03-22 08:10:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:0.6980541348457336 norm:0.0011358095798641443 max memory_allocated 29276.64697265625 
[2025-03-22 08:10:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:0.6971381902694702 norm:0.0010581037495285273 max memory_allocated 29276.64697265625 
[2025-03-22 08:11:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:0.6963819265365601 norm:0.0010086199035868049 max memory_allocated 29276.64697265625 
[2025-03-22 08:12:30 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:0.6957406401634216 norm:0.0009472660021856427 max memory_allocated 29276.64697265625 
[2025-03-22 08:13:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:0.6952680349349976 norm:0.0008883302216418087 max memory_allocated 29276.64697265625 
[2025-03-22 08:14:06 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:0.6948630213737488 norm:0.0008648618822917342 max memory_allocated 29276.64697265625 
[2025-03-22 08:14:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:0.6945366263389587 norm:0.0008394364267587662 max memory_allocated 29276.64697265625 
[2025-03-22 08:15:42 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:0.6942150592803955 norm:0.0008131102076731622 max memory_allocated 29276.64697265625 
[2025-03-22 08:16:30 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:0.693949818611145 norm:0.0007921447977423668 max memory_allocated 29276.64697265625 
[2025-03-22 08:17:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:0.6936942338943481 norm:0.0007756592822261155 max memory_allocated 29276.64697265625 
[2025-03-22 08:18:06 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:0.6934572458267212 norm:0.000757975154556334 max memory_allocated 29276.64697265625 
[2025-03-22 08:18:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:0.6932290196418762 norm:0.000744778080843389 max memory_allocated 29276.64697265625 
[2025-03-22 08:19:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 08:19:59 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:0.8041091561317444 norm:0.015720965340733528 max memory_allocated 29276.83447265625 
[2025-03-22 08:20:48 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:0.7942581176757812 norm:0.010586965829133987 max memory_allocated 29276.83447265625 
[2025-03-22 08:21:36 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:0.7813971042633057 norm:0.007362653035670519 max memory_allocated 29276.83447265625 
[2025-03-22 08:22:24 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:0.7757908701896667 norm:0.005253099836409092 max memory_allocated 29276.83447265625 
[2025-03-22 08:23:12 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:0.772666871547699 norm:0.003977903630584478 max memory_allocated 29276.83447265625 
[2025-03-22 08:24:00 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:0.7699587345123291 norm:0.0030227790120989084 max memory_allocated 29276.83447265625 
[2025-03-22 08:24:48 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:0.7673226594924927 norm:0.0020930105820298195 max memory_allocated 29276.83447265625 
[2025-03-22 08:25:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:0.7655046582221985 norm:0.001732607837766409 max memory_allocated 29276.83447265625 
[2025-03-22 08:26:25 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:0.764250636100769 norm:0.0016615319764241576 max memory_allocated 29276.83447265625 
[2025-03-22 08:27:13 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:0.7631202936172485 norm:0.0015818839892745018 max memory_allocated 29276.83447265625 
[2025-03-22 08:28:01 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:0.7622572183609009 norm:0.0015249944990500808 max memory_allocated 29276.83447265625 
[2025-03-22 08:28:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:0.7615575194358826 norm:0.0014618239365518093 max memory_allocated 29276.83447265625 
[2025-03-22 08:29:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:0.7609260082244873 norm:0.001443119253963232 max memory_allocated 29276.83447265625 
[2025-03-22 08:30:25 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:0.760453999042511 norm:0.0013947103871032596 max memory_allocated 29276.83447265625 
[2025-03-22 08:31:14 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:0.7600849866867065 norm:0.001368455239571631 max memory_allocated 29276.83447265625 
[2025-03-22 08:32:02 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:0.7598137855529785 norm:0.0013494425220414996 max memory_allocated 29276.83447265625 
[2025-03-22 08:32:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:0.7595498561859131 norm:0.0013062385842204094 max memory_allocated 29276.83447265625 
[2025-03-22 08:33:38 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:0.7592838406562805 norm:0.001281870063394308 max memory_allocated 29276.83447265625 
[2025-03-22 08:34:26 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:0.759009063243866 norm:0.0012503658654168248 max memory_allocated 29276.83447265625 
[2025-03-22 08:35:14 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:0.7588751912117004 norm:0.0012246655533090234 max memory_allocated 29276.83447265625 
[2025-03-22 08:35:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 08:36:19 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:0.8564949631690979 norm:0.0050177243538200855 max memory_allocated 29277.02197265625 
[2025-03-22 08:37:07 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:0.8480895757675171 norm:0.002815454499796033 max memory_allocated 29277.02197265625 
[2025-03-22 08:37:56 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:0.8377390503883362 norm:0.0016357933636754751 max memory_allocated 29277.02197265625 
[2025-03-22 08:38:44 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:0.8338591456413269 norm:0.0012124975910410285 max memory_allocated 29277.02197265625 
[2025-03-22 08:39:32 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:0.8319018483161926 norm:0.00108283048029989 max memory_allocated 29277.02197265625 
[2025-03-22 08:40:20 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:0.8302295804023743 norm:0.000976715818978846 max memory_allocated 29277.02197265625 
[2025-03-22 08:41:09 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:0.8284462690353394 norm:0.0008351794676855206 max memory_allocated 29277.02197265625 
[2025-03-22 08:41:57 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:0.8268964886665344 norm:0.0007944878307171166 max memory_allocated 29277.02197265625 
[2025-03-22 08:42:45 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:0.8256046175956726 norm:0.0007661834242753685 max memory_allocated 29277.02197265625 
[2025-03-22 08:43:33 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:0.824669599533081 norm:0.0007506894180551171 max memory_allocated 29277.02197265625 
[2025-03-22 08:44:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:0.823876678943634 norm:0.0007311415974982083 max memory_allocated 29277.02197265625 
[2025-03-22 08:45:09 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:0.8232866525650024 norm:0.000719457573723048 max memory_allocated 29277.02197265625 
[2025-03-22 08:45:57 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:0.8227659463882446 norm:0.0007192561752162874 max memory_allocated 29277.02197265625 
[2025-03-22 08:46:45 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:0.8223526477813721 norm:0.0007112564635463059 max memory_allocated 29277.02197265625 
[2025-03-22 08:47:33 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:0.8220376968383789 norm:0.0007066464168019593 max memory_allocated 29277.02197265625 
[2025-03-22 08:48:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:0.8217297196388245 norm:0.0006989641697145998 max memory_allocated 29277.02197265625 
[2025-03-22 08:49:09 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:0.8214301466941833 norm:0.0006991897244006395 max memory_allocated 29277.02197265625 
[2025-03-22 08:49:57 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:0.8211371302604675 norm:0.0006891735829412937 max memory_allocated 29277.02197265625 
[2025-03-22 08:50:46 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:0.820893406867981 norm:0.0006894882535561919 max memory_allocated 29277.02197265625 
[2025-03-22 08:51:34 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:0.8206863403320312 norm:0.0006826078752055764 max memory_allocated 29277.02197265625 
[2025-03-22 08:51:47 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 08:52:39 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:0.9433245658874512 norm:0.014481207355856895 max memory_allocated 29277.20947265625 
[2025-03-22 08:53:28 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:0.9326890110969543 norm:0.009169867262244225 max memory_allocated 29277.20947265625 
[2025-03-22 08:54:16 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:0.9213933944702148 norm:0.006288836244493723 max memory_allocated 29277.20947265625 
[2025-03-22 08:55:04 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:0.9159303307533264 norm:0.004264036659151316 max memory_allocated 29277.20947265625 
[2025-03-22 08:55:52 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:0.9126781225204468 norm:0.003432796336710453 max memory_allocated 29277.20947265625 
[2025-03-22 08:56:40 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:0.9096747636795044 norm:0.0025643249973654747 max memory_allocated 29277.20947265625 
[2025-03-22 08:57:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:0.9071589708328247 norm:0.0019143121317029 max memory_allocated 29277.20947265625 
[2025-03-22 08:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:0.9053388237953186 norm:0.0018148517701774836 max memory_allocated 29277.20947265625 
[2025-03-22 08:59:05 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:0.9038504362106323 norm:0.0017029664013534784 max memory_allocated 29277.20947265625 
[2025-03-22 08:59:53 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:0.9027531147003174 norm:0.0016099996864795685 max memory_allocated 29277.20947265625 
[2025-03-22 09:00:41 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:0.9018798470497131 norm:0.0016136084450408816 max memory_allocated 29277.20947265625 
[2025-03-22 09:01:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:0.9011552929878235 norm:0.0015157898887991905 max memory_allocated 29277.20947265625 
[2025-03-22 09:02:17 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:0.9005286693572998 norm:0.001445618225261569 max memory_allocated 29277.20947265625 
[2025-03-22 09:03:05 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:0.9000706672668457 norm:0.0014534558868035674 max memory_allocated 29277.20947265625 
[2025-03-22 09:03:53 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:0.8997341990470886 norm:0.0013640783727169037 max memory_allocated 29277.20947265625 
[2025-03-22 09:04:41 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:0.8992748260498047 norm:0.0012598957400768995 max memory_allocated 29277.20947265625 
[2025-03-22 09:05:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:0.898909330368042 norm:0.00121987028978765 max memory_allocated 29277.20947265625 
[2025-03-22 09:06:17 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:0.8986570239067078 norm:0.0012125701177865267 max memory_allocated 29277.20947265625 
[2025-03-22 09:07:05 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:0.8985621929168701 norm:0.0012040104484185576 max memory_allocated 29277.20947265625 
[2025-03-22 09:07:54 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:0.8983094692230225 norm:0.0012210237327963114 max memory_allocated 29277.20947265625 
[2025-03-22 09:08:07 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 09:08:59 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:1.0153555870056152 norm:0.008085951209068298 max memory_allocated 29277.39697265625 
[2025-03-22 09:09:47 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:1.0062928199768066 norm:0.004590106662362814 max memory_allocated 29277.39697265625 
[2025-03-22 09:10:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:0.9943307042121887 norm:0.0024850214831531048 max memory_allocated 29277.39697265625 
[2025-03-22 09:11:23 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:0.9897050261497498 norm:0.0018697244813665748 max memory_allocated 29277.39697265625 
[2025-03-22 09:12:11 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:0.9871546626091003 norm:0.0015366716543212533 max memory_allocated 29277.39697265625 
[2025-03-22 09:13:00 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:0.9849252700805664 norm:0.0011619683355093002 max memory_allocated 29277.39697265625 
[2025-03-22 09:13:48 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:0.9829604625701904 norm:0.001040122122503817 max memory_allocated 29277.39697265625 
[2025-03-22 09:14:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:0.9813101887702942 norm:0.0009634435409680009 max memory_allocated 29277.39697265625 
[2025-03-22 09:15:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:0.9799787998199463 norm:0.0009221674408763647 max memory_allocated 29277.39697265625 
[2025-03-22 09:16:12 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:0.9789839386940002 norm:0.0008760574273765087 max memory_allocated 29277.39697265625 
[2025-03-22 09:17:00 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:0.9781427979469299 norm:0.0008356970502063632 max memory_allocated 29277.39697265625 
[2025-03-22 09:17:48 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:0.9774152040481567 norm:0.0008056166116148233 max memory_allocated 29277.39697265625 
[2025-03-22 09:18:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:0.9768185019493103 norm:0.0007857694290578365 max memory_allocated 29277.39697265625 
[2025-03-22 09:19:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:0.9763903021812439 norm:0.000775483320467174 max memory_allocated 29277.39697265625 
[2025-03-22 09:20:11 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:0.9760115742683411 norm:0.0007592002511955798 max memory_allocated 29277.39697265625 
[2025-03-22 09:20:59 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:0.975690484046936 norm:0.000747992075048387 max memory_allocated 29277.39697265625 
[2025-03-22 09:21:47 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:0.9754173755645752 norm:0.0007364003104157746 max memory_allocated 29277.39697265625 
[2025-03-22 09:22:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:0.9751849174499512 norm:0.0007233534706756473 max memory_allocated 29277.39697265625 
[2025-03-22 09:23:23 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:0.9749538898468018 norm:0.0007162916008383036 max memory_allocated 29277.39697265625 
[2025-03-22 09:24:11 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:0.9747453927993774 norm:0.0007117443019524217 max memory_allocated 29277.39697265625 
[2025-03-22 09:24:25 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 09:25:17 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:1.1083836555480957 norm:0.01180294994264841 max memory_allocated 29277.58447265625 
[2025-03-22 09:26:05 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:1.096172571182251 norm:0.006896698847413063 max memory_allocated 29277.58447265625 
[2025-03-22 09:26:53 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:1.0816149711608887 norm:0.004360141232609749 max memory_allocated 29277.58447265625 
[2025-03-22 09:27:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:1.0751410722732544 norm:0.0028863856568932533 max memory_allocated 29277.58447265625 
[2025-03-22 09:28:29 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:1.0717458724975586 norm:0.0015975367277860641 max memory_allocated 29277.58447265625 
[2025-03-22 09:29:17 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:1.0692440271377563 norm:0.001318255439400673 max memory_allocated 29277.58447265625 
[2025-03-22 09:30:05 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:1.0670098066329956 norm:0.0012036248808726668 max memory_allocated 29277.58447265625 
[2025-03-22 09:30:53 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:1.0651068687438965 norm:0.0011518268147483468 max memory_allocated 29277.58447265625 
[2025-03-22 09:31:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:1.0635066032409668 norm:0.0010813864646479487 max memory_allocated 29277.58447265625 
[2025-03-22 09:32:29 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:1.0623794794082642 norm:0.0010378569131717086 max memory_allocated 29277.58447265625 
[2025-03-22 09:33:17 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:1.061479091644287 norm:0.0010085745016112924 max memory_allocated 29277.58447265625 
[2025-03-22 09:34:05 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:1.0607267618179321 norm:0.0009832467185333371 max memory_allocated 29277.58447265625 
[2025-03-22 09:34:53 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:1.0600669384002686 norm:0.0009493915131315589 max memory_allocated 29277.58447265625 
[2025-03-22 09:35:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:1.059488296508789 norm:0.0009217938641086221 max memory_allocated 29277.58447265625 
[2025-03-22 09:36:29 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:1.0590171813964844 norm:0.0008949334733188152 max memory_allocated 29277.58447265625 
[2025-03-22 09:37:17 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:1.0586755275726318 norm:0.0008743699872866273 max memory_allocated 29277.58447265625 
[2025-03-22 09:38:05 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:1.0583127737045288 norm:0.0008562073344364762 max memory_allocated 29277.58447265625 
[2025-03-22 09:38:53 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:1.057995319366455 norm:0.000834057864267379 max memory_allocated 29277.58447265625 
[2025-03-22 09:39:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:1.0576982498168945 norm:0.0008256336441263556 max memory_allocated 29277.58447265625 
[2025-03-22 09:40:29 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:1.0573809146881104 norm:0.0008048008312471211 max memory_allocated 29277.58447265625 
[2025-03-22 09:40:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 09:41:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:1.194994568824768 norm:0.010844019241631031 max memory_allocated 29277.77197265625 
[2025-03-22 09:42:23 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:1.1844969987869263 norm:0.006747087463736534 max memory_allocated 29277.77197265625 
[2025-03-22 09:43:11 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:1.169663906097412 norm:0.003218378871679306 max memory_allocated 29277.77197265625 
[2025-03-22 09:43:59 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:1.163110613822937 norm:0.0026552367489784956 max memory_allocated 29277.77197265625 
[2025-03-22 09:44:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:1.1586264371871948 norm:0.001304828329011798 max memory_allocated 29277.77197265625 
[2025-03-22 09:45:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:1.1555877923965454 norm:0.001022647600620985 max memory_allocated 29277.77197265625 
[2025-03-22 09:46:23 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:1.153083324432373 norm:0.0009385918965563178 max memory_allocated 29277.77197265625 
[2025-03-22 09:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:1.1508862972259521 norm:0.0008865398121997714 max memory_allocated 29277.77197265625 
[2025-03-22 09:47:59 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:1.1492646932601929 norm:0.000846788811031729 max memory_allocated 29277.77197265625 
[2025-03-22 09:48:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:1.1479926109313965 norm:0.0008124710875563323 max memory_allocated 29277.77197265625 
[2025-03-22 09:49:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:1.146976351737976 norm:0.0007921609794721007 max memory_allocated 29277.77197265625 
[2025-03-22 09:50:23 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:1.1463972330093384 norm:0.0007798705482855439 max memory_allocated 29277.77197265625 
[2025-03-22 09:51:11 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:1.1457542181015015 norm:0.0007746755727566779 max memory_allocated 29277.77197265625 
[2025-03-22 09:51:59 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:1.1452423334121704 norm:0.0007604463608004153 max memory_allocated 29277.77197265625 
[2025-03-22 09:52:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:1.144837737083435 norm:0.000746050791349262 max memory_allocated 29277.77197265625 
[2025-03-22 09:53:34 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:1.144524335861206 norm:0.0007428459939546883 max memory_allocated 29277.77197265625 
[2025-03-22 09:54:22 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:1.144221305847168 norm:0.0007380457827821374 max memory_allocated 29277.77197265625 
[2025-03-22 09:55:10 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:1.1439591646194458 norm:0.0007369316881522536 max memory_allocated 29277.77197265625 
[2025-03-22 09:55:59 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:1.1436936855316162 norm:0.000736178073566407 max memory_allocated 29277.77197265625 
[2025-03-22 09:56:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:1.1434905529022217 norm:0.0007304244791157544 max memory_allocated 29277.77197265625 
[2025-03-22 09:57:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 32 ===
[2025-03-22 09:57:52 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 0 loss:1.2993625402450562 norm:0.015577862039208412 max memory_allocated 29277.95947265625 
[2025-03-22 09:58:40 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 1 loss:1.2857304811477661 norm:0.0093302633613348 max memory_allocated 29277.95947265625 
[2025-03-22 09:59:28 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 2 loss:1.2692763805389404 norm:0.005985200870782137 max memory_allocated 29277.95947265625 
[2025-03-22 10:00:16 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 3 loss:1.2614049911499023 norm:0.004191214218735695 max memory_allocated 29277.95947265625 
[2025-03-22 10:01:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 4 loss:1.2570834159851074 norm:0.0030453798826783895 max memory_allocated 29277.95947265625 
[2025-03-22 10:01:52 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 5 loss:1.253575325012207 norm:0.0024849805049598217 max memory_allocated 29277.95947265625 
[2025-03-22 10:02:40 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 6 loss:1.2505383491516113 norm:0.001911027473397553 max memory_allocated 29277.95947265625 
[2025-03-22 10:03:28 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 7 loss:1.247740626335144 norm:0.0010846992954611778 max memory_allocated 29277.95947265625 
[2025-03-22 10:04:16 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 8 loss:1.245798945426941 norm:0.0010479319607838988 max memory_allocated 29277.95947265625 
[2025-03-22 10:05:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 9 loss:1.2445086240768433 norm:0.0010073755402117968 max memory_allocated 29277.95947265625 
[2025-03-22 10:05:52 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 10 loss:1.2436121702194214 norm:0.000986173516139388 max memory_allocated 29277.95947265625 
[2025-03-22 10:06:40 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 11 loss:1.2428323030471802 norm:0.0009583326173014939 max memory_allocated 29277.95947265625 
[2025-03-22 10:07:28 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 12 loss:1.2422723770141602 norm:0.0009458959102630615 max memory_allocated 29277.95947265625 
[2025-03-22 10:08:16 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 13 loss:1.2416988611221313 norm:0.0009074514964595437 max memory_allocated 29277.95947265625 
[2025-03-22 10:09:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 14 loss:1.2411491870880127 norm:0.0009023346356116235 max memory_allocated 29277.95947265625 
[2025-03-22 10:09:52 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 15 loss:1.2407716512680054 norm:0.0008802940137684345 max memory_allocated 29277.95947265625 
[2025-03-22 10:10:40 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 16 loss:1.2405316829681396 norm:0.0008577575790695846 max memory_allocated 29277.95947265625 
[2025-03-22 10:11:28 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 17 loss:1.2402070760726929 norm:0.0008482144912704825 max memory_allocated 29277.95947265625 
[2025-03-22 10:12:16 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 18 loss:1.2399978637695312 norm:0.0008408123394474387 max memory_allocated 29277.95947265625 
[2025-03-22 10:13:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 19 loss:1.239720344543457 norm:0.0008328584372065961 max memory_allocated 29277.95947265625 
[2025-03-22 10:13:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 33 ===
[2025-03-22 10:14:10 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 0 loss:1.408177137374878 norm:0.013316220603883266 max memory_allocated 29278.14697265625 
[2025-03-22 10:14:58 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 1 loss:1.393455982208252 norm:0.009225680492818356 max memory_allocated 29278.14697265625 
[2025-03-22 10:15:46 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 2 loss:1.3759299516677856 norm:0.0066366177052259445 max memory_allocated 29278.14697265625 
[2025-03-22 10:16:34 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 3 loss:1.3672007322311401 norm:0.004839570727199316 max memory_allocated 29278.14697265625 
[2025-03-22 10:17:22 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 4 loss:1.3620216846466064 norm:0.0037870306987315416 max memory_allocated 29278.14697265625 
[2025-03-22 10:18:10 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 5 loss:1.3575869798660278 norm:0.0028457152657210827 max memory_allocated 29278.14697265625 
[2025-03-22 10:18:58 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 6 loss:1.35354745388031 norm:0.002159803407266736 max memory_allocated 29278.14697265625 
[2025-03-22 10:19:46 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 7 loss:1.3511922359466553 norm:0.0019321752479299903 max memory_allocated 29278.14697265625 
[2025-03-22 10:20:34 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 8 loss:1.3491230010986328 norm:0.001718134037218988 max memory_allocated 29278.14697265625 
[2025-03-22 10:21:22 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 9 loss:1.3471720218658447 norm:0.0015593573916703463 max memory_allocated 29278.14697265625 
[2025-03-22 10:22:10 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 10 loss:1.345773696899414 norm:0.0014359618071466684 max memory_allocated 29278.14697265625 
[2025-03-22 10:22:58 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 11 loss:1.345015525817871 norm:0.001338926493190229 max memory_allocated 29278.14697265625 
[2025-03-22 10:23:46 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 12 loss:1.3443572521209717 norm:0.001254675560630858 max memory_allocated 29278.14697265625 
[2025-03-22 10:24:34 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 13 loss:1.3439329862594604 norm:0.0011692708358168602 max memory_allocated 29278.14697265625 
[2025-03-22 10:25:22 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 14 loss:1.3435258865356445 norm:0.0011231070384383202 max memory_allocated 29278.14697265625 
[2025-03-22 10:26:10 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 15 loss:1.3431968688964844 norm:0.001076250453479588 max memory_allocated 29278.14697265625 
[2025-03-22 10:26:58 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 16 loss:1.3424580097198486 norm:0.0010159889934584498 max memory_allocated 29278.14697265625 
[2025-03-22 10:27:46 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 17 loss:1.3420034646987915 norm:0.0009819833794608712 max memory_allocated 29278.14697265625 
[2025-03-22 10:28:34 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 18 loss:1.3415926694869995 norm:0.0009427473414689302 max memory_allocated 29278.14697265625 
[2025-03-22 10:29:22 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 19 loss:1.3411343097686768 norm:0.0009160544723272324 max memory_allocated 29278.14697265625 
[2025-03-22 10:29:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 34 ===
[2025-03-22 10:30:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 0 loss:1.5326875448226929 norm:0.015038645826280117 max memory_allocated 29278.33447265625 
[2025-03-22 10:31:16 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 1 loss:1.516262412071228 norm:0.009146066382527351 max memory_allocated 29278.33447265625 
[2025-03-22 10:32:04 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 2 loss:1.4972279071807861 norm:0.006010012701153755 max memory_allocated 29278.33447265625 
[2025-03-22 10:32:52 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 3 loss:1.48819100856781 norm:0.004353209398686886 max memory_allocated 29278.33447265625 
[2025-03-22 10:33:40 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 4 loss:1.4830877780914307 norm:0.0035310869570821524 max memory_allocated 29278.33447265625 
[2025-03-22 10:34:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 5 loss:1.4787710905075073 norm:0.002863026224076748 max memory_allocated 29278.33447265625 
[2025-03-22 10:35:16 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 6 loss:1.474884271621704 norm:0.002249007346108556 max memory_allocated 29278.33447265625 
[2025-03-22 10:36:04 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 7 loss:1.4718518257141113 norm:0.001962956739589572 max memory_allocated 29278.33447265625 
[2025-03-22 10:36:52 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 8 loss:1.4696831703186035 norm:0.0017847632989287376 max memory_allocated 29278.33447265625 
[2025-03-22 10:37:40 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 9 loss:1.468066930770874 norm:0.0016459045000374317 max memory_allocated 29278.33447265625 
[2025-03-22 10:38:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 10 loss:1.4668469429016113 norm:0.001518375240266323 max memory_allocated 29278.33447265625 
[2025-03-22 10:39:15 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 11 loss:1.4659137725830078 norm:0.0013972754823043942 max memory_allocated 29278.33447265625 
[2025-03-22 10:40:03 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 12 loss:1.46498703956604 norm:0.0012931785313412547 max memory_allocated 29278.33447265625 
[2025-03-22 10:40:51 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 13 loss:1.4641563892364502 norm:0.0012052108068019152 max memory_allocated 29278.33447265625 
[2025-03-22 10:41:39 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 14 loss:1.463624119758606 norm:0.0011298272293061018 max memory_allocated 29278.33447265625 
[2025-03-22 10:42:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 15 loss:1.4630752801895142 norm:0.0009935288690030575 max memory_allocated 29278.33447265625 
[2025-03-22 10:43:16 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 16 loss:1.4626586437225342 norm:0.0008901668479666114 max memory_allocated 29278.33447265625 
[2025-03-22 10:44:04 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 17 loss:1.462375283241272 norm:0.0008758420590311289 max memory_allocated 29278.33447265625 
[2025-03-22 10:44:52 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 18 loss:1.4620213508605957 norm:0.0008613427635282278 max memory_allocated 29278.33447265625 
[2025-03-22 10:45:40 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 19 loss:1.4617016315460205 norm:0.0008525949670001864 max memory_allocated 29278.33447265625 
[2025-03-22 10:45:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 35 ===
[2025-03-22 10:46:46 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 0 loss:1.6676462888717651 norm:0.008344948291778564 max memory_allocated 29278.52197265625 
[2025-03-22 10:47:34 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 1 loss:1.6502759456634521 norm:0.0056862132623791695 max memory_allocated 29278.52197265625 
[2025-03-22 10:48:22 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 2 loss:1.628828525543213 norm:0.0038125282153487206 max memory_allocated 29278.52197265625 
[2025-03-22 10:49:10 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 3 loss:1.618308424949646 norm:0.0027447277680039406 max memory_allocated 29278.52197265625 
[2025-03-22 10:49:57 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 4 loss:1.6125245094299316 norm:0.002111522713676095 max memory_allocated 29278.52197265625 
[2025-03-22 10:50:45 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 5 loss:1.6078112125396729 norm:0.0017979233525693417 max memory_allocated 29278.52197265625 
[2025-03-22 10:51:33 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 6 loss:1.6038461923599243 norm:0.001603892887942493 max memory_allocated 29278.52197265625 
[2025-03-22 10:52:21 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 7 loss:1.6006439924240112 norm:0.0014613443054258823 max memory_allocated 29278.52197265625 
[2025-03-22 10:53:09 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 8 loss:1.598197102546692 norm:0.0013561954256147146 max memory_allocated 29278.52197265625 
[2025-03-22 10:53:57 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 9 loss:1.596493124961853 norm:0.0012855710228905082 max memory_allocated 29278.52197265625 
[2025-03-22 10:54:45 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 10 loss:1.5952210426330566 norm:0.001224114210344851 max memory_allocated 29278.52197265625 
[2025-03-22 10:55:33 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 11 loss:1.5942519903182983 norm:0.0011811110889539123 max memory_allocated 29278.52197265625 
[2025-03-22 10:56:21 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 12 loss:1.5934101343154907 norm:0.0011355180758982897 max memory_allocated 29278.52197265625 
[2025-03-22 10:57:09 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 13 loss:1.592512607574463 norm:0.001104893395677209 max memory_allocated 29278.52197265625 
[2025-03-22 10:57:57 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 14 loss:1.5918773412704468 norm:0.0010568241123110056 max memory_allocated 29278.52197265625 
[2025-03-22 10:58:45 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 15 loss:1.591141700744629 norm:0.0010235423687845469 max memory_allocated 29278.52197265625 
[2025-03-22 10:59:34 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 16 loss:1.5905513763427734 norm:0.0009920246666297317 max memory_allocated 29278.52197265625 
[2025-03-22 11:00:22 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 17 loss:1.5900715589523315 norm:0.0009677493362687528 max memory_allocated 29278.52197265625 
[2025-03-22 11:01:10 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 18 loss:1.5897154808044434 norm:0.0009518283186480403 max memory_allocated 29278.52197265625 
[2025-03-22 11:01:58 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 19 loss:1.5894824266433716 norm:0.0009355350630357862 max memory_allocated 29278.52197265625 
[2025-03-22 11:02:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 36 ===
[2025-03-22 11:02:15 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:03:04 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 0 loss:1.8825881481170654 norm:0.07427746057510376 max memory_allocated 29281.02197265625 
[2025-03-22 11:03:52 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 1 loss:1.8555020093917847 norm:0.06472019851207733 max memory_allocated 29281.02197265625 
[2025-03-22 11:04:40 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 2 loss:1.8188258409500122 norm:0.04332622513175011 max memory_allocated 29281.02197265625 
[2025-03-22 11:05:29 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 3 loss:1.7995957136154175 norm:0.036901120096445084 max memory_allocated 29281.02197265625 
[2025-03-22 11:06:17 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 4 loss:1.7907756567001343 norm:0.03260888531804085 max memory_allocated 29281.02197265625 
[2025-03-22 11:07:05 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 5 loss:1.7902734279632568 norm:0.029293330386281013 max memory_allocated 29281.02197265625 
[2025-03-22 11:07:54 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 6 loss:1.7781516313552856 norm:0.02057165466248989 max memory_allocated 29281.02197265625 
[2025-03-22 11:08:42 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 7 loss:1.788132667541504 norm:0.027964940294623375 max memory_allocated 29281.02197265625 
[2025-03-22 11:09:30 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 8 loss:1.8572341203689575 norm:0.049722906202077866 max memory_allocated 29281.02197265625 
[2025-03-22 11:10:18 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 9 loss:1.7732503414154053 norm:0.018438220024108887 max memory_allocated 29281.02197265625 
[2025-03-22 11:11:06 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 10 loss:1.7707035541534424 norm:0.018351547420024872 max memory_allocated 29281.02197265625 
[2025-03-22 11:11:54 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 11 loss:1.7671622037887573 norm:0.015689406543970108 max memory_allocated 29281.02197265625 
[2025-03-22 11:12:43 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 12 loss:1.764261245727539 norm:0.01401364617049694 max memory_allocated 29281.02197265625 
[2025-03-22 11:13:31 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 13 loss:1.7622463703155518 norm:0.013029650785028934 max memory_allocated 29281.02197265625 
[2025-03-22 11:14:19 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 14 loss:1.7603904008865356 norm:0.012306705117225647 max memory_allocated 29281.02197265625 
[2025-03-22 11:15:07 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 15 loss:1.758471965789795 norm:0.011545835062861443 max memory_allocated 29281.02197265625 
[2025-03-22 11:15:55 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 16 loss:1.7570064067840576 norm:0.0113028883934021 max memory_allocated 29281.02197265625 
[2025-03-22 11:16:44 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 17 loss:1.7552695274353027 norm:0.010711701586842537 max memory_allocated 29281.02197265625 
[2025-03-22 11:17:32 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 18 loss:1.7537387609481812 norm:0.010296009480953217 max memory_allocated 29281.02197265625 
[2025-03-22 11:18:20 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 19 loss:1.7528841495513916 norm:0.01001870445907116 max memory_allocated 29281.02197265625 
[2025-03-22 11:18:34 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 37 ===
[2025-03-22 11:18:38 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:19:26 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 0 loss:2.0890660285949707 norm:0.08996296674013138 max memory_allocated 29281.20947265625 
[2025-03-22 11:20:14 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 1 loss:2.051138162612915 norm:0.070586659014225 max memory_allocated 29281.20947265625 
[2025-03-22 11:21:02 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 2 loss:2.0113015174865723 norm:0.060040440410375595 max memory_allocated 29281.20947265625 
[2025-03-22 11:21:51 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 3 loss:1.983971357345581 norm:0.04151912406086922 max memory_allocated 29281.20947265625 
[2025-03-22 11:22:39 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 4 loss:1.9731907844543457 norm:0.04005502536892891 max memory_allocated 29281.20947265625 
[2025-03-22 11:23:28 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 5 loss:1.9693231582641602 norm:0.032921262085437775 max memory_allocated 29281.20947265625 
[2025-03-22 11:24:16 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 6 loss:1.9579237699508667 norm:0.024909650906920433 max memory_allocated 29281.20947265625 
[2025-03-22 11:25:04 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 7 loss:1.9593945741653442 norm:0.02734026499092579 max memory_allocated 29281.20947265625 
[2025-03-22 11:25:52 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 8 loss:1.9514168500900269 norm:0.019849956035614014 max memory_allocated 29281.20947265625 
[2025-03-22 11:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 9 loss:1.9464986324310303 norm:0.020052270963788033 max memory_allocated 29281.20947265625 
[2025-03-22 11:27:29 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 10 loss:2.004950523376465 norm:0.04271309822797775 max memory_allocated 29281.20947265625 
[2025-03-22 11:28:17 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 11 loss:1.9994767904281616 norm:0.03633207455277443 max memory_allocated 29281.20947265625 
[2025-03-22 11:29:05 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 12 loss:1.9426898956298828 norm:0.015237949788570404 max memory_allocated 29281.20947265625 
[2025-03-22 11:29:53 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 13 loss:1.938710331916809 norm:0.01511326339095831 max memory_allocated 29281.20947265625 
[2025-03-22 11:30:41 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 14 loss:1.9378392696380615 norm:0.01504475437104702 max memory_allocated 29281.20947265625 
[2025-03-22 11:31:29 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 15 loss:1.9352182149887085 norm:0.01350499503314495 max memory_allocated 29281.20947265625 
[2025-03-22 11:32:18 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 16 loss:1.93691086769104 norm:0.014696958474814892 max memory_allocated 29281.20947265625 
[2025-03-22 11:33:06 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 17 loss:1.9330732822418213 norm:0.011819652281701565 max memory_allocated 29281.20947265625 
[2025-03-22 11:33:54 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 18 loss:1.9331352710723877 norm:0.011807722970843315 max memory_allocated 29281.20947265625 
[2025-03-22 11:34:42 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 19 loss:1.9305107593536377 norm:0.011382987722754478 max memory_allocated 29281.20947265625 
[2025-03-22 11:34:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 38 ===
[2025-03-22 11:35:00 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:35:48 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 0 loss:2.447723388671875 norm:0.14310893416404724 max memory_allocated 29281.39697265625 
[2025-03-22 11:36:36 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 1 loss:2.387861490249634 norm:0.1162799522280693 max memory_allocated 29281.39697265625 
[2025-03-22 11:37:25 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 2 loss:2.3419435024261475 norm:0.09250777959823608 max memory_allocated 29281.39697265625 
[2025-03-22 11:38:13 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 3 loss:2.30049467086792 norm:0.07062213122844696 max memory_allocated 29281.39697265625 
[2025-03-22 11:39:01 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 4 loss:2.2968297004699707 norm:0.06513698399066925 max memory_allocated 29281.39697265625 
[2025-03-22 11:39:50 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 5 loss:2.2701735496520996 norm:0.04749087244272232 max memory_allocated 29281.39697265625 
[2025-03-22 11:40:38 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 6 loss:2.257004976272583 norm:0.0395861454308033 max memory_allocated 29281.39697265625 
[2025-03-22 11:41:26 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 7 loss:2.2700438499450684 norm:0.04939236119389534 max memory_allocated 29281.39697265625 
[2025-03-22 11:42:15 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 8 loss:2.286834955215454 norm:0.056303225457668304 max memory_allocated 29281.39697265625 
[2025-03-22 11:43:03 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 9 loss:2.241711139678955 norm:0.03150273114442825 max memory_allocated 29281.39697265625 
[2025-03-22 11:43:52 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 10 loss:2.239095687866211 norm:0.029277916997671127 max memory_allocated 29281.39697265625 
[2025-03-22 11:44:40 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 11 loss:2.2338552474975586 norm:0.02472788095474243 max memory_allocated 29281.39697265625 
[2025-03-22 11:45:28 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 12 loss:2.2420713901519775 norm:0.029355749487876892 max memory_allocated 29281.39697265625 
[2025-03-22 11:46:16 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 13 loss:2.2267608642578125 norm:0.02378244884312153 max memory_allocated 29281.39697265625 
[2025-03-22 11:47:05 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 14 loss:2.2304558753967285 norm:0.024683544412255287 max memory_allocated 29281.39697265625 
[2025-03-22 11:47:53 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 15 loss:2.2223427295684814 norm:0.021687548607587814 max memory_allocated 29281.39697265625 
[2025-03-22 11:48:41 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 16 loss:2.223076581954956 norm:0.021952494978904724 max memory_allocated 29281.39697265625 
[2025-03-22 11:49:29 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 17 loss:2.2168140411376953 norm:0.018666842952370644 max memory_allocated 29281.39697265625 
[2025-03-22 11:50:17 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 18 loss:2.2135658264160156 norm:0.018304340541362762 max memory_allocated 29281.39697265625 
[2025-03-22 11:51:05 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 19 loss:2.2114920616149902 norm:0.017704715952277184 max memory_allocated 29281.39697265625 
[2025-03-22 11:51:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 39 ===
[2025-03-22 11:51:23 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:52:11 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 0 loss:3.4952216148376465 norm:0.43776124715805054 max memory_allocated 29281.58447265625 
[2025-03-22 11:52:59 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 1 loss:3.260457992553711 norm:0.33258605003356934 max memory_allocated 29281.58447265625 
[2025-03-22 11:53:47 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 2 loss:3.137528419494629 norm:0.256431519985199 max memory_allocated 29281.58447265625 
[2025-03-22 11:54:35 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 3 loss:3.027575969696045 norm:0.18556326627731323 max memory_allocated 29281.58447265625 
[2025-03-22 11:55:23 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 4 loss:2.971803665161133 norm:0.1419837921857834 max memory_allocated 29281.58447265625 
[2025-03-22 11:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 5 loss:2.929530143737793 norm:0.10730183124542236 max memory_allocated 29281.58447265625 
[2025-03-22 11:57:00 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 6 loss:2.906242847442627 norm:0.08768212050199509 max memory_allocated 29281.58447265625 
[2025-03-22 11:57:48 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 7 loss:2.883155584335327 norm:0.07384029030799866 max memory_allocated 29281.58447265625 
[2025-03-22 11:58:37 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 8 loss:2.8769731521606445 norm:0.06804108619689941 max memory_allocated 29281.58447265625 
[2025-03-22 11:59:25 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 9 loss:2.86163067817688 norm:0.06398893892765045 max memory_allocated 29281.58447265625 
[2025-03-22 12:00:13 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 10 loss:2.873589038848877 norm:0.06410873681306839 max memory_allocated 29281.58447265625 
[2025-03-22 12:01:02 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 11 loss:2.8488247394561768 norm:0.0577191598713398 max memory_allocated 29281.58447265625 
[2025-03-22 12:01:50 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 12 loss:2.8412230014801025 norm:0.051641300320625305 max memory_allocated 29281.58447265625 
[2025-03-22 12:02:38 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 13 loss:2.8283488750457764 norm:0.046275876462459564 max memory_allocated 29281.58447265625 
[2025-03-22 12:03:26 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 14 loss:2.8272347450256348 norm:0.04459173604846001 max memory_allocated 29281.58447265625 
[2025-03-22 12:04:14 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 15 loss:2.8196792602539062 norm:0.04349324852228165 max memory_allocated 29281.58447265625 
[2025-03-22 12:05:02 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 16 loss:2.826200008392334 norm:0.04566124826669693 max memory_allocated 29281.58447265625 
[2025-03-22 12:05:50 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 17 loss:2.8268985748291016 norm:0.050078995525836945 max memory_allocated 29281.58447265625 
[2025-03-22 12:06:39 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 18 loss:2.872858762741089 norm:0.07506643980741501 max memory_allocated 29281.58447265625 
[2025-03-22 12:07:27 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 19 loss:2.873566150665283 norm:0.07500999420881271 max memory_allocated 29281.58447265625 
[2025-03-22 12:07:40 root] (main_calibration_a.py 369): INFO 39149.657143354416
[2025-03-22 12:07:55 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 12:09:47 root] (main_calibration_a.py 158): INFO wikitext2 : 7.8834428787231445
[2025-03-22 12:09:47 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 12:12:39 root] (main_calibration_a.py 158): INFO c4 : 11.693465232849121
