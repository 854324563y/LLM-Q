[2025-03-22 14:42:02 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/llama-13b-hf-w4a4-2', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=2)
[2025-03-22 14:48:36 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 14:48:36 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:48:37 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 14:48:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 14:48:43 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:49:31 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.10564219206571579 norm:0.08215760439634323 max memory_allocated 29268.23681640625 
[2025-03-22 14:50:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.05700576305389404 norm:0.03614802286028862 max memory_allocated 29268.23681640625 
[2025-03-22 14:51:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.044386014342308044 norm:0.02294953726232052 max memory_allocated 29268.23681640625 
[2025-03-22 14:51:55 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.039604827761650085 norm:0.019803958013653755 max memory_allocated 29268.23681640625 
[2025-03-22 14:52:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.03738895431160927 norm:0.017049552872776985 max memory_allocated 29268.23681640625 
[2025-03-22 14:53:32 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.035721682012081146 norm:0.01518948469310999 max memory_allocated 29268.23681640625 
[2025-03-22 14:54:20 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.03461955860257149 norm:0.013460516929626465 max memory_allocated 29268.23681640625 
[2025-03-22 14:55:08 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.033921509981155396 norm:0.011865232139825821 max memory_allocated 29268.23681640625 
[2025-03-22 14:55:56 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.03331133723258972 norm:0.0107170594856143 max memory_allocated 29268.23681640625 
[2025-03-22 14:56:44 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.03273627161979675 norm:0.009511844255030155 max memory_allocated 29268.23681640625 
[2025-03-22 14:57:32 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.0325017049908638 norm:0.008670348674058914 max memory_allocated 29268.23681640625 
[2025-03-22 14:58:21 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.03231513500213623 norm:0.008002907037734985 max memory_allocated 29268.23681640625 
[2025-03-22 14:59:09 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.03213370218873024 norm:0.007341752760112286 max memory_allocated 29268.23681640625 
[2025-03-22 14:59:57 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.03203247860074043 norm:0.00675215432420373 max memory_allocated 29268.23681640625 
[2025-03-22 15:00:45 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.03198684751987457 norm:0.006351327523589134 max memory_allocated 29268.23681640625 
[2025-03-22 15:01:33 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.03185996040701866 norm:0.0058189877308905125 max memory_allocated 29268.23681640625 
[2025-03-22 15:02:21 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.03189115598797798 norm:0.00561711797490716 max memory_allocated 29268.23681640625 
[2025-03-22 15:03:10 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.031963035464286804 norm:0.005461584776639938 max memory_allocated 29268.23681640625 
[2025-03-22 15:03:58 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.03185690939426422 norm:0.0052049169316887856 max memory_allocated 29268.23681640625 
[2025-03-22 15:04:46 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.0318562313914299 norm:0.004996706731617451 max memory_allocated 29268.23681640625 
[2025-03-22 15:05:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 15:05:04 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:05:53 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.2800196409225464 norm:0.1295257806777954 max memory_allocated 29268.23681640625 
[2025-03-22 15:06:41 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.17560334503650665 norm:0.05694857984781265 max memory_allocated 29268.23681640625 
[2025-03-22 15:07:29 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.12673768401145935 norm:0.02670552209019661 max memory_allocated 29268.23681640625 
[2025-03-22 15:08:18 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.10625538229942322 norm:0.019682133570313454 max memory_allocated 29268.23681640625 
[2025-03-22 15:09:06 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.09697911888360977 norm:0.016490725800395012 max memory_allocated 29268.23681640625 
[2025-03-22 15:09:55 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.09159894287586212 norm:0.013669168576598167 max memory_allocated 29268.23681640625 
[2025-03-22 15:10:43 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.08804669976234436 norm:0.011982850730419159 max memory_allocated 29268.23681640625 
[2025-03-22 15:11:31 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.08575194329023361 norm:0.010652365162968636 max memory_allocated 29268.23681640625 
[2025-03-22 15:12:20 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.08397120237350464 norm:0.009186286479234695 max memory_allocated 29268.23681640625 
[2025-03-22 15:13:08 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.08283591270446777 norm:0.008175276219844818 max memory_allocated 29268.23681640625 
[2025-03-22 15:13:56 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.0819455161690712 norm:0.007411510217934847 max memory_allocated 29268.23681640625 
[2025-03-22 15:14:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.08116228133440018 norm:0.006702132988721132 max memory_allocated 29268.23681640625 
[2025-03-22 15:15:32 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.08054625988006592 norm:0.006243242882192135 max memory_allocated 29268.23681640625 
[2025-03-22 15:16:21 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.08012193441390991 norm:0.005818076431751251 max memory_allocated 29268.23681640625 
[2025-03-22 15:17:09 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.07974383980035782 norm:0.005776057951152325 max memory_allocated 29268.23681640625 
[2025-03-22 15:17:58 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07944655418395996 norm:0.005821093916893005 max memory_allocated 29268.23681640625 
[2025-03-22 15:18:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.07937934249639511 norm:0.0059151360765099525 max memory_allocated 29268.23681640625 
[2025-03-22 15:19:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07929819822311401 norm:0.0059540243819355965 max memory_allocated 29268.23681640625 
[2025-03-22 15:20:23 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07922869175672531 norm:0.005510389339178801 max memory_allocated 29268.23681640625 
[2025-03-22 15:21:11 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07889304310083389 norm:0.005351105239242315 max memory_allocated 29268.23681640625 
[2025-03-22 15:21:25 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 15:21:29 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:22:17 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.296617716550827 norm:0.05519954487681389 max memory_allocated 29268.23681640625 
[2025-03-22 15:23:06 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.23398791253566742 norm:0.036477308720350266 max memory_allocated 29268.23681640625 
[2025-03-22 15:23:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.20103594660758972 norm:0.02707299217581749 max memory_allocated 29268.23681640625 
[2025-03-22 15:24:43 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.18503129482269287 norm:0.023820556700229645 max memory_allocated 29268.23681640625 
[2025-03-22 15:25:31 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.17600084841251373 norm:0.021156951785087585 max memory_allocated 29268.23681640625 
[2025-03-22 15:26:20 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.17011304199695587 norm:0.019284067675471306 max memory_allocated 29268.23681640625 
[2025-03-22 15:27:08 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.16280579566955566 norm:0.019191669300198555 max memory_allocated 29268.23681640625 
[2025-03-22 15:27:57 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.1587100625038147 norm:0.01978294737637043 max memory_allocated 29268.23681640625 
[2025-03-22 15:28:45 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.15666411817073822 norm:0.01936420053243637 max memory_allocated 29268.23681640625 
[2025-03-22 15:29:33 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.15398558974266052 norm:0.018306050449609756 max memory_allocated 29268.23681640625 
[2025-03-22 15:30:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.15039971470832825 norm:0.01862802356481552 max memory_allocated 29268.23681640625 
[2025-03-22 15:31:10 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.14738823473453522 norm:0.017044909298419952 max memory_allocated 29268.23681640625 
[2025-03-22 15:31:58 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.15051773190498352 norm:0.024508358910679817 max memory_allocated 29268.23681640625 
[2025-03-22 15:32:47 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.14366188645362854 norm:0.017371829599142075 max memory_allocated 29268.23681640625 
[2025-03-22 15:33:35 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.14565709233283997 norm:0.015536738559603691 max memory_allocated 29268.23681640625 
[2025-03-22 15:34:24 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.1461915820837021 norm:0.01659459061920643 max memory_allocated 29268.23681640625 
[2025-03-22 15:35:12 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.14552707970142365 norm:0.01795843057334423 max memory_allocated 29268.23681640625 
[2025-03-22 15:36:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.14428864419460297 norm:0.015355214476585388 max memory_allocated 29268.23681640625 
[2025-03-22 15:36:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.1430196315050125 norm:0.014287417754530907 max memory_allocated 29268.23681640625 
[2025-03-22 15:37:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.14309105277061462 norm:0.015390604734420776 max memory_allocated 29268.23681640625 
[2025-03-22 15:37:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 15:38:44 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.2505790591239929 norm:0.03273171931505203 max memory_allocated 29268.51025390625 
[2025-03-22 15:39:32 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.22008296847343445 norm:0.01440197043120861 max memory_allocated 29268.51025390625 
[2025-03-22 15:40:20 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.1931152492761612 norm:0.006731658708304167 max memory_allocated 29268.51025390625 
[2025-03-22 15:41:09 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.18315882980823517 norm:0.004628587514162064 max memory_allocated 29268.51025390625 
[2025-03-22 15:41:57 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.17834477126598358 norm:0.003695513354614377 max memory_allocated 29268.51025390625 
[2025-03-22 15:42:45 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.17543978989124298 norm:0.0031302175484597683 max memory_allocated 29268.51025390625 
[2025-03-22 15:43:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.1723003387451172 norm:0.0025449309032410383 max memory_allocated 29268.51025390625 
[2025-03-22 15:44:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.1709509938955307 norm:0.0023173519875854254 max memory_allocated 29268.51025390625 
[2025-03-22 15:45:10 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.16974233090877533 norm:0.002092749113216996 max memory_allocated 29268.51025390625 
[2025-03-22 15:45:58 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.1686733514070511 norm:0.0018679104978218675 max memory_allocated 29268.51025390625 
[2025-03-22 15:46:46 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.16759330034255981 norm:0.0017203332390636206 max memory_allocated 29268.51025390625 
[2025-03-22 15:47:34 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.16747625172138214 norm:0.0017409177962690592 max memory_allocated 29268.51025390625 
[2025-03-22 15:48:22 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.16775977611541748 norm:0.001839812146499753 max memory_allocated 29268.51025390625 
[2025-03-22 15:49:11 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.16744613647460938 norm:0.0017777645261958241 max memory_allocated 29268.51025390625 
[2025-03-22 15:49:59 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.16721753776073456 norm:0.0016792620299383998 max memory_allocated 29268.51025390625 
[2025-03-22 15:50:47 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.16687005758285522 norm:0.0015652396250516176 max memory_allocated 29268.51025390625 
[2025-03-22 15:51:36 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.16681939363479614 norm:0.0015346664004027843 max memory_allocated 29268.51025390625 
[2025-03-22 15:52:24 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.16664491593837738 norm:0.0015528323128819466 max memory_allocated 29268.51025390625 
[2025-03-22 15:53:12 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.16626198589801788 norm:0.0015139812603592873 max memory_allocated 29268.51025390625 
[2025-03-22 15:54:01 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.1660892367362976 norm:0.0014687870861962438 max memory_allocated 29268.51025390625 
[2025-03-22 15:54:15 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 15:55:07 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.3024899959564209 norm:0.04752861708402634 max memory_allocated 29268.69775390625 
[2025-03-22 15:55:55 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.26094144582748413 norm:0.012170733883976936 max memory_allocated 29268.69775390625 
[2025-03-22 15:56:44 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.23423437774181366 norm:0.00638030469417572 max memory_allocated 29268.69775390625 
[2025-03-22 15:57:32 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.22564533352851868 norm:0.00472728256136179 max memory_allocated 29268.69775390625 
[2025-03-22 15:58:20 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.22124715149402618 norm:0.0038897739723324776 max memory_allocated 29268.69775390625 
[2025-03-22 15:59:09 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.21868136525154114 norm:0.0035476461052894592 max memory_allocated 29268.69775390625 
[2025-03-22 15:59:57 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.21683484315872192 norm:0.003326806239783764 max memory_allocated 29268.69775390625 
[2025-03-22 16:00:46 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.21484068036079407 norm:0.002768187550827861 max memory_allocated 29268.69775390625 
[2025-03-22 16:01:34 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.21406635642051697 norm:0.0025882578920572996 max memory_allocated 29268.69775390625 
[2025-03-22 16:02:22 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.21288169920444489 norm:0.0022809812799096107 max memory_allocated 29268.69775390625 
[2025-03-22 16:03:11 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.21205580234527588 norm:0.0020455727353692055 max memory_allocated 29268.69775390625 
[2025-03-22 16:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.21188823878765106 norm:0.0020554529037326574 max memory_allocated 29268.69775390625 
[2025-03-22 16:04:47 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.2119879126548767 norm:0.0020405801478773355 max memory_allocated 29268.69775390625 
[2025-03-22 16:05:35 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.21240004897117615 norm:0.002117101103067398 max memory_allocated 29268.69775390625 
[2025-03-22 16:06:23 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.2123386263847351 norm:0.002094554016366601 max memory_allocated 29268.69775390625 
[2025-03-22 16:07:12 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.2117340862751007 norm:0.001867171609774232 max memory_allocated 29268.69775390625 
[2025-03-22 16:08:00 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.21119807660579681 norm:0.0017870612209662795 max memory_allocated 29268.69775390625 
[2025-03-22 16:08:48 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.210148885846138 norm:0.0015613444847986102 max memory_allocated 29268.69775390625 
[2025-03-22 16:09:37 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.2100757360458374 norm:0.0015300221275538206 max memory_allocated 29268.69775390625 
[2025-03-22 16:10:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.21021141111850739 norm:0.0015468905912712216 max memory_allocated 29268.69775390625 
[2025-03-22 16:10:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 16:11:31 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.35568562150001526 norm:0.0621202290058136 max memory_allocated 29268.88525390625 
[2025-03-22 16:12:19 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.30873802304267883 norm:0.021415947005152702 max memory_allocated 29268.88525390625 
[2025-03-22 16:13:08 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.2674940824508667 norm:0.006466974038630724 max memory_allocated 29268.88525390625 
[2025-03-22 16:13:56 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.25592949986457825 norm:0.004477928392589092 max memory_allocated 29268.88525390625 
[2025-03-22 16:14:44 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.2506730258464813 norm:0.0036678409669548273 max memory_allocated 29268.88525390625 
[2025-03-22 16:15:33 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.2471294403076172 norm:0.0032157746609300375 max memory_allocated 29268.88525390625 
[2025-03-22 16:16:21 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.244768887758255 norm:0.002931870287284255 max memory_allocated 29268.88525390625 
[2025-03-22 16:17:10 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.24308525025844574 norm:0.002712335204705596 max memory_allocated 29268.88525390625 
[2025-03-22 16:17:58 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.2421044111251831 norm:0.002647091168910265 max memory_allocated 29268.88525390625 
[2025-03-22 16:18:47 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.24149098992347717 norm:0.0025694826617836952 max memory_allocated 29268.88525390625 
[2025-03-22 16:19:35 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.240549698472023 norm:0.0022818283177912235 max memory_allocated 29268.88525390625 
[2025-03-22 16:20:24 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.2398679256439209 norm:0.002127457642927766 max memory_allocated 29268.88525390625 
[2025-03-22 16:21:12 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.23975685238838196 norm:0.0020891453605145216 max memory_allocated 29268.88525390625 
[2025-03-22 16:22:00 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.23970545828342438 norm:0.0021829919423907995 max memory_allocated 29268.88525390625 
[2025-03-22 16:22:49 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.23938506841659546 norm:0.002047397894784808 max memory_allocated 29268.88525390625 
[2025-03-22 16:23:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.23877550661563873 norm:0.0018665072275325656 max memory_allocated 29268.88525390625 
[2025-03-22 16:24:26 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.23781026899814606 norm:0.0015799097018316388 max memory_allocated 29268.88525390625 
[2025-03-22 16:25:14 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.23708590865135193 norm:0.0014653857797384262 max memory_allocated 29268.88525390625 
[2025-03-22 16:26:02 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.23701032996177673 norm:0.001428996678441763 max memory_allocated 29268.88525390625 
[2025-03-22 16:26:51 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.23703661561012268 norm:0.001440722495317459 max memory_allocated 29268.88525390625 
[2025-03-22 16:27:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 16:27:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.40531712770462036 norm:0.015661969780921936 max memory_allocated 29269.07275390625 
[2025-03-22 16:28:45 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.36997342109680176 norm:0.008214072324335575 max memory_allocated 29269.07275390625 
[2025-03-22 16:29:33 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.3363456130027771 norm:0.004971817135810852 max memory_allocated 29269.07275390625 
[2025-03-22 16:30:21 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.32733839750289917 norm:0.0036991294473409653 max memory_allocated 29269.07275390625 
[2025-03-22 16:31:09 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.3207396864891052 norm:0.0033595990389585495 max memory_allocated 29269.07275390625 
[2025-03-22 16:31:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.3169666528701782 norm:0.004053543321788311 max memory_allocated 29269.07275390625 
[2025-03-22 16:32:46 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.3096473217010498 norm:0.003216599579900503 max memory_allocated 29269.07275390625 
[2025-03-22 16:33:34 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.30644547939300537 norm:0.0031288103200495243 max memory_allocated 29269.07275390625 
[2025-03-22 16:34:23 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.30409345030784607 norm:0.0029924637638032436 max memory_allocated 29269.07275390625 
[2025-03-22 16:35:11 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.3027452826499939 norm:0.0029332637786865234 max memory_allocated 29269.07275390625 
[2025-03-22 16:35:59 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.3015974760055542 norm:0.0029267321806401014 max memory_allocated 29269.07275390625 
[2025-03-22 16:36:48 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.2994343638420105 norm:0.0028158302884548903 max memory_allocated 29269.07275390625 
[2025-03-22 16:37:36 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.29901862144470215 norm:0.0029493665788322687 max memory_allocated 29269.07275390625 
[2025-03-22 16:38:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.29914066195487976 norm:0.002987287938594818 max memory_allocated 29269.07275390625 
[2025-03-22 16:39:13 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.2984215021133423 norm:0.003073193831369281 max memory_allocated 29269.07275390625 
[2025-03-22 16:40:02 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.2990606427192688 norm:0.0030186877120286226 max memory_allocated 29269.07275390625 
[2025-03-22 16:40:50 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.2980409860610962 norm:0.003128495765849948 max memory_allocated 29269.07275390625 
[2025-03-22 16:41:39 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.2987237274646759 norm:0.003042357275262475 max memory_allocated 29269.07275390625 
[2025-03-22 16:42:27 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.2984441816806793 norm:0.0029375760350376368 max memory_allocated 29269.07275390625 
[2025-03-22 16:43:16 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.29852110147476196 norm:0.0030646082013845444 max memory_allocated 29269.07275390625 
[2025-03-22 16:43:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 16:44:22 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.42234763503074646 norm:0.019311487674713135 max memory_allocated 29269.26025390625 
[2025-03-22 16:45:10 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.3899368941783905 norm:0.009355710819363594 max memory_allocated 29269.26025390625 
[2025-03-22 16:45:58 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.3537124991416931 norm:0.004348001442849636 max memory_allocated 29269.26025390625 
[2025-03-22 16:46:47 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.3360427916049957 norm:0.0023086806759238243 max memory_allocated 29269.26025390625 
[2025-03-22 16:47:35 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.32964515686035156 norm:0.001768380170688033 max memory_allocated 29269.26025390625 
[2025-03-22 16:48:23 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.3252539038658142 norm:0.0014527570456266403 max memory_allocated 29269.26025390625 
[2025-03-22 16:49:11 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.32180145382881165 norm:0.0012671973090618849 max memory_allocated 29269.26025390625 
[2025-03-22 16:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.3194819986820221 norm:0.00114109693095088 max memory_allocated 29269.26025390625 
[2025-03-22 16:50:48 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.3178154528141022 norm:0.0010776328854262829 max memory_allocated 29269.26025390625 
[2025-03-22 16:51:36 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.316618949174881 norm:0.0010489283595234156 max memory_allocated 29269.26025390625 
[2025-03-22 16:52:24 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.31584668159484863 norm:0.0010643554851412773 max memory_allocated 29269.26025390625 
[2025-03-22 16:53:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.31508612632751465 norm:0.0010511266300454736 max memory_allocated 29269.26025390625 
[2025-03-22 16:54:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.3145252466201782 norm:0.0010331568773835897 max memory_allocated 29269.26025390625 
[2025-03-22 16:54:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.3141021132469177 norm:0.000993636786006391 max memory_allocated 29269.26025390625 
[2025-03-22 16:55:38 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.3138158917427063 norm:0.001006108708679676 max memory_allocated 29269.26025390625 
[2025-03-22 16:56:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.31357577443122864 norm:0.0009645281825214624 max memory_allocated 29269.26025390625 
[2025-03-22 16:57:14 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.31336355209350586 norm:0.0009496206766925752 max memory_allocated 29269.26025390625 
[2025-03-22 16:58:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.31311118602752686 norm:0.0009308503358624876 max memory_allocated 29269.26025390625 
[2025-03-22 16:58:51 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.31305235624313354 norm:0.0009401277638971806 max memory_allocated 29269.26025390625 
[2025-03-22 16:59:40 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.3129958212375641 norm:0.0009421958820894361 max memory_allocated 29269.26025390625 
[2025-03-22 16:59:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 17:00:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.4433009922504425 norm:0.017789185047149658 max memory_allocated 29269.44775390625 
[2025-03-22 17:01:34 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.41189736127853394 norm:0.009753013029694557 max memory_allocated 29269.44775390625 
[2025-03-22 17:02:23 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.3723825514316559 norm:0.0042505147866904736 max memory_allocated 29269.44775390625 
[2025-03-22 17:03:11 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.35549208521842957 norm:0.0022555929608643055 max memory_allocated 29269.44775390625 
[2025-03-22 17:04:00 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.349263995885849 norm:0.0017789454432204366 max memory_allocated 29269.44775390625 
[2025-03-22 17:04:48 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.34503135085105896 norm:0.0013861164916306734 max memory_allocated 29269.44775390625 
[2025-03-22 17:05:37 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.3419038653373718 norm:0.0012254908215254545 max memory_allocated 29269.44775390625 
[2025-03-22 17:06:25 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.3396208584308624 norm:0.0011401396477594972 max memory_allocated 29269.44775390625 
[2025-03-22 17:07:13 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.33795008063316345 norm:0.0010504091624170542 max memory_allocated 29269.44775390625 
[2025-03-22 17:08:02 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.33684101700782776 norm:0.0010073258308693767 max memory_allocated 29269.44775390625 
[2025-03-22 17:08:50 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.3360029458999634 norm:0.000988033483736217 max memory_allocated 29269.44775390625 
[2025-03-22 17:09:38 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.33540162444114685 norm:0.0009539463208056986 max memory_allocated 29269.44775390625 
[2025-03-22 17:10:27 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.3348723351955414 norm:0.0009262188104912639 max memory_allocated 29269.44775390625 
[2025-03-22 17:11:15 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.33433297276496887 norm:0.0009118247544392943 max memory_allocated 29269.44775390625 
[2025-03-22 17:12:03 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.3340495526790619 norm:0.0009192353463731706 max memory_allocated 29269.44775390625 
[2025-03-22 17:12:51 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.3338284492492676 norm:0.0009080498712137341 max memory_allocated 29269.44775390625 
[2025-03-22 17:13:39 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.33361339569091797 norm:0.0009048799402080476 max memory_allocated 29269.44775390625 
[2025-03-22 17:14:28 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.33346670866012573 norm:0.0008830775041133165 max memory_allocated 29269.44775390625 
[2025-03-22 17:15:16 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.3332083523273468 norm:0.0008823903044685721 max memory_allocated 29269.44775390625 
[2025-03-22 17:16:04 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.33326900005340576 norm:0.0008696889854036272 max memory_allocated 29269.44775390625 
[2025-03-22 17:16:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 17:17:11 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.4768749177455902 norm:0.017386866733431816 max memory_allocated 29269.63525390625 
[2025-03-22 17:17:59 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.43701618909835815 norm:0.009097881615161896 max memory_allocated 29269.63525390625 
[2025-03-22 17:18:47 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.3895095884799957 norm:0.0035030324943363667 max memory_allocated 29269.63525390625 
[2025-03-22 17:19:36 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.3734697103500366 norm:0.0018108569784089923 max memory_allocated 29269.63525390625 
[2025-03-22 17:20:24 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.36779046058654785 norm:0.001380715286359191 max memory_allocated 29269.63525390625 
[2025-03-22 17:21:13 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3642056882381439 norm:0.0011984894517809153 max memory_allocated 29269.63525390625 
[2025-03-22 17:22:01 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.3616487979888916 norm:0.0010784220648929477 max memory_allocated 29269.63525390625 
[2025-03-22 17:22:50 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.3595767617225647 norm:0.0010127696441486478 max memory_allocated 29269.63525390625 
[2025-03-22 17:23:38 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.3581124544143677 norm:0.0009841606952250004 max memory_allocated 29269.63525390625 
[2025-03-22 17:24:26 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.35703417658805847 norm:0.0009510809322819114 max memory_allocated 29269.63525390625 
[2025-03-22 17:25:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.35625627636909485 norm:0.0009319900418631732 max memory_allocated 29269.63525390625 
[2025-03-22 17:26:03 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.35557398200035095 norm:0.0009082333417609334 max memory_allocated 29269.63525390625 
[2025-03-22 17:26:51 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.3549571633338928 norm:0.0008974936790764332 max memory_allocated 29269.63525390625 
[2025-03-22 17:27:39 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.35452592372894287 norm:0.000883203640114516 max memory_allocated 29269.63525390625 
[2025-03-22 17:28:28 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.3542429506778717 norm:0.0008730233530513942 max memory_allocated 29269.63525390625 
[2025-03-22 17:29:16 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.35397180914878845 norm:0.0008648319053463638 max memory_allocated 29269.63525390625 
[2025-03-22 17:30:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.35376203060150146 norm:0.0008624031906947494 max memory_allocated 29269.63525390625 
[2025-03-22 17:30:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.35357123613357544 norm:0.0008535258821211755 max memory_allocated 29269.63525390625 
[2025-03-22 17:31:41 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.35341930389404297 norm:0.0008440058445557952 max memory_allocated 29269.63525390625 
[2025-03-22 17:32:29 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.3532657325267792 norm:0.0008303256472572684 max memory_allocated 29269.63525390625 
[2025-03-22 17:32:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 17:33:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.47640901803970337 norm:0.028646940365433693 max memory_allocated 29269.82275390625 
[2025-03-22 17:34:23 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.4433881640434265 norm:0.011403761804103851 max memory_allocated 29269.82275390625 
[2025-03-22 17:35:12 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.40731868147850037 norm:0.0038545511197298765 max memory_allocated 29269.82275390625 
[2025-03-22 17:36:00 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.39217472076416016 norm:0.0018632555147632957 max memory_allocated 29269.82275390625 
[2025-03-22 17:36:49 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.3865888714790344 norm:0.001411075354553759 max memory_allocated 29269.82275390625 
[2025-03-22 17:37:37 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3830852806568146 norm:0.001229366404004395 max memory_allocated 29269.82275390625 
[2025-03-22 17:38:26 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.38060271739959717 norm:0.0011218246072530746 max memory_allocated 29269.82275390625 
[2025-03-22 17:39:14 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.3786955773830414 norm:0.0010343770263716578 max memory_allocated 29269.82275390625 
[2025-03-22 17:40:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.37744802236557007 norm:0.0009867605986073613 max memory_allocated 29269.82275390625 
[2025-03-22 17:40:51 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.3765079081058502 norm:0.0009562850464135408 max memory_allocated 29269.82275390625 
[2025-03-22 17:41:39 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.3757005035877228 norm:0.0009240460931323469 max memory_allocated 29269.82275390625 
[2025-03-22 17:42:28 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.374997079372406 norm:0.0009040000149980187 max memory_allocated 29269.82275390625 
[2025-03-22 17:43:16 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.3744626045227051 norm:0.000880024628713727 max memory_allocated 29269.82275390625 
[2025-03-22 17:44:05 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3741131126880646 norm:0.0008446081774309278 max memory_allocated 29269.82275390625 
[2025-03-22 17:44:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3737710416316986 norm:0.0008278796449303627 max memory_allocated 29269.82275390625 
[2025-03-22 17:45:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.3733188509941101 norm:0.0008198305731639266 max memory_allocated 29269.82275390625 
[2025-03-22 17:46:29 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.3730279207229614 norm:0.0008138459525071084 max memory_allocated 29269.82275390625 
[2025-03-22 17:47:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.37296098470687866 norm:0.0007969310972839594 max memory_allocated 29269.82275390625 
[2025-03-22 17:48:06 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.3729672431945801 norm:0.0007900928612798452 max memory_allocated 29269.82275390625 
[2025-03-22 17:48:54 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.3728354573249817 norm:0.000780897680670023 max memory_allocated 29269.82275390625 
[2025-03-22 17:49:09 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 17:50:01 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.5052216649055481 norm:0.020133070647716522 max memory_allocated 29270.01025390625 
[2025-03-22 17:50:49 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.4717905521392822 norm:0.0088117066770792 max memory_allocated 29270.01025390625 
[2025-03-22 17:51:37 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.4306994080543518 norm:0.003499198704957962 max memory_allocated 29270.01025390625 
[2025-03-22 17:52:26 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.4143182635307312 norm:0.0016592631582170725 max memory_allocated 29270.01025390625 
[2025-03-22 17:53:14 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.40882307291030884 norm:0.0012347735464572906 max memory_allocated 29270.01025390625 
[2025-03-22 17:54:03 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.40535956621170044 norm:0.0010631748009473085 max memory_allocated 29270.01025390625 
[2025-03-22 17:54:51 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.40276944637298584 norm:0.0009767417795956135 max memory_allocated 29270.01025390625 
[2025-03-22 17:55:39 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.400909960269928 norm:0.0009159243199974298 max memory_allocated 29270.01025390625 
[2025-03-22 17:56:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.39959481358528137 norm:0.0008950226474553347 max memory_allocated 29270.01025390625 
[2025-03-22 17:57:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.398477166891098 norm:0.0008794810273684561 max memory_allocated 29270.01025390625 
[2025-03-22 17:58:05 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.3976232707500458 norm:0.0008661766187287867 max memory_allocated 29270.01025390625 
[2025-03-22 17:58:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.3969573378562927 norm:0.0008473513880744576 max memory_allocated 29270.01025390625 
[2025-03-22 17:59:42 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3963541090488434 norm:0.0008266372606158257 max memory_allocated 29270.01025390625 
[2025-03-22 18:00:30 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.3959610164165497 norm:0.0008197461138479412 max memory_allocated 29270.01025390625 
[2025-03-22 18:01:18 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3956057131290436 norm:0.000789562938734889 max memory_allocated 29270.01025390625 
[2025-03-22 18:02:07 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.3953593969345093 norm:0.0007910503773018718 max memory_allocated 29270.01025390625 
[2025-03-22 18:02:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.3952213227748871 norm:0.0007924080127850175 max memory_allocated 29270.01025390625 
[2025-03-22 18:03:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.39507344365119934 norm:0.0007802851614542305 max memory_allocated 29270.01025390625 
[2025-03-22 18:04:31 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.39499813318252563 norm:0.000782271905336529 max memory_allocated 29270.01025390625 
[2025-03-22 18:05:20 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.3948728144168854 norm:0.0007622624398209155 max memory_allocated 29270.01025390625 
[2025-03-22 18:05:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 18:06:25 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.5189719200134277 norm:0.021997347474098206 max memory_allocated 29270.19775390625 
[2025-03-22 18:07:14 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.4863053858280182 norm:0.008251707069575787 max memory_allocated 29270.19775390625 
[2025-03-22 18:08:02 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.45410943031311035 norm:0.0037459214217960835 max memory_allocated 29270.19775390625 
[2025-03-22 18:08:50 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.4384450614452362 norm:0.0017229561926797032 max memory_allocated 29270.19775390625 
[2025-03-22 18:09:39 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.43224552273750305 norm:0.0012930667726323009 max memory_allocated 29270.19775390625 
[2025-03-22 18:10:27 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.4282025992870331 norm:0.0011109707411378622 max memory_allocated 29270.19775390625 
[2025-03-22 18:11:15 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.42563396692276 norm:0.0010276532266288996 max memory_allocated 29270.19775390625 
[2025-03-22 18:12:04 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.4235329031944275 norm:0.0009645168902352452 max memory_allocated 29270.19775390625 
[2025-03-22 18:12:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.42193204164505005 norm:0.0009274613694287837 max memory_allocated 29270.19775390625 
[2025-03-22 18:13:41 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.4207717776298523 norm:0.0009120061295107007 max memory_allocated 29270.19775390625 
[2025-03-22 18:14:29 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.41993412375450134 norm:0.0009098313166759908 max memory_allocated 29270.19775390625 
[2025-03-22 18:15:18 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.4192872941493988 norm:0.0008993512019515038 max memory_allocated 29270.19775390625 
[2025-03-22 18:16:06 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.41855502128601074 norm:0.0008848055731505156 max memory_allocated 29270.19775390625 
[2025-03-22 18:16:54 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.41808778047561646 norm:0.0008898195228539407 max memory_allocated 29270.19775390625 
[2025-03-22 18:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.4175468683242798 norm:0.0008703731000423431 max memory_allocated 29270.19775390625 
[2025-03-22 18:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.41714656352996826 norm:0.0008715579751878977 max memory_allocated 29270.19775390625 
[2025-03-22 18:19:19 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.416922003030777 norm:0.000863588007632643 max memory_allocated 29270.19775390625 
[2025-03-22 18:20:08 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.41663169860839844 norm:0.000861643347889185 max memory_allocated 29270.19775390625 
[2025-03-22 18:20:56 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.41635486483573914 norm:0.0008530566119588912 max memory_allocated 29270.19775390625 
[2025-03-22 18:21:44 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.4162088930606842 norm:0.0008579063578508794 max memory_allocated 29270.19775390625 
[2025-03-22 18:21:58 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 18:22:50 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.5235685110092163 norm:0.008034070953726768 max memory_allocated 29270.38525390625 
[2025-03-22 18:23:38 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.5009621381759644 norm:0.004871149081736803 max memory_allocated 29270.38525390625 
[2025-03-22 18:24:26 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.472611665725708 norm:0.002515906235203147 max memory_allocated 29270.38525390625 
[2025-03-22 18:25:15 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.4599064886569977 norm:0.0015269089490175247 max memory_allocated 29270.38525390625 
[2025-03-22 18:26:03 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.4541967809200287 norm:0.0012279953807592392 max memory_allocated 29270.38525390625 
[2025-03-22 18:26:51 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.45093590021133423 norm:0.001129649579524994 max memory_allocated 29270.38525390625 
[2025-03-22 18:27:40 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.44829174876213074 norm:0.0010572414612397552 max memory_allocated 29270.38525390625 
[2025-03-22 18:28:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.44630002975463867 norm:0.001020026276819408 max memory_allocated 29270.38525390625 
[2025-03-22 18:29:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.4445803761482239 norm:0.0009888531640172005 max memory_allocated 29270.38525390625 
[2025-03-22 18:30:05 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.4433120787143707 norm:0.0009694158216007054 max memory_allocated 29270.38525390625 
[2025-03-22 18:30:54 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.4422840178012848 norm:0.0009461887530051172 max memory_allocated 29270.38525390625 
[2025-03-22 18:31:42 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.4416212737560272 norm:0.0009506492642685771 max memory_allocated 29270.38525390625 
[2025-03-22 18:32:31 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.44089025259017944 norm:0.0009355320944450796 max memory_allocated 29270.38525390625 
[2025-03-22 18:33:19 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.44033369421958923 norm:0.0009339129319414496 max memory_allocated 29270.38525390625 
[2025-03-22 18:34:07 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.43992486596107483 norm:0.0009287887951359153 max memory_allocated 29270.38525390625 
[2025-03-22 18:34:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.4394604563713074 norm:0.0009281343081966043 max memory_allocated 29270.38525390625 
[2025-03-22 18:35:44 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.4391855001449585 norm:0.0009425307507626712 max memory_allocated 29270.38525390625 
[2025-03-22 18:36:32 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.4389398694038391 norm:0.0009321855031885207 max memory_allocated 29270.38525390625 
[2025-03-22 18:37:20 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.4388046860694885 norm:0.000919478596188128 max memory_allocated 29270.38525390625 
[2025-03-22 18:38:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.4386425316333771 norm:0.0009188131080009043 max memory_allocated 29270.38525390625 
[2025-03-22 18:38:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 18:39:14 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.5738162994384766 norm:0.03283175081014633 max memory_allocated 29270.57275390625 
[2025-03-22 18:40:02 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.5454347729682922 norm:0.01725088432431221 max memory_allocated 29270.57275390625 
[2025-03-22 18:40:50 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.5085893869400024 norm:0.008492819033563137 max memory_allocated 29270.57275390625 
[2025-03-22 18:41:39 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.4904879927635193 norm:0.004914104472845793 max memory_allocated 29270.57275390625 
[2025-03-22 18:42:27 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.48185083270072937 norm:0.003095981664955616 max memory_allocated 29270.57275390625 
[2025-03-22 18:43:15 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.4764031767845154 norm:0.0017852152232080698 max memory_allocated 29270.57275390625 
[2025-03-22 18:44:04 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.4730844497680664 norm:0.0015157589223235846 max memory_allocated 29270.57275390625 
[2025-03-22 18:44:52 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.4706951975822449 norm:0.0013890224508941174 max memory_allocated 29270.57275390625 
[2025-03-22 18:45:41 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.4687245488166809 norm:0.0011410090373829007 max memory_allocated 29270.57275390625 
[2025-03-22 18:46:29 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.4671917259693146 norm:0.001084235729649663 max memory_allocated 29270.57275390625 
[2025-03-22 18:47:18 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.46588337421417236 norm:0.0010397112928330898 max memory_allocated 29270.57275390625 
[2025-03-22 18:48:06 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.4650423526763916 norm:0.0010309482458978891 max memory_allocated 29270.57275390625 
[2025-03-22 18:48:54 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.4642578363418579 norm:0.0010043662041425705 max memory_allocated 29270.57275390625 
[2025-03-22 18:49:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.4636562168598175 norm:0.0009868080960586667 max memory_allocated 29270.57275390625 
[2025-03-22 18:50:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.46317020058631897 norm:0.000984497251920402 max memory_allocated 29270.57275390625 
[2025-03-22 18:51:20 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.46269553899765015 norm:0.0009773956844583154 max memory_allocated 29270.57275390625 
[2025-03-22 18:52:08 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.4623045325279236 norm:0.0009849203051999211 max memory_allocated 29270.57275390625 
[2025-03-22 18:52:57 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.4619729518890381 norm:0.0009713711915537715 max memory_allocated 29270.57275390625 
[2025-03-22 18:53:45 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.4617646634578705 norm:0.000974629248958081 max memory_allocated 29270.57275390625 
[2025-03-22 18:54:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.4616415798664093 norm:0.0009715085616335273 max memory_allocated 29270.57275390625 
[2025-03-22 18:54:47 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 18:55:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.5945861339569092 norm:0.06453175097703934 max memory_allocated 29270.76025390625 
[2025-03-22 18:56:28 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.5765545964241028 norm:0.03579637408256531 max memory_allocated 29270.76025390625 
[2025-03-22 18:57:16 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.542814314365387 norm:0.01344587467610836 max memory_allocated 29270.76025390625 
[2025-03-22 18:58:04 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.5251940488815308 norm:0.006309334188699722 max memory_allocated 29270.76025390625 
[2025-03-22 18:58:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.5178042054176331 norm:0.004204733297228813 max memory_allocated 29270.76025390625 
[2025-03-22 18:59:40 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.5131338834762573 norm:0.002823173999786377 max memory_allocated 29270.76025390625 
[2025-03-22 19:00:29 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.5100154876708984 norm:0.002582521177828312 max memory_allocated 29270.76025390625 
[2025-03-22 19:01:17 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.5077285766601562 norm:0.002365163993090391 max memory_allocated 29270.76025390625 
[2025-03-22 19:02:05 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.50529545545578 norm:0.0015049907378852367 max memory_allocated 29270.76025390625 
[2025-03-22 19:02:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.5037272572517395 norm:0.0014388944255188107 max memory_allocated 29270.76025390625 
[2025-03-22 19:03:42 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.5025438070297241 norm:0.001373086590319872 max memory_allocated 29270.76025390625 
[2025-03-22 19:04:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.5015942454338074 norm:0.001340671326033771 max memory_allocated 29270.76025390625 
[2025-03-22 19:05:19 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.5009384155273438 norm:0.0013382217148318887 max memory_allocated 29270.76025390625 
[2025-03-22 19:06:07 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.5003256797790527 norm:0.0013157229404896498 max memory_allocated 29270.76025390625 
[2025-03-22 19:06:55 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.49990350008010864 norm:0.0013011391274631023 max memory_allocated 29270.76025390625 
[2025-03-22 19:07:44 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.4994294047355652 norm:0.0012689391151070595 max memory_allocated 29270.76025390625 
[2025-03-22 19:08:32 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.49895012378692627 norm:0.0012424795422703028 max memory_allocated 29270.76025390625 
[2025-03-22 19:09:21 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.49839991331100464 norm:0.0012329911114647985 max memory_allocated 29270.76025390625 
[2025-03-22 19:10:09 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.4981864094734192 norm:0.001223803381435573 max memory_allocated 29270.76025390625 
[2025-03-22 19:10:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.49780407547950745 norm:0.0011924577411264181 max memory_allocated 29270.76025390625 
[2025-03-22 19:11:11 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 19:12:04 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.6049643158912659 norm:0.014696796424686909 max memory_allocated 29270.94775390625 
[2025-03-22 19:12:52 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.5818624496459961 norm:0.007705777417868376 max memory_allocated 29270.94775390625 
[2025-03-22 19:13:40 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.5593385696411133 norm:0.003995239268988371 max memory_allocated 29270.94775390625 
[2025-03-22 19:14:29 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.547879695892334 norm:0.002143050543963909 max memory_allocated 29270.94775390625 
[2025-03-22 19:15:17 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.5432574152946472 norm:0.0015345936408266425 max memory_allocated 29270.94775390625 
[2025-03-22 19:16:05 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.540617823600769 norm:0.0013342639431357384 max memory_allocated 29270.94775390625 
[2025-03-22 19:16:53 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.5383187532424927 norm:0.0012033614329993725 max memory_allocated 29270.94775390625 
[2025-03-22 19:17:42 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.5366018414497375 norm:0.0011464313138276339 max memory_allocated 29270.94775390625 
[2025-03-22 19:18:30 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.5351965427398682 norm:0.0011089411564171314 max memory_allocated 29270.94775390625 
[2025-03-22 19:19:18 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.5337696671485901 norm:0.0010740678990259767 max memory_allocated 29270.94775390625 
[2025-03-22 19:20:06 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.5329820513725281 norm:0.0010657991515472531 max memory_allocated 29270.94775390625 
[2025-03-22 19:20:54 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.532277524471283 norm:0.0010481772478669882 max memory_allocated 29270.94775390625 
[2025-03-22 19:21:43 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.5315768122673035 norm:0.0010176629293709993 max memory_allocated 29270.94775390625 
[2025-03-22 19:22:31 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.5309644937515259 norm:0.000991286477074027 max memory_allocated 29270.94775390625 
[2025-03-22 19:23:20 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.5304998159408569 norm:0.0009818688267841935 max memory_allocated 29270.94775390625 
[2025-03-22 19:24:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.529910683631897 norm:0.0009556045988574624 max memory_allocated 29270.94775390625 
[2025-03-22 19:24:56 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.5295370817184448 norm:0.0009374282089993358 max memory_allocated 29270.94775390625 
[2025-03-22 19:25:45 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.5293064713478088 norm:0.0009314185008406639 max memory_allocated 29270.94775390625 
[2025-03-22 19:26:33 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.529066801071167 norm:0.0009319806122221053 max memory_allocated 29270.94775390625 
[2025-03-22 19:27:22 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.5287740230560303 norm:0.0009202369255945086 max memory_allocated 29270.94775390625 
[2025-03-22 19:27:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 19:28:28 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.650775134563446 norm:0.012416747398674488 max memory_allocated 29271.13525390625 
[2025-03-22 19:29:16 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.6308082938194275 norm:0.00733353104442358 max memory_allocated 29271.13525390625 
[2025-03-22 19:30:05 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.6027867794036865 norm:0.0036989145446568727 max memory_allocated 29271.13525390625 
[2025-03-22 19:30:53 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.5917108654975891 norm:0.0023783836513757706 max memory_allocated 29271.13525390625 
[2025-03-22 19:31:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.5868480801582336 norm:0.001824321923777461 max memory_allocated 29271.13525390625 
[2025-03-22 19:32:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.583344042301178 norm:0.0014762512873858213 max memory_allocated 29271.13525390625 
[2025-03-22 19:33:18 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.5808569192886353 norm:0.0013181488029658794 max memory_allocated 29271.13525390625 
[2025-03-22 19:34:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.5790475606918335 norm:0.0012339823879301548 max memory_allocated 29271.13525390625 
[2025-03-22 19:34:55 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.5774605870246887 norm:0.0011699346359819174 max memory_allocated 29271.13525390625 
[2025-03-22 19:35:43 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.5762197375297546 norm:0.001134154386818409 max memory_allocated 29271.13525390625 
[2025-03-22 19:36:31 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.5752657651901245 norm:0.0011039249366149306 max memory_allocated 29271.13525390625 
[2025-03-22 19:37:19 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.5744685530662537 norm:0.0010836238507181406 max memory_allocated 29271.13525390625 
[2025-03-22 19:38:07 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.5737418532371521 norm:0.0010744215687736869 max memory_allocated 29271.13525390625 
[2025-03-22 19:38:56 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.5732125043869019 norm:0.0010655662044882774 max memory_allocated 29271.13525390625 
[2025-03-22 19:39:44 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.5727525949478149 norm:0.0010436683660373092 max memory_allocated 29271.13525390625 
[2025-03-22 19:40:32 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.5723028182983398 norm:0.0010221682023257017 max memory_allocated 29271.13525390625 
[2025-03-22 19:41:21 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.5718721747398376 norm:0.0009925898630172014 max memory_allocated 29271.13525390625 
[2025-03-22 19:42:09 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.5714737176895142 norm:0.0009736070642247796 max memory_allocated 29271.13525390625 
[2025-03-22 19:42:58 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.5711972713470459 norm:0.0009668389102444053 max memory_allocated 29271.13525390625 
[2025-03-22 19:43:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.5709993839263916 norm:0.000961987825576216 max memory_allocated 29271.13525390625 
[2025-03-22 19:44:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 19:44:52 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.7330482006072998 norm:0.05368223786354065 max memory_allocated 29271.32275390625 
[2025-03-22 19:45:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.7043777704238892 norm:0.028505517169833183 max memory_allocated 29271.32275390625 
[2025-03-22 19:46:29 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.6741718649864197 norm:0.014195993542671204 max memory_allocated 29271.32275390625 
[2025-03-22 19:47:17 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.658721923828125 norm:0.008100408129394054 max memory_allocated 29271.32275390625 
[2025-03-22 19:48:06 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.6519055366516113 norm:0.005378250498324633 max memory_allocated 29271.32275390625 
[2025-03-22 19:48:54 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.6476605534553528 norm:0.004049043171107769 max memory_allocated 29271.32275390625 
[2025-03-22 19:49:42 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.6446096897125244 norm:0.0035123536363244057 max memory_allocated 29271.32275390625 
[2025-03-22 19:50:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.6418817043304443 norm:0.002886222442612052 max memory_allocated 29271.32275390625 
[2025-03-22 19:51:19 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.639568567276001 norm:0.002360125770792365 max memory_allocated 29271.32275390625 
[2025-03-22 19:52:07 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.6381812691688538 norm:0.002348406007513404 max memory_allocated 29271.32275390625 
[2025-03-22 19:52:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.6371046304702759 norm:0.0022896872833371162 max memory_allocated 29271.32275390625 
[2025-03-22 19:53:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.6362445950508118 norm:0.0022010551765561104 max memory_allocated 29271.32275390625 
[2025-03-22 19:54:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.635492742061615 norm:0.002087770029902458 max memory_allocated 29271.32275390625 
[2025-03-22 19:55:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.6348392963409424 norm:0.002007122617214918 max memory_allocated 29271.32275390625 
[2025-03-22 19:56:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.6341480612754822 norm:0.001834687776863575 max memory_allocated 29271.32275390625 
[2025-03-22 19:56:57 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.6336565613746643 norm:0.0018295448971912265 max memory_allocated 29271.32275390625 
[2025-03-22 19:57:45 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.6324926018714905 norm:0.0013242127606645226 max memory_allocated 29271.32275390625 
[2025-03-22 19:58:33 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.6318570971488953 norm:0.0011871277820318937 max memory_allocated 29271.32275390625 
[2025-03-22 19:59:22 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.6315325498580933 norm:0.0012110716197639704 max memory_allocated 29271.32275390625 
[2025-03-22 20:00:10 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.6311736106872559 norm:0.0012108917580917478 max memory_allocated 29271.32275390625 
[2025-03-22 20:00:24 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 20:01:16 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.7713726758956909 norm:0.016496561467647552 max memory_allocated 29271.51025390625 
[2025-03-22 20:02:05 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.7534319758415222 norm:0.009052414447069168 max memory_allocated 29271.51025390625 
[2025-03-22 20:02:53 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.7320117354393005 norm:0.004945605527609587 max memory_allocated 29271.51025390625 
[2025-03-22 20:03:42 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.7213363647460938 norm:0.0029516189824789762 max memory_allocated 29271.51025390625 
[2025-03-22 20:04:30 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.716884970664978 norm:0.002457339782267809 max memory_allocated 29271.51025390625 
[2025-03-22 20:05:18 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.7136867642402649 norm:0.001847358071245253 max memory_allocated 29271.51025390625 
[2025-03-22 20:06:07 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.7109335660934448 norm:0.0013248580507934093 max memory_allocated 29271.51025390625 
[2025-03-22 20:06:55 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.7090438604354858 norm:0.001279046293348074 max memory_allocated 29271.51025390625 
[2025-03-22 20:07:43 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.7075241804122925 norm:0.0012554890708997846 max memory_allocated 29271.51025390625 
[2025-03-22 20:08:31 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.7063843011856079 norm:0.001239997916854918 max memory_allocated 29271.51025390625 
[2025-03-22 20:09:20 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.7054635882377625 norm:0.0012193785514682531 max memory_allocated 29271.51025390625 
[2025-03-22 20:10:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.7047906517982483 norm:0.0012031781952828169 max memory_allocated 29271.51025390625 
[2025-03-22 20:10:56 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.7042222619056702 norm:0.0011807645205408335 max memory_allocated 29271.51025390625 
[2025-03-22 20:11:44 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.7036986351013184 norm:0.001175055280327797 max memory_allocated 29271.51025390625 
[2025-03-22 20:12:33 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.7032415270805359 norm:0.0011709982063621283 max memory_allocated 29271.51025390625 
[2025-03-22 20:13:21 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.7026737928390503 norm:0.0011557424440979958 max memory_allocated 29271.51025390625 
[2025-03-22 20:14:09 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.7021135687828064 norm:0.0011475450592115521 max memory_allocated 29271.51025390625 
[2025-03-22 20:14:58 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.701896607875824 norm:0.0011521897977218032 max memory_allocated 29271.51025390625 
[2025-03-22 20:15:46 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.7017195224761963 norm:0.0011438943911343813 max memory_allocated 29271.51025390625 
[2025-03-22 20:16:35 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.7014509439468384 norm:0.0011405445402488112 max memory_allocated 29271.51025390625 
[2025-03-22 20:16:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 20:17:41 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.8595105409622192 norm:0.029366610571742058 max memory_allocated 29271.69775390625 
[2025-03-22 20:18:29 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.8369354009628296 norm:0.017332617193460464 max memory_allocated 29271.69775390625 
[2025-03-22 20:19:18 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.8120508193969727 norm:0.010173274204134941 max memory_allocated 29271.69775390625 
[2025-03-22 20:20:06 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.7990521788597107 norm:0.0054415552876889706 max memory_allocated 29271.69775390625 
[2025-03-22 20:20:54 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.7933933138847351 norm:0.0043808347545564175 max memory_allocated 29271.69775390625 
[2025-03-22 20:21:43 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.7895298004150391 norm:0.003367590717971325 max memory_allocated 29271.69775390625 
[2025-03-22 20:22:31 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.7870172262191772 norm:0.0030277366749942303 max memory_allocated 29271.69775390625 
[2025-03-22 20:23:19 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.7849206924438477 norm:0.0027633397839963436 max memory_allocated 29271.69775390625 
[2025-03-22 20:24:07 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.7831569314002991 norm:0.00239626900292933 max memory_allocated 29271.69775390625 
[2025-03-22 20:24:56 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.7816981673240662 norm:0.0021854909136891365 max memory_allocated 29271.69775390625 
[2025-03-22 20:25:44 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.7805927991867065 norm:0.002005568705499172 max memory_allocated 29271.69775390625 
[2025-03-22 20:26:32 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.7797869443893433 norm:0.0019157814094796777 max memory_allocated 29271.69775390625 
[2025-03-22 20:27:20 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.7785813808441162 norm:0.0015762404073029757 max memory_allocated 29271.69775390625 
[2025-03-22 20:28:08 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.7775831818580627 norm:0.0011636215494945645 max memory_allocated 29271.69775390625 
[2025-03-22 20:28:57 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.7770055532455444 norm:0.0011696367291733623 max memory_allocated 29271.69775390625 
[2025-03-22 20:29:45 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.7766563892364502 norm:0.0011730939149856567 max memory_allocated 29271.69775390625 
[2025-03-22 20:30:33 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.7764794230461121 norm:0.0011741735506802797 max memory_allocated 29271.69775390625 
[2025-03-22 20:31:22 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.7761850953102112 norm:0.0011575170792639256 max memory_allocated 29271.69775390625 
[2025-03-22 20:32:10 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.7758505940437317 norm:0.001148657756857574 max memory_allocated 29271.69775390625 
[2025-03-22 20:32:59 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.775648832321167 norm:0.0011464630952104926 max memory_allocated 29271.69775390625 
[2025-03-22 20:33:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 20:34:05 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.9468536376953125 norm:0.009599138051271439 max memory_allocated 29271.88525390625 
[2025-03-22 20:34:53 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.9279156923294067 norm:0.005882567726075649 max memory_allocated 29271.88525390625 
[2025-03-22 20:35:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.9042387008666992 norm:0.0031395279802381992 max memory_allocated 29271.88525390625 
[2025-03-22 20:36:30 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.8948755264282227 norm:0.0018638535402715206 max memory_allocated 29271.88525390625 
[2025-03-22 20:37:18 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.8906712532043457 norm:0.0015939162112772465 max memory_allocated 29271.88525390625 
[2025-03-22 20:38:07 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.8877534866333008 norm:0.0014932185877114534 max memory_allocated 29271.88525390625 
[2025-03-22 20:38:55 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.8855582475662231 norm:0.001439073821529746 max memory_allocated 29271.88525390625 
[2025-03-22 20:39:43 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.883906364440918 norm:0.001401418587192893 max memory_allocated 29271.88525390625 
[2025-03-22 20:40:32 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.8825086355209351 norm:0.0013536668848246336 max memory_allocated 29271.88525390625 
[2025-03-22 20:41:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.8813661336898804 norm:0.001339673763141036 max memory_allocated 29271.88525390625 
[2025-03-22 20:42:08 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.8803298473358154 norm:0.0013480379711836576 max memory_allocated 29271.88525390625 
[2025-03-22 20:42:56 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.8795942664146423 norm:0.0013191818725317717 max memory_allocated 29271.88525390625 
[2025-03-22 20:43:45 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.8788108825683594 norm:0.001307895639911294 max memory_allocated 29271.88525390625 
[2025-03-22 20:44:33 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.8783643841743469 norm:0.001301408396102488 max memory_allocated 29271.88525390625 
[2025-03-22 20:45:21 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.8778706789016724 norm:0.001284172642044723 max memory_allocated 29271.88525390625 
[2025-03-22 20:46:09 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.87742680311203 norm:0.0012629134580492973 max memory_allocated 29271.88525390625 
[2025-03-22 20:46:58 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.8770735263824463 norm:0.0012515389826148748 max memory_allocated 29271.88525390625 
[2025-03-22 20:47:46 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.8768795132637024 norm:0.0012400067644193769 max memory_allocated 29271.88525390625 
[2025-03-22 20:48:35 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.87644362449646 norm:0.00122652982827276 max memory_allocated 29271.88525390625 
[2025-03-22 20:49:23 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.8762017488479614 norm:0.0012331490870565176 max memory_allocated 29271.88525390625 
[2025-03-22 20:49:37 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 20:50:29 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:1.0710816383361816 norm:0.018382584676146507 max memory_allocated 29272.07275390625 
[2025-03-22 20:51:18 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:1.0498993396759033 norm:0.009436453692615032 max memory_allocated 29272.07275390625 
[2025-03-22 20:52:06 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:1.0257662534713745 norm:0.004992272239178419 max memory_allocated 29272.07275390625 
[2025-03-22 20:52:55 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:1.0149459838867188 norm:0.0025508508551865816 max memory_allocated 29272.07275390625 
[2025-03-22 20:53:43 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:1.0103601217269897 norm:0.0017778459005057812 max memory_allocated 29272.07275390625 
[2025-03-22 20:54:32 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:1.0074329376220703 norm:0.0016486933454871178 max memory_allocated 29272.07275390625 
[2025-03-22 20:55:20 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:1.0049439668655396 norm:0.0015861951978877187 max memory_allocated 29272.07275390625 
[2025-03-22 20:56:09 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:1.0030525922775269 norm:0.0015353644266724586 max memory_allocated 29272.07275390625 
[2025-03-22 20:56:57 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:1.0015349388122559 norm:0.0014979226980358362 max memory_allocated 29272.07275390625 
[2025-03-22 20:57:46 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:1.0002638101577759 norm:0.0014530621701851487 max memory_allocated 29272.07275390625 
[2025-03-22 20:58:34 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.9992204904556274 norm:0.0014235936105251312 max memory_allocated 29272.07275390625 
[2025-03-22 20:59:22 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.9983232021331787 norm:0.0014087535673752427 max memory_allocated 29272.07275390625 
[2025-03-22 21:00:11 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.9976720809936523 norm:0.001394782681018114 max memory_allocated 29272.07275390625 
[2025-03-22 21:00:59 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.997267484664917 norm:0.001395554281771183 max memory_allocated 29272.07275390625 
[2025-03-22 21:01:47 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.996788740158081 norm:0.0013746527256444097 max memory_allocated 29272.07275390625 
[2025-03-22 21:02:36 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.9964820146560669 norm:0.0013791414676234126 max memory_allocated 29272.07275390625 
[2025-03-22 21:03:24 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.9961162805557251 norm:0.0013658878160640597 max memory_allocated 29272.07275390625 
[2025-03-22 21:04:12 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.9958838224411011 norm:0.0013577652862295508 max memory_allocated 29272.07275390625 
[2025-03-22 21:05:01 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.9956803321838379 norm:0.0013557018246501684 max memory_allocated 29272.07275390625 
[2025-03-22 21:05:49 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.9954081177711487 norm:0.0013598320074379444 max memory_allocated 29272.07275390625 
[2025-03-22 21:06:03 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 21:06:55 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:1.2096853256225586 norm:0.01924177259206772 max memory_allocated 29272.26025390625 
[2025-03-22 21:07:44 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:1.1912204027175903 norm:0.011524828150868416 max memory_allocated 29272.26025390625 
[2025-03-22 21:08:32 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:1.1695748567581177 norm:0.006176471244543791 max memory_allocated 29272.26025390625 
[2025-03-22 21:09:21 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:1.1603891849517822 norm:0.0023661551531404257 max memory_allocated 29272.26025390625 
[2025-03-22 21:10:09 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:1.1562254428863525 norm:0.0020095340441912413 max memory_allocated 29272.26025390625 
[2025-03-22 21:10:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:1.1532204151153564 norm:0.001878266572020948 max memory_allocated 29272.26025390625 
[2025-03-22 21:11:46 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:1.1506483554840088 norm:0.001809923443943262 max memory_allocated 29272.26025390625 
[2025-03-22 21:12:35 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:1.1487966775894165 norm:0.0017655889969319105 max memory_allocated 29272.26025390625 
[2025-03-22 21:13:23 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:1.1472293138504028 norm:0.001728696166537702 max memory_allocated 29272.26025390625 
[2025-03-22 21:14:12 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:1.1459980010986328 norm:0.0017076455987989902 max memory_allocated 29272.26025390625 
[2025-03-22 21:15:00 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:1.1451038122177124 norm:0.0016893188003450632 max memory_allocated 29272.26025390625 
[2025-03-22 21:15:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:1.1442453861236572 norm:0.0016709316987544298 max memory_allocated 29272.26025390625 
[2025-03-22 21:16:37 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:1.1435573101043701 norm:0.0016692612553015351 max memory_allocated 29272.26025390625 
[2025-03-22 21:17:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:1.1429225206375122 norm:0.0016508335247635841 max memory_allocated 29272.26025390625 
[2025-03-22 21:18:15 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:1.142354965209961 norm:0.0016310536302626133 max memory_allocated 29272.26025390625 
[2025-03-22 21:19:03 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:1.1419181823730469 norm:0.0016341842710971832 max memory_allocated 29272.26025390625 
[2025-03-22 21:19:52 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:1.1415555477142334 norm:0.001622115494683385 max memory_allocated 29272.26025390625 
[2025-03-22 21:20:40 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:1.141237497329712 norm:0.0016104000387713313 max memory_allocated 29272.26025390625 
[2025-03-22 21:21:29 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:1.1408593654632568 norm:0.0016002379124984145 max memory_allocated 29272.26025390625 
[2025-03-22 21:22:17 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:1.1404650211334229 norm:0.0015911567024886608 max memory_allocated 29272.26025390625 
[2025-03-22 21:22:31 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 21:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:1.3467605113983154 norm:0.012854763306677341 max memory_allocated 29272.44775390625 
[2025-03-22 21:24:12 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:1.3286824226379395 norm:0.007376077119261026 max memory_allocated 29272.44775390625 
[2025-03-22 21:25:01 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:1.310304045677185 norm:0.004911791533231735 max memory_allocated 29272.44775390625 
[2025-03-22 21:25:49 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:1.3035337924957275 norm:0.003616148140281439 max memory_allocated 29272.44775390625 
[2025-03-22 21:26:38 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:1.2993366718292236 norm:0.003024366218596697 max memory_allocated 29272.44775390625 
[2025-03-22 21:27:26 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:1.2960498332977295 norm:0.002688807900995016 max memory_allocated 29272.44775390625 
[2025-03-22 21:28:15 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:1.2934770584106445 norm:0.0023714229464530945 max memory_allocated 29272.44775390625 
[2025-03-22 21:29:03 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:1.2914373874664307 norm:0.0021615810692310333 max memory_allocated 29272.44775390625 
[2025-03-22 21:29:52 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:1.2896877527236938 norm:0.001987229101359844 max memory_allocated 29272.44775390625 
[2025-03-22 21:30:41 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:1.2882683277130127 norm:0.0018649024423211813 max memory_allocated 29272.44775390625 
[2025-03-22 21:31:29 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:1.2871414422988892 norm:0.0017736006993800402 max memory_allocated 29272.44775390625 
[2025-03-22 21:32:18 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:1.2862855195999146 norm:0.0016802458558231592 max memory_allocated 29272.44775390625 
[2025-03-22 21:33:06 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:1.2856855392456055 norm:0.0016338462010025978 max memory_allocated 29272.44775390625 
[2025-03-22 21:33:55 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:1.285109043121338 norm:0.0015912677627056837 max memory_allocated 29272.44775390625 
[2025-03-22 21:34:43 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:1.2844454050064087 norm:0.0015512576792389154 max memory_allocated 29272.44775390625 
[2025-03-22 21:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:1.2838932275772095 norm:0.0015280375955626369 max memory_allocated 29272.44775390625 
[2025-03-22 21:36:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:1.2834585905075073 norm:0.0015092540998011827 max memory_allocated 29272.44775390625 
[2025-03-22 21:37:08 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:1.2830860614776611 norm:0.0014900087844580412 max memory_allocated 29272.44775390625 
[2025-03-22 21:37:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:1.2827967405319214 norm:0.0014820550568401814 max memory_allocated 29272.44775390625 
[2025-03-22 21:38:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:1.2825220823287964 norm:0.0014693262055516243 max memory_allocated 29272.44775390625 
[2025-03-22 21:38:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 21:39:51 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:1.5135834217071533 norm:0.019250525161623955 max memory_allocated 29272.63525390625 
[2025-03-22 21:40:39 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:1.4921382665634155 norm:0.011105000972747803 max memory_allocated 29272.63525390625 
[2025-03-22 21:41:27 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:1.4712868928909302 norm:0.006540719419717789 max memory_allocated 29272.63525390625 
[2025-03-22 21:42:16 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:1.461801290512085 norm:0.0038338173180818558 max memory_allocated 29272.63525390625 
[2025-03-22 21:43:04 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:1.4563195705413818 norm:0.0017385322134941816 max memory_allocated 29272.63525390625 
[2025-03-22 21:43:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:1.453298807144165 norm:0.001646592514589429 max memory_allocated 29272.63525390625 
[2025-03-22 21:44:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:1.4505069255828857 norm:0.0015541811008006334 max memory_allocated 29272.63525390625 
[2025-03-22 21:45:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:1.4484004974365234 norm:0.0015082358149811625 max memory_allocated 29272.63525390625 
[2025-03-22 21:46:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:1.4470938444137573 norm:0.0015015562530606985 max memory_allocated 29272.63525390625 
[2025-03-22 21:47:06 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:1.4458134174346924 norm:0.00148346449714154 max memory_allocated 29272.63525390625 
[2025-03-22 21:47:55 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:1.4449291229248047 norm:0.0014866616111248732 max memory_allocated 29272.63525390625 
[2025-03-22 21:48:43 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:1.44400155544281 norm:0.0014748703688383102 max memory_allocated 29272.63525390625 
[2025-03-22 21:49:32 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:1.4432921409606934 norm:0.0014636543346568942 max memory_allocated 29272.63525390625 
[2025-03-22 21:50:20 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:1.4427464008331299 norm:0.0014639931032434106 max memory_allocated 29272.63525390625 
[2025-03-22 21:51:09 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:1.4421371221542358 norm:0.0014491027686744928 max memory_allocated 29272.63525390625 
[2025-03-22 21:51:57 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:1.4417238235473633 norm:0.0014461654936894774 max memory_allocated 29272.63525390625 
[2025-03-22 21:52:46 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:1.441330075263977 norm:0.0014413440367206931 max memory_allocated 29272.63525390625 
[2025-03-22 21:53:34 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:1.4410357475280762 norm:0.001428377116099 max memory_allocated 29272.63525390625 
[2025-03-22 21:54:23 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:1.4409068822860718 norm:0.0014283505734056234 max memory_allocated 29272.63525390625 
[2025-03-22 21:55:11 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:1.4407027959823608 norm:0.0014229747466742992 max memory_allocated 29272.63525390625 
[2025-03-22 21:55:25 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 21:56:17 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:1.6696895360946655 norm:0.014740933664143085 max memory_allocated 29272.82275390625 
[2025-03-22 21:57:05 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:1.6504473686218262 norm:0.008024432696402073 max memory_allocated 29272.82275390625 
[2025-03-22 21:57:53 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:1.6297521591186523 norm:0.00494147464632988 max memory_allocated 29272.82275390625 
[2025-03-22 21:58:41 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:1.620511531829834 norm:0.002372706774622202 max memory_allocated 29272.82275390625 
[2025-03-22 21:59:30 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:1.6160391569137573 norm:0.0017873805481940508 max memory_allocated 29272.82275390625 
[2025-03-22 22:00:18 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:1.6126303672790527 norm:0.0015384356956928968 max memory_allocated 29272.82275390625 
[2025-03-22 22:01:06 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:1.6099750995635986 norm:0.001410681870765984 max memory_allocated 29272.82275390625 
[2025-03-22 22:01:55 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:1.6079164743423462 norm:0.0013909689150750637 max memory_allocated 29272.82275390625 
[2025-03-22 22:02:43 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:1.6065194606781006 norm:0.0013886832166463137 max memory_allocated 29272.82275390625 
[2025-03-22 22:03:32 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:1.6054134368896484 norm:0.0013914493611082435 max memory_allocated 29272.82275390625 
[2025-03-22 22:04:20 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:1.6046258211135864 norm:0.0013939687050879002 max memory_allocated 29272.82275390625 
[2025-03-22 22:05:09 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:1.6039235591888428 norm:0.0013891886919736862 max memory_allocated 29272.82275390625 
[2025-03-22 22:05:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:1.6033940315246582 norm:0.0013796398416161537 max memory_allocated 29272.82275390625 
[2025-03-22 22:06:46 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:1.6027891635894775 norm:0.0013686782913282514 max memory_allocated 29272.82275390625 
[2025-03-22 22:07:34 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:1.6022374629974365 norm:0.0013731451472267509 max memory_allocated 29272.82275390625 
[2025-03-22 22:08:23 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:1.6018180847167969 norm:0.0013695290545001626 max memory_allocated 29272.82275390625 
[2025-03-22 22:09:11 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:1.6014701128005981 norm:0.0013681820128113031 max memory_allocated 29272.82275390625 
[2025-03-22 22:10:00 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:1.6011892557144165 norm:0.001359896850772202 max memory_allocated 29272.82275390625 
[2025-03-22 22:10:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:1.6009646654129028 norm:0.00135361950378865 max memory_allocated 29272.82275390625 
[2025-03-22 22:11:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:1.600777268409729 norm:0.0013541466323658824 max memory_allocated 29272.82275390625 
[2025-03-22 22:11:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 22:12:43 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:1.8558647632598877 norm:0.006040854845196009 max memory_allocated 29273.01025390625 
[2025-03-22 22:13:31 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:1.8373767137527466 norm:0.003650868311524391 max memory_allocated 29273.01025390625 
[2025-03-22 22:14:20 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:1.8158972263336182 norm:0.0025706442538648844 max memory_allocated 29273.01025390625 
[2025-03-22 22:15:08 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:1.807235598564148 norm:0.0016758441925048828 max memory_allocated 29273.01025390625 
[2025-03-22 22:15:56 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:1.8027690649032593 norm:0.0014874250628054142 max memory_allocated 29273.01025390625 
[2025-03-22 22:16:45 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:1.7994245290756226 norm:0.0014291813131421804 max memory_allocated 29273.01025390625 
[2025-03-22 22:17:33 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:1.7963659763336182 norm:0.001398949301801622 max memory_allocated 29273.01025390625 
[2025-03-22 22:18:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:1.7940490245819092 norm:0.0013867771485820413 max memory_allocated 29273.01025390625 
[2025-03-22 22:19:10 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:1.7924041748046875 norm:0.001383929280564189 max memory_allocated 29273.01025390625 
[2025-03-22 22:19:58 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:1.7910654544830322 norm:0.0013726071920245886 max memory_allocated 29273.01025390625 
[2025-03-22 22:20:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:1.7900190353393555 norm:0.0013782334281131625 max memory_allocated 29273.01025390625 
[2025-03-22 22:21:35 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:1.7891026735305786 norm:0.0013720289571210742 max memory_allocated 29273.01025390625 
[2025-03-22 22:22:24 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:1.7884882688522339 norm:0.0013770066434517503 max memory_allocated 29273.01025390625 
[2025-03-22 22:23:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:1.7878843545913696 norm:0.0013875685399398208 max memory_allocated 29273.01025390625 
[2025-03-22 22:24:01 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:1.7874302864074707 norm:0.0013913523871451616 max memory_allocated 29273.01025390625 
[2025-03-22 22:24:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:1.7870408296585083 norm:0.001391173922456801 max memory_allocated 29273.01025390625 
[2025-03-22 22:25:38 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:1.7867395877838135 norm:0.0013901818310841918 max memory_allocated 29273.01025390625 
[2025-03-22 22:26:26 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:1.7863680124282837 norm:0.0013922674115747213 max memory_allocated 29273.01025390625 
[2025-03-22 22:27:15 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:1.7862300872802734 norm:0.0013913547154515982 max memory_allocated 29273.01025390625 
[2025-03-22 22:28:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:1.7858953475952148 norm:0.001380471745505929 max memory_allocated 29273.01025390625 
[2025-03-22 22:28:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 22:29:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:2.0471179485321045 norm:0.006329396273940802 max memory_allocated 29273.19775390625 
[2025-03-22 22:29:58 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:2.0266218185424805 norm:0.003700984176248312 max memory_allocated 29273.19775390625 
[2025-03-22 22:30:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:2.0048952102661133 norm:0.002748801140114665 max memory_allocated 29273.19775390625 
[2025-03-22 22:31:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:1.9977200031280518 norm:0.0023100310936570168 max memory_allocated 29273.19775390625 
[2025-03-22 22:32:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:1.9935322999954224 norm:0.0020550908520817757 max memory_allocated 29273.19775390625 
[2025-03-22 22:33:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:1.990254521369934 norm:0.001959301298484206 max memory_allocated 29273.19775390625 
[2025-03-22 22:33:59 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:1.9870171546936035 norm:0.0018811883637681603 max memory_allocated 29273.19775390625 
[2025-03-22 22:34:48 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:1.9847134351730347 norm:0.0018042140873149037 max memory_allocated 29273.19775390625 
[2025-03-22 22:35:36 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:1.9832451343536377 norm:0.0017505483701825142 max memory_allocated 29273.19775390625 
[2025-03-22 22:36:24 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:1.9820446968078613 norm:0.0017170652281492949 max memory_allocated 29273.19775390625 
[2025-03-22 22:37:13 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:1.980993628501892 norm:0.0017541740089654922 max memory_allocated 29273.19775390625 
[2025-03-22 22:38:01 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:1.979966640472412 norm:0.001703281537629664 max memory_allocated 29273.19775390625 
[2025-03-22 22:38:50 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:1.9793510437011719 norm:0.0016962442314252257 max memory_allocated 29273.19775390625 
[2025-03-22 22:39:38 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:1.978738784790039 norm:0.0016846285434439778 max memory_allocated 29273.19775390625 
[2025-03-22 22:40:26 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:1.9779530763626099 norm:0.0016337035922333598 max memory_allocated 29273.19775390625 
[2025-03-22 22:41:15 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:1.977447748184204 norm:0.001635087071917951 max memory_allocated 29273.19775390625 
[2025-03-22 22:42:03 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:1.977077841758728 norm:0.0016141146188601851 max memory_allocated 29273.19775390625 
[2025-03-22 22:42:52 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:1.9767022132873535 norm:0.0016718283295631409 max memory_allocated 29273.19775390625 
[2025-03-22 22:43:40 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:1.9765753746032715 norm:0.0017268089577555656 max memory_allocated 29273.19775390625 
[2025-03-22 22:44:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:1.976151466369629 norm:0.0017013116739690304 max memory_allocated 29273.19775390625 
[2025-03-22 22:44:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 22:45:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:2.227609872817993 norm:0.011231488548219204 max memory_allocated 29273.38525390625 
[2025-03-22 22:46:23 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:2.208062171936035 norm:0.006165092810988426 max memory_allocated 29273.38525390625 
[2025-03-22 22:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:2.1850783824920654 norm:0.003636543173342943 max memory_allocated 29273.38525390625 
[2025-03-22 22:47:59 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:2.176525592803955 norm:0.0025515349116176367 max memory_allocated 29273.38525390625 
[2025-03-22 22:48:48 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:2.171414852142334 norm:0.0016645797295495868 max memory_allocated 29273.38525390625 
[2025-03-22 22:49:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:2.167550563812256 norm:0.0014883877011016011 max memory_allocated 29273.38525390625 
[2025-03-22 22:50:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:2.1643919944763184 norm:0.0014202342135831714 max memory_allocated 29273.38525390625 
[2025-03-22 22:51:12 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:2.162135124206543 norm:0.0013886501546949148 max memory_allocated 29273.38525390625 
[2025-03-22 22:52:00 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:2.1602730751037598 norm:0.0013551689917221665 max memory_allocated 29273.38525390625 
[2025-03-22 22:52:49 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:2.1589348316192627 norm:0.0013378113508224487 max memory_allocated 29273.38525390625 
[2025-03-22 22:53:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:2.158017158508301 norm:0.0013172217877581716 max memory_allocated 29273.38525390625 
[2025-03-22 22:54:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:2.1572210788726807 norm:0.0013114247703924775 max memory_allocated 29273.38525390625 
[2025-03-22 22:55:14 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:2.1566667556762695 norm:0.0013097942573949695 max memory_allocated 29273.38525390625 
[2025-03-22 22:56:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:2.1562063694000244 norm:0.001312025124207139 max memory_allocated 29273.38525390625 
[2025-03-22 22:56:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:2.1558070182800293 norm:0.0013102879747748375 max memory_allocated 29273.38525390625 
[2025-03-22 22:57:39 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:2.155374765396118 norm:0.0013110276777297258 max memory_allocated 29273.38525390625 
[2025-03-22 22:58:28 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:2.1551167964935303 norm:0.0013117732014507055 max memory_allocated 29273.38525390625 
[2025-03-22 22:59:16 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:2.154777765274048 norm:0.0013116641202941537 max memory_allocated 29273.38525390625 
[2025-03-22 23:00:04 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:2.1545495986938477 norm:0.0013088580453768373 max memory_allocated 29273.38525390625 
[2025-03-22 23:00:53 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:2.1543216705322266 norm:0.001305170706473291 max memory_allocated 29273.38525390625 
[2025-03-22 23:01:07 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 23:02:00 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:2.4443697929382324 norm:0.015955541282892227 max memory_allocated 29273.57275390625 
[2025-03-22 23:02:48 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:2.4246251583099365 norm:0.011142916977405548 max memory_allocated 29273.57275390625 
[2025-03-22 23:03:37 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:2.400529146194458 norm:0.007998766377568245 max memory_allocated 29273.57275390625 
[2025-03-22 23:04:25 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:2.390932559967041 norm:0.006061842665076256 max memory_allocated 29273.57275390625 
[2025-03-22 23:05:13 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:2.3838486671447754 norm:0.004537245258688927 max memory_allocated 29273.57275390625 
[2025-03-22 23:06:01 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:2.3770639896392822 norm:0.0036673054564744234 max memory_allocated 29273.57275390625 
[2025-03-22 23:06:49 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:2.373241424560547 norm:0.003527355147525668 max memory_allocated 29273.57275390625 
[2025-03-22 23:07:38 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:2.3702964782714844 norm:0.0031732795760035515 max memory_allocated 29273.57275390625 
[2025-03-22 23:08:26 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:2.368098735809326 norm:0.002880895510315895 max memory_allocated 29273.57275390625 
[2025-03-22 23:09:14 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:2.3663203716278076 norm:0.002660471946001053 max memory_allocated 29273.57275390625 
[2025-03-22 23:10:02 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:2.3649134635925293 norm:0.002479513641446829 max memory_allocated 29273.57275390625 
[2025-03-22 23:10:51 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:2.3638501167297363 norm:0.0023247271310538054 max memory_allocated 29273.57275390625 
[2025-03-22 23:11:39 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:2.362874984741211 norm:0.002184429205954075 max memory_allocated 29273.57275390625 
[2025-03-22 23:12:27 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:2.3619697093963623 norm:0.0020834608003497124 max memory_allocated 29273.57275390625 
[2025-03-22 23:13:16 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:2.3611834049224854 norm:0.001984409987926483 max memory_allocated 29273.57275390625 
[2025-03-22 23:14:04 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:2.3605220317840576 norm:0.0018993093399330974 max memory_allocated 29273.57275390625 
[2025-03-22 23:14:53 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:2.359910726547241 norm:0.00182434543967247 max memory_allocated 29273.57275390625 
[2025-03-22 23:15:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:2.359527587890625 norm:0.001800290308892727 max memory_allocated 29273.57275390625 
[2025-03-22 23:16:29 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:2.359055757522583 norm:0.0017499425448477268 max memory_allocated 29273.57275390625 
[2025-03-22 23:17:18 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:2.35856556892395 norm:0.00169936113525182 max memory_allocated 29273.57275390625 
[2025-03-22 23:17:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 23:18:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:2.6424882411956787 norm:0.006505193188786507 max memory_allocated 29273.76025390625 
[2025-03-22 23:19:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:2.6212210655212402 norm:0.003836297430098057 max memory_allocated 29273.76025390625 
[2025-03-22 23:20:01 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:2.5971968173980713 norm:0.0027000783011317253 max memory_allocated 29273.76025390625 
[2025-03-22 23:20:49 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:2.5880208015441895 norm:0.0022532264702022076 max memory_allocated 29273.76025390625 
[2025-03-22 23:21:37 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:2.5824899673461914 norm:0.0020380583591759205 max memory_allocated 29273.76025390625 
[2025-03-22 23:22:26 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:2.5773653984069824 norm:0.0018775016069412231 max memory_allocated 29273.76025390625 
[2025-03-22 23:23:14 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:2.573707103729248 norm:0.0017908720765262842 max memory_allocated 29273.76025390625 
[2025-03-22 23:24:03 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:2.5707943439483643 norm:0.0017434090841561556 max memory_allocated 29273.76025390625 
[2025-03-22 23:24:51 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:2.568772792816162 norm:0.0017328988760709763 max memory_allocated 29273.76025390625 
[2025-03-22 23:25:40 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:2.5673727989196777 norm:0.0017154759261757135 max memory_allocated 29273.76025390625 
[2025-03-22 23:26:28 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:2.566225528717041 norm:0.001673058606684208 max memory_allocated 29273.76025390625 
[2025-03-22 23:27:16 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:2.5652995109558105 norm:0.0016358530847355723 max memory_allocated 29273.76025390625 
[2025-03-22 23:28:05 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:2.5645151138305664 norm:0.0016282153083011508 max memory_allocated 29273.76025390625 
[2025-03-22 23:28:53 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:2.5637738704681396 norm:0.0016061963979154825 max memory_allocated 29273.76025390625 
[2025-03-22 23:29:42 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:2.56318736076355 norm:0.0015957677969709039 max memory_allocated 29273.76025390625 
[2025-03-22 23:30:30 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:2.5627520084381104 norm:0.0015784923452883959 max memory_allocated 29273.76025390625 
[2025-03-22 23:31:19 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:2.5622963905334473 norm:0.0015715026529505849 max memory_allocated 29273.76025390625 
[2025-03-22 23:32:07 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:2.561870574951172 norm:0.0015652459114789963 max memory_allocated 29273.76025390625 
[2025-03-22 23:32:56 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:2.5615437030792236 norm:0.0015589121030643582 max memory_allocated 29273.76025390625 
[2025-03-22 23:33:44 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:2.5613417625427246 norm:0.0015573418932035565 max memory_allocated 29273.76025390625 
[2025-03-22 23:33:58 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 32 ===
[2025-03-22 23:34:50 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 0 loss:2.8556904792785645 norm:0.011495249345898628 max memory_allocated 29273.94775390625 
[2025-03-22 23:35:38 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 1 loss:2.832819700241089 norm:0.006767857354134321 max memory_allocated 29273.94775390625 
[2025-03-22 23:36:27 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 2 loss:2.805182933807373 norm:0.004411549307405949 max memory_allocated 29273.94775390625 
[2025-03-22 23:37:15 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 3 loss:2.793391466140747 norm:0.0029296616557985544 max memory_allocated 29273.94775390625 
[2025-03-22 23:38:03 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 4 loss:2.7879316806793213 norm:0.002315498422831297 max memory_allocated 29273.94775390625 
[2025-03-22 23:38:51 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 5 loss:2.782836437225342 norm:0.0020451871678233147 max memory_allocated 29273.94775390625 
[2025-03-22 23:39:39 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 6 loss:2.778306484222412 norm:0.0017698958981782198 max memory_allocated 29273.94775390625 
[2025-03-22 23:40:28 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 7 loss:2.7748494148254395 norm:0.0016531342407688498 max memory_allocated 29273.94775390625 
[2025-03-22 23:41:16 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 8 loss:2.7725043296813965 norm:0.0015967137878760695 max memory_allocated 29273.94775390625 
[2025-03-22 23:42:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 9 loss:2.7706806659698486 norm:0.0015230977442115545 max memory_allocated 29273.94775390625 
[2025-03-22 23:42:52 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 10 loss:2.7693705558776855 norm:0.0014712194679304957 max memory_allocated 29273.94775390625 
[2025-03-22 23:43:41 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 11 loss:2.7681994438171387 norm:0.0014306865632534027 max memory_allocated 29273.94775390625 
[2025-03-22 23:44:29 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 12 loss:2.7673892974853516 norm:0.0014151346404105425 max memory_allocated 29273.94775390625 
[2025-03-22 23:45:18 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 13 loss:2.7666704654693604 norm:0.0014144083252176642 max memory_allocated 29273.94775390625 
[2025-03-22 23:46:06 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 14 loss:2.7661893367767334 norm:0.0014046758878976107 max memory_allocated 29273.94775390625 
[2025-03-22 23:46:54 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 15 loss:2.7658395767211914 norm:0.0013965887483209372 max memory_allocated 29273.94775390625 
[2025-03-22 23:47:43 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 16 loss:2.765383243560791 norm:0.0013844917993992567 max memory_allocated 29273.94775390625 
[2025-03-22 23:48:31 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 17 loss:2.7648983001708984 norm:0.0013604004634544253 max memory_allocated 29273.94775390625 
[2025-03-22 23:49:20 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 18 loss:2.764540672302246 norm:0.0013571117306128144 max memory_allocated 29273.94775390625 
[2025-03-22 23:50:08 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 19 loss:2.7641429901123047 norm:0.0013517161132767797 max memory_allocated 29273.94775390625 
[2025-03-22 23:50:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 33 ===
[2025-03-22 23:51:14 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 0 loss:3.096108913421631 norm:0.010876458138227463 max memory_allocated 29274.13525390625 
[2025-03-22 23:52:03 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 1 loss:3.0711421966552734 norm:0.006032113451510668 max memory_allocated 29274.13525390625 
[2025-03-22 23:52:51 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 2 loss:3.0417838096618652 norm:0.004042251966893673 max memory_allocated 29274.13525390625 
[2025-03-22 23:53:39 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 3 loss:3.0294785499572754 norm:0.0029364058282226324 max memory_allocated 29274.13525390625 
[2025-03-22 23:54:27 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 4 loss:3.0226800441741943 norm:0.002215651562437415 max memory_allocated 29274.13525390625 
[2025-03-22 23:55:16 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 5 loss:3.0167906284332275 norm:0.0018700892105698586 max memory_allocated 29274.13525390625 
[2025-03-22 23:56:04 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 6 loss:3.0116963386535645 norm:0.0017651550006121397 max memory_allocated 29274.13525390625 
[2025-03-22 23:56:52 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 7 loss:3.008362293243408 norm:0.0016902246279641986 max memory_allocated 29274.13525390625 
[2025-03-22 23:57:40 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 8 loss:3.0057532787323 norm:0.0016272410284727812 max memory_allocated 29274.13525390625 
[2025-03-22 23:58:28 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 9 loss:3.0038509368896484 norm:0.0015788138844072819 max memory_allocated 29274.13525390625 
[2025-03-22 23:59:17 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 10 loss:3.0025343894958496 norm:0.0015400266274809837 max memory_allocated 29274.13525390625 
[2025-03-23 00:00:05 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 11 loss:3.001237392425537 norm:0.0015082164900377393 max memory_allocated 29274.13525390625 
[2025-03-23 00:00:53 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 12 loss:3.0002152919769287 norm:0.0014839612413197756 max memory_allocated 29274.13525390625 
[2025-03-23 00:01:42 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 13 loss:2.9994425773620605 norm:0.001471654511988163 max memory_allocated 29274.13525390625 
[2025-03-23 00:02:30 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 14 loss:2.9988269805908203 norm:0.001476063160225749 max memory_allocated 29274.13525390625 
[2025-03-23 00:03:19 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 15 loss:2.9980292320251465 norm:0.0014569160994142294 max memory_allocated 29274.13525390625 
[2025-03-23 00:04:07 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 16 loss:2.9975061416625977 norm:0.001439297804608941 max memory_allocated 29274.13525390625 
[2025-03-23 00:04:55 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 17 loss:2.997087001800537 norm:0.0014456277713179588 max memory_allocated 29274.13525390625 
[2025-03-23 00:05:44 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 18 loss:2.9966862201690674 norm:0.0014301131013780832 max memory_allocated 29274.13525390625 
[2025-03-23 00:06:32 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 19 loss:2.996321678161621 norm:0.0014266060898080468 max memory_allocated 29274.13525390625 
[2025-03-23 00:06:46 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 34 ===
[2025-03-23 00:07:38 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 0 loss:3.4134321212768555 norm:0.034119006246328354 max memory_allocated 29274.32275390625 
[2025-03-23 00:08:27 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 1 loss:3.3796825408935547 norm:0.022663991898298264 max memory_allocated 29274.32275390625 
[2025-03-23 00:09:15 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 2 loss:3.3400003910064697 norm:0.014898603782057762 max memory_allocated 29274.32275390625 
[2025-03-23 00:10:03 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 3 loss:3.3187928199768066 norm:0.009643946774303913 max memory_allocated 29274.32275390625 
[2025-03-23 00:10:52 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 4 loss:3.3061389923095703 norm:0.007488831877708435 max memory_allocated 29274.32275390625 
[2025-03-23 00:11:40 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 5 loss:3.297553300857544 norm:0.006351819261908531 max memory_allocated 29274.32275390625 
[2025-03-23 00:12:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 6 loss:3.2916219234466553 norm:0.005586603190749884 max memory_allocated 29274.32275390625 
[2025-03-23 00:13:16 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 7 loss:3.2864034175872803 norm:0.004746992141008377 max memory_allocated 29274.32275390625 
[2025-03-23 00:14:05 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 8 loss:3.2823057174682617 norm:0.003986304625868797 max memory_allocated 29274.32275390625 
[2025-03-23 00:14:53 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 9 loss:3.2775280475616455 norm:0.0018640959169715643 max memory_allocated 29274.32275390625 
[2025-03-23 00:15:41 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 10 loss:3.2751622200012207 norm:0.0018547463696449995 max memory_allocated 29274.32275390625 
[2025-03-23 00:16:29 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 11 loss:3.2732741832733154 norm:0.0018428992480039597 max memory_allocated 29274.32275390625 
[2025-03-23 00:17:18 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 12 loss:3.272188186645508 norm:0.0018283847020938993 max memory_allocated 29274.32275390625 
[2025-03-23 00:18:06 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 13 loss:3.2713630199432373 norm:0.0018316254718229175 max memory_allocated 29274.32275390625 
[2025-03-23 00:18:54 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 14 loss:3.270315408706665 norm:0.0018165697110816836 max memory_allocated 29274.32275390625 
[2025-03-23 00:19:43 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 15 loss:3.2695515155792236 norm:0.0018305721459910274 max memory_allocated 29274.32275390625 
[2025-03-23 00:20:31 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 16 loss:3.268967628479004 norm:0.0018149754032492638 max memory_allocated 29274.32275390625 
[2025-03-23 00:21:20 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 17 loss:3.268451452255249 norm:0.0018234632443636656 max memory_allocated 29274.32275390625 
[2025-03-23 00:22:08 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 18 loss:3.267859935760498 norm:0.0018177266465499997 max memory_allocated 29274.32275390625 
[2025-03-23 00:22:56 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 19 loss:3.2673251628875732 norm:0.00181164825335145 max memory_allocated 29274.32275390625 
[2025-03-23 00:23:10 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 35 ===
[2025-03-23 00:24:03 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 0 loss:3.70327091217041 norm:0.022688237950205803 max memory_allocated 29274.51025390625 
[2025-03-23 00:24:51 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 1 loss:3.663550615310669 norm:0.01529720239341259 max memory_allocated 29274.51025390625 
[2025-03-23 00:25:39 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 2 loss:3.620558977127075 norm:0.010459503158926964 max memory_allocated 29274.51025390625 
[2025-03-23 00:26:28 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 3 loss:3.5985355377197266 norm:0.0074142711237072945 max memory_allocated 29274.51025390625 
[2025-03-23 00:27:16 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 4 loss:3.5850117206573486 norm:0.005458163097500801 max memory_allocated 29274.51025390625 
[2025-03-23 00:28:05 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 5 loss:3.5758745670318604 norm:0.005180873442441225 max memory_allocated 29274.51025390625 
[2025-03-23 00:28:53 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 6 loss:3.568495035171509 norm:0.004402889870107174 max memory_allocated 29274.51025390625 
[2025-03-23 00:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 7 loss:3.563405752182007 norm:0.00396688561886549 max memory_allocated 29274.51025390625 
[2025-03-23 00:30:30 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 8 loss:3.559251070022583 norm:0.0036169157829135656 max memory_allocated 29274.51025390625 
[2025-03-23 00:31:18 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 9 loss:3.5560402870178223 norm:0.0032620809506624937 max memory_allocated 29274.51025390625 
[2025-03-23 00:32:07 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 10 loss:3.5536000728607178 norm:0.0031161948572844267 max memory_allocated 29274.51025390625 
[2025-03-23 00:32:55 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 11 loss:3.5511667728424072 norm:0.002899867482483387 max memory_allocated 29274.51025390625 
[2025-03-23 00:33:44 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 12 loss:3.549469470977783 norm:0.0027675852179527283 max memory_allocated 29274.51025390625 
[2025-03-23 00:34:32 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 13 loss:3.547947883605957 norm:0.002689509652554989 max memory_allocated 29274.51025390625 
[2025-03-23 00:35:21 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 14 loss:3.5469300746917725 norm:0.0026162455324083567 max memory_allocated 29274.51025390625 
[2025-03-23 00:36:09 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 15 loss:3.5457100868225098 norm:0.0025816422421485186 max memory_allocated 29274.51025390625 
[2025-03-23 00:36:58 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 16 loss:3.544924020767212 norm:0.0026243897154927254 max memory_allocated 29274.51025390625 
[2025-03-23 00:37:46 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 17 loss:3.5439698696136475 norm:0.00257431180216372 max memory_allocated 29274.51025390625 
[2025-03-23 00:38:34 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 18 loss:3.5427372455596924 norm:0.0025309983175247908 max memory_allocated 29274.51025390625 
[2025-03-23 00:39:23 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 19 loss:3.5416154861450195 norm:0.002533215330913663 max memory_allocated 29274.51025390625 
[2025-03-23 00:39:37 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 36 ===
[2025-03-23 00:39:41 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 00:40:29 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 0 loss:4.015591621398926 norm:0.05075360834598541 max memory_allocated 29274.98681640625 
[2025-03-23 00:41:17 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 1 loss:3.9675726890563965 norm:0.04590392857789993 max memory_allocated 29274.98681640625 
[2025-03-23 00:42:06 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 2 loss:3.9147942066192627 norm:0.03700203448534012 max memory_allocated 29274.98681640625 
[2025-03-23 00:42:54 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 3 loss:3.89094614982605 norm:0.03127496317028999 max memory_allocated 29274.98681640625 
[2025-03-23 00:43:43 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 4 loss:3.8759117126464844 norm:0.027239205315709114 max memory_allocated 29274.98681640625 
[2025-03-23 00:44:31 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 5 loss:3.8639726638793945 norm:0.024840164929628372 max memory_allocated 29274.98681640625 
[2025-03-23 00:45:19 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 6 loss:3.854233741760254 norm:0.022100619971752167 max memory_allocated 29274.98681640625 
[2025-03-23 00:46:08 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 7 loss:3.8479251861572266 norm:0.020357707515358925 max memory_allocated 29274.98681640625 
[2025-03-23 00:46:56 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 8 loss:3.8430380821228027 norm:0.019271522760391235 max memory_allocated 29274.98681640625 
[2025-03-23 00:47:45 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 9 loss:3.839425563812256 norm:0.018454471603035927 max memory_allocated 29274.98681640625 
[2025-03-23 00:48:33 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 10 loss:3.836414098739624 norm:0.017907125875353813 max memory_allocated 29274.98681640625 
[2025-03-23 00:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 11 loss:3.834186315536499 norm:0.017438855022192 max memory_allocated 29274.98681640625 
[2025-03-23 00:50:11 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 12 loss:3.8325579166412354 norm:0.017031176015734673 max memory_allocated 29274.98681640625 
[2025-03-23 00:50:59 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 13 loss:3.8308725357055664 norm:0.016764933243393898 max memory_allocated 29274.98681640625 
[2025-03-23 00:51:48 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 14 loss:3.8289003372192383 norm:0.016621943563222885 max memory_allocated 29274.98681640625 
[2025-03-23 00:52:36 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 15 loss:3.827256679534912 norm:0.01642552949488163 max memory_allocated 29274.98681640625 
[2025-03-23 00:53:25 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 16 loss:3.8259670734405518 norm:0.016438163816928864 max memory_allocated 29274.98681640625 
[2025-03-23 00:54:14 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 17 loss:3.8249449729919434 norm:0.016256961971521378 max memory_allocated 29274.98681640625 
[2025-03-23 00:55:02 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 18 loss:3.8240137100219727 norm:0.01613292470574379 max memory_allocated 29274.98681640625 
[2025-03-23 00:55:51 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 19 loss:3.823187828063965 norm:0.01597629301249981 max memory_allocated 29274.98681640625 
[2025-03-23 00:56:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 37 ===
[2025-03-23 00:56:09 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 00:56:57 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 0 loss:4.483173370361328 norm:0.07410326600074768 max memory_allocated 29275.17431640625 
[2025-03-23 00:57:46 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 1 loss:4.4117112159729 norm:0.060288578271865845 max memory_allocated 29275.17431640625 
[2025-03-23 00:58:34 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 2 loss:4.336954593658447 norm:0.04539388418197632 max memory_allocated 29275.17431640625 
[2025-03-23 00:59:23 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 3 loss:4.302773952484131 norm:0.034422945231199265 max memory_allocated 29275.17431640625 
[2025-03-23 01:00:11 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 4 loss:4.282510280609131 norm:0.026195352897047997 max memory_allocated 29275.17431640625 
[2025-03-23 01:01:00 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 5 loss:4.2691650390625 norm:0.0222855806350708 max memory_allocated 29275.17431640625 
[2025-03-23 01:01:48 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 6 loss:4.260692596435547 norm:0.022456157952547073 max memory_allocated 29275.17431640625 
[2025-03-23 01:02:37 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 7 loss:4.2535600662231445 norm:0.02152717486023903 max memory_allocated 29275.17431640625 
[2025-03-23 01:03:25 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 8 loss:4.247107982635498 norm:0.02059117518365383 max memory_allocated 29275.17431640625 
[2025-03-23 01:04:13 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 9 loss:4.242343902587891 norm:0.019708413630723953 max memory_allocated 29275.17431640625 
[2025-03-23 01:05:02 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 10 loss:4.239140510559082 norm:0.018907062709331512 max memory_allocated 29275.17431640625 
[2025-03-23 01:05:50 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 11 loss:4.235572338104248 norm:0.018115689978003502 max memory_allocated 29275.17431640625 
[2025-03-23 01:06:39 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 12 loss:4.2330641746521 norm:0.01785160042345524 max memory_allocated 29275.17431640625 
[2025-03-23 01:07:27 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 13 loss:4.230055809020996 norm:0.017734011635184288 max memory_allocated 29275.17431640625 
[2025-03-23 01:08:16 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 14 loss:4.22800350189209 norm:0.017796436324715614 max memory_allocated 29275.17431640625 
[2025-03-23 01:09:04 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 15 loss:4.22683572769165 norm:0.01810700073838234 max memory_allocated 29275.17431640625 
[2025-03-23 01:09:53 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 16 loss:4.225312232971191 norm:0.018483201041817665 max memory_allocated 29275.17431640625 
[2025-03-23 01:10:42 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 17 loss:4.223575592041016 norm:0.0187689159065485 max memory_allocated 29275.17431640625 
[2025-03-23 01:11:30 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 18 loss:4.222894191741943 norm:0.019234241917729378 max memory_allocated 29275.17431640625 
[2025-03-23 01:12:19 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 19 loss:4.22408390045166 norm:0.020067065954208374 max memory_allocated 29275.17431640625 
[2025-03-23 01:12:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 38 ===
[2025-03-23 01:12:37 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 01:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 0 loss:6.001282215118408 norm:0.49281564354896545 max memory_allocated 29275.36181640625 
[2025-03-23 01:14:14 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 1 loss:5.6524786949157715 norm:0.3584567904472351 max memory_allocated 29275.36181640625 
[2025-03-23 01:15:02 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 2 loss:5.524484157562256 norm:0.27812641859054565 max memory_allocated 29275.36181640625 
[2025-03-23 01:15:51 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 3 loss:5.438230514526367 norm:0.21885362267494202 max memory_allocated 29275.36181640625 
[2025-03-23 01:16:39 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 4 loss:5.371129512786865 norm:0.17163777351379395 max memory_allocated 29275.36181640625 
[2025-03-23 01:17:28 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 5 loss:5.282135009765625 norm:0.10924254357814789 max memory_allocated 29275.36181640625 
[2025-03-23 01:18:16 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 6 loss:5.219357013702393 norm:0.09014659374952316 max memory_allocated 29275.36181640625 
[2025-03-23 01:19:05 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 7 loss:5.186160087585449 norm:0.08047826588153839 max memory_allocated 29275.36181640625 
[2025-03-23 01:19:54 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 8 loss:5.163506031036377 norm:0.06979569792747498 max memory_allocated 29275.36181640625 
[2025-03-23 01:20:42 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 9 loss:5.143863677978516 norm:0.06642704457044601 max memory_allocated 29275.36181640625 
[2025-03-23 01:21:31 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 10 loss:5.127861976623535 norm:0.06564074754714966 max memory_allocated 29275.36181640625 
[2025-03-23 01:22:20 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 11 loss:5.11330509185791 norm:0.06282198429107666 max memory_allocated 29275.36181640625 
[2025-03-23 01:23:08 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 12 loss:5.102581024169922 norm:0.06268259882926941 max memory_allocated 29275.36181640625 
[2025-03-23 01:23:57 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 13 loss:5.093896865844727 norm:0.06024814024567604 max memory_allocated 29275.36181640625 
[2025-03-23 01:24:45 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 14 loss:5.086805820465088 norm:0.05831176042556763 max memory_allocated 29275.36181640625 
[2025-03-23 01:25:34 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 15 loss:5.082707405090332 norm:0.05833725258708 max memory_allocated 29275.36181640625 
[2025-03-23 01:26:22 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 16 loss:5.072903633117676 norm:0.055662497878074646 max memory_allocated 29275.36181640625 
[2025-03-23 01:27:11 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 17 loss:5.0681891441345215 norm:0.053236108273267746 max memory_allocated 29275.36181640625 
[2025-03-23 01:27:59 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 18 loss:5.061132431030273 norm:0.04935979098081589 max memory_allocated 29275.36181640625 
[2025-03-23 01:28:47 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 19 loss:5.057681560516357 norm:0.04827415570616722 max memory_allocated 29275.36181640625 
[2025-03-23 01:29:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 39 ===
[2025-03-23 01:29:05 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 01:29:53 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 0 loss:9.889629364013672 norm:0.5422664880752563 max memory_allocated 29275.54931640625 
[2025-03-23 01:30:41 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 1 loss:9.438358306884766 norm:0.4280000627040863 max memory_allocated 29275.54931640625 
[2025-03-23 01:31:30 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 2 loss:8.987171173095703 norm:0.3604465425014496 max memory_allocated 29275.54931640625 
[2025-03-23 01:32:18 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 3 loss:8.717367172241211 norm:0.3608277142047882 max memory_allocated 29275.54931640625 
[2025-03-23 01:33:07 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 4 loss:8.58964729309082 norm:0.34921637177467346 max memory_allocated 29275.54931640625 
[2025-03-23 01:33:55 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 5 loss:8.513903617858887 norm:0.33188164234161377 max memory_allocated 29275.54931640625 
[2025-03-23 01:34:44 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 6 loss:8.427468299865723 norm:0.3213985562324524 max memory_allocated 29275.54931640625 
[2025-03-23 01:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 7 loss:8.354872703552246 norm:0.30102014541625977 max memory_allocated 29275.54931640625 
[2025-03-23 01:36:21 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 8 loss:8.275400161743164 norm:0.2885398268699646 max memory_allocated 29275.54931640625 
[2025-03-23 01:37:09 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 9 loss:8.232481956481934 norm:0.27868983149528503 max memory_allocated 29275.54931640625 
[2025-03-23 01:37:58 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 10 loss:8.203495979309082 norm:0.26829713582992554 max memory_allocated 29275.54931640625 
[2025-03-23 01:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 11 loss:8.177742958068848 norm:0.2574300467967987 max memory_allocated 29275.54931640625 
[2025-03-23 01:39:35 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 12 loss:8.15490436553955 norm:0.24279265105724335 max memory_allocated 29275.54931640625 
[2025-03-23 01:40:24 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 13 loss:8.142901420593262 norm:0.23817798495292664 max memory_allocated 29275.54931640625 
[2025-03-23 01:41:13 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 14 loss:8.134920120239258 norm:0.2375507950782776 max memory_allocated 29275.54931640625 
[2025-03-23 01:42:01 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 15 loss:8.1149320602417 norm:0.23367977142333984 max memory_allocated 29275.54931640625 
[2025-03-23 01:42:50 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 16 loss:8.100409507751465 norm:0.23238566517829895 max memory_allocated 29275.54931640625 
[2025-03-23 01:43:38 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 17 loss:8.095985412597656 norm:0.2448967546224594 max memory_allocated 29275.54931640625 
[2025-03-23 01:44:27 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 18 loss:8.070975303649902 norm:0.22592657804489136 max memory_allocated 29275.54931640625 
[2025-03-23 01:45:15 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 19 loss:8.056017875671387 norm:0.20875665545463562 max memory_allocated 29275.54931640625 
[2025-03-23 01:45:29 root] (main_calibration_a.py 369): INFO 39412.76582694054
[2025-03-23 01:45:46 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 01:47:38 root] (main_calibration_a.py 158): INFO wikitext2 : 7.589779376983643
[2025-03-23 01:47:38 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 01:50:32 root] (main_calibration_a.py 158): INFO c4 : 10.715372085571289
[2025-03-23 04:03:37 root] (main_calibration_a.py 169): INFO {'wikitext2': 7.589779376983643, 'c4': 10.715372085571289, 'results': {'hellaswag': {'acc': 0.4814777932682733, 'acc_stderr': 0.004986356526063967, 'acc_norm': 0.6336387173869747, 'acc_norm_stderr': 0.004808251269682429}, 'arc_easy': {'acc': 0.5075757575757576, 'acc_stderr': 0.010258605792153323, 'acc_norm': 0.4297138047138047, 'acc_norm_stderr': 0.01015790800576368}, 'winogrande': {'acc': 0.5872138910812944, 'acc_stderr': 0.013837060648682092}, 'arc_challenge': {'acc': 0.2764505119453925, 'acc_stderr': 0.013069662474252425, 'acc_norm': 0.32337883959044367, 'acc_norm_stderr': 0.013669421630012115}, 'boolq': {'acc': 0.636697247706422, 'acc_stderr': 0.008411885836787158}, 'piqa': {'acc': 0.6893362350380848, 'acc_stderr': 0.01079707893372768, 'acc_norm': 0.6724700761697497, 'acc_norm_stderr': 0.010949830482825471}}, 'versions': {'hellaswag': 0, 'arc_easy': 0, 'winogrande': 0, 'arc_challenge': 0, 'boolq': 1, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 04:03:37 root] (main_calibration_a.py 172): INFO 27.65,50.76,63.67,48.15,68.93,58.72
