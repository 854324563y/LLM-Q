[2025-03-22 01:14:40 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/Llama-2-13b-hf-w4a4-2', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=2)
[2025-03-22 01:14:51 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 01:14:51 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:14:52 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 01:15:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:15:21 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.06867904216051102 norm:0.06179095804691315 max memory_allocated 29271.23681640625 
[2025-03-22 01:16:56 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.0403316505253315 norm:0.03223378211259842 max memory_allocated 29271.23681640625 
[2025-03-22 01:17:44 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.03092196024954319 norm:0.024570999667048454 max memory_allocated 29271.23681640625 
[2025-03-22 01:18:33 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.026542779058218002 norm:0.019790643826127052 max memory_allocated 29271.23681640625 
[2025-03-22 01:19:21 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.02427130565047264 norm:0.01678904891014099 max memory_allocated 29271.23681640625 
[2025-03-22 01:20:09 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.023067370057106018 norm:0.015081769786775112 max memory_allocated 29271.23681640625 
[2025-03-22 01:20:57 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.022315455600619316 norm:0.013128810562193394 max memory_allocated 29271.23681640625 
[2025-03-22 01:21:46 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.021838685497641563 norm:0.012299950234591961 max memory_allocated 29271.23681640625 
[2025-03-22 01:22:34 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.021418780088424683 norm:0.010609282180666924 max memory_allocated 29271.23681640625 
[2025-03-22 01:23:22 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.020968910306692123 norm:0.009053977206349373 max memory_allocated 29271.23681640625 
[2025-03-22 01:24:11 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.020987460389733315 norm:0.008582878857851028 max memory_allocated 29271.23681640625 
[2025-03-22 01:24:59 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.020733367651700974 norm:0.011117946356534958 max memory_allocated 29271.23681640625 
[2025-03-22 01:25:47 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.02072254940867424 norm:0.007322604302316904 max memory_allocated 29271.23681640625 
[2025-03-22 01:26:36 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.020658276975154877 norm:0.006535357795655727 max memory_allocated 29271.23681640625 
[2025-03-22 01:27:24 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.020446520298719406 norm:0.006278902292251587 max memory_allocated 29271.23681640625 
[2025-03-22 01:28:12 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.020426353439688683 norm:0.006361382082104683 max memory_allocated 29271.23681640625 
[2025-03-22 01:29:01 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.02035418339073658 norm:0.00543250422924757 max memory_allocated 29271.23681640625 
[2025-03-22 01:29:49 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.020313888788223267 norm:0.005174365360289812 max memory_allocated 29271.23681640625 
[2025-03-22 01:30:37 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.02023140713572502 norm:0.005589156877249479 max memory_allocated 29271.23681640625 
[2025-03-22 01:31:26 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.020272400230169296 norm:0.0051353806629776955 max memory_allocated 29271.23681640625 
[2025-03-22 01:31:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:31:43 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:32:31 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.13804833590984344 norm:0.04411100968718529 max memory_allocated 29271.23681640625 
[2025-03-22 01:33:19 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.10376343131065369 norm:0.03374247997999191 max memory_allocated 29271.23681640625 
[2025-03-22 01:34:08 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.0880526751279831 norm:0.02314550243318081 max memory_allocated 29271.23681640625 
[2025-03-22 01:34:56 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.08160854876041412 norm:0.016800755634903908 max memory_allocated 29271.23681640625 
[2025-03-22 01:35:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.07864941656589508 norm:0.013469848781824112 max memory_allocated 29271.23681640625 
[2025-03-22 01:36:32 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.0766187459230423 norm:0.011166919022798538 max memory_allocated 29271.23681640625 
[2025-03-22 01:37:21 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.07502927631139755 norm:0.009471585974097252 max memory_allocated 29271.23681640625 
[2025-03-22 01:38:09 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.07400747388601303 norm:0.008146111853420734 max memory_allocated 29271.23681640625 
[2025-03-22 01:38:58 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.07314275205135345 norm:0.006999392993748188 max memory_allocated 29271.23681640625 
[2025-03-22 01:39:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.07270155102014542 norm:0.006340948864817619 max memory_allocated 29271.23681640625 
[2025-03-22 01:40:35 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.07232712209224701 norm:0.005825412459671497 max memory_allocated 29271.23681640625 
[2025-03-22 01:41:23 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.07189381122589111 norm:0.005521344020962715 max memory_allocated 29271.23681640625 
[2025-03-22 01:42:12 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.07151524722576141 norm:0.00522427586838603 max memory_allocated 29271.23681640625 
[2025-03-22 01:43:00 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.07133636623620987 norm:0.005178586579859257 max memory_allocated 29271.23681640625 
[2025-03-22 01:43:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.07116790860891342 norm:0.005067107267677784 max memory_allocated 29271.23681640625 
[2025-03-22 01:44:38 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07100005447864532 norm:0.00496595399454236 max memory_allocated 29271.23681640625 
[2025-03-22 01:45:26 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.0708189308643341 norm:0.004825013689696789 max memory_allocated 29271.23681640625 
[2025-03-22 01:46:15 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07075604051351547 norm:0.004712282679975033 max memory_allocated 29271.23681640625 
[2025-03-22 01:47:03 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07064473628997803 norm:0.004553400911390781 max memory_allocated 29271.23681640625 
[2025-03-22 01:47:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07056818902492523 norm:0.004568726290017366 max memory_allocated 29271.23681640625 
[2025-03-22 01:48:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:48:09 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:48:58 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.15852634608745575 norm:0.02916276641190052 max memory_allocated 29271.23681640625 
[2025-03-22 01:49:46 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.13567383587360382 norm:0.02287982776761055 max memory_allocated 29271.23681640625 
[2025-03-22 01:50:35 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.12271159887313843 norm:0.016255352646112442 max memory_allocated 29271.23681640625 
[2025-03-22 01:51:23 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.11649561673402786 norm:0.011809003539383411 max memory_allocated 29271.23681640625 
[2025-03-22 01:52:11 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.11307798326015472 norm:0.008537828922271729 max memory_allocated 29271.23681640625 
[2025-03-22 01:53:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.11080965399742126 norm:0.006253998726606369 max memory_allocated 29271.23681640625 
[2025-03-22 01:53:48 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.10948821157217026 norm:0.005466930102556944 max memory_allocated 29271.23681640625 
[2025-03-22 01:54:36 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.10867105424404144 norm:0.0051283081993460655 max memory_allocated 29271.23681640625 
[2025-03-22 01:55:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.10815779119729996 norm:0.005181554704904556 max memory_allocated 29271.23681640625 
[2025-03-22 01:56:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.10774574428796768 norm:0.004935999400913715 max memory_allocated 29271.23681640625 
[2025-03-22 01:57:02 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.10755620896816254 norm:0.004806883167475462 max memory_allocated 29271.23681640625 
[2025-03-22 01:57:50 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.10730493813753128 norm:0.00467093987390399 max memory_allocated 29271.23681640625 
[2025-03-22 01:58:39 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.1071125715970993 norm:0.004634646233171225 max memory_allocated 29271.23681640625 
[2025-03-22 01:59:28 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.10700317472219467 norm:0.004520272836089134 max memory_allocated 29271.23681640625 
[2025-03-22 02:00:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.10706276446580887 norm:0.004507947247475386 max memory_allocated 29271.23681640625 
[2025-03-22 02:01:05 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.10706233978271484 norm:0.00437175901606679 max memory_allocated 29271.23681640625 
[2025-03-22 02:01:53 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.10711979866027832 norm:0.004361532628536224 max memory_allocated 29271.23681640625 
[2025-03-22 02:02:42 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.10718496143817902 norm:0.004264793358743191 max memory_allocated 29271.23681640625 
[2025-03-22 02:03:31 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.10707007348537445 norm:0.004182934295386076 max memory_allocated 29271.23681640625 
[2025-03-22 02:04:19 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.10707827657461166 norm:0.004120460711419582 max memory_allocated 29271.23681640625 
[2025-03-22 02:04:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 02:05:25 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.3645082116127014 norm:0.1655707061290741 max memory_allocated 29271.51025390625 
[2025-03-22 02:06:13 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.2691192924976349 norm:0.03968003764748573 max memory_allocated 29271.51025390625 
[2025-03-22 02:07:01 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.23590736091136932 norm:0.02282112091779709 max memory_allocated 29271.51025390625 
[2025-03-22 02:07:50 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.2187894582748413 norm:0.02078338898718357 max memory_allocated 29271.51025390625 
[2025-03-22 02:08:38 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.2053614854812622 norm:0.016775745898485184 max memory_allocated 29271.51025390625 
[2025-03-22 02:09:26 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.19552218914031982 norm:0.014354070648550987 max memory_allocated 29271.51025390625 
[2025-03-22 02:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.1929553747177124 norm:0.014866180717945099 max memory_allocated 29271.51025390625 
[2025-03-22 02:11:02 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.18799039721488953 norm:0.01235213503241539 max memory_allocated 29271.51025390625 
[2025-03-22 02:11:50 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.1851503700017929 norm:0.011521182022988796 max memory_allocated 29271.51025390625 
[2025-03-22 02:12:38 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.1825479120016098 norm:0.011446108110249043 max memory_allocated 29271.51025390625 
[2025-03-22 02:13:26 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.18238675594329834 norm:0.011622130870819092 max memory_allocated 29271.51025390625 
[2025-03-22 02:14:15 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.17883934080600739 norm:0.010891486890614033 max memory_allocated 29271.51025390625 
[2025-03-22 02:15:03 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.18355685472488403 norm:0.013546643778681755 max memory_allocated 29271.51025390625 
[2025-03-22 02:15:52 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.18374496698379517 norm:0.015072082169353962 max memory_allocated 29271.51025390625 
[2025-03-22 02:16:40 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.17725354433059692 norm:0.011165379546582699 max memory_allocated 29271.51025390625 
[2025-03-22 02:17:28 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.17690788209438324 norm:0.011565104126930237 max memory_allocated 29271.51025390625 
[2025-03-22 02:18:17 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.179273322224617 norm:0.012302671559154987 max memory_allocated 29271.51025390625 
[2025-03-22 02:19:05 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.18393456935882568 norm:0.013999256305396557 max memory_allocated 29271.51025390625 
[2025-03-22 02:19:54 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.1822766661643982 norm:0.01380984578281641 max memory_allocated 29271.51025390625 
[2025-03-22 02:20:42 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.18352243304252625 norm:0.013913668692111969 max memory_allocated 29271.51025390625 
[2025-03-22 02:20:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:21:48 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.22739265859127045 norm:0.00984133780002594 max memory_allocated 29271.51025390625 
[2025-03-22 02:22:36 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.20677371323108673 norm:0.004270552657544613 max memory_allocated 29271.51025390625 
[2025-03-22 02:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.1932477205991745 norm:0.0023406734690070152 max memory_allocated 29271.51025390625 
[2025-03-22 02:24:13 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.1882808804512024 norm:0.0018163914792239666 max memory_allocated 29271.51025390625 
[2025-03-22 02:25:01 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.18497565388679504 norm:0.001364150084555149 max memory_allocated 29271.51025390625 
[2025-03-22 02:25:49 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.18274734914302826 norm:0.0011322530917823315 max memory_allocated 29271.51025390625 
[2025-03-22 02:26:37 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.18136899173259735 norm:0.0010253338841721416 max memory_allocated 29271.51025390625 
[2025-03-22 02:27:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.18076154589653015 norm:0.0009998026071116328 max memory_allocated 29271.51025390625 
[2025-03-22 02:28:14 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.180198073387146 norm:0.0009676717454567552 max memory_allocated 29271.51025390625 
[2025-03-22 02:29:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.17966262996196747 norm:0.0009570735273882747 max memory_allocated 29271.51025390625 
[2025-03-22 02:29:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.17926734685897827 norm:0.0009642430813983083 max memory_allocated 29271.51025390625 
[2025-03-22 02:30:38 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.17888040840625763 norm:0.0009589808178134263 max memory_allocated 29271.51025390625 
[2025-03-22 02:31:27 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.1783725917339325 norm:0.0009221144136972725 max memory_allocated 29271.51025390625 
[2025-03-22 02:32:15 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.17816585302352905 norm:0.0009063187753781676 max memory_allocated 29271.51025390625 
[2025-03-22 02:33:03 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.17794963717460632 norm:0.0009070776286534965 max memory_allocated 29271.51025390625 
[2025-03-22 02:33:52 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.17773690819740295 norm:0.0008929530740715563 max memory_allocated 29271.51025390625 
[2025-03-22 02:34:40 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.1773892641067505 norm:0.0008833094616420567 max memory_allocated 29271.51025390625 
[2025-03-22 02:35:29 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.17707446217536926 norm:0.0008720987825654447 max memory_allocated 29271.51025390625 
[2025-03-22 02:36:17 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.17697331309318542 norm:0.0008664929191581905 max memory_allocated 29271.51025390625 
[2025-03-22 02:37:06 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.17686647176742554 norm:0.0008664830238558352 max memory_allocated 29271.51025390625 
[2025-03-22 02:37:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:38:11 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.2598133087158203 norm:0.012355856597423553 max memory_allocated 29271.51025390625 
[2025-03-22 02:39:00 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2358555793762207 norm:0.004997989162802696 max memory_allocated 29271.51025390625 
[2025-03-22 02:39:48 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.22003304958343506 norm:0.0032069594599306583 max memory_allocated 29271.51025390625 
[2025-03-22 02:40:36 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.21399825811386108 norm:0.002137514064088464 max memory_allocated 29271.51025390625 
[2025-03-22 02:41:25 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.2108212262392044 norm:0.0016392313409596682 max memory_allocated 29271.51025390625 
[2025-03-22 02:42:13 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.20852245390415192 norm:0.001267602783627808 max memory_allocated 29271.51025390625 
[2025-03-22 02:43:01 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.2069980800151825 norm:0.00114265491720289 max memory_allocated 29271.51025390625 
[2025-03-22 02:43:49 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.20598238706588745 norm:0.0011195323895663023 max memory_allocated 29271.51025390625 
[2025-03-22 02:44:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.20511403679847717 norm:0.0010595459025353193 max memory_allocated 29271.51025390625 
[2025-03-22 02:45:26 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.20432451367378235 norm:0.0010094037279486656 max memory_allocated 29271.51025390625 
[2025-03-22 02:46:14 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.20388932526111603 norm:0.0009968457743525505 max memory_allocated 29271.51025390625 
[2025-03-22 02:47:02 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.20348325371742249 norm:0.0009546279907226562 max memory_allocated 29271.51025390625 
[2025-03-22 02:47:50 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.20328786969184875 norm:0.0009428746998310089 max memory_allocated 29271.51025390625 
[2025-03-22 02:48:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.203041672706604 norm:0.0009130349499173462 max memory_allocated 29271.51025390625 
[2025-03-22 02:49:27 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.20296509563922882 norm:0.0009268356952816248 max memory_allocated 29271.51025390625 
[2025-03-22 02:50:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.202772855758667 norm:0.0009013514500111341 max memory_allocated 29271.51025390625 
[2025-03-22 02:51:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.20245161652565002 norm:0.0008839997462928295 max memory_allocated 29271.51025390625 
[2025-03-22 02:51:52 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.20237790048122406 norm:0.0008857763023115695 max memory_allocated 29271.51025390625 
[2025-03-22 02:52:41 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.20229151844978333 norm:0.0008727727690711617 max memory_allocated 29271.51025390625 
[2025-03-22 02:53:29 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.2021266669034958 norm:0.0008552736835554242 max memory_allocated 29271.51025390625 
[2025-03-22 02:53:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:54:35 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.29564639925956726 norm:0.023921210318803787 max memory_allocated 29272.07275390625 
[2025-03-22 02:55:24 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.26800185441970825 norm:0.011816943064332008 max memory_allocated 29272.07275390625 
[2025-03-22 02:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.24451017379760742 norm:0.0031778672710061073 max memory_allocated 29272.07275390625 
[2025-03-22 02:57:00 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.23728911578655243 norm:0.0015649771085008979 max memory_allocated 29272.07275390625 
[2025-03-22 02:57:48 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.2334234118461609 norm:0.001327328965999186 max memory_allocated 29272.07275390625 
[2025-03-22 02:58:37 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.23074015974998474 norm:0.0012082972098141909 max memory_allocated 29272.07275390625 
[2025-03-22 02:59:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.2288231998682022 norm:0.0011290294351056218 max memory_allocated 29272.07275390625 
[2025-03-22 03:00:13 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.22740790247917175 norm:0.0011037999065592885 max memory_allocated 29272.07275390625 
[2025-03-22 03:01:01 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.22623732686042786 norm:0.0010324164759367704 max memory_allocated 29272.07275390625 
[2025-03-22 03:01:49 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.2253611832857132 norm:0.0010069924173876643 max memory_allocated 29272.07275390625 
[2025-03-22 03:02:38 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.22475354373455048 norm:0.0009790105978026986 max memory_allocated 29272.07275390625 
[2025-03-22 03:03:26 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.22432680428028107 norm:0.0009709229343570769 max memory_allocated 29272.07275390625 
[2025-03-22 03:04:14 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.22404992580413818 norm:0.0009624933591112494 max memory_allocated 29272.07275390625 
[2025-03-22 03:05:03 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.2237597405910492 norm:0.0009541316539980471 max memory_allocated 29272.07275390625 
[2025-03-22 03:05:51 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.22355975210666656 norm:0.0009463604656048119 max memory_allocated 29272.07275390625 
[2025-03-22 03:06:39 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.22340232133865356 norm:0.0009484761394560337 max memory_allocated 29272.07275390625 
[2025-03-22 03:07:28 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.22329406440258026 norm:0.0009414886590093374 max memory_allocated 29272.07275390625 
[2025-03-22 03:08:16 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.22318004071712494 norm:0.0009355322108604014 max memory_allocated 29272.07275390625 
[2025-03-22 03:09:05 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.22315648198127747 norm:0.0009380584815517068 max memory_allocated 29272.07275390625 
[2025-03-22 03:09:53 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.2231127768754959 norm:0.0009290319285355508 max memory_allocated 29272.07275390625 
[2025-03-22 03:10:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 03:11:00 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.3443378508090973 norm:0.013708757236599922 max memory_allocated 29272.26025390625 
[2025-03-22 03:11:48 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.30455687642097473 norm:0.005203670356422663 max memory_allocated 29272.26025390625 
[2025-03-22 03:12:36 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.2753759026527405 norm:0.0022738962434232235 max memory_allocated 29272.26025390625 
[2025-03-22 03:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.2665053606033325 norm:0.0015697148628532887 max memory_allocated 29272.26025390625 
[2025-03-22 03:14:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.26243552565574646 norm:0.0013449469115585089 max memory_allocated 29272.26025390625 
[2025-03-22 03:15:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.25902941823005676 norm:0.0012395362136885524 max memory_allocated 29272.26025390625 
[2025-03-22 03:15:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.25672149658203125 norm:0.0011585033498704433 max memory_allocated 29272.26025390625 
[2025-03-22 03:16:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.25491154193878174 norm:0.0010840939357876778 max memory_allocated 29272.26025390625 
[2025-03-22 03:17:26 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.2538054585456848 norm:0.0010533917229622602 max memory_allocated 29272.26025390625 
[2025-03-22 03:18:14 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.25314903259277344 norm:0.0010397713631391525 max memory_allocated 29272.26025390625 
[2025-03-22 03:19:02 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.25252851843833923 norm:0.0010362485190853477 max memory_allocated 29272.26025390625 
[2025-03-22 03:19:51 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.2521785795688629 norm:0.0010393200209364295 max memory_allocated 29272.26025390625 
[2025-03-22 03:20:39 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.2519051134586334 norm:0.0010288077173754573 max memory_allocated 29272.26025390625 
[2025-03-22 03:21:28 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.25169050693511963 norm:0.0010274857049807906 max memory_allocated 29272.26025390625 
[2025-03-22 03:22:16 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.2514974772930145 norm:0.0010262730065733194 max memory_allocated 29272.26025390625 
[2025-03-22 03:23:04 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.2512810230255127 norm:0.0010247137397527695 max memory_allocated 29272.26025390625 
[2025-03-22 03:23:53 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.25109848380088806 norm:0.0010248079197481275 max memory_allocated 29272.26025390625 
[2025-03-22 03:24:41 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.2510325610637665 norm:0.0010174121707677841 max memory_allocated 29272.26025390625 
[2025-03-22 03:25:30 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.25082847476005554 norm:0.001023756107315421 max memory_allocated 29272.26025390625 
[2025-03-22 03:26:18 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.25053662061691284 norm:0.00103852862957865 max memory_allocated 29272.26025390625 
[2025-03-22 03:26:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 03:27:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.3696275055408478 norm:0.031978707760572433 max memory_allocated 29272.44775390625 
[2025-03-22 03:28:13 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.33550775051116943 norm:0.0207064189016819 max memory_allocated 29272.44775390625 
[2025-03-22 03:29:01 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.30467668175697327 norm:0.012971206568181515 max memory_allocated 29272.44775390625 
[2025-03-22 03:29:49 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.2925666570663452 norm:0.008546479046344757 max memory_allocated 29272.44775390625 
[2025-03-22 03:30:38 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.28461432456970215 norm:0.0038120278622955084 max memory_allocated 29272.44775390625 
[2025-03-22 03:31:26 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.27938055992126465 norm:0.0014655559789389372 max memory_allocated 29272.44775390625 
[2025-03-22 03:32:14 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.2763075530529022 norm:0.001317937159910798 max memory_allocated 29272.44775390625 
[2025-03-22 03:33:02 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.27425462007522583 norm:0.0012123846681788564 max memory_allocated 29272.44775390625 
[2025-03-22 03:33:51 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.27277424931526184 norm:0.0011457137297838926 max memory_allocated 29272.44775390625 
[2025-03-22 03:34:39 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.27178025245666504 norm:0.0010998088400810957 max memory_allocated 29272.44775390625 
[2025-03-22 03:35:27 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.27098894119262695 norm:0.0010701403953135014 max memory_allocated 29272.44775390625 
[2025-03-22 03:36:15 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.270484983921051 norm:0.001054286491125822 max memory_allocated 29272.44775390625 
[2025-03-22 03:37:04 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.2699187099933624 norm:0.0010514154564589262 max memory_allocated 29272.44775390625 
[2025-03-22 03:37:52 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.2694741487503052 norm:0.0010325434850528836 max memory_allocated 29272.44775390625 
[2025-03-22 03:38:40 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.2692837715148926 norm:0.0010095486650243402 max memory_allocated 29272.44775390625 
[2025-03-22 03:39:29 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.26903262734413147 norm:0.000997077440842986 max memory_allocated 29272.44775390625 
[2025-03-22 03:40:17 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.2688426971435547 norm:0.0009859040146693587 max memory_allocated 29272.44775390625 
[2025-03-22 03:41:06 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.26869726181030273 norm:0.0009674234897829592 max memory_allocated 29272.44775390625 
[2025-03-22 03:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.26863014698028564 norm:0.0009677726775407791 max memory_allocated 29272.44775390625 
[2025-03-22 03:42:43 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.26852238178253174 norm:0.0009516168502159417 max memory_allocated 29272.44775390625 
[2025-03-22 03:42:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 03:43:49 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.42944881319999695 norm:0.02977573126554489 max memory_allocated 29272.63525390625 
[2025-03-22 03:44:37 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.37329262495040894 norm:0.01179977785795927 max memory_allocated 29272.63525390625 
[2025-03-22 03:45:25 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.3272944390773773 norm:0.00468137301504612 max memory_allocated 29272.63525390625 
[2025-03-22 03:46:14 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.3109125792980194 norm:0.002560352673754096 max memory_allocated 29272.63525390625 
[2025-03-22 03:47:02 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.3045269250869751 norm:0.001987248659133911 max memory_allocated 29272.63525390625 
[2025-03-22 03:47:51 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.30053144693374634 norm:0.0016475191805511713 max memory_allocated 29272.63525390625 
[2025-03-22 03:48:39 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.29772111773490906 norm:0.0014555453089997172 max memory_allocated 29272.63525390625 
[2025-03-22 03:49:27 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.2955636978149414 norm:0.001293784356676042 max memory_allocated 29272.63525390625 
[2025-03-22 03:50:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.29385513067245483 norm:0.0011720183538272977 max memory_allocated 29272.63525390625 
[2025-03-22 03:51:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.292741596698761 norm:0.0011446713469922543 max memory_allocated 29272.63525390625 
[2025-03-22 03:51:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.29185470938682556 norm:0.001101621426641941 max memory_allocated 29272.63525390625 
[2025-03-22 03:52:40 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.2912902235984802 norm:0.0010770820081233978 max memory_allocated 29272.63525390625 
[2025-03-22 03:53:28 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.29085245728492737 norm:0.0010682771680876613 max memory_allocated 29272.63525390625 
[2025-03-22 03:54:16 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.2905713617801666 norm:0.0010502299992367625 max memory_allocated 29272.63525390625 
[2025-03-22 03:55:05 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.29049426317214966 norm:0.0010696706594899297 max memory_allocated 29272.63525390625 
[2025-03-22 03:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.29021355509757996 norm:0.001054763444699347 max memory_allocated 29272.63525390625 
[2025-03-22 03:56:42 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.2900174856185913 norm:0.0010546570410951972 max memory_allocated 29272.63525390625 
[2025-03-22 03:57:30 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.2898310422897339 norm:0.0010521386284381151 max memory_allocated 29272.63525390625 
[2025-03-22 03:58:19 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.28934529423713684 norm:0.0010020425543189049 max memory_allocated 29272.63525390625 
[2025-03-22 03:59:07 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.28916123509407043 norm:0.0009771316545084119 max memory_allocated 29272.63525390625 
[2025-03-22 03:59:21 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 04:00:13 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.41576650738716125 norm:0.020705873146653175 max memory_allocated 29272.82275390625 
[2025-03-22 04:01:02 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.37187743186950684 norm:0.007919078692793846 max memory_allocated 29272.82275390625 
[2025-03-22 04:01:50 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.3374667167663574 norm:0.0032180154230445623 max memory_allocated 29272.82275390625 
[2025-03-22 04:02:38 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3246142864227295 norm:0.0019591364543884993 max memory_allocated 29272.82275390625 
[2025-03-22 04:03:27 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.31926026940345764 norm:0.0015686932019889355 max memory_allocated 29272.82275390625 
[2025-03-22 04:04:15 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.3158252239227295 norm:0.001339616603218019 max memory_allocated 29272.82275390625 
[2025-03-22 04:05:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3133127689361572 norm:0.0012068983633071184 max memory_allocated 29272.82275390625 
[2025-03-22 04:05:51 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.31159505248069763 norm:0.0011327117681503296 max memory_allocated 29272.82275390625 
[2025-03-22 04:06:40 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.31017622351646423 norm:0.0010585944401100278 max memory_allocated 29272.82275390625 
[2025-03-22 04:07:28 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.3091762065887451 norm:0.0009969122475013137 max memory_allocated 29272.82275390625 
[2025-03-22 04:08:16 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.3084091544151306 norm:0.0009592188871465623 max memory_allocated 29272.82275390625 
[2025-03-22 04:09:04 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.30788078904151917 norm:0.0009265352855436504 max memory_allocated 29272.82275390625 
[2025-03-22 04:09:53 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.3074662983417511 norm:0.0009192009456455708 max memory_allocated 29272.82275390625 
[2025-03-22 04:10:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3071029782295227 norm:0.0009037640993483365 max memory_allocated 29272.82275390625 
[2025-03-22 04:11:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3068256080150604 norm:0.0008896407671272755 max memory_allocated 29272.82275390625 
[2025-03-22 04:12:18 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.30655884742736816 norm:0.0008609522483311594 max memory_allocated 29272.82275390625 
[2025-03-22 04:13:06 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.3063393831253052 norm:0.0008501693955622613 max memory_allocated 29272.82275390625 
[2025-03-22 04:13:55 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3060828447341919 norm:0.0008321291534230113 max memory_allocated 29272.82275390625 
[2025-03-22 04:14:43 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.3058510422706604 norm:0.0008283221977762878 max memory_allocated 29272.82275390625 
[2025-03-22 04:15:32 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.3057643175125122 norm:0.0008357090409845114 max memory_allocated 29272.82275390625 
[2025-03-22 04:15:46 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 04:16:38 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.4264615476131439 norm:0.028153151273727417 max memory_allocated 29273.01025390625 
[2025-03-22 04:17:26 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.3891461491584778 norm:0.011142858304083347 max memory_allocated 29273.01025390625 
[2025-03-22 04:18:15 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3524303436279297 norm:0.00380307761952281 max memory_allocated 29273.01025390625 
[2025-03-22 04:19:03 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.34010979533195496 norm:0.0017764420481398702 max memory_allocated 29273.01025390625 
[2025-03-22 04:19:51 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.33477452397346497 norm:0.0011227078502997756 max memory_allocated 29273.01025390625 
[2025-03-22 04:20:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.33129599690437317 norm:0.0009976897854357958 max memory_allocated 29273.01025390625 
[2025-03-22 04:21:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.32875072956085205 norm:0.0009404940064996481 max memory_allocated 29273.01025390625 
[2025-03-22 04:22:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3269555866718292 norm:0.0008921626722440124 max memory_allocated 29273.01025390625 
[2025-03-22 04:23:04 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.32567811012268066 norm:0.0008588289492763579 max memory_allocated 29273.01025390625 
[2025-03-22 04:23:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.3246648609638214 norm:0.0008274427964352071 max memory_allocated 29273.01025390625 
[2025-03-22 04:24:41 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.3239813446998596 norm:0.0008090712944976985 max memory_allocated 29273.01025390625 
[2025-03-22 04:25:29 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.3234105110168457 norm:0.0007962163654156029 max memory_allocated 29273.01025390625 
[2025-03-22 04:26:18 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3229398727416992 norm:0.0007873285794630647 max memory_allocated 29273.01025390625 
[2025-03-22 04:27:06 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.3226783275604248 norm:0.0007904020021669567 max memory_allocated 29273.01025390625 
[2025-03-22 04:27:54 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3224383294582367 norm:0.0007897044997662306 max memory_allocated 29273.01025390625 
[2025-03-22 04:28:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.32216936349868774 norm:0.0007593401242047548 max memory_allocated 29273.01025390625 
[2025-03-22 04:29:31 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.3220111131668091 norm:0.0007491880096495152 max memory_allocated 29273.01025390625 
[2025-03-22 04:30:20 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.32181844115257263 norm:0.0007489880081266165 max memory_allocated 29273.01025390625 
[2025-03-22 04:31:08 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.3215973377227783 norm:0.0007317786221392453 max memory_allocated 29273.01025390625 
[2025-03-22 04:31:57 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.3214889466762543 norm:0.000723842007573694 max memory_allocated 29273.01025390625 
[2025-03-22 04:32:11 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 04:33:03 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.44527173042297363 norm:0.016329407691955566 max memory_allocated 29273.19775390625 
[2025-03-22 04:33:51 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.3988894522190094 norm:0.006418986711651087 max memory_allocated 29273.19775390625 
[2025-03-22 04:34:39 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.36176174879074097 norm:0.002668483182787895 max memory_allocated 29273.19775390625 
[2025-03-22 04:35:28 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.3477003574371338 norm:0.0015083866892382503 max memory_allocated 29273.19775390625 
[2025-03-22 04:36:16 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.3424016237258911 norm:0.0011357198236510158 max memory_allocated 29273.19775390625 
[2025-03-22 04:37:04 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.33899012207984924 norm:0.000987824983894825 max memory_allocated 29273.19775390625 
[2025-03-22 04:37:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.3364601135253906 norm:0.0009258213103748858 max memory_allocated 29273.19775390625 
[2025-03-22 04:38:41 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.33458322286605835 norm:0.0008739334880374372 max memory_allocated 29273.19775390625 
[2025-03-22 04:39:29 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.33333897590637207 norm:0.0008530896157026291 max memory_allocated 29273.19775390625 
[2025-03-22 04:40:17 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.3324410617351532 norm:0.0008212709799408913 max memory_allocated 29273.19775390625 
[2025-03-22 04:41:06 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.33175504207611084 norm:0.0007870022091083229 max memory_allocated 29273.19775390625 
[2025-03-22 04:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.3311888575553894 norm:0.0007613259949721396 max memory_allocated 29273.19775390625 
[2025-03-22 04:42:42 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.3307252824306488 norm:0.0007464074296876788 max memory_allocated 29273.19775390625 
[2025-03-22 04:43:31 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.3303394019603729 norm:0.0007409721147269011 max memory_allocated 29273.19775390625 
[2025-03-22 04:44:19 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.3300374746322632 norm:0.0007283819140866399 max memory_allocated 29273.19775390625 
[2025-03-22 04:45:08 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.32976803183555603 norm:0.0007193636847659945 max memory_allocated 29273.19775390625 
[2025-03-22 04:45:56 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.32957223057746887 norm:0.0007279911660589278 max memory_allocated 29273.19775390625 
[2025-03-22 04:46:45 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.3293440341949463 norm:0.0007224322762340307 max memory_allocated 29273.19775390625 
[2025-03-22 04:47:34 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.32917171716690063 norm:0.0007180845132097602 max memory_allocated 29273.19775390625 
[2025-03-22 04:48:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.32903313636779785 norm:0.0007114764885045588 max memory_allocated 29273.19775390625 
[2025-03-22 04:48:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 04:49:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.43971484899520874 norm:0.02137010172009468 max memory_allocated 29273.38525390625 
[2025-03-22 04:50:16 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.4027664363384247 norm:0.009681754745543003 max memory_allocated 29273.38525390625 
[2025-03-22 04:51:04 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.36985817551612854 norm:0.004285753238946199 max memory_allocated 29273.38525390625 
[2025-03-22 04:51:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.3569470942020416 norm:0.001742308959364891 max memory_allocated 29273.38525390625 
[2025-03-22 04:52:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.35185912251472473 norm:0.0012730314629152417 max memory_allocated 29273.38525390625 
[2025-03-22 04:53:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3482479751110077 norm:0.0010933695593848825 max memory_allocated 29273.38525390625 
[2025-03-22 04:54:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3456866443157196 norm:0.000996296526864171 max memory_allocated 29273.38525390625 
[2025-03-22 04:55:06 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.3439839482307434 norm:0.0009512888500466943 max memory_allocated 29273.38525390625 
[2025-03-22 04:55:54 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.3426373302936554 norm:0.0008931941702030599 max memory_allocated 29273.38525390625 
[2025-03-22 04:56:42 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.34160637855529785 norm:0.0008482146076858044 max memory_allocated 29273.38525390625 
[2025-03-22 04:57:31 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3408234715461731 norm:0.0008371265139430761 max memory_allocated 29273.38525390625 
[2025-03-22 04:58:19 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.34025999903678894 norm:0.0008173878886736929 max memory_allocated 29273.38525390625 
[2025-03-22 04:59:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.33985182642936707 norm:0.0008070914191193879 max memory_allocated 29273.38525390625 
[2025-03-22 04:59:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3394656479358673 norm:0.0007809142116457224 max memory_allocated 29273.38525390625 
[2025-03-22 05:00:45 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.339181512594223 norm:0.0007684208685532212 max memory_allocated 29273.38525390625 
[2025-03-22 05:01:33 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3390570878982544 norm:0.0007677882676944137 max memory_allocated 29273.38525390625 
[2025-03-22 05:02:22 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.3388359844684601 norm:0.000764358788728714 max memory_allocated 29273.38525390625 
[2025-03-22 05:03:10 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.33861374855041504 norm:0.0007468403782695532 max memory_allocated 29273.38525390625 
[2025-03-22 05:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.33838027715682983 norm:0.0007484173984266818 max memory_allocated 29273.38525390625 
[2025-03-22 05:04:47 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.3382391631603241 norm:0.0007445095106959343 max memory_allocated 29273.38525390625 
[2025-03-22 05:05:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 05:05:54 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.4445665180683136 norm:0.01827254146337509 max memory_allocated 29273.57275390625 
[2025-03-22 05:06:42 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.40575122833251953 norm:0.009229504503309727 max memory_allocated 29273.57275390625 
[2025-03-22 05:07:30 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.38047125935554504 norm:0.005365506745874882 max memory_allocated 29273.57275390625 
[2025-03-22 05:08:18 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.36928269267082214 norm:0.003346556331962347 max memory_allocated 29273.57275390625 
[2025-03-22 05:09:07 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.36382582783699036 norm:0.002430178690701723 max memory_allocated 29273.57275390625 
[2025-03-22 05:09:55 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.36019352078437805 norm:0.001851876499131322 max memory_allocated 29273.57275390625 
[2025-03-22 05:10:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.3577074706554413 norm:0.0015724798431620002 max memory_allocated 29273.57275390625 
[2025-03-22 05:11:32 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.355724960565567 norm:0.0012958517763763666 max memory_allocated 29273.57275390625 
[2025-03-22 05:12:20 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.354299932718277 norm:0.0010556665947660804 max memory_allocated 29273.57275390625 
[2025-03-22 05:13:08 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.35330694913864136 norm:0.0009758297237567604 max memory_allocated 29273.57275390625 
[2025-03-22 05:13:57 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.3525577485561371 norm:0.0009108144440688193 max memory_allocated 29273.57275390625 
[2025-03-22 05:14:45 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.3520573377609253 norm:0.0009063567849807441 max memory_allocated 29273.57275390625 
[2025-03-22 05:15:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.3516940474510193 norm:0.0008967723697423935 max memory_allocated 29273.57275390625 
[2025-03-22 05:16:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.35132864117622375 norm:0.0008681065519340336 max memory_allocated 29273.57275390625 
[2025-03-22 05:17:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.35100993514060974 norm:0.0008507916936650872 max memory_allocated 29273.57275390625 
[2025-03-22 05:17:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.35067471861839294 norm:0.0008173722890205681 max memory_allocated 29273.57275390625 
[2025-03-22 05:18:48 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.35040634870529175 norm:0.0007846917142160237 max memory_allocated 29273.57275390625 
[2025-03-22 05:19:36 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.35015344619750977 norm:0.0007625266443938017 max memory_allocated 29273.57275390625 
[2025-03-22 05:20:25 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.3500000536441803 norm:0.0007499590283259749 max memory_allocated 29273.57275390625 
[2025-03-22 05:21:13 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.34985893964767456 norm:0.000739511102437973 max memory_allocated 29273.57275390625 
[2025-03-22 05:21:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 05:22:19 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.43828585743904114 norm:0.01308276318013668 max memory_allocated 29273.76025390625 
[2025-03-22 05:23:07 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.4092146158218384 norm:0.0067480928264558315 max memory_allocated 29273.76025390625 
[2025-03-22 05:23:55 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.3830649256706238 norm:0.0035701566375792027 max memory_allocated 29273.76025390625 
[2025-03-22 05:24:43 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.37191304564476013 norm:0.0019252817146480083 max memory_allocated 29273.76025390625 
[2025-03-22 05:25:31 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.366793692111969 norm:0.0012838230468332767 max memory_allocated 29273.76025390625 
[2025-03-22 05:26:20 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.3635967969894409 norm:0.0010293114464730024 max memory_allocated 29273.76025390625 
[2025-03-22 05:27:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.3612733483314514 norm:0.000952850270550698 max memory_allocated 29273.76025390625 
[2025-03-22 05:27:56 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.3597102165222168 norm:0.0008999654091894627 max memory_allocated 29273.76025390625 
[2025-03-22 05:28:45 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.3584655225276947 norm:0.0008535312372259796 max memory_allocated 29273.76025390625 
[2025-03-22 05:29:33 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.3574576675891876 norm:0.0008143698796629906 max memory_allocated 29273.76025390625 
[2025-03-22 05:30:22 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.3566666543483734 norm:0.0007949168793857098 max memory_allocated 29273.76025390625 
[2025-03-22 05:31:10 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.35605859756469727 norm:0.0007769621443003416 max memory_allocated 29273.76025390625 
[2025-03-22 05:31:59 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.3555864989757538 norm:0.0007698785630054772 max memory_allocated 29273.76025390625 
[2025-03-22 05:32:47 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.35518187284469604 norm:0.0007556380005553365 max memory_allocated 29273.76025390625 
[2025-03-22 05:33:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.3547602891921997 norm:0.0007463999791070819 max memory_allocated 29273.76025390625 
[2025-03-22 05:34:24 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.3544434905052185 norm:0.0007407030207104981 max memory_allocated 29273.76025390625 
[2025-03-22 05:35:12 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.35423988103866577 norm:0.0007345334743149579 max memory_allocated 29273.76025390625 
[2025-03-22 05:36:01 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.35416659712791443 norm:0.0007320974254980683 max memory_allocated 29273.76025390625 
[2025-03-22 05:36:49 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.3540545701980591 norm:0.0007262456347234547 max memory_allocated 29273.76025390625 
[2025-03-22 05:37:37 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.35393422842025757 norm:0.0007199146784842014 max memory_allocated 29273.76025390625 
[2025-03-22 05:37:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 05:38:43 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.4544486105442047 norm:0.028800375759601593 max memory_allocated 29273.94775390625 
[2025-03-22 05:39:31 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.42700543999671936 norm:0.013631482608616352 max memory_allocated 29273.94775390625 
[2025-03-22 05:40:19 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.39724454283714294 norm:0.005813973490148783 max memory_allocated 29273.94775390625 
[2025-03-22 05:41:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.3855440318584442 norm:0.0033156934659928083 max memory_allocated 29273.94775390625 
[2025-03-22 05:41:56 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.3806930184364319 norm:0.002211213344708085 max memory_allocated 29273.94775390625 
[2025-03-22 05:42:44 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.37737756967544556 norm:0.0018982728943228722 max memory_allocated 29273.94775390625 
[2025-03-22 05:43:32 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.37492284178733826 norm:0.00153136916924268 max memory_allocated 29273.94775390625 
[2025-03-22 05:44:21 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.37269675731658936 norm:0.0011626064078882337 max memory_allocated 29273.94775390625 
[2025-03-22 05:45:09 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.3712572157382965 norm:0.0011147756595164537 max memory_allocated 29273.94775390625 
[2025-03-22 05:45:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.37015143036842346 norm:0.0010649600299075246 max memory_allocated 29273.94775390625 
[2025-03-22 05:46:46 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.3693030774593353 norm:0.001038218499161303 max memory_allocated 29273.94775390625 
[2025-03-22 05:47:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.3686528205871582 norm:0.0010154861956834793 max memory_allocated 29273.94775390625 
[2025-03-22 05:48:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.36802828311920166 norm:0.0010028518736362457 max memory_allocated 29273.94775390625 
[2025-03-22 05:49:12 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.3676283657550812 norm:0.0009852719958871603 max memory_allocated 29273.94775390625 
[2025-03-22 05:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.3672318756580353 norm:0.0009611825807951391 max memory_allocated 29273.94775390625 
[2025-03-22 05:50:48 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.36691951751708984 norm:0.0009561663027852774 max memory_allocated 29273.94775390625 
[2025-03-22 05:51:37 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.3665916919708252 norm:0.0009381331037729979 max memory_allocated 29273.94775390625 
[2025-03-22 05:52:25 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.3663497567176819 norm:0.0009153159917332232 max memory_allocated 29273.94775390625 
[2025-03-22 05:53:13 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.36616051197052 norm:0.0008904634742066264 max memory_allocated 29273.94775390625 
[2025-03-22 05:54:02 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.36600881814956665 norm:0.0008785089594312012 max memory_allocated 29273.94775390625 
[2025-03-22 05:54:15 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 05:55:07 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.4574308395385742 norm:0.029087379574775696 max memory_allocated 29274.13525390625 
[2025-03-22 05:55:55 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.4296494424343109 norm:0.012878313660621643 max memory_allocated 29274.13525390625 
[2025-03-22 05:56:44 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.4025209844112396 norm:0.004637789912521839 max memory_allocated 29274.13525390625 
[2025-03-22 05:57:32 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.39249491691589355 norm:0.002550858072936535 max memory_allocated 29274.13525390625 
[2025-03-22 05:58:20 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.38832902908325195 norm:0.001961330184713006 max memory_allocated 29274.13525390625 
[2025-03-22 05:59:08 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.3856758773326874 norm:0.0016994382021948695 max memory_allocated 29274.13525390625 
[2025-03-22 05:59:57 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.3835144340991974 norm:0.0014942812267690897 max memory_allocated 29274.13525390625 
[2025-03-22 06:00:45 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.3817370533943176 norm:0.0013190062018111348 max memory_allocated 29274.13525390625 
[2025-03-22 06:01:34 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.38015782833099365 norm:0.0011792996665462852 max memory_allocated 29274.13525390625 
[2025-03-22 06:02:22 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.37913116812705994 norm:0.0011745095252990723 max memory_allocated 29274.13525390625 
[2025-03-22 06:03:11 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.3781987130641937 norm:0.001096584601327777 max memory_allocated 29274.13525390625 
[2025-03-22 06:03:59 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.3773820996284485 norm:0.0010570441372692585 max memory_allocated 29274.13525390625 
[2025-03-22 06:04:47 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.3766931891441345 norm:0.0010009072721004486 max memory_allocated 29274.13525390625 
[2025-03-22 06:05:36 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.37605658173561096 norm:0.0009797373786568642 max memory_allocated 29274.13525390625 
[2025-03-22 06:06:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.3756163716316223 norm:0.0009590791305527091 max memory_allocated 29274.13525390625 
[2025-03-22 06:07:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.37523990869522095 norm:0.000947596738114953 max memory_allocated 29274.13525390625 
[2025-03-22 06:08:01 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.37500250339508057 norm:0.0009511016542091966 max memory_allocated 29274.13525390625 
[2025-03-22 06:08:49 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.37477636337280273 norm:0.0009216370526701212 max memory_allocated 29274.13525390625 
[2025-03-22 06:09:37 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.37458425760269165 norm:0.000916284741833806 max memory_allocated 29274.13525390625 
[2025-03-22 06:10:25 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.3744102716445923 norm:0.0009030502405948937 max memory_allocated 29274.13525390625 
[2025-03-22 06:10:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 06:11:31 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.45453381538391113 norm:0.025083882734179497 max memory_allocated 29274.32275390625 
[2025-03-22 06:12:19 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.42974308133125305 norm:0.010719183832406998 max memory_allocated 29274.32275390625 
[2025-03-22 06:13:07 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.41057249903678894 norm:0.005519959609955549 max memory_allocated 29274.32275390625 
[2025-03-22 06:13:56 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.40076327323913574 norm:0.00261687021702528 max memory_allocated 29274.32275390625 
[2025-03-22 06:14:44 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.39663204550743103 norm:0.001700175809673965 max memory_allocated 29274.32275390625 
[2025-03-22 06:15:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.394316166639328 norm:0.0014850301668047905 max memory_allocated 29274.32275390625 
[2025-03-22 06:16:21 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.39256641268730164 norm:0.0013360233278945088 max memory_allocated 29274.32275390625 
[2025-03-22 06:17:09 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.3912578225135803 norm:0.001263864804059267 max memory_allocated 29274.32275390625 
[2025-03-22 06:17:58 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.3899408280849457 norm:0.001175403012894094 max memory_allocated 29274.32275390625 
[2025-03-22 06:18:46 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.3889716863632202 norm:0.001139241736382246 max memory_allocated 29274.32275390625 
[2025-03-22 06:19:35 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.38821959495544434 norm:0.00106534652877599 max memory_allocated 29274.32275390625 
[2025-03-22 06:20:23 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.38757896423339844 norm:0.0010154012124985456 max memory_allocated 29274.32275390625 
[2025-03-22 06:21:11 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.38696229457855225 norm:0.0009589430992491543 max memory_allocated 29274.32275390625 
[2025-03-22 06:22:00 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.3863547742366791 norm:0.0009189218399114907 max memory_allocated 29274.32275390625 
[2025-03-22 06:22:48 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.3858630657196045 norm:0.0008898461237549782 max memory_allocated 29274.32275390625 
[2025-03-22 06:23:36 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.3855449855327606 norm:0.00087546999566257 max memory_allocated 29274.32275390625 
[2025-03-22 06:24:24 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.3852466344833374 norm:0.0008754578884691 max memory_allocated 29274.32275390625 
[2025-03-22 06:25:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.3849516808986664 norm:0.0008749881526455283 max memory_allocated 29274.32275390625 
[2025-03-22 06:26:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.38460639119148254 norm:0.0008384901448152959 max memory_allocated 29274.32275390625 
[2025-03-22 06:26:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.3844524025917053 norm:0.0008459185482934117 max memory_allocated 29274.32275390625 
[2025-03-22 06:27:03 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 06:27:55 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.46538376808166504 norm:0.017354823648929596 max memory_allocated 29274.51025390625 
[2025-03-22 06:28:43 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.4470199942588806 norm:0.00838922243565321 max memory_allocated 29274.51025390625 
[2025-03-22 06:29:31 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.4286470115184784 norm:0.004369557369500399 max memory_allocated 29274.51025390625 
[2025-03-22 06:30:20 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.41926997900009155 norm:0.002189698163419962 max memory_allocated 29274.51025390625 
[2025-03-22 06:31:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.4154582619667053 norm:0.0016244673170149326 max memory_allocated 29274.51025390625 
[2025-03-22 06:31:57 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.41315093636512756 norm:0.001452191499993205 max memory_allocated 29274.51025390625 
[2025-03-22 06:32:45 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.4114033281803131 norm:0.0013095922768115997 max memory_allocated 29274.51025390625 
[2025-03-22 06:33:33 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.41006577014923096 norm:0.001253028167411685 max memory_allocated 29274.51025390625 
[2025-03-22 06:34:22 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.4089338481426239 norm:0.0011718937894329429 max memory_allocated 29274.51025390625 
[2025-03-22 06:35:10 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.4078591763973236 norm:0.0010780415032058954 max memory_allocated 29274.51025390625 
[2025-03-22 06:35:59 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.4070091247558594 norm:0.001019064337015152 max memory_allocated 29274.51025390625 
[2025-03-22 06:36:47 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.4063894748687744 norm:0.000986692844890058 max memory_allocated 29274.51025390625 
[2025-03-22 06:37:35 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.4059008061885834 norm:0.0009706641430966556 max memory_allocated 29274.51025390625 
[2025-03-22 06:38:24 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.4053787887096405 norm:0.0009182110661640763 max memory_allocated 29274.51025390625 
[2025-03-22 06:39:12 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.40498822927474976 norm:0.0009065413614735007 max memory_allocated 29274.51025390625 
[2025-03-22 06:40:00 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.40456247329711914 norm:0.0008765491656959057 max memory_allocated 29274.51025390625 
[2025-03-22 06:40:48 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.40424609184265137 norm:0.000856280152220279 max memory_allocated 29274.51025390625 
[2025-03-22 06:41:36 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.4039453864097595 norm:0.0008391740266233683 max memory_allocated 29274.51025390625 
[2025-03-22 06:42:25 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.40377289056777954 norm:0.0008373355376534164 max memory_allocated 29274.51025390625 
[2025-03-22 06:43:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.4035243093967438 norm:0.0008227769867517054 max memory_allocated 29274.51025390625 
[2025-03-22 06:43:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 06:44:19 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.48292088508605957 norm:0.01595819741487503 max memory_allocated 29274.69775390625 
[2025-03-22 06:45:07 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.4657793641090393 norm:0.007845578715205193 max memory_allocated 29274.69775390625 
[2025-03-22 06:45:55 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.44915229082107544 norm:0.004028813913464546 max memory_allocated 29274.69775390625 
[2025-03-22 06:46:44 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.44033366441726685 norm:0.0020630424842238426 max memory_allocated 29274.69775390625 
[2025-03-22 06:47:32 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.4369114339351654 norm:0.0015537742292508483 max memory_allocated 29274.69775390625 
[2025-03-22 06:48:21 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.4348987340927124 norm:0.0013871563132852316 max memory_allocated 29274.69775390625 
[2025-03-22 06:49:09 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.4331207275390625 norm:0.0011969375191256404 max memory_allocated 29274.69775390625 
[2025-03-22 06:49:58 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.4317270815372467 norm:0.0010869319085031748 max memory_allocated 29274.69775390625 
[2025-03-22 06:50:46 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.4306606650352478 norm:0.0010293753584846854 max memory_allocated 29274.69775390625 
[2025-03-22 06:51:34 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.4297325015068054 norm:0.0009789164178073406 max memory_allocated 29274.69775390625 
[2025-03-22 06:52:23 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.4289793074131012 norm:0.0009425960015505552 max memory_allocated 29274.69775390625 
[2025-03-22 06:53:11 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.4283643364906311 norm:0.0009212367003783584 max memory_allocated 29274.69775390625 
[2025-03-22 06:53:59 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.42787402868270874 norm:0.000888590409886092 max memory_allocated 29274.69775390625 
[2025-03-22 06:54:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.4273169934749603 norm:0.0008673578849993646 max memory_allocated 29274.69775390625 
[2025-03-22 06:55:36 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.4268137216567993 norm:0.0008310097036883235 max memory_allocated 29274.69775390625 
[2025-03-22 06:56:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.4264181852340698 norm:0.0008073713397607207 max memory_allocated 29274.69775390625 
[2025-03-22 06:57:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.42612987756729126 norm:0.0008023488335311413 max memory_allocated 29274.69775390625 
[2025-03-22 06:58:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.42591166496276855 norm:0.000799390603788197 max memory_allocated 29274.69775390625 
[2025-03-22 06:58:49 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.4257259964942932 norm:0.0007926427642814815 max memory_allocated 29274.69775390625 
[2025-03-22 06:59:37 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.42554256319999695 norm:0.0007826157379895449 max memory_allocated 29274.69775390625 
[2025-03-22 06:59:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 07:00:43 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.5226582288742065 norm:0.01698560267686844 max memory_allocated 29274.88525390625 
[2025-03-22 07:01:31 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.5041220784187317 norm:0.008419294841587543 max memory_allocated 29274.88525390625 
[2025-03-22 07:02:20 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.48675820231437683 norm:0.004469588398933411 max memory_allocated 29274.88525390625 
[2025-03-22 07:03:08 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.4773399233818054 norm:0.002575618214905262 max memory_allocated 29274.88525390625 
[2025-03-22 07:03:56 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.4732111692428589 norm:0.0017437731148675084 max memory_allocated 29274.88525390625 
[2025-03-22 07:04:45 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.47098302841186523 norm:0.0015104708727449179 max memory_allocated 29274.88525390625 
[2025-03-22 07:05:33 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.46929189562797546 norm:0.0013736874097958207 max memory_allocated 29274.88525390625 
[2025-03-22 07:06:22 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.46796131134033203 norm:0.001325785182416439 max memory_allocated 29274.88525390625 
[2025-03-22 07:07:10 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.4668869972229004 norm:0.0012586750090122223 max memory_allocated 29274.88525390625 
[2025-03-22 07:07:58 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.46601545810699463 norm:0.0012258386705070734 max memory_allocated 29274.88525390625 
[2025-03-22 07:08:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.4652741849422455 norm:0.0011662552133202553 max memory_allocated 29274.88525390625 
[2025-03-22 07:09:35 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.46466386318206787 norm:0.001121404697187245 max memory_allocated 29274.88525390625 
[2025-03-22 07:10:23 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.4641028046607971 norm:0.0010908568510785699 max memory_allocated 29274.88525390625 
[2025-03-22 07:11:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.46356672048568726 norm:0.001056866953149438 max memory_allocated 29274.88525390625 
[2025-03-22 07:11:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.4631853699684143 norm:0.0010416971053928137 max memory_allocated 29274.88525390625 
[2025-03-22 07:12:48 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.462882399559021 norm:0.0010251854546368122 max memory_allocated 29274.88525390625 
[2025-03-22 07:13:36 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.4625019133090973 norm:0.0009873555973172188 max memory_allocated 29274.88525390625 
[2025-03-22 07:14:24 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.46212518215179443 norm:0.0009737113723531365 max memory_allocated 29274.88525390625 
[2025-03-22 07:15:13 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.4617183208465576 norm:0.0009660320356488228 max memory_allocated 29274.88525390625 
[2025-03-22 07:16:01 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.4614313840866089 norm:0.0009450737852603197 max memory_allocated 29274.88525390625 
[2025-03-22 07:16:15 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 07:17:07 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:0.540214478969574 norm:0.008988642133772373 max memory_allocated 29275.07275390625 
[2025-03-22 07:17:55 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:0.5282825827598572 norm:0.005247324705123901 max memory_allocated 29275.07275390625 
[2025-03-22 07:18:44 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:0.5161247849464417 norm:0.0030822434928268194 max memory_allocated 29275.07275390625 
[2025-03-22 07:19:32 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:0.5102388858795166 norm:0.0023790369741618633 max memory_allocated 29275.07275390625 
[2025-03-22 07:20:21 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:0.5074918866157532 norm:0.0013218042440712452 max memory_allocated 29275.07275390625 
[2025-03-22 07:21:09 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:0.5056695342063904 norm:0.0011384175159037113 max memory_allocated 29275.07275390625 
[2025-03-22 07:21:58 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:0.504284143447876 norm:0.0010589935118332505 max memory_allocated 29275.07275390625 
[2025-03-22 07:22:46 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:0.5030399560928345 norm:0.0009937828872352839 max memory_allocated 29275.07275390625 
[2025-03-22 07:23:35 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:0.501929521560669 norm:0.0009490290540270507 max memory_allocated 29275.07275390625 
[2025-03-22 07:24:23 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:0.5010018348693848 norm:0.000911394483409822 max memory_allocated 29275.07275390625 
[2025-03-22 07:25:11 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.5004097819328308 norm:0.0009007395128719509 max memory_allocated 29275.07275390625 
[2025-03-22 07:26:00 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.4998338222503662 norm:0.0008928421884775162 max memory_allocated 29275.07275390625 
[2025-03-22 07:26:48 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.49928295612335205 norm:0.000867363705765456 max memory_allocated 29275.07275390625 
[2025-03-22 07:27:36 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.4988406002521515 norm:0.0008514210348948836 max memory_allocated 29275.07275390625 
[2025-03-22 07:28:25 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.4985358417034149 norm:0.0008506077574566007 max memory_allocated 29275.07275390625 
[2025-03-22 07:29:13 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.4982546269893646 norm:0.0008381101069971919 max memory_allocated 29275.07275390625 
[2025-03-22 07:30:01 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.49795907735824585 norm:0.0008230094681493938 max memory_allocated 29275.07275390625 
[2025-03-22 07:30:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.4977739453315735 norm:0.0008195174741558731 max memory_allocated 29275.07275390625 
[2025-03-22 07:31:38 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.4975694715976715 norm:0.0008130709175020456 max memory_allocated 29275.07275390625 
[2025-03-22 07:32:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.49728888273239136 norm:0.0008094301447272301 max memory_allocated 29275.07275390625 
[2025-03-22 07:32:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 07:33:33 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:0.5849279165267944 norm:0.022006990388035774 max memory_allocated 29275.26025390625 
[2025-03-22 07:34:21 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:0.5736238956451416 norm:0.01242484338581562 max memory_allocated 29275.26025390625 
[2025-03-22 07:35:10 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:0.5620198249816895 norm:0.007544653490185738 max memory_allocated 29275.26025390625 
[2025-03-22 07:35:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:0.5568431615829468 norm:0.005045177415013313 max memory_allocated 29275.26025390625 
[2025-03-22 07:36:47 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:0.5540054440498352 norm:0.003768853610381484 max memory_allocated 29275.26025390625 
[2025-03-22 07:37:35 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:0.55198734998703 norm:0.003156203543767333 max memory_allocated 29275.26025390625 
[2025-03-22 07:38:24 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:0.5501116514205933 norm:0.0026274865958839655 max memory_allocated 29275.26025390625 
[2025-03-22 07:39:12 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:0.5486857295036316 norm:0.0023104576393961906 max memory_allocated 29275.26025390625 
[2025-03-22 07:40:01 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:0.5475091934204102 norm:0.002054418670013547 max memory_allocated 29275.26025390625 
[2025-03-22 07:40:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:0.5465032458305359 norm:0.001844518119469285 max memory_allocated 29275.26025390625 
[2025-03-22 07:41:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:0.5456485748291016 norm:0.0016939921770244837 max memory_allocated 29275.26025390625 
[2025-03-22 07:42:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:0.5449512600898743 norm:0.00153927446808666 max memory_allocated 29275.26025390625 
[2025-03-22 07:43:14 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:0.5444051623344421 norm:0.0014279569732025266 max memory_allocated 29275.26025390625 
[2025-03-22 07:44:03 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:0.5438829064369202 norm:0.0013269194168969989 max memory_allocated 29275.26025390625 
[2025-03-22 07:44:51 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:0.5435001254081726 norm:0.0012376595987007022 max memory_allocated 29275.26025390625 
[2025-03-22 07:45:39 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:0.5431244373321533 norm:0.0011781434295699 max memory_allocated 29275.26025390625 
[2025-03-22 07:46:28 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:0.5427817702293396 norm:0.001108618569560349 max memory_allocated 29275.26025390625 
[2025-03-22 07:47:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:0.5424956679344177 norm:0.0010421432089060545 max memory_allocated 29275.26025390625 
[2025-03-22 07:48:05 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:0.5422715544700623 norm:0.0009905295446515083 max memory_allocated 29275.26025390625 
[2025-03-22 07:48:53 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:0.5420510768890381 norm:0.0009550250833854079 max memory_allocated 29275.26025390625 
[2025-03-22 07:49:07 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 07:49:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:0.6232139468193054 norm:0.006947014480829239 max memory_allocated 29275.44775390625 
[2025-03-22 07:50:48 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:0.6153224110603333 norm:0.004870758391916752 max memory_allocated 29275.44775390625 
[2025-03-22 07:51:36 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:0.6055768132209778 norm:0.003288710955530405 max memory_allocated 29275.44775390625 
[2025-03-22 07:52:25 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:0.6008141040802002 norm:0.002200591377913952 max memory_allocated 29275.44775390625 
[2025-03-22 07:53:13 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:0.5982626676559448 norm:0.0016798288561403751 max memory_allocated 29275.44775390625 
[2025-03-22 07:54:02 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:0.5963994860649109 norm:0.0014130771160125732 max memory_allocated 29275.44775390625 
[2025-03-22 07:54:50 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:0.5949164628982544 norm:0.0012828619219362736 max memory_allocated 29275.44775390625 
[2025-03-22 07:55:39 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:0.5937108397483826 norm:0.0012094103731215 max memory_allocated 29275.44775390625 
[2025-03-22 07:56:27 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:0.5927023887634277 norm:0.0011719660833477974 max memory_allocated 29275.44775390625 
[2025-03-22 07:57:15 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:0.5918164849281311 norm:0.001123117981478572 max memory_allocated 29275.44775390625 
[2025-03-22 07:58:04 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:0.5911508798599243 norm:0.001082685892470181 max memory_allocated 29275.44775390625 
[2025-03-22 07:58:52 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:0.5906248092651367 norm:0.0010418383171781898 max memory_allocated 29275.44775390625 
[2025-03-22 07:59:40 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:0.5900755524635315 norm:0.0010060382774099708 max memory_allocated 29275.44775390625 
[2025-03-22 08:00:29 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:0.5897058248519897 norm:0.0009851332288235426 max memory_allocated 29275.44775390625 
[2025-03-22 08:01:17 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:0.5893744230270386 norm:0.0009625516249798238 max memory_allocated 29275.44775390625 
[2025-03-22 08:02:05 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:0.5890630483627319 norm:0.0009447244228795171 max memory_allocated 29275.44775390625 
[2025-03-22 08:02:53 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:0.5888305902481079 norm:0.00091765564866364 max memory_allocated 29275.44775390625 
[2025-03-22 08:03:42 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:0.588606595993042 norm:0.0008991619106382132 max memory_allocated 29275.44775390625 
[2025-03-22 08:04:30 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:0.5883675813674927 norm:0.0008932310156524181 max memory_allocated 29275.44775390625 
[2025-03-22 08:05:19 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:0.5882012248039246 norm:0.0008607603958807886 max memory_allocated 29275.44775390625 
[2025-03-22 08:05:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 08:06:25 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:0.6750516295433044 norm:0.004743622150272131 max memory_allocated 29275.63525390625 
[2025-03-22 08:07:13 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:0.6666557192802429 norm:0.0029031692538410425 max memory_allocated 29275.63525390625 
[2025-03-22 08:08:02 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:0.6568881273269653 norm:0.001835451927036047 max memory_allocated 29275.63525390625 
[2025-03-22 08:08:51 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:0.653176486492157 norm:0.0012564701028168201 max memory_allocated 29275.63525390625 
[2025-03-22 08:09:39 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:0.6511632800102234 norm:0.0009557497105561197 max memory_allocated 29275.63525390625 
[2025-03-22 08:10:28 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:0.6497255563735962 norm:0.0008730613044463098 max memory_allocated 29275.63525390625 
[2025-03-22 08:11:16 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:0.6484992504119873 norm:0.000845246366225183 max memory_allocated 29275.63525390625 
[2025-03-22 08:12:04 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:0.6473404765129089 norm:0.000804541225079447 max memory_allocated 29275.63525390625 
[2025-03-22 08:12:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:0.6463472843170166 norm:0.0007754429243505001 max memory_allocated 29275.63525390625 
[2025-03-22 08:13:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:0.6455812454223633 norm:0.0007590503664687276 max memory_allocated 29275.63525390625 
[2025-03-22 08:14:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:0.6449061632156372 norm:0.0007368902442976832 max memory_allocated 29275.63525390625 
[2025-03-22 08:15:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:0.6444321274757385 norm:0.0007243319996632636 max memory_allocated 29275.63525390625 
[2025-03-22 08:16:06 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:0.6440596580505371 norm:0.0007121654343791306 max memory_allocated 29275.63525390625 
[2025-03-22 08:16:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:0.6437171697616577 norm:0.0007020551129244268 max memory_allocated 29275.63525390625 
[2025-03-22 08:17:42 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:0.6434182524681091 norm:0.0006948501686565578 max memory_allocated 29275.63525390625 
[2025-03-22 08:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:0.6431810855865479 norm:0.0006898354622535408 max memory_allocated 29275.63525390625 
[2025-03-22 08:19:19 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:0.6429482698440552 norm:0.0006727271247655153 max memory_allocated 29275.63525390625 
[2025-03-22 08:20:08 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:0.6427268981933594 norm:0.0006631121505051851 max memory_allocated 29275.63525390625 
[2025-03-22 08:20:56 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:0.6425570249557495 norm:0.0006527075311169028 max memory_allocated 29275.63525390625 
[2025-03-22 08:21:45 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:0.6423740983009338 norm:0.000649412686470896 max memory_allocated 29275.63525390625 
[2025-03-22 08:21:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 08:22:51 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:0.7442740797996521 norm:0.011167576536536217 max memory_allocated 29275.82275390625 
[2025-03-22 08:23:39 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:0.7352607846260071 norm:0.007172148674726486 max memory_allocated 29275.82275390625 
[2025-03-22 08:24:28 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:0.7239225506782532 norm:0.0047705452889204025 max memory_allocated 29275.82275390625 
[2025-03-22 08:25:17 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:0.719333827495575 norm:0.003246787004172802 max memory_allocated 29275.82275390625 
[2025-03-22 08:26:05 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:0.7164132595062256 norm:0.0021345310378819704 max memory_allocated 29275.82275390625 
[2025-03-22 08:26:53 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:0.7139481902122498 norm:0.0011546474415808916 max memory_allocated 29275.82275390625 
[2025-03-22 08:27:42 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:0.7123162150382996 norm:0.0009958315640687943 max memory_allocated 29275.82275390625 
[2025-03-22 08:28:30 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:0.710918128490448 norm:0.000979239004664123 max memory_allocated 29275.82275390625 
[2025-03-22 08:29:19 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:0.7097848057746887 norm:0.0009725230047479272 max memory_allocated 29275.82275390625 
[2025-03-22 08:30:07 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:0.7088723182678223 norm:0.0009506620699539781 max memory_allocated 29275.82275390625 
[2025-03-22 08:30:55 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:0.7081794142723083 norm:0.0009375622612424195 max memory_allocated 29275.82275390625 
[2025-03-22 08:31:44 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:0.7075635194778442 norm:0.0009351171320304275 max memory_allocated 29275.82275390625 
[2025-03-22 08:32:32 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:0.707139253616333 norm:0.0009426757460460067 max memory_allocated 29275.82275390625 
[2025-03-22 08:33:20 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:0.706779420375824 norm:0.0009331909823231399 max memory_allocated 29275.82275390625 
[2025-03-22 08:34:09 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:0.7066165208816528 norm:0.0009469841606914997 max memory_allocated 29275.82275390625 
[2025-03-22 08:34:57 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:0.7063112258911133 norm:0.0009350123000331223 max memory_allocated 29275.82275390625 
[2025-03-22 08:35:46 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:0.7060538530349731 norm:0.0009541488252580166 max memory_allocated 29275.82275390625 
[2025-03-22 08:36:34 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:0.7058176398277283 norm:0.0009579636971466243 max memory_allocated 29275.82275390625 
[2025-03-22 08:37:23 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:0.7056585550308228 norm:0.0009551058174110949 max memory_allocated 29275.82275390625 
[2025-03-22 08:38:11 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:0.7054790258407593 norm:0.0009591310517862439 max memory_allocated 29275.82275390625 
[2025-03-22 08:38:25 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 08:39:17 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:0.7984733581542969 norm:0.00438288226723671 max memory_allocated 29276.01025390625 
[2025-03-22 08:40:06 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:0.7899692058563232 norm:0.0020796353928744793 max memory_allocated 29276.01025390625 
[2025-03-22 08:40:54 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:0.7799777388572693 norm:0.0012159167090430856 max memory_allocated 29276.01025390625 
[2025-03-22 08:41:43 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:0.7767351865768433 norm:0.0009073223918676376 max memory_allocated 29276.01025390625 
[2025-03-22 08:42:31 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:0.7748754620552063 norm:0.0007078628987073898 max memory_allocated 29276.01025390625 
[2025-03-22 08:43:20 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:0.773311972618103 norm:0.0006655752658843994 max memory_allocated 29276.01025390625 
[2025-03-22 08:44:08 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:0.7717528343200684 norm:0.0006369605544023216 max memory_allocated 29276.01025390625 
[2025-03-22 08:44:57 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:0.7703319787979126 norm:0.0006238806527107954 max memory_allocated 29276.01025390625 
[2025-03-22 08:45:45 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:0.7692000865936279 norm:0.0006188963307067752 max memory_allocated 29276.01025390625 
[2025-03-22 08:46:33 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:0.7683379054069519 norm:0.0006188714760355651 max memory_allocated 29276.01025390625 
[2025-03-22 08:47:22 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:0.7675769329071045 norm:0.0006120131001807749 max memory_allocated 29276.01025390625 
[2025-03-22 08:48:10 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:0.7669739127159119 norm:0.0006073335534892976 max memory_allocated 29276.01025390625 
[2025-03-22 08:48:58 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:0.7664949297904968 norm:0.0006044553592801094 max memory_allocated 29276.01025390625 
[2025-03-22 08:49:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:0.7661423683166504 norm:0.0006124036153778434 max memory_allocated 29276.01025390625 
[2025-03-22 08:50:35 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:0.7658388614654541 norm:0.000614540942478925 max memory_allocated 29276.01025390625 
[2025-03-22 08:51:24 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:0.7656355500221252 norm:0.0006172718130983412 max memory_allocated 29276.01025390625 
[2025-03-22 08:52:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:0.7654514312744141 norm:0.0006173645379021764 max memory_allocated 29276.01025390625 
[2025-03-22 08:53:01 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:0.7652366757392883 norm:0.0006186285172589123 max memory_allocated 29276.01025390625 
[2025-03-22 08:53:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:0.7650808095932007 norm:0.0006177015602588654 max memory_allocated 29276.01025390625 
[2025-03-22 08:54:38 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:0.7649567127227783 norm:0.0006168996915221214 max memory_allocated 29276.01025390625 
[2025-03-22 08:54:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 08:55:44 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:0.8783688545227051 norm:0.01274730172008276 max memory_allocated 29276.19775390625 
[2025-03-22 08:56:33 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:0.8688805103302002 norm:0.00792898889631033 max memory_allocated 29276.19775390625 
[2025-03-22 08:57:21 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:0.8583368062973022 norm:0.005433475598692894 max memory_allocated 29276.19775390625 
[2025-03-22 08:58:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:0.8542863130569458 norm:0.00400811992585659 max memory_allocated 29276.19775390625 
[2025-03-22 08:58:58 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:0.8513273000717163 norm:0.0030242828652262688 max memory_allocated 29276.19775390625 
[2025-03-22 08:59:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:0.8485242128372192 norm:0.0022748750634491444 max memory_allocated 29276.19775390625 
[2025-03-22 09:00:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:0.8461892008781433 norm:0.0017301642801612616 max memory_allocated 29276.19775390625 
[2025-03-22 09:01:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:0.8444628119468689 norm:0.001553503330796957 max memory_allocated 29276.19775390625 
[2025-03-22 09:02:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:0.8430553078651428 norm:0.0015072451205924153 max memory_allocated 29276.19775390625 
[2025-03-22 09:03:00 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:0.8419666886329651 norm:0.0013651511399075389 max memory_allocated 29276.19775390625 
[2025-03-22 09:03:48 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:0.8408598899841309 norm:0.0013299529673531651 max memory_allocated 29276.19775390625 
[2025-03-22 09:04:36 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:0.8406656980514526 norm:0.0012555220164358616 max memory_allocated 29276.19775390625 
[2025-03-22 09:05:24 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:0.8402265310287476 norm:0.0012395947705954313 max memory_allocated 29276.19775390625 
[2025-03-22 09:06:13 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:0.8399307131767273 norm:0.001196073368191719 max memory_allocated 29276.19775390625 
[2025-03-22 09:07:01 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:0.8395686149597168 norm:0.001136713195592165 max memory_allocated 29276.19775390625 
[2025-03-22 09:07:49 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:0.8387847542762756 norm:0.0011003478430211544 max memory_allocated 29276.19775390625 
[2025-03-22 09:08:38 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:0.8389041423797607 norm:0.0009268129360862076 max memory_allocated 29276.19775390625 
[2025-03-22 09:09:26 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:0.838809609413147 norm:0.0008852528408169746 max memory_allocated 29276.19775390625 
[2025-03-22 09:10:15 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:0.8381876349449158 norm:0.0010219970718026161 max memory_allocated 29276.19775390625 
[2025-03-22 09:11:03 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:0.8376356363296509 norm:0.001032284228131175 max memory_allocated 29276.19775390625 
[2025-03-22 09:11:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 09:12:09 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:0.9474585056304932 norm:0.006912275217473507 max memory_allocated 29276.38525390625 
[2025-03-22 09:12:58 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:0.9385610818862915 norm:0.0034117132890969515 max memory_allocated 29276.38525390625 
[2025-03-22 09:13:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:0.9276005625724792 norm:0.0018779091769829392 max memory_allocated 29276.38525390625 
[2025-03-22 09:14:34 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:0.9235320091247559 norm:0.0011233689729124308 max memory_allocated 29276.38525390625 
[2025-03-22 09:15:23 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:0.9213181138038635 norm:0.0008202142780646682 max memory_allocated 29276.38525390625 
[2025-03-22 09:16:11 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:0.9195093512535095 norm:0.0007368526421487331 max memory_allocated 29276.38525390625 
[2025-03-22 09:16:59 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:0.917832612991333 norm:0.0007047031540423632 max memory_allocated 29276.38525390625 
[2025-03-22 09:17:48 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:0.9163551330566406 norm:0.0006858428241685033 max memory_allocated 29276.38525390625 
[2025-03-22 09:18:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:0.9151895642280579 norm:0.0006681171362288296 max memory_allocated 29276.38525390625 
[2025-03-22 09:19:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:0.9142523407936096 norm:0.0006560133770108223 max memory_allocated 29276.38525390625 
[2025-03-22 09:20:12 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:0.9135151505470276 norm:0.0006478879950009286 max memory_allocated 29276.38525390625 
[2025-03-22 09:21:00 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:0.9129507541656494 norm:0.0006394726224243641 max memory_allocated 29276.38525390625 
[2025-03-22 09:21:49 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:0.9125831723213196 norm:0.0006383894360624254 max memory_allocated 29276.38525390625 
[2025-03-22 09:22:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:0.9122326970100403 norm:0.0006343117565847933 max memory_allocated 29276.38525390625 
[2025-03-22 09:23:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:0.9119442105293274 norm:0.0006295985076576471 max memory_allocated 29276.38525390625 
[2025-03-22 09:24:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:0.9117002487182617 norm:0.0006281388341449201 max memory_allocated 29276.38525390625 
[2025-03-22 09:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:0.9115042686462402 norm:0.0006289590965025127 max memory_allocated 29276.38525390625 
[2025-03-22 09:25:50 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:0.9113048315048218 norm:0.0006276635103859007 max memory_allocated 29276.38525390625 
[2025-03-22 09:26:39 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:0.9111528992652893 norm:0.0006294886698015034 max memory_allocated 29276.38525390625 
[2025-03-22 09:27:27 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:0.9109996557235718 norm:0.0006389664486050606 max memory_allocated 29276.38525390625 
[2025-03-22 09:27:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 09:28:33 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:1.0365012884140015 norm:0.01058680284768343 max memory_allocated 29276.57275390625 
[2025-03-22 09:29:22 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:1.024702548980713 norm:0.005995589308440685 max memory_allocated 29276.57275390625 
[2025-03-22 09:30:10 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:1.0107473134994507 norm:0.0036778401117771864 max memory_allocated 29276.57275390625 
[2025-03-22 09:30:59 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:1.0049883127212524 norm:0.0021863963920623064 max memory_allocated 29276.57275390625 
[2025-03-22 09:31:47 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:1.0022557973861694 norm:0.0010142354294657707 max memory_allocated 29276.57275390625 
[2025-03-22 09:32:35 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:1.0000051259994507 norm:0.0008764086524024606 max memory_allocated 29276.57275390625 
[2025-03-22 09:33:23 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:0.997974157333374 norm:0.0008039554813876748 max memory_allocated 29276.57275390625 
[2025-03-22 09:34:12 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:0.9962081909179688 norm:0.0007491832948289812 max memory_allocated 29276.57275390625 
[2025-03-22 09:35:00 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:0.9949163198471069 norm:0.0007320247241295874 max memory_allocated 29276.57275390625 
[2025-03-22 09:35:48 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:0.9939320087432861 norm:0.0007179922540672123 max memory_allocated 29276.57275390625 
[2025-03-22 09:36:36 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:0.9932069778442383 norm:0.0007030495326034725 max memory_allocated 29276.57275390625 
[2025-03-22 09:37:24 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:0.992624819278717 norm:0.00069042929681018 max memory_allocated 29276.57275390625 
[2025-03-22 09:38:13 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:0.9921625256538391 norm:0.0006832971703261137 max memory_allocated 29276.57275390625 
[2025-03-22 09:39:01 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:0.9917677044868469 norm:0.0006800213013775647 max memory_allocated 29276.57275390625 
[2025-03-22 09:39:49 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:0.9914831519126892 norm:0.000676866911817342 max memory_allocated 29276.57275390625 
[2025-03-22 09:40:38 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:0.9912264347076416 norm:0.0006733136251568794 max memory_allocated 29276.57275390625 
[2025-03-22 09:41:26 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:0.9909855127334595 norm:0.0006708173314109445 max memory_allocated 29276.57275390625 
[2025-03-22 09:42:14 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:0.9907223582267761 norm:0.0006680088117718697 max memory_allocated 29276.57275390625 
[2025-03-22 09:43:03 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:0.9905062913894653 norm:0.0006608909461647272 max memory_allocated 29276.57275390625 
[2025-03-22 09:43:51 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:0.9903020262718201 norm:0.0006616112659685314 max memory_allocated 29276.57275390625 
[2025-03-22 09:44:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 09:44:58 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:1.1218501329421997 norm:0.01044861227273941 max memory_allocated 29276.76025390625 
[2025-03-22 09:45:46 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:1.111207127571106 norm:0.006025394890457392 max memory_allocated 29276.76025390625 
[2025-03-22 09:46:34 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:1.0968884229660034 norm:0.0030085365287959576 max memory_allocated 29276.76025390625 
[2025-03-22 09:47:23 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:1.0901952981948853 norm:0.001610713778063655 max memory_allocated 29276.76025390625 
[2025-03-22 09:48:11 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:1.0868346691131592 norm:0.000857256818562746 max memory_allocated 29276.76025390625 
[2025-03-22 09:48:59 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:1.0842229127883911 norm:0.000709502084646374 max memory_allocated 29276.76025390625 
[2025-03-22 09:49:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:1.0816787481307983 norm:0.0006596610182896256 max memory_allocated 29276.76025390625 
[2025-03-22 09:50:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:1.0795098543167114 norm:0.0006348562892526388 max memory_allocated 29276.76025390625 
[2025-03-22 09:51:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:1.078056812286377 norm:0.0006215358735062182 max memory_allocated 29276.76025390625 
[2025-03-22 09:52:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:1.0770903825759888 norm:0.0006180106429383159 max memory_allocated 29276.76025390625 
[2025-03-22 09:53:00 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:1.0763710737228394 norm:0.0006123597850091755 max memory_allocated 29276.76025390625 
[2025-03-22 09:53:48 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:1.0758483409881592 norm:0.0006137753953225911 max memory_allocated 29276.76025390625 
[2025-03-22 09:54:37 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:1.0753815174102783 norm:0.0006150099216029048 max memory_allocated 29276.76025390625 
[2025-03-22 09:55:25 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:1.0750548839569092 norm:0.0006185840466059744 max memory_allocated 29276.76025390625 
[2025-03-22 09:56:13 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:1.0747865438461304 norm:0.0006169236730784178 max memory_allocated 29276.76025390625 
[2025-03-22 09:57:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:1.0745577812194824 norm:0.0006170944543555379 max memory_allocated 29276.76025390625 
[2025-03-22 09:57:50 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:1.074367880821228 norm:0.0006183783407323062 max memory_allocated 29276.76025390625 
[2025-03-22 09:58:39 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:1.0741748809814453 norm:0.0006196802132762969 max memory_allocated 29276.76025390625 
[2025-03-22 09:59:27 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:1.0740481615066528 norm:0.0006240613874979317 max memory_allocated 29276.76025390625 
[2025-03-22 10:00:16 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:1.0739166736602783 norm:0.0006242123781703413 max memory_allocated 29276.76025390625 
[2025-03-22 10:00:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 32 ===
[2025-03-22 10:01:22 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 0 loss:1.2219982147216797 norm:0.01520595047622919 max memory_allocated 29276.94775390625 
[2025-03-22 10:02:10 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 1 loss:1.2082723379135132 norm:0.008922123350203037 max memory_allocated 29276.94775390625 
[2025-03-22 10:02:58 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 2 loss:1.1921440362930298 norm:0.005587447434663773 max memory_allocated 29276.94775390625 
[2025-03-22 10:03:46 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 3 loss:1.1850630044937134 norm:0.0038137012161314487 max memory_allocated 29276.94775390625 
[2025-03-22 10:04:35 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 4 loss:1.1809489727020264 norm:0.002702200785279274 max memory_allocated 29276.94775390625 
[2025-03-22 10:05:23 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 5 loss:1.1776654720306396 norm:0.00215764413587749 max memory_allocated 29276.94775390625 
[2025-03-22 10:06:11 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 6 loss:1.1745225191116333 norm:0.0012679437641054392 max memory_allocated 29276.94775390625 
[2025-03-22 10:06:59 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 7 loss:1.1721932888031006 norm:0.0007922934601083398 max memory_allocated 29276.94775390625 
[2025-03-22 10:07:47 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 8 loss:1.1707944869995117 norm:0.000778047542553395 max memory_allocated 29276.94775390625 
[2025-03-22 10:08:36 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 9 loss:1.1697856187820435 norm:0.0007751428056508303 max memory_allocated 29276.94775390625 
[2025-03-22 10:09:24 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 10 loss:1.1690582036972046 norm:0.0007703906740061939 max memory_allocated 29276.94775390625 
[2025-03-22 10:10:12 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 11 loss:1.1684855222702026 norm:0.0007679237751290202 max memory_allocated 29276.94775390625 
[2025-03-22 10:11:01 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 12 loss:1.168014407157898 norm:0.000755482236854732 max memory_allocated 29276.94775390625 
[2025-03-22 10:11:49 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 13 loss:1.1676664352416992 norm:0.0007476987084373832 max memory_allocated 29276.94775390625 
[2025-03-22 10:12:37 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 14 loss:1.1674060821533203 norm:0.000745272496715188 max memory_allocated 29276.94775390625 
[2025-03-22 10:13:26 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 15 loss:1.1671459674835205 norm:0.0007391294930130243 max memory_allocated 29276.94775390625 
[2025-03-22 10:14:14 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 16 loss:1.1669292449951172 norm:0.0007336141425184906 max memory_allocated 29276.94775390625 
[2025-03-22 10:15:03 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 17 loss:1.166741132736206 norm:0.0007322335732169449 max memory_allocated 29276.94775390625 
[2025-03-22 10:15:51 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 18 loss:1.1665608882904053 norm:0.0007349659572355449 max memory_allocated 29276.94775390625 
[2025-03-22 10:16:40 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 19 loss:1.1663897037506104 norm:0.0007218808750621974 max memory_allocated 29276.94775390625 
[2025-03-22 10:16:53 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 33 ===
[2025-03-22 10:17:45 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 0 loss:1.3222311735153198 norm:0.01081610843539238 max memory_allocated 29277.13525390625 
[2025-03-22 10:18:34 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 1 loss:1.3086888790130615 norm:0.007364677730947733 max memory_allocated 29277.13525390625 
[2025-03-22 10:19:22 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 2 loss:1.2920587062835693 norm:0.005124853458255529 max memory_allocated 29277.13525390625 
[2025-03-22 10:20:10 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 3 loss:1.284797191619873 norm:0.003680514870211482 max memory_allocated 29277.13525390625 
[2025-03-22 10:20:58 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 4 loss:1.2803153991699219 norm:0.002654890762642026 max memory_allocated 29277.13525390625 
[2025-03-22 10:21:46 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 5 loss:1.2771480083465576 norm:0.0022708899341523647 max memory_allocated 29277.13525390625 
[2025-03-22 10:22:35 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 6 loss:1.2740594148635864 norm:0.002015006262809038 max memory_allocated 29277.13525390625 
[2025-03-22 10:23:23 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 7 loss:1.2710986137390137 norm:0.0017575534293428063 max memory_allocated 29277.13525390625 
[2025-03-22 10:24:11 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 8 loss:1.2694709300994873 norm:0.0014977875398471951 max memory_allocated 29277.13525390625 
[2025-03-22 10:24:59 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 9 loss:1.2684195041656494 norm:0.0013791477540507913 max memory_allocated 29277.13525390625 
[2025-03-22 10:25:48 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 10 loss:1.2671116590499878 norm:0.0011976290261372924 max memory_allocated 29277.13525390625 
[2025-03-22 10:26:36 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 11 loss:1.2664259672164917 norm:0.001109639066271484 max memory_allocated 29277.13525390625 
[2025-03-22 10:27:24 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 12 loss:1.2663391828536987 norm:0.0010769014479592443 max memory_allocated 29277.13525390625 
[2025-03-22 10:28:13 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 13 loss:1.2653685808181763 norm:0.0009922129102051258 max memory_allocated 29277.13525390625 
[2025-03-22 10:29:01 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 14 loss:1.265167474746704 norm:0.00091035186778754 max memory_allocated 29277.13525390625 
[2025-03-22 10:29:50 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 15 loss:1.2649204730987549 norm:0.0008708825334906578 max memory_allocated 29277.13525390625 
[2025-03-22 10:30:38 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 16 loss:1.2647335529327393 norm:0.0008449139422737062 max memory_allocated 29277.13525390625 
[2025-03-22 10:31:26 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 17 loss:1.2643532752990723 norm:0.0008352430886588991 max memory_allocated 29277.13525390625 
[2025-03-22 10:32:15 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 18 loss:1.2636117935180664 norm:0.0008370958385057747 max memory_allocated 29277.13525390625 
[2025-03-22 10:33:03 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 19 loss:1.2636550664901733 norm:0.0007516855839639902 max memory_allocated 29277.13525390625 
[2025-03-22 10:33:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 34 ===
[2025-03-22 10:34:09 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 0 loss:1.4451227188110352 norm:0.013934027403593063 max memory_allocated 29277.32275390625 
[2025-03-22 10:34:57 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 1 loss:1.429327130317688 norm:0.008413271978497505 max memory_allocated 29277.32275390625 
[2025-03-22 10:35:45 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 2 loss:1.4109382629394531 norm:0.005392978899180889 max memory_allocated 29277.32275390625 
[2025-03-22 10:36:34 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 3 loss:1.4025593996047974 norm:0.00383755867369473 max memory_allocated 29277.32275390625 
[2025-03-22 10:37:22 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 4 loss:1.3980977535247803 norm:0.003094547661021352 max memory_allocated 29277.32275390625 
[2025-03-22 10:38:10 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 5 loss:1.3941327333450317 norm:0.002417383249849081 max memory_allocated 29277.32275390625 
[2025-03-22 10:38:58 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 6 loss:1.3904629945755005 norm:0.001918239169754088 max memory_allocated 29277.32275390625 
[2025-03-22 10:39:46 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 7 loss:1.3878605365753174 norm:0.0016736581455916166 max memory_allocated 29277.32275390625 
[2025-03-22 10:40:35 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 8 loss:1.3860851526260376 norm:0.001531250192783773 max memory_allocated 29277.32275390625 
[2025-03-22 10:41:23 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 9 loss:1.3847496509552002 norm:0.0013904410880059004 max memory_allocated 29277.32275390625 
[2025-03-22 10:42:11 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 10 loss:1.3837862014770508 norm:0.0012700303923338652 max memory_allocated 29277.32275390625 
[2025-03-22 10:43:00 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 11 loss:1.3829482793807983 norm:0.001162063330411911 max memory_allocated 29277.32275390625 
[2025-03-22 10:43:48 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 12 loss:1.382260799407959 norm:0.0010770171647891402 max memory_allocated 29277.32275390625 
[2025-03-22 10:44:37 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 13 loss:1.38169527053833 norm:0.0010078934719786048 max memory_allocated 29277.32275390625 
[2025-03-22 10:45:25 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 14 loss:1.3812263011932373 norm:0.0009479868458583951 max memory_allocated 29277.32275390625 
[2025-03-22 10:46:14 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 15 loss:1.3807809352874756 norm:0.0009012033115141094 max memory_allocated 29277.32275390625 
[2025-03-22 10:47:02 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 16 loss:1.3804352283477783 norm:0.0008709346293471754 max memory_allocated 29277.32275390625 
[2025-03-22 10:47:50 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 17 loss:1.3801337480545044 norm:0.0007234144140966237 max memory_allocated 29277.32275390625 
[2025-03-22 10:48:39 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 18 loss:1.379831075668335 norm:0.0007089694845490158 max memory_allocated 29277.32275390625 
[2025-03-22 10:49:27 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 19 loss:1.3796374797821045 norm:0.0007094391039572656 max memory_allocated 29277.32275390625 
[2025-03-22 10:49:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 35 ===
[2025-03-22 10:50:33 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 0 loss:1.571610927581787 norm:0.005538526922464371 max memory_allocated 29277.51025390625 
[2025-03-22 10:51:21 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 1 loss:1.5548542737960815 norm:0.003598693525418639 max memory_allocated 29277.51025390625 
[2025-03-22 10:52:09 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 2 loss:1.534902572631836 norm:0.0024240438360720873 max memory_allocated 29277.51025390625 
[2025-03-22 10:52:57 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 3 loss:1.5258852243423462 norm:0.0016426752554252744 max memory_allocated 29277.51025390625 
[2025-03-22 10:53:45 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 4 loss:1.520697832107544 norm:0.001259505283087492 max memory_allocated 29277.51025390625 
[2025-03-22 10:54:33 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 5 loss:1.5164103507995605 norm:0.0010976914782077074 max memory_allocated 29277.51025390625 
[2025-03-22 10:55:22 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 6 loss:1.5125676393508911 norm:0.0010079608764499426 max memory_allocated 29277.51025390625 
[2025-03-22 10:56:10 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 7 loss:1.509567379951477 norm:0.0008694640710018575 max memory_allocated 29277.51025390625 
[2025-03-22 10:56:58 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 8 loss:1.5076773166656494 norm:0.0008321229252032936 max memory_allocated 29277.51025390625 
[2025-03-22 10:57:47 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 9 loss:1.506424903869629 norm:0.00080154585884884 max memory_allocated 29277.51025390625 
[2025-03-22 10:58:35 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 10 loss:1.5053330659866333 norm:0.0007979752263054252 max memory_allocated 29277.51025390625 
[2025-03-22 10:59:24 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 11 loss:1.5045431852340698 norm:0.0007921795477159321 max memory_allocated 29277.51025390625 
[2025-03-22 11:00:12 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 12 loss:1.5038862228393555 norm:0.000774560438003391 max memory_allocated 29277.51025390625 
[2025-03-22 11:01:00 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 13 loss:1.503351092338562 norm:0.0007619023090228438 max memory_allocated 29277.51025390625 
[2025-03-22 11:01:49 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 14 loss:1.5028409957885742 norm:0.0007458206964656711 max memory_allocated 29277.51025390625 
[2025-03-22 11:02:37 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 15 loss:1.5024442672729492 norm:0.0007429542019963264 max memory_allocated 29277.51025390625 
[2025-03-22 11:03:26 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 16 loss:1.5021449327468872 norm:0.0007364089251495898 max memory_allocated 29277.51025390625 
[2025-03-22 11:04:14 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 17 loss:1.5018320083618164 norm:0.0007359622395597398 max memory_allocated 29277.51025390625 
[2025-03-22 11:05:03 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 18 loss:1.5015830993652344 norm:0.0007261038408614695 max memory_allocated 29277.51025390625 
[2025-03-22 11:05:51 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 19 loss:1.501297950744629 norm:0.0007235610391944647 max memory_allocated 29277.51025390625 
[2025-03-22 11:06:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 36 ===
[2025-03-22 11:06:09 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:06:58 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 0 loss:1.7504485845565796 norm:0.03692914545536041 max memory_allocated 29277.98681640625 
[2025-03-22 11:07:46 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 1 loss:1.7265002727508545 norm:0.029572783038020134 max memory_allocated 29277.98681640625 
[2025-03-22 11:08:35 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 2 loss:1.698899269104004 norm:0.021734386682510376 max memory_allocated 29277.98681640625 
[2025-03-22 11:09:23 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 3 loss:1.6842141151428223 norm:0.017230071127414703 max memory_allocated 29277.98681640625 
[2025-03-22 11:10:12 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 4 loss:1.6722625494003296 norm:0.012977929785847664 max memory_allocated 29277.98681640625 
[2025-03-22 11:11:00 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 5 loss:1.6643304824829102 norm:0.01084609143435955 max memory_allocated 29277.98681640625 
[2025-03-22 11:11:49 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 6 loss:1.6586952209472656 norm:0.009728662669658661 max memory_allocated 29277.98681640625 
[2025-03-22 11:12:37 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 7 loss:1.6548433303833008 norm:0.008981139399111271 max memory_allocated 29277.98681640625 
[2025-03-22 11:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 8 loss:1.6521786451339722 norm:0.008471732959151268 max memory_allocated 29277.98681640625 
[2025-03-22 11:14:14 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 9 loss:1.6504000425338745 norm:0.008069075644016266 max memory_allocated 29277.98681640625 
[2025-03-22 11:15:02 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 10 loss:1.6491012573242188 norm:0.00774348946288228 max memory_allocated 29277.98681640625 
[2025-03-22 11:15:51 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 11 loss:1.6479524374008179 norm:0.007476216182112694 max memory_allocated 29277.98681640625 
[2025-03-22 11:16:39 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 12 loss:1.6470787525177002 norm:0.007224933244287968 max memory_allocated 29277.98681640625 
[2025-03-22 11:17:28 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 13 loss:1.6465221643447876 norm:0.007108285557478666 max memory_allocated 29277.98681640625 
[2025-03-22 11:18:16 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 14 loss:1.6458094120025635 norm:0.0071010407991707325 max memory_allocated 29277.98681640625 
[2025-03-22 11:19:05 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 15 loss:1.6453065872192383 norm:0.006990319117903709 max memory_allocated 29277.98681640625 
[2025-03-22 11:19:54 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 16 loss:1.6448473930358887 norm:0.006815051194280386 max memory_allocated 29277.98681640625 
[2025-03-22 11:20:42 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 17 loss:1.6442734003067017 norm:0.006642235908657312 max memory_allocated 29277.98681640625 
[2025-03-22 11:21:31 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 18 loss:1.6440541744232178 norm:0.006663527339696884 max memory_allocated 29277.98681640625 
[2025-03-22 11:22:20 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 19 loss:1.6436312198638916 norm:0.0066613792441785336 max memory_allocated 29277.98681640625 
[2025-03-22 11:22:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 37 ===
[2025-03-22 11:22:37 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:23:26 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 0 loss:1.9219614267349243 norm:0.037924375385046005 max memory_allocated 29278.17431640625 
[2025-03-22 11:24:14 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 1 loss:1.893174648284912 norm:0.03154650330543518 max memory_allocated 29278.17431640625 
[2025-03-22 11:25:03 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 2 loss:1.8624526262283325 norm:0.02454824559390545 max memory_allocated 29278.17431640625 
[2025-03-22 11:25:51 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 3 loss:1.8424313068389893 norm:0.01867024414241314 max memory_allocated 29278.17431640625 
[2025-03-22 11:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 4 loss:1.832504391670227 norm:0.014929678291082382 max memory_allocated 29278.17431640625 
[2025-03-22 11:27:28 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 5 loss:1.8250253200531006 norm:0.012273921631276608 max memory_allocated 29278.17431640625 
[2025-03-22 11:28:17 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 6 loss:1.8197808265686035 norm:0.010825085453689098 max memory_allocated 29278.17431640625 
[2025-03-22 11:29:05 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 7 loss:1.8161078691482544 norm:0.010679170489311218 max memory_allocated 29278.17431640625 
[2025-03-22 11:29:53 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 8 loss:1.8135225772857666 norm:0.01029742881655693 max memory_allocated 29278.17431640625 
[2025-03-22 11:30:42 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 9 loss:1.81101393699646 norm:0.009661972522735596 max memory_allocated 29278.17431640625 
[2025-03-22 11:31:30 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 10 loss:1.8097162246704102 norm:0.009430458769202232 max memory_allocated 29278.17431640625 
[2025-03-22 11:32:19 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 11 loss:1.8077619075775146 norm:0.008721524849534035 max memory_allocated 29278.17431640625 
[2025-03-22 11:33:07 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 12 loss:1.8065659999847412 norm:0.008522002026438713 max memory_allocated 29278.17431640625 
[2025-03-22 11:33:56 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 13 loss:1.8052473068237305 norm:0.008188711479306221 max memory_allocated 29278.17431640625 
[2025-03-22 11:34:44 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 14 loss:1.8047800064086914 norm:0.008463002741336823 max memory_allocated 29278.17431640625 
[2025-03-22 11:35:33 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 15 loss:1.8041208982467651 norm:0.008365413174033165 max memory_allocated 29278.17431640625 
[2025-03-22 11:36:21 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 16 loss:1.8035485744476318 norm:0.008249993436038494 max memory_allocated 29278.17431640625 
[2025-03-22 11:37:10 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 17 loss:1.8027396202087402 norm:0.007862173020839691 max memory_allocated 29278.17431640625 
[2025-03-22 11:37:59 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 18 loss:1.8021762371063232 norm:0.007846447639167309 max memory_allocated 29278.17431640625 
[2025-03-22 11:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 19 loss:1.8018301725387573 norm:0.007850749418139458 max memory_allocated 29278.17431640625 
[2025-03-22 11:39:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 38 ===
[2025-03-22 11:39:05 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:39:54 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 0 loss:2.217993974685669 norm:0.06103686988353729 max memory_allocated 29278.36181640625 
[2025-03-22 11:40:42 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 1 loss:2.179333448410034 norm:0.05206964164972305 max memory_allocated 29278.36181640625 
[2025-03-22 11:41:31 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 2 loss:2.139324426651001 norm:0.040207140147686005 max memory_allocated 29278.36181640625 
[2025-03-22 11:42:19 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 3 loss:2.1118340492248535 norm:0.03140052780508995 max memory_allocated 29278.36181640625 
[2025-03-22 11:43:08 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 4 loss:2.097578763961792 norm:0.025618666782975197 max memory_allocated 29278.36181640625 
[2025-03-22 11:43:57 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 5 loss:2.0884108543395996 norm:0.021590694785118103 max memory_allocated 29278.36181640625 
[2025-03-22 11:44:45 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 6 loss:2.082005739212036 norm:0.019381888210773468 max memory_allocated 29278.36181640625 
[2025-03-22 11:45:34 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 7 loss:2.0769431591033936 norm:0.01765340194106102 max memory_allocated 29278.36181640625 
[2025-03-22 11:46:22 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 8 loss:2.073012351989746 norm:0.016735535115003586 max memory_allocated 29278.36181640625 
[2025-03-22 11:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 9 loss:2.070225715637207 norm:0.015738319605588913 max memory_allocated 29278.36181640625 
[2025-03-22 11:47:59 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 10 loss:2.0677573680877686 norm:0.01553177461028099 max memory_allocated 29278.36181640625 
[2025-03-22 11:48:48 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 11 loss:2.0660204887390137 norm:0.015369037166237831 max memory_allocated 29278.36181640625 
[2025-03-22 11:49:36 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 12 loss:2.0641720294952393 norm:0.014136003330349922 max memory_allocated 29278.36181640625 
[2025-03-22 11:50:25 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 13 loss:2.0623579025268555 norm:0.01425323449075222 max memory_allocated 29278.36181640625 
[2025-03-22 11:51:13 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 14 loss:2.059373140335083 norm:0.013618982397019863 max memory_allocated 29278.36181640625 
[2025-03-22 11:52:02 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 15 loss:2.057614803314209 norm:0.014266951009631157 max memory_allocated 29278.36181640625 
[2025-03-22 11:52:50 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 16 loss:2.055697441101074 norm:0.0137151088565588 max memory_allocated 29278.36181640625 
[2025-03-22 11:53:39 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 17 loss:2.0540695190429688 norm:0.012082353234291077 max memory_allocated 29278.36181640625 
[2025-03-22 11:54:27 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 18 loss:2.0532948970794678 norm:0.012228738516569138 max memory_allocated 29278.36181640625 
[2025-03-22 11:55:16 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 19 loss:2.0532026290893555 norm:0.013865099288523197 max memory_allocated 29278.36181640625 
[2025-03-22 11:55:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 39 ===
[2025-03-22 11:55:33 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:56:22 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 0 loss:3.2258052825927734 norm:0.19806109368801117 max memory_allocated 29278.54931640625 
[2025-03-22 11:57:10 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 1 loss:3.012589454650879 norm:0.15889257192611694 max memory_allocated 29278.54931640625 
[2025-03-22 11:57:59 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 2 loss:2.9093520641326904 norm:0.13210178911685944 max memory_allocated 29278.54931640625 
[2025-03-22 11:58:47 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 3 loss:2.8372294902801514 norm:0.11230907589197159 max memory_allocated 29278.54931640625 
[2025-03-22 11:59:36 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 4 loss:2.797861099243164 norm:0.09462038427591324 max memory_allocated 29278.54931640625 
[2025-03-22 12:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 5 loss:2.7750191688537598 norm:0.08360914885997772 max memory_allocated 29278.54931640625 
[2025-03-22 12:01:13 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 6 loss:2.753349542617798 norm:0.0697551965713501 max memory_allocated 29278.54931640625 
[2025-03-22 12:02:02 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 7 loss:2.7387216091156006 norm:0.0632161796092987 max memory_allocated 29278.54931640625 
[2025-03-22 12:02:50 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 8 loss:2.727236032485962 norm:0.05693155899643898 max memory_allocated 29278.54931640625 
[2025-03-22 12:03:39 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 9 loss:2.717918872833252 norm:0.05368936434388161 max memory_allocated 29278.54931640625 
[2025-03-22 12:04:27 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 10 loss:2.708895683288574 norm:0.04840272292494774 max memory_allocated 29278.54931640625 
[2025-03-22 12:05:15 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 11 loss:2.6993048191070557 norm:0.04573237895965576 max memory_allocated 29278.54931640625 
[2025-03-22 12:06:04 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 12 loss:2.692369222640991 norm:0.04408780112862587 max memory_allocated 29278.54931640625 
[2025-03-22 12:06:52 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 13 loss:2.6846139430999756 norm:0.042436957359313965 max memory_allocated 29278.54931640625 
[2025-03-22 12:07:40 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 14 loss:2.6767563819885254 norm:0.04073893651366234 max memory_allocated 29278.54931640625 
[2025-03-22 12:08:29 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 15 loss:2.6712646484375 norm:0.03919666260480881 max memory_allocated 29278.54931640625 
[2025-03-22 12:09:17 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 16 loss:2.6663103103637695 norm:0.03742588683962822 max memory_allocated 29278.54931640625 
[2025-03-22 12:10:06 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 17 loss:2.6618943214416504 norm:0.0360848531126976 max memory_allocated 29278.54931640625 
[2025-03-22 12:10:54 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 18 loss:2.6594605445861816 norm:0.03594664856791496 max memory_allocated 29278.54931640625 
[2025-03-22 12:11:43 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 19 loss:2.656247138977051 norm:0.03474472090601921 max memory_allocated 29278.54931640625 
[2025-03-22 12:11:57 root] (main_calibration_a.py 369): INFO 39426.3833963871
[2025-03-22 12:12:06 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 12:13:58 root] (main_calibration_a.py 158): INFO wikitext2 : 7.590626239776611
[2025-03-22 12:13:58 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 12:16:52 root] (main_calibration_a.py 158): INFO c4 : 12.023787498474121
[2025-03-22 12:19:11 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/hellaswag/512a66dd8b1b1643ab4a48aa4f150d04c91680da6a4096498a5e5f799623d5ae (last modified on Tue Feb 18 03:27:10 2025) since it couldn't be found locally at hellaswag., or remotely on the Hugging Face Hub.
[2025-03-22 14:26:14 root] (main_calibration_a.py 169): INFO {'wikitext2': 7.590626239776611, 'c4': 12.023787498474121, 'results': {'arc_challenge': {'acc': 0.3199658703071672, 'acc_stderr': 0.013631345807016193, 'acc_norm': 0.3447098976109215, 'acc_norm_stderr': 0.01388881628678211}, 'arc_easy': {'acc': 0.6094276094276094, 'acc_stderr': 0.010011059112064243, 'acc_norm': 0.48358585858585856, 'acc_norm_stderr': 0.010254253565929301}, 'piqa': {'acc': 0.705658324265506, 'acc_stderr': 0.010633311470347497, 'acc_norm': 0.7170837867247007, 'acc_norm_stderr': 0.010508949177489672}, 'winogrande': {'acc': 0.5643251775848461, 'acc_stderr': 0.013935709739615719}, 'boolq': {'acc': 0.6382262996941896, 'acc_stderr': 0.008404238796949254}, 'hellaswag': {'acc': 0.49830711013742285, 'acc_stderr': 0.004989752811173407, 'acc_norm': 0.6485759808803028, 'acc_norm_stderr': 0.004764393985111033}}, 'versions': {'arc_challenge': 0, 'arc_easy': 0, 'piqa': 0, 'winogrande': 0, 'boolq': 1, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 14:26:14 root] (main_calibration_a.py 172): INFO 32.00,60.94,63.82,49.83,70.57,56.43
