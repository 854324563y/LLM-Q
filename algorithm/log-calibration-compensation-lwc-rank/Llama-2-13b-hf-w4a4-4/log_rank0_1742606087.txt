[2025-03-22 01:14:47 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/Llama-2-13b-hf-w4a4-4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=4)
[2025-03-22 01:14:51 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 01:14:51 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:14:52 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 01:15:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:15:21 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.08575064688920975 norm:0.09532452374696732 max memory_allocated 29271.67041015625 
[2025-03-22 01:16:55 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.04635024070739746 norm:0.04661953076720238 max memory_allocated 29271.67041015625 
[2025-03-22 01:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.03571473807096481 norm:0.03802202269434929 max memory_allocated 29271.67041015625 
[2025-03-22 01:18:30 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.03029058873653412 norm:0.02959534339606762 max memory_allocated 29271.67041015625 
[2025-03-22 01:19:18 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.02713608555495739 norm:0.02522694692015648 max memory_allocated 29271.67041015625 
[2025-03-22 01:20:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.02552756480872631 norm:0.022346438840031624 max memory_allocated 29271.67041015625 
[2025-03-22 01:20:53 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.02415366843342781 norm:0.020784780383110046 max memory_allocated 29271.67041015625 
[2025-03-22 01:21:41 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.023304065689444542 norm:0.01781650073826313 max memory_allocated 29271.67041015625 
[2025-03-22 01:22:28 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.02281695231795311 norm:0.016121242195367813 max memory_allocated 29271.67041015625 
[2025-03-22 01:23:16 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.022389966994524002 norm:0.014733054675161839 max memory_allocated 29271.67041015625 
[2025-03-22 01:24:04 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.021853474900126457 norm:0.013142910785973072 max memory_allocated 29271.67041015625 
[2025-03-22 01:24:51 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.021489068865776062 norm:0.011897734366357327 max memory_allocated 29271.67041015625 
[2025-03-22 01:25:39 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.02133987657725811 norm:0.01071254350244999 max memory_allocated 29271.67041015625 
[2025-03-22 01:26:27 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.021261442452669144 norm:0.009824652224779129 max memory_allocated 29271.67041015625 
[2025-03-22 01:27:15 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.021131301298737526 norm:0.00947653315961361 max memory_allocated 29271.67041015625 
[2025-03-22 01:28:02 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.021026577800512314 norm:0.008790474385023117 max memory_allocated 29271.67041015625 
[2025-03-22 01:28:50 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.02085854858160019 norm:0.008064910769462585 max memory_allocated 29271.67041015625 
[2025-03-22 01:29:38 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.020831938832998276 norm:0.008718358352780342 max memory_allocated 29271.67041015625 
[2025-03-22 01:30:26 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.020643044263124466 norm:0.007238391321152449 max memory_allocated 29271.67041015625 
[2025-03-22 01:31:13 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.0204507764428854 norm:0.0066893454641103745 max memory_allocated 29271.67041015625 
[2025-03-22 01:31:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:31:41 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:32:28 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.14796648919582367 norm:0.058170393109321594 max memory_allocated 29271.67041015625 
[2025-03-22 01:33:16 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.10852053016424179 norm:0.04732532054185867 max memory_allocated 29271.67041015625 
[2025-03-22 01:34:04 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.09133906662464142 norm:0.03189544379711151 max memory_allocated 29271.67041015625 
[2025-03-22 01:34:51 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.08390078693628311 norm:0.023231444880366325 max memory_allocated 29271.67041015625 
[2025-03-22 01:35:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.08051341027021408 norm:0.01907205581665039 max memory_allocated 29271.67041015625 
[2025-03-22 01:36:26 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.07822749018669128 norm:0.015944140031933784 max memory_allocated 29271.67041015625 
[2025-03-22 01:37:14 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.07646802812814713 norm:0.013075906783342361 max memory_allocated 29271.67041015625 
[2025-03-22 01:38:02 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.07547317445278168 norm:0.01121373288333416 max memory_allocated 29271.67041015625 
[2025-03-22 01:38:50 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.0744372233748436 norm:0.009351004846394062 max memory_allocated 29271.67041015625 
[2025-03-22 01:39:37 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.07366763055324554 norm:0.008116272278130054 max memory_allocated 29271.67041015625 
[2025-03-22 01:40:25 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.07321891188621521 norm:0.0074033113196492195 max memory_allocated 29271.67041015625 
[2025-03-22 01:41:13 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.07288633286952972 norm:0.006967122200876474 max memory_allocated 29271.67041015625 
[2025-03-22 01:42:01 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.07260735332965851 norm:0.00663866987451911 max memory_allocated 29271.67041015625 
[2025-03-22 01:42:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.07226181030273438 norm:0.006250716745853424 max memory_allocated 29271.67041015625 
[2025-03-22 01:43:37 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.07205343246459961 norm:0.005955887958407402 max memory_allocated 29271.67041015625 
[2025-03-22 01:44:25 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07205942273139954 norm:0.006024655885994434 max memory_allocated 29271.67041015625 
[2025-03-22 01:45:13 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.07186094671487808 norm:0.005933266133069992 max memory_allocated 29271.67041015625 
[2025-03-22 01:46:01 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07178305089473724 norm:0.005795601289719343 max memory_allocated 29271.67041015625 
[2025-03-22 01:46:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07169289886951447 norm:0.005627025850117207 max memory_allocated 29271.67041015625 
[2025-03-22 01:47:37 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.07165294885635376 norm:0.00544786499813199 max memory_allocated 29271.67041015625 
[2025-03-22 01:47:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:48:01 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.15879420936107635 norm:0.030219636857509613 max memory_allocated 29271.67041015625 
[2025-03-22 01:49:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.1344541609287262 norm:0.03226491063833237 max memory_allocated 29271.67041015625 
[2025-03-22 01:50:24 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.12115932255983353 norm:0.022984493523836136 max memory_allocated 29271.67041015625 
[2025-03-22 01:51:12 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.11517085134983063 norm:0.01784113235771656 max memory_allocated 29271.67041015625 
[2025-03-22 01:52:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.11181515455245972 norm:0.014254024252295494 max memory_allocated 29271.67041015625 
[2025-03-22 01:52:48 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.10970769077539444 norm:0.010919438675045967 max memory_allocated 29271.67041015625 
[2025-03-22 01:53:35 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.10792715847492218 norm:0.008431140333414078 max memory_allocated 29271.67041015625 
[2025-03-22 01:54:23 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.10648433864116669 norm:0.006629486568272114 max memory_allocated 29271.67041015625 
[2025-03-22 01:55:11 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.1056320071220398 norm:0.005873389542102814 max memory_allocated 29271.67041015625 
[2025-03-22 01:55:58 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.10519267618656158 norm:0.005994864739477634 max memory_allocated 29271.67041015625 
[2025-03-22 01:56:46 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.10471773892641068 norm:0.005449725314974785 max memory_allocated 29271.67041015625 
[2025-03-22 01:57:34 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.10450553894042969 norm:0.005331152584403753 max memory_allocated 29271.67041015625 
[2025-03-22 01:58:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.10428769141435623 norm:0.00515535706654191 max memory_allocated 29271.67041015625 
[2025-03-22 01:59:10 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.10419193655252457 norm:0.005147218704223633 max memory_allocated 29271.67041015625 
[2025-03-22 01:59:58 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.10423274338245392 norm:0.005091337952762842 max memory_allocated 29271.67041015625 
[2025-03-22 02:00:46 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.10416287183761597 norm:0.004825250245630741 max memory_allocated 29271.67041015625 
[2025-03-22 02:01:34 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.10422436892986298 norm:0.004800432361662388 max memory_allocated 29271.67041015625 
[2025-03-22 02:02:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.10428130626678467 norm:0.0046655829064548016 max memory_allocated 29271.67041015625 
[2025-03-22 02:03:10 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.10429927706718445 norm:0.004644031170755625 max memory_allocated 29271.67041015625 
[2025-03-22 02:03:58 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.10422886908054352 norm:0.0044469935819506645 max memory_allocated 29271.67041015625 
[2025-03-22 02:04:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 02:05:11 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.40131402015686035 norm:0.17600677907466888 max memory_allocated 29271.67041015625 
[2025-03-22 02:05:58 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.28869372606277466 norm:0.039141032844781876 max memory_allocated 29271.67041015625 
[2025-03-22 02:06:46 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.24243056774139404 norm:0.024164976552128792 max memory_allocated 29271.67041015625 
[2025-03-22 02:07:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.2227640599012375 norm:0.024708520621061325 max memory_allocated 29271.67041015625 
[2025-03-22 02:08:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.21421274542808533 norm:0.020420098677277565 max memory_allocated 29271.67041015625 
[2025-03-22 02:09:08 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.2078152745962143 norm:0.017156898975372314 max memory_allocated 29271.67041015625 
[2025-03-22 02:09:56 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.20275287330150604 norm:0.014396009966731071 max memory_allocated 29271.67041015625 
[2025-03-22 02:10:43 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.20234180986881256 norm:0.014340149238705635 max memory_allocated 29271.67041015625 
[2025-03-22 02:11:31 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.1984773576259613 norm:0.012874091044068336 max memory_allocated 29271.67041015625 
[2025-03-22 02:12:18 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.19618822634220123 norm:0.013131997548043728 max memory_allocated 29271.67041015625 
[2025-03-22 02:13:06 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.19346323609352112 norm:0.011379344388842583 max memory_allocated 29271.67041015625 
[2025-03-22 02:13:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.19343948364257812 norm:0.011771761812269688 max memory_allocated 29271.67041015625 
[2025-03-22 02:14:41 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.19223695993423462 norm:0.012584676966071129 max memory_allocated 29271.67041015625 
[2025-03-22 02:15:29 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.19221705198287964 norm:0.01142529584467411 max memory_allocated 29271.67041015625 
[2025-03-22 02:16:17 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.1927173137664795 norm:0.011260416358709335 max memory_allocated 29271.67041015625 
[2025-03-22 02:17:04 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.19076336920261383 norm:0.011061076074838638 max memory_allocated 29271.67041015625 
[2025-03-22 02:17:52 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.18914350867271423 norm:0.010622816160321236 max memory_allocated 29271.67041015625 
[2025-03-22 02:18:40 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.1880047619342804 norm:0.009998961351811886 max memory_allocated 29271.67041015625 
[2025-03-22 02:19:28 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.18810756504535675 norm:0.009320413693785667 max memory_allocated 29271.67041015625 
[2025-03-22 02:20:16 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.18726420402526855 norm:0.01041453704237938 max memory_allocated 29271.67041015625 
[2025-03-22 02:20:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:21:25 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.2361617237329483 norm:0.008303635753691196 max memory_allocated 29271.67041015625 
[2025-03-22 02:22:13 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.21779265999794006 norm:0.003948601894080639 max memory_allocated 29271.67041015625 
[2025-03-22 02:23:01 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.20446829497814178 norm:0.00233450741507113 max memory_allocated 29271.67041015625 
[2025-03-22 02:23:48 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.19908003509044647 norm:0.0016545089893043041 max memory_allocated 29271.67041015625 
[2025-03-22 02:24:36 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.19608956575393677 norm:0.0012905257754027843 max memory_allocated 29271.67041015625 
[2025-03-22 02:25:23 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.19408029317855835 norm:0.0011203092290088534 max memory_allocated 29271.67041015625 
[2025-03-22 02:26:11 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.1929762214422226 norm:0.001054130494594574 max memory_allocated 29271.67041015625 
[2025-03-22 02:26:59 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.19218413531780243 norm:0.0010229229228571057 max memory_allocated 29271.67041015625 
[2025-03-22 02:27:46 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.19149291515350342 norm:0.001016397261992097 max memory_allocated 29271.67041015625 
[2025-03-22 02:28:34 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.19105619192123413 norm:0.0010051521239802241 max memory_allocated 29271.67041015625 
[2025-03-22 02:29:21 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.19067245721817017 norm:0.00099079345818609 max memory_allocated 29271.67041015625 
[2025-03-22 02:30:09 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.19012689590454102 norm:0.0009556574514135718 max memory_allocated 29271.67041015625 
[2025-03-22 02:30:57 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.18970821797847748 norm:0.0009414408123120666 max memory_allocated 29271.67041015625 
[2025-03-22 02:31:44 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.189351886510849 norm:0.0009155425941571593 max memory_allocated 29271.67041015625 
[2025-03-22 02:32:32 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.18899209797382355 norm:0.0009068174404092133 max memory_allocated 29271.67041015625 
[2025-03-22 02:33:20 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.18879911303520203 norm:0.0008974560769274831 max memory_allocated 29271.67041015625 
[2025-03-22 02:34:08 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.18865661323070526 norm:0.0008858760120347142 max memory_allocated 29271.67041015625 
[2025-03-22 02:34:55 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.1885479986667633 norm:0.0008875405765138566 max memory_allocated 29271.67041015625 
[2025-03-22 02:35:43 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.188441202044487 norm:0.000893876887857914 max memory_allocated 29271.67041015625 
[2025-03-22 02:36:31 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.18837660551071167 norm:0.0009046553168445826 max memory_allocated 29271.67041015625 
[2025-03-22 02:36:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:37:40 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.27026110887527466 norm:0.011597180739045143 max memory_allocated 29271.67041015625 
[2025-03-22 02:38:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2473028302192688 norm:0.004595128353685141 max memory_allocated 29271.67041015625 
[2025-03-22 02:39:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.23148047924041748 norm:0.0025255035143345594 max memory_allocated 29271.67041015625 
[2025-03-22 02:40:03 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.2253318727016449 norm:0.0016642967239022255 max memory_allocated 29271.67041015625 
[2025-03-22 02:40:51 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.2221672683954239 norm:0.0012519031297415495 max memory_allocated 29271.67041015625 
[2025-03-22 02:41:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.2201424539089203 norm:0.001053319312632084 max memory_allocated 29271.67041015625 
[2025-03-22 02:42:26 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.2185502052307129 norm:0.0009929859079420567 max memory_allocated 29271.67041015625 
[2025-03-22 02:43:14 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.21742799878120422 norm:0.0009662308730185032 max memory_allocated 29271.67041015625 
[2025-03-22 02:44:02 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.2165932059288025 norm:0.0009577166056260467 max memory_allocated 29271.67041015625 
[2025-03-22 02:44:49 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.21579813957214355 norm:0.0009070081869140267 max memory_allocated 29271.67041015625 
[2025-03-22 02:45:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.21529418230056763 norm:0.0008918620296753943 max memory_allocated 29271.67041015625 
[2025-03-22 02:46:24 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.21496425569057465 norm:0.0008785338141024113 max memory_allocated 29271.67041015625 
[2025-03-22 02:47:12 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.21480639278888702 norm:0.0008666001376695931 max memory_allocated 29271.67041015625 
[2025-03-22 02:48:00 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.214660182595253 norm:0.0008716880111023784 max memory_allocated 29271.67041015625 
[2025-03-22 02:48:47 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.21447612345218658 norm:0.0008673004922457039 max memory_allocated 29271.67041015625 
[2025-03-22 02:49:35 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.21431145071983337 norm:0.0008581285364925861 max memory_allocated 29271.67041015625 
[2025-03-22 02:50:23 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.21406309306621552 norm:0.0008367577102035284 max memory_allocated 29271.67041015625 
[2025-03-22 02:51:11 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.2138996720314026 norm:0.0008289351244457066 max memory_allocated 29271.67041015625 
[2025-03-22 02:51:59 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.2137094885110855 norm:0.0008273826679214835 max memory_allocated 29271.67041015625 
[2025-03-22 02:52:46 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.21369300782680511 norm:0.0008220035815611482 max memory_allocated 29271.67041015625 
[2025-03-22 02:53:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:53:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.30654585361480713 norm:0.023963311687111855 max memory_allocated 29272.21728515625 
[2025-03-22 02:54:45 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.2794691026210785 norm:0.011727394536137581 max memory_allocated 29272.21728515625 
[2025-03-22 02:55:33 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.2566724419593811 norm:0.002980866003781557 max memory_allocated 29272.21728515625 
[2025-03-22 02:56:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.24931493401527405 norm:0.0015417688991874456 max memory_allocated 29272.21728515625 
[2025-03-22 02:57:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.24569445848464966 norm:0.0013337439158931375 max memory_allocated 29272.21728515625 
[2025-03-22 02:57:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.243163600564003 norm:0.0012108379742130637 max memory_allocated 29272.21728515625 
[2025-03-22 02:58:43 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.2411346733570099 norm:0.0011162866139784455 max memory_allocated 29272.21728515625 
[2025-03-22 02:59:31 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.23961490392684937 norm:0.0010732121299952269 max memory_allocated 29272.21728515625 
[2025-03-22 03:00:18 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.23841331899166107 norm:0.0010476214811205864 max memory_allocated 29272.21728515625 
[2025-03-22 03:01:06 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.2376871407032013 norm:0.0010126957204192877 max memory_allocated 29272.21728515625 
[2025-03-22 03:01:54 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.23714976012706757 norm:0.000990720116533339 max memory_allocated 29272.21728515625 
[2025-03-22 03:02:41 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.23655952513217926 norm:0.0009747428121045232 max memory_allocated 29272.21728515625 
[2025-03-22 03:03:29 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.236222505569458 norm:0.0009591196430847049 max memory_allocated 29272.21728515625 
[2025-03-22 03:04:17 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.23599888384342194 norm:0.0009534694836474955 max memory_allocated 29272.21728515625 
[2025-03-22 03:05:04 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.23578771948814392 norm:0.0009518135921098292 max memory_allocated 29272.21728515625 
[2025-03-22 03:05:52 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.23560026288032532 norm:0.0009517836733721197 max memory_allocated 29272.21728515625 
[2025-03-22 03:06:40 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.235402449965477 norm:0.0009485224727541208 max memory_allocated 29272.21728515625 
[2025-03-22 03:07:28 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.23529204726219177 norm:0.0009482121677137911 max memory_allocated 29272.21728515625 
[2025-03-22 03:08:16 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.2351641207933426 norm:0.0009424361633136868 max memory_allocated 29272.21728515625 
[2025-03-22 03:09:04 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.23513320088386536 norm:0.0009409593767486513 max memory_allocated 29272.21728515625 
[2025-03-22 03:09:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 03:10:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.35820865631103516 norm:0.014780394732952118 max memory_allocated 29272.40478515625 
[2025-03-22 03:11:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.3163173794746399 norm:0.005234916228801012 max memory_allocated 29272.40478515625 
[2025-03-22 03:11:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.2882586717605591 norm:0.002268071286380291 max memory_allocated 29272.40478515625 
[2025-03-22 03:12:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.27914509177207947 norm:0.0015548019437119365 max memory_allocated 29272.40478515625 
[2025-03-22 03:13:24 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.2751438021659851 norm:0.0013277052203193307 max memory_allocated 29272.40478515625 
[2025-03-22 03:14:12 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.2718921899795532 norm:0.0012404090957716107 max memory_allocated 29272.40478515625 
[2025-03-22 03:14:59 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.2697365880012512 norm:0.0011682137846946716 max memory_allocated 29272.40478515625 
[2025-03-22 03:15:47 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.2680560350418091 norm:0.0011331691639497876 max memory_allocated 29272.40478515625 
[2025-03-22 03:16:35 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.26700425148010254 norm:0.0011080566328018904 max memory_allocated 29272.40478515625 
[2025-03-22 03:17:22 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.26629340648651123 norm:0.0010906123789027333 max memory_allocated 29272.40478515625 
[2025-03-22 03:18:10 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.26575368642807007 norm:0.0010861248010769486 max memory_allocated 29272.40478515625 
[2025-03-22 03:18:58 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.26525866985321045 norm:0.0010527143022045493 max memory_allocated 29272.40478515625 
[2025-03-22 03:19:45 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.26487863063812256 norm:0.0010509116109460592 max memory_allocated 29272.40478515625 
[2025-03-22 03:20:33 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2646511495113373 norm:0.0010435421718284488 max memory_allocated 29272.40478515625 
[2025-03-22 03:21:21 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.2644708454608917 norm:0.0010446103988215327 max memory_allocated 29272.40478515625 
[2025-03-22 03:22:09 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.26431623101234436 norm:0.001057914923876524 max memory_allocated 29272.40478515625 
[2025-03-22 03:22:57 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.26395881175994873 norm:0.0010490776039659977 max memory_allocated 29272.40478515625 
[2025-03-22 03:23:44 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.26393282413482666 norm:0.0010370977688580751 max memory_allocated 29272.40478515625 
[2025-03-22 03:24:32 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.2638387680053711 norm:0.0010399093152955174 max memory_allocated 29272.40478515625 
[2025-03-22 03:25:20 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.26353710889816284 norm:0.0010308021446689963 max memory_allocated 29272.40478515625 
[2025-03-22 03:25:34 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 03:26:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.38290297985076904 norm:0.0322745107114315 max memory_allocated 29272.59228515625 
[2025-03-22 03:27:17 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.3503246307373047 norm:0.02083348110318184 max memory_allocated 29272.59228515625 
[2025-03-22 03:28:05 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.31730467081069946 norm:0.012884808704257011 max memory_allocated 29272.59228515625 
[2025-03-22 03:28:53 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.3051382005214691 norm:0.008370857685804367 max memory_allocated 29272.59228515625 
[2025-03-22 03:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.29679885506629944 norm:0.003313907887786627 max memory_allocated 29272.59228515625 
[2025-03-22 03:30:28 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.29190534353256226 norm:0.001543046673759818 max memory_allocated 29272.59228515625 
[2025-03-22 03:31:16 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.2888910472393036 norm:0.0013738551642745733 max memory_allocated 29272.59228515625 
[2025-03-22 03:32:04 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.2868170738220215 norm:0.001276256749406457 max memory_allocated 29272.59228515625 
[2025-03-22 03:32:51 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.28526943922042847 norm:0.0011940737022086978 max memory_allocated 29272.59228515625 
[2025-03-22 03:33:39 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.2842729687690735 norm:0.0011638590367510915 max memory_allocated 29272.59228515625 
[2025-03-22 03:34:27 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.2835756242275238 norm:0.0011383835226297379 max memory_allocated 29272.59228515625 
[2025-03-22 03:35:14 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.28299495577812195 norm:0.0010914526646956801 max memory_allocated 29272.59228515625 
[2025-03-22 03:36:02 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.2825402021408081 norm:0.001076578046195209 max memory_allocated 29272.59228515625 
[2025-03-22 03:36:49 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.28206977248191833 norm:0.0010686267632991076 max memory_allocated 29272.59228515625 
[2025-03-22 03:37:37 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.28179460763931274 norm:0.0010676828678697348 max memory_allocated 29272.59228515625 
[2025-03-22 03:38:25 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.2815331518650055 norm:0.0010408343514427543 max memory_allocated 29272.59228515625 
[2025-03-22 03:39:13 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.2812790870666504 norm:0.0010175382485613227 max memory_allocated 29272.59228515625 
[2025-03-22 03:40:00 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.2811548113822937 norm:0.0010120475199073553 max memory_allocated 29272.59228515625 
[2025-03-22 03:40:48 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.2810629904270172 norm:0.0010040143970400095 max memory_allocated 29272.59228515625 
[2025-03-22 03:41:36 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.28102439641952515 norm:0.0009928412036970258 max memory_allocated 29272.59228515625 
[2025-03-22 03:41:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 03:42:46 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.4457787275314331 norm:0.031136149540543556 max memory_allocated 29272.77978515625 
[2025-03-22 03:43:33 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.3860034942626953 norm:0.012274041771888733 max memory_allocated 29272.77978515625 
[2025-03-22 03:44:21 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.3414459824562073 norm:0.005232840310782194 max memory_allocated 29272.77978515625 
[2025-03-22 03:45:09 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.32410043478012085 norm:0.00268390029668808 max memory_allocated 29272.77978515625 
[2025-03-22 03:45:57 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.31762024760246277 norm:0.0021344718988984823 max memory_allocated 29272.77978515625 
[2025-03-22 03:46:45 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3135733902454376 norm:0.001668972778134048 max memory_allocated 29272.77978515625 
[2025-03-22 03:47:32 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.31073692440986633 norm:0.0015454378444701433 max memory_allocated 29272.77978515625 
[2025-03-22 03:48:20 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.30866676568984985 norm:0.0014550734777003527 max memory_allocated 29272.77978515625 
[2025-03-22 03:49:08 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.30706796050071716 norm:0.0013885903172194958 max memory_allocated 29272.77978515625 
[2025-03-22 03:49:56 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3058382570743561 norm:0.0012883221497759223 max memory_allocated 29272.77978515625 
[2025-03-22 03:50:43 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.3049565851688385 norm:0.001191417220979929 max memory_allocated 29272.77978515625 
[2025-03-22 03:51:31 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.30426961183547974 norm:0.0011593019589781761 max memory_allocated 29272.77978515625 
[2025-03-22 03:52:18 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.30362385511398315 norm:0.0011024056002497673 max memory_allocated 29272.77978515625 
[2025-03-22 03:53:06 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.30332741141319275 norm:0.001102879294194281 max memory_allocated 29272.77978515625 
[2025-03-22 03:53:54 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.30313965678215027 norm:0.001069836551323533 max memory_allocated 29272.77978515625 
[2025-03-22 03:54:41 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.3029825985431671 norm:0.0010602273978292942 max memory_allocated 29272.77978515625 
[2025-03-22 03:55:29 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.30260759592056274 norm:0.0010487892432138324 max memory_allocated 29272.77978515625 
[2025-03-22 03:56:17 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.30244889855384827 norm:0.001043421565555036 max memory_allocated 29272.77978515625 
[2025-03-22 03:57:05 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.3023078739643097 norm:0.0010613326448947191 max memory_allocated 29272.77978515625 
[2025-03-22 03:57:53 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.3019777834415436 norm:0.0010218785610049963 max memory_allocated 29272.77978515625 
[2025-03-22 03:58:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:59:02 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.431399405002594 norm:0.02188469097018242 max memory_allocated 29272.96728515625 
[2025-03-22 03:59:50 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.38611191511154175 norm:0.008370858617126942 max memory_allocated 29272.96728515625 
[2025-03-22 04:00:38 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.3515230417251587 norm:0.00348532572388649 max memory_allocated 29272.96728515625 
[2025-03-22 04:01:26 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3385733962059021 norm:0.0021131415851414204 max memory_allocated 29272.96728515625 
[2025-03-22 04:02:13 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.33282071352005005 norm:0.0016307930927723646 max memory_allocated 29272.96728515625 
[2025-03-22 04:03:01 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.32906579971313477 norm:0.0013506682589650154 max memory_allocated 29272.96728515625 
[2025-03-22 04:03:49 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3264094591140747 norm:0.0012077187420800328 max memory_allocated 29272.96728515625 
[2025-03-22 04:04:36 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.32474714517593384 norm:0.0011303431820124388 max memory_allocated 29272.96728515625 
[2025-03-22 04:05:24 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.3234894573688507 norm:0.0010776829440146685 max memory_allocated 29272.96728515625 
[2025-03-22 04:06:12 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.3225340247154236 norm:0.0010356273269280791 max memory_allocated 29272.96728515625 
[2025-03-22 04:06:59 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.32175445556640625 norm:0.001001563505269587 max memory_allocated 29272.96728515625 
[2025-03-22 04:07:47 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.3211842477321625 norm:0.0010037358151748776 max memory_allocated 29272.96728515625 
[2025-03-22 04:08:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.3206944763660431 norm:0.0009922303725033998 max memory_allocated 29272.96728515625 
[2025-03-22 04:09:22 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3203427195549011 norm:0.0009523505577817559 max memory_allocated 29272.96728515625 
[2025-03-22 04:10:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3202510476112366 norm:0.000949468114413321 max memory_allocated 29272.96728515625 
[2025-03-22 04:10:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.3199516534805298 norm:0.000900799932423979 max memory_allocated 29272.96728515625 
[2025-03-22 04:11:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.3196779191493988 norm:0.0008903160342015326 max memory_allocated 29272.96728515625 
[2025-03-22 04:12:34 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.31954288482666016 norm:0.0008865125710144639 max memory_allocated 29272.96728515625 
[2025-03-22 04:13:21 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.31928497552871704 norm:0.0008714395808055997 max memory_allocated 29272.96728515625 
[2025-03-22 04:14:09 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.3190714716911316 norm:0.0008742640493437648 max memory_allocated 29272.96728515625 
[2025-03-22 04:14:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 04:15:18 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.43918830156326294 norm:0.02883159928023815 max memory_allocated 29273.15478515625 
[2025-03-22 04:16:06 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.4028191864490509 norm:0.011696099303662777 max memory_allocated 29273.15478515625 
[2025-03-22 04:16:54 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3661864697933197 norm:0.003964724950492382 max memory_allocated 29273.15478515625 
[2025-03-22 04:17:42 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.35389959812164307 norm:0.0019416180439293385 max memory_allocated 29273.15478515625 
[2025-03-22 04:18:30 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.34846240282058716 norm:0.001208147732540965 max memory_allocated 29273.15478515625 
[2025-03-22 04:19:17 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.34498363733291626 norm:0.0010808722581714392 max memory_allocated 29273.15478515625 
[2025-03-22 04:20:05 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.342361718416214 norm:0.0009715347550809383 max memory_allocated 29273.15478515625 
[2025-03-22 04:20:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3406086564064026 norm:0.0009254551841877401 max memory_allocated 29273.15478515625 
[2025-03-22 04:21:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.3392068147659302 norm:0.0008924024878069758 max memory_allocated 29273.15478515625 
[2025-03-22 04:22:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.3382315933704376 norm:0.0008785843383520842 max memory_allocated 29273.15478515625 
[2025-03-22 04:23:15 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.33742737770080566 norm:0.0008517762180417776 max memory_allocated 29273.15478515625 
[2025-03-22 04:24:03 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.3369048833847046 norm:0.0008293410064652562 max memory_allocated 29273.15478515625 
[2025-03-22 04:24:51 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.33650651574134827 norm:0.0008067304152064025 max memory_allocated 29273.15478515625 
[2025-03-22 04:25:38 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.3361094892024994 norm:0.0007943804375827312 max memory_allocated 29273.15478515625 
[2025-03-22 04:26:26 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3359116315841675 norm:0.000792434555478394 max memory_allocated 29273.15478515625 
[2025-03-22 04:27:14 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.33563169836997986 norm:0.0007821773178875446 max memory_allocated 29273.15478515625 
[2025-03-22 04:28:02 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.33543214201927185 norm:0.0007708872435614467 max memory_allocated 29273.15478515625 
[2025-03-22 04:28:50 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.3353223204612732 norm:0.0007734624086879194 max memory_allocated 29273.15478515625 
[2025-03-22 04:29:38 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.33519384264945984 norm:0.000773649022448808 max memory_allocated 29273.15478515625 
[2025-03-22 04:30:25 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.3350498676300049 norm:0.0007661176496185362 max memory_allocated 29273.15478515625 
[2025-03-22 04:30:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 04:31:35 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.4622611403465271 norm:0.01766742765903473 max memory_allocated 29273.34228515625 
[2025-03-22 04:32:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.41156402230262756 norm:0.006687227636575699 max memory_allocated 29273.34228515625 
[2025-03-22 04:33:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.3740428686141968 norm:0.0027014301158487797 max memory_allocated 29273.34228515625 
[2025-03-22 04:33:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.36111360788345337 norm:0.0015862610889598727 max memory_allocated 29273.34228515625 
[2025-03-22 04:34:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.35591182112693787 norm:0.0012535004643723369 max memory_allocated 29273.34228515625 
[2025-03-22 04:35:33 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.3524361848831177 norm:0.0011131783248856664 max memory_allocated 29273.34228515625 
[2025-03-22 04:36:21 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.3500659763813019 norm:0.0010161552345380187 max memory_allocated 29273.34228515625 
[2025-03-22 04:37:08 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.34836822748184204 norm:0.0009523281478323042 max memory_allocated 29273.34228515625 
[2025-03-22 04:37:56 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.3470096290111542 norm:0.0008981884457170963 max memory_allocated 29273.34228515625 
[2025-03-22 04:38:44 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.34605738520622253 norm:0.0008581462898291647 max memory_allocated 29273.34228515625 
[2025-03-22 04:39:31 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.34537410736083984 norm:0.0008302158094011247 max memory_allocated 29273.34228515625 
[2025-03-22 04:40:19 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.3448658585548401 norm:0.0008162726298905909 max memory_allocated 29273.34228515625 
[2025-03-22 04:41:07 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.34433242678642273 norm:0.0007922786753624678 max memory_allocated 29273.34228515625 
[2025-03-22 04:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.3439885377883911 norm:0.0007794840494170785 max memory_allocated 29273.34228515625 
[2025-03-22 04:42:42 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.34361332654953003 norm:0.000765833246987313 max memory_allocated 29273.34228515625 
[2025-03-22 04:43:30 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.34339776635169983 norm:0.0007575784111395478 max memory_allocated 29273.34228515625 
[2025-03-22 04:44:18 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.3431541919708252 norm:0.0007503113010898232 max memory_allocated 29273.34228515625 
[2025-03-22 04:45:06 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.3429712653160095 norm:0.000742583186365664 max memory_allocated 29273.34228515625 
[2025-03-22 04:45:54 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.3428006172180176 norm:0.0007408507517538965 max memory_allocated 29273.34228515625 
[2025-03-22 04:46:42 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.342634916305542 norm:0.0007489651907235384 max memory_allocated 29273.34228515625 
[2025-03-22 04:46:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 04:47:51 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.45711764693260193 norm:0.023063700646162033 max memory_allocated 29273.52978515625 
[2025-03-22 04:48:39 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.4179654121398926 norm:0.010382318869233131 max memory_allocated 29273.52978515625 
[2025-03-22 04:49:26 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.3843168020248413 norm:0.00462305685505271 max memory_allocated 29273.52978515625 
[2025-03-22 04:50:14 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.37109172344207764 norm:0.0018642462091520429 max memory_allocated 29273.52978515625 
[2025-03-22 04:51:02 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.3657718300819397 norm:0.0013222163543105125 max memory_allocated 29273.52978515625 
[2025-03-22 04:51:49 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3622998297214508 norm:0.0011528051691129804 max memory_allocated 29273.52978515625 
[2025-03-22 04:52:37 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3597499430179596 norm:0.0010461938800290227 max memory_allocated 29273.52978515625 
[2025-03-22 04:53:25 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.357914537191391 norm:0.0009754647617228329 max memory_allocated 29273.52978515625 
[2025-03-22 04:54:12 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.3566192388534546 norm:0.000925811764318496 max memory_allocated 29273.52978515625 
[2025-03-22 04:55:00 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.3555779457092285 norm:0.0008840917143970728 max memory_allocated 29273.52978515625 
[2025-03-22 04:55:48 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.35477739572525024 norm:0.0008602245361544192 max memory_allocated 29273.52978515625 
[2025-03-22 04:56:35 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.35410669445991516 norm:0.0008471163455396891 max memory_allocated 29273.52978515625 
[2025-03-22 04:57:23 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.3537435531616211 norm:0.0008358928025700152 max memory_allocated 29273.52978515625 
[2025-03-22 04:58:11 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3533720076084137 norm:0.0008088053436949849 max memory_allocated 29273.52978515625 
[2025-03-22 04:58:59 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.35305556654930115 norm:0.000800950569100678 max memory_allocated 29273.52978515625 
[2025-03-22 04:59:47 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3528905510902405 norm:0.0007904619560576975 max memory_allocated 29273.52978515625 
[2025-03-22 05:00:35 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.35270750522613525 norm:0.0007800769526511431 max memory_allocated 29273.52978515625 
[2025-03-22 05:01:23 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.3525370955467224 norm:0.0007700162823311985 max memory_allocated 29273.52978515625 
[2025-03-22 05:02:10 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.352497398853302 norm:0.0007699765847064555 max memory_allocated 29273.52978515625 
[2025-03-22 05:02:58 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.3523516058921814 norm:0.0007582231773994863 max memory_allocated 29273.52978515625 
[2025-03-22 05:03:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 05:04:08 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.45941585302352905 norm:0.018271222710609436 max memory_allocated 29273.71728515625 
[2025-03-22 05:04:56 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.42188435792922974 norm:0.00937926210463047 max memory_allocated 29273.71728515625 
[2025-03-22 05:05:43 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.3946833610534668 norm:0.0053747836500406265 max memory_allocated 29273.71728515625 
[2025-03-22 05:06:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.38318997621536255 norm:0.0033401690889149904 max memory_allocated 29273.71728515625 
[2025-03-22 05:07:18 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.37776607275009155 norm:0.0023671104572713375 max memory_allocated 29273.71728515625 
[2025-03-22 05:08:06 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3742145895957947 norm:0.0018421889981254935 max memory_allocated 29273.71728515625 
[2025-03-22 05:08:54 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.3717672526836395 norm:0.0015027191257104278 max memory_allocated 29273.71728515625 
[2025-03-22 05:09:41 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.3697644770145416 norm:0.001164249493740499 max memory_allocated 29273.71728515625 
[2025-03-22 05:10:29 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.36840760707855225 norm:0.001074997242540121 max memory_allocated 29273.71728515625 
[2025-03-22 05:11:17 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.3673713207244873 norm:0.0009865057654678822 max memory_allocated 29273.71728515625 
[2025-03-22 05:12:05 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.3665362298488617 norm:0.000955236959271133 max memory_allocated 29273.71728515625 
[2025-03-22 05:12:52 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.36597537994384766 norm:0.0009227347909472883 max memory_allocated 29273.71728515625 
[2025-03-22 05:13:40 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.36554113030433655 norm:0.0008794987224973738 max memory_allocated 29273.71728515625 
[2025-03-22 05:14:28 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.36511555314064026 norm:0.0008456653449684381 max memory_allocated 29273.71728515625 
[2025-03-22 05:15:16 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.36484378576278687 norm:0.0008236036519519985 max memory_allocated 29273.71728515625 
[2025-03-22 05:16:04 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.36460521817207336 norm:0.0007996326312422752 max memory_allocated 29273.71728515625 
[2025-03-22 05:16:52 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.3643542528152466 norm:0.0007826033397577703 max memory_allocated 29273.71728515625 
[2025-03-22 05:17:40 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.3641497790813446 norm:0.000756795285269618 max memory_allocated 29273.71728515625 
[2025-03-22 05:18:27 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.3639809489250183 norm:0.0007448670221492648 max memory_allocated 29273.71728515625 
[2025-03-22 05:19:15 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.36382848024368286 norm:0.0007364074699580669 max memory_allocated 29273.71728515625 
[2025-03-22 05:19:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 05:20:25 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.4537167251110077 norm:0.013659922406077385 max memory_allocated 29273.90478515625 
[2025-03-22 05:21:13 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.42532801628112793 norm:0.007313130889087915 max memory_allocated 29273.90478515625 
[2025-03-22 05:22:00 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.3979488015174866 norm:0.003721692366525531 max memory_allocated 29273.90478515625 
[2025-03-22 05:22:48 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.3861275017261505 norm:0.002099913079291582 max memory_allocated 29273.90478515625 
[2025-03-22 05:23:35 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.3809237778186798 norm:0.0014366521500051022 max memory_allocated 29273.90478515625 
[2025-03-22 05:24:23 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.3775481581687927 norm:0.0010448130778968334 max memory_allocated 29273.90478515625 
[2025-03-22 05:25:10 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.37525132298469543 norm:0.0009615429444238544 max memory_allocated 29273.90478515625 
[2025-03-22 05:25:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.3737356960773468 norm:0.0009209481650032103 max memory_allocated 29273.90478515625 
[2025-03-22 05:26:46 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.37239959836006165 norm:0.000867750495672226 max memory_allocated 29273.90478515625 
[2025-03-22 05:27:33 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.3714352250099182 norm:0.000838750449474901 max memory_allocated 29273.90478515625 
[2025-03-22 05:28:21 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.3706313967704773 norm:0.0007994801853783429 max memory_allocated 29273.90478515625 
[2025-03-22 05:29:09 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.36996278166770935 norm:0.0007819848833605647 max memory_allocated 29273.90478515625 
[2025-03-22 05:29:57 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.369488000869751 norm:0.0007655140361748636 max memory_allocated 29273.90478515625 
[2025-03-22 05:30:45 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.36916500329971313 norm:0.0007581815589219332 max memory_allocated 29273.90478515625 
[2025-03-22 05:31:33 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.36877796053886414 norm:0.0007387215737253428 max memory_allocated 29273.90478515625 
[2025-03-22 05:32:20 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.3684265613555908 norm:0.0007347080390900373 max memory_allocated 29273.90478515625 
[2025-03-22 05:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.36813148856163025 norm:0.000724531477317214 max memory_allocated 29273.90478515625 
[2025-03-22 05:33:56 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.36796995997428894 norm:0.0007251097704283893 max memory_allocated 29273.90478515625 
[2025-03-22 05:34:44 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.36781981587409973 norm:0.000719999021384865 max memory_allocated 29273.90478515625 
[2025-03-22 05:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.36777403950691223 norm:0.0007186351576820016 max memory_allocated 29273.90478515625 
[2025-03-22 05:35:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 05:36:42 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.47027868032455444 norm:0.030291665345430374 max memory_allocated 29274.09228515625 
[2025-03-22 05:37:29 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.44454774260520935 norm:0.015500807203352451 max memory_allocated 29274.09228515625 
[2025-03-22 05:38:17 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.41330868005752563 norm:0.006591074168682098 max memory_allocated 29274.09228515625 
[2025-03-22 05:39:04 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.40062540769577026 norm:0.003724310314282775 max memory_allocated 29274.09228515625 
[2025-03-22 05:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.39537858963012695 norm:0.002393990056589246 max memory_allocated 29274.09228515625 
[2025-03-22 05:40:39 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.39209359884262085 norm:0.00204848381690681 max memory_allocated 29274.09228515625 
[2025-03-22 05:41:27 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.3893754184246063 norm:0.00174542679451406 max memory_allocated 29274.09228515625 
[2025-03-22 05:42:14 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.387132465839386 norm:0.0012787040323019028 max memory_allocated 29274.09228515625 
[2025-03-22 05:43:02 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.3857705891132355 norm:0.0012129392707720399 max memory_allocated 29274.09228515625 
[2025-03-22 05:43:50 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.3846791088581085 norm:0.0011662795441225171 max memory_allocated 29274.09228515625 
[2025-03-22 05:44:38 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.3837811350822449 norm:0.0011099200928583741 max memory_allocated 29274.09228515625 
[2025-03-22 05:45:25 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.38304418325424194 norm:0.0010606535943225026 max memory_allocated 29274.09228515625 
[2025-03-22 05:46:13 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.3824402093887329 norm:0.0010329561773687601 max memory_allocated 29274.09228515625 
[2025-03-22 05:47:01 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.38200393319129944 norm:0.0010024609509855509 max memory_allocated 29274.09228515625 
[2025-03-22 05:47:49 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.3815981447696686 norm:0.000987757695838809 max memory_allocated 29274.09228515625 
[2025-03-22 05:48:37 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.38125669956207275 norm:0.0009577696910127997 max memory_allocated 29274.09228515625 
[2025-03-22 05:49:24 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.3809701204299927 norm:0.0009349211468361318 max memory_allocated 29274.09228515625 
[2025-03-22 05:50:12 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.3807218074798584 norm:0.0009170294506475329 max memory_allocated 29274.09228515625 
[2025-03-22 05:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.38049033284187317 norm:0.0009021286386996508 max memory_allocated 29274.09228515625 
[2025-03-22 05:51:48 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.38021689653396606 norm:0.0008661592728458345 max memory_allocated 29274.09228515625 
[2025-03-22 05:52:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 05:52:57 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.4761170446872711 norm:0.03331756964325905 max memory_allocated 29274.27978515625 
[2025-03-22 05:53:45 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.44897961616516113 norm:0.0162980780005455 max memory_allocated 29274.27978515625 
[2025-03-22 05:54:32 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.4182608723640442 norm:0.0055261095985770226 max memory_allocated 29274.27978515625 
[2025-03-22 05:55:20 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.40751561522483826 norm:0.0029358475003391504 max memory_allocated 29274.27978515625 
[2025-03-22 05:56:07 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.4030114710330963 norm:0.0021769327577203512 max memory_allocated 29274.27978515625 
[2025-03-22 05:56:55 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.40017104148864746 norm:0.0018565107602626085 max memory_allocated 29274.27978515625 
[2025-03-22 05:57:42 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.3981882929801941 norm:0.0016297571128234267 max memory_allocated 29274.27978515625 
[2025-03-22 05:58:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.3963668942451477 norm:0.0014443520922213793 max memory_allocated 29274.27978515625 
[2025-03-22 05:59:18 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.39481592178344727 norm:0.0012786819133907557 max memory_allocated 29274.27978515625 
[2025-03-22 06:00:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.39349862933158875 norm:0.0011847293935716152 max memory_allocated 29274.27978515625 
[2025-03-22 06:00:53 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.3924618363380432 norm:0.0011295187287032604 max memory_allocated 29274.27978515625 
[2025-03-22 06:01:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.39171165227890015 norm:0.0011004810221493244 max memory_allocated 29274.27978515625 
[2025-03-22 06:02:29 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.39107078313827515 norm:0.001048144418746233 max memory_allocated 29274.27978515625 
[2025-03-22 06:03:17 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.39052411913871765 norm:0.0010335547849535942 max memory_allocated 29274.27978515625 
[2025-03-22 06:04:05 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.3900311291217804 norm:0.001002986216917634 max memory_allocated 29274.27978515625 
[2025-03-22 06:04:53 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.38965919613838196 norm:0.0009759785607457161 max memory_allocated 29274.27978515625 
[2025-03-22 06:05:40 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.3893142342567444 norm:0.0009593914728611708 max memory_allocated 29274.27978515625 
[2025-03-22 06:06:28 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.3890814185142517 norm:0.0009439690038561821 max memory_allocated 29274.27978515625 
[2025-03-22 06:07:16 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.38887834548950195 norm:0.0009234275203198195 max memory_allocated 29274.27978515625 
[2025-03-22 06:08:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.3886561393737793 norm:0.0009095642599277198 max memory_allocated 29274.27978515625 
[2025-03-22 06:08:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 06:09:11 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.475045382976532 norm:0.031111467629671097 max memory_allocated 29274.46728515625 
[2025-03-22 06:09:59 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.4478417932987213 norm:0.013820545747876167 max memory_allocated 29274.46728515625 
[2025-03-22 06:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.42660707235336304 norm:0.006662632804363966 max memory_allocated 29274.46728515625 
[2025-03-22 06:11:34 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.4160238802433014 norm:0.0031121326610445976 max memory_allocated 29274.46728515625 
[2025-03-22 06:12:21 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.41165903210639954 norm:0.0019778599962592125 max memory_allocated 29274.46728515625 
[2025-03-22 06:13:09 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.4093571901321411 norm:0.0017545797163620591 max memory_allocated 29274.46728515625 
[2025-03-22 06:13:57 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.40767571330070496 norm:0.0016139675863087177 max memory_allocated 29274.46728515625 
[2025-03-22 06:14:45 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.4061894118785858 norm:0.0014690884854644537 max memory_allocated 29274.46728515625 
[2025-03-22 06:15:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.40497317910194397 norm:0.0013619266683235765 max memory_allocated 29274.46728515625 
[2025-03-22 06:16:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.4039425253868103 norm:0.0012526498176157475 max memory_allocated 29274.46728515625 
[2025-03-22 06:17:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.4031522870063782 norm:0.0012143288040533662 max memory_allocated 29274.46728515625 
[2025-03-22 06:17:56 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.402450829744339 norm:0.0011355188908055425 max memory_allocated 29274.46728515625 
[2025-03-22 06:18:44 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.40185225009918213 norm:0.0010638706153258681 max memory_allocated 29274.46728515625 
[2025-03-22 06:19:32 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.4013403654098511 norm:0.0010381729807704687 max memory_allocated 29274.46728515625 
[2025-03-22 06:20:20 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.40073636174201965 norm:0.0009640967473387718 max memory_allocated 29274.46728515625 
[2025-03-22 06:21:07 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.4003545045852661 norm:0.0009612854337319732 max memory_allocated 29274.46728515625 
[2025-03-22 06:21:55 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.4000312089920044 norm:0.0009283644612878561 max memory_allocated 29274.46728515625 
[2025-03-22 06:22:43 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.3996909260749817 norm:0.0009068948565982282 max memory_allocated 29274.46728515625 
[2025-03-22 06:23:30 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.39948099851608276 norm:0.0008912190678529441 max memory_allocated 29274.46728515625 
[2025-03-22 06:24:18 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.3992099165916443 norm:0.0008692044066265225 max memory_allocated 29274.46728515625 
[2025-03-22 06:24:31 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 06:25:28 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.48512598872184753 norm:0.021538222208619118 max memory_allocated 29274.65478515625 
[2025-03-22 06:26:16 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.4637528359889984 norm:0.010078896768391132 max memory_allocated 29274.65478515625 
[2025-03-22 06:27:03 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.44427305459976196 norm:0.005146409850567579 max memory_allocated 29274.65478515625 
[2025-03-22 06:27:51 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.43441665172576904 norm:0.0025509458500891924 max memory_allocated 29274.65478515625 
[2025-03-22 06:28:39 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.43048691749572754 norm:0.0018132389523088932 max memory_allocated 29274.65478515625 
[2025-03-22 06:29:26 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.428189218044281 norm:0.0016019379254430532 max memory_allocated 29274.65478515625 
[2025-03-22 06:30:14 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.42650750279426575 norm:0.0014592790976166725 max memory_allocated 29274.65478515625 
[2025-03-22 06:31:02 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.42520108819007874 norm:0.001388678909279406 max memory_allocated 29274.65478515625 
[2025-03-22 06:31:50 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.4239754378795624 norm:0.0012868053745478392 max memory_allocated 29274.65478515625 
[2025-03-22 06:32:38 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.4229107201099396 norm:0.0011827375274151564 max memory_allocated 29274.65478515625 
[2025-03-22 06:33:26 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.4219907224178314 norm:0.0010977244237437844 max memory_allocated 29274.65478515625 
[2025-03-22 06:34:14 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.42131486535072327 norm:0.0010382652981206775 max memory_allocated 29274.65478515625 
[2025-03-22 06:35:01 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.4206511378288269 norm:0.0009854045929387212 max memory_allocated 29274.65478515625 
[2025-03-22 06:35:49 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.4200827479362488 norm:0.0009648241684772074 max memory_allocated 29274.65478515625 
[2025-03-22 06:36:37 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.4197176992893219 norm:0.0009509104420430958 max memory_allocated 29274.65478515625 
[2025-03-22 06:37:25 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.41931700706481934 norm:0.0009107741061598063 max memory_allocated 29274.65478515625 
[2025-03-22 06:38:12 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.41897648572921753 norm:0.0008873263723216951 max memory_allocated 29274.65478515625 
[2025-03-22 06:39:00 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.41876021027565 norm:0.0008886465802788734 max memory_allocated 29274.65478515625 
[2025-03-22 06:39:48 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.4185982942581177 norm:0.000885823683347553 max memory_allocated 29274.65478515625 
[2025-03-22 06:40:35 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.41840696334838867 norm:0.0008833587053231895 max memory_allocated 29274.65478515625 
[2025-03-22 06:40:49 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 06:41:44 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.5035260319709778 norm:0.021536685526371002 max memory_allocated 29274.84228515625 
[2025-03-22 06:42:31 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.4831486642360687 norm:0.010190906003117561 max memory_allocated 29274.84228515625 
[2025-03-22 06:43:19 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.4660685360431671 norm:0.005359724164009094 max memory_allocated 29274.84228515625 
[2025-03-22 06:44:07 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.4561026692390442 norm:0.002531551057472825 max memory_allocated 29274.84228515625 
[2025-03-22 06:44:54 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.4523880183696747 norm:0.0018102485919371247 max memory_allocated 29274.84228515625 
[2025-03-22 06:45:42 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.45038875937461853 norm:0.001599085284397006 max memory_allocated 29274.84228515625 
[2025-03-22 06:46:30 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.44870394468307495 norm:0.0014075826620683074 max memory_allocated 29274.84228515625 
[2025-03-22 06:47:18 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.44717490673065186 norm:0.0012260668445378542 max memory_allocated 29274.84228515625 
[2025-03-22 06:48:06 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.44599416851997375 norm:0.0011661750031635165 max memory_allocated 29274.84228515625 
[2025-03-22 06:48:54 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.44496476650238037 norm:0.0010731946676969528 max memory_allocated 29274.84228515625 
[2025-03-22 06:49:41 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.44424015283584595 norm:0.0010554763721302152 max memory_allocated 29274.84228515625 
[2025-03-22 06:50:29 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.4435955286026001 norm:0.0009976874571293592 max memory_allocated 29274.84228515625 
[2025-03-22 06:51:17 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.4430612623691559 norm:0.0009743327973410487 max memory_allocated 29274.84228515625 
[2025-03-22 06:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.44257763028144836 norm:0.0009443795424886048 max memory_allocated 29274.84228515625 
[2025-03-22 06:52:53 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.44215041399002075 norm:0.0009355631191283464 max memory_allocated 29274.84228515625 
[2025-03-22 06:53:40 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.441733717918396 norm:0.0008947485475800931 max memory_allocated 29274.84228515625 
[2025-03-22 06:54:28 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.4413570463657379 norm:0.0008702202467247844 max memory_allocated 29274.84228515625 
[2025-03-22 06:55:16 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.44111523032188416 norm:0.0008572147926315665 max memory_allocated 29274.84228515625 
[2025-03-22 06:56:03 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.440864622592926 norm:0.0008290319237858057 max memory_allocated 29274.84228515625 
[2025-03-22 06:56:51 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.440610408782959 norm:0.000809134857263416 max memory_allocated 29274.84228515625 
[2025-03-22 06:57:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 06:58:03 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.5419735312461853 norm:0.02096523903310299 max memory_allocated 29275.02978515625 
[2025-03-22 06:58:50 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.5220234394073486 norm:0.010317099280655384 max memory_allocated 29275.02978515625 
[2025-03-22 06:59:38 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.5039172172546387 norm:0.0056534986943006516 max memory_allocated 29275.02978515625 
[2025-03-22 07:00:26 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.49362245202064514 norm:0.003111091209575534 max memory_allocated 29275.02978515625 
[2025-03-22 07:01:14 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.4891502559185028 norm:0.0020452181342989206 max memory_allocated 29275.02978515625 
[2025-03-22 07:02:01 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.4868405759334564 norm:0.0017434069886803627 max memory_allocated 29275.02978515625 
[2025-03-22 07:02:49 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.48508256673812866 norm:0.001566730672493577 max memory_allocated 29275.02978515625 
[2025-03-22 07:03:37 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.4836440682411194 norm:0.0014613483799621463 max memory_allocated 29275.02978515625 
[2025-03-22 07:04:25 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.48242852091789246 norm:0.0013738698326051235 max memory_allocated 29275.02978515625 
[2025-03-22 07:05:13 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.48157820105552673 norm:0.0013331100344657898 max memory_allocated 29275.02978515625 
[2025-03-22 07:06:01 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.4808092713356018 norm:0.0012779843527823687 max memory_allocated 29275.02978515625 
[2025-03-22 07:06:48 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.480209082365036 norm:0.0012233416782692075 max memory_allocated 29275.02978515625 
[2025-03-22 07:07:36 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.4795885384082794 norm:0.0011761833447963 max memory_allocated 29275.02978515625 
[2025-03-22 07:08:24 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.479145884513855 norm:0.0011361812939867377 max memory_allocated 29275.02978515625 
[2025-03-22 07:09:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.47870033979415894 norm:0.0011147251352667809 max memory_allocated 29275.02978515625 
[2025-03-22 07:09:59 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.4783063232898712 norm:0.0010762304300442338 max memory_allocated 29275.02978515625 
[2025-03-22 07:10:47 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.47796282172203064 norm:0.0010516033507883549 max memory_allocated 29275.02978515625 
[2025-03-22 07:11:34 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.47756296396255493 norm:0.001026043202728033 max memory_allocated 29275.02978515625 
[2025-03-22 07:12:22 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.47726744413375854 norm:0.0010061556240543723 max memory_allocated 29275.02978515625 
[2025-03-22 07:13:09 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.476936936378479 norm:0.0009766578441485763 max memory_allocated 29275.02978515625 
[2025-03-22 07:13:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 07:14:18 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:0.5583286881446838 norm:0.01083378866314888 max memory_allocated 29275.21728515625 
[2025-03-22 07:15:06 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:0.5459774136543274 norm:0.006627427879720926 max memory_allocated 29275.21728515625 
[2025-03-22 07:15:54 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:0.5329192876815796 norm:0.004027282819151878 max memory_allocated 29275.21728515625 
[2025-03-22 07:16:41 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:0.5261440277099609 norm:0.0022795347031205893 max memory_allocated 29275.21728515625 
[2025-03-22 07:17:29 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:0.5231319069862366 norm:0.0018637535395100713 max memory_allocated 29275.21728515625 
[2025-03-22 07:18:17 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:0.5210574865341187 norm:0.0013106472324579954 max memory_allocated 29275.21728515625 
[2025-03-22 07:19:05 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:0.5196709632873535 norm:0.0012017613044008613 max memory_allocated 29275.21728515625 
[2025-03-22 07:19:53 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:0.5185348987579346 norm:0.0011414093896746635 max memory_allocated 29275.21728515625 
[2025-03-22 07:20:41 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:0.5174840688705444 norm:0.001076792017556727 max memory_allocated 29275.21728515625 
[2025-03-22 07:21:29 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:0.5164764523506165 norm:0.0010087199043482542 max memory_allocated 29275.21728515625 
[2025-03-22 07:22:17 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.5157230496406555 norm:0.0009764605201780796 max memory_allocated 29275.21728515625 
[2025-03-22 07:23:05 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.5150697231292725 norm:0.0009593620779924095 max memory_allocated 29275.21728515625 
[2025-03-22 07:23:53 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.5145097970962524 norm:0.0009356955415569246 max memory_allocated 29275.21728515625 
[2025-03-22 07:24:40 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.5140975117683411 norm:0.0009175532031804323 max memory_allocated 29275.21728515625 
[2025-03-22 07:25:28 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.5136641263961792 norm:0.000888458511326462 max memory_allocated 29275.21728515625 
[2025-03-22 07:26:16 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.5133735537528992 norm:0.0008754411828704178 max memory_allocated 29275.21728515625 
[2025-03-22 07:27:04 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.5130695104598999 norm:0.0008640621090307832 max memory_allocated 29275.21728515625 
[2025-03-22 07:27:51 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.5127825736999512 norm:0.0008498040842823684 max memory_allocated 29275.21728515625 
[2025-03-22 07:28:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.5125980973243713 norm:0.0008521795971319079 max memory_allocated 29275.21728515625 
[2025-03-22 07:29:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.51241135597229 norm:0.0008487326558679342 max memory_allocated 29275.21728515625 
[2025-03-22 07:29:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 07:30:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:0.6031282544136047 norm:0.02415827289223671 max memory_allocated 29275.40478515625 
[2025-03-22 07:31:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:0.5910474061965942 norm:0.013969660736620426 max memory_allocated 29275.40478515625 
[2025-03-22 07:32:14 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:0.5787693858146667 norm:0.008463945239782333 max memory_allocated 29275.40478515625 
[2025-03-22 07:33:02 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:0.5730085968971252 norm:0.005636834539473057 max memory_allocated 29275.40478515625 
[2025-03-22 07:33:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:0.5698450207710266 norm:0.004135227296501398 max memory_allocated 29275.40478515625 
[2025-03-22 07:34:37 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:0.567794680595398 norm:0.0034117044415324926 max memory_allocated 29275.40478515625 
[2025-03-22 07:35:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:0.5658661127090454 norm:0.0028575854375958443 max memory_allocated 29275.40478515625 
[2025-03-22 07:36:13 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:0.5643937587738037 norm:0.0025092666037380695 max memory_allocated 29275.40478515625 
[2025-03-22 07:37:02 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:0.5631088018417358 norm:0.0022072438150644302 max memory_allocated 29275.40478515625 
[2025-03-22 07:37:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:0.5620241165161133 norm:0.0019375239498913288 max memory_allocated 29275.40478515625 
[2025-03-22 07:38:37 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:0.5612298846244812 norm:0.0017687202198430896 max memory_allocated 29275.40478515625 
[2025-03-22 07:39:25 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:0.5604902505874634 norm:0.001628768048249185 max memory_allocated 29275.40478515625 
[2025-03-22 07:40:13 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:0.5599130988121033 norm:0.001509950845502317 max memory_allocated 29275.40478515625 
[2025-03-22 07:41:01 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:0.5594037771224976 norm:0.0014170248759910464 max memory_allocated 29275.40478515625 
[2025-03-22 07:41:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:0.5589957237243652 norm:0.0013432627310976386 max memory_allocated 29275.40478515625 
[2025-03-22 07:42:36 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:0.5585883259773254 norm:0.00126114790327847 max memory_allocated 29275.40478515625 
[2025-03-22 07:43:24 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:0.5582313537597656 norm:0.001180911436676979 max memory_allocated 29275.40478515625 
[2025-03-22 07:44:12 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:0.557907223701477 norm:0.0011198422871530056 max memory_allocated 29275.40478515625 
[2025-03-22 07:45:00 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:0.5576601624488831 norm:0.0010679226834326982 max memory_allocated 29275.40478515625 
[2025-03-22 07:45:47 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:0.557420551776886 norm:0.0010267532197758555 max memory_allocated 29275.40478515625 
[2025-03-22 07:46:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 07:46:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:0.6405813694000244 norm:0.008401556871831417 max memory_allocated 29275.59228515625 
[2025-03-22 07:47:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:0.6324273943901062 norm:0.006051076576113701 max memory_allocated 29275.59228515625 
[2025-03-22 07:48:33 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:0.6222683787345886 norm:0.004042311105877161 max memory_allocated 29275.59228515625 
[2025-03-22 07:49:21 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:0.6175681352615356 norm:0.0025996710173785686 max memory_allocated 29275.59228515625 
[2025-03-22 07:50:09 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:0.614656388759613 norm:0.001878880662843585 max memory_allocated 29275.59228515625 
[2025-03-22 07:50:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:0.6128345131874084 norm:0.0016249609179794788 max memory_allocated 29275.59228515625 
[2025-03-22 07:51:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:0.6113768815994263 norm:0.0015331521863117814 max memory_allocated 29275.59228515625 
[2025-03-22 07:52:33 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:0.610150933265686 norm:0.0014279799070209265 max memory_allocated 29275.59228515625 
[2025-03-22 07:53:21 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:0.6088966727256775 norm:0.0012821969576179981 max memory_allocated 29275.59228515625 
[2025-03-22 07:54:09 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:0.6079314351081848 norm:0.0012057707644999027 max memory_allocated 29275.59228515625 
[2025-03-22 07:54:57 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:0.6071125864982605 norm:0.001139504020102322 max memory_allocated 29275.59228515625 
[2025-03-22 07:55:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:0.6063631772994995 norm:0.0010789823718369007 max memory_allocated 29275.59228515625 
[2025-03-22 07:56:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:0.6059505939483643 norm:0.0010778727009892464 max memory_allocated 29275.59228515625 
[2025-03-22 07:57:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:0.6055818200111389 norm:0.001023042481392622 max memory_allocated 29275.59228515625 
[2025-03-22 07:58:08 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:0.6052296161651611 norm:0.000992174493148923 max memory_allocated 29275.59228515625 
[2025-03-22 07:58:56 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:0.6048890948295593 norm:0.0009755141800269485 max memory_allocated 29275.59228515625 
[2025-03-22 07:59:44 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:0.6047110557556152 norm:0.0009808713803067803 max memory_allocated 29275.59228515625 
[2025-03-22 08:00:31 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:0.6044738292694092 norm:0.0009605264058336616 max memory_allocated 29275.59228515625 
[2025-03-22 08:01:19 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:0.6042631268501282 norm:0.0009343335404992104 max memory_allocated 29275.59228515625 
[2025-03-22 08:02:07 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:0.6040856242179871 norm:0.0009005086030811071 max memory_allocated 29275.59228515625 
[2025-03-22 08:02:20 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 08:03:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:0.6930991411209106 norm:0.006064967717975378 max memory_allocated 29275.77978515625 
[2025-03-22 08:04:06 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:0.6844779849052429 norm:0.00401797890663147 max memory_allocated 29275.77978515625 
[2025-03-22 08:04:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:0.6741470098495483 norm:0.0025206496939063072 max memory_allocated 29275.77978515625 
[2025-03-22 08:05:42 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:0.670147716999054 norm:0.001713283360004425 max memory_allocated 29275.77978515625 
[2025-03-22 08:06:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:0.6678457260131836 norm:0.001204691594466567 max memory_allocated 29275.77978515625 
[2025-03-22 08:07:17 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:0.6663541793823242 norm:0.001076976885087788 max memory_allocated 29275.77978515625 
[2025-03-22 08:08:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:0.6650168299674988 norm:0.0009912739042192698 max memory_allocated 29275.77978515625 
[2025-03-22 08:08:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:0.6637873649597168 norm:0.0009305794956162572 max memory_allocated 29275.77978515625 
[2025-03-22 08:09:42 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:0.6627854108810425 norm:0.0008893171907402575 max memory_allocated 29275.77978515625 
[2025-03-22 08:10:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:0.6619787216186523 norm:0.000858229526784271 max memory_allocated 29275.77978515625 
[2025-03-22 08:11:17 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:0.6613554358482361 norm:0.0008265337673947215 max memory_allocated 29275.77978515625 
[2025-03-22 08:12:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:0.6608327031135559 norm:0.0008093904471024871 max memory_allocated 29275.77978515625 
[2025-03-22 08:12:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:0.6604236364364624 norm:0.0007880787597969174 max memory_allocated 29275.77978515625 
[2025-03-22 08:13:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:0.660082995891571 norm:0.0007711414946243167 max memory_allocated 29275.77978515625 
[2025-03-22 08:14:28 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:0.6598114967346191 norm:0.0007427232339978218 max memory_allocated 29275.77978515625 
[2025-03-22 08:15:16 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:0.6595612168312073 norm:0.0007376044522970915 max memory_allocated 29275.77978515625 
[2025-03-22 08:16:04 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:0.6593540906906128 norm:0.0007191105978563428 max memory_allocated 29275.77978515625 
[2025-03-22 08:16:51 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:0.6591545939445496 norm:0.0007069194689393044 max memory_allocated 29275.77978515625 
[2025-03-22 08:17:39 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:0.6589653491973877 norm:0.0006957833538763225 max memory_allocated 29275.77978515625 
[2025-03-22 08:18:27 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:0.6588181853294373 norm:0.0006858731503598392 max memory_allocated 29275.77978515625 
[2025-03-22 08:18:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 08:19:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:0.7631129622459412 norm:0.012296486645936966 max memory_allocated 29275.96728515625 
[2025-03-22 08:20:25 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:0.7533518075942993 norm:0.007945233955979347 max memory_allocated 29275.96728515625 
[2025-03-22 08:21:13 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:0.7414274215698242 norm:0.005378575064241886 max memory_allocated 29275.96728515625 
[2025-03-22 08:22:01 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:0.7365022301673889 norm:0.0038457775954157114 max memory_allocated 29275.96728515625 
[2025-03-22 08:22:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:0.7338709831237793 norm:0.0029234755784273148 max memory_allocated 29275.96728515625 
[2025-03-22 08:23:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:0.7313026189804077 norm:0.001926823752000928 max memory_allocated 29275.96728515625 
[2025-03-22 08:24:25 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:0.7294111251831055 norm:0.001332371961325407 max memory_allocated 29275.96728515625 
[2025-03-22 08:25:13 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:0.7280164957046509 norm:0.0013131581945344806 max memory_allocated 29275.96728515625 
[2025-03-22 08:26:01 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:0.7267776727676392 norm:0.001260400633327663 max memory_allocated 29275.96728515625 
[2025-03-22 08:26:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:0.7258756160736084 norm:0.0012222235091030598 max memory_allocated 29275.96728515625 
[2025-03-22 08:27:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:0.7251375317573547 norm:0.0011724780779331923 max memory_allocated 29275.96728515625 
[2025-03-22 08:28:24 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:0.7245066165924072 norm:0.0011587913613766432 max memory_allocated 29275.96728515625 
[2025-03-22 08:29:12 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:0.7240390777587891 norm:0.0011113419895991683 max memory_allocated 29275.96728515625 
[2025-03-22 08:30:00 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:0.7236974239349365 norm:0.0010727475164458156 max memory_allocated 29275.96728515625 
[2025-03-22 08:30:48 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:0.7234762907028198 norm:0.0010572109604254365 max memory_allocated 29275.96728515625 
[2025-03-22 08:31:35 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:0.7231976985931396 norm:0.0010471523273736238 max memory_allocated 29275.96728515625 
[2025-03-22 08:32:23 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:0.7229453921318054 norm:0.0010430305264890194 max memory_allocated 29275.96728515625 
[2025-03-22 08:33:11 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:0.7227084636688232 norm:0.001022706157527864 max memory_allocated 29275.96728515625 
[2025-03-22 08:33:59 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:0.7225120067596436 norm:0.0010205815779045224 max memory_allocated 29275.96728515625 
[2025-03-22 08:34:47 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:0.7223132252693176 norm:0.0010187767911702394 max memory_allocated 29275.96728515625 
[2025-03-22 08:35:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 08:35:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:0.816136360168457 norm:0.004517761990427971 max memory_allocated 29276.15478515625 
[2025-03-22 08:36:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:0.8077126145362854 norm:0.0022086857352405787 max memory_allocated 29276.15478515625 
[2025-03-22 08:37:35 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:0.7976167798042297 norm:0.0013059305492788553 max memory_allocated 29276.15478515625 
[2025-03-22 08:38:23 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:0.7941573262214661 norm:0.0009719567024149001 max memory_allocated 29276.15478515625 
[2025-03-22 08:39:11 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:0.792134702205658 norm:0.0007568174041807652 max memory_allocated 29276.15478515625 
[2025-03-22 08:39:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:0.7904607057571411 norm:0.0006936079589650035 max memory_allocated 29276.15478515625 
[2025-03-22 08:40:48 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:0.7889662981033325 norm:0.0006731729954481125 max memory_allocated 29276.15478515625 
[2025-03-22 08:41:36 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:0.787564754486084 norm:0.0006518631125800312 max memory_allocated 29276.15478515625 
[2025-03-22 08:42:24 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:0.7864294052124023 norm:0.000639702077023685 max memory_allocated 29276.15478515625 
[2025-03-22 08:43:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:0.7855345010757446 norm:0.0006351730553433299 max memory_allocated 29276.15478515625 
[2025-03-22 08:44:00 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:0.7849182486534119 norm:0.000633704592473805 max memory_allocated 29276.15478515625 
[2025-03-22 08:44:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:0.7843426465988159 norm:0.0006297420477494597 max memory_allocated 29276.15478515625 
[2025-03-22 08:45:35 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:0.7838588953018188 norm:0.0006281749228946865 max memory_allocated 29276.15478515625 
[2025-03-22 08:46:23 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:0.7834628224372864 norm:0.0006239852518774569 max memory_allocated 29276.15478515625 
[2025-03-22 08:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:0.7830963730812073 norm:0.0006245888653211296 max memory_allocated 29276.15478515625 
[2025-03-22 08:47:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:0.7827896475791931 norm:0.0006237182533368468 max memory_allocated 29276.15478515625 
[2025-03-22 08:48:46 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:0.782585084438324 norm:0.0006243328680284321 max memory_allocated 29276.15478515625 
[2025-03-22 08:49:34 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:0.7823893427848816 norm:0.0006295227212831378 max memory_allocated 29276.15478515625 
[2025-03-22 08:50:22 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:0.782223105430603 norm:0.0006283047259785235 max memory_allocated 29276.15478515625 
[2025-03-22 08:51:10 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:0.7820835709571838 norm:0.0006283425027504563 max memory_allocated 29276.15478515625 
[2025-03-22 08:51:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 08:52:20 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:0.8973988890647888 norm:0.013365738093852997 max memory_allocated 29276.34228515625 
[2025-03-22 08:53:08 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:0.8874605894088745 norm:0.008326544426381588 max memory_allocated 29276.34228515625 
[2025-03-22 08:53:56 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:0.876973569393158 norm:0.005709669087082148 max memory_allocated 29276.34228515625 
[2025-03-22 08:54:44 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:0.8721655011177063 norm:0.0040473295375704765 max memory_allocated 29276.34228515625 
[2025-03-22 08:55:32 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:0.869208574295044 norm:0.0028929763939231634 max memory_allocated 29276.34228515625 
[2025-03-22 08:56:20 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:0.866802453994751 norm:0.00253039738163352 max memory_allocated 29276.34228515625 
[2025-03-22 08:57:08 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:0.8645955920219421 norm:0.002072277944535017 max memory_allocated 29276.34228515625 
[2025-03-22 08:57:56 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:0.8624328970909119 norm:0.0015399791300296783 max memory_allocated 29276.34228515625 
[2025-03-22 08:58:44 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:0.8609553575515747 norm:0.0014530805638059974 max memory_allocated 29276.34228515625 
[2025-03-22 08:59:32 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:0.8597781658172607 norm:0.0014391735894605517 max memory_allocated 29276.34228515625 
[2025-03-22 09:00:19 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:0.8587825298309326 norm:0.0013940691715106368 max memory_allocated 29276.34228515625 
[2025-03-22 09:01:07 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:0.8584056496620178 norm:0.001251825480721891 max memory_allocated 29276.34228515625 
[2025-03-22 09:01:55 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:0.8574182987213135 norm:0.0013681051786988974 max memory_allocated 29276.34228515625 
[2025-03-22 09:02:43 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:0.8567033410072327 norm:0.001083274488337338 max memory_allocated 29276.34228515625 
[2025-03-22 09:03:30 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:0.8572290539741516 norm:0.0009665938559919596 max memory_allocated 29276.34228515625 
[2025-03-22 09:04:18 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:0.8569116592407227 norm:0.0010330748045817018 max memory_allocated 29276.34228515625 
[2025-03-22 09:05:06 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:0.856621265411377 norm:0.001011746353469789 max memory_allocated 29276.34228515625 
[2025-03-22 09:05:53 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:0.8563019633293152 norm:0.0009232194861397147 max memory_allocated 29276.34228515625 
[2025-03-22 09:06:41 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:0.8559091091156006 norm:0.0009414507076144218 max memory_allocated 29276.34228515625 
[2025-03-22 09:07:29 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:0.855903148651123 norm:0.0008943111170083284 max memory_allocated 29276.34228515625 
[2025-03-22 09:07:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 09:08:38 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:0.9674752354621887 norm:0.0072621749714016914 max memory_allocated 29276.52978515625 
[2025-03-22 09:09:26 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:0.9585055112838745 norm:0.003794042393565178 max memory_allocated 29276.52978515625 
[2025-03-22 09:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:0.9471573829650879 norm:0.0020503548439592123 max memory_allocated 29276.52978515625 
[2025-03-22 09:11:01 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:0.9428954124450684 norm:0.0012802447890862823 max memory_allocated 29276.52978515625 
[2025-03-22 09:11:49 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:0.9405311942100525 norm:0.0009008122724480927 max memory_allocated 29276.52978515625 
[2025-03-22 09:12:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:0.9386852979660034 norm:0.0007987108547240496 max memory_allocated 29276.52978515625 
[2025-03-22 09:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:0.9369974136352539 norm:0.0007591934408992529 max memory_allocated 29276.52978515625 
[2025-03-22 09:14:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:0.9354720711708069 norm:0.0007229954935610294 max memory_allocated 29276.52978515625 
[2025-03-22 09:15:01 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:0.9342779517173767 norm:0.0007056864560581744 max memory_allocated 29276.52978515625 
[2025-03-22 09:15:48 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:0.933365523815155 norm:0.0006903526373207569 max memory_allocated 29276.52978515625 
[2025-03-22 09:16:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:0.9326701760292053 norm:0.0006738200900144875 max memory_allocated 29276.52978515625 
[2025-03-22 09:17:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:0.9321175217628479 norm:0.0006626405520364642 max memory_allocated 29276.52978515625 
[2025-03-22 09:18:11 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:0.931667685508728 norm:0.0006529525853693485 max memory_allocated 29276.52978515625 
[2025-03-22 09:18:59 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:0.9313161373138428 norm:0.0006521631730720401 max memory_allocated 29276.52978515625 
[2025-03-22 09:19:47 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:0.9310397505760193 norm:0.0006462446763180196 max memory_allocated 29276.52978515625 
[2025-03-22 09:20:34 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:0.9307992458343506 norm:0.0006422809674404562 max memory_allocated 29276.52978515625 
[2025-03-22 09:21:22 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:0.9306062459945679 norm:0.0006386929308064282 max memory_allocated 29276.52978515625 
[2025-03-22 09:22:10 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:0.9304109215736389 norm:0.0006363552529364824 max memory_allocated 29276.52978515625 
[2025-03-22 09:22:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:0.9302207231521606 norm:0.0006321555119939148 max memory_allocated 29276.52978515625 
[2025-03-22 09:23:45 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:0.9300444722175598 norm:0.000627372763119638 max memory_allocated 29276.52978515625 
[2025-03-22 09:23:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 09:24:53 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:1.0569132566452026 norm:0.011031385511159897 max memory_allocated 29276.71728515625 
[2025-03-22 09:25:41 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:1.0447896718978882 norm:0.006283095106482506 max memory_allocated 29276.71728515625 
[2025-03-22 09:26:29 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:1.030615210533142 norm:0.0038798258174210787 max memory_allocated 29276.71728515625 
[2025-03-22 09:27:17 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:1.0248032808303833 norm:0.002417478244751692 max memory_allocated 29276.71728515625 
[2025-03-22 09:28:05 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:1.0219217538833618 norm:0.001123150810599327 max memory_allocated 29276.71728515625 
[2025-03-22 09:28:52 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:1.019744634628296 norm:0.0009586194646544755 max memory_allocated 29276.71728515625 
[2025-03-22 09:29:40 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:1.0176209211349487 norm:0.0008590876823291183 max memory_allocated 29276.71728515625 
[2025-03-22 09:30:28 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:1.0157780647277832 norm:0.0008034111233428121 max memory_allocated 29276.71728515625 
[2025-03-22 09:31:16 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:1.0144455432891846 norm:0.0007765836780890822 max memory_allocated 29276.71728515625 
[2025-03-22 09:32:04 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:1.0134433507919312 norm:0.0007569858571514487 max memory_allocated 29276.71728515625 
[2025-03-22 09:32:51 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:1.012719988822937 norm:0.0007409359095618129 max memory_allocated 29276.71728515625 
[2025-03-22 09:33:39 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:1.012129783630371 norm:0.0007266828906722367 max memory_allocated 29276.71728515625 
[2025-03-22 09:34:27 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:1.011636734008789 norm:0.0007172313635237515 max memory_allocated 29276.71728515625 
[2025-03-22 09:35:14 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:1.0112438201904297 norm:0.0007107719429768622 max memory_allocated 29276.71728515625 
[2025-03-22 09:36:02 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:1.010930061340332 norm:0.000705066486261785 max memory_allocated 29276.71728515625 
[2025-03-22 09:36:49 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:1.0107002258300781 norm:0.0007034711888991296 max memory_allocated 29276.71728515625 
[2025-03-22 09:37:37 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:1.0104694366455078 norm:0.0006946725188754499 max memory_allocated 29276.71728515625 
[2025-03-22 09:38:25 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:1.0102250576019287 norm:0.0006917247665114701 max memory_allocated 29276.71728515625 
[2025-03-22 09:39:12 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:1.009958267211914 norm:0.0006858373526483774 max memory_allocated 29276.71728515625 
[2025-03-22 09:40:00 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:1.009769320487976 norm:0.0006794261280447245 max memory_allocated 29276.71728515625 
[2025-03-22 09:40:14 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 09:41:09 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:1.141950249671936 norm:0.01048949919641018 max memory_allocated 29276.90478515625 
[2025-03-22 09:41:57 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:1.1315377950668335 norm:0.0062475488521158695 max memory_allocated 29276.90478515625 
[2025-03-22 09:42:45 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:1.1170856952667236 norm:0.002897530095651746 max memory_allocated 29276.90478515625 
[2025-03-22 09:43:33 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:1.110589623451233 norm:0.0021323461551219225 max memory_allocated 29276.90478515625 
[2025-03-22 09:44:21 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:1.1067975759506226 norm:0.0009770424803718925 max memory_allocated 29276.90478515625 
[2025-03-22 09:45:08 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:1.1041702032089233 norm:0.0007671277853660285 max memory_allocated 29276.90478515625 
[2025-03-22 09:45:56 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:1.1016569137573242 norm:0.0006967998924665153 max memory_allocated 29276.90478515625 
[2025-03-22 09:46:44 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:1.0995129346847534 norm:0.0006678423960693181 max memory_allocated 29276.90478515625 
[2025-03-22 09:47:32 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:1.0980327129364014 norm:0.0006541627226397395 max memory_allocated 29276.90478515625 
[2025-03-22 09:48:19 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:1.0970377922058105 norm:0.0006458330317400396 max memory_allocated 29276.90478515625 
[2025-03-22 09:49:07 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:1.0962754487991333 norm:0.000641498772893101 max memory_allocated 29276.90478515625 
[2025-03-22 09:49:55 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:1.0957287549972534 norm:0.0006310242461040616 max memory_allocated 29276.90478515625 
[2025-03-22 09:50:42 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:1.0953168869018555 norm:0.000625370186753571 max memory_allocated 29276.90478515625 
[2025-03-22 09:51:30 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:1.0949592590332031 norm:0.0006253774045035243 max memory_allocated 29276.90478515625 
[2025-03-22 09:52:18 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:1.0946248769760132 norm:0.0006195804453454912 max memory_allocated 29276.90478515625 
[2025-03-22 09:53:05 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:1.094340205192566 norm:0.0006180893979035318 max memory_allocated 29276.90478515625 
[2025-03-22 09:53:53 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:1.094122290611267 norm:0.0006173162255436182 max memory_allocated 29276.90478515625 
[2025-03-22 09:54:41 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:1.0939172506332397 norm:0.0006196927279233932 max memory_allocated 29276.90478515625 
[2025-03-22 09:55:28 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:1.0937271118164062 norm:0.0006304220296442509 max memory_allocated 29276.90478515625 
[2025-03-22 09:56:16 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:1.0935382843017578 norm:0.0006290330784395337 max memory_allocated 29276.90478515625 
[2025-03-22 09:56:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 32 ===
[2025-03-22 09:57:25 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 0 loss:1.2423498630523682 norm:0.015273293480277061 max memory_allocated 29277.09228515625 
[2025-03-22 09:58:13 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 1 loss:1.228914499282837 norm:0.00900527834892273 max memory_allocated 29277.09228515625 
[2025-03-22 09:59:00 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 2 loss:1.2128437757492065 norm:0.005666583310812712 max memory_allocated 29277.09228515625 
[2025-03-22 09:59:48 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 3 loss:1.2058045864105225 norm:0.003894679481163621 max memory_allocated 29277.09228515625 
[2025-03-22 10:00:36 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 4 loss:1.2015212774276733 norm:0.002668877365067601 max memory_allocated 29277.09228515625 
[2025-03-22 10:01:24 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 5 loss:1.198152780532837 norm:0.002139874966815114 max memory_allocated 29277.09228515625 
[2025-03-22 10:02:12 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 6 loss:1.1948504447937012 norm:0.0009415054228156805 max memory_allocated 29277.09228515625 
[2025-03-22 10:02:59 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 7 loss:1.1926474571228027 norm:0.0008301195921376348 max memory_allocated 29277.09228515625 
[2025-03-22 10:03:47 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 8 loss:1.1912003755569458 norm:0.0008131080539897084 max memory_allocated 29277.09228515625 
[2025-03-22 10:04:35 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 9 loss:1.190193772315979 norm:0.0008050490287132561 max memory_allocated 29277.09228515625 
[2025-03-22 10:05:22 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 10 loss:1.189409613609314 norm:0.0007935299072414637 max memory_allocated 29277.09228515625 
[2025-03-22 10:06:10 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 11 loss:1.1888633966445923 norm:0.0007877358002588153 max memory_allocated 29277.09228515625 
[2025-03-22 10:06:58 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 12 loss:1.1883652210235596 norm:0.0007766811759211123 max memory_allocated 29277.09228515625 
[2025-03-22 10:07:45 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 13 loss:1.1879953145980835 norm:0.0007633685600012541 max memory_allocated 29277.09228515625 
[2025-03-22 10:08:33 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 14 loss:1.187699794769287 norm:0.0007635240908712149 max memory_allocated 29277.09228515625 
[2025-03-22 10:09:20 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 15 loss:1.1874291896820068 norm:0.0007471321150660515 max memory_allocated 29277.09228515625 
[2025-03-22 10:10:08 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 16 loss:1.187268614768982 norm:0.0007421054178848863 max memory_allocated 29277.09228515625 
[2025-03-22 10:10:56 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 17 loss:1.18704092502594 norm:0.0007339906296692789 max memory_allocated 29277.09228515625 
[2025-03-22 10:11:44 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 18 loss:1.1868760585784912 norm:0.000732713844627142 max memory_allocated 29277.09228515625 
[2025-03-22 10:12:32 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 19 loss:1.1866718530654907 norm:0.0007226172019727528 max memory_allocated 29277.09228515625 
[2025-03-22 10:12:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 33 ===
[2025-03-22 10:13:40 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 0 loss:1.3449207544326782 norm:0.011408036574721336 max memory_allocated 29277.27978515625 
[2025-03-22 10:14:28 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 1 loss:1.330910325050354 norm:0.007798763923346996 max memory_allocated 29277.27978515625 
[2025-03-22 10:15:16 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 2 loss:1.3139700889587402 norm:0.005428781267255545 max memory_allocated 29277.27978515625 
[2025-03-22 10:16:04 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 3 loss:1.306561827659607 norm:0.003965322859585285 max memory_allocated 29277.27978515625 
[2025-03-22 10:16:52 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 4 loss:1.3019214868545532 norm:0.002913874574005604 max memory_allocated 29277.27978515625 
[2025-03-22 10:17:39 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 5 loss:1.2985475063323975 norm:0.002427853411063552 max memory_allocated 29277.27978515625 
[2025-03-22 10:18:27 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 6 loss:1.2953643798828125 norm:0.0021456731483340263 max memory_allocated 29277.27978515625 
[2025-03-22 10:19:15 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 7 loss:1.2930493354797363 norm:0.001893799053505063 max memory_allocated 29277.27978515625 
[2025-03-22 10:20:02 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 8 loss:1.29099440574646 norm:0.0016433884156867862 max memory_allocated 29277.27978515625 
[2025-03-22 10:20:50 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 9 loss:1.2898545265197754 norm:0.0014824862591922283 max memory_allocated 29277.27978515625 
[2025-03-22 10:21:38 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 10 loss:1.288926601409912 norm:0.0013232636265456676 max memory_allocated 29277.27978515625 
[2025-03-22 10:22:25 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 11 loss:1.288135051727295 norm:0.0011869537411257625 max memory_allocated 29277.27978515625 
[2025-03-22 10:23:13 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 12 loss:1.287731409072876 norm:0.0011230054078623652 max memory_allocated 29277.27978515625 
[2025-03-22 10:24:00 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 13 loss:1.2870800495147705 norm:0.0010348913492634892 max memory_allocated 29277.27978515625 
[2025-03-22 10:24:48 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 14 loss:1.2864115238189697 norm:0.0010017255553975701 max memory_allocated 29277.27978515625 
[2025-03-22 10:25:36 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 15 loss:1.2857778072357178 norm:0.0009436802938580513 max memory_allocated 29277.27978515625 
[2025-03-22 10:26:24 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 16 loss:1.2854197025299072 norm:0.0008668374503031373 max memory_allocated 29277.27978515625 
[2025-03-22 10:27:12 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 17 loss:1.285359263420105 norm:0.0007857578457333148 max memory_allocated 29277.27978515625 
[2025-03-22 10:27:59 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 18 loss:1.2849106788635254 norm:0.000735919107683003 max memory_allocated 29277.27978515625 
[2025-03-22 10:28:47 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 19 loss:1.2846533060073853 norm:0.000714395719114691 max memory_allocated 29277.27978515625 
[2025-03-22 10:29:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 34 ===
[2025-03-22 10:29:55 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 0 loss:1.4675755500793457 norm:0.014263545162975788 max memory_allocated 29277.46728515625 
[2025-03-22 10:30:43 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 1 loss:1.4515888690948486 norm:0.00863301008939743 max memory_allocated 29277.46728515625 
[2025-03-22 10:31:31 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 2 loss:1.4332903623580933 norm:0.005555767100304365 max memory_allocated 29277.46728515625 
[2025-03-22 10:32:19 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 3 loss:1.4248230457305908 norm:0.003974473103880882 max memory_allocated 29277.46728515625 
[2025-03-22 10:33:07 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 4 loss:1.4202169179916382 norm:0.0031714735087007284 max memory_allocated 29277.46728515625 
[2025-03-22 10:33:54 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 5 loss:1.4161195755004883 norm:0.0024785646237432957 max memory_allocated 29277.46728515625 
[2025-03-22 10:34:42 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 6 loss:1.4124431610107422 norm:0.0019828015938401222 max memory_allocated 29277.46728515625 
[2025-03-22 10:35:30 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 7 loss:1.4096753597259521 norm:0.0017237365245819092 max memory_allocated 29277.46728515625 
[2025-03-22 10:36:17 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 8 loss:1.4078246355056763 norm:0.0015685083344578743 max memory_allocated 29277.46728515625 
[2025-03-22 10:37:05 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 9 loss:1.4065042734146118 norm:0.0014229940716177225 max memory_allocated 29277.46728515625 
[2025-03-22 10:37:53 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 10 loss:1.4054521322250366 norm:0.0013074949383735657 max memory_allocated 29277.46728515625 
[2025-03-22 10:38:40 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 11 loss:1.4046446084976196 norm:0.0012046383926644921 max memory_allocated 29277.46728515625 
[2025-03-22 10:39:28 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 12 loss:1.4039756059646606 norm:0.001106170122511685 max memory_allocated 29277.46728515625 
[2025-03-22 10:40:15 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 13 loss:1.4034138917922974 norm:0.0010389467934146523 max memory_allocated 29277.46728515625 
[2025-03-22 10:41:03 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 14 loss:1.4029693603515625 norm:0.0009839467238634825 max memory_allocated 29277.46728515625 
[2025-03-22 10:41:51 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 15 loss:1.4025094509124756 norm:0.0009354135254397988 max memory_allocated 29277.46728515625 
[2025-03-22 10:42:39 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 16 loss:1.4021315574645996 norm:0.0008919680258259177 max memory_allocated 29277.46728515625 
[2025-03-22 10:43:27 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 17 loss:1.4017980098724365 norm:0.0007414139108732343 max memory_allocated 29277.46728515625 
[2025-03-22 10:44:15 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 18 loss:1.401517391204834 norm:0.0007294562528841197 max memory_allocated 29277.46728515625 
[2025-03-22 10:45:03 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 19 loss:1.401228427886963 norm:0.000729432562366128 max memory_allocated 29277.46728515625 
[2025-03-22 10:45:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 35 ===
[2025-03-22 10:46:10 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 0 loss:1.596062421798706 norm:0.006230417639017105 max memory_allocated 29277.65478515625 
[2025-03-22 10:46:58 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 1 loss:1.579507827758789 norm:0.004136201925575733 max memory_allocated 29277.65478515625 
[2025-03-22 10:47:46 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 2 loss:1.5585814714431763 norm:0.0027054923120886087 max memory_allocated 29277.65478515625 
[2025-03-22 10:48:34 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 3 loss:1.5491292476654053 norm:0.0018291249871253967 max memory_allocated 29277.65478515625 
[2025-03-22 10:49:21 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 4 loss:1.5438390970230103 norm:0.0013873715652152896 max memory_allocated 29277.65478515625 
[2025-03-22 10:50:09 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 5 loss:1.5395689010620117 norm:0.001197241828776896 max memory_allocated 29277.65478515625 
[2025-03-22 10:50:57 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 6 loss:1.5358829498291016 norm:0.0010971127776429057 max memory_allocated 29277.65478515625 
[2025-03-22 10:51:44 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 7 loss:1.5327118635177612 norm:0.0009140830370597541 max memory_allocated 29277.65478515625 
[2025-03-22 10:52:32 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 8 loss:1.5306739807128906 norm:0.0008786220569163561 max memory_allocated 29277.65478515625 
[2025-03-22 10:53:20 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 9 loss:1.5292699337005615 norm:0.0008559076813980937 max memory_allocated 29277.65478515625 
[2025-03-22 10:54:07 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 10 loss:1.528259038925171 norm:0.0008346179965883493 max memory_allocated 29277.65478515625 
[2025-03-22 10:54:55 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 11 loss:1.5273802280426025 norm:0.0008175453986041248 max memory_allocated 29277.65478515625 
[2025-03-22 10:55:43 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 12 loss:1.5267159938812256 norm:0.0008036348153837025 max memory_allocated 29277.65478515625 
[2025-03-22 10:56:30 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 13 loss:1.5261799097061157 norm:0.0007868642569519579 max memory_allocated 29277.65478515625 
[2025-03-22 10:57:18 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 14 loss:1.5257179737091064 norm:0.000776448636315763 max memory_allocated 29277.65478515625 
[2025-03-22 10:58:06 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 15 loss:1.5252900123596191 norm:0.0007605482242070138 max memory_allocated 29277.65478515625 
[2025-03-22 10:58:54 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 16 loss:1.5248948335647583 norm:0.0007553074974566698 max memory_allocated 29277.65478515625 
[2025-03-22 10:59:42 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 17 loss:1.5245769023895264 norm:0.0007533892057836056 max memory_allocated 29277.65478515625 
[2025-03-22 11:00:30 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 18 loss:1.5243357419967651 norm:0.0007547776331193745 max memory_allocated 29277.65478515625 
[2025-03-22 11:01:17 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 19 loss:1.5240867137908936 norm:0.0007413344574160874 max memory_allocated 29277.65478515625 
[2025-03-22 11:01:31 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 36 ===
[2025-03-22 11:01:39 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:02:27 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 0 loss:1.7837706804275513 norm:0.04300826042890549 max memory_allocated 29278.42041015625 
[2025-03-22 11:03:15 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 1 loss:1.7571574449539185 norm:0.03403705358505249 max memory_allocated 29278.42041015625 
[2025-03-22 11:04:03 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 2 loss:1.7277525663375854 norm:0.024524185806512833 max memory_allocated 29278.42041015625 
[2025-03-22 11:04:51 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 3 loss:1.7120128870010376 norm:0.019433580338954926 max memory_allocated 29278.42041015625 
[2025-03-22 11:05:39 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 4 loss:1.6996647119522095 norm:0.014750364236533642 max memory_allocated 29278.42041015625 
[2025-03-22 11:06:27 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 5 loss:1.691051721572876 norm:0.011759086512029171 max memory_allocated 29278.42041015625 
[2025-03-22 11:07:15 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 6 loss:1.68535315990448 norm:0.010188505053520203 max memory_allocated 29278.42041015625 
[2025-03-22 11:08:03 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 7 loss:1.680912971496582 norm:0.00947206187993288 max memory_allocated 29278.42041015625 
[2025-03-22 11:08:51 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 8 loss:1.6779762506484985 norm:0.008706972934305668 max memory_allocated 29278.42041015625 
[2025-03-22 11:09:39 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 9 loss:1.6760351657867432 norm:0.00869174487888813 max memory_allocated 29278.42041015625 
[2025-03-22 11:10:27 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 10 loss:1.6748709678649902 norm:0.008919154293835163 max memory_allocated 29278.42041015625 
[2025-03-22 11:11:15 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 11 loss:1.6739037036895752 norm:0.008723556064069271 max memory_allocated 29278.42041015625 
[2025-03-22 11:12:03 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 12 loss:1.6723921298980713 norm:0.008439302444458008 max memory_allocated 29278.42041015625 
[2025-03-22 11:12:50 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 13 loss:1.6717572212219238 norm:0.00823831744492054 max memory_allocated 29278.42041015625 
[2025-03-22 11:13:38 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 14 loss:1.6706852912902832 norm:0.008121551014482975 max memory_allocated 29278.42041015625 
[2025-03-22 11:14:26 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 15 loss:1.6705543994903564 norm:0.00819963775575161 max memory_allocated 29278.42041015625 
[2025-03-22 11:15:14 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 16 loss:1.669526219367981 norm:0.00790339708328247 max memory_allocated 29278.42041015625 
[2025-03-22 11:16:02 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 17 loss:1.6698448657989502 norm:0.007877513766288757 max memory_allocated 29278.42041015625 
[2025-03-22 11:16:50 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 18 loss:1.6688618659973145 norm:0.007841246202588081 max memory_allocated 29278.42041015625 
[2025-03-22 11:17:38 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 19 loss:1.6694238185882568 norm:0.007662754040211439 max memory_allocated 29278.42041015625 
[2025-03-22 11:17:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 37 ===
[2025-03-22 11:17:58 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:18:46 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 0 loss:1.9604960680007935 norm:0.04837973788380623 max memory_allocated 29278.60791015625 
[2025-03-22 11:19:34 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 1 loss:1.9287569522857666 norm:0.0387180857360363 max memory_allocated 29278.60791015625 
[2025-03-22 11:20:22 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 2 loss:1.896360158920288 norm:0.030112996697425842 max memory_allocated 29278.60791015625 
[2025-03-22 11:21:10 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 3 loss:1.8742111921310425 norm:0.02293868362903595 max memory_allocated 29278.60791015625 
[2025-03-22 11:21:58 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 4 loss:1.863606333732605 norm:0.01856049709022045 max memory_allocated 29278.60791015625 
[2025-03-22 11:22:46 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 5 loss:1.8557252883911133 norm:0.015324333682656288 max memory_allocated 29278.60791015625 
[2025-03-22 11:23:34 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 6 loss:1.8496252298355103 norm:0.01306407991796732 max memory_allocated 29278.60791015625 
[2025-03-22 11:24:22 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 7 loss:1.8450123071670532 norm:0.011270459741353989 max memory_allocated 29278.60791015625 
[2025-03-22 11:25:10 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 8 loss:1.8413536548614502 norm:0.010694162920117378 max memory_allocated 29278.60791015625 
[2025-03-22 11:25:58 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 9 loss:1.839385747909546 norm:0.010602892376482487 max memory_allocated 29278.60791015625 
[2025-03-22 11:26:46 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 10 loss:1.8377878665924072 norm:0.010776486247777939 max memory_allocated 29278.60791015625 
[2025-03-22 11:27:34 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 11 loss:1.8362101316452026 norm:0.010124084539711475 max memory_allocated 29278.60791015625 
[2025-03-22 11:28:22 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 12 loss:1.8345818519592285 norm:0.009656968526542187 max memory_allocated 29278.60791015625 
[2025-03-22 11:29:10 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 13 loss:1.8344045877456665 norm:0.009134668856859207 max memory_allocated 29278.60791015625 
[2025-03-22 11:29:58 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 14 loss:1.832923173904419 norm:0.009331153705716133 max memory_allocated 29278.60791015625 
[2025-03-22 11:30:45 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 15 loss:1.8349583148956299 norm:0.00928536243736744 max memory_allocated 29278.60791015625 
[2025-03-22 11:31:33 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 16 loss:1.8325121402740479 norm:0.009485123679041862 max memory_allocated 29278.60791015625 
[2025-03-22 11:32:21 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 17 loss:1.8345723152160645 norm:0.009909335523843765 max memory_allocated 29278.60791015625 
[2025-03-22 11:33:09 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 18 loss:1.830918312072754 norm:0.009007425978779793 max memory_allocated 29278.60791015625 
[2025-03-22 11:33:57 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 19 loss:1.8291010856628418 norm:0.0071977414190769196 max memory_allocated 29278.60791015625 
[2025-03-22 11:34:10 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 38 ===
[2025-03-22 11:34:17 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:35:05 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 0 loss:2.251756191253662 norm:0.0761510357260704 max memory_allocated 29278.79541015625 
[2025-03-22 11:35:53 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 1 loss:2.2119812965393066 norm:0.06350453943014145 max memory_allocated 29278.79541015625 
[2025-03-22 11:36:41 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 2 loss:2.1705026626586914 norm:0.0488455668091774 max memory_allocated 29278.79541015625 
[2025-03-22 11:37:29 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 3 loss:2.1417620182037354 norm:0.03728851303458214 max memory_allocated 29278.79541015625 
[2025-03-22 11:38:17 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 4 loss:2.1252167224884033 norm:0.029708728194236755 max memory_allocated 29278.79541015625 
[2025-03-22 11:39:05 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 5 loss:2.114821434020996 norm:0.025060420855879784 max memory_allocated 29278.79541015625 
[2025-03-22 11:39:53 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 6 loss:2.1072261333465576 norm:0.021885402500629425 max memory_allocated 29278.79541015625 
[2025-03-22 11:40:41 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 7 loss:2.1015031337738037 norm:0.0195029154419899 max memory_allocated 29278.79541015625 
[2025-03-22 11:41:29 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 8 loss:2.09671950340271 norm:0.017639033496379852 max memory_allocated 29278.79541015625 
[2025-03-22 11:42:17 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 9 loss:2.093191385269165 norm:0.016775771975517273 max memory_allocated 29278.79541015625 
[2025-03-22 11:43:06 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 10 loss:2.0902860164642334 norm:0.015619530342519283 max memory_allocated 29278.79541015625 
[2025-03-22 11:43:54 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 11 loss:2.0890228748321533 norm:0.016319794580340385 max memory_allocated 29278.79541015625 
[2025-03-22 11:44:42 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 12 loss:2.087662696838379 norm:0.016537433490157127 max memory_allocated 29278.79541015625 
[2025-03-22 11:45:30 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 13 loss:2.0848710536956787 norm:0.014623302035033703 max memory_allocated 29278.79541015625 
[2025-03-22 11:46:18 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 14 loss:2.0841097831726074 norm:0.014857213944196701 max memory_allocated 29278.79541015625 
[2025-03-22 11:47:06 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 15 loss:2.083376169204712 norm:0.013826839625835419 max memory_allocated 29278.79541015625 
[2025-03-22 11:47:53 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 16 loss:2.0839173793792725 norm:0.014259791001677513 max memory_allocated 29278.79541015625 
[2025-03-22 11:48:41 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 17 loss:2.0834527015686035 norm:0.013798285275697708 max memory_allocated 29278.79541015625 
[2025-03-22 11:49:29 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 18 loss:2.080824613571167 norm:0.011756295338273048 max memory_allocated 29278.79541015625 
[2025-03-22 11:50:17 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 19 loss:2.0787813663482666 norm:0.011954276822507381 max memory_allocated 29278.79541015625 
[2025-03-22 11:50:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 39 ===
[2025-03-22 11:50:39 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:51:26 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 0 loss:3.238387107849121 norm:0.25098544359207153 max memory_allocated 29278.98291015625 
[2025-03-22 11:52:14 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 1 loss:3.036226749420166 norm:0.20404790341854095 max memory_allocated 29278.98291015625 
[2025-03-22 11:53:02 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 2 loss:2.928785800933838 norm:0.16759252548217773 max memory_allocated 29278.98291015625 
[2025-03-22 11:53:50 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 3 loss:2.8480539321899414 norm:0.13359986245632172 max memory_allocated 29278.98291015625 
[2025-03-22 11:54:37 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 4 loss:2.806089401245117 norm:0.11004329472780228 max memory_allocated 29278.98291015625 
[2025-03-22 11:55:25 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 5 loss:2.773240089416504 norm:0.08704905956983566 max memory_allocated 29278.98291015625 
[2025-03-22 11:56:13 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 6 loss:2.7496256828308105 norm:0.07228491455316544 max memory_allocated 29278.98291015625 
[2025-03-22 11:57:01 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 7 loss:2.7329697608947754 norm:0.0633607804775238 max memory_allocated 29278.98291015625 
[2025-03-22 11:57:49 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 8 loss:2.7188773155212402 norm:0.05641515925526619 max memory_allocated 29278.98291015625 
[2025-03-22 11:58:38 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 9 loss:2.707396984100342 norm:0.05231472849845886 max memory_allocated 29278.98291015625 
[2025-03-22 11:59:26 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 10 loss:2.6986002922058105 norm:0.048218678683042526 max memory_allocated 29278.98291015625 
[2025-03-22 12:00:14 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 11 loss:2.690070629119873 norm:0.045164965093135834 max memory_allocated 29278.98291015625 
[2025-03-22 12:01:02 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 12 loss:2.684262275695801 norm:0.043819356709718704 max memory_allocated 29278.98291015625 
[2025-03-22 12:01:50 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 13 loss:2.6785099506378174 norm:0.042538031935691833 max memory_allocated 29278.98291015625 
[2025-03-22 12:02:38 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 14 loss:2.673373222351074 norm:0.04155031964182854 max memory_allocated 29278.98291015625 
[2025-03-22 12:03:26 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 15 loss:2.669220447540283 norm:0.03981240093708038 max memory_allocated 29278.98291015625 
[2025-03-22 12:04:13 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 16 loss:2.6646101474761963 norm:0.038923539221286774 max memory_allocated 29278.98291015625 
[2025-03-22 12:05:01 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 17 loss:2.6595523357391357 norm:0.03697501868009567 max memory_allocated 29278.98291015625 
[2025-03-22 12:05:49 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 18 loss:2.6563334465026855 norm:0.03576686978340149 max memory_allocated 29278.98291015625 
[2025-03-22 12:06:37 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 19 loss:2.6539440155029297 norm:0.03513054549694061 max memory_allocated 29278.98291015625 
[2025-03-22 12:06:50 root] (main_calibration_a.py 369): INFO 39119.70876121521
[2025-03-22 12:07:05 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 12:08:56 root] (main_calibration_a.py 158): INFO wikitext2 : 7.622220039367676
[2025-03-22 12:08:56 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 12:11:47 root] (main_calibration_a.py 158): INFO c4 : 10.826369285583496
[2025-03-22 14:18:23 root] (main_calibration_a.py 169): INFO {'wikitext2': 7.622220039367676, 'c4': 10.826369285583496, 'results': {'arc_easy': {'acc': 0.6111111111111112, 'acc_stderr': 0.010003248335313764, 'acc_norm': 0.484006734006734, 'acc_norm_stderr': 0.010254533589288163}, 'boolq': {'acc': 0.6443425076452599, 'acc_stderr': 0.008372726639977393}, 'arc_challenge': {'acc': 0.3165529010238908, 'acc_stderr': 0.01359243151906808, 'acc_norm': 0.34982935153583616, 'acc_norm_stderr': 0.013936809212158287}, 'winogrande': {'acc': 0.5824782951854776, 'acc_stderr': 0.013859978264440251}, 'piqa': {'acc': 0.7110990206746464, 'acc_stderr': 0.010575111841364906, 'acc_norm': 0.7023939064200218, 'acc_norm_stderr': 0.010667353792388208}, 'hellaswag': {'acc': 0.4940250946026688, 'acc_stderr': 0.004989425133377909, 'acc_norm': 0.6424019119697272, 'acc_norm_stderr': 0.004783133725599501}}, 'versions': {'arc_easy': 0, 'boolq': 1, 'arc_challenge': 0, 'winogrande': 0, 'piqa': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 14:18:23 root] (main_calibration_a.py 172): INFO 31.66,61.11,64.43,49.40,71.11,58.25
