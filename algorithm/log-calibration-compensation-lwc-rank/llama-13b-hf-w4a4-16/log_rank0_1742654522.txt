[2025-03-22 14:42:02 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/llama-13b-hf-w4a4-16', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=16)
[2025-03-22 14:48:36 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 14:48:36 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-22 14:48:37 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 14:48:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 14:48:43 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 14:49:31 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.18012912571430206 norm:0.20062269270420074 max memory_allocated 29271.27197265625 
[2025-03-22 14:50:18 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.07677197456359863 norm:0.09342318028211594 max memory_allocated 29271.27197265625 
[2025-03-22 14:51:06 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.05397607758641243 norm:0.057672515511512756 max memory_allocated 29271.27197265625 
[2025-03-22 14:51:54 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.04700932279229164 norm:0.04732683673501015 max memory_allocated 29271.27197265625 
[2025-03-22 14:52:42 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.044239237904548645 norm:0.041852980852127075 max memory_allocated 29271.27197265625 
[2025-03-22 14:53:30 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.04231065884232521 norm:0.03863043710589409 max memory_allocated 29271.27197265625 
[2025-03-22 14:54:18 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.04061184823513031 norm:0.03512319549918175 max memory_allocated 29271.27197265625 
[2025-03-22 14:55:06 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.03916165977716446 norm:0.031091146171092987 max memory_allocated 29271.27197265625 
[2025-03-22 14:55:54 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.038053978234529495 norm:0.02819664403796196 max memory_allocated 29271.27197265625 
[2025-03-22 14:56:42 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.03713405877351761 norm:0.0251726396381855 max memory_allocated 29271.27197265625 
[2025-03-22 14:57:29 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.03630969673395157 norm:0.02244166098535061 max memory_allocated 29271.27197265625 
[2025-03-22 14:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.03561139106750488 norm:0.020058682188391685 max memory_allocated 29271.27197265625 
[2025-03-22 14:59:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.035102054476737976 norm:0.018201477825641632 max memory_allocated 29271.27197265625 
[2025-03-22 14:59:53 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.03465384989976883 norm:0.016408540308475494 max memory_allocated 29271.27197265625 
[2025-03-22 15:00:40 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.034204594790935516 norm:0.014736272394657135 max memory_allocated 29271.27197265625 
[2025-03-22 15:01:28 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.03381648659706116 norm:0.013436305336654186 max memory_allocated 29271.27197265625 
[2025-03-22 15:02:16 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.033503834158182144 norm:0.012241201475262642 max memory_allocated 29271.27197265625 
[2025-03-22 15:03:04 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.03343040496110916 norm:0.011381792835891247 max memory_allocated 29271.27197265625 
[2025-03-22 15:03:52 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.0331265926361084 norm:0.0106821833178401 max memory_allocated 29271.27197265625 
[2025-03-22 15:04:40 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.033110808581113815 norm:0.010231385007500648 max memory_allocated 29271.27197265625 
[2025-03-22 15:04:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 15:04:58 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:05:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.3621482849121094 norm:0.2218835949897766 max memory_allocated 29271.27197265625 
[2025-03-22 15:06:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.2067873179912567 norm:0.12667956948280334 max memory_allocated 29271.27197265625 
[2025-03-22 15:07:22 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.14312750101089478 norm:0.05763343349099159 max memory_allocated 29271.27197265625 
[2025-03-22 15:08:10 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.11858581751585007 norm:0.0412500724196434 max memory_allocated 29271.27197265625 
[2025-03-22 15:08:58 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.10703827440738678 norm:0.03313588351011276 max memory_allocated 29271.27197265625 
[2025-03-22 15:09:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.10037750750780106 norm:0.027180712670087814 max memory_allocated 29271.27197265625 
[2025-03-22 15:10:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.09636315703392029 norm:0.023646961897611618 max memory_allocated 29271.27197265625 
[2025-03-22 15:11:22 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.09289835393428802 norm:0.02040732093155384 max memory_allocated 29271.27197265625 
[2025-03-22 15:12:10 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.09065231680870056 norm:0.01778550073504448 max memory_allocated 29271.27197265625 
[2025-03-22 15:12:58 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.0887206494808197 norm:0.015258563682436943 max memory_allocated 29271.27197265625 
[2025-03-22 15:13:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.08718732744455338 norm:0.012941495515406132 max memory_allocated 29271.27197265625 
[2025-03-22 15:14:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.08599802106618881 norm:0.011345315724611282 max memory_allocated 29271.27197265625 
[2025-03-22 15:15:22 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.08512222766876221 norm:0.00998566672205925 max memory_allocated 29271.27197265625 
[2025-03-22 15:16:10 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.08438992500305176 norm:0.009013737551867962 max memory_allocated 29271.27197265625 
[2025-03-22 15:16:58 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.0840027779340744 norm:0.008667660877108574 max memory_allocated 29271.27197265625 
[2025-03-22 15:17:46 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.08381946384906769 norm:0.008712446317076683 max memory_allocated 29271.27197265625 
[2025-03-22 15:18:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.0841861143708229 norm:0.008852112106978893 max memory_allocated 29271.27197265625 
[2025-03-22 15:19:23 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.08321411907672882 norm:0.007275210693478584 max memory_allocated 29271.27197265625 
[2025-03-22 15:20:11 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.08281807601451874 norm:0.00704929931089282 max memory_allocated 29271.27197265625 
[2025-03-22 15:20:59 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.08255445212125778 norm:0.006915166974067688 max memory_allocated 29271.27197265625 
[2025-03-22 15:21:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 15:21:16 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 15:22:05 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.3524530529975891 norm:0.08822514116764069 max memory_allocated 29271.27197265625 
[2025-03-22 15:22:53 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.2815367579460144 norm:0.0686432495713234 max memory_allocated 29271.27197265625 
[2025-03-22 15:23:41 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.24429988861083984 norm:0.0662148967385292 max memory_allocated 29271.27197265625 
[2025-03-22 15:24:29 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.22893425822257996 norm:0.06580033153295517 max memory_allocated 29271.27197265625 
[2025-03-22 15:25:17 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.2080165445804596 norm:0.05980829894542694 max memory_allocated 29271.27197265625 
[2025-03-22 15:26:06 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.1993687003850937 norm:0.058043740689754486 max memory_allocated 29271.27197265625 
[2025-03-22 15:26:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.19563530385494232 norm:0.055224500596523285 max memory_allocated 29271.27197265625 
[2025-03-22 15:27:42 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.19228258728981018 norm:0.06356605142354965 max memory_allocated 29271.27197265625 
[2025-03-22 15:28:30 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.18498560786247253 norm:0.05386443808674812 max memory_allocated 29271.27197265625 
[2025-03-22 15:29:18 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.18687011301517487 norm:0.055091824382543564 max memory_allocated 29271.27197265625 
[2025-03-22 15:30:06 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.1750621497631073 norm:0.04830033704638481 max memory_allocated 29271.27197265625 
[2025-03-22 15:30:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.18028615415096283 norm:0.04084537923336029 max memory_allocated 29271.27197265625 
[2025-03-22 15:31:42 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.1729087382555008 norm:0.03710062801837921 max memory_allocated 29271.27197265625 
[2025-03-22 15:32:30 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.1760239601135254 norm:0.04215913265943527 max memory_allocated 29271.27197265625 
[2025-03-22 15:33:18 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.17024071514606476 norm:0.04059695079922676 max memory_allocated 29271.27197265625 
[2025-03-22 15:34:06 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.16663751006126404 norm:0.030263792723417282 max memory_allocated 29271.27197265625 
[2025-03-22 15:34:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.1679612547159195 norm:0.03231606259942055 max memory_allocated 29271.27197265625 
[2025-03-22 15:35:42 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.16184522211551666 norm:0.029910583049058914 max memory_allocated 29271.27197265625 
[2025-03-22 15:36:31 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.1666344553232193 norm:0.028589244931936264 max memory_allocated 29271.27197265625 
[2025-03-22 15:37:19 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.16453269124031067 norm:0.03136255592107773 max memory_allocated 29271.27197265625 
[2025-03-22 15:37:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 15:38:24 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.26512807607650757 norm:0.031748659908771515 max memory_allocated 29271.27197265625 
[2025-03-22 15:39:12 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.24051393568515778 norm:0.01635131426155567 max memory_allocated 29271.27197265625 
[2025-03-22 15:40:00 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.21581360697746277 norm:0.00842551700770855 max memory_allocated 29271.27197265625 
[2025-03-22 15:40:49 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.20443904399871826 norm:0.005273567046970129 max memory_allocated 29271.27197265625 
[2025-03-22 15:41:37 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.19929777085781097 norm:0.004134393762797117 max memory_allocated 29271.27197265625 
[2025-03-22 15:42:24 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.19558750092983246 norm:0.003383743343874812 max memory_allocated 29271.27197265625 
[2025-03-22 15:43:12 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.1928740292787552 norm:0.0028504987712949514 max memory_allocated 29271.27197265625 
[2025-03-22 15:44:00 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.1913989931344986 norm:0.002605595625936985 max memory_allocated 29271.27197265625 
[2025-03-22 15:44:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.18999627232551575 norm:0.0023698583245277405 max memory_allocated 29271.27197265625 
[2025-03-22 15:45:36 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.1891387552022934 norm:0.002252174774184823 max memory_allocated 29271.27197265625 
[2025-03-22 15:46:24 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.18784871697425842 norm:0.001928976271301508 max memory_allocated 29271.27197265625 
[2025-03-22 15:47:12 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.18731218576431274 norm:0.0018492579692974687 max memory_allocated 29271.27197265625 
[2025-03-22 15:48:00 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.18710851669311523 norm:0.001829237211495638 max memory_allocated 29271.27197265625 
[2025-03-22 15:48:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.18664656579494476 norm:0.0018048093188554049 max memory_allocated 29271.27197265625 
[2025-03-22 15:49:36 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.1863022744655609 norm:0.0017363319639116526 max memory_allocated 29271.27197265625 
[2025-03-22 15:50:24 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.1864202469587326 norm:0.0017331107519567013 max memory_allocated 29271.27197265625 
[2025-03-22 15:51:12 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.1860896646976471 norm:0.001647789147682488 max memory_allocated 29271.27197265625 
[2025-03-22 15:52:00 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.18544496595859528 norm:0.0015852975193411112 max memory_allocated 29271.27197265625 
[2025-03-22 15:52:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.18503910303115845 norm:0.001565444516018033 max memory_allocated 29271.27197265625 
[2025-03-22 15:53:36 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.1846700757741928 norm:0.0015157877933233976 max memory_allocated 29271.27197265625 
[2025-03-22 15:53:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 15:54:41 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.3190856873989105 norm:0.045017391443252563 max memory_allocated 29271.27197265625 
[2025-03-22 15:55:29 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.2831302881240845 norm:0.014186637476086617 max memory_allocated 29271.27197265625 
[2025-03-22 15:56:17 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.25822749733924866 norm:0.007736633997410536 max memory_allocated 29271.27197265625 
[2025-03-22 15:57:06 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.24975234270095825 norm:0.005766908172518015 max memory_allocated 29271.27197265625 
[2025-03-22 15:57:54 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.2455330491065979 norm:0.004698547068983316 max memory_allocated 29271.27197265625 
[2025-03-22 15:58:42 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.24305865168571472 norm:0.004294593818485737 max memory_allocated 29271.27197265625 
[2025-03-22 15:59:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.24186564981937408 norm:0.004048292525112629 max memory_allocated 29271.27197265625 
[2025-03-22 16:00:18 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.24053965508937836 norm:0.003653327003121376 max memory_allocated 29271.27197265625 
[2025-03-22 16:01:06 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.23836101591587067 norm:0.002966149477288127 max memory_allocated 29271.27197265625 
[2025-03-22 16:01:54 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.23756276071071625 norm:0.0026932223699986935 max memory_allocated 29271.27197265625 
[2025-03-22 16:02:42 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.23717167973518372 norm:0.002607053378596902 max memory_allocated 29271.27197265625 
[2025-03-22 16:03:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.23657046258449554 norm:0.00245174253359437 max memory_allocated 29271.27197265625 
[2025-03-22 16:04:18 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.23613975942134857 norm:0.0023528162855654955 max memory_allocated 29271.27197265625 
[2025-03-22 16:05:06 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.2356727421283722 norm:0.0022274022921919823 max memory_allocated 29271.27197265625 
[2025-03-22 16:05:54 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.23471878468990326 norm:0.0020048851147294044 max memory_allocated 29271.27197265625 
[2025-03-22 16:06:42 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.23461422324180603 norm:0.001979370601475239 max memory_allocated 29271.27197265625 
[2025-03-22 16:07:30 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.23459792137145996 norm:0.0019831585232168436 max memory_allocated 29271.27197265625 
[2025-03-22 16:08:17 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.23458322882652283 norm:0.0019577927887439728 max memory_allocated 29271.27197265625 
[2025-03-22 16:09:05 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.23443490266799927 norm:0.0019057616591453552 max memory_allocated 29271.27197265625 
[2025-03-22 16:09:53 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.23383983969688416 norm:0.0017848240677267313 max memory_allocated 29271.27197265625 
[2025-03-22 16:10:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 16:11:00 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.3835572600364685 norm:0.0670846700668335 max memory_allocated 29271.27197265625 
[2025-03-22 16:11:48 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.33937591314315796 norm:0.02588607929646969 max memory_allocated 29271.27197265625 
[2025-03-22 16:12:36 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.3023435175418854 norm:0.01058880053460598 max memory_allocated 29271.27197265625 
[2025-03-22 16:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.28843867778778076 norm:0.007038474082946777 max memory_allocated 29271.27197265625 
[2025-03-22 16:14:13 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.2823871672153473 norm:0.005647565238177776 max memory_allocated 29271.27197265625 
[2025-03-22 16:15:01 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.2781449258327484 norm:0.004618394188582897 max memory_allocated 29271.27197265625 
[2025-03-22 16:15:49 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.2755666673183441 norm:0.004098852165043354 max memory_allocated 29271.27197265625 
[2025-03-22 16:16:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.2737621068954468 norm:0.0036888867616653442 max memory_allocated 29271.27197265625 
[2025-03-22 16:17:25 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.2723217308521271 norm:0.003361903363838792 max memory_allocated 29271.27197265625 
[2025-03-22 16:18:13 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.2706215977668762 norm:0.002811598591506481 max memory_allocated 29271.27197265625 
[2025-03-22 16:19:02 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.27001067996025085 norm:0.002642536535859108 max memory_allocated 29271.27197265625 
[2025-03-22 16:19:50 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.26955825090408325 norm:0.0025709308683872223 max memory_allocated 29271.27197265625 
[2025-03-22 16:20:38 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.2687298655509949 norm:0.002294298727065325 max memory_allocated 29271.27197265625 
[2025-03-22 16:21:26 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.26826879382133484 norm:0.0022321678698062897 max memory_allocated 29271.27197265625 
[2025-03-22 16:22:14 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.26758795976638794 norm:0.002091487869620323 max memory_allocated 29271.27197265625 
[2025-03-22 16:23:02 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.2672272324562073 norm:0.001970615703612566 max memory_allocated 29271.27197265625 
[2025-03-22 16:23:51 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.26670584082603455 norm:0.0018306910060346127 max memory_allocated 29271.27197265625 
[2025-03-22 16:24:39 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.26631903648376465 norm:0.0017642863094806671 max memory_allocated 29271.27197265625 
[2025-03-22 16:25:27 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.2661052942276001 norm:0.0017498541856184602 max memory_allocated 29271.27197265625 
[2025-03-22 16:26:15 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.26593321561813354 norm:0.0017658392898738384 max memory_allocated 29271.27197265625 
[2025-03-22 16:26:28 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 16:27:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.4710274934768677 norm:0.01754850149154663 max memory_allocated 29271.27197265625 
[2025-03-22 16:28:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.4309443533420563 norm:0.008638986386358738 max memory_allocated 29271.27197265625 
[2025-03-22 16:28:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.4014907777309418 norm:0.005828950088471174 max memory_allocated 29271.27197265625 
[2025-03-22 16:29:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.39009833335876465 norm:0.004495932254940271 max memory_allocated 29271.27197265625 
[2025-03-22 16:30:31 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.3842350244522095 norm:0.004167960956692696 max memory_allocated 29271.27197265625 
[2025-03-22 16:31:19 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.37851276993751526 norm:0.004065983463078737 max memory_allocated 29271.27197265625 
[2025-03-22 16:32:07 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.3777371346950531 norm:0.004875116515904665 max memory_allocated 29271.27197265625 
[2025-03-22 16:32:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.3697521686553955 norm:0.0039107888005673885 max memory_allocated 29271.27197265625 
[2025-03-22 16:33:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.3674585521221161 norm:0.003928872756659985 max memory_allocated 29271.27197265625 
[2025-03-22 16:34:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.3653656542301178 norm:0.003819177858531475 max memory_allocated 29271.27197265625 
[2025-03-22 16:35:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.36462539434432983 norm:0.0039057591930031776 max memory_allocated 29271.27197265625 
[2025-03-22 16:36:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.36452025175094604 norm:0.0035035843029618263 max memory_allocated 29271.27197265625 
[2025-03-22 16:36:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.36370301246643066 norm:0.003438076237216592 max memory_allocated 29271.27197265625 
[2025-03-22 16:37:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.36239928007125854 norm:0.0034368454944342375 max memory_allocated 29271.27197265625 
[2025-03-22 16:38:33 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.36534592509269714 norm:0.004701416008174419 max memory_allocated 29271.27197265625 
[2025-03-22 16:39:21 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.36234670877456665 norm:0.0036501630675047636 max memory_allocated 29271.27197265625 
[2025-03-22 16:40:09 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.36133116483688354 norm:0.0036556192208081484 max memory_allocated 29271.27197265625 
[2025-03-22 16:40:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.36078110337257385 norm:0.003651600331068039 max memory_allocated 29271.27197265625 
[2025-03-22 16:41:45 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.36023247241973877 norm:0.003701199544593692 max memory_allocated 29271.27197265625 
[2025-03-22 16:42:33 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.3599761128425598 norm:0.003762793494388461 max memory_allocated 29271.27197265625 
[2025-03-22 16:42:47 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 16:43:39 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.48249998688697815 norm:0.02008218877017498 max memory_allocated 29271.27197265625 
[2025-03-22 16:44:27 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.4506725072860718 norm:0.010076640173792839 max memory_allocated 29271.27197265625 
[2025-03-22 16:45:15 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.413449227809906 norm:0.004734968300908804 max memory_allocated 29271.27197265625 
[2025-03-22 16:46:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.3964990973472595 norm:0.0026700873859226704 max memory_allocated 29271.27197265625 
[2025-03-22 16:46:51 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.38918158411979675 norm:0.001935305423103273 max memory_allocated 29271.27197265625 
[2025-03-22 16:47:39 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.3844902515411377 norm:0.0015784547431394458 max memory_allocated 29271.27197265625 
[2025-03-22 16:48:27 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.3811124265193939 norm:0.0013989178696647286 max memory_allocated 29271.27197265625 
[2025-03-22 16:49:15 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.3786645531654358 norm:0.001289659645408392 max memory_allocated 29271.27197265625 
[2025-03-22 16:50:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.3768668472766876 norm:0.0012317078653723001 max memory_allocated 29271.27197265625 
[2025-03-22 16:50:51 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.37563803791999817 norm:0.0012079642619937658 max memory_allocated 29271.27197265625 
[2025-03-22 16:51:39 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.3745613992214203 norm:0.0011656517162919044 max memory_allocated 29271.27197265625 
[2025-03-22 16:52:27 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.37368741631507874 norm:0.0011432803003117442 max memory_allocated 29271.27197265625 
[2025-03-22 16:53:15 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.3731549382209778 norm:0.00112427887506783 max memory_allocated 29271.27197265625 
[2025-03-22 16:54:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.3725413680076599 norm:0.0010863172356039286 max memory_allocated 29271.27197265625 
[2025-03-22 16:54:51 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.37223106622695923 norm:0.0010653939098119736 max memory_allocated 29271.27197265625 
[2025-03-22 16:55:39 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.37184610962867737 norm:0.001065037795342505 max memory_allocated 29271.27197265625 
[2025-03-22 16:56:27 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.3715741038322449 norm:0.0010491732973605394 max memory_allocated 29271.27197265625 
[2025-03-22 16:57:15 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.3714950680732727 norm:0.0010609407909214497 max memory_allocated 29271.27197265625 
[2025-03-22 16:58:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.3713270425796509 norm:0.001056381268426776 max memory_allocated 29271.27197265625 
[2025-03-22 16:58:51 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.37111780047416687 norm:0.001058313064277172 max memory_allocated 29271.27197265625 
[2025-03-22 16:59:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 16:59:57 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.5036812424659729 norm:0.017760129645466805 max memory_allocated 29271.27197265625 
[2025-03-22 17:00:45 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.4725589156150818 norm:0.010033529251813889 max memory_allocated 29271.27197265625 
[2025-03-22 17:01:33 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.434268057346344 norm:0.004902365617454052 max memory_allocated 29271.27197265625 
[2025-03-22 17:02:21 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.41555899381637573 norm:0.0025017692241817713 max memory_allocated 29271.27197265625 
[2025-03-22 17:03:09 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.4083433151245117 norm:0.0018380233086645603 max memory_allocated 29271.27197265625 
[2025-03-22 17:03:57 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.40387165546417236 norm:0.0014476062497124076 max memory_allocated 29271.27197265625 
[2025-03-22 17:04:45 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.40046173334121704 norm:0.0012527917278930545 max memory_allocated 29271.27197265625 
[2025-03-22 17:05:34 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.3980833888053894 norm:0.0011496596271172166 max memory_allocated 29271.27197265625 
[2025-03-22 17:06:22 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.3965796232223511 norm:0.0011066017905250192 max memory_allocated 29271.27197265625 
[2025-03-22 17:07:10 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.3955445885658264 norm:0.0010746533516794443 max memory_allocated 29271.27197265625 
[2025-03-22 17:07:58 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.39456093311309814 norm:0.0010329843498766422 max memory_allocated 29271.27197265625 
[2025-03-22 17:08:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.39389798045158386 norm:0.0010101560037583113 max memory_allocated 29271.27197265625 
[2025-03-22 17:09:34 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.3933087885379791 norm:0.0009922618046402931 max memory_allocated 29271.27197265625 
[2025-03-22 17:10:22 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.39283716678619385 norm:0.0010008033132180572 max memory_allocated 29271.27197265625 
[2025-03-22 17:11:10 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.3925977051258087 norm:0.001003878191113472 max memory_allocated 29271.27197265625 
[2025-03-22 17:11:58 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.3921950161457062 norm:0.0009901137091219425 max memory_allocated 29271.27197265625 
[2025-03-22 17:12:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.39202702045440674 norm:0.0009711486054584384 max memory_allocated 29271.27197265625 
[2025-03-22 17:13:34 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.39168551564216614 norm:0.0009616836323402822 max memory_allocated 29271.27197265625 
[2025-03-22 17:14:21 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.3914262056350708 norm:0.0009489000076428056 max memory_allocated 29271.27197265625 
[2025-03-22 17:15:10 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.3913070559501648 norm:0.0009498686413280666 max memory_allocated 29271.27197265625 
[2025-03-22 17:15:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 17:16:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.5303831100463867 norm:0.016905123367905617 max memory_allocated 29271.27197265625 
[2025-03-22 17:17:03 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.4940308928489685 norm:0.009498003870248795 max memory_allocated 29271.27197265625 
[2025-03-22 17:17:51 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.44805291295051575 norm:0.003961753565818071 max memory_allocated 29271.27197265625 
[2025-03-22 17:18:39 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.43152666091918945 norm:0.0020119280088692904 max memory_allocated 29271.27197265625 
[2025-03-22 17:19:27 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.4254303276538849 norm:0.0014823441160842776 max memory_allocated 29271.27197265625 
[2025-03-22 17:20:15 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.4213194251060486 norm:0.0012089491356164217 max memory_allocated 29271.27197265625 
[2025-03-22 17:21:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.4185527563095093 norm:0.0011033940827473998 max memory_allocated 29271.27197265625 
[2025-03-22 17:21:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.41666609048843384 norm:0.0010535644832998514 max memory_allocated 29271.27197265625 
[2025-03-22 17:22:40 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.41526418924331665 norm:0.0010235966183245182 max memory_allocated 29271.27197265625 
[2025-03-22 17:23:28 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.4140135645866394 norm:0.0009851481299847364 max memory_allocated 29271.27197265625 
[2025-03-22 17:24:16 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.41312843561172485 norm:0.0009686359553597867 max memory_allocated 29271.27197265625 
[2025-03-22 17:25:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.4124539792537689 norm:0.0009604475344531238 max memory_allocated 29271.27197265625 
[2025-03-22 17:25:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.4119487702846527 norm:0.0009375025983899832 max memory_allocated 29271.27197265625 
[2025-03-22 17:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.4115029573440552 norm:0.000939401681534946 max memory_allocated 29271.27197265625 
[2025-03-22 17:27:28 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.41120871901512146 norm:0.0009311127942055464 max memory_allocated 29271.27197265625 
[2025-03-22 17:28:16 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.41091474890708923 norm:0.0009269634028896689 max memory_allocated 29271.27197265625 
[2025-03-22 17:29:04 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.4107150733470917 norm:0.000922349514439702 max memory_allocated 29271.27197265625 
[2025-03-22 17:29:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.41060250997543335 norm:0.0009380425326526165 max memory_allocated 29271.27197265625 
[2025-03-22 17:30:40 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.4104664623737335 norm:0.0009413756197318435 max memory_allocated 29271.27197265625 
[2025-03-22 17:31:28 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.41033196449279785 norm:0.0009348518215119839 max memory_allocated 29271.27197265625 
[2025-03-22 17:31:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 17:32:33 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.5404083132743835 norm:0.030583154410123825 max memory_allocated 29271.27197265625 
[2025-03-22 17:33:21 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.5057114958763123 norm:0.01274100411683321 max memory_allocated 29271.27197265625 
[2025-03-22 17:34:09 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.4684966206550598 norm:0.004398590885102749 max memory_allocated 29271.27197265625 
[2025-03-22 17:34:57 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.4515337646007538 norm:0.002153499983251095 max memory_allocated 29271.27197265625 
[2025-03-22 17:35:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.4456501007080078 norm:0.0016139092622324824 max memory_allocated 29271.27197265625 
[2025-03-22 17:36:34 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.4417622685432434 norm:0.0013495908351615071 max memory_allocated 29271.27197265625 
[2025-03-22 17:37:22 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.4389616847038269 norm:0.0012323635164648294 max memory_allocated 29271.27197265625 
[2025-03-22 17:38:10 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.43690022826194763 norm:0.001147157629020512 max memory_allocated 29271.27197265625 
[2025-03-22 17:38:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.43543165922164917 norm:0.001086471602320671 max memory_allocated 29271.27197265625 
[2025-03-22 17:39:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.43422359228134155 norm:0.0010367394424974918 max memory_allocated 29271.27197265625 
[2025-03-22 17:40:34 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.43348151445388794 norm:0.0010284378658980131 max memory_allocated 29271.27197265625 
[2025-03-22 17:41:23 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.43299600481987 norm:0.0010121555533260107 max memory_allocated 29271.27197265625 
[2025-03-22 17:42:11 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.4326794743537903 norm:0.0010072843870148063 max memory_allocated 29271.27197265625 
[2025-03-22 17:42:59 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.4324319362640381 norm:0.0010023085633292794 max memory_allocated 29271.27197265625 
[2025-03-22 17:43:47 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.4319881796836853 norm:0.000971476431004703 max memory_allocated 29271.27197265625 
[2025-03-22 17:44:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.4316271245479584 norm:0.0009461060399189591 max memory_allocated 29271.27197265625 
[2025-03-22 17:45:23 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.4312885105609894 norm:0.000929413887206465 max memory_allocated 29271.27197265625 
[2025-03-22 17:46:11 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.43096864223480225 norm:0.0009141326299868524 max memory_allocated 29271.27197265625 
[2025-03-22 17:46:59 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.43073102831840515 norm:0.0008781156502664089 max memory_allocated 29271.27197265625 
[2025-03-22 17:47:47 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.43056806921958923 norm:0.0008683179621584713 max memory_allocated 29271.27197265625 
[2025-03-22 17:48:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 17:48:52 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.5621519088745117 norm:0.020121896639466286 max memory_allocated 29271.27197265625 
[2025-03-22 17:49:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.5327048301696777 norm:0.009486236609518528 max memory_allocated 29271.27197265625 
[2025-03-22 17:50:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.4908447563648224 norm:0.003918918780982494 max memory_allocated 29271.27197265625 
[2025-03-22 17:51:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.47386664152145386 norm:0.0018814284121617675 max memory_allocated 29271.27197265625 
[2025-03-22 17:52:04 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.4679945111274719 norm:0.0014134998200461268 max memory_allocated 29271.27197265625 
[2025-03-22 17:52:52 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.4640357196331024 norm:0.0011718147434294224 max memory_allocated 29271.27197265625 
[2025-03-22 17:53:40 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.461221307516098 norm:0.0010534487664699554 max memory_allocated 29271.27197265625 
[2025-03-22 17:54:28 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.45926499366760254 norm:0.0009985575452446938 max memory_allocated 29271.27197265625 
[2025-03-22 17:55:16 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.4579734802246094 norm:0.0009761301917023957 max memory_allocated 29271.27197265625 
[2025-03-22 17:56:04 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.45680761337280273 norm:0.0009367864695377648 max memory_allocated 29271.27197265625 
[2025-03-22 17:56:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.4560525417327881 norm:0.0009257944766432047 max memory_allocated 29271.27197265625 
[2025-03-22 17:57:41 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.45540329813957214 norm:0.0009190445416606963 max memory_allocated 29271.27197265625 
[2025-03-22 17:58:29 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.45486903190612793 norm:0.0009038902935571969 max memory_allocated 29271.27197265625 
[2025-03-22 17:59:17 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.45451128482818604 norm:0.0009123051422648132 max memory_allocated 29271.27197265625 
[2025-03-22 18:00:05 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.4541846215724945 norm:0.0009002891601994634 max memory_allocated 29271.27197265625 
[2025-03-22 18:00:53 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.45402124524116516 norm:0.0008957968675531447 max memory_allocated 29271.27197265625 
[2025-03-22 18:01:41 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.45358186960220337 norm:0.0008857112261466682 max memory_allocated 29271.27197265625 
[2025-03-22 18:02:29 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.453299343585968 norm:0.000869014416821301 max memory_allocated 29271.27197265625 
[2025-03-22 18:03:17 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.45298635959625244 norm:0.0008454406051896513 max memory_allocated 29271.27197265625 
[2025-03-22 18:04:05 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.4528215825557709 norm:0.0008363684173673391 max memory_allocated 29271.27197265625 
[2025-03-22 18:04:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 18:05:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.5781537294387817 norm:0.022573919966816902 max memory_allocated 29271.27197265625 
[2025-03-22 18:05:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.5464326739311218 norm:0.00912416074424982 max memory_allocated 29271.27197265625 
[2025-03-22 18:06:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.5128737688064575 norm:0.003986135590821505 max memory_allocated 29271.27197265625 
[2025-03-22 18:07:34 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.4966500699520111 norm:0.0019004388013854623 max memory_allocated 29271.27197265625 
[2025-03-22 18:08:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.49035879969596863 norm:0.0013828756054863334 max memory_allocated 29271.27197265625 
[2025-03-22 18:09:10 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.4862431585788727 norm:0.001197556033730507 max memory_allocated 29271.27197265625 
[2025-03-22 18:09:58 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.48325660824775696 norm:0.0010925548849627376 max memory_allocated 29271.27197265625 
[2025-03-22 18:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.4810675084590912 norm:0.0010204972932115197 max memory_allocated 29271.27197265625 
[2025-03-22 18:11:34 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.47954967617988586 norm:0.000983391422778368 max memory_allocated 29271.27197265625 
[2025-03-22 18:12:22 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.47825926542282104 norm:0.000960349221713841 max memory_allocated 29271.27197265625 
[2025-03-22 18:13:11 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.4772590696811676 norm:0.0009447747142985463 max memory_allocated 29271.27197265625 
[2025-03-22 18:13:59 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.476628839969635 norm:0.0009495841222815216 max memory_allocated 29271.27197265625 
[2025-03-22 18:14:47 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.47609829902648926 norm:0.000943188788369298 max memory_allocated 29271.27197265625 
[2025-03-22 18:15:35 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.4756838083267212 norm:0.0009437450207769871 max memory_allocated 29271.27197265625 
[2025-03-22 18:16:23 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.47530147433280945 norm:0.0009377180831506848 max memory_allocated 29271.27197265625 
[2025-03-22 18:17:11 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.4748535752296448 norm:0.0009223519591614604 max memory_allocated 29271.27197265625 
[2025-03-22 18:17:59 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.4744652807712555 norm:0.0009103460470214486 max memory_allocated 29271.27197265625 
[2025-03-22 18:18:47 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.47420820593833923 norm:0.0009058616124093533 max memory_allocated 29271.27197265625 
[2025-03-22 18:19:35 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.47393980622291565 norm:0.0009077376453205943 max memory_allocated 29271.27197265625 
[2025-03-22 18:20:23 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.47362053394317627 norm:0.0008981795981526375 max memory_allocated 29271.27197265625 
[2025-03-22 18:20:37 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 18:21:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.5819092392921448 norm:0.008347160182893276 max memory_allocated 29271.39697265625 
[2025-03-22 18:22:16 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.5603115558624268 norm:0.00533944321796298 max memory_allocated 29271.39697265625 
[2025-03-22 18:23:04 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.5319778919219971 norm:0.0029882544185966253 max memory_allocated 29271.39697265625 
[2025-03-22 18:23:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.517686665058136 norm:0.001700031803920865 max memory_allocated 29271.39697265625 
[2025-03-22 18:24:40 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.5117157101631165 norm:0.0013268321054056287 max memory_allocated 29271.39697265625 
[2025-03-22 18:25:28 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.5079498291015625 norm:0.0011787766125053167 max memory_allocated 29271.39697265625 
[2025-03-22 18:26:16 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.5051850080490112 norm:0.0011165286414325237 max memory_allocated 29271.39697265625 
[2025-03-22 18:27:04 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.5031272172927856 norm:0.0010652157943695784 max memory_allocated 29271.39697265625 
[2025-03-22 18:27:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.5015944242477417 norm:0.0010476545430719852 max memory_allocated 29271.39697265625 
[2025-03-22 18:28:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.5002666115760803 norm:0.0010275470558553934 max memory_allocated 29271.39697265625 
[2025-03-22 18:29:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.4992711842060089 norm:0.0010225288569927216 max memory_allocated 29271.39697265625 
[2025-03-22 18:30:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.4984271824359894 norm:0.0010058156913146377 max memory_allocated 29271.39697265625 
[2025-03-22 18:31:05 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.49778345227241516 norm:0.0009916563285514712 max memory_allocated 29271.39697265625 
[2025-03-22 18:31:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.49714744091033936 norm:0.0009782317792996764 max memory_allocated 29271.39697265625 
[2025-03-22 18:32:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.49661752581596375 norm:0.0009696810739114881 max memory_allocated 29271.39697265625 
[2025-03-22 18:33:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.49621132016181946 norm:0.0009636658942326903 max memory_allocated 29271.39697265625 
[2025-03-22 18:34:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.4957934319972992 norm:0.0009536466095596552 max memory_allocated 29271.39697265625 
[2025-03-22 18:35:05 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.4955192804336548 norm:0.000956484058406204 max memory_allocated 29271.39697265625 
[2025-03-22 18:35:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.4952808618545532 norm:0.0009567576926201582 max memory_allocated 29271.39697265625 
[2025-03-22 18:36:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.49504852294921875 norm:0.0009502735920250416 max memory_allocated 29271.39697265625 
[2025-03-22 18:36:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 18:37:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.6335809230804443 norm:0.03254002332687378 max memory_allocated 29271.58447265625 
[2025-03-22 18:38:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.6043357253074646 norm:0.01739375852048397 max memory_allocated 29271.58447265625 
[2025-03-22 18:39:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.568426251411438 norm:0.008941047824919224 max memory_allocated 29271.58447265625 
[2025-03-22 18:40:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.5496154427528381 norm:0.005216303747147322 max memory_allocated 29271.58447265625 
[2025-03-22 18:40:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.5400494337081909 norm:0.003203324042260647 max memory_allocated 29271.58447265625 
[2025-03-22 18:41:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.5346379280090332 norm:0.002012516837567091 max memory_allocated 29271.58447265625 
[2025-03-22 18:42:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.5309371948242188 norm:0.0016567179700359702 max memory_allocated 29271.58447265625 
[2025-03-22 18:43:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.5283171534538269 norm:0.0015255298931151628 max memory_allocated 29271.58447265625 
[2025-03-22 18:44:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.5260811448097229 norm:0.0012534457491710782 max memory_allocated 29271.58447265625 
[2025-03-22 18:44:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.5245676040649414 norm:0.0011650824453681707 max memory_allocated 29271.58447265625 
[2025-03-22 18:45:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.5233479738235474 norm:0.0011400759685784578 max memory_allocated 29271.58447265625 
[2025-03-22 18:46:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.5224135518074036 norm:0.0011110272025689483 max memory_allocated 29271.58447265625 
[2025-03-22 18:47:23 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.5215869545936584 norm:0.0010803204495459795 max memory_allocated 29271.58447265625 
[2025-03-22 18:48:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.5209968090057373 norm:0.001077716238796711 max memory_allocated 29271.58447265625 
[2025-03-22 18:48:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.5204983949661255 norm:0.0010669971816241741 max memory_allocated 29271.58447265625 
[2025-03-22 18:49:47 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.5199800729751587 norm:0.0010441027116030455 max memory_allocated 29271.58447265625 
[2025-03-22 18:50:35 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.5196784138679504 norm:0.0010381904430687428 max memory_allocated 29271.58447265625 
[2025-03-22 18:51:24 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.5193386077880859 norm:0.001022636191919446 max memory_allocated 29271.58447265625 
[2025-03-22 18:52:12 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.5189664363861084 norm:0.0010073252487927675 max memory_allocated 29271.58447265625 
[2025-03-22 18:53:00 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.5185768604278564 norm:0.0009927044156938791 max memory_allocated 29271.58447265625 
[2025-03-22 18:53:14 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 18:54:05 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.6626800894737244 norm:0.07540397346019745 max memory_allocated 29271.77197265625 
[2025-03-22 18:54:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.6367731690406799 norm:0.038396045565605164 max memory_allocated 29271.77197265625 
[2025-03-22 18:55:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.605459988117218 norm:0.01683250069618225 max memory_allocated 29271.77197265625 
[2025-03-22 18:56:29 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.5863260626792908 norm:0.007657936774194241 max memory_allocated 29271.77197265625 
[2025-03-22 18:57:17 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.5783953666687012 norm:0.005227710120379925 max memory_allocated 29271.77197265625 
[2025-03-22 18:58:05 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.5731151103973389 norm:0.0035513113252818584 max memory_allocated 29271.77197265625 
[2025-03-22 18:58:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.5699378252029419 norm:0.0032302956096827984 max memory_allocated 29271.77197265625 
[2025-03-22 18:59:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.5675339698791504 norm:0.0029931035824120045 max memory_allocated 29271.77197265625 
[2025-03-22 19:00:29 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.565796971321106 norm:0.0028389720246195793 max memory_allocated 29271.77197265625 
[2025-03-22 19:01:17 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.5635470151901245 norm:0.0020243532489985228 max memory_allocated 29271.77197265625 
[2025-03-22 19:02:05 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.5620912909507751 norm:0.001771730137988925 max memory_allocated 29271.77197265625 
[2025-03-22 19:02:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.5608516335487366 norm:0.001691118930466473 max memory_allocated 29271.77197265625 
[2025-03-22 19:03:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.5597242712974548 norm:0.0015844422159716487 max memory_allocated 29271.77197265625 
[2025-03-22 19:04:29 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.5590243339538574 norm:0.0015530334785580635 max memory_allocated 29271.77197265625 
[2025-03-22 19:05:17 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.5584731101989746 norm:0.0015242521185427904 max memory_allocated 29271.77197265625 
[2025-03-22 19:06:05 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.5578799843788147 norm:0.0014572323998436332 max memory_allocated 29271.77197265625 
[2025-03-22 19:06:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.5575065612792969 norm:0.0014530594926327467 max memory_allocated 29271.77197265625 
[2025-03-22 19:07:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.5569749474525452 norm:0.001401835703290999 max memory_allocated 29271.77197265625 
[2025-03-22 19:08:29 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.5566010475158691 norm:0.0013667616294696927 max memory_allocated 29271.77197265625 
[2025-03-22 19:09:17 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.556341826915741 norm:0.0013483818620443344 max memory_allocated 29271.77197265625 
[2025-03-22 19:09:31 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 19:10:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.6697012782096863 norm:0.017483113333582878 max memory_allocated 29271.95947265625 
[2025-03-22 19:11:11 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.6499677300453186 norm:0.010806628502905369 max memory_allocated 29271.95947265625 
[2025-03-22 19:11:59 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.6243504285812378 norm:0.00586666539311409 max memory_allocated 29271.95947265625 
[2025-03-22 19:12:47 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.6098694801330566 norm:0.002908530179411173 max memory_allocated 29271.95947265625 
[2025-03-22 19:13:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.6039904356002808 norm:0.001957325730472803 max memory_allocated 29271.95947265625 
[2025-03-22 19:14:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.6006276607513428 norm:0.0015919774305075407 max memory_allocated 29271.95947265625 
[2025-03-22 19:15:11 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.5983452796936035 norm:0.0014258656883612275 max memory_allocated 29271.95947265625 
[2025-03-22 19:15:59 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.5965346693992615 norm:0.0013405270874500275 max memory_allocated 29271.95947265625 
[2025-03-22 19:16:47 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.5946646332740784 norm:0.0012182523496448994 max memory_allocated 29271.95947265625 
[2025-03-22 19:17:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.5929229259490967 norm:0.0011193715035915375 max memory_allocated 29271.95947265625 
[2025-03-22 19:18:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.5916842222213745 norm:0.0010608285665512085 max memory_allocated 29271.95947265625 
[2025-03-22 19:19:11 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.5908043384552002 norm:0.0010347526986151934 max memory_allocated 29271.95947265625 
[2025-03-22 19:19:59 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.5900192856788635 norm:0.0009971186518669128 max memory_allocated 29271.95947265625 
[2025-03-22 19:20:47 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.5894453525543213 norm:0.0009962160838767886 max memory_allocated 29271.95947265625 
[2025-03-22 19:21:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.5888975262641907 norm:0.0009775315411388874 max memory_allocated 29271.95947265625 
[2025-03-22 19:22:23 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.5883741974830627 norm:0.0009602062636986375 max memory_allocated 29271.95947265625 
[2025-03-22 19:23:11 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.5879654884338379 norm:0.0009463608730584383 max memory_allocated 29271.95947265625 
[2025-03-22 19:23:59 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.5876350402832031 norm:0.0009504336630925536 max memory_allocated 29271.95947265625 
[2025-03-22 19:24:47 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.5874233245849609 norm:0.0009393409127369523 max memory_allocated 29271.95947265625 
[2025-03-22 19:25:36 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.5872550010681152 norm:0.0009450376383028924 max memory_allocated 29271.95947265625 
[2025-03-22 19:25:49 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 19:26:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.7141797542572021 norm:0.013586161658167839 max memory_allocated 29272.14697265625 
[2025-03-22 19:27:29 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.6943400502204895 norm:0.008369037881493568 max memory_allocated 29272.14697265625 
[2025-03-22 19:28:17 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.6660692095756531 norm:0.0045737940818071365 max memory_allocated 29272.14697265625 
[2025-03-22 19:29:05 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.6528098583221436 norm:0.0026728149969130754 max memory_allocated 29272.14697265625 
[2025-03-22 19:29:54 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.6474111080169678 norm:0.00199818704277277 max memory_allocated 29272.14697265625 
[2025-03-22 19:30:42 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.6440621018409729 norm:0.0016402550972998142 max memory_allocated 29272.14697265625 
[2025-03-22 19:31:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.6412033438682556 norm:0.0014174232492223382 max memory_allocated 29272.14697265625 
[2025-03-22 19:32:18 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.6389655470848083 norm:0.001296775764785707 max memory_allocated 29272.14697265625 
[2025-03-22 19:33:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.637298047542572 norm:0.0012274387991055846 max memory_allocated 29272.14697265625 
[2025-03-22 19:33:54 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.6358441710472107 norm:0.0011812265729531646 max memory_allocated 29272.14697265625 
[2025-03-22 19:34:42 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.6348325610160828 norm:0.001157160964794457 max memory_allocated 29272.14697265625 
[2025-03-22 19:35:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.6340452432632446 norm:0.001117975334636867 max memory_allocated 29272.14697265625 
[2025-03-22 19:36:18 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.6331268548965454 norm:0.0010780580341815948 max memory_allocated 29272.14697265625 
[2025-03-22 19:37:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.6325085163116455 norm:0.0010572751052677631 max memory_allocated 29272.14697265625 
[2025-03-22 19:37:54 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.631866455078125 norm:0.0010331823723390698 max memory_allocated 29272.14697265625 
[2025-03-22 19:38:42 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.6314034461975098 norm:0.0010050003184005618 max memory_allocated 29272.14697265625 
[2025-03-22 19:39:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.630957305431366 norm:0.0009871957590803504 max memory_allocated 29272.14697265625 
[2025-03-22 19:40:18 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.6305806636810303 norm:0.000972387904766947 max memory_allocated 29272.14697265625 
[2025-03-22 19:41:06 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.6302851438522339 norm:0.0009782935958355665 max memory_allocated 29272.14697265625 
[2025-03-22 19:41:54 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.6299904584884644 norm:0.0009743834962137043 max memory_allocated 29272.14697265625 
[2025-03-22 19:42:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 19:43:00 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.8033828735351562 norm:0.059800706803798676 max memory_allocated 29272.33447265625 
[2025-03-22 19:43:48 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.7773547172546387 norm:0.034110236912965775 max memory_allocated 29272.33447265625 
[2025-03-22 19:44:36 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.7441691160202026 norm:0.017302900552749634 max memory_allocated 29272.33447265625 
[2025-03-22 19:45:25 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.7236987948417664 norm:0.008812524378299713 max memory_allocated 29272.33447265625 
[2025-03-22 19:46:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.7160112261772156 norm:0.005914755165576935 max memory_allocated 29272.33447265625 
[2025-03-22 19:47:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.7119524478912354 norm:0.004840672481805086 max memory_allocated 29272.33447265625 
[2025-03-22 19:47:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.7083789110183716 norm:0.0038210360798984766 max memory_allocated 29272.33447265625 
[2025-03-22 19:48:37 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.7056308388710022 norm:0.003217541379854083 max memory_allocated 29272.33447265625 
[2025-03-22 19:49:25 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.7030261754989624 norm:0.0025268467143177986 max memory_allocated 29272.33447265625 
[2025-03-22 19:50:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.7015863656997681 norm:0.0025700489059090614 max memory_allocated 29272.33447265625 
[2025-03-22 19:51:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.7003212571144104 norm:0.0024377971421927214 max memory_allocated 29272.33447265625 
[2025-03-22 19:51:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.6992637515068054 norm:0.0022700235713273287 max memory_allocated 29272.33447265625 
[2025-03-22 19:52:37 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.6985092163085938 norm:0.0021967326756566763 max memory_allocated 29272.33447265625 
[2025-03-22 19:53:25 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.6975619792938232 norm:0.0020432439632713795 max memory_allocated 29272.33447265625 
[2025-03-22 19:54:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.6967721581459045 norm:0.0019606025889515877 max memory_allocated 29272.33447265625 
[2025-03-22 19:55:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.695915699005127 norm:0.001834463095292449 max memory_allocated 29272.33447265625 
[2025-03-22 19:55:49 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.6951522827148438 norm:0.0016851474065333605 max memory_allocated 29272.33447265625 
[2025-03-22 19:56:37 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.6940032839775085 norm:0.0012337900698184967 max memory_allocated 29272.33447265625 
[2025-03-22 19:57:25 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.6935787796974182 norm:0.0012317767832428217 max memory_allocated 29272.33447265625 
[2025-03-22 19:58:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.6931962370872498 norm:0.0011996857356280088 max memory_allocated 29272.33447265625 
[2025-03-22 19:58:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 19:59:18 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.8416581153869629 norm:0.018797002732753754 max memory_allocated 29272.52197265625 
[2025-03-22 20:00:06 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.8219929933547974 norm:0.010322785936295986 max memory_allocated 29272.52197265625 
[2025-03-22 20:00:54 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.7999328970909119 norm:0.005857780110090971 max memory_allocated 29272.52197265625 
[2025-03-22 20:01:43 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.787691056728363 norm:0.0034117295872420073 max memory_allocated 29272.52197265625 
[2025-03-22 20:02:31 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.7823705673217773 norm:0.0027047297917306423 max memory_allocated 29272.52197265625 
[2025-03-22 20:03:19 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.7794809341430664 norm:0.0024482219014316797 max memory_allocated 29272.52197265625 
[2025-03-22 20:04:07 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.776506781578064 norm:0.001595610985532403 max memory_allocated 29272.52197265625 
[2025-03-22 20:04:55 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.7745899558067322 norm:0.0014789315173402429 max memory_allocated 29272.52197265625 
[2025-03-22 20:05:43 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.773062527179718 norm:0.0014358765911310911 max memory_allocated 29272.52197265625 
[2025-03-22 20:06:31 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.771752119064331 norm:0.0014092010678723454 max memory_allocated 29272.52197265625 
[2025-03-22 20:07:19 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.7705953121185303 norm:0.001349041354842484 max memory_allocated 29272.52197265625 
[2025-03-22 20:08:07 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.7694342732429504 norm:0.001305971061810851 max memory_allocated 29272.52197265625 
[2025-03-22 20:08:55 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.7685954570770264 norm:0.0012836637906730175 max memory_allocated 29272.52197265625 
[2025-03-22 20:09:43 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.7678173780441284 norm:0.001254126662388444 max memory_allocated 29272.52197265625 
[2025-03-22 20:10:30 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.7671893835067749 norm:0.0012410067720338702 max memory_allocated 29272.52197265625 
[2025-03-22 20:11:18 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.7666619420051575 norm:0.0012290769955143332 max memory_allocated 29272.52197265625 
[2025-03-22 20:12:06 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.7662271857261658 norm:0.001207473804242909 max memory_allocated 29272.52197265625 
[2025-03-22 20:12:54 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.7658523917198181 norm:0.0011988641927018762 max memory_allocated 29272.52197265625 
[2025-03-22 20:13:42 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.7656071186065674 norm:0.0011925948783755302 max memory_allocated 29272.52197265625 
[2025-03-22 20:14:31 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.7651698589324951 norm:0.0011721063638105989 max memory_allocated 29272.52197265625 
[2025-03-22 20:14:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 20:15:36 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.9324949979782104 norm:0.03333562612533569 max memory_allocated 29272.70947265625 
[2025-03-22 20:16:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.9084815979003906 norm:0.019425971433520317 max memory_allocated 29272.70947265625 
[2025-03-22 20:17:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.881538450717926 norm:0.011231837794184685 max memory_allocated 29272.70947265625 
[2025-03-22 20:18:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.8667315244674683 norm:0.0059515018947422504 max memory_allocated 29272.70947265625 
[2025-03-22 20:18:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.8609193563461304 norm:0.004712529480457306 max memory_allocated 29272.70947265625 
[2025-03-22 20:19:37 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.8573147654533386 norm:0.0038905253168195486 max memory_allocated 29272.70947265625 
[2025-03-22 20:20:25 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.8545240759849548 norm:0.0033820581156760454 max memory_allocated 29272.70947265625 
[2025-03-22 20:21:13 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.8523796796798706 norm:0.0030799605883657932 max memory_allocated 29272.70947265625 
[2025-03-22 20:22:01 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.8506309986114502 norm:0.002763675758615136 max memory_allocated 29272.70947265625 
[2025-03-22 20:22:49 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.8490450382232666 norm:0.00239545083604753 max memory_allocated 29272.70947265625 
[2025-03-22 20:23:37 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.847886323928833 norm:0.002210608683526516 max memory_allocated 29272.70947265625 
[2025-03-22 20:24:25 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.8465943932533264 norm:0.0020120057743042707 max memory_allocated 29272.70947265625 
[2025-03-22 20:25:13 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.8457843661308289 norm:0.0019562486559152603 max memory_allocated 29272.70947265625 
[2025-03-22 20:26:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.844347357749939 norm:0.0014139133272692561 max memory_allocated 29272.70947265625 
[2025-03-22 20:26:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.8435816168785095 norm:0.0012411253992468119 max memory_allocated 29272.70947265625 
[2025-03-22 20:27:36 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.8431333303451538 norm:0.0012382443528622389 max memory_allocated 29272.70947265625 
[2025-03-22 20:28:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.842482328414917 norm:0.0012132397387176752 max memory_allocated 29272.70947265625 
[2025-03-22 20:29:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.8420796990394592 norm:0.0011972006177529693 max memory_allocated 29272.70947265625 
[2025-03-22 20:30:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.8415497541427612 norm:0.0011784437811002135 max memory_allocated 29272.70947265625 
[2025-03-22 20:30:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.8412589430809021 norm:0.0011693056439980865 max memory_allocated 29272.70947265625 
[2025-03-22 20:31:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 20:31:54 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:1.0215766429901123 norm:0.012069839984178543 max memory_allocated 29272.89697265625 
[2025-03-22 20:32:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:1.000174880027771 norm:0.007140306290239096 max memory_allocated 29272.89697265625 
[2025-03-22 20:33:30 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.9746142625808716 norm:0.0037838572170585394 max memory_allocated 29272.89697265625 
[2025-03-22 20:34:18 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.9642500877380371 norm:0.0021485376637429 max memory_allocated 29272.89697265625 
[2025-03-22 20:35:06 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.960026204586029 norm:0.001774204196408391 max memory_allocated 29272.89697265625 
[2025-03-22 20:35:54 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.9572232961654663 norm:0.0016594448825344443 max memory_allocated 29272.89697265625 
[2025-03-22 20:36:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.9548609256744385 norm:0.0015732801984995604 max memory_allocated 29272.89697265625 
[2025-03-22 20:37:31 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.9531065821647644 norm:0.0015292420284822583 max memory_allocated 29272.89697265625 
[2025-03-22 20:38:19 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.9516003131866455 norm:0.0014963429421186447 max memory_allocated 29272.89697265625 
[2025-03-22 20:39:07 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.950219452381134 norm:0.0014574506785720587 max memory_allocated 29272.89697265625 
[2025-03-22 20:39:55 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.9493457674980164 norm:0.0014406549744307995 max memory_allocated 29272.89697265625 
[2025-03-22 20:40:43 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.9485918879508972 norm:0.0014385682297870517 max memory_allocated 29272.89697265625 
[2025-03-22 20:41:30 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.9478492140769958 norm:0.0014033440966159105 max memory_allocated 29272.89697265625 
[2025-03-22 20:42:18 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.9471112489700317 norm:0.0013934882590547204 max memory_allocated 29272.89697265625 
[2025-03-22 20:43:06 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.9464823603630066 norm:0.001368289114907384 max memory_allocated 29272.89697265625 
[2025-03-22 20:43:54 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.9459368586540222 norm:0.0013330309884622693 max memory_allocated 29272.89697265625 
[2025-03-22 20:44:42 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.945417046546936 norm:0.0013153100153431296 max memory_allocated 29272.89697265625 
[2025-03-22 20:45:30 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.9450562596321106 norm:0.0012962514301761985 max memory_allocated 29272.89697265625 
[2025-03-22 20:46:18 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.9445586204528809 norm:0.0012792155612260103 max memory_allocated 29272.89697265625 
[2025-03-22 20:47:06 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.9442201852798462 norm:0.0012767253210768104 max memory_allocated 29272.89697265625 
[2025-03-22 20:47:20 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 20:48:12 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:1.1482958793640137 norm:0.019626330584287643 max memory_allocated 29273.08447265625 
[2025-03-22 20:49:00 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:1.128063678741455 norm:0.010659422725439072 max memory_allocated 29273.08447265625 
[2025-03-22 20:49:49 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:1.1012030839920044 norm:0.005735128186643124 max memory_allocated 29273.08447265625 
[2025-03-22 20:50:37 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:1.08882737159729 norm:0.0028974527958780527 max memory_allocated 29273.08447265625 
[2025-03-22 20:51:25 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:1.0834325551986694 norm:0.0019310907227918506 max memory_allocated 29273.08447265625 
[2025-03-22 20:52:13 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:1.0800193548202515 norm:0.0018079973524436355 max memory_allocated 29273.08447265625 
[2025-03-22 20:53:02 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:1.0774002075195312 norm:0.001738671911880374 max memory_allocated 29273.08447265625 
[2025-03-22 20:53:50 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:1.0753446817398071 norm:0.0016491208225488663 max memory_allocated 29273.08447265625 
[2025-03-22 20:54:38 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:1.0738236904144287 norm:0.0015815689694136381 max memory_allocated 29273.08447265625 
[2025-03-22 20:55:26 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:1.0725114345550537 norm:0.0015398492105305195 max memory_allocated 29273.08447265625 
[2025-03-22 20:56:15 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:1.071424961090088 norm:0.0014973371289670467 max memory_allocated 29273.08447265625 
[2025-03-22 20:57:03 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:1.0705257654190063 norm:0.0014852090971544385 max memory_allocated 29273.08447265625 
[2025-03-22 20:57:51 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:1.0697845220565796 norm:0.0014716677833348513 max memory_allocated 29273.08447265625 
[2025-03-22 20:58:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:1.069157361984253 norm:0.0014516422525048256 max memory_allocated 29273.08447265625 
[2025-03-22 20:59:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:1.0686116218566895 norm:0.0014418690698221326 max memory_allocated 29273.08447265625 
[2025-03-22 21:00:15 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:1.0681253671646118 norm:0.0014198184944689274 max memory_allocated 29273.08447265625 
[2025-03-22 21:01:03 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:1.0678296089172363 norm:0.0014078314416110516 max memory_allocated 29273.08447265625 
[2025-03-22 21:01:51 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:1.067571759223938 norm:0.0014007618883624673 max memory_allocated 29273.08447265625 
[2025-03-22 21:02:39 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:1.0672742128372192 norm:0.0013996671186760068 max memory_allocated 29273.08447265625 
[2025-03-22 21:03:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:1.0670037269592285 norm:0.0014042516704648733 max memory_allocated 29273.08447265625 
[2025-03-22 21:03:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 21:04:33 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:1.289713740348816 norm:0.019928734749555588 max memory_allocated 29273.27197265625 
[2025-03-22 21:05:21 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:1.271104335784912 norm:0.01197130512446165 max memory_allocated 29273.27197265625 
[2025-03-22 21:06:09 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:1.2481224536895752 norm:0.006446570158004761 max memory_allocated 29273.27197265625 
[2025-03-22 21:06:57 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:1.2373642921447754 norm:0.0025657115038484335 max memory_allocated 29273.27197265625 
[2025-03-22 21:07:46 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:1.2329070568084717 norm:0.0021104763727635145 max memory_allocated 29273.27197265625 
[2025-03-22 21:08:34 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:1.2298674583435059 norm:0.001977731939405203 max memory_allocated 29273.27197265625 
[2025-03-22 21:09:22 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:1.2273681163787842 norm:0.0019327563932165504 max memory_allocated 29273.27197265625 
[2025-03-22 21:10:10 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:1.2253268957138062 norm:0.001867564395070076 max memory_allocated 29273.27197265625 
[2025-03-22 21:10:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:1.2237541675567627 norm:0.0018244922393932939 max memory_allocated 29273.27197265625 
[2025-03-22 21:11:47 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:1.222412109375 norm:0.001782817766070366 max memory_allocated 29273.27197265625 
[2025-03-22 21:12:35 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:1.2212388515472412 norm:0.0017688199877738953 max memory_allocated 29273.27197265625 
[2025-03-22 21:13:23 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:1.2203080654144287 norm:0.0017539754044264555 max memory_allocated 29273.27197265625 
[2025-03-22 21:14:11 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:1.2194492816925049 norm:0.0017342917853966355 max memory_allocated 29273.27197265625 
[2025-03-22 21:15:00 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:1.2186882495880127 norm:0.0017051572212949395 max memory_allocated 29273.27197265625 
[2025-03-22 21:15:48 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:1.2181103229522705 norm:0.0017084726132452488 max memory_allocated 29273.27197265625 
[2025-03-22 21:16:36 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:1.2175756692886353 norm:0.0017057594377547503 max memory_allocated 29273.27197265625 
[2025-03-22 21:17:25 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:1.2170491218566895 norm:0.001676223473623395 max memory_allocated 29273.27197265625 
[2025-03-22 21:18:13 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:1.2166484594345093 norm:0.0016713953809812665 max memory_allocated 29273.27197265625 
[2025-03-22 21:19:01 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:1.2162621021270752 norm:0.0016577853821218014 max memory_allocated 29273.27197265625 
[2025-03-22 21:19:49 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:1.2159278392791748 norm:0.0016479217447340488 max memory_allocated 29273.27197265625 
[2025-03-22 21:20:03 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 21:20:55 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:1.4287188053131104 norm:0.012939771637320518 max memory_allocated 29273.45947265625 
[2025-03-22 21:21:43 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:1.4110125303268433 norm:0.007499543484300375 max memory_allocated 29273.45947265625 
[2025-03-22 21:22:32 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:1.3922845125198364 norm:0.005020015873014927 max memory_allocated 29273.45947265625 
[2025-03-22 21:23:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:1.384645938873291 norm:0.0036682188510894775 max memory_allocated 29273.45947265625 
[2025-03-22 21:24:08 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:1.3802365064620972 norm:0.003043963573873043 max memory_allocated 29273.45947265625 
[2025-03-22 21:24:56 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:1.3768939971923828 norm:0.0027225250378251076 max memory_allocated 29273.45947265625 
[2025-03-22 21:25:45 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:1.374086856842041 norm:0.0023901769891381264 max memory_allocated 29273.45947265625 
[2025-03-22 21:26:33 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:1.3718461990356445 norm:0.0021641026251018047 max memory_allocated 29273.45947265625 
[2025-03-22 21:27:21 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:1.3701176643371582 norm:0.0020082565024495125 max memory_allocated 29273.45947265625 
[2025-03-22 21:28:09 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:1.3688454627990723 norm:0.0018952247919514775 max memory_allocated 29273.45947265625 
[2025-03-22 21:28:58 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:1.3678603172302246 norm:0.0018137807492166758 max memory_allocated 29273.45947265625 
[2025-03-22 21:29:46 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:1.3669341802597046 norm:0.0017347184475511312 max memory_allocated 29273.45947265625 
[2025-03-22 21:30:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:1.366071343421936 norm:0.0016915762098506093 max memory_allocated 29273.45947265625 
[2025-03-22 21:31:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:1.3652957677841187 norm:0.0016368948854506016 max memory_allocated 29273.45947265625 
[2025-03-22 21:32:11 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:1.3646657466888428 norm:0.0015920930309221148 max memory_allocated 29273.45947265625 
[2025-03-22 21:32:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:1.364183783531189 norm:0.00155617983546108 max memory_allocated 29273.45947265625 
[2025-03-22 21:33:47 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:1.3638604879379272 norm:0.0015388309257104993 max memory_allocated 29273.45947265625 
[2025-03-22 21:34:35 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:1.3634858131408691 norm:0.0015202534850686789 max memory_allocated 29273.45947265625 
[2025-03-22 21:35:23 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:1.36312997341156 norm:0.001514761010184884 max memory_allocated 29273.45947265625 
[2025-03-22 21:36:11 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:1.3625553846359253 norm:0.0014965602895244956 max memory_allocated 29273.45947265625 
[2025-03-22 21:36:25 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 21:37:17 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:1.6017851829528809 norm:0.020122651010751724 max memory_allocated 29273.64697265625 
[2025-03-22 21:38:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:1.5808677673339844 norm:0.011902617290616035 max memory_allocated 29273.64697265625 
[2025-03-22 21:38:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:1.55925714969635 norm:0.007082020398229361 max memory_allocated 29273.64697265625 
[2025-03-22 21:39:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:1.5492348670959473 norm:0.004436848685145378 max memory_allocated 29273.64697265625 
[2025-03-22 21:40:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:1.5427658557891846 norm:0.0018775546923279762 max memory_allocated 29273.64697265625 
[2025-03-22 21:41:17 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:1.5391775369644165 norm:0.0017171751242130995 max memory_allocated 29273.64697265625 
[2025-03-22 21:42:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:1.5363699197769165 norm:0.001649998128414154 max memory_allocated 29273.64697265625 
[2025-03-22 21:42:53 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:1.5342357158660889 norm:0.0016211261972784996 max memory_allocated 29273.64697265625 
[2025-03-22 21:43:41 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:1.5327290296554565 norm:0.001610609469935298 max memory_allocated 29273.64697265625 
[2025-03-22 21:44:29 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:1.53156316280365 norm:0.0015945341438055038 max memory_allocated 29273.64697265625 
[2025-03-22 21:45:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:1.530452013015747 norm:0.001575117465108633 max memory_allocated 29273.64697265625 
[2025-03-22 21:46:06 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:1.5294880867004395 norm:0.0015371652552857995 max memory_allocated 29273.64697265625 
[2025-03-22 21:46:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:1.528639554977417 norm:0.0015177899040281773 max memory_allocated 29273.64697265625 
[2025-03-22 21:47:42 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:1.5281744003295898 norm:0.0015160457696765661 max memory_allocated 29273.64697265625 
[2025-03-22 21:48:31 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:1.5275875329971313 norm:0.0015045278705656528 max memory_allocated 29273.64697265625 
[2025-03-22 21:49:19 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:1.5271364450454712 norm:0.0014925366267561913 max memory_allocated 29273.64697265625 
[2025-03-22 21:50:07 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:1.526655673980713 norm:0.001482702326029539 max memory_allocated 29273.64697265625 
[2025-03-22 21:50:56 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:1.5262327194213867 norm:0.001468353671953082 max memory_allocated 29273.64697265625 
[2025-03-22 21:51:44 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:1.525802731513977 norm:0.001454941462725401 max memory_allocated 29273.64697265625 
[2025-03-22 21:52:32 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:1.5255343914031982 norm:0.001466678106226027 max memory_allocated 29273.64697265625 
[2025-03-22 21:52:46 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 21:53:38 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:1.7616521120071411 norm:0.015104797668755054 max memory_allocated 29273.83447265625 
[2025-03-22 21:54:26 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:1.7416456937789917 norm:0.00824594497680664 max memory_allocated 29273.83447265625 
[2025-03-22 21:55:14 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:1.7201751470565796 norm:0.0051300921477377415 max memory_allocated 29273.83447265625 
[2025-03-22 21:56:02 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:1.7108386754989624 norm:0.0024973638355731964 max memory_allocated 29273.83447265625 
[2025-03-22 21:56:50 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:1.706268310546875 norm:0.001818963559344411 max memory_allocated 29273.83447265625 
[2025-03-22 21:57:38 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:1.7025821208953857 norm:0.001524952705949545 max memory_allocated 29273.83447265625 
[2025-03-22 21:58:26 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:1.6999558210372925 norm:0.0014609131030738354 max memory_allocated 29273.83447265625 
[2025-03-22 21:59:14 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:1.6978561878204346 norm:0.0014321519993245602 max memory_allocated 29273.83447265625 
[2025-03-22 22:00:02 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:1.696290135383606 norm:0.0014151032082736492 max memory_allocated 29273.83447265625 
[2025-03-22 22:00:50 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:1.6950404644012451 norm:0.0013952703448012471 max memory_allocated 29273.83447265625 
[2025-03-22 22:01:39 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:1.6941412687301636 norm:0.0013813740806654096 max memory_allocated 29273.83447265625 
[2025-03-22 22:02:27 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:1.693331003189087 norm:0.0013646641746163368 max memory_allocated 29273.83447265625 
[2025-03-22 22:03:15 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:1.6926770210266113 norm:0.0013612693874165416 max memory_allocated 29273.83447265625 
[2025-03-22 22:04:03 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:1.6921238899230957 norm:0.001357434899546206 max memory_allocated 29273.83447265625 
[2025-03-22 22:04:52 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:1.691537618637085 norm:0.0013526452239602804 max memory_allocated 29273.83447265625 
[2025-03-22 22:05:40 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:1.69113290309906 norm:0.0013545427937060595 max memory_allocated 29273.83447265625 
[2025-03-22 22:06:28 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:1.6907891035079956 norm:0.0013548405840992928 max memory_allocated 29273.83447265625 
[2025-03-22 22:07:16 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:1.6905174255371094 norm:0.001354940002784133 max memory_allocated 29273.83447265625 
[2025-03-22 22:08:04 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:1.6902172565460205 norm:0.0013540529180318117 max memory_allocated 29273.83447265625 
[2025-03-22 22:08:53 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:1.6898784637451172 norm:0.001344487420283258 max memory_allocated 29273.83447265625 
[2025-03-22 22:09:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 22:09:58 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:1.9510791301727295 norm:0.006267856806516647 max memory_allocated 29274.02197265625 
[2025-03-22 22:10:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:1.932673692703247 norm:0.003856247989460826 max memory_allocated 29274.02197265625 
[2025-03-22 22:11:35 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:1.910775065422058 norm:0.0027369090821594 max memory_allocated 29274.02197265625 
[2025-03-22 22:12:23 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:1.9019190073013306 norm:0.0018280678195878863 max memory_allocated 29274.02197265625 
[2025-03-22 22:13:11 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:1.897613763809204 norm:0.0015980990137904882 max memory_allocated 29274.02197265625 
[2025-03-22 22:13:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:1.894100546836853 norm:0.0015210669953376055 max memory_allocated 29274.02197265625 
[2025-03-22 22:14:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:1.8910622596740723 norm:0.0014837305061519146 max memory_allocated 29274.02197265625 
[2025-03-22 22:15:35 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:1.8885612487792969 norm:0.0014630061341449618 max memory_allocated 29274.02197265625 
[2025-03-22 22:16:23 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:1.8866842985153198 norm:0.0014459512894973159 max memory_allocated 29274.02197265625 
[2025-03-22 22:17:11 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:1.8852540254592896 norm:0.001436780788935721 max memory_allocated 29274.02197265625 
[2025-03-22 22:17:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:1.8841698169708252 norm:0.0014275909634307027 max memory_allocated 29274.02197265625 
[2025-03-22 22:18:47 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:1.8833507299423218 norm:0.0014178474666550756 max memory_allocated 29274.02197265625 
[2025-03-22 22:19:35 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:1.882655143737793 norm:0.0014096865197643638 max memory_allocated 29274.02197265625 
[2025-03-22 22:20:24 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:1.8820463418960571 norm:0.0014163085725158453 max memory_allocated 29274.02197265625 
[2025-03-22 22:21:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:1.881564974784851 norm:0.0014239655574783683 max memory_allocated 29274.02197265625 
[2025-03-22 22:22:00 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:1.881140947341919 norm:0.0014311542036011815 max memory_allocated 29274.02197265625 
[2025-03-22 22:22:48 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:1.880889654159546 norm:0.0014219158329069614 max memory_allocated 29274.02197265625 
[2025-03-22 22:23:36 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:1.8805015087127686 norm:0.0014243321493268013 max memory_allocated 29274.02197265625 
[2025-03-22 22:24:25 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:1.8801698684692383 norm:0.001420330721884966 max memory_allocated 29274.02197265625 
[2025-03-22 22:25:13 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:1.879870891571045 norm:0.0014111652271822095 max memory_allocated 29274.02197265625 
[2025-03-22 22:25:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 22:26:19 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:2.149291753768921 norm:0.007485796231776476 max memory_allocated 29274.20947265625 
[2025-03-22 22:27:07 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:2.1291279792785645 norm:0.004580475389957428 max memory_allocated 29274.20947265625 
[2025-03-22 22:27:55 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:2.1070616245269775 norm:0.00345361465588212 max memory_allocated 29274.20947265625 
[2025-03-22 22:28:43 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:2.09893798828125 norm:0.0026764729991555214 max memory_allocated 29274.20947265625 
[2025-03-22 22:29:31 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:2.0947494506835938 norm:0.002371007576584816 max memory_allocated 29274.20947265625 
[2025-03-22 22:30:19 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:2.0914885997772217 norm:0.0021933475509285927 max memory_allocated 29274.20947265625 
[2025-03-22 22:31:07 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:2.088222026824951 norm:0.001992193516343832 max memory_allocated 29274.20947265625 
[2025-03-22 22:31:55 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:2.085970878601074 norm:0.0019641651306301355 max memory_allocated 29274.20947265625 
[2025-03-22 22:32:43 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:2.084094762802124 norm:0.0018832740606740117 max memory_allocated 29274.20947265625 
[2025-03-22 22:33:31 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:2.0824503898620605 norm:0.0018498855642974377 max memory_allocated 29274.20947265625 
[2025-03-22 22:34:19 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:2.081355571746826 norm:0.0018583222990855575 max memory_allocated 29274.20947265625 
[2025-03-22 22:35:07 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:2.080300807952881 norm:0.0018021337455138564 max memory_allocated 29274.20947265625 
[2025-03-22 22:35:55 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:2.0794050693511963 norm:0.001742060179822147 max memory_allocated 29274.20947265625 
[2025-03-22 22:36:43 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:2.07865309715271 norm:0.0017558002145960927 max memory_allocated 29274.20947265625 
[2025-03-22 22:37:31 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:2.0779576301574707 norm:0.0016956962645053864 max memory_allocated 29274.20947265625 
[2025-03-22 22:38:19 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:2.077489137649536 norm:0.0018337976653128862 max memory_allocated 29274.20947265625 
[2025-03-22 22:39:07 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:2.0769124031066895 norm:0.0017117373645305634 max memory_allocated 29274.20947265625 
[2025-03-22 22:39:55 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:2.0764474868774414 norm:0.0016871511470526457 max memory_allocated 29274.20947265625 
[2025-03-22 22:40:43 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:2.0758988857269287 norm:0.0016331052174791694 max memory_allocated 29274.20947265625 
[2025-03-22 22:41:32 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:2.0755057334899902 norm:0.0017571051139384508 max memory_allocated 29274.20947265625 
[2025-03-22 22:41:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 22:42:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:2.3327529430389404 norm:0.011555330827832222 max memory_allocated 29274.39697265625 
[2025-03-22 22:43:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:2.313030958175659 norm:0.006393663585186005 max memory_allocated 29274.39697265625 
[2025-03-22 22:44:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:2.2897777557373047 norm:0.003887346712872386 max memory_allocated 29274.39697265625 
[2025-03-22 22:45:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:2.2809696197509766 norm:0.002709099790081382 max memory_allocated 29274.39697265625 
[2025-03-22 22:45:50 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:2.27590012550354 norm:0.0018070877995342016 max memory_allocated 29274.39697265625 
[2025-03-22 22:46:38 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:2.2720420360565186 norm:0.0015658721094951034 max memory_allocated 29274.39697265625 
[2025-03-22 22:47:26 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:2.268768787384033 norm:0.0014913015766069293 max memory_allocated 29274.39697265625 
[2025-03-22 22:48:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:2.266331195831299 norm:0.0014357221079990268 max memory_allocated 29274.39697265625 
[2025-03-22 22:49:01 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:2.26455020904541 norm:0.0014158445410430431 max memory_allocated 29274.39697265625 
[2025-03-22 22:49:49 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:2.263221025466919 norm:0.0013817790895700455 max memory_allocated 29274.39697265625 
[2025-03-22 22:50:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:2.2622156143188477 norm:0.001367119257338345 max memory_allocated 29274.39697265625 
[2025-03-22 22:51:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:2.261369466781616 norm:0.0013589677400887012 max memory_allocated 29274.39697265625 
[2025-03-22 22:52:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:2.2606372833251953 norm:0.0013488717377185822 max memory_allocated 29274.39697265625 
[2025-03-22 22:53:01 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:2.2599384784698486 norm:0.0013355675619095564 max memory_allocated 29274.39697265625 
[2025-03-22 22:53:49 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:2.2594785690307617 norm:0.0013331883819773793 max memory_allocated 29274.39697265625 
[2025-03-22 22:54:37 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:2.258979558944702 norm:0.0013263898435980082 max memory_allocated 29274.39697265625 
[2025-03-22 22:55:25 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:2.2586841583251953 norm:0.0013259252300485969 max memory_allocated 29274.39697265625 
[2025-03-22 22:56:14 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:2.2583868503570557 norm:0.0013253373326733708 max memory_allocated 29274.39697265625 
[2025-03-22 22:57:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:2.258065700531006 norm:0.0013249017065390944 max memory_allocated 29274.39697265625 
[2025-03-22 22:57:50 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:2.257777214050293 norm:0.0013169541489332914 max memory_allocated 29274.39697265625 
[2025-03-22 22:58:03 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 22:58:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:2.552299976348877 norm:0.015924843028187752 max memory_allocated 29274.58447265625 
[2025-03-22 22:59:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:2.5328056812286377 norm:0.011271292343735695 max memory_allocated 29274.58447265625 
[2025-03-22 23:00:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:2.509342908859253 norm:0.008098749443888664 max memory_allocated 29274.58447265625 
[2025-03-22 23:01:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:2.4994935989379883 norm:0.00615119282156229 max memory_allocated 29274.58447265625 
[2025-03-22 23:02:09 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:2.4927268028259277 norm:0.004691613372415304 max memory_allocated 29274.58447265625 
[2025-03-22 23:02:57 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:2.486076593399048 norm:0.0036328483838588 max memory_allocated 29274.58447265625 
[2025-03-22 23:03:45 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:2.482147693634033 norm:0.0035338953603059053 max memory_allocated 29274.58447265625 
[2025-03-22 23:04:33 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:2.4791505336761475 norm:0.0031899812165647745 max memory_allocated 29274.58447265625 
[2025-03-22 23:05:21 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:2.476783275604248 norm:0.0028903442434966564 max memory_allocated 29274.58447265625 
[2025-03-22 23:06:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:2.4749608039855957 norm:0.0026946058496832848 max memory_allocated 29274.58447265625 
[2025-03-22 23:06:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:2.473501205444336 norm:0.0025025850627571344 max memory_allocated 29274.58447265625 
[2025-03-22 23:07:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:2.4722461700439453 norm:0.0023243476171046495 max memory_allocated 29274.58447265625 
[2025-03-22 23:08:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:2.4713313579559326 norm:0.0022048302926123142 max memory_allocated 29274.58447265625 
[2025-03-22 23:09:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:2.470463275909424 norm:0.002091007074341178 max memory_allocated 29274.58447265625 
[2025-03-22 23:10:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:2.469752550125122 norm:0.0019791589584201574 max memory_allocated 29274.58447265625 
[2025-03-22 23:10:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:2.4690232276916504 norm:0.0018873875960707664 max memory_allocated 29274.58447265625 
[2025-03-22 23:11:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:2.468402147293091 norm:0.0018065691692754626 max memory_allocated 29274.58447265625 
[2025-03-22 23:12:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:2.467804431915283 norm:0.0017637324053794146 max memory_allocated 29274.58447265625 
[2025-03-22 23:13:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:2.46750807762146 norm:0.0017461678944528103 max memory_allocated 29274.58447265625 
[2025-03-22 23:14:09 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:2.4669699668884277 norm:0.0016891352133825421 max memory_allocated 29274.58447265625 
[2025-03-22 23:14:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 23:15:14 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:2.7561709880828857 norm:0.006705444771796465 max memory_allocated 29274.77197265625 
[2025-03-22 23:16:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:2.735008716583252 norm:0.003996542189270258 max memory_allocated 29274.77197265625 
[2025-03-22 23:16:50 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:2.7108726501464844 norm:0.0028487148229032755 max memory_allocated 29274.77197265625 
[2025-03-22 23:17:38 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:2.7012746334075928 norm:0.0023109999019652605 max memory_allocated 29274.77197265625 
[2025-03-22 23:18:26 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:2.6958439350128174 norm:0.002082387451082468 max memory_allocated 29274.77197265625 
[2025-03-22 23:19:14 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:2.6911427974700928 norm:0.0019282505381852388 max memory_allocated 29274.77197265625 
[2025-03-22 23:20:03 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:2.687044143676758 norm:0.001801088685169816 max memory_allocated 29274.77197265625 
[2025-03-22 23:20:51 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:2.684156894683838 norm:0.0017581236315891147 max memory_allocated 29274.77197265625 
[2025-03-22 23:21:39 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:2.68194580078125 norm:0.0017037562793120742 max memory_allocated 29274.77197265625 
[2025-03-22 23:22:27 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:2.680352210998535 norm:0.0016859782626852393 max memory_allocated 29274.77197265625 
[2025-03-22 23:23:15 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:2.6791133880615234 norm:0.001657309359870851 max memory_allocated 29274.77197265625 
[2025-03-22 23:24:03 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:2.6780450344085693 norm:0.0016288785263895988 max memory_allocated 29274.77197265625 
[2025-03-22 23:24:52 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:2.677246570587158 norm:0.0016048091929405928 max memory_allocated 29274.77197265625 
[2025-03-22 23:25:40 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:2.6764910221099854 norm:0.0015785777941346169 max memory_allocated 29274.77197265625 
[2025-03-22 23:26:28 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:2.6758975982666016 norm:0.001561894896440208 max memory_allocated 29274.77197265625 
[2025-03-22 23:27:16 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:2.6754977703094482 norm:0.0015377008821815252 max memory_allocated 29274.77197265625 
[2025-03-22 23:28:04 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:2.6750495433807373 norm:0.001559012453071773 max memory_allocated 29274.77197265625 
[2025-03-22 23:28:52 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:2.6745870113372803 norm:0.0015478592831641436 max memory_allocated 29274.77197265625 
[2025-03-22 23:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:2.6741116046905518 norm:0.001557977986522019 max memory_allocated 29274.77197265625 
[2025-03-22 23:30:29 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:2.6738500595092773 norm:0.001552328933030367 max memory_allocated 29274.77197265625 
[2025-03-22 23:30:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 32 ===
[2025-03-22 23:31:34 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 0 loss:2.9755454063415527 norm:0.01270714309066534 max memory_allocated 29274.95947265625 
[2025-03-22 23:32:22 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 1 loss:2.9526607990264893 norm:0.007969001308083534 max memory_allocated 29274.95947265625 
[2025-03-22 23:33:11 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 2 loss:2.9242162704467773 norm:0.005176427308470011 max memory_allocated 29274.95947265625 
[2025-03-22 23:33:59 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 3 loss:2.9118142127990723 norm:0.0034893923439085484 max memory_allocated 29274.95947265625 
[2025-03-22 23:34:47 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 4 loss:2.9052603244781494 norm:0.00258253188803792 max memory_allocated 29274.95947265625 
[2025-03-22 23:35:35 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 5 loss:2.8995635509490967 norm:0.002166800433769822 max memory_allocated 29274.95947265625 
[2025-03-22 23:36:23 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 6 loss:2.8946542739868164 norm:0.0018508373759686947 max memory_allocated 29274.95947265625 
[2025-03-22 23:37:11 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 7 loss:2.891143798828125 norm:0.0017171347280964255 max memory_allocated 29274.95947265625 
[2025-03-22 23:37:59 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 8 loss:2.8886985778808594 norm:0.0016305085737258196 max memory_allocated 29274.95947265625 
[2025-03-22 23:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 9 loss:2.886760711669922 norm:0.0015630569541826844 max memory_allocated 29274.95947265625 
[2025-03-22 23:39:34 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 10 loss:2.885124921798706 norm:0.0015222772490233183 max memory_allocated 29274.95947265625 
[2025-03-22 23:40:22 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 11 loss:2.883867025375366 norm:0.0014814920723438263 max memory_allocated 29274.95947265625 
[2025-03-22 23:41:10 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 12 loss:2.8828468322753906 norm:0.0014515421353280544 max memory_allocated 29274.95947265625 
[2025-03-22 23:41:58 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 13 loss:2.882084846496582 norm:0.0014227634528651834 max memory_allocated 29274.95947265625 
[2025-03-22 23:42:46 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 14 loss:2.881394386291504 norm:0.0014033254701644182 max memory_allocated 29274.95947265625 
[2025-03-22 23:43:35 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 15 loss:2.880819797515869 norm:0.0013922587968409061 max memory_allocated 29274.95947265625 
[2025-03-22 23:44:23 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 16 loss:2.880312919616699 norm:0.0013793667312711477 max memory_allocated 29274.95947265625 
[2025-03-22 23:45:11 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 17 loss:2.8798465728759766 norm:0.0013630806934088469 max memory_allocated 29274.95947265625 
[2025-03-22 23:45:59 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 18 loss:2.87935733795166 norm:0.0013577885692939162 max memory_allocated 29274.95947265625 
[2025-03-22 23:46:47 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 19 loss:2.8789587020874023 norm:0.0013549026334658265 max memory_allocated 29274.95947265625 
[2025-03-22 23:47:01 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 33 ===
[2025-03-22 23:47:53 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 0 loss:3.2162539958953857 norm:0.011108102276921272 max memory_allocated 29275.14697265625 
[2025-03-22 23:48:41 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 1 loss:3.191373586654663 norm:0.006173989735543728 max memory_allocated 29275.14697265625 
[2025-03-22 23:49:29 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 2 loss:3.162004232406616 norm:0.004237267188727856 max memory_allocated 29275.14697265625 
[2025-03-22 23:50:17 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 3 loss:3.1494336128234863 norm:0.00308029493317008 max memory_allocated 29275.14697265625 
[2025-03-22 23:51:05 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 4 loss:3.1421570777893066 norm:0.0023131342604756355 max memory_allocated 29275.14697265625 
[2025-03-22 23:51:53 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 5 loss:3.1361968517303467 norm:0.0018888975027948618 max memory_allocated 29275.14697265625 
[2025-03-22 23:52:41 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 6 loss:3.1308248043060303 norm:0.00174333481118083 max memory_allocated 29275.14697265625 
[2025-03-22 23:53:29 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 7 loss:3.1272926330566406 norm:0.0016626354772597551 max memory_allocated 29275.14697265625 
[2025-03-22 23:54:17 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 8 loss:3.1247661113739014 norm:0.001634626416489482 max memory_allocated 29275.14697265625 
[2025-03-22 23:55:05 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 9 loss:3.1229751110076904 norm:0.0016102787340059876 max memory_allocated 29275.14697265625 
[2025-03-22 23:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 10 loss:3.1215949058532715 norm:0.00157946499530226 max memory_allocated 29275.14697265625 
[2025-03-22 23:56:41 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 11 loss:3.1201987266540527 norm:0.0015361140249297023 max memory_allocated 29275.14697265625 
[2025-03-22 23:57:29 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 12 loss:3.119204044342041 norm:0.001505429157987237 max memory_allocated 29275.14697265625 
[2025-03-22 23:58:16 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 13 loss:3.1183393001556396 norm:0.0014786293031647801 max memory_allocated 29275.14697265625 
[2025-03-22 23:59:04 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 14 loss:3.1176035404205322 norm:0.001463492400944233 max memory_allocated 29275.14697265625 
[2025-03-22 23:59:52 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 15 loss:3.116903781890869 norm:0.0014721089974045753 max memory_allocated 29275.14697265625 
[2025-03-23 00:00:41 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 16 loss:3.1162662506103516 norm:0.0014651516685262322 max memory_allocated 29275.14697265625 
[2025-03-23 00:01:29 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 17 loss:3.1157994270324707 norm:0.0014620654983446002 max memory_allocated 29275.14697265625 
[2025-03-23 00:02:17 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 18 loss:3.1154420375823975 norm:0.0014647626085206866 max memory_allocated 29275.14697265625 
[2025-03-23 00:03:05 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 19 loss:3.115109443664551 norm:0.0014568421756848693 max memory_allocated 29275.14697265625 
[2025-03-23 00:03:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 34 ===
[2025-03-23 00:04:10 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 0 loss:3.536555051803589 norm:0.034474242478609085 max memory_allocated 29275.33447265625 
[2025-03-23 00:04:58 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 1 loss:3.503122329711914 norm:0.022822726517915726 max memory_allocated 29275.33447265625 
[2025-03-23 00:05:46 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 2 loss:3.463552713394165 norm:0.015071030706167221 max memory_allocated 29275.33447265625 
[2025-03-23 00:06:35 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 3 loss:3.441018581390381 norm:0.009912664070725441 max memory_allocated 29275.33447265625 
[2025-03-23 00:07:23 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 4 loss:3.4283297061920166 norm:0.007589039858430624 max memory_allocated 29275.33447265625 
[2025-03-23 00:08:11 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 5 loss:3.4199795722961426 norm:0.0065026734955608845 max memory_allocated 29275.33447265625 
[2025-03-23 00:08:59 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 6 loss:3.4137306213378906 norm:0.005690122488886118 max memory_allocated 29275.33447265625 
[2025-03-23 00:09:47 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 7 loss:3.408658027648926 norm:0.004897898528724909 max memory_allocated 29275.33447265625 
[2025-03-23 00:10:35 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 8 loss:3.4052181243896484 norm:0.00439938111230731 max memory_allocated 29275.33447265625 
[2025-03-23 00:11:23 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 9 loss:3.4014225006103516 norm:0.003473097924143076 max memory_allocated 29275.33447265625 
[2025-03-23 00:12:11 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 10 loss:3.3974146842956543 norm:0.001860658172518015 max memory_allocated 29275.33447265625 
[2025-03-23 00:12:59 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 11 loss:3.395155668258667 norm:0.0018799909157678485 max memory_allocated 29275.33447265625 
[2025-03-23 00:13:47 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 12 loss:3.393453598022461 norm:0.001846006140112877 max memory_allocated 29275.33447265625 
[2025-03-23 00:14:35 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 13 loss:3.3921849727630615 norm:0.0018209052504971623 max memory_allocated 29275.33447265625 
[2025-03-23 00:15:23 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 14 loss:3.391284227371216 norm:0.0018151859985664487 max memory_allocated 29275.33447265625 
[2025-03-23 00:16:11 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 15 loss:3.390442132949829 norm:0.001810296205803752 max memory_allocated 29275.33447265625 
[2025-03-23 00:16:59 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 16 loss:3.3897652626037598 norm:0.0018059376161545515 max memory_allocated 29275.33447265625 
[2025-03-23 00:17:47 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 17 loss:3.389220952987671 norm:0.0017929688328877091 max memory_allocated 29275.33447265625 
[2025-03-23 00:18:35 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 18 loss:3.38926362991333 norm:0.0017818027408793569 max memory_allocated 29275.33447265625 
[2025-03-23 00:19:23 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 19 loss:3.388880968093872 norm:0.0017924553249031305 max memory_allocated 29275.33447265625 
[2025-03-23 00:19:37 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 35 ===
[2025-03-23 00:20:28 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 0 loss:3.825538158416748 norm:0.023054592311382294 max memory_allocated 29275.52197265625 
[2025-03-23 00:21:16 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 1 loss:3.786831855773926 norm:0.015659555792808533 max memory_allocated 29275.52197265625 
[2025-03-23 00:22:04 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 2 loss:3.743640661239624 norm:0.01078752800822258 max memory_allocated 29275.52197265625 
[2025-03-23 00:22:53 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 3 loss:3.7214725017547607 norm:0.007812271360307932 max memory_allocated 29275.52197265625 
[2025-03-23 00:23:41 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 4 loss:3.706998109817505 norm:0.005462830886244774 max memory_allocated 29275.52197265625 
[2025-03-23 00:24:29 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 5 loss:3.6984431743621826 norm:0.005274395924061537 max memory_allocated 29275.52197265625 
[2025-03-23 00:25:17 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 6 loss:3.6911206245422363 norm:0.00433569960296154 max memory_allocated 29275.52197265625 
[2025-03-23 00:26:05 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 7 loss:3.686016798019409 norm:0.0039036041125655174 max memory_allocated 29275.52197265625 
[2025-03-23 00:26:53 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 8 loss:3.682415008544922 norm:0.0035829758271574974 max memory_allocated 29275.52197265625 
[2025-03-23 00:27:41 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 9 loss:3.6791114807128906 norm:0.003190329298377037 max memory_allocated 29275.52197265625 
[2025-03-23 00:28:30 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 10 loss:3.6768693923950195 norm:0.0030564814805984497 max memory_allocated 29275.52197265625 
[2025-03-23 00:29:18 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 11 loss:3.674107313156128 norm:0.0029801311902701855 max memory_allocated 29275.52197265625 
[2025-03-23 00:30:06 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 12 loss:3.6720221042633057 norm:0.002794204745441675 max memory_allocated 29275.52197265625 
[2025-03-23 00:30:54 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 13 loss:3.670599937438965 norm:0.0027330853044986725 max memory_allocated 29275.52197265625 
[2025-03-23 00:31:42 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 14 loss:3.669161558151245 norm:0.002675948664546013 max memory_allocated 29275.52197265625 
[2025-03-23 00:32:30 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 15 loss:3.667771816253662 norm:0.002584949368610978 max memory_allocated 29275.52197265625 
[2025-03-23 00:33:18 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 16 loss:3.6664695739746094 norm:0.0025602716486901045 max memory_allocated 29275.52197265625 
[2025-03-23 00:34:07 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 17 loss:3.6658554077148438 norm:0.002561324741691351 max memory_allocated 29275.52197265625 
[2025-03-23 00:34:55 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 18 loss:3.6646323204040527 norm:0.002506482880562544 max memory_allocated 29275.52197265625 
[2025-03-23 00:35:43 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 19 loss:3.664371967315674 norm:0.0024905321188271046 max memory_allocated 29275.52197265625 
[2025-03-23 00:35:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 36 ===
[2025-03-23 00:36:00 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 00:36:49 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 0 loss:4.176346778869629 norm:0.0940852165222168 max memory_allocated 29278.02197265625 
[2025-03-23 00:37:37 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 1 loss:4.1184821128845215 norm:0.07626482844352722 max memory_allocated 29278.02197265625 
[2025-03-23 00:38:25 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 2 loss:4.067044734954834 norm:0.06510742008686066 max memory_allocated 29278.02197265625 
[2025-03-23 00:39:14 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 3 loss:4.040803909301758 norm:0.0494915172457695 max memory_allocated 29278.02197265625 
[2025-03-23 00:40:02 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 4 loss:4.027163505554199 norm:0.044874366372823715 max memory_allocated 29278.02197265625 
[2025-03-23 00:40:50 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 5 loss:4.015841960906982 norm:0.04144031181931496 max memory_allocated 29278.02197265625 
[2025-03-23 00:41:38 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 6 loss:4.007994174957275 norm:0.03666039556264877 max memory_allocated 29278.02197265625 
[2025-03-23 00:42:26 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 7 loss:4.054685115814209 norm:0.053231462836265564 max memory_allocated 29278.02197265625 
[2025-03-23 00:43:14 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 8 loss:4.017281532287598 norm:0.03931712359189987 max memory_allocated 29278.02197265625 
[2025-03-23 00:44:03 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 9 loss:3.989069700241089 norm:0.02387229911983013 max memory_allocated 29278.02197265625 
[2025-03-23 00:44:51 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 10 loss:3.981315851211548 norm:0.020178167149424553 max memory_allocated 29278.02197265625 
[2025-03-23 00:45:39 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 11 loss:3.9774811267852783 norm:0.019065948203206062 max memory_allocated 29278.02197265625 
[2025-03-23 00:46:27 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 12 loss:3.9746737480163574 norm:0.018770355731248856 max memory_allocated 29278.02197265625 
[2025-03-23 00:47:15 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 13 loss:3.971932888031006 norm:0.017715584486722946 max memory_allocated 29278.02197265625 
[2025-03-23 00:48:04 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 14 loss:3.9684865474700928 norm:0.01705339178442955 max memory_allocated 29278.02197265625 
[2025-03-23 00:48:52 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 15 loss:3.9662630558013916 norm:0.016415953636169434 max memory_allocated 29278.02197265625 
[2025-03-23 00:49:40 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 16 loss:3.964647054672241 norm:0.01586122252047062 max memory_allocated 29278.02197265625 
[2025-03-23 00:50:28 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 17 loss:3.963495969772339 norm:0.01620611362159252 max memory_allocated 29278.02197265625 
[2025-03-23 00:51:17 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 18 loss:3.9640445709228516 norm:0.01661084219813347 max memory_allocated 29278.02197265625 
[2025-03-23 00:52:05 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 19 loss:3.968127727508545 norm:0.0199919231235981 max memory_allocated 29278.02197265625 
[2025-03-23 00:52:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 37 ===
[2025-03-23 00:52:23 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 00:53:11 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 0 loss:4.701338291168213 norm:0.13390687108039856 max memory_allocated 29278.20947265625 
[2025-03-23 00:53:59 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 1 loss:4.612340927124023 norm:0.11552238464355469 max memory_allocated 29278.20947265625 
[2025-03-23 00:54:47 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 2 loss:4.539400100708008 norm:0.09659355878829956 max memory_allocated 29278.20947265625 
[2025-03-23 00:55:36 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 3 loss:4.504058361053467 norm:0.07852527499198914 max memory_allocated 29278.20947265625 
[2025-03-23 00:56:24 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 4 loss:4.483480453491211 norm:0.06518904119729996 max memory_allocated 29278.20947265625 
[2025-03-23 00:57:12 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 5 loss:4.467291355133057 norm:0.05288553610444069 max memory_allocated 29278.20947265625 
[2025-03-23 00:58:01 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 6 loss:4.457218170166016 norm:0.04454709216952324 max memory_allocated 29278.20947265625 
[2025-03-23 00:58:49 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 7 loss:4.449148654937744 norm:0.04249418526887894 max memory_allocated 29278.20947265625 
[2025-03-23 00:59:37 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 8 loss:4.467249393463135 norm:0.04625869169831276 max memory_allocated 29278.20947265625 
[2025-03-23 01:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 9 loss:4.448293685913086 norm:0.0399419441819191 max memory_allocated 29278.20947265625 
[2025-03-23 01:01:13 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 10 loss:4.474807262420654 norm:0.04717390984296799 max memory_allocated 29278.20947265625 
[2025-03-23 01:02:02 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 11 loss:4.4275407791137695 norm:0.030587904155254364 max memory_allocated 29278.20947265625 
[2025-03-23 01:02:50 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 12 loss:4.419366836547852 norm:0.028099581599235535 max memory_allocated 29278.20947265625 
[2025-03-23 01:03:38 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 13 loss:4.411826133728027 norm:0.024923820048570633 max memory_allocated 29278.20947265625 
[2025-03-23 01:04:26 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 14 loss:4.406391620635986 norm:0.022095369175076485 max memory_allocated 29278.20947265625 
[2025-03-23 01:05:14 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 15 loss:4.402157306671143 norm:0.022225672379136086 max memory_allocated 29278.20947265625 
[2025-03-23 01:06:02 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 16 loss:4.398470878601074 norm:0.020034167915582657 max memory_allocated 29278.20947265625 
[2025-03-23 01:06:50 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 17 loss:4.395642280578613 norm:0.018968820571899414 max memory_allocated 29278.20947265625 
[2025-03-23 01:07:39 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 18 loss:4.39320707321167 norm:0.018180573359131813 max memory_allocated 29278.20947265625 
[2025-03-23 01:08:27 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 19 loss:4.392019271850586 norm:0.018001345917582512 max memory_allocated 29278.20947265625 
[2025-03-23 01:08:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 38 ===
[2025-03-23 01:08:44 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 01:09:33 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 0 loss:6.287691593170166 norm:0.6203171610832214 max memory_allocated 29278.39697265625 
[2025-03-23 01:10:21 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 1 loss:6.067736625671387 norm:0.5526062250137329 max memory_allocated 29278.39697265625 
[2025-03-23 01:11:09 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 2 loss:5.885663986206055 norm:0.44594883918762207 max memory_allocated 29278.39697265625 
[2025-03-23 01:11:57 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 3 loss:5.725385665893555 norm:0.31466031074523926 max memory_allocated 29278.39697265625 
[2025-03-23 01:12:45 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 4 loss:5.614840507507324 norm:0.21952392160892487 max memory_allocated 29278.39697265625 
[2025-03-23 01:13:34 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 5 loss:5.527436256408691 norm:0.15260672569274902 max memory_allocated 29278.39697265625 
[2025-03-23 01:14:22 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 6 loss:5.479117393493652 norm:0.12769947946071625 max memory_allocated 29278.39697265625 
[2025-03-23 01:15:10 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 7 loss:5.447883605957031 norm:0.11163944005966187 max memory_allocated 29278.39697265625 
[2025-03-23 01:15:59 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 8 loss:5.423525333404541 norm:0.09984292834997177 max memory_allocated 29278.39697265625 
[2025-03-23 01:16:47 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 9 loss:5.405491828918457 norm:0.09395783394575119 max memory_allocated 29278.39697265625 
[2025-03-23 01:17:35 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 10 loss:5.393580913543701 norm:0.09002592414617538 max memory_allocated 29278.39697265625 
[2025-03-23 01:18:23 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 11 loss:5.382441520690918 norm:0.08765513449907303 max memory_allocated 29278.39697265625 
[2025-03-23 01:19:12 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 12 loss:5.367451190948486 norm:0.08000802993774414 max memory_allocated 29278.39697265625 
[2025-03-23 01:20:00 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 13 loss:5.352974891662598 norm:0.0707077831029892 max memory_allocated 29278.39697265625 
[2025-03-23 01:20:49 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 14 loss:5.344542980194092 norm:0.06572423875331879 max memory_allocated 29278.39697265625 
[2025-03-23 01:21:37 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 15 loss:5.3378095626831055 norm:0.06165188178420067 max memory_allocated 29278.39697265625 
[2025-03-23 01:22:25 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 16 loss:5.327815532684326 norm:0.05701475590467453 max memory_allocated 29278.39697265625 
[2025-03-23 01:23:14 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 17 loss:5.323648929595947 norm:0.0547829307615757 max memory_allocated 29278.39697265625 
[2025-03-23 01:24:02 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 18 loss:5.3318705558776855 norm:0.06073794513940811 max memory_allocated 29278.39697265625 
[2025-03-23 01:24:50 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 19 loss:5.32127571105957 norm:0.055920109152793884 max memory_allocated 29278.39697265625 
[2025-03-23 01:25:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 39 ===
[2025-03-23 01:25:09 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-23 01:25:57 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 0 loss:10.736964225769043 norm:1.1319434642791748 max memory_allocated 29278.58447265625 
[2025-03-23 01:26:45 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 1 loss:10.114567756652832 norm:0.8373605608940125 max memory_allocated 29278.58447265625 
[2025-03-23 01:27:33 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 2 loss:9.639023780822754 norm:0.7014089226722717 max memory_allocated 29278.58447265625 
[2025-03-23 01:28:21 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 3 loss:9.341660499572754 norm:0.6693843603134155 max memory_allocated 29278.58447265625 
[2025-03-23 01:29:09 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 4 loss:9.166868209838867 norm:0.5841843485832214 max memory_allocated 29278.58447265625 
[2025-03-23 01:29:57 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 5 loss:9.040675163269043 norm:0.4932073950767517 max memory_allocated 29278.58447265625 
[2025-03-23 01:30:45 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 6 loss:8.944334030151367 norm:0.438850462436676 max memory_allocated 29278.58447265625 
[2025-03-23 01:31:33 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 7 loss:8.88654899597168 norm:0.409475177526474 max memory_allocated 29278.58447265625 
[2025-03-23 01:32:21 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 8 loss:8.810205459594727 norm:0.3935590982437134 max memory_allocated 29278.58447265625 
[2025-03-23 01:33:09 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 9 loss:8.769158363342285 norm:0.4151112735271454 max memory_allocated 29278.58447265625 
[2025-03-23 01:33:58 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 10 loss:8.707619667053223 norm:0.37287774682044983 max memory_allocated 29278.58447265625 
[2025-03-23 01:34:46 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 11 loss:8.657898902893066 norm:0.3270746171474457 max memory_allocated 29278.58447265625 
[2025-03-23 01:35:34 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 12 loss:8.621662139892578 norm:0.3052397072315216 max memory_allocated 29278.58447265625 
[2025-03-23 01:36:22 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 13 loss:8.616497993469238 norm:0.3062676787376404 max memory_allocated 29278.58447265625 
[2025-03-23 01:37:11 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 14 loss:8.618366241455078 norm:0.32711297273635864 max memory_allocated 29278.58447265625 
[2025-03-23 01:37:59 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 15 loss:8.588285446166992 norm:0.3146743178367615 max memory_allocated 29278.58447265625 
[2025-03-23 01:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 16 loss:8.540444374084473 norm:0.27680209279060364 max memory_allocated 29278.58447265625 
[2025-03-23 01:39:36 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 17 loss:8.560585975646973 norm:0.28734949231147766 max memory_allocated 29278.58447265625 
[2025-03-23 01:40:24 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 18 loss:8.594306945800781 norm:0.32541394233703613 max memory_allocated 29278.58447265625 
[2025-03-23 01:41:12 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 19 loss:8.569085121154785 norm:0.3172551095485687 max memory_allocated 29278.58447265625 
[2025-03-23 01:41:26 root] (main_calibration_a.py 369): INFO 39169.86276817322
[2025-03-23 01:41:36 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-23 01:43:28 root] (main_calibration_a.py 158): INFO wikitext2 : 7.57777738571167
[2025-03-23 01:43:28 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-23 01:46:20 root] (main_calibration_a.py 158): INFO c4 : 11.002607345581055
[2025-03-23 03:56:55 root] (main_calibration_a.py 169): INFO {'wikitext2': 7.57777738571167, 'c4': 11.002607345581055, 'results': {'winogrande': {'acc': 0.5524861878453039, 'acc_stderr': 0.013974847640536204}, 'piqa': {'acc': 0.6882480957562568, 'acc_stderr': 0.010807431424873674, 'acc_norm': 0.6969532100108814, 'acc_norm_stderr': 0.010722648689531503}, 'arc_challenge': {'acc': 0.2935153583617747, 'acc_stderr': 0.013307250444941122, 'acc_norm': 0.3395904436860068, 'acc_norm_stderr': 0.013839039762820164}, 'arc_easy': {'acc': 0.5576599326599326, 'acc_stderr': 0.010191334444220853, 'acc_norm': 0.4494949494949495, 'acc_norm_stderr': 0.010207308833916035}, 'hellaswag': {'acc': 0.4816769567815176, 'acc_stderr': 0.004986429808146775, 'acc_norm': 0.6318462457677754, 'acc_norm_stderr': 0.004813177057496271}, 'boolq': {'acc': 0.6336391437308868, 'acc_stderr': 0.008426904488635877}}, 'versions': {'winogrande': 0, 'piqa': 0, 'arc_challenge': 0, 'arc_easy': 0, 'hellaswag': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-23 03:56:55 root] (main_calibration_a.py 172): INFO 29.35,55.77,63.36,48.17,68.82,55.25
