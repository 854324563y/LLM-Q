[2025-03-22 01:14:09 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/Llama-2-7b-hf-w4a4-2', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=2)
[2025-03-22 01:17:55 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 01:17:55 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:17:55 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 01:17:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:18:00 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:18:32 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.06150413677096367 norm:0.06422977149486542 max memory_allocated 22562.27978515625 
[2025-03-22 01:19:04 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.03439243882894516 norm:0.03119448572397232 max memory_allocated 22562.27978515625 
[2025-03-22 01:19:37 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.02658170461654663 norm:0.02250455878674984 max memory_allocated 22562.27978515625 
[2025-03-22 01:20:09 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.023551279678940773 norm:0.020886778831481934 max memory_allocated 22562.27978515625 
[2025-03-22 01:20:42 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.02167610079050064 norm:0.017904633656144142 max memory_allocated 22562.27978515625 
[2025-03-22 01:21:14 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.0208220686763525 norm:0.015778547152876854 max memory_allocated 22562.27978515625 
[2025-03-22 01:21:47 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.02048230729997158 norm:0.013216441497206688 max memory_allocated 22562.27978515625 
[2025-03-22 01:22:19 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.02007162943482399 norm:0.011013010516762733 max memory_allocated 22562.27978515625 
[2025-03-22 01:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.019661175087094307 norm:0.009866359643638134 max memory_allocated 22562.27978515625 
[2025-03-22 01:23:24 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.019394654780626297 norm:0.008558325469493866 max memory_allocated 22562.27978515625 
[2025-03-22 01:23:57 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.019330933690071106 norm:0.007576321717351675 max memory_allocated 22562.27978515625 
[2025-03-22 01:24:29 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.0190728846937418 norm:0.007002000696957111 max memory_allocated 22562.27978515625 
[2025-03-22 01:25:02 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.018992284312844276 norm:0.006513780914247036 max memory_allocated 22562.27978515625 
[2025-03-22 01:25:34 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.019127581268548965 norm:0.005910790525376797 max memory_allocated 22562.27978515625 
[2025-03-22 01:26:07 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.01906040869653225 norm:0.005931911524385214 max memory_allocated 22562.27978515625 
[2025-03-22 01:26:40 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.018719874322414398 norm:0.0047967564314603806 max memory_allocated 22562.27978515625 
[2025-03-22 01:27:12 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.018907876685261726 norm:0.0052040982991456985 max memory_allocated 22562.27978515625 
[2025-03-22 01:27:45 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.018659254536032677 norm:0.00509443087503314 max memory_allocated 22562.27978515625 
[2025-03-22 01:28:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.01867305114865303 norm:0.004614008590579033 max memory_allocated 22562.27978515625 
[2025-03-22 01:28:50 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.018628038465976715 norm:0.004629215691238642 max memory_allocated 22562.27978515625 
[2025-03-22 01:28:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:29:02 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:29:34 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.27019399404525757 norm:0.12429191917181015 max memory_allocated 22562.45166015625 
[2025-03-22 01:30:07 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.1955147683620453 norm:0.07260245829820633 max memory_allocated 22562.45166015625 
[2025-03-22 01:30:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.15763601660728455 norm:0.046683721244335175 max memory_allocated 22562.45166015625 
[2025-03-22 01:31:12 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.14402350783348083 norm:0.042302507907152176 max memory_allocated 22562.45166015625 
[2025-03-22 01:31:44 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.134274423122406 norm:0.03976147249341011 max memory_allocated 22562.45166015625 
[2025-03-22 01:32:17 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.12919063866138458 norm:0.038324352353811264 max memory_allocated 22562.45166015625 
[2025-03-22 01:32:49 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.1232503280043602 norm:0.0369156114757061 max memory_allocated 22562.45166015625 
[2025-03-22 01:33:22 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.11984594911336899 norm:0.0350150465965271 max memory_allocated 22562.45166015625 
[2025-03-22 01:33:54 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.11837194859981537 norm:0.034379225224256516 max memory_allocated 22562.45166015625 
[2025-03-22 01:34:27 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.1165103167295456 norm:0.03237948566675186 max memory_allocated 22562.45166015625 
[2025-03-22 01:34:59 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.11705850064754486 norm:0.03103654831647873 max memory_allocated 22562.45166015625 
[2025-03-22 01:35:32 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.11651937663555145 norm:0.031481195241212845 max memory_allocated 22562.45166015625 
[2025-03-22 01:36:04 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.11548884958028793 norm:0.029975296929478645 max memory_allocated 22562.45166015625 
[2025-03-22 01:36:37 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.11504325270652771 norm:0.02854955941438675 max memory_allocated 22562.45166015625 
[2025-03-22 01:37:09 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.11496637016534805 norm:0.02837861329317093 max memory_allocated 22562.45166015625 
[2025-03-22 01:37:42 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.11551891267299652 norm:0.028198810294270515 max memory_allocated 22562.45166015625 
[2025-03-22 01:38:15 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.11371664702892303 norm:0.027404069900512695 max memory_allocated 22562.45166015625 
[2025-03-22 01:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.11449162662029266 norm:0.02665082924067974 max memory_allocated 22562.45166015625 
[2025-03-22 01:39:20 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.11339621990919113 norm:0.026648417115211487 max memory_allocated 22562.45166015625 
[2025-03-22 01:39:53 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.11333537101745605 norm:0.025352466851472855 max memory_allocated 22562.45166015625 
[2025-03-22 01:40:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:40:05 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:40:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.18566514551639557 norm:0.060691773891448975 max memory_allocated 22562.62353515625 
[2025-03-22 01:41:11 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.15799984335899353 norm:0.04201166331768036 max memory_allocated 22562.62353515625 
[2025-03-22 01:41:43 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.14136725664138794 norm:0.028849180787801743 max memory_allocated 22562.62353515625 
[2025-03-22 01:42:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.13401749730110168 norm:0.021336913108825684 max memory_allocated 22562.62353515625 
[2025-03-22 01:42:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.13063597679138184 norm:0.017440561205148697 max memory_allocated 22562.62353515625 
[2025-03-22 01:43:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.12805405259132385 norm:0.013633286580443382 max memory_allocated 22562.62353515625 
[2025-03-22 01:43:54 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.12624062597751617 norm:0.010687424801290035 max memory_allocated 22562.62353515625 
[2025-03-22 01:44:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.12514394521713257 norm:0.008996701799333096 max memory_allocated 22562.62353515625 
[2025-03-22 01:45:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.12374746054410934 norm:0.00719488225877285 max memory_allocated 22562.62353515625 
[2025-03-22 01:45:33 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.12308793514966965 norm:0.0067718252539634705 max memory_allocated 22562.62353515625 
[2025-03-22 01:46:05 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.12277599424123764 norm:0.0064039467833936214 max memory_allocated 22562.62353515625 
[2025-03-22 01:46:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.12256847321987152 norm:0.006259475369006395 max memory_allocated 22562.62353515625 
[2025-03-22 01:47:11 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.12240380793809891 norm:0.006308000534772873 max memory_allocated 22562.62353515625 
[2025-03-22 01:47:44 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.12232968211174011 norm:0.005960728973150253 max memory_allocated 22562.62353515625 
[2025-03-22 01:48:16 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.122361920773983 norm:0.00566051434725523 max memory_allocated 22562.62353515625 
[2025-03-22 01:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.1223333328962326 norm:0.0056929439306259155 max memory_allocated 22562.62353515625 
[2025-03-22 01:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.12212801724672318 norm:0.005245479289442301 max memory_allocated 22562.62353515625 
[2025-03-22 01:49:55 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.1220446452498436 norm:0.005174298770725727 max memory_allocated 22562.62353515625 
[2025-03-22 01:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.12209809571504593 norm:0.005040670745074749 max memory_allocated 22562.62353515625 
[2025-03-22 01:51:00 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.12209917604923248 norm:0.004960205871611834 max memory_allocated 22562.62353515625 
[2025-03-22 01:51:09 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 01:51:45 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.22018587589263916 norm:0.019464362412691116 max memory_allocated 22562.62353515625 
[2025-03-22 01:52:17 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.19280774891376495 norm:0.0077443802729249 max memory_allocated 22562.62353515625 
[2025-03-22 01:52:50 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.17344744503498077 norm:0.003754837904125452 max memory_allocated 22562.62353515625 
[2025-03-22 01:53:22 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.1666068583726883 norm:0.0029491414315998554 max memory_allocated 22562.62353515625 
[2025-03-22 01:53:55 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.16359487175941467 norm:0.002412771340459585 max memory_allocated 22562.62353515625 
[2025-03-22 01:54:27 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.16136804223060608 norm:0.0019645271822810173 max memory_allocated 22562.62353515625 
[2025-03-22 01:55:00 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.15992197394371033 norm:0.0017529502511024475 max memory_allocated 22562.62353515625 
[2025-03-22 01:55:32 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.15912649035453796 norm:0.001606434234417975 max memory_allocated 22562.62353515625 
[2025-03-22 01:56:05 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.1582612842321396 norm:0.0014855725457891822 max memory_allocated 22562.62353515625 
[2025-03-22 01:56:37 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.1578548401594162 norm:0.00141772604547441 max memory_allocated 22562.62353515625 
[2025-03-22 01:57:10 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.1576431542634964 norm:0.0013640298275277019 max memory_allocated 22562.62353515625 
[2025-03-22 01:57:43 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.15748010575771332 norm:0.0013157300418242812 max memory_allocated 22562.62353515625 
[2025-03-22 01:58:15 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.1571975201368332 norm:0.0012681290972977877 max memory_allocated 22562.62353515625 
[2025-03-22 01:58:48 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.15711194276809692 norm:0.0012410235358402133 max memory_allocated 22562.62353515625 
[2025-03-22 01:59:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.15707270801067352 norm:0.001203211722895503 max memory_allocated 22562.62353515625 
[2025-03-22 01:59:53 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.1569887399673462 norm:0.0011883096303790808 max memory_allocated 22562.62353515625 
[2025-03-22 02:00:26 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.1570892482995987 norm:0.0012069923104718328 max memory_allocated 22562.62353515625 
[2025-03-22 02:00:59 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.15782250463962555 norm:0.0012327185831964016 max memory_allocated 22562.62353515625 
[2025-03-22 02:01:32 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.15754885971546173 norm:0.0012069682125002146 max memory_allocated 22562.62353515625 
[2025-03-22 02:02:04 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.15745557844638824 norm:0.00119936338160187 max memory_allocated 22562.62353515625 
[2025-03-22 02:02:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:02:49 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.27902525663375854 norm:0.03813435882329941 max memory_allocated 22562.73681640625 
[2025-03-22 02:03:21 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.2379918247461319 norm:0.012169549241662025 max memory_allocated 22562.73681640625 
[2025-03-22 02:03:54 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.20944783091545105 norm:0.004871174227446318 max memory_allocated 22562.73681640625 
[2025-03-22 02:04:27 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.20052646100521088 norm:0.0030539128929376602 max memory_allocated 22562.73681640625 
[2025-03-22 02:04:59 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.1961519420146942 norm:0.00228067091666162 max memory_allocated 22562.73681640625 
[2025-03-22 02:05:32 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.19307439029216766 norm:0.001846072729676962 max memory_allocated 22562.73681640625 
[2025-03-22 02:06:05 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.19130733609199524 norm:0.0017402238445356488 max memory_allocated 22562.73681640625 
[2025-03-22 02:06:37 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.18999379873275757 norm:0.001530494075268507 max memory_allocated 22562.73681640625 
[2025-03-22 02:07:10 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.1891176998615265 norm:0.0014460874954238534 max memory_allocated 22562.73681640625 
[2025-03-22 02:07:43 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.18856459856033325 norm:0.0013665970182046294 max memory_allocated 22562.73681640625 
[2025-03-22 02:08:15 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.18812397122383118 norm:0.0013691466301679611 max memory_allocated 22562.73681640625 
[2025-03-22 02:08:48 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.18780431151390076 norm:0.0013339482247829437 max memory_allocated 22562.73681640625 
[2025-03-22 02:09:21 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.18739736080169678 norm:0.0012885099276900291 max memory_allocated 22562.73681640625 
[2025-03-22 02:09:53 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.18709991872310638 norm:0.0012585320509970188 max memory_allocated 22562.73681640625 
[2025-03-22 02:10:26 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.18695762753486633 norm:0.0012658919440582395 max memory_allocated 22562.73681640625 
[2025-03-22 02:10:58 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.18682414293289185 norm:0.0012269672006368637 max memory_allocated 22562.73681640625 
[2025-03-22 02:11:31 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.18670903146266937 norm:0.001214972697198391 max memory_allocated 22562.73681640625 
[2025-03-22 02:12:03 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.18656890094280243 norm:0.0012116702273488045 max memory_allocated 22562.73681640625 
[2025-03-22 02:12:36 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.1865183413028717 norm:0.0012171664275228977 max memory_allocated 22562.73681640625 
[2025-03-22 02:13:09 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.18653512001037598 norm:0.0012069076765328646 max memory_allocated 22562.73681640625 
[2025-03-22 02:13:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:13:53 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.3005681335926056 norm:0.018467171117663383 max memory_allocated 22562.90869140625 
[2025-03-22 02:14:26 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.26723700761795044 norm:0.008003120310604572 max memory_allocated 22562.90869140625 
[2025-03-22 02:14:58 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.23644018173217773 norm:0.0029066551942378283 max memory_allocated 22562.90869140625 
[2025-03-22 02:15:31 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.2263370156288147 norm:0.001979392720386386 max memory_allocated 22562.90869140625 
[2025-03-22 02:16:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.22154249250888824 norm:0.0016274743247777224 max memory_allocated 22562.90869140625 
[2025-03-22 02:16:37 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.21862533688545227 norm:0.001513924216851592 max memory_allocated 22562.90869140625 
[2025-03-22 02:17:09 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.21682916581630707 norm:0.0014478345401585102 max memory_allocated 22562.90869140625 
[2025-03-22 02:17:42 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.21557217836380005 norm:0.0013470101403072476 max memory_allocated 22562.90869140625 
[2025-03-22 02:18:15 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.21473804116249084 norm:0.0013109721476212144 max memory_allocated 22562.90869140625 
[2025-03-22 02:18:47 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.21420490741729736 norm:0.001342603238299489 max memory_allocated 22562.90869140625 
[2025-03-22 02:19:20 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.2139885574579239 norm:0.0013423211639747024 max memory_allocated 22562.90869140625 
[2025-03-22 02:19:53 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.2134697586297989 norm:0.0012870844220742583 max memory_allocated 22562.90869140625 
[2025-03-22 02:20:26 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.2132038176059723 norm:0.0012181764468550682 max memory_allocated 22562.90869140625 
[2025-03-22 02:20:58 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.21304884552955627 norm:0.0012009404599666595 max memory_allocated 22562.90869140625 
[2025-03-22 02:21:31 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.21282124519348145 norm:0.0011458892840892076 max memory_allocated 22562.90869140625 
[2025-03-22 02:22:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.2128111571073532 norm:0.0011431773891672492 max memory_allocated 22562.90869140625 
[2025-03-22 02:22:36 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.21279409527778625 norm:0.00114050495903939 max memory_allocated 22562.90869140625 
[2025-03-22 02:23:09 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.21276196837425232 norm:0.0011646163184195757 max memory_allocated 22562.90869140625 
[2025-03-22 02:23:42 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.21264345943927765 norm:0.0011455725179985166 max memory_allocated 22562.90869140625 
[2025-03-22 02:24:14 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.21257971227169037 norm:0.0011438220972195268 max memory_allocated 22562.90869140625 
[2025-03-22 02:24:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:24:59 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.3736361265182495 norm:0.03739601746201515 max memory_allocated 22563.08056640625 
[2025-03-22 02:25:31 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.32028090953826904 norm:0.014383526518940926 max memory_allocated 22563.08056640625 
[2025-03-22 02:26:04 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.28104645013809204 norm:0.0064647747203707695 max memory_allocated 22563.08056640625 
[2025-03-22 02:26:36 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.26687106490135193 norm:0.004276489373296499 max memory_allocated 22563.08056640625 
[2025-03-22 02:27:09 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.2607276141643524 norm:0.00340341217815876 max memory_allocated 22563.08056640625 
[2025-03-22 02:27:42 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.25609493255615234 norm:0.002834882354363799 max memory_allocated 22563.08056640625 
[2025-03-22 02:28:14 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.252918004989624 norm:0.0024850070476531982 max memory_allocated 22563.08056640625 
[2025-03-22 02:28:47 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.25071457028388977 norm:0.0021759115625172853 max memory_allocated 22563.08056640625 
[2025-03-22 02:29:19 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.24919164180755615 norm:0.0019992724992334843 max memory_allocated 22563.08056640625 
[2025-03-22 02:29:52 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.2480320781469345 norm:0.0018359292298555374 max memory_allocated 22563.08056640625 
[2025-03-22 02:30:25 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.24724361300468445 norm:0.0017472149338573217 max memory_allocated 22563.08056640625 
[2025-03-22 02:30:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.24641971290111542 norm:0.001594499684870243 max memory_allocated 22563.08056640625 
[2025-03-22 02:31:30 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.24628481268882751 norm:0.001541351666674018 max memory_allocated 22563.08056640625 
[2025-03-22 02:32:03 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.24604645371437073 norm:0.001520389225333929 max memory_allocated 22563.08056640625 
[2025-03-22 02:32:35 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.24572855234146118 norm:0.001480663544498384 max memory_allocated 22563.08056640625 
[2025-03-22 02:33:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.24536184966564178 norm:0.001493362826295197 max memory_allocated 22563.08056640625 
[2025-03-22 02:33:41 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.24501632153987885 norm:0.0014867361169308424 max memory_allocated 22563.08056640625 
[2025-03-22 02:34:14 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.244551420211792 norm:0.0014167120680212975 max memory_allocated 22563.08056640625 
[2025-03-22 02:34:46 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.24445141851902008 norm:0.001368337427265942 max memory_allocated 22563.08056640625 
[2025-03-22 02:35:19 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.2445189654827118 norm:0.001366203767247498 max memory_allocated 22563.08056640625 
[2025-03-22 02:35:28 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 02:36:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.41110116243362427 norm:0.0423496812582016 max memory_allocated 22563.25244140625 
[2025-03-22 02:36:36 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.3410671651363373 norm:0.01419759914278984 max memory_allocated 22563.25244140625 
[2025-03-22 02:37:09 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.300010085105896 norm:0.00587160000577569 max memory_allocated 22563.25244140625 
[2025-03-22 02:37:42 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.2872478663921356 norm:0.0037708687596023083 max memory_allocated 22563.25244140625 
[2025-03-22 02:38:14 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.281753808259964 norm:0.003106413409113884 max memory_allocated 22563.25244140625 
[2025-03-22 02:38:47 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.2779967188835144 norm:0.0027103221509605646 max memory_allocated 22563.25244140625 
[2025-03-22 02:39:20 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.2750834822654724 norm:0.0023181051947176456 max memory_allocated 22563.25244140625 
[2025-03-22 02:39:52 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.2728806138038635 norm:0.002060568891465664 max memory_allocated 22563.25244140625 
[2025-03-22 02:40:25 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.2714928388595581 norm:0.0019381269812583923 max memory_allocated 22563.25244140625 
[2025-03-22 02:40:58 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.27050355076789856 norm:0.0018267121631652117 max memory_allocated 22563.25244140625 
[2025-03-22 02:41:30 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.2697390019893646 norm:0.0017505765426903963 max memory_allocated 22563.25244140625 
[2025-03-22 02:42:03 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.2690284550189972 norm:0.0016683005960658193 max memory_allocated 22563.25244140625 
[2025-03-22 02:42:36 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.2684342861175537 norm:0.001593761844560504 max memory_allocated 22563.25244140625 
[2025-03-22 02:43:08 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.268072247505188 norm:0.0015818849205970764 max memory_allocated 22563.25244140625 
[2025-03-22 02:43:41 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.26762253046035767 norm:0.0015049622161313891 max memory_allocated 22563.25244140625 
[2025-03-22 02:44:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.26711440086364746 norm:0.0014348217519000173 max memory_allocated 22563.25244140625 
[2025-03-22 02:44:46 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.2668040990829468 norm:0.0013924188679084182 max memory_allocated 22563.25244140625 
[2025-03-22 02:45:19 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.2666495144367218 norm:0.0013659714022651315 max memory_allocated 22563.25244140625 
[2025-03-22 02:45:51 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.26637953519821167 norm:0.0013420989271253347 max memory_allocated 22563.25244140625 
[2025-03-22 02:46:24 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.2663176655769348 norm:0.0013092650333419442 max memory_allocated 22563.25244140625 
[2025-03-22 02:46:33 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 02:47:08 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.3958219885826111 norm:0.024688608944416046 max memory_allocated 22563.42431640625 
[2025-03-22 02:47:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.35010436177253723 norm:0.009029578417539597 max memory_allocated 22563.42431640625 
[2025-03-22 02:48:14 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.316182017326355 norm:0.003800135338678956 max memory_allocated 22563.42431640625 
[2025-03-22 02:48:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.3040510416030884 norm:0.0024228058755397797 max memory_allocated 22563.42431640625 
[2025-03-22 02:49:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.29859694838523865 norm:0.0021168524399399757 max memory_allocated 22563.42431640625 
[2025-03-22 02:49:52 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.2947206497192383 norm:0.0019339703721925616 max memory_allocated 22563.42431640625 
[2025-03-22 02:50:24 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.2919241487979889 norm:0.001752245705574751 max memory_allocated 22563.42431640625 
[2025-03-22 02:50:57 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.29010143876075745 norm:0.0016211046604439616 max memory_allocated 22563.42431640625 
[2025-03-22 02:51:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.28858280181884766 norm:0.0014939629472792149 max memory_allocated 22563.42431640625 
[2025-03-22 02:52:03 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.28744542598724365 norm:0.0014318471075966954 max memory_allocated 22563.42431640625 
[2025-03-22 02:52:35 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.28668707609176636 norm:0.001375012332573533 max memory_allocated 22563.42431640625 
[2025-03-22 02:53:08 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.2861595153808594 norm:0.0013278957922011614 max memory_allocated 22563.42431640625 
[2025-03-22 02:53:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.28565001487731934 norm:0.0012855001259595156 max memory_allocated 22563.42431640625 
[2025-03-22 02:54:14 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.28536519408226013 norm:0.0012722332030534744 max memory_allocated 22563.42431640625 
[2025-03-22 02:54:46 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.28519558906555176 norm:0.0012824517907574773 max memory_allocated 22563.42431640625 
[2025-03-22 02:55:19 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.2849980890750885 norm:0.001252866117283702 max memory_allocated 22563.42431640625 
[2025-03-22 02:55:52 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.28483691811561584 norm:0.0012438952689990401 max memory_allocated 22563.42431640625 
[2025-03-22 02:56:25 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.2845197916030884 norm:0.0012391499476507306 max memory_allocated 22563.42431640625 
[2025-03-22 02:56:57 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.28438740968704224 norm:0.0012271745363250375 max memory_allocated 22563.42431640625 
[2025-03-22 02:57:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.2843059301376343 norm:0.0012038616696372628 max memory_allocated 22563.42431640625 
[2025-03-22 02:57:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 02:58:14 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.4426330626010895 norm:0.041933994740247726 max memory_allocated 22563.59619140625 
[2025-03-22 02:58:47 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.3789820671081543 norm:0.016443831846117973 max memory_allocated 22563.59619140625 
[2025-03-22 02:59:20 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.332331120967865 norm:0.0049171457067132 max memory_allocated 22563.59619140625 
[2025-03-22 02:59:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.3191536068916321 norm:0.002939928323030472 max memory_allocated 22563.59619140625 
[2025-03-22 03:00:25 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.31329286098480225 norm:0.002395499963313341 max memory_allocated 22563.59619140625 
[2025-03-22 03:00:58 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3095901608467102 norm:0.002162726828828454 max memory_allocated 22563.59619140625 
[2025-03-22 03:01:30 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.30691617727279663 norm:0.0019314035307615995 max memory_allocated 22563.59619140625 
[2025-03-22 03:02:03 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.3049490749835968 norm:0.0017691738903522491 max memory_allocated 22563.59619140625 
[2025-03-22 03:02:36 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.30346256494522095 norm:0.0017239265143871307 max memory_allocated 22563.59619140625 
[2025-03-22 03:03:08 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3022695481777191 norm:0.0016354286344721913 max memory_allocated 22563.59619140625 
[2025-03-22 03:03:41 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.3013588786125183 norm:0.0014994002413004637 max memory_allocated 22563.59619140625 
[2025-03-22 03:04:14 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.3007768988609314 norm:0.001475962228141725 max memory_allocated 22563.59619140625 
[2025-03-22 03:04:46 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.3002122938632965 norm:0.0014325387310236692 max memory_allocated 22563.59619140625 
[2025-03-22 03:05:19 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.29977497458457947 norm:0.0013702798169106245 max memory_allocated 22563.59619140625 
[2025-03-22 03:05:52 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.2993161678314209 norm:0.0013267466565594077 max memory_allocated 22563.59619140625 
[2025-03-22 03:06:24 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.2990681529045105 norm:0.0013027852401137352 max memory_allocated 22563.59619140625 
[2025-03-22 03:06:57 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.29888561367988586 norm:0.0012979954481124878 max memory_allocated 22563.59619140625 
[2025-03-22 03:07:30 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.29867327213287354 norm:0.0012347640004009008 max memory_allocated 22563.59619140625 
[2025-03-22 03:08:03 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.29852786660194397 norm:0.0012252216693013906 max memory_allocated 22563.59619140625 
[2025-03-22 03:08:35 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.29834872484207153 norm:0.0011798674240708351 max memory_allocated 22563.59619140625 
[2025-03-22 03:08:44 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:09:20 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.4209434986114502 norm:0.031821109354496 max memory_allocated 22563.76806640625 
[2025-03-22 03:09:52 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.3805701434612274 norm:0.013932176865637302 max memory_allocated 22563.76806640625 
[2025-03-22 03:10:25 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.3458179533481598 norm:0.005978414323180914 max memory_allocated 22563.76806640625 
[2025-03-22 03:10:58 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3300405442714691 norm:0.002574794925749302 max memory_allocated 22563.76806640625 
[2025-03-22 03:11:31 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.3229101300239563 norm:0.0016446957597509027 max memory_allocated 22563.76806640625 
[2025-03-22 03:12:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.31847503781318665 norm:0.0014922111295163631 max memory_allocated 22563.76806640625 
[2025-03-22 03:12:36 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3154738247394562 norm:0.0013642573030665517 max memory_allocated 22563.76806640625 
[2025-03-22 03:13:09 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.3130716383457184 norm:0.0012896992266178131 max memory_allocated 22563.76806640625 
[2025-03-22 03:13:41 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.3115365207195282 norm:0.001220685662701726 max memory_allocated 22563.76806640625 
[2025-03-22 03:14:14 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.3104241192340851 norm:0.0011702778283506632 max memory_allocated 22563.76806640625 
[2025-03-22 03:14:46 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.3096342086791992 norm:0.0011382084339857101 max memory_allocated 22563.76806640625 
[2025-03-22 03:15:19 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.3090330958366394 norm:0.001095691230148077 max memory_allocated 22563.76806640625 
[2025-03-22 03:15:52 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.30845561623573303 norm:0.0010708421468734741 max memory_allocated 22563.76806640625 
[2025-03-22 03:16:24 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3080504834651947 norm:0.0010591302998363972 max memory_allocated 22563.76806640625 
[2025-03-22 03:16:57 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.30775517225265503 norm:0.0010429394897073507 max memory_allocated 22563.76806640625 
[2025-03-22 03:17:30 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.30744555592536926 norm:0.0010201097466051579 max memory_allocated 22563.76806640625 
[2025-03-22 03:18:02 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.307187020778656 norm:0.0009969092207029462 max memory_allocated 22563.76806640625 
[2025-03-22 03:18:35 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.30701103806495667 norm:0.0009964327327907085 max memory_allocated 22563.76806640625 
[2025-03-22 03:19:08 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.30688923597335815 norm:0.0009810726623982191 max memory_allocated 22563.76806640625 
[2025-03-22 03:19:40 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.30668577551841736 norm:0.0009735751082189381 max memory_allocated 22563.76806640625 
[2025-03-22 03:19:49 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 03:20:25 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.4233102798461914 norm:0.022603662684559822 max memory_allocated 22563.93994140625 
[2025-03-22 03:20:57 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.3746408522129059 norm:0.008748955093324184 max memory_allocated 22563.93994140625 
[2025-03-22 03:21:30 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3445896804332733 norm:0.0044455393217504025 max memory_allocated 22563.93994140625 
[2025-03-22 03:22:03 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.33175793290138245 norm:0.0027022736612707376 max memory_allocated 22563.93994140625 
[2025-03-22 03:22:36 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.3256147801876068 norm:0.0018262432422488928 max memory_allocated 22563.93994140625 
[2025-03-22 03:23:08 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.3216603398323059 norm:0.0016100314678624272 max memory_allocated 22563.93994140625 
[2025-03-22 03:23:41 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.31912341713905334 norm:0.0014759915648028255 max memory_allocated 22563.93994140625 
[2025-03-22 03:24:14 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3171854615211487 norm:0.0013700757408514619 max memory_allocated 22563.93994140625 
[2025-03-22 03:24:46 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.315801203250885 norm:0.0012959940358996391 max memory_allocated 22563.93994140625 
[2025-03-22 03:25:19 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.31478238105773926 norm:0.0012275094632059336 max memory_allocated 22563.93994140625 
[2025-03-22 03:25:52 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.314090758562088 norm:0.0011833917815238237 max memory_allocated 22563.93994140625 
[2025-03-22 03:26:25 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.31353676319122314 norm:0.0011389895807951689 max memory_allocated 22563.93994140625 
[2025-03-22 03:26:57 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.31309908628463745 norm:0.0011053034104406834 max memory_allocated 22563.93994140625 
[2025-03-22 03:27:30 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.3128092885017395 norm:0.0010867130476981401 max memory_allocated 22563.93994140625 
[2025-03-22 03:28:03 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.3125056326389313 norm:0.0010417263256385922 max memory_allocated 22563.93994140625 
[2025-03-22 03:28:36 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.3122829794883728 norm:0.0010254803346469998 max memory_allocated 22563.93994140625 
[2025-03-22 03:29:08 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.3121289312839508 norm:0.0009977782610803843 max memory_allocated 22563.93994140625 
[2025-03-22 03:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.31197893619537354 norm:0.001007855054922402 max memory_allocated 22563.93994140625 
[2025-03-22 03:30:14 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.3119608461856842 norm:0.0009937625145539641 max memory_allocated 22563.93994140625 
[2025-03-22 03:30:46 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.3116520047187805 norm:0.0009694414329715073 max memory_allocated 22563.93994140625 
[2025-03-22 03:30:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 03:31:31 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.4110191762447357 norm:0.013697889633476734 max memory_allocated 22564.11181640625 
[2025-03-22 03:32:03 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.37562742829322815 norm:0.005606642458587885 max memory_allocated 22564.11181640625 
[2025-03-22 03:32:36 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.348785936832428 norm:0.002864471636712551 max memory_allocated 22564.11181640625 
[2025-03-22 03:33:09 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.3369087278842926 norm:0.0019037151942029595 max memory_allocated 22564.11181640625 
[2025-03-22 03:33:41 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.3314199447631836 norm:0.0014723293716087937 max memory_allocated 22564.11181640625 
[2025-03-22 03:34:14 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.3278525471687317 norm:0.0012977226870134473 max memory_allocated 22564.11181640625 
[2025-03-22 03:34:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.3254319727420807 norm:0.0011793543817475438 max memory_allocated 22564.11181640625 
[2025-03-22 03:35:19 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.32373639941215515 norm:0.0011044091079384089 max memory_allocated 22564.11181640625 
[2025-03-22 03:35:52 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.3224716782569885 norm:0.0010636596707627177 max memory_allocated 22564.11181640625 
[2025-03-22 03:36:24 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.32153990864753723 norm:0.000991216627880931 max memory_allocated 22564.11181640625 
[2025-03-22 03:36:57 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.32085663080215454 norm:0.0009524165070615709 max memory_allocated 22564.11181640625 
[2025-03-22 03:37:30 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.32034605741500854 norm:0.0009276982164010406 max memory_allocated 22564.11181640625 
[2025-03-22 03:38:02 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.31995925307273865 norm:0.0009212484583258629 max memory_allocated 22564.11181640625 
[2025-03-22 03:38:35 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.31962287425994873 norm:0.0009004189632833004 max memory_allocated 22564.11181640625 
[2025-03-22 03:39:08 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.31930822134017944 norm:0.0008785641985014081 max memory_allocated 22564.11181640625 
[2025-03-22 03:39:40 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.3190765678882599 norm:0.0008653918630443513 max memory_allocated 22564.11181640625 
[2025-03-22 03:40:13 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.3189311921596527 norm:0.0008612837409600616 max memory_allocated 22564.11181640625 
[2025-03-22 03:40:46 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.31882357597351074 norm:0.0008478960953652859 max memory_allocated 22564.11181640625 
[2025-03-22 03:41:19 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.31866633892059326 norm:0.0008451560861431062 max memory_allocated 22564.11181640625 
[2025-03-22 03:41:51 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.3185334801673889 norm:0.0008456790819764137 max memory_allocated 22564.11181640625 
[2025-03-22 03:42:00 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 03:42:36 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.4463723599910736 norm:0.04666155204176903 max memory_allocated 22564.28369140625 
[2025-03-22 03:43:09 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.38626372814178467 norm:0.016288800165057182 max memory_allocated 22564.28369140625 
[2025-03-22 03:43:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.35154640674591064 norm:0.007098001427948475 max memory_allocated 22564.28369140625 
[2025-03-22 03:44:14 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.3393562138080597 norm:0.004297386854887009 max memory_allocated 22564.28369140625 
[2025-03-22 03:44:47 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.3327188491821289 norm:0.0027530589140951633 max memory_allocated 22564.28369140625 
[2025-03-22 03:45:19 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.3285124599933624 norm:0.002209763042628765 max memory_allocated 22564.28369140625 
[2025-03-22 03:45:52 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.32538583874702454 norm:0.0019547955598682165 max memory_allocated 22564.28369140625 
[2025-03-22 03:46:25 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.3230302333831787 norm:0.0017549246549606323 max memory_allocated 22564.28369140625 
[2025-03-22 03:46:57 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.3213314414024353 norm:0.001593235065229237 max memory_allocated 22564.28369140625 
[2025-03-22 03:47:30 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.3201797604560852 norm:0.0014375826576724648 max memory_allocated 22564.28369140625 
[2025-03-22 03:48:03 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3189375400543213 norm:0.001289090607315302 max memory_allocated 22564.28369140625 
[2025-03-22 03:48:35 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.31809577345848083 norm:0.0012545313220471144 max memory_allocated 22564.28369140625 
[2025-03-22 03:49:08 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.3174150586128235 norm:0.0011995363747701049 max memory_allocated 22564.28369140625 
[2025-03-22 03:49:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3169039189815521 norm:0.0011402226518839598 max memory_allocated 22564.28369140625 
[2025-03-22 03:50:13 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.3165576457977295 norm:0.001122077228501439 max memory_allocated 22564.28369140625 
[2025-03-22 03:50:46 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.3161734640598297 norm:0.001077307853847742 max memory_allocated 22564.28369140625 
[2025-03-22 03:51:19 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.31585872173309326 norm:0.001054462045431137 max memory_allocated 22564.28369140625 
[2025-03-22 03:51:51 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.31567293405532837 norm:0.0010265476303175092 max memory_allocated 22564.28369140625 
[2025-03-22 03:52:24 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.3155210614204407 norm:0.0010285981697961688 max memory_allocated 22564.28369140625 
[2025-03-22 03:52:56 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.31537801027297974 norm:0.0010228244354948401 max memory_allocated 22564.28369140625 
[2025-03-22 03:53:05 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 03:53:42 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.40864235162734985 norm:0.0164534579962492 max memory_allocated 22564.45556640625 
[2025-03-22 03:54:15 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.37309446930885315 norm:0.006725548300892115 max memory_allocated 22564.45556640625 
[2025-03-22 03:54:47 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.3474471867084503 norm:0.00295061944052577 max memory_allocated 22564.45556640625 
[2025-03-22 03:55:20 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.33741456270217896 norm:0.0016770700458437204 max memory_allocated 22564.45556640625 
[2025-03-22 03:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.3325485587120056 norm:0.0013571783201768994 max memory_allocated 22564.45556640625 
[2025-03-22 03:56:25 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3296223282814026 norm:0.0012113051488995552 max memory_allocated 22564.45556640625 
[2025-03-22 03:56:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.3276011645793915 norm:0.0011216987622901797 max memory_allocated 22564.45556640625 
[2025-03-22 03:57:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.3261280655860901 norm:0.0010523255914449692 max memory_allocated 22564.45556640625 
[2025-03-22 03:58:03 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3248380124568939 norm:0.0009842825820669532 max memory_allocated 22564.45556640625 
[2025-03-22 03:58:36 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.3239281177520752 norm:0.0009301907848566771 max memory_allocated 22564.45556640625 
[2025-03-22 03:59:09 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.3231675326824188 norm:0.0009048894280567765 max memory_allocated 22564.45556640625 
[2025-03-22 03:59:42 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.32258814573287964 norm:0.0008927297312766314 max memory_allocated 22564.45556640625 
[2025-03-22 04:00:14 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.3221195340156555 norm:0.0008688242523930967 max memory_allocated 22564.45556640625 
[2025-03-22 04:00:47 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.321744829416275 norm:0.0008512212662026286 max memory_allocated 22564.45556640625 
[2025-03-22 04:01:20 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.32140353322029114 norm:0.0008394959149882197 max memory_allocated 22564.45556640625 
[2025-03-22 04:01:52 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.32114097476005554 norm:0.0008093640790320933 max memory_allocated 22564.45556640625 
[2025-03-22 04:02:25 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.32093560695648193 norm:0.0008151584770530462 max memory_allocated 22564.45556640625 
[2025-03-22 04:02:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.32083335518836975 norm:0.0008224798948504031 max memory_allocated 22564.45556640625 
[2025-03-22 04:03:31 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.3206806480884552 norm:0.0008240690804086626 max memory_allocated 22564.45556640625 
[2025-03-22 04:04:03 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.3205415606498718 norm:0.0008239754242822528 max memory_allocated 22564.45556640625 
[2025-03-22 04:04:12 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 04:04:47 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.449582040309906 norm:0.040166161954402924 max memory_allocated 22564.62744140625 
[2025-03-22 04:05:20 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.39388924837112427 norm:0.014423597604036331 max memory_allocated 22564.62744140625 
[2025-03-22 04:05:53 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.3552583158016205 norm:0.005522248335182667 max memory_allocated 22564.62744140625 
[2025-03-22 04:06:25 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.34169888496398926 norm:0.0026840954087674618 max memory_allocated 22564.62744140625 
[2025-03-22 04:06:58 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.33622580766677856 norm:0.0017394045135006309 max memory_allocated 22564.62744140625 
[2025-03-22 04:07:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.3327709436416626 norm:0.0015424469020217657 max memory_allocated 22564.62744140625 
[2025-03-22 04:08:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.33034470677375793 norm:0.0014201949816197157 max memory_allocated 22564.62744140625 
[2025-03-22 04:08:36 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.32840368151664734 norm:0.0013375594280660152 max memory_allocated 22564.62744140625 
[2025-03-22 04:09:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.32682937383651733 norm:0.001272103050723672 max memory_allocated 22564.62744140625 
[2025-03-22 04:09:41 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.32583069801330566 norm:0.0012358147650957108 max memory_allocated 22564.62744140625 
[2025-03-22 04:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.3248918652534485 norm:0.0012074491241946816 max memory_allocated 22564.62744140625 
[2025-03-22 04:10:46 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.3242014944553375 norm:0.0011730623664334416 max memory_allocated 22564.62744140625 
[2025-03-22 04:11:19 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.3235979974269867 norm:0.0011105649173259735 max memory_allocated 22564.62744140625 
[2025-03-22 04:11:52 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.3230966031551361 norm:0.0010829538805410266 max memory_allocated 22564.62744140625 
[2025-03-22 04:12:24 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.3227517604827881 norm:0.0010507428087294102 max memory_allocated 22564.62744140625 
[2025-03-22 04:12:57 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.322447806596756 norm:0.0010181746911257505 max memory_allocated 22564.62744140625 
[2025-03-22 04:13:30 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.3222685754299164 norm:0.0009957361035048962 max memory_allocated 22564.62744140625 
[2025-03-22 04:14:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.3220614790916443 norm:0.0009588643442839384 max memory_allocated 22564.62744140625 
[2025-03-22 04:14:35 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.3218769431114197 norm:0.0009440027642995119 max memory_allocated 22564.62744140625 
[2025-03-22 04:15:08 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.32161474227905273 norm:0.0009273334871977568 max memory_allocated 22564.62744140625 
[2025-03-22 04:15:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 04:15:52 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.45217379927635193 norm:0.0430389903485775 max memory_allocated 22564.79931640625 
[2025-03-22 04:16:25 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.40907174348831177 norm:0.019215751439332962 max memory_allocated 22564.79931640625 
[2025-03-22 04:16:58 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.36956435441970825 norm:0.007963636890053749 max memory_allocated 22564.79931640625 
[2025-03-22 04:17:31 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.3526252806186676 norm:0.00346161425113678 max memory_allocated 22564.79931640625 
[2025-03-22 04:18:03 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.34674543142318726 norm:0.0023906261194497347 max memory_allocated 22564.79931640625 
[2025-03-22 04:18:36 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.34353604912757874 norm:0.002053540199995041 max memory_allocated 22564.79931640625 
[2025-03-22 04:19:09 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.34087589383125305 norm:0.0018395934021100402 max memory_allocated 22564.79931640625 
[2025-03-22 04:19:41 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.3387725353240967 norm:0.0017121379496529698 max memory_allocated 22564.79931640625 
[2025-03-22 04:20:14 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.33701542019844055 norm:0.0015948009677231312 max memory_allocated 22564.79931640625 
[2025-03-22 04:20:47 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.33581283688545227 norm:0.0015546942595392466 max memory_allocated 22564.79931640625 
[2025-03-22 04:21:19 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.3348616361618042 norm:0.001522235805168748 max memory_allocated 22564.79931640625 
[2025-03-22 04:21:52 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.3338499665260315 norm:0.0014194337418302894 max memory_allocated 22564.79931640625 
[2025-03-22 04:22:25 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.3329233229160309 norm:0.0012980126775801182 max memory_allocated 22564.79931640625 
[2025-03-22 04:22:57 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.33219459652900696 norm:0.001223463681526482 max memory_allocated 22564.79931640625 
[2025-03-22 04:23:30 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.3315125107765198 norm:0.001180625637061894 max memory_allocated 22564.79931640625 
[2025-03-22 04:24:02 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.3310478925704956 norm:0.001159489038400352 max memory_allocated 22564.79931640625 
[2025-03-22 04:24:35 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.3307221531867981 norm:0.0011524262372404337 max memory_allocated 22564.79931640625 
[2025-03-22 04:25:08 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.3303842842578888 norm:0.0011315247975289822 max memory_allocated 22564.79931640625 
[2025-03-22 04:25:40 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.330100417137146 norm:0.0011079905088990927 max memory_allocated 22564.79931640625 
[2025-03-22 04:26:13 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.3298422694206238 norm:0.0010873344726860523 max memory_allocated 22564.79931640625 
[2025-03-22 04:26:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 04:26:57 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.4301365613937378 norm:0.033719126135110855 max memory_allocated 22564.97119140625 
[2025-03-22 04:27:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.3985216021537781 norm:0.015891442075371742 max memory_allocated 22564.97119140625 
[2025-03-22 04:28:03 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.3655862510204315 norm:0.004982742480933666 max memory_allocated 22564.97119140625 
[2025-03-22 04:28:35 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.354531854391098 norm:0.0018871321808546782 max memory_allocated 22564.97119140625 
[2025-03-22 04:29:08 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.35115528106689453 norm:0.001591708860360086 max memory_allocated 22564.97119140625 
[2025-03-22 04:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.3487296998500824 norm:0.0014038493391126394 max memory_allocated 22564.97119140625 
[2025-03-22 04:30:14 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.34692254662513733 norm:0.001267046551220119 max memory_allocated 22564.97119140625 
[2025-03-22 04:30:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.3454286754131317 norm:0.0012252182932570577 max memory_allocated 22564.97119140625 
[2025-03-22 04:31:19 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.344249963760376 norm:0.0012057453859597445 max memory_allocated 22564.97119140625 
[2025-03-22 04:31:52 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.3433229625225067 norm:0.0011770668206736445 max memory_allocated 22564.97119140625 
[2025-03-22 04:32:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.34261444211006165 norm:0.0011651597451418638 max memory_allocated 22564.97119140625 
[2025-03-22 04:32:57 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.34197670221328735 norm:0.0011373471934348345 max memory_allocated 22564.97119140625 
[2025-03-22 04:33:30 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.34144023060798645 norm:0.0011355793103575706 max memory_allocated 22564.97119140625 
[2025-03-22 04:34:02 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.34098613262176514 norm:0.0011129615595564246 max memory_allocated 22564.97119140625 
[2025-03-22 04:34:35 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.34060102701187134 norm:0.0011305790394544601 max memory_allocated 22564.97119140625 
[2025-03-22 04:35:08 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.3402899503707886 norm:0.001101939007639885 max memory_allocated 22564.97119140625 
[2025-03-22 04:35:40 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.3399088680744171 norm:0.0010429184185341 max memory_allocated 22564.97119140625 
[2025-03-22 04:36:13 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.33963632583618164 norm:0.001054409658536315 max memory_allocated 22564.97119140625 
[2025-03-22 04:36:46 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.33940762281417847 norm:0.001031145453453064 max memory_allocated 22564.97119140625 
[2025-03-22 04:37:18 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.3391914963722229 norm:0.0009972804691642523 max memory_allocated 22564.97119140625 
[2025-03-22 04:37:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 04:38:02 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.45426812767982483 norm:0.04674983769655228 max memory_allocated 22565.14306640625 
[2025-03-22 04:38:35 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.4275011420249939 norm:0.022191815078258514 max memory_allocated 22565.14306640625 
[2025-03-22 04:39:08 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.39441075921058655 norm:0.008085647597908974 max memory_allocated 22565.14306640625 
[2025-03-22 04:39:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.3798190653324127 norm:0.0036045487504452467 max memory_allocated 22565.14306640625 
[2025-03-22 04:40:13 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.3741185963153839 norm:0.00206317869015038 max memory_allocated 22565.14306640625 
[2025-03-22 04:40:45 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.37095972895622253 norm:0.0013264365261420608 max memory_allocated 22565.14306640625 
[2025-03-22 04:41:18 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.36915478110313416 norm:0.0012768541928380728 max memory_allocated 22565.14306640625 
[2025-03-22 04:41:51 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.36762163043022156 norm:0.0012544402852654457 max memory_allocated 22565.14306640625 
[2025-03-22 04:42:23 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.36632609367370605 norm:0.0012287760619074106 max memory_allocated 22565.14306640625 
[2025-03-22 04:42:56 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.36538752913475037 norm:0.0012295454507693648 max memory_allocated 22565.14306640625 
[2025-03-22 04:43:29 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.3646223843097687 norm:0.0011998772388324142 max memory_allocated 22565.14306640625 
[2025-03-22 04:44:01 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.3639645278453827 norm:0.0011567153269425035 max memory_allocated 22565.14306640625 
[2025-03-22 04:44:34 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.36346301436424255 norm:0.00114451942499727 max memory_allocated 22565.14306640625 
[2025-03-22 04:45:07 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.36287611722946167 norm:0.0010806124191731215 max memory_allocated 22565.14306640625 
[2025-03-22 04:45:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.3625081777572632 norm:0.0010665740119293332 max memory_allocated 22565.14306640625 
[2025-03-22 04:46:12 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.3621884882450104 norm:0.0010697245597839355 max memory_allocated 22565.14306640625 
[2025-03-22 04:46:45 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.36194536089897156 norm:0.0010502621298655868 max memory_allocated 22565.14306640625 
[2025-03-22 04:47:18 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.3616848289966583 norm:0.0010122007224708796 max memory_allocated 22565.14306640625 
[2025-03-22 04:47:51 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.3615400791168213 norm:0.0009964947821572423 max memory_allocated 22565.14306640625 
[2025-03-22 04:48:23 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.3613462746143341 norm:0.0009510701056569815 max memory_allocated 22565.14306640625 
[2025-03-22 04:48:32 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 04:49:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.46434739232063293 norm:0.03981246054172516 max memory_allocated 22565.31494140625 
[2025-03-22 04:49:40 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.4388299882411957 norm:0.017809992656111717 max memory_allocated 22565.31494140625 
[2025-03-22 04:50:13 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.41352593898773193 norm:0.007646680343896151 max memory_allocated 22565.31494140625 
[2025-03-22 04:50:45 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.4020206034183502 norm:0.0030088950879871845 max memory_allocated 22565.31494140625 
[2025-03-22 04:51:18 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.3980240225791931 norm:0.0011856979690492153 max memory_allocated 22565.31494140625 
[2025-03-22 04:51:51 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.3960397243499756 norm:0.0010677400277927518 max memory_allocated 22565.31494140625 
[2025-03-22 04:52:23 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.39446765184402466 norm:0.0010146566201001406 max memory_allocated 22565.31494140625 
[2025-03-22 04:52:56 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.39312541484832764 norm:0.0009700991795398295 max memory_allocated 22565.31494140625 
[2025-03-22 04:53:28 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.3920133411884308 norm:0.0009531095856800675 max memory_allocated 22565.31494140625 
[2025-03-22 04:54:01 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.39112839102745056 norm:0.000923733226954937 max memory_allocated 22565.31494140625 
[2025-03-22 04:54:34 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.3904899060726166 norm:0.0009070662199519575 max memory_allocated 22565.31494140625 
[2025-03-22 04:55:06 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.38991856575012207 norm:0.000897781050298363 max memory_allocated 22565.31494140625 
[2025-03-22 04:55:39 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.3895080089569092 norm:0.0008866112912073731 max memory_allocated 22565.31494140625 
[2025-03-22 04:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.3891177773475647 norm:0.0008449821034446359 max memory_allocated 22565.31494140625 
[2025-03-22 04:56:44 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.3888201117515564 norm:0.0008158946293406188 max memory_allocated 22565.31494140625 
[2025-03-22 04:57:17 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.38851481676101685 norm:0.0008023862610571086 max memory_allocated 22565.31494140625 
[2025-03-22 04:57:50 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.38827505707740784 norm:0.0008134861127473414 max memory_allocated 22565.31494140625 
[2025-03-22 04:58:22 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.3880285322666168 norm:0.0008014784543775022 max memory_allocated 22565.31494140625 
[2025-03-22 04:58:55 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.3878506124019623 norm:0.0007871181587688625 max memory_allocated 22565.31494140625 
[2025-03-22 04:59:28 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.38768264651298523 norm:0.0007773223333060741 max memory_allocated 22565.31494140625 
[2025-03-22 04:59:37 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 05:00:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.4975566267967224 norm:0.02368130534887314 max memory_allocated 22565.48681640625 
[2025-03-22 05:00:45 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.47794634103775024 norm:0.012934504076838493 max memory_allocated 22565.48681640625 
[2025-03-22 05:01:18 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.4547487199306488 norm:0.0058893500827252865 max memory_allocated 22565.48681640625 
[2025-03-22 05:01:50 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.44392648339271545 norm:0.003237218828871846 max memory_allocated 22565.48681640625 
[2025-03-22 05:02:23 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.44015175104141235 norm:0.0024877511896193027 max memory_allocated 22565.48681640625 
[2025-03-22 05:02:56 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.4370211958885193 norm:0.0015770620666444302 max memory_allocated 22565.48681640625 
[2025-03-22 05:03:28 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.4348449110984802 norm:0.0013948134146630764 max memory_allocated 22565.48681640625 
[2025-03-22 05:04:01 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.4332003593444824 norm:0.0012877949047833681 max memory_allocated 22565.48681640625 
[2025-03-22 05:04:34 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.43194660544395447 norm:0.0012172760907560587 max memory_allocated 22565.48681640625 
[2025-03-22 05:05:06 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.4309692680835724 norm:0.0011691920226439834 max memory_allocated 22565.48681640625 
[2025-03-22 05:05:39 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.430159330368042 norm:0.0011055520735681057 max memory_allocated 22565.48681640625 
[2025-03-22 05:06:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.42961370944976807 norm:0.0010894101578742266 max memory_allocated 22565.48681640625 
[2025-03-22 05:06:44 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.42910242080688477 norm:0.0010612132027745247 max memory_allocated 22565.48681640625 
[2025-03-22 05:07:17 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.4287271201610565 norm:0.0010515598114579916 max memory_allocated 22565.48681640625 
[2025-03-22 05:07:50 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.4283170998096466 norm:0.0010169179877266288 max memory_allocated 22565.48681640625 
[2025-03-22 05:08:22 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.42796480655670166 norm:0.0009955376153811812 max memory_allocated 22565.48681640625 
[2025-03-22 05:08:55 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.4276561737060547 norm:0.0009882989106699824 max memory_allocated 22565.48681640625 
[2025-03-22 05:09:27 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.4273609220981598 norm:0.0009537471923977137 max memory_allocated 22565.48681640625 
[2025-03-22 05:10:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.4270592927932739 norm:0.0009250541334040463 max memory_allocated 22565.48681640625 
[2025-03-22 05:10:33 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.4268205463886261 norm:0.0009122215560637414 max memory_allocated 22565.48681640625 
[2025-03-22 05:10:42 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 05:11:17 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.5196141004562378 norm:0.018201585859060287 max memory_allocated 22565.65869140625 
[2025-03-22 05:11:50 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.5022901296615601 norm:0.007957711815834045 max memory_allocated 22565.65869140625 
[2025-03-22 05:12:22 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.4877460300922394 norm:0.0039395117200911045 max memory_allocated 22565.65869140625 
[2025-03-22 05:12:55 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.4803811013698578 norm:0.001516018994152546 max memory_allocated 22565.65869140625 
[2025-03-22 05:13:28 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.4776827096939087 norm:0.0009967725491151214 max memory_allocated 22565.65869140625 
[2025-03-22 05:14:00 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.47603335976600647 norm:0.0009470449876971543 max memory_allocated 22565.65869140625 
[2025-03-22 05:14:33 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.4744867980480194 norm:0.0008982031140476465 max memory_allocated 22565.65869140625 
[2025-03-22 05:15:06 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.47317516803741455 norm:0.0008683718042448163 max memory_allocated 22565.65869140625 
[2025-03-22 05:15:38 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.47220316529273987 norm:0.0008571561775170267 max memory_allocated 22565.65869140625 
[2025-03-22 05:16:11 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.4715051054954529 norm:0.0008164095925167203 max memory_allocated 22565.65869140625 
[2025-03-22 05:16:44 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.4709886908531189 norm:0.000829841592349112 max memory_allocated 22565.65869140625 
[2025-03-22 05:17:17 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.4705086648464203 norm:0.0008121986174955964 max memory_allocated 22565.65869140625 
[2025-03-22 05:17:49 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.4700857996940613 norm:0.0008011682075448334 max memory_allocated 22565.65869140625 
[2025-03-22 05:18:22 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.469751238822937 norm:0.0007815199205651879 max memory_allocated 22565.65869140625 
[2025-03-22 05:18:55 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.4694853127002716 norm:0.0007610604516230524 max memory_allocated 22565.65869140625 
[2025-03-22 05:19:27 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.46926450729370117 norm:0.0007641766569577157 max memory_allocated 22565.65869140625 
[2025-03-22 05:20:00 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.46900442242622375 norm:0.0007371092797257006 max memory_allocated 22565.65869140625 
[2025-03-22 05:20:33 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.468858540058136 norm:0.0007421073969453573 max memory_allocated 22565.65869140625 
[2025-03-22 05:21:05 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.46879544854164124 norm:0.0007332558743655682 max memory_allocated 22565.65869140625 
[2025-03-22 05:21:38 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.46866923570632935 norm:0.0007154895574785769 max memory_allocated 22565.65869140625 
[2025-03-22 05:21:47 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 05:22:22 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:0.584558367729187 norm:0.0163127314299345 max memory_allocated 22565.83056640625 
[2025-03-22 05:22:55 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:0.5643240213394165 norm:0.006333574187010527 max memory_allocated 22565.83056640625 
[2025-03-22 05:23:27 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:0.548436164855957 norm:0.0031775967217981815 max memory_allocated 22565.83056640625 
[2025-03-22 05:24:00 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:0.541351318359375 norm:0.0015336648793891072 max memory_allocated 22565.83056640625 
[2025-03-22 05:24:32 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:0.5387812852859497 norm:0.0012267802376300097 max memory_allocated 22565.83056640625 
[2025-03-22 05:25:05 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:0.5368927121162415 norm:0.0010769948130473495 max memory_allocated 22565.83056640625 
[2025-03-22 05:25:38 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:0.5353981852531433 norm:0.0011132729705423117 max memory_allocated 22565.83056640625 
[2025-03-22 05:26:10 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:0.5342932939529419 norm:0.0010755524272099137 max memory_allocated 22565.83056640625 
[2025-03-22 05:26:43 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:0.533216655254364 norm:0.0009930982487276196 max memory_allocated 22565.83056640625 
[2025-03-22 05:27:15 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:0.5323318243026733 norm:0.000955107796471566 max memory_allocated 22565.83056640625 
[2025-03-22 05:27:48 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.5318530797958374 norm:0.0010336502455174923 max memory_allocated 22565.83056640625 
[2025-03-22 05:28:21 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.5313961505889893 norm:0.0009677776833996177 max memory_allocated 22565.83056640625 
[2025-03-22 05:28:54 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.5308963060379028 norm:0.0009110445971600711 max memory_allocated 22565.83056640625 
[2025-03-22 05:29:26 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.5305140614509583 norm:0.0009664201643317938 max memory_allocated 22565.83056640625 
[2025-03-22 05:29:59 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.5304188132286072 norm:0.000901143008377403 max memory_allocated 22565.83056640625 
[2025-03-22 05:30:32 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.5300254225730896 norm:0.0008655918063595891 max memory_allocated 22565.83056640625 
[2025-03-22 05:31:04 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.5298980474472046 norm:0.0008782397489994764 max memory_allocated 22565.83056640625 
[2025-03-22 05:31:37 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.5296844244003296 norm:0.0008772425353527069 max memory_allocated 22565.83056640625 
[2025-03-22 05:32:10 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.5294326543807983 norm:0.0008674244745634496 max memory_allocated 22565.83056640625 
[2025-03-22 05:32:43 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.529403030872345 norm:0.0008815610781311989 max memory_allocated 22565.83056640625 
[2025-03-22 05:32:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 05:33:27 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:0.633388876914978 norm:0.008283092640340328 max memory_allocated 22566.00244140625 
[2025-03-22 05:34:00 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:0.6196842193603516 norm:0.004256872460246086 max memory_allocated 22566.00244140625 
[2025-03-22 05:34:32 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:0.6055775880813599 norm:0.0022956093307584524 max memory_allocated 22566.00244140625 
[2025-03-22 05:35:05 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:0.5999042987823486 norm:0.0012967786751687527 max memory_allocated 22566.00244140625 
[2025-03-22 05:35:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:0.5971766114234924 norm:0.001011668355204165 max memory_allocated 22566.00244140625 
[2025-03-22 05:36:10 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:0.5951544642448425 norm:0.0008614276885055006 max memory_allocated 22566.00244140625 
[2025-03-22 05:36:43 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:0.5936236381530762 norm:0.0008108055917546153 max memory_allocated 22566.00244140625 
[2025-03-22 05:37:15 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:0.5924375653266907 norm:0.0007854232098907232 max memory_allocated 22566.00244140625 
[2025-03-22 05:37:48 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:0.591525673866272 norm:0.000746563368011266 max memory_allocated 22566.00244140625 
[2025-03-22 05:38:21 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:0.5908613801002502 norm:0.0007220756961032748 max memory_allocated 22566.00244140625 
[2025-03-22 05:38:53 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:0.5903868079185486 norm:0.000704405247233808 max memory_allocated 22566.00244140625 
[2025-03-22 05:39:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:0.5899917483329773 norm:0.000692924833856523 max memory_allocated 22566.00244140625 
[2025-03-22 05:39:58 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:0.5897207260131836 norm:0.0006910254014655948 max memory_allocated 22566.00244140625 
[2025-03-22 05:40:31 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:0.5894623398780823 norm:0.000684100785292685 max memory_allocated 22566.00244140625 
[2025-03-22 05:41:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:0.5892269015312195 norm:0.0006753238849341869 max memory_allocated 22566.00244140625 
[2025-03-22 05:41:36 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:0.5890514850616455 norm:0.0006705429987050593 max memory_allocated 22566.00244140625 
[2025-03-22 05:42:09 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:0.5889242887496948 norm:0.0006622457294724882 max memory_allocated 22566.00244140625 
[2025-03-22 05:42:42 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:0.5887942910194397 norm:0.0006581968627870083 max memory_allocated 22566.00244140625 
[2025-03-22 05:43:14 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:0.588679850101471 norm:0.0006490368978120387 max memory_allocated 22566.00244140625 
[2025-03-22 05:43:47 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:0.5885292887687683 norm:0.0006484168116003275 max memory_allocated 22566.00244140625 
[2025-03-22 05:43:56 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 05:44:31 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:0.7086125612258911 norm:0.01328841783106327 max memory_allocated 22566.17431640625 
[2025-03-22 05:45:04 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:0.691769003868103 norm:0.006124190520495176 max memory_allocated 22566.17431640625 
[2025-03-22 05:45:37 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:0.6755340099334717 norm:0.0030381439719349146 max memory_allocated 22566.17431640625 
[2025-03-22 05:46:09 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:0.6684093475341797 norm:0.0013424784410744905 max memory_allocated 22566.17431640625 
[2025-03-22 05:46:42 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:0.6654039621353149 norm:0.0008748205727897584 max memory_allocated 22566.17431640625 
[2025-03-22 05:47:15 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:0.6632710695266724 norm:0.0008223045151680708 max memory_allocated 22566.17431640625 
[2025-03-22 05:47:48 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:0.6615511178970337 norm:0.0008327972027473152 max memory_allocated 22566.17431640625 
[2025-03-22 05:48:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:0.6601893305778503 norm:0.0008270748076029122 max memory_allocated 22566.17431640625 
[2025-03-22 05:48:53 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:0.6592713594436646 norm:0.0008060622494667768 max memory_allocated 22566.17431640625 
[2025-03-22 05:49:26 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:0.6585068702697754 norm:0.0008053675410337746 max memory_allocated 22566.17431640625 
[2025-03-22 05:49:58 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:0.6579481363296509 norm:0.0007787601789459586 max memory_allocated 22566.17431640625 
[2025-03-22 05:50:31 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:0.6574752926826477 norm:0.0007728544296696782 max memory_allocated 22566.17431640625 
[2025-03-22 05:51:04 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:0.6571715474128723 norm:0.0007668457692489028 max memory_allocated 22566.17431640625 
[2025-03-22 05:51:37 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:0.6568754315376282 norm:0.0007620775140821934 max memory_allocated 22566.17431640625 
[2025-03-22 05:52:09 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:0.6566140055656433 norm:0.0007581860991194844 max memory_allocated 22566.17431640625 
[2025-03-22 05:52:42 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:0.6563703417778015 norm:0.0007547978311777115 max memory_allocated 22566.17431640625 
[2025-03-22 05:53:15 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:0.6561525464057922 norm:0.0007455112063325942 max memory_allocated 22566.17431640625 
[2025-03-22 05:53:47 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:0.6559947729110718 norm:0.0007438427419401705 max memory_allocated 22566.17431640625 
[2025-03-22 05:54:20 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:0.6558327674865723 norm:0.0007487396360374987 max memory_allocated 22566.17431640625 
[2025-03-22 05:54:53 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:0.6557279229164124 norm:0.000760999508202076 max memory_allocated 22566.17431640625 
[2025-03-22 05:55:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 05:55:38 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:0.8003787994384766 norm:0.024676430970430374 max memory_allocated 22566.34619140625 
[2025-03-22 05:56:11 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:0.7842981815338135 norm:0.014810740947723389 max memory_allocated 22566.34619140625 
[2025-03-22 05:56:43 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:0.7665027379989624 norm:0.009183633141219616 max memory_allocated 22566.34619140625 
[2025-03-22 05:57:16 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:0.7595406770706177 norm:0.006499104201793671 max memory_allocated 22566.34619140625 
[2025-03-22 05:57:49 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:0.755998969078064 norm:0.004972795490175486 max memory_allocated 22566.34619140625 
[2025-03-22 05:58:21 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:0.7510658502578735 norm:0.003432708326727152 max memory_allocated 22566.34619140625 
[2025-03-22 05:58:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:0.7482137680053711 norm:0.0032984642311930656 max memory_allocated 22566.34619140625 
[2025-03-22 05:59:27 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:0.7465644478797913 norm:0.0030597851146012545 max memory_allocated 22566.34619140625 
[2025-03-22 05:59:59 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:0.7453472018241882 norm:0.0028220717795193195 max memory_allocated 22566.34619140625 
[2025-03-22 06:00:32 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:0.7439746856689453 norm:0.002218976616859436 max memory_allocated 22566.34619140625 
[2025-03-22 06:01:05 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:0.7435224652290344 norm:0.002455364912748337 max memory_allocated 22566.34619140625 
[2025-03-22 06:01:37 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:0.7430620193481445 norm:0.0021453769877552986 max memory_allocated 22566.34619140625 
[2025-03-22 06:02:10 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:0.7418174743652344 norm:0.001672604470513761 max memory_allocated 22566.34619140625 
[2025-03-22 06:02:43 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:0.7417997717857361 norm:0.0019019910832867026 max memory_allocated 22566.34619140625 
[2025-03-22 06:03:16 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:0.7408319711685181 norm:0.0015900316648185253 max memory_allocated 22566.34619140625 
[2025-03-22 06:03:48 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:0.7408790588378906 norm:0.0015957783907651901 max memory_allocated 22566.34619140625 
[2025-03-22 06:04:21 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:0.7400830984115601 norm:0.0013247336028143764 max memory_allocated 22566.34619140625 
[2025-03-22 06:04:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:0.7398667335510254 norm:0.0014512107009068131 max memory_allocated 22566.34619140625 
[2025-03-22 06:05:26 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:0.7392966747283936 norm:0.0011657752329483628 max memory_allocated 22566.34619140625 
[2025-03-22 06:05:59 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:0.7391231656074524 norm:0.0012973144184798002 max memory_allocated 22566.34619140625 
[2025-03-22 06:06:08 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 06:06:44 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:0.8883445858955383 norm:0.01001671515405178 max memory_allocated 22566.51806640625 
[2025-03-22 06:07:16 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:0.870108962059021 norm:0.005113024264574051 max memory_allocated 22566.51806640625 
[2025-03-22 06:07:49 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:0.8502361178398132 norm:0.0022651180624961853 max memory_allocated 22566.51806640625 
[2025-03-22 06:08:21 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:0.8421264886856079 norm:0.0012783200945705175 max memory_allocated 22566.51806640625 
[2025-03-22 06:08:54 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:0.8386728167533875 norm:0.0010398507583886385 max memory_allocated 22566.51806640625 
[2025-03-22 06:09:27 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:0.8358827233314514 norm:0.0009309756569564342 max memory_allocated 22566.51806640625 
[2025-03-22 06:09:59 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:0.8337385654449463 norm:0.0009136844892054796 max memory_allocated 22566.51806640625 
[2025-03-22 06:10:32 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:0.8319810628890991 norm:0.0008699920726940036 max memory_allocated 22566.51806640625 
[2025-03-22 06:11:04 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:0.8306642770767212 norm:0.000803825561888516 max memory_allocated 22566.51806640625 
[2025-03-22 06:11:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:0.8297153115272522 norm:0.0007754231337457895 max memory_allocated 22566.51806640625 
[2025-03-22 06:12:10 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:0.8290299773216248 norm:0.0007588909938931465 max memory_allocated 22566.51806640625 
[2025-03-22 06:12:42 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:0.8284931182861328 norm:0.0007639338145963848 max memory_allocated 22566.51806640625 
[2025-03-22 06:13:15 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:0.8280812501907349 norm:0.0007817893638275564 max memory_allocated 22566.51806640625 
[2025-03-22 06:13:48 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:0.8277143836021423 norm:0.0008081170963123441 max memory_allocated 22566.51806640625 
[2025-03-22 06:14:21 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:0.8273836970329285 norm:0.0008165706531144679 max memory_allocated 22566.51806640625 
[2025-03-22 06:14:53 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:0.8270304203033447 norm:0.0008195065311156213 max memory_allocated 22566.51806640625 
[2025-03-22 06:15:26 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:0.8267554640769958 norm:0.0008202409371733665 max memory_allocated 22566.51806640625 
[2025-03-22 06:15:59 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:0.8265433311462402 norm:0.0008226982317864895 max memory_allocated 22566.51806640625 
[2025-03-22 06:16:31 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:0.8263061046600342 norm:0.0008233743137679994 max memory_allocated 22566.51806640625 
[2025-03-22 06:17:04 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:0.8261399865150452 norm:0.000824588118121028 max memory_allocated 22566.51806640625 
[2025-03-22 06:17:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 06:17:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:0.9950205087661743 norm:0.015227975323796272 max memory_allocated 22566.68994140625 
[2025-03-22 06:18:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:0.9741057753562927 norm:0.007207831833511591 max memory_allocated 22566.68994140625 
[2025-03-22 06:18:54 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:0.9536298513412476 norm:0.003587200539186597 max memory_allocated 22566.68994140625 
[2025-03-22 06:19:27 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:0.9448221921920776 norm:0.0022930512204766273 max memory_allocated 22566.68994140625 
[2025-03-22 06:19:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:0.939968466758728 norm:0.0009605597006157041 max memory_allocated 22566.68994140625 
[2025-03-22 06:20:32 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:0.9367918372154236 norm:0.0007871632114984095 max memory_allocated 22566.68994140625 
[2025-03-22 06:21:05 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:0.9342833161354065 norm:0.0007451741257682443 max memory_allocated 22566.68994140625 
[2025-03-22 06:21:37 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:0.9323397278785706 norm:0.000730637926608324 max memory_allocated 22566.68994140625 
[2025-03-22 06:22:10 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:0.9311381578445435 norm:0.0007254242664203048 max memory_allocated 22566.68994140625 
[2025-03-22 06:22:43 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:0.930233359336853 norm:0.0007189377793110907 max memory_allocated 22566.68994140625 
[2025-03-22 06:23:15 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:0.9295386672019958 norm:0.000718605937436223 max memory_allocated 22566.68994140625 
[2025-03-22 06:23:48 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:0.9290052652359009 norm:0.0007079781498759985 max memory_allocated 22566.68994140625 
[2025-03-22 06:24:21 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:0.9285634160041809 norm:0.0007050444837659597 max memory_allocated 22566.68994140625 
[2025-03-22 06:24:53 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:0.9282442927360535 norm:0.0006955942953936756 max memory_allocated 22566.68994140625 
[2025-03-22 06:25:26 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:0.927920937538147 norm:0.0006802013958804309 max memory_allocated 22566.68994140625 
[2025-03-22 06:25:59 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:0.9276260137557983 norm:0.0006734059425070882 max memory_allocated 22566.68994140625 
[2025-03-22 06:26:31 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:0.9274226427078247 norm:0.0006679936777800322 max memory_allocated 22566.68994140625 
[2025-03-22 06:27:04 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:0.9271618723869324 norm:0.0006674811011180282 max memory_allocated 22566.68994140625 
[2025-03-22 06:27:36 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:0.9269634485244751 norm:0.0006645797984674573 max memory_allocated 22566.68994140625 
[2025-03-22 06:28:09 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:0.9268224239349365 norm:0.0006568585522472858 max memory_allocated 22566.68994140625 
[2025-03-22 06:28:18 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 06:28:21 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:28:54 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:1.130024790763855 norm:0.03584645688533783 max memory_allocated 22567.09228515625 
[2025-03-22 06:29:26 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:1.1064836978912354 norm:0.026979763060808182 max memory_allocated 22567.09228515625 
[2025-03-22 06:29:59 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:1.0814634561538696 norm:0.018902789801359177 max memory_allocated 22567.09228515625 
[2025-03-22 06:30:32 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:1.0696649551391602 norm:0.014539776369929314 max memory_allocated 22567.09228515625 
[2025-03-22 06:31:05 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:1.0637383460998535 norm:0.01185044925659895 max memory_allocated 22567.09228515625 
[2025-03-22 06:31:38 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:1.059282898902893 norm:0.009848284535109997 max memory_allocated 22567.09228515625 
[2025-03-22 06:32:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:1.055936336517334 norm:0.008399073034524918 max memory_allocated 22567.09228515625 
[2025-03-22 06:32:44 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:1.0535740852355957 norm:0.007707048207521439 max memory_allocated 22567.09228515625 
[2025-03-22 06:33:16 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:1.0519263744354248 norm:0.007424145005643368 max memory_allocated 22567.09228515625 
[2025-03-22 06:33:49 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:1.0508878231048584 norm:0.007289445959031582 max memory_allocated 22567.09228515625 
[2025-03-22 06:34:22 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:1.0499210357666016 norm:0.007444203831255436 max memory_allocated 22567.09228515625 
[2025-03-22 06:34:55 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:1.0490351915359497 norm:0.007030557841062546 max memory_allocated 22567.09228515625 
[2025-03-22 06:35:28 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:1.048266053199768 norm:0.0068533155135810375 max memory_allocated 22567.09228515625 
[2025-03-22 06:36:01 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:1.0475445985794067 norm:0.006542915012687445 max memory_allocated 22567.09228515625 
[2025-03-22 06:36:34 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:1.0470958948135376 norm:0.006569825578480959 max memory_allocated 22567.09228515625 
[2025-03-22 06:37:06 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:1.0466269254684448 norm:0.006409866735339165 max memory_allocated 22567.09228515625 
[2025-03-22 06:37:39 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:1.046172857284546 norm:0.006482089404016733 max memory_allocated 22567.09228515625 
[2025-03-22 06:38:12 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:1.0457463264465332 norm:0.006243728566914797 max memory_allocated 22567.09228515625 
[2025-03-22 06:38:45 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:1.045394778251648 norm:0.006360460538417101 max memory_allocated 22567.09228515625 
[2025-03-22 06:39:18 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:1.045006513595581 norm:0.005955392494797707 max memory_allocated 22567.09228515625 
[2025-03-22 06:39:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 06:39:29 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:40:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:1.2785639762878418 norm:0.03460787236690521 max memory_allocated 22567.26416015625 
[2025-03-22 06:40:35 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:1.2479281425476074 norm:0.02640281245112419 max memory_allocated 22567.26416015625 
[2025-03-22 06:41:08 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:1.2190611362457275 norm:0.01934644766151905 max memory_allocated 22567.26416015625 
[2025-03-22 06:41:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:1.204189658164978 norm:0.015295514836907387 max memory_allocated 22567.26416015625 
[2025-03-22 06:42:13 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:1.1956932544708252 norm:0.011956584639847279 max memory_allocated 22567.26416015625 
[2025-03-22 06:42:46 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:1.190022349357605 norm:0.010021010413765907 max memory_allocated 22567.26416015625 
[2025-03-22 06:43:19 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:1.1863900423049927 norm:0.009388048201799393 max memory_allocated 22567.26416015625 
[2025-03-22 06:43:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:1.1842460632324219 norm:0.00988504383713007 max memory_allocated 22567.26416015625 
[2025-03-22 06:44:24 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:1.1821990013122559 norm:0.009384741075336933 max memory_allocated 22567.26416015625 
[2025-03-22 06:44:57 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:1.1806708574295044 norm:0.008829140104353428 max memory_allocated 22567.26416015625 
[2025-03-22 06:45:30 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:1.179419755935669 norm:0.00829358957707882 max memory_allocated 22567.26416015625 
[2025-03-22 06:46:03 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:1.1786926984786987 norm:0.008341202512383461 max memory_allocated 22567.26416015625 
[2025-03-22 06:46:36 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:1.1775941848754883 norm:0.007983513176441193 max memory_allocated 22567.26416015625 
[2025-03-22 06:47:09 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:1.1769704818725586 norm:0.007928293198347092 max memory_allocated 22567.26416015625 
[2025-03-22 06:47:42 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:1.176303505897522 norm:0.007618040777742863 max memory_allocated 22567.26416015625 
[2025-03-22 06:48:15 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:1.1760168075561523 norm:0.007735928520560265 max memory_allocated 22567.26416015625 
[2025-03-22 06:48:47 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:1.175374984741211 norm:0.007746071554720402 max memory_allocated 22567.26416015625 
[2025-03-22 06:49:20 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:1.175046682357788 norm:0.007588206324726343 max memory_allocated 22567.26416015625 
[2025-03-22 06:49:53 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:1.1743863821029663 norm:0.007079657632857561 max memory_allocated 22567.26416015625 
[2025-03-22 06:50:26 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:1.1742775440216064 norm:0.007384745869785547 max memory_allocated 22567.26416015625 
[2025-03-22 06:50:35 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 06:50:38 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 06:51:11 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:2.9497408866882324 norm:0.6946677565574646 max memory_allocated 22567.43603515625 
[2025-03-22 06:51:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:3.0526742935180664 norm:1.2329537868499756 max memory_allocated 22567.43603515625 
[2025-03-22 06:52:16 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:2.382540225982666 norm:0.5353910326957703 max memory_allocated 22567.43603515625 
[2025-03-22 06:52:49 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:1.9609878063201904 norm:0.2917216420173645 max memory_allocated 22567.43603515625 
[2025-03-22 06:53:22 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:1.8776137828826904 norm:0.236357182264328 max memory_allocated 22567.43603515625 
[2025-03-22 06:53:55 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:1.856687307357788 norm:0.23339764773845673 max memory_allocated 22567.43603515625 
[2025-03-22 06:54:27 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:1.8285558223724365 norm:0.2290075570344925 max memory_allocated 22567.43603515625 
[2025-03-22 06:55:00 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:1.8165019750595093 norm:0.2080320566892624 max memory_allocated 22567.43603515625 
[2025-03-22 06:55:33 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:1.8101701736450195 norm:0.24224719405174255 max memory_allocated 22567.43603515625 
[2025-03-22 06:56:06 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:1.7880182266235352 norm:0.20086225867271423 max memory_allocated 22567.43603515625 
[2025-03-22 06:56:39 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:1.7857855558395386 norm:0.19352054595947266 max memory_allocated 22567.43603515625 
[2025-03-22 06:57:11 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:1.782155990600586 norm:0.19668176770210266 max memory_allocated 22567.43603515625 
[2025-03-22 06:57:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:1.7666712999343872 norm:0.17560914158821106 max memory_allocated 22567.43603515625 
[2025-03-22 06:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:1.7667979001998901 norm:0.16846679151058197 max memory_allocated 22567.43603515625 
[2025-03-22 06:58:50 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:1.762445330619812 norm:0.14238448441028595 max memory_allocated 22567.43603515625 
[2025-03-22 06:59:22 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:1.751671314239502 norm:0.11950676143169403 max memory_allocated 22567.43603515625 
[2025-03-22 06:59:55 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:1.7187352180480957 norm:0.12703779339790344 max memory_allocated 22567.43603515625 
[2025-03-22 07:00:28 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:1.7029424905776978 norm:0.14717957377433777 max memory_allocated 22567.43603515625 
[2025-03-22 07:01:01 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:1.7016527652740479 norm:0.1465742588043213 max memory_allocated 22567.43603515625 
[2025-03-22 07:01:34 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:1.7037900686264038 norm:0.15188461542129517 max memory_allocated 22567.43603515625 
[2025-03-22 07:01:43 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 07:01:45 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 07:02:18 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:3.8860952854156494 norm:0.35526177287101746 max memory_allocated 22567.60791015625 
[2025-03-22 07:02:51 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:3.4769904613494873 norm:0.2752184569835663 max memory_allocated 22567.60791015625 
[2025-03-22 07:03:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:3.141319990158081 norm:0.21099793910980225 max memory_allocated 22567.60791015625 
[2025-03-22 07:03:57 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:3.0471348762512207 norm:0.19574077427387238 max memory_allocated 22567.60791015625 
[2025-03-22 07:04:30 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:3.001711845397949 norm:0.18642082810401917 max memory_allocated 22567.60791015625 
[2025-03-22 07:05:02 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:2.9639956951141357 norm:0.16491959989070892 max memory_allocated 22567.60791015625 
[2025-03-22 07:05:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:2.9299051761627197 norm:0.14804255962371826 max memory_allocated 22567.60791015625 
[2025-03-22 07:06:08 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:2.901904344558716 norm:0.13604587316513062 max memory_allocated 22567.60791015625 
[2025-03-22 07:06:41 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:2.870960235595703 norm:0.12558969855308533 max memory_allocated 22567.60791015625 
[2025-03-22 07:07:14 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:2.8494973182678223 norm:0.11946631222963333 max memory_allocated 22567.60791015625 
[2025-03-22 07:07:46 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:2.830465793609619 norm:0.11018738150596619 max memory_allocated 22567.60791015625 
[2025-03-22 07:08:19 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:2.812507390975952 norm:0.10649873316287994 max memory_allocated 22567.60791015625 
[2025-03-22 07:08:52 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:2.798002243041992 norm:0.09981133043766022 max memory_allocated 22567.60791015625 
[2025-03-22 07:09:25 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:2.7828023433685303 norm:0.09380476176738739 max memory_allocated 22567.60791015625 
[2025-03-22 07:09:58 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:2.775135040283203 norm:0.09586641192436218 max memory_allocated 22567.60791015625 
[2025-03-22 07:10:30 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:2.766648054122925 norm:0.0924578607082367 max memory_allocated 22567.60791015625 
[2025-03-22 07:11:03 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:2.7579293251037598 norm:0.09193817526102066 max memory_allocated 22567.60791015625 
[2025-03-22 07:11:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:2.7496156692504883 norm:0.08730698376893997 max memory_allocated 22567.60791015625 
[2025-03-22 07:12:09 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:2.7444307804107666 norm:0.08890525251626968 max memory_allocated 22567.60791015625 
[2025-03-22 07:12:42 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:2.743553400039673 norm:0.09567159414291382 max memory_allocated 22567.60791015625 
[2025-03-22 07:12:51 root] (main_calibration_a.py 369): INFO 21295.895469665527
[2025-03-22 07:12:55 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 07:14:03 root] (main_calibration_a.py 158): INFO wikitext2 : 8.866567611694336
[2025-03-22 07:14:03 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 07:15:49 root] (main_calibration_a.py 158): INFO c4 : 12.166703224182129
[2025-03-22 09:08:04 root] (main_calibration_a.py 169): INFO {'wikitext2': 8.866567611694336, 'c4': 12.166703224182129, 'results': {'arc_challenge': {'acc': 0.28754266211604096, 'acc_stderr': 0.013226719056266129, 'acc_norm': 0.32337883959044367, 'acc_norm_stderr': 0.013669421630012127}, 'winogrande': {'acc': 0.5706393054459353, 'acc_stderr': 0.013911537499969165}, 'boolq': {'acc': 0.6434250764525994, 'acc_stderr': 0.008377548099415485}, 'hellaswag': {'acc': 0.4431388169687313, 'acc_stderr': 0.004957410545559415, 'acc_norm': 0.5809599681338379, 'acc_norm_stderr': 0.0049239357498424945}, 'piqa': {'acc': 0.6969532100108814, 'acc_stderr': 0.010722648689531517, 'acc_norm': 0.6964091403699674, 'acc_norm_stderr': 0.010728079893076373}, 'arc_easy': {'acc': 0.5627104377104377, 'acc_stderr': 0.0101787684293216, 'acc_norm': 0.44612794612794615, 'acc_norm_stderr': 0.010200057828765008}}, 'versions': {'arc_challenge': 0, 'winogrande': 0, 'boolq': 1, 'hellaswag': 0, 'piqa': 0, 'arc_easy': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 09:08:04 root] (main_calibration_a.py 172): INFO 28.75,56.27,64.34,44.31,69.70,57.06
