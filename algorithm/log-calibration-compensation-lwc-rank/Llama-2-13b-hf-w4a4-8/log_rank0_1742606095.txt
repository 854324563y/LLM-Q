[2025-03-22 01:14:55 root] (main_calibration_a.py 273): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-calibration-compensation-lwc-rank/Llama-2-13b-hf-w4a4-8', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, scale_calibration=True, compensation_calibration=True, rank=8)
[2025-03-22 01:14:57 root] (main_calibration_a.py 340): INFO === start quantization ===
[2025-03-22 01:14:57 root] (main_calibration_a.py 346): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-22 01:14:57 root] (abq_llm_calibration_a.py 62): INFO Starting ...
[2025-03-22 01:15:17 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 0 ===
[2025-03-22 01:15:21 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 0 loss:0.10928213596343994 norm:0.1287369579076767 max memory_allocated 29272.53759765625 
[2025-03-22 01:16:55 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 1 loss:0.054641518741846085 norm:0.06549292802810669 max memory_allocated 29272.53759765625 
[2025-03-22 01:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 2 loss:0.041390277445316315 norm:0.04908289387822151 max memory_allocated 29272.53759765625 
[2025-03-22 01:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 3 loss:0.0342373363673687 norm:0.041087113320827484 max memory_allocated 29272.53759765625 
[2025-03-22 01:19:18 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 4 loss:0.031301140785217285 norm:0.035992685705423355 max memory_allocated 29272.53759765625 
[2025-03-22 01:20:06 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 5 loss:0.029143337160348892 norm:0.03235802799463272 max memory_allocated 29272.53759765625 
[2025-03-22 01:20:54 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 6 loss:0.0274564977735281 norm:0.02849384769797325 max memory_allocated 29272.53759765625 
[2025-03-22 01:21:42 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 7 loss:0.026132548227906227 norm:0.025927899405360222 max memory_allocated 29272.53759765625 
[2025-03-22 01:22:30 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 8 loss:0.025252042338252068 norm:0.023169662803411484 max memory_allocated 29272.53759765625 
[2025-03-22 01:23:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 9 loss:0.024516869336366653 norm:0.020753270015120506 max memory_allocated 29272.53759765625 
[2025-03-22 01:24:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 10 loss:0.02360888570547104 norm:0.018402449786663055 max memory_allocated 29272.53759765625 
[2025-03-22 01:24:53 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 11 loss:0.023084040731191635 norm:0.016630062833428383 max memory_allocated 29272.53759765625 
[2025-03-22 01:25:41 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 12 loss:0.022717460989952087 norm:0.015258640050888062 max memory_allocated 29272.53759765625 
[2025-03-22 01:26:29 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 13 loss:0.02231680415570736 norm:0.014249765314161777 max memory_allocated 29272.53759765625 
[2025-03-22 01:27:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 14 loss:0.022145025432109833 norm:0.013010497204959393 max memory_allocated 29272.53759765625 
[2025-03-22 01:28:05 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 15 loss:0.021947816014289856 norm:0.012106649577617645 max memory_allocated 29272.53759765625 
[2025-03-22 01:28:53 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 16 loss:0.021676478907465935 norm:0.011261295527219772 max memory_allocated 29272.53759765625 
[2025-03-22 01:29:41 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 17 loss:0.021544374525547028 norm:0.010638570412993431 max memory_allocated 29272.53759765625 
[2025-03-22 01:30:29 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 18 loss:0.021349532529711723 norm:0.009871521033346653 max memory_allocated 29272.53759765625 
[2025-03-22 01:31:17 root] (abq_llm_calibration_a.py 358): INFO layer 0 iter 19 loss:0.021124955266714096 norm:0.009297186508774757 max memory_allocated 29272.53759765625 
[2025-03-22 01:31:30 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 1 ===
[2025-03-22 01:31:41 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:32:29 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 0 loss:0.1648978739976883 norm:0.07041044533252716 max memory_allocated 29272.53759765625 
[2025-03-22 01:33:16 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 1 loss:0.11641668528318405 norm:0.06156161054968834 max memory_allocated 29272.53759765625 
[2025-03-22 01:34:04 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 2 loss:0.09815496951341629 norm:0.04502619430422783 max memory_allocated 29272.53759765625 
[2025-03-22 01:34:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 3 loss:0.08995207399129868 norm:0.03584128990769386 max memory_allocated 29272.53759765625 
[2025-03-22 01:35:40 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 4 loss:0.0856948271393776 norm:0.030255882069468498 max memory_allocated 29272.53759765625 
[2025-03-22 01:36:27 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 5 loss:0.0829007625579834 norm:0.02675814926624298 max memory_allocated 29272.53759765625 
[2025-03-22 01:37:15 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 6 loss:0.08076344430446625 norm:0.023217596113681793 max memory_allocated 29272.53759765625 
[2025-03-22 01:38:03 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 7 loss:0.07952035963535309 norm:0.020710010081529617 max memory_allocated 29272.53759765625 
[2025-03-22 01:38:51 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 8 loss:0.07834197580814362 norm:0.018170831725001335 max memory_allocated 29272.53759765625 
[2025-03-22 01:39:39 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 9 loss:0.07732859998941422 norm:0.015653982758522034 max memory_allocated 29272.53759765625 
[2025-03-22 01:40:27 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 10 loss:0.07653580605983734 norm:0.013391926884651184 max memory_allocated 29272.53759765625 
[2025-03-22 01:41:15 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 11 loss:0.07594229280948639 norm:0.011546509340405464 max memory_allocated 29272.53759765625 
[2025-03-22 01:42:03 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 12 loss:0.07537057250738144 norm:0.010120246559381485 max memory_allocated 29272.53759765625 
[2025-03-22 01:42:51 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 13 loss:0.07498281449079514 norm:0.009076854214072227 max memory_allocated 29272.53759765625 
[2025-03-22 01:43:40 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 14 loss:0.074700266122818 norm:0.008321394212543964 max memory_allocated 29272.53759765625 
[2025-03-22 01:44:28 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 15 loss:0.07461046427488327 norm:0.007897473871707916 max memory_allocated 29272.53759765625 
[2025-03-22 01:45:16 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 16 loss:0.07448865473270416 norm:0.007497324142605066 max memory_allocated 29272.53759765625 
[2025-03-22 01:46:04 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 17 loss:0.07436267286539078 norm:0.007179527543485165 max memory_allocated 29272.53759765625 
[2025-03-22 01:46:52 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 18 loss:0.07428488880395889 norm:0.007072877138853073 max memory_allocated 29272.53759765625 
[2025-03-22 01:47:40 root] (abq_llm_calibration_a.py 358): INFO layer 1 iter 19 loss:0.0743846669793129 norm:0.006785923615098 max memory_allocated 29272.53759765625 
[2025-03-22 01:47:54 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 2 ===
[2025-03-22 01:48:02 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 01:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 0 loss:0.17188486456871033 norm:0.04953252896666527 max memory_allocated 29272.91259765625 
[2025-03-22 01:49:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 1 loss:0.14275535941123962 norm:0.04742303490638733 max memory_allocated 29272.91259765625 
[2025-03-22 01:50:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 2 loss:0.12963026762008667 norm:0.035641565918922424 max memory_allocated 29272.91259765625 
[2025-03-22 01:51:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 3 loss:0.1229860931634903 norm:0.029221851378679276 max memory_allocated 29272.91259765625 
[2025-03-22 01:52:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 4 loss:0.11966587603092194 norm:0.025664014741778374 max memory_allocated 29272.91259765625 
[2025-03-22 01:52:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 5 loss:0.11731824278831482 norm:0.021471582353115082 max memory_allocated 29272.91259765625 
[2025-03-22 01:53:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 6 loss:0.1157013475894928 norm:0.018510324880480766 max memory_allocated 29272.91259765625 
[2025-03-22 01:54:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 7 loss:0.11432294547557831 norm:0.016144635155797005 max memory_allocated 29272.91259765625 
[2025-03-22 01:55:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 8 loss:0.11313606053590775 norm:0.01341939251869917 max memory_allocated 29272.91259765625 
[2025-03-22 01:56:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 9 loss:0.11220579594373703 norm:0.011085622012615204 max memory_allocated 29272.91259765625 
[2025-03-22 01:56:49 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 10 loss:0.11133783310651779 norm:0.00898672454059124 max memory_allocated 29272.91259765625 
[2025-03-22 01:57:37 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 11 loss:0.11051782965660095 norm:0.007580183446407318 max memory_allocated 29272.91259765625 
[2025-03-22 01:58:25 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 12 loss:0.11014357209205627 norm:0.007055218331515789 max memory_allocated 29272.91259765625 
[2025-03-22 01:59:13 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 13 loss:0.10993798077106476 norm:0.006733763962984085 max memory_allocated 29272.91259765625 
[2025-03-22 02:00:01 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 14 loss:0.10984832048416138 norm:0.006461122073233128 max memory_allocated 29272.91259765625 
[2025-03-22 02:00:50 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 15 loss:0.10982248187065125 norm:0.006310730706900358 max memory_allocated 29272.91259765625 
[2025-03-22 02:01:38 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 16 loss:0.10963354259729385 norm:0.006050376687198877 max memory_allocated 29272.91259765625 
[2025-03-22 02:02:26 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 17 loss:0.10970868170261383 norm:0.005902760662138462 max memory_allocated 29272.91259765625 
[2025-03-22 02:03:14 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 18 loss:0.10942614078521729 norm:0.005690496414899826 max memory_allocated 29272.91259765625 
[2025-03-22 02:04:03 root] (abq_llm_calibration_a.py 358): INFO layer 2 iter 19 loss:0.10949788987636566 norm:0.005551130976527929 max memory_allocated 29272.91259765625 
[2025-03-22 02:04:16 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 3 ===
[2025-03-22 02:05:11 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 0 loss:0.4665653109550476 norm:0.2077525109052658 max memory_allocated 29272.91259765625 
[2025-03-22 02:05:59 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 1 loss:0.3526856303215027 norm:0.05098257213830948 max memory_allocated 29272.91259765625 
[2025-03-22 02:06:47 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 2 loss:0.292001336812973 norm:0.023555273190140724 max memory_allocated 29272.91259765625 
[2025-03-22 02:07:34 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 3 loss:0.2658630609512329 norm:0.02241692505776882 max memory_allocated 29272.91259765625 
[2025-03-22 02:08:22 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 4 loss:0.25768035650253296 norm:0.02343832515180111 max memory_allocated 29272.91259765625 
[2025-03-22 02:09:10 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 5 loss:0.25028154253959656 norm:0.021289318799972534 max memory_allocated 29272.91259765625 
[2025-03-22 02:09:57 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 6 loss:0.246742844581604 norm:0.017768962308764458 max memory_allocated 29272.91259765625 
[2025-03-22 02:10:45 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 7 loss:0.24431034922599792 norm:0.01485022809356451 max memory_allocated 29272.91259765625 
[2025-03-22 02:11:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 8 loss:0.24228206276893616 norm:0.014186320826411247 max memory_allocated 29272.91259765625 
[2025-03-22 02:12:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 9 loss:0.23979179561138153 norm:0.013827872462570667 max memory_allocated 29272.91259765625 
[2025-03-22 02:13:08 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 10 loss:0.23696167767047882 norm:0.01265656016767025 max memory_allocated 29272.91259765625 
[2025-03-22 02:13:56 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 11 loss:0.23746463656425476 norm:0.01230051089078188 max memory_allocated 29272.91259765625 
[2025-03-22 02:14:44 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 12 loss:0.23434875905513763 norm:0.011751154437661171 max memory_allocated 29272.91259765625 
[2025-03-22 02:15:32 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 13 loss:0.2321397215127945 norm:0.011482259258627892 max memory_allocated 29272.91259765625 
[2025-03-22 02:16:20 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 14 loss:0.23134085536003113 norm:0.011145959608256817 max memory_allocated 29272.91259765625 
[2025-03-22 02:17:08 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 15 loss:0.22925537824630737 norm:0.011118466034531593 max memory_allocated 29272.91259765625 
[2025-03-22 02:17:57 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 16 loss:0.22813737392425537 norm:0.01150590367615223 max memory_allocated 29272.91259765625 
[2025-03-22 02:18:45 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 17 loss:0.22833183407783508 norm:0.010967335663735867 max memory_allocated 29272.91259765625 
[2025-03-22 02:19:33 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 18 loss:0.22718501091003418 norm:0.01069976668804884 max memory_allocated 29272.91259765625 
[2025-03-22 02:20:21 root] (abq_llm_calibration_a.py 358): INFO layer 3 iter 19 loss:0.22462981939315796 norm:0.009799525141716003 max memory_allocated 29272.91259765625 
[2025-03-22 02:20:34 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 4 ===
[2025-03-22 02:21:26 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 0 loss:0.261967271566391 norm:0.01090686023235321 max memory_allocated 29272.91259765625 
[2025-03-22 02:22:14 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 1 loss:0.24084343016147614 norm:0.004814981948584318 max memory_allocated 29272.91259765625 
[2025-03-22 02:23:02 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 2 loss:0.225897416472435 norm:0.0027688629925251007 max memory_allocated 29272.91259765625 
[2025-03-22 02:23:50 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 3 loss:0.21988141536712646 norm:0.0020322827622294426 max memory_allocated 29272.91259765625 
[2025-03-22 02:24:38 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 4 loss:0.21694809198379517 norm:0.0016783595783635974 max memory_allocated 29272.91259765625 
[2025-03-22 02:25:26 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 5 loss:0.21456709504127502 norm:0.0014265399659052491 max memory_allocated 29272.91259765625 
[2025-03-22 02:26:13 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 6 loss:0.21321198344230652 norm:0.0013159796362742782 max memory_allocated 29272.91259765625 
[2025-03-22 02:27:01 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 7 loss:0.21248844265937805 norm:0.0012682706583291292 max memory_allocated 29272.91259765625 
[2025-03-22 02:27:49 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 8 loss:0.21184401214122772 norm:0.001215770491398871 max memory_allocated 29272.91259765625 
[2025-03-22 02:28:37 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 9 loss:0.2112598419189453 norm:0.0011859312653541565 max memory_allocated 29272.91259765625 
[2025-03-22 02:29:24 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 10 loss:0.21050170063972473 norm:0.0011353307636454701 max memory_allocated 29272.91259765625 
[2025-03-22 02:30:12 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 11 loss:0.21001844108104706 norm:0.0010847249068319798 max memory_allocated 29272.91259765625 
[2025-03-22 02:31:00 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 12 loss:0.2096307873725891 norm:0.0010659074177965522 max memory_allocated 29272.91259765625 
[2025-03-22 02:31:48 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 13 loss:0.20940804481506348 norm:0.0010376344434916973 max memory_allocated 29272.91259765625 
[2025-03-22 02:32:36 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 14 loss:0.2091185748577118 norm:0.0010227581951767206 max memory_allocated 29272.91259765625 
[2025-03-22 02:33:24 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 15 loss:0.20859192311763763 norm:0.0010035271989181638 max memory_allocated 29272.91259765625 
[2025-03-22 02:34:12 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 16 loss:0.208358496427536 norm:0.0009823620785027742 max memory_allocated 29272.91259765625 
[2025-03-22 02:35:00 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 17 loss:0.2083607316017151 norm:0.0009750744793564081 max memory_allocated 29272.91259765625 
[2025-03-22 02:35:48 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 18 loss:0.20826175808906555 norm:0.0009735775529406965 max memory_allocated 29272.91259765625 
[2025-03-22 02:36:37 root] (abq_llm_calibration_a.py 358): INFO layer 4 iter 19 loss:0.2082035094499588 norm:0.000982103869318962 max memory_allocated 29272.91259765625 
[2025-03-22 02:36:50 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 5 ===
[2025-03-22 02:37:42 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 0 loss:0.2937099039554596 norm:0.012325872667133808 max memory_allocated 29272.91259765625 
[2025-03-22 02:38:30 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 1 loss:0.2683756351470947 norm:0.004624882247298956 max memory_allocated 29272.91259765625 
[2025-03-22 02:39:18 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 2 loss:0.2509649693965912 norm:0.0024930869694799185 max memory_allocated 29272.91259765625 
[2025-03-22 02:40:06 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 3 loss:0.2451419234275818 norm:0.001780270249582827 max memory_allocated 29272.91259765625 
[2025-03-22 02:40:54 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 4 loss:0.24214695394039154 norm:0.0013682160060852766 max memory_allocated 29272.91259765625 
[2025-03-22 02:41:42 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 5 loss:0.23974865674972534 norm:0.001156318816356361 max memory_allocated 29272.91259765625 
[2025-03-22 02:42:30 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 6 loss:0.2381192147731781 norm:0.0010609447490423918 max memory_allocated 29272.91259765625 
[2025-03-22 02:43:17 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 7 loss:0.23697958886623383 norm:0.001011619926430285 max memory_allocated 29272.91259765625 
[2025-03-22 02:44:05 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 8 loss:0.2360628843307495 norm:0.0009796470403671265 max memory_allocated 29272.91259765625 
[2025-03-22 02:44:53 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 9 loss:0.23546506464481354 norm:0.0009682592935860157 max memory_allocated 29272.91259765625 
[2025-03-22 02:45:40 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 10 loss:0.23502413928508759 norm:0.0009470326476730406 max memory_allocated 29272.91259765625 
[2025-03-22 02:46:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 11 loss:0.2346983104944229 norm:0.0009319250239059329 max memory_allocated 29272.91259765625 
[2025-03-22 02:47:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 12 loss:0.2343355417251587 norm:0.0009175142040476203 max memory_allocated 29272.91259765625 
[2025-03-22 02:48:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 13 loss:0.23391035199165344 norm:0.0009028513450175524 max memory_allocated 29272.91259765625 
[2025-03-22 02:48:52 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 14 loss:0.23372741043567657 norm:0.0008832759922370315 max memory_allocated 29272.91259765625 
[2025-03-22 02:49:40 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 15 loss:0.23366820812225342 norm:0.0008826615521684289 max memory_allocated 29272.91259765625 
[2025-03-22 02:50:28 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 16 loss:0.23358604311943054 norm:0.0008847322314977646 max memory_allocated 29272.91259765625 
[2025-03-22 02:51:16 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 17 loss:0.23341163992881775 norm:0.0008841241360642016 max memory_allocated 29272.91259765625 
[2025-03-22 02:52:04 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 18 loss:0.23332566022872925 norm:0.0008815938490442932 max memory_allocated 29272.91259765625 
[2025-03-22 02:52:52 root] (abq_llm_calibration_a.py 358): INFO layer 5 iter 19 loss:0.2331196665763855 norm:0.0008641488384455442 max memory_allocated 29272.91259765625 
[2025-03-22 02:53:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 6 ===
[2025-03-22 02:53:58 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 0 loss:0.32914796471595764 norm:0.02436566911637783 max memory_allocated 29272.91259765625 
[2025-03-22 02:54:46 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 1 loss:0.3030582666397095 norm:0.012953017838299274 max memory_allocated 29272.91259765625 
[2025-03-22 02:55:34 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 2 loss:0.27698999643325806 norm:0.0044395821169018745 max memory_allocated 29272.91259765625 
[2025-03-22 02:56:22 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 3 loss:0.26882266998291016 norm:0.0017798829358071089 max memory_allocated 29272.91259765625 
[2025-03-22 02:57:09 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 4 loss:0.26473671197891235 norm:0.0014643680769950151 max memory_allocated 29272.91259765625 
[2025-03-22 02:57:57 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 5 loss:0.26205140352249146 norm:0.0013462193310260773 max memory_allocated 29272.91259765625 
[2025-03-22 02:58:45 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 6 loss:0.25987958908081055 norm:0.0012398614780977368 max memory_allocated 29272.91259765625 
[2025-03-22 02:59:33 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 7 loss:0.2581261098384857 norm:0.0011603491147980094 max memory_allocated 29272.91259765625 
[2025-03-22 03:00:21 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 8 loss:0.25717437267303467 norm:0.0011375019093975425 max memory_allocated 29272.91259765625 
[2025-03-22 03:01:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 9 loss:0.25629961490631104 norm:0.0010972443269565701 max memory_allocated 29272.91259765625 
[2025-03-22 03:01:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 10 loss:0.255707710981369 norm:0.0010656547965481877 max memory_allocated 29272.91259765625 
[2025-03-22 03:02:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 11 loss:0.2553037405014038 norm:0.0010462485952302814 max memory_allocated 29272.91259765625 
[2025-03-22 03:03:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 12 loss:0.2551072835922241 norm:0.001036621630191803 max memory_allocated 29272.91259765625 
[2025-03-22 03:04:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 13 loss:0.25489699840545654 norm:0.0010234207147732377 max memory_allocated 29272.91259765625 
[2025-03-22 03:05:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 14 loss:0.25478318333625793 norm:0.0010177454678341746 max memory_allocated 29272.91259765625 
[2025-03-22 03:05:56 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 15 loss:0.2544994652271271 norm:0.0010099970968440175 max memory_allocated 29272.91259765625 
[2025-03-22 03:06:44 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 16 loss:0.2542693018913269 norm:0.0009982582414522767 max memory_allocated 29272.91259765625 
[2025-03-22 03:07:32 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 17 loss:0.2541651129722595 norm:0.000990519649349153 max memory_allocated 29272.91259765625 
[2025-03-22 03:08:20 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 18 loss:0.25412529706954956 norm:0.000984738813713193 max memory_allocated 29272.91259765625 
[2025-03-22 03:09:08 root] (abq_llm_calibration_a.py 358): INFO layer 6 iter 19 loss:0.25402674078941345 norm:0.000980104086920619 max memory_allocated 29272.91259765625 
[2025-03-22 03:09:22 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 7 ===
[2025-03-22 03:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 0 loss:0.38235560059547424 norm:0.015666188672184944 max memory_allocated 29272.91259765625 
[2025-03-22 03:11:02 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 1 loss:0.3378527760505676 norm:0.005544730927795172 max memory_allocated 29272.91259765625 
[2025-03-22 03:11:50 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 2 loss:0.30872058868408203 norm:0.0025829989463090897 max memory_allocated 29272.91259765625 
[2025-03-22 03:12:38 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 3 loss:0.29916927218437195 norm:0.0017638521967455745 max memory_allocated 29272.91259765625 
[2025-03-22 03:13:25 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 4 loss:0.29489392042160034 norm:0.0015066327759996057 max memory_allocated 29272.91259765625 
[2025-03-22 03:14:13 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 5 loss:0.2917802035808563 norm:0.001379318768158555 max memory_allocated 29272.91259765625 
[2025-03-22 03:15:01 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 6 loss:0.28918468952178955 norm:0.0013011398259550333 max memory_allocated 29272.91259765625 
[2025-03-22 03:15:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 7 loss:0.2876475751399994 norm:0.001243852311745286 max memory_allocated 29272.91259765625 
[2025-03-22 03:16:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 8 loss:0.2864936590194702 norm:0.001184977125376463 max memory_allocated 29272.91259765625 
[2025-03-22 03:17:24 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 9 loss:0.2858826816082001 norm:0.0011707741068676114 max memory_allocated 29272.91259765625 
[2025-03-22 03:18:12 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 10 loss:0.2852298617362976 norm:0.0011455805506557226 max memory_allocated 29272.91259765625 
[2025-03-22 03:19:00 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 11 loss:0.2847665548324585 norm:0.0011380924843251705 max memory_allocated 29272.91259765625 
[2025-03-22 03:19:48 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 12 loss:0.28436553478240967 norm:0.001130870427004993 max memory_allocated 29272.91259765625 
[2025-03-22 03:20:36 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 13 loss:0.2839929759502411 norm:0.0011269201058894396 max memory_allocated 29272.91259765625 
[2025-03-22 03:21:24 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 14 loss:0.2836807370185852 norm:0.0011267950758337975 max memory_allocated 29272.91259765625 
[2025-03-22 03:22:12 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 15 loss:0.28357645869255066 norm:0.0011151029029861093 max memory_allocated 29272.91259765625 
[2025-03-22 03:23:00 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 16 loss:0.2833586633205414 norm:0.0011054814094677567 max memory_allocated 29272.91259765625 
[2025-03-22 03:23:49 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 17 loss:0.2829284369945526 norm:0.0010874401777982712 max memory_allocated 29272.91259765625 
[2025-03-22 03:24:37 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 18 loss:0.2828480005264282 norm:0.001082800910808146 max memory_allocated 29272.91259765625 
[2025-03-22 03:25:25 root] (abq_llm_calibration_a.py 358): INFO layer 7 iter 19 loss:0.2827576994895935 norm:0.0010962944943457842 max memory_allocated 29272.91259765625 
[2025-03-22 03:25:38 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 8 ===
[2025-03-22 03:26:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 0 loss:0.40810588002204895 norm:0.032681163400411606 max memory_allocated 29272.91259765625 
[2025-03-22 03:27:18 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 1 loss:0.3745056986808777 norm:0.021121667698025703 max memory_allocated 29272.91259765625 
[2025-03-22 03:28:06 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 2 loss:0.3390156626701355 norm:0.013356790877878666 max memory_allocated 29272.91259765625 
[2025-03-22 03:28:54 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 3 loss:0.3250012993812561 norm:0.008677937090396881 max memory_allocated 29272.91259765625 
[2025-03-22 03:29:42 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 4 loss:0.3181638717651367 norm:0.00597921758890152 max memory_allocated 29272.91259765625 
[2025-03-22 03:30:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 5 loss:0.31366732716560364 norm:0.004817137960344553 max memory_allocated 29272.91259765625 
[2025-03-22 03:31:18 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 6 loss:0.3104631304740906 norm:0.003961221314966679 max memory_allocated 29272.91259765625 
[2025-03-22 03:32:06 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 7 loss:0.3081413805484772 norm:0.003415010403841734 max memory_allocated 29272.91259765625 
[2025-03-22 03:32:54 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 8 loss:0.30625781416893005 norm:0.0028607905842363834 max memory_allocated 29272.91259765625 
[2025-03-22 03:33:42 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 9 loss:0.3045024871826172 norm:0.0023242628667503595 max memory_allocated 29272.91259765625 
[2025-03-22 03:34:30 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 10 loss:0.303407222032547 norm:0.0020350103732198477 max memory_allocated 29272.91259765625 
[2025-03-22 03:35:17 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 11 loss:0.3025631308555603 norm:0.0018142318585887551 max memory_allocated 29272.91259765625 
[2025-03-22 03:36:05 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 12 loss:0.30198854207992554 norm:0.0017044771229848266 max memory_allocated 29272.91259765625 
[2025-03-22 03:36:53 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 13 loss:0.3014814257621765 norm:0.0015724770491942763 max memory_allocated 29272.91259765625 
[2025-03-22 03:37:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 14 loss:0.3010196387767792 norm:0.001412359531968832 max memory_allocated 29272.91259765625 
[2025-03-22 03:38:29 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 15 loss:0.3007314205169678 norm:0.0013298357371240854 max memory_allocated 29272.91259765625 
[2025-03-22 03:39:17 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 16 loss:0.3004207909107208 norm:0.0012789801694452763 max memory_allocated 29272.91259765625 
[2025-03-22 03:40:05 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 17 loss:0.30032482743263245 norm:0.0012601857306435704 max memory_allocated 29272.91259765625 
[2025-03-22 03:40:53 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 18 loss:0.3001469373703003 norm:0.001222945167683065 max memory_allocated 29272.91259765625 
[2025-03-22 03:41:41 root] (abq_llm_calibration_a.py 358): INFO layer 8 iter 19 loss:0.29996976256370544 norm:0.0011575908865779638 max memory_allocated 29272.91259765625 
[2025-03-22 03:41:55 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 9 ===
[2025-03-22 03:42:46 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 0 loss:0.4672507047653198 norm:0.03325192630290985 max memory_allocated 29272.91259765625 
[2025-03-22 03:43:34 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 1 loss:0.4137575030326843 norm:0.015163330361247063 max memory_allocated 29272.91259765625 
[2025-03-22 03:44:23 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 2 loss:0.36380863189697266 norm:0.005684233270585537 max memory_allocated 29272.91259765625 
[2025-03-22 03:45:11 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 3 loss:0.34593304991722107 norm:0.002954592229798436 max memory_allocated 29272.91259765625 
[2025-03-22 03:45:59 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 4 loss:0.3394673466682434 norm:0.0023438825737684965 max memory_allocated 29272.91259765625 
[2025-03-22 03:46:47 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 5 loss:0.3351556062698364 norm:0.0019094953313469887 max memory_allocated 29272.91259765625 
[2025-03-22 03:47:35 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 6 loss:0.3319166600704193 norm:0.0016169166192412376 max memory_allocated 29272.91259765625 
[2025-03-22 03:48:23 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 7 loss:0.3297087550163269 norm:0.0014840703224763274 max memory_allocated 29272.91259765625 
[2025-03-22 03:49:11 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 8 loss:0.3281503915786743 norm:0.0013983873650431633 max memory_allocated 29272.91259765625 
[2025-03-22 03:49:58 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 9 loss:0.3269982933998108 norm:0.0013394885463640094 max memory_allocated 29272.91259765625 
[2025-03-22 03:50:46 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 10 loss:0.32601428031921387 norm:0.0012886372860521078 max memory_allocated 29272.91259765625 
[2025-03-22 03:51:34 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 11 loss:0.32540592551231384 norm:0.0012625348754227161 max memory_allocated 29272.91259765625 
[2025-03-22 03:52:22 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 12 loss:0.3249082565307617 norm:0.001266638981178403 max memory_allocated 29272.91259765625 
[2025-03-22 03:53:10 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 13 loss:0.324236124753952 norm:0.0011518425308167934 max memory_allocated 29272.91259765625 
[2025-03-22 03:53:58 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 14 loss:0.3238012492656708 norm:0.0011222658213227987 max memory_allocated 29272.91259765625 
[2025-03-22 03:54:46 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 15 loss:0.3236295282840729 norm:0.0011615504045039415 max memory_allocated 29272.91259765625 
[2025-03-22 03:55:34 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 16 loss:0.3232623338699341 norm:0.0011174315586686134 max memory_allocated 29272.91259765625 
[2025-03-22 03:56:22 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 17 loss:0.3227674961090088 norm:0.0010591726750135422 max memory_allocated 29272.91259765625 
[2025-03-22 03:57:10 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 18 loss:0.322579026222229 norm:0.001071383710950613 max memory_allocated 29272.91259765625 
[2025-03-22 03:57:58 root] (abq_llm_calibration_a.py 358): INFO layer 9 iter 19 loss:0.3225235641002655 norm:0.0010618736268952489 max memory_allocated 29272.91259765625 
[2025-03-22 03:58:11 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 10 ===
[2025-03-22 03:59:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 0 loss:0.45154204964637756 norm:0.023443466052412987 max memory_allocated 29272.91259765625 
[2025-03-22 03:59:51 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 1 loss:0.4070449471473694 norm:0.009372041560709476 max memory_allocated 29272.91259765625 
[2025-03-22 04:00:39 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 2 loss:0.37217995524406433 norm:0.003920132294297218 max memory_allocated 29272.91259765625 
[2025-03-22 04:01:27 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 3 loss:0.3590015172958374 norm:0.002410375513136387 max memory_allocated 29272.91259765625 
[2025-03-22 04:02:15 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 4 loss:0.3533976078033447 norm:0.0019340829458087683 max memory_allocated 29272.91259765625 
[2025-03-22 04:03:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 5 loss:0.34952932596206665 norm:0.0015992088010534644 max memory_allocated 29272.91259765625 
[2025-03-22 04:03:51 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 6 loss:0.3468877375125885 norm:0.0014165702741593122 max memory_allocated 29272.91259765625 
[2025-03-22 04:04:39 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 7 loss:0.3450263440608978 norm:0.0012709423899650574 max memory_allocated 29272.91259765625 
[2025-03-22 04:05:27 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 8 loss:0.3436567485332489 norm:0.001177649013698101 max memory_allocated 29272.91259765625 
[2025-03-22 04:06:15 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 9 loss:0.3425571620464325 norm:0.0011136933462694287 max memory_allocated 29272.91259765625 
[2025-03-22 04:07:03 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 10 loss:0.34187060594558716 norm:0.001092306338250637 max memory_allocated 29272.91259765625 
[2025-03-22 04:07:50 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 11 loss:0.3413149118423462 norm:0.0010760943405330181 max memory_allocated 29272.91259765625 
[2025-03-22 04:08:38 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 12 loss:0.340801864862442 norm:0.0010572081664577127 max memory_allocated 29272.91259765625 
[2025-03-22 04:09:26 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 13 loss:0.3405653238296509 norm:0.001066621975041926 max memory_allocated 29272.91259765625 
[2025-03-22 04:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 14 loss:0.3401072323322296 norm:0.0010198339587077498 max memory_allocated 29272.91259765625 
[2025-03-22 04:11:02 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 15 loss:0.33969804644584656 norm:0.0009874063543975353 max memory_allocated 29272.91259765625 
[2025-03-22 04:11:50 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 16 loss:0.3393564820289612 norm:0.0009612786816433072 max memory_allocated 29272.91259765625 
[2025-03-22 04:12:38 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 17 loss:0.3392431139945984 norm:0.0009394834632985294 max memory_allocated 29272.91259765625 
[2025-03-22 04:13:26 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 18 loss:0.33900782465934753 norm:0.0008965926244854927 max memory_allocated 29272.91259765625 
[2025-03-22 04:14:15 root] (abq_llm_calibration_a.py 358): INFO layer 10 iter 19 loss:0.33875760436058044 norm:0.0008921423577703536 max memory_allocated 29272.91259765625 
[2025-03-22 04:14:28 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 11 ===
[2025-03-22 04:15:20 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 0 loss:0.4623475968837738 norm:0.030602969229221344 max memory_allocated 29273.44384765625 
[2025-03-22 04:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 1 loss:0.42327427864074707 norm:0.01231978740543127 max memory_allocated 29273.44384765625 
[2025-03-22 04:16:56 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 2 loss:0.3864210844039917 norm:0.0043074870482087135 max memory_allocated 29273.44384765625 
[2025-03-22 04:17:44 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 3 loss:0.373881071805954 norm:0.001990535296499729 max memory_allocated 29273.44384765625 
[2025-03-22 04:18:32 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 4 loss:0.36834561824798584 norm:0.0013001488987356424 max memory_allocated 29273.44384765625 
[2025-03-22 04:19:20 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 5 loss:0.36473023891448975 norm:0.00116737000644207 max memory_allocated 29273.44384765625 
[2025-03-22 04:20:08 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 6 loss:0.3622732162475586 norm:0.0010842571500688791 max memory_allocated 29273.44384765625 
[2025-03-22 04:20:56 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 7 loss:0.3604671061038971 norm:0.001021711272187531 max memory_allocated 29273.44384765625 
[2025-03-22 04:21:44 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 8 loss:0.35909897089004517 norm:0.0009627951076254249 max memory_allocated 29273.44384765625 
[2025-03-22 04:22:32 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 9 loss:0.35807380080223083 norm:0.0009266408160328865 max memory_allocated 29273.44384765625 
[2025-03-22 04:23:19 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 10 loss:0.3572920560836792 norm:0.0008948445320129395 max memory_allocated 29273.44384765625 
[2025-03-22 04:24:07 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 11 loss:0.35663536190986633 norm:0.0008703098283149302 max memory_allocated 29273.44384765625 
[2025-03-22 04:24:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 12 loss:0.3561975955963135 norm:0.0008523651631549001 max memory_allocated 29273.44384765625 
[2025-03-22 04:25:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 13 loss:0.3558667004108429 norm:0.0008323799120262265 max memory_allocated 29273.44384765625 
[2025-03-22 04:26:31 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 14 loss:0.35556912422180176 norm:0.0008299343753606081 max memory_allocated 29273.44384765625 
[2025-03-22 04:27:19 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 15 loss:0.3553784489631653 norm:0.0008213816909119487 max memory_allocated 29273.44384765625 
[2025-03-22 04:28:07 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 16 loss:0.3551383912563324 norm:0.0008248284575529397 max memory_allocated 29273.44384765625 
[2025-03-22 04:28:55 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 17 loss:0.354934424161911 norm:0.0008033941849134862 max memory_allocated 29273.44384765625 
[2025-03-22 04:29:43 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 18 loss:0.35475972294807434 norm:0.0007897793548181653 max memory_allocated 29273.44384765625 
[2025-03-22 04:30:32 root] (abq_llm_calibration_a.py 358): INFO layer 11 iter 19 loss:0.35457468032836914 norm:0.0007940485957078636 max memory_allocated 29273.44384765625 
[2025-03-22 04:30:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 12 ===
[2025-03-22 04:31:37 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 0 loss:0.47985684871673584 norm:0.01773875765502453 max memory_allocated 29273.44384765625 
[2025-03-22 04:32:25 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 1 loss:0.43446898460388184 norm:0.007619826588779688 max memory_allocated 29273.44384765625 
[2025-03-22 04:33:13 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 2 loss:0.3950537443161011 norm:0.00294869695790112 max memory_allocated 29273.44384765625 
[2025-03-22 04:34:01 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 3 loss:0.38147515058517456 norm:0.0016648953314870596 max memory_allocated 29273.44384765625 
[2025-03-22 04:34:49 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 4 loss:0.37617963552474976 norm:0.0012682033702731133 max memory_allocated 29273.44384765625 
[2025-03-22 04:35:37 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 5 loss:0.37273111939430237 norm:0.0010830684332177043 max memory_allocated 29273.44384765625 
[2025-03-22 04:36:25 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 6 loss:0.37021055817604065 norm:0.0009904581820592284 max memory_allocated 29273.44384765625 
[2025-03-22 04:37:12 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 7 loss:0.36836686730384827 norm:0.0009515666170045733 max memory_allocated 29273.44384765625 
[2025-03-22 04:38:00 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 8 loss:0.3670154809951782 norm:0.0009127204539254308 max memory_allocated 29273.44384765625 
[2025-03-22 04:38:48 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 9 loss:0.3659171760082245 norm:0.0008807408739812672 max memory_allocated 29273.44384765625 
[2025-03-22 04:39:36 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 10 loss:0.36510977149009705 norm:0.0008659836603328586 max memory_allocated 29273.44384765625 
[2025-03-22 04:40:24 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 11 loss:0.36450058221817017 norm:0.0008540570270270109 max memory_allocated 29273.44384765625 
[2025-03-22 04:41:12 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 12 loss:0.3640919327735901 norm:0.0008510304032824934 max memory_allocated 29273.44384765625 
[2025-03-22 04:42:00 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 13 loss:0.36370977759361267 norm:0.0008044933201745152 max memory_allocated 29273.44384765625 
[2025-03-22 04:42:48 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 14 loss:0.36340564489364624 norm:0.0007902310462668538 max memory_allocated 29273.44384765625 
[2025-03-22 04:43:36 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 15 loss:0.3630952537059784 norm:0.0007794949342496693 max memory_allocated 29273.44384765625 
[2025-03-22 04:44:24 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 16 loss:0.36299964785575867 norm:0.0007888554246164858 max memory_allocated 29273.44384765625 
[2025-03-22 04:45:12 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 17 loss:0.3627597391605377 norm:0.0007737516425549984 max memory_allocated 29273.44384765625 
[2025-03-22 04:46:00 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 18 loss:0.36259520053863525 norm:0.0007701886352151632 max memory_allocated 29273.44384765625 
[2025-03-22 04:46:49 root] (abq_llm_calibration_a.py 358): INFO layer 12 iter 19 loss:0.3624018728733063 norm:0.0007582781836390495 max memory_allocated 29273.44384765625 
[2025-03-22 04:47:02 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 13 ===
[2025-03-22 04:47:54 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 0 loss:0.4750822186470032 norm:0.023526256904006004 max memory_allocated 29273.44384765625 
[2025-03-22 04:48:42 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 1 loss:0.44094833731651306 norm:0.011469459161162376 max memory_allocated 29273.44384765625 
[2025-03-22 04:49:30 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 2 loss:0.405173659324646 norm:0.005068002734333277 max memory_allocated 29273.44384765625 
[2025-03-22 04:50:18 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 3 loss:0.39101311564445496 norm:0.0020843371748924255 max memory_allocated 29273.44384765625 
[2025-03-22 04:51:06 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 4 loss:0.3855557441711426 norm:0.0014492840273305774 max memory_allocated 29273.44384765625 
[2025-03-22 04:51:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 5 loss:0.38189762830734253 norm:0.0012226355029270053 max memory_allocated 29273.44384765625 
[2025-03-22 04:52:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 6 loss:0.3794681429862976 norm:0.0011272362899035215 max memory_allocated 29273.44384765625 
[2025-03-22 04:53:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 7 loss:0.37745726108551025 norm:0.0010412695119157434 max memory_allocated 29273.44384765625 
[2025-03-22 04:54:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 8 loss:0.3760613799095154 norm:0.0009925463236868382 max memory_allocated 29273.44384765625 
[2025-03-22 04:55:05 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 9 loss:0.3750440180301666 norm:0.0009459840948693454 max memory_allocated 29273.44384765625 
[2025-03-22 04:55:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 10 loss:0.3741651475429535 norm:0.0009283756953664124 max memory_allocated 29273.44384765625 
[2025-03-22 04:56:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 11 loss:0.3735336363315582 norm:0.0009084895718842745 max memory_allocated 29273.44384765625 
[2025-03-22 04:57:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 12 loss:0.37306100130081177 norm:0.0008901458350010216 max memory_allocated 29273.44384765625 
[2025-03-22 04:58:17 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 13 loss:0.3726453185081482 norm:0.0008832000894472003 max memory_allocated 29273.44384765625 
[2025-03-22 04:59:05 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 14 loss:0.37225374579429626 norm:0.0008539829286746681 max memory_allocated 29273.44384765625 
[2025-03-22 04:59:53 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 15 loss:0.37198030948638916 norm:0.000836326100397855 max memory_allocated 29273.44384765625 
[2025-03-22 05:00:41 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 16 loss:0.37174931168556213 norm:0.0008116507669910789 max memory_allocated 29273.44384765625 
[2025-03-22 05:01:29 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 17 loss:0.3716341257095337 norm:0.0008110422641038895 max memory_allocated 29273.44384765625 
[2025-03-22 05:02:18 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 18 loss:0.3714763820171356 norm:0.0008042494882829487 max memory_allocated 29273.44384765625 
[2025-03-22 05:03:06 root] (abq_llm_calibration_a.py 358): INFO layer 13 iter 19 loss:0.37114638090133667 norm:0.0007852718699723482 max memory_allocated 29273.44384765625 
[2025-03-22 05:03:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 14 ===
[2025-03-22 05:04:11 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 0 loss:0.4795190095901489 norm:0.018827129155397415 max memory_allocated 29274.00634765625 
[2025-03-22 05:04:59 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 1 loss:0.4450892210006714 norm:0.009861276485025883 max memory_allocated 29274.00634765625 
[2025-03-22 05:05:47 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 2 loss:0.4153462052345276 norm:0.005374584812670946 max memory_allocated 29274.00634765625 
[2025-03-22 05:06:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 3 loss:0.4034901559352875 norm:0.0034228998702019453 max memory_allocated 29274.00634765625 
[2025-03-22 05:07:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 4 loss:0.3977421522140503 norm:0.0024502298329025507 max memory_allocated 29274.00634765625 
[2025-03-22 05:08:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 5 loss:0.3941507041454315 norm:0.0018996729049831629 max memory_allocated 29274.00634765625 
[2025-03-22 05:08:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 6 loss:0.3912254571914673 norm:0.0015954882837831974 max memory_allocated 29274.00634765625 
[2025-03-22 05:09:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 7 loss:0.38932743668556213 norm:0.0013736238470301032 max memory_allocated 29274.00634765625 
[2025-03-22 05:10:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 8 loss:0.3878893554210663 norm:0.0011882332619279623 max memory_allocated 29274.00634765625 
[2025-03-22 05:11:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 9 loss:0.38679829239845276 norm:0.0010156853822991252 max memory_allocated 29274.00634765625 
[2025-03-22 05:12:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 10 loss:0.3860218822956085 norm:0.0009248433052562177 max memory_allocated 29274.00634765625 
[2025-03-22 05:12:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 11 loss:0.3853737413883209 norm:0.0008598904241807759 max memory_allocated 29274.00634765625 
[2025-03-22 05:13:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 12 loss:0.38494306802749634 norm:0.0008498911047354341 max memory_allocated 29274.00634765625 
[2025-03-22 05:14:34 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 13 loss:0.3846316635608673 norm:0.0008500770200043917 max memory_allocated 29274.00634765625 
[2025-03-22 05:15:22 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 14 loss:0.3844052255153656 norm:0.0008362510707229376 max memory_allocated 29274.00634765625 
[2025-03-22 05:16:10 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 15 loss:0.3841448426246643 norm:0.0008105991291813552 max memory_allocated 29274.00634765625 
[2025-03-22 05:16:58 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 16 loss:0.38390278816223145 norm:0.0008029064629226923 max memory_allocated 29274.00634765625 
[2025-03-22 05:17:46 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 17 loss:0.3835815489292145 norm:0.0007907317485660315 max memory_allocated 29274.00634765625 
[2025-03-22 05:18:35 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 18 loss:0.3832891583442688 norm:0.0007919545751065016 max memory_allocated 29274.00634765625 
[2025-03-22 05:19:23 root] (abq_llm_calibration_a.py 358): INFO layer 14 iter 19 loss:0.3831075429916382 norm:0.0007938392227515578 max memory_allocated 29274.00634765625 
[2025-03-22 05:19:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 15 ===
[2025-03-22 05:20:28 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 0 loss:0.47150638699531555 norm:0.014221617951989174 max memory_allocated 29274.00634765625 
[2025-03-22 05:21:16 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 1 loss:0.44628360867500305 norm:0.008248064666986465 max memory_allocated 29274.00634765625 
[2025-03-22 05:22:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 2 loss:0.4192574918270111 norm:0.004366999492049217 max memory_allocated 29274.00634765625 
[2025-03-22 05:22:51 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 3 loss:0.4062502980232239 norm:0.002300721127539873 max memory_allocated 29274.00634765625 
[2025-03-22 05:23:39 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 4 loss:0.400631308555603 norm:0.0014613877283409238 max memory_allocated 29274.00634765625 
[2025-03-22 05:24:27 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 5 loss:0.3972208499908447 norm:0.0011485018767416477 max memory_allocated 29274.00634765625 
[2025-03-22 05:25:15 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 6 loss:0.3948582112789154 norm:0.0010550209553912282 max memory_allocated 29274.00634765625 
[2025-03-22 05:26:02 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 7 loss:0.39285799860954285 norm:0.0009750325116328895 max memory_allocated 29274.00634765625 
[2025-03-22 05:26:50 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 8 loss:0.39150702953338623 norm:0.0009356471127830446 max memory_allocated 29274.00634765625 
[2025-03-22 05:27:38 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 9 loss:0.3905019462108612 norm:0.000910286558791995 max memory_allocated 29274.00634765625 
[2025-03-22 05:28:26 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 10 loss:0.3896903693675995 norm:0.000886353780515492 max memory_allocated 29274.00634765625 
[2025-03-22 05:29:14 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 11 loss:0.389065146446228 norm:0.000857515842653811 max memory_allocated 29274.00634765625 
[2025-03-22 05:30:02 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 12 loss:0.38853171467781067 norm:0.0008351424476131797 max memory_allocated 29274.00634765625 
[2025-03-22 05:30:50 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 13 loss:0.3881607949733734 norm:0.0008178601856343448 max memory_allocated 29274.00634765625 
[2025-03-22 05:31:38 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 14 loss:0.3878372013568878 norm:0.0007958407513797283 max memory_allocated 29274.00634765625 
[2025-03-22 05:32:27 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 15 loss:0.3874754011631012 norm:0.000789003330282867 max memory_allocated 29274.00634765625 
[2025-03-22 05:33:15 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 16 loss:0.38721585273742676 norm:0.0007811081013642251 max memory_allocated 29274.00634765625 
[2025-03-22 05:34:03 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 17 loss:0.3870280385017395 norm:0.0007676956010982394 max memory_allocated 29274.00634765625 
[2025-03-22 05:34:51 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 18 loss:0.38687920570373535 norm:0.000766331737395376 max memory_allocated 29274.00634765625 
[2025-03-22 05:35:39 root] (abq_llm_calibration_a.py 358): INFO layer 15 iter 19 loss:0.38677436113357544 norm:0.0007632313063368201 max memory_allocated 29274.00634765625 
[2025-03-22 05:35:52 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 16 ===
[2025-03-22 05:36:44 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 0 loss:0.49007683992385864 norm:0.031501274555921555 max memory_allocated 29274.00634765625 
[2025-03-22 05:37:32 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 1 loss:0.46240681409835815 norm:0.015142771415412426 max memory_allocated 29274.00634765625 
[2025-03-22 05:38:19 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 2 loss:0.4330686330795288 norm:0.0069144112057983875 max memory_allocated 29274.00634765625 
[2025-03-22 05:39:07 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 3 loss:0.4199146032333374 norm:0.003848254680633545 max memory_allocated 29274.00634765625 
[2025-03-22 05:39:55 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 4 loss:0.41435572504997253 norm:0.0024260927457362413 max memory_allocated 29274.00634765625 
[2025-03-22 05:40:43 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 5 loss:0.4111790359020233 norm:0.0020623616874217987 max memory_allocated 29274.00634765625 
[2025-03-22 05:41:31 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 6 loss:0.40878456830978394 norm:0.0016718183178454638 max memory_allocated 29274.00634765625 
[2025-03-22 05:42:18 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 7 loss:0.40658336877822876 norm:0.0013124659890308976 max memory_allocated 29274.00634765625 
[2025-03-22 05:43:06 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 8 loss:0.4049146771430969 norm:0.001221652957610786 max memory_allocated 29274.00634765625 
[2025-03-22 05:43:54 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 9 loss:0.4037909209728241 norm:0.0011808971175923944 max memory_allocated 29274.00634765625 
[2025-03-22 05:44:42 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 10 loss:0.4028996229171753 norm:0.0011187864001840353 max memory_allocated 29274.00634765625 
[2025-03-22 05:45:30 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 11 loss:0.4021989107131958 norm:0.0011025596177205443 max memory_allocated 29274.00634765625 
[2025-03-22 05:46:19 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 12 loss:0.40163126587867737 norm:0.00107754732016474 max memory_allocated 29274.00634765625 
[2025-03-22 05:47:07 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 13 loss:0.4012893736362457 norm:0.0010667676106095314 max memory_allocated 29274.00634765625 
[2025-03-22 05:47:55 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 14 loss:0.4007946848869324 norm:0.0010239165276288986 max memory_allocated 29274.00634765625 
[2025-03-22 05:48:43 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 15 loss:0.40038782358169556 norm:0.0010066285030916333 max memory_allocated 29274.00634765625 
[2025-03-22 05:49:31 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 16 loss:0.4000893235206604 norm:0.0009856183314695954 max memory_allocated 29274.00634765625 
[2025-03-22 05:50:19 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 17 loss:0.3997366726398468 norm:0.0009491434320807457 max memory_allocated 29274.00634765625 
[2025-03-22 05:51:07 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 18 loss:0.39949938654899597 norm:0.0009648347040638328 max memory_allocated 29274.00634765625 
[2025-03-22 05:51:55 root] (abq_llm_calibration_a.py 358): INFO layer 16 iter 19 loss:0.3992745578289032 norm:0.0009483865578658879 max memory_allocated 29274.00634765625 
[2025-03-22 05:52:10 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 17 ===
[2025-03-22 05:53:01 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 0 loss:0.49188223481178284 norm:0.03207617998123169 max memory_allocated 29274.56884765625 
[2025-03-22 05:53:49 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 1 loss:0.46907931566238403 norm:0.016857096925377846 max memory_allocated 29274.56884765625 
[2025-03-22 05:54:37 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 2 loss:0.43873831629753113 norm:0.0058484454639256 max memory_allocated 29274.56884765625 
[2025-03-22 05:55:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 3 loss:0.4272427260875702 norm:0.002887152601033449 max memory_allocated 29274.56884765625 
[2025-03-22 05:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 4 loss:0.42273467779159546 norm:0.0021408468019217253 max memory_allocated 29274.56884765625 
[2025-03-22 05:57:00 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 5 loss:0.41988736391067505 norm:0.0018329332815483212 max memory_allocated 29274.56884765625 
[2025-03-22 05:57:48 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 6 loss:0.4177204370498657 norm:0.0016210947651416063 max memory_allocated 29274.56884765625 
[2025-03-22 05:58:36 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 7 loss:0.41586124897003174 norm:0.0014117001555860043 max memory_allocated 29274.56884765625 
[2025-03-22 05:59:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 8 loss:0.41425633430480957 norm:0.0012806490994989872 max memory_allocated 29274.56884765625 
[2025-03-22 06:00:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 9 loss:0.4130868911743164 norm:0.001231661532074213 max memory_allocated 29274.56884765625 
[2025-03-22 06:01:00 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 10 loss:0.41206997632980347 norm:0.0011763700749725103 max memory_allocated 29274.56884765625 
[2025-03-22 06:01:48 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 11 loss:0.4113713502883911 norm:0.0011451507452875376 max memory_allocated 29274.56884765625 
[2025-03-22 06:02:36 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 12 loss:0.4107643961906433 norm:0.0011057171504944563 max memory_allocated 29274.56884765625 
[2025-03-22 06:03:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 13 loss:0.4102165400981903 norm:0.0010719564743340015 max memory_allocated 29274.56884765625 
[2025-03-22 06:04:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 14 loss:0.40969425439834595 norm:0.0010190170723944902 max memory_allocated 29274.56884765625 
[2025-03-22 06:05:00 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 15 loss:0.4092041254043579 norm:0.0009836314711719751 max memory_allocated 29274.56884765625 
[2025-03-22 06:05:48 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 16 loss:0.40880832076072693 norm:0.000966426741797477 max memory_allocated 29274.56884765625 
[2025-03-22 06:06:36 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 17 loss:0.40855318307876587 norm:0.0009664437966421247 max memory_allocated 29274.56884765625 
[2025-03-22 06:07:24 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 18 loss:0.40814363956451416 norm:0.0009293655748479068 max memory_allocated 29274.56884765625 
[2025-03-22 06:08:12 root] (abq_llm_calibration_a.py 358): INFO layer 17 iter 19 loss:0.4078708291053772 norm:0.00090829748660326 max memory_allocated 29274.56884765625 
[2025-03-22 06:08:26 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 18 ===
[2025-03-22 06:09:17 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 0 loss:0.49158811569213867 norm:0.02944989502429962 max memory_allocated 29274.56884765625 
[2025-03-22 06:10:05 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 1 loss:0.46969640254974365 norm:0.015164332464337349 max memory_allocated 29274.56884765625 
[2025-03-22 06:10:53 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 2 loss:0.4476877748966217 norm:0.00745267141610384 max memory_allocated 29274.56884765625 
[2025-03-22 06:11:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 3 loss:0.4358639419078827 norm:0.00315710692666471 max memory_allocated 29274.56884765625 
[2025-03-22 06:12:28 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 4 loss:0.4313768744468689 norm:0.0020061300601810217 max memory_allocated 29274.56884765625 
[2025-03-22 06:13:16 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 5 loss:0.42899274826049805 norm:0.0017278249142691493 max memory_allocated 29274.56884765625 
[2025-03-22 06:14:04 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 6 loss:0.4271085262298584 norm:0.001498898258432746 max memory_allocated 29274.56884765625 
[2025-03-22 06:14:52 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 7 loss:0.4256584644317627 norm:0.001409588847309351 max memory_allocated 29274.56884765625 
[2025-03-22 06:15:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 8 loss:0.4244023859500885 norm:0.001317025045864284 max memory_allocated 29274.56884765625 
[2025-03-22 06:16:28 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 9 loss:0.4232967495918274 norm:0.0012171142734587193 max memory_allocated 29274.56884765625 
[2025-03-22 06:17:16 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 10 loss:0.42238226532936096 norm:0.0011550160124897957 max memory_allocated 29274.56884765625 
[2025-03-22 06:18:04 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 11 loss:0.4215080738067627 norm:0.0010736240074038506 max memory_allocated 29274.56884765625 
[2025-03-22 06:18:52 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 12 loss:0.4207437038421631 norm:0.0010156771168112755 max memory_allocated 29274.56884765625 
[2025-03-22 06:19:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 13 loss:0.42017725110054016 norm:0.0009817596292123199 max memory_allocated 29274.56884765625 
[2025-03-22 06:20:28 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 14 loss:0.4196476340293884 norm:0.0009538572048768401 max memory_allocated 29274.56884765625 
[2025-03-22 06:21:16 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 15 loss:0.419317364692688 norm:0.0009620618657208979 max memory_allocated 29274.56884765625 
[2025-03-22 06:22:04 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 16 loss:0.41891101002693176 norm:0.0009294346091337502 max memory_allocated 29274.56884765625 
[2025-03-22 06:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 17 loss:0.4186520278453827 norm:0.0009084215271286666 max memory_allocated 29274.56884765625 
[2025-03-22 06:23:40 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 18 loss:0.4183823764324188 norm:0.000881552288774401 max memory_allocated 29274.56884765625 
[2025-03-22 06:24:28 root] (abq_llm_calibration_a.py 358): INFO layer 18 iter 19 loss:0.4182092845439911 norm:0.0008815046749077737 max memory_allocated 29274.56884765625 
[2025-03-22 06:24:41 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 19 ===
[2025-03-22 06:25:33 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 0 loss:0.5031793713569641 norm:0.02127229981124401 max memory_allocated 29274.56884765625 
[2025-03-22 06:26:20 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 1 loss:0.4862058460712433 norm:0.011831684038043022 max memory_allocated 29274.56884765625 
[2025-03-22 06:27:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 2 loss:0.4648980498313904 norm:0.005563851445913315 max memory_allocated 29274.56884765625 
[2025-03-22 06:27:56 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 3 loss:0.45426997542381287 norm:0.0027106052730232477 max memory_allocated 29274.56884765625 
[2025-03-22 06:28:44 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 4 loss:0.4500940144062042 norm:0.001924946322105825 max memory_allocated 29274.56884765625 
[2025-03-22 06:29:32 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 5 loss:0.447591632604599 norm:0.0015888039488345385 max memory_allocated 29274.56884765625 
[2025-03-22 06:30:20 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 6 loss:0.4457138180732727 norm:0.0014087678864598274 max memory_allocated 29274.56884765625 
[2025-03-22 06:31:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 7 loss:0.4441337585449219 norm:0.0012851986102759838 max memory_allocated 29274.56884765625 
[2025-03-22 06:31:56 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 8 loss:0.44291380047798157 norm:0.001238142023794353 max memory_allocated 29274.56884765625 
[2025-03-22 06:32:44 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 9 loss:0.44180262088775635 norm:0.0011618267744779587 max memory_allocated 29274.56884765625 
[2025-03-22 06:33:32 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 10 loss:0.44090738892555237 norm:0.001106946961954236 max memory_allocated 29274.56884765625 
[2025-03-22 06:34:20 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 11 loss:0.44019025564193726 norm:0.001058196765370667 max memory_allocated 29274.56884765625 
[2025-03-22 06:35:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 12 loss:0.4396613538265228 norm:0.0010186962317675352 max memory_allocated 29274.56884765625 
[2025-03-22 06:35:56 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 13 loss:0.4392945468425751 norm:0.001018122537061572 max memory_allocated 29274.56884765625 
[2025-03-22 06:36:44 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 14 loss:0.4388696849346161 norm:0.000989104388281703 max memory_allocated 29274.56884765625 
[2025-03-22 06:37:32 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 15 loss:0.4384891092777252 norm:0.0009729519952088594 max memory_allocated 29274.56884765625 
[2025-03-22 06:38:20 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 16 loss:0.4381856918334961 norm:0.0009531343239359558 max memory_allocated 29274.56884765625 
[2025-03-22 06:39:08 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 17 loss:0.4377705156803131 norm:0.0009119111928157508 max memory_allocated 29274.56884765625 
[2025-03-22 06:39:56 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 18 loss:0.4375954866409302 norm:0.0009232109878212214 max memory_allocated 29274.56884765625 
[2025-03-22 06:40:43 root] (abq_llm_calibration_a.py 358): INFO layer 19 iter 19 loss:0.4373326897621155 norm:0.0008866083226166666 max memory_allocated 29274.56884765625 
[2025-03-22 06:40:57 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 20 ===
[2025-03-22 06:41:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 0 loss:0.5210669040679932 norm:0.020378470420837402 max memory_allocated 29275.13134765625 
[2025-03-22 06:42:36 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 1 loss:0.5052539110183716 norm:0.011366051621735096 max memory_allocated 29275.13134765625 
[2025-03-22 06:43:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 2 loss:0.48615044355392456 norm:0.005542510189116001 max memory_allocated 29275.13134765625 
[2025-03-22 06:44:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 3 loss:0.47649723291397095 norm:0.00277383578941226 max memory_allocated 29275.13134765625 
[2025-03-22 06:45:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 4 loss:0.4724835157394409 norm:0.0018212698632851243 max memory_allocated 29275.13134765625 
[2025-03-22 06:45:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 5 loss:0.4702287018299103 norm:0.0015568110393360257 max memory_allocated 29275.13134765625 
[2025-03-22 06:46:36 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 6 loss:0.46852606534957886 norm:0.0013846313813701272 max memory_allocated 29275.13134765625 
[2025-03-22 06:47:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 7 loss:0.4669240713119507 norm:0.0012169984402135015 max memory_allocated 29275.13134765625 
[2025-03-22 06:48:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 8 loss:0.46572041511535645 norm:0.001131659490056336 max memory_allocated 29275.13134765625 
[2025-03-22 06:49:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 9 loss:0.4647313356399536 norm:0.001099147368222475 max memory_allocated 29275.13134765625 
[2025-03-22 06:49:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 10 loss:0.46383553743362427 norm:0.0010410752147436142 max memory_allocated 29275.13134765625 
[2025-03-22 06:50:36 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 11 loss:0.46312808990478516 norm:0.0010054938029497862 max memory_allocated 29275.13134765625 
[2025-03-22 06:51:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 12 loss:0.4625500440597534 norm:0.0009855530224740505 max memory_allocated 29275.13134765625 
[2025-03-22 06:52:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 13 loss:0.461973637342453 norm:0.0009447740740142763 max memory_allocated 29275.13134765625 
[2025-03-22 06:53:00 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 14 loss:0.4615024924278259 norm:0.000914336706046015 max memory_allocated 29275.13134765625 
[2025-03-22 06:53:48 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 15 loss:0.4611033797264099 norm:0.0008883303962647915 max memory_allocated 29275.13134765625 
[2025-03-22 06:54:36 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 16 loss:0.46086210012435913 norm:0.0008769131381995976 max memory_allocated 29275.13134765625 
[2025-03-22 06:55:24 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 17 loss:0.46065595746040344 norm:0.0008715694420970976 max memory_allocated 29275.13134765625 
[2025-03-22 06:56:12 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 18 loss:0.46039116382598877 norm:0.0008570243371650577 max memory_allocated 29275.13134765625 
[2025-03-22 06:56:59 root] (abq_llm_calibration_a.py 358): INFO layer 20 iter 19 loss:0.46018165349960327 norm:0.0008494226494804025 max memory_allocated 29275.13134765625 
[2025-03-22 06:57:13 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 21 ===
[2025-03-22 06:58:04 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 0 loss:0.5603659749031067 norm:0.019304892048239708 max memory_allocated 29275.31884765625 
[2025-03-22 06:58:52 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 1 loss:0.5434110164642334 norm:0.01095803827047348 max memory_allocated 29275.31884765625 
[2025-03-22 06:59:40 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 2 loss:0.5253679752349854 norm:0.006113184615969658 max memory_allocated 29275.31884765625 
[2025-03-22 07:00:28 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 3 loss:0.5145875811576843 norm:0.0032868790440261364 max memory_allocated 29275.31884765625 
[2025-03-22 07:01:16 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 4 loss:0.5098408460617065 norm:0.002219589427113533 max memory_allocated 29275.31884765625 
[2025-03-22 07:02:04 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 5 loss:0.5074914693832397 norm:0.0018345618154853582 max memory_allocated 29275.31884765625 
[2025-03-22 07:02:52 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 6 loss:0.5057194828987122 norm:0.0016669428441673517 max memory_allocated 29275.31884765625 
[2025-03-22 07:03:40 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 7 loss:0.5041871666908264 norm:0.001521129161119461 max memory_allocated 29275.31884765625 
[2025-03-22 07:04:28 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 8 loss:0.5029065608978271 norm:0.001434477511793375 max memory_allocated 29275.31884765625 
[2025-03-22 07:05:16 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 9 loss:0.5018531084060669 norm:0.0013572650495916605 max memory_allocated 29275.31884765625 
[2025-03-22 07:06:05 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 10 loss:0.5009245276451111 norm:0.0013091163709759712 max memory_allocated 29275.31884765625 
[2025-03-22 07:06:53 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 11 loss:0.5001567602157593 norm:0.0012392885982990265 max memory_allocated 29275.31884765625 
[2025-03-22 07:07:41 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 12 loss:0.4995250999927521 norm:0.0011848128633573651 max memory_allocated 29275.31884765625 
[2025-03-22 07:08:28 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 13 loss:0.4989672899246216 norm:0.0011375518515706062 max memory_allocated 29275.31884765625 
[2025-03-22 07:09:16 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 14 loss:0.4984845519065857 norm:0.0011029369197785854 max memory_allocated 29275.31884765625 
[2025-03-22 07:10:04 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 15 loss:0.4980185329914093 norm:0.0010749532375484705 max memory_allocated 29275.31884765625 
[2025-03-22 07:10:52 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 16 loss:0.49760907888412476 norm:0.001045492710545659 max memory_allocated 29275.31884765625 
[2025-03-22 07:11:40 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 17 loss:0.49719610810279846 norm:0.0010317997075617313 max memory_allocated 29275.31884765625 
[2025-03-22 07:12:28 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 18 loss:0.4969002604484558 norm:0.0010283897863700986 max memory_allocated 29275.31884765625 
[2025-03-22 07:13:15 root] (abq_llm_calibration_a.py 358): INFO layer 21 iter 19 loss:0.4966508448123932 norm:0.0010055163875222206 max memory_allocated 29275.31884765625 
[2025-03-22 07:13:29 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 22 ===
[2025-03-22 07:14:20 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 0 loss:0.580802321434021 norm:0.01130523532629013 max memory_allocated 29275.50634765625 
[2025-03-22 07:15:08 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 1 loss:0.5686174631118774 norm:0.007079934701323509 max memory_allocated 29275.50634765625 
[2025-03-22 07:15:56 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 2 loss:0.5551490783691406 norm:0.004276023246347904 max memory_allocated 29275.50634765625 
[2025-03-22 07:16:44 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 3 loss:0.5478059649467468 norm:0.0024088064674288034 max memory_allocated 29275.50634765625 
[2025-03-22 07:17:32 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 4 loss:0.5447939038276672 norm:0.0018803178099915385 max memory_allocated 29275.50634765625 
[2025-03-22 07:18:20 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 5 loss:0.5428286790847778 norm:0.0015302886022254825 max memory_allocated 29275.50634765625 
[2025-03-22 07:19:08 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 6 loss:0.5411455631256104 norm:0.001338380970992148 max memory_allocated 29275.50634765625 
[2025-03-22 07:19:57 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 7 loss:0.5397670865058899 norm:0.0012399439001455903 max memory_allocated 29275.50634765625 
[2025-03-22 07:20:45 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 8 loss:0.538640558719635 norm:0.0011506167938932776 max memory_allocated 29275.50634765625 
[2025-03-22 07:21:33 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 9 loss:0.5376931428909302 norm:0.0010638268431648612 max memory_allocated 29275.50634765625 
[2025-03-22 07:22:22 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 10 loss:0.536866307258606 norm:0.001043691299855709 max memory_allocated 29275.50634765625 
[2025-03-22 07:23:10 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 11 loss:0.5362458825111389 norm:0.0010206594597548246 max memory_allocated 29275.50634765625 
[2025-03-22 07:23:58 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 12 loss:0.5357431769371033 norm:0.0010222672717645764 max memory_allocated 29275.50634765625 
[2025-03-22 07:24:46 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 13 loss:0.5352318286895752 norm:0.0009835159871727228 max memory_allocated 29275.50634765625 
[2025-03-22 07:25:34 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 14 loss:0.5347911715507507 norm:0.0009180022752843797 max memory_allocated 29275.50634765625 
[2025-03-22 07:26:22 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 15 loss:0.534487247467041 norm:0.0009338001837022603 max memory_allocated 29275.50634765625 
[2025-03-22 07:27:09 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 16 loss:0.534083902835846 norm:0.00091952906223014 max memory_allocated 29275.50634765625 
[2025-03-22 07:27:57 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 17 loss:0.533726692199707 norm:0.0009032581001520157 max memory_allocated 29275.50634765625 
[2025-03-22 07:28:45 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 18 loss:0.533424437046051 norm:0.0008730149129405618 max memory_allocated 29275.50634765625 
[2025-03-22 07:29:33 root] (abq_llm_calibration_a.py 358): INFO layer 22 iter 19 loss:0.5331902503967285 norm:0.0008701019687578082 max memory_allocated 29275.50634765625 
[2025-03-22 07:29:47 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 23 ===
[2025-03-22 07:30:38 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 0 loss:0.6273151636123657 norm:0.02481072209775448 max memory_allocated 29275.69384765625 
[2025-03-22 07:31:26 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 1 loss:0.6150920391082764 norm:0.01434029545634985 max memory_allocated 29275.69384765625 
[2025-03-22 07:32:15 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 2 loss:0.6027935147285461 norm:0.008855979889631271 max memory_allocated 29275.69384765625 
[2025-03-22 07:33:03 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 3 loss:0.596971333026886 norm:0.006166409235447645 max memory_allocated 29275.69384765625 
[2025-03-22 07:33:51 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 4 loss:0.5938135385513306 norm:0.004638437181711197 max memory_allocated 29275.69384765625 
[2025-03-22 07:34:39 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 5 loss:0.5914286971092224 norm:0.0035643381997942924 max memory_allocated 29275.69384765625 
[2025-03-22 07:35:27 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 6 loss:0.5893117189407349 norm:0.0029404014348983765 max memory_allocated 29275.69384765625 
[2025-03-22 07:36:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 7 loss:0.5877013206481934 norm:0.002591426018625498 max memory_allocated 29275.69384765625 
[2025-03-22 07:37:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 8 loss:0.5863155722618103 norm:0.0022989679127931595 max memory_allocated 29275.69384765625 
[2025-03-22 07:37:52 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 9 loss:0.5851569175720215 norm:0.0020550370682030916 max memory_allocated 29275.69384765625 
[2025-03-22 07:38:40 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 10 loss:0.5842462778091431 norm:0.0018715044716373086 max memory_allocated 29275.69384765625 
[2025-03-22 07:39:28 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 11 loss:0.5834577679634094 norm:0.0016978567000478506 max memory_allocated 29275.69384765625 
[2025-03-22 07:40:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 12 loss:0.5827711820602417 norm:0.001542667276225984 max memory_allocated 29275.69384765625 
[2025-03-22 07:41:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 13 loss:0.5821666121482849 norm:0.0014442619867622852 max memory_allocated 29275.69384765625 
[2025-03-22 07:41:52 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 14 loss:0.5816870331764221 norm:0.0013600178062915802 max memory_allocated 29275.69384765625 
[2025-03-22 07:42:40 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 15 loss:0.5812387466430664 norm:0.0012750020250678062 max memory_allocated 29275.69384765625 
[2025-03-22 07:43:28 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 16 loss:0.5808937549591064 norm:0.001222121762111783 max memory_allocated 29275.69384765625 
[2025-03-22 07:44:16 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 17 loss:0.5805842280387878 norm:0.0011827197158709168 max memory_allocated 29275.69384765625 
[2025-03-22 07:45:04 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 18 loss:0.5800774097442627 norm:0.000912567600607872 max memory_allocated 29275.69384765625 
[2025-03-22 07:45:52 root] (abq_llm_calibration_a.py 358): INFO layer 23 iter 19 loss:0.5797396898269653 norm:0.0008093822398222983 max memory_allocated 29275.69384765625 
[2025-03-22 07:46:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 24 ===
[2025-03-22 07:46:58 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 0 loss:0.6662743091583252 norm:0.009390563704073429 max memory_allocated 29275.88134765625 
[2025-03-22 07:47:46 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 1 loss:0.6586871147155762 norm:0.00688779354095459 max memory_allocated 29275.88134765625 
[2025-03-22 07:48:34 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 2 loss:0.6480684280395508 norm:0.00448860926553607 max memory_allocated 29275.88134765625 
[2025-03-22 07:49:22 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 3 loss:0.642507016658783 norm:0.0028054481372237206 max memory_allocated 29275.88134765625 
[2025-03-22 07:50:10 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 4 loss:0.6395624279975891 norm:0.002194714266806841 max memory_allocated 29275.88134765625 
[2025-03-22 07:50:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 5 loss:0.6376056671142578 norm:0.0020056923385709524 max memory_allocated 29275.88134765625 
[2025-03-22 07:51:47 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 6 loss:0.6358939409255981 norm:0.0017323041101917624 max memory_allocated 29275.88134765625 
[2025-03-22 07:52:35 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 7 loss:0.6344146728515625 norm:0.0015643676742911339 max memory_allocated 29275.88134765625 
[2025-03-22 07:53:23 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 8 loss:0.6331272125244141 norm:0.0014750639675185084 max memory_allocated 29275.88134765625 
[2025-03-22 07:54:11 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 9 loss:0.6320366859436035 norm:0.0013997775968164206 max memory_allocated 29275.88134765625 
[2025-03-22 07:54:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 10 loss:0.6312037706375122 norm:0.0013353659305721521 max memory_allocated 29275.88134765625 
[2025-03-22 07:55:48 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 11 loss:0.6304728984832764 norm:0.0012488075299188495 max memory_allocated 29275.88134765625 
[2025-03-22 07:56:36 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 12 loss:0.6298699378967285 norm:0.0011644393671303988 max memory_allocated 29275.88134765625 
[2025-03-22 07:57:23 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 13 loss:0.6294585466384888 norm:0.0011121537536382675 max memory_allocated 29275.88134765625 
[2025-03-22 07:58:11 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 14 loss:0.6290601491928101 norm:0.0010612470796331763 max memory_allocated 29275.88134765625 
[2025-03-22 07:58:59 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 15 loss:0.628689706325531 norm:0.0010253428481519222 max memory_allocated 29275.88134765625 
[2025-03-22 07:59:47 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 16 loss:0.6284239292144775 norm:0.000989662017673254 max memory_allocated 29275.88134765625 
[2025-03-22 08:00:35 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 17 loss:0.6281564235687256 norm:0.0009510318050161004 max memory_allocated 29275.88134765625 
[2025-03-22 08:01:23 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 18 loss:0.6279292702674866 norm:0.0009249976137652993 max memory_allocated 29275.88134765625 
[2025-03-22 08:02:11 root] (abq_llm_calibration_a.py 358): INFO layer 24 iter 19 loss:0.627700924873352 norm:0.0008956193923950195 max memory_allocated 29275.88134765625 
[2025-03-22 08:02:25 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 25 ===
[2025-03-22 08:03:18 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 0 loss:0.7204173803329468 norm:0.006790819577872753 max memory_allocated 29276.06884765625 
[2025-03-22 08:04:06 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 1 loss:0.7112578749656677 norm:0.004217352252453566 max memory_allocated 29276.06884765625 
[2025-03-22 08:04:54 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 2 loss:0.7006640434265137 norm:0.002534800674766302 max memory_allocated 29276.06884765625 
[2025-03-22 08:05:42 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 3 loss:0.6966824531555176 norm:0.0018336393404752016 max memory_allocated 29276.06884765625 
[2025-03-22 08:06:31 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 4 loss:0.6942816972732544 norm:0.001403101603500545 max memory_allocated 29276.06884765625 
[2025-03-22 08:07:19 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 5 loss:0.6926696300506592 norm:0.0012454723473638296 max memory_allocated 29276.06884765625 
[2025-03-22 08:08:07 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 6 loss:0.6912047266960144 norm:0.001131415250711143 max memory_allocated 29276.06884765625 
[2025-03-22 08:08:55 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 7 loss:0.6898505091667175 norm:0.0010281201684847474 max memory_allocated 29276.06884765625 
[2025-03-22 08:09:44 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 8 loss:0.6887337565422058 norm:0.0009786548325791955 max memory_allocated 29276.06884765625 
[2025-03-22 08:10:32 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 9 loss:0.6878474950790405 norm:0.0009319894597865641 max memory_allocated 29276.06884765625 
[2025-03-22 08:11:20 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 10 loss:0.6870753169059753 norm:0.0008873760234564543 max memory_allocated 29276.06884765625 
[2025-03-22 08:12:08 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 11 loss:0.6864787340164185 norm:0.0008521659183315933 max memory_allocated 29276.06884765625 
[2025-03-22 08:12:56 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 12 loss:0.6860178709030151 norm:0.000815899926237762 max memory_allocated 29276.06884765625 
[2025-03-22 08:13:44 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 13 loss:0.6855841875076294 norm:0.0007976249326020479 max memory_allocated 29276.06884765625 
[2025-03-22 08:14:32 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 14 loss:0.6852234601974487 norm:0.0007714974344708025 max memory_allocated 29276.06884765625 
[2025-03-22 08:15:20 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 15 loss:0.6848883628845215 norm:0.000762475305236876 max memory_allocated 29276.06884765625 
[2025-03-22 08:16:08 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 16 loss:0.6845917701721191 norm:0.0007376854191534221 max memory_allocated 29276.06884765625 
[2025-03-22 08:16:56 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 17 loss:0.6843256950378418 norm:0.0007284361636266112 max memory_allocated 29276.06884765625 
[2025-03-22 08:17:44 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 18 loss:0.6840795278549194 norm:0.0007190218893811107 max memory_allocated 29276.06884765625 
[2025-03-22 08:18:32 root] (abq_llm_calibration_a.py 358): INFO layer 25 iter 19 loss:0.6839054822921753 norm:0.0007072248263284564 max memory_allocated 29276.06884765625 
[2025-03-22 08:18:45 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 26 ===
[2025-03-22 08:19:37 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 0 loss:0.7940845489501953 norm:0.014123841188848019 max memory_allocated 29276.25634765625 
[2025-03-22 08:20:25 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 1 loss:0.7846499681472778 norm:0.009478656575083733 max memory_allocated 29276.25634765625 
[2025-03-22 08:21:14 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 2 loss:0.7723878622055054 norm:0.006694026291370392 max memory_allocated 29276.25634765625 
[2025-03-22 08:22:02 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 3 loss:0.7672246098518372 norm:0.004990320187062025 max memory_allocated 29276.25634765625 
[2025-03-22 08:22:50 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 4 loss:0.7645203471183777 norm:0.004085404332727194 max memory_allocated 29276.25634765625 
[2025-03-22 08:23:38 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 5 loss:0.762145459651947 norm:0.003369805635884404 max memory_allocated 29276.25634765625 
[2025-03-22 08:24:26 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 6 loss:0.7599942088127136 norm:0.0028613044414669275 max memory_allocated 29276.25634765625 
[2025-03-22 08:25:15 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 7 loss:0.7580430507659912 norm:0.002473452128469944 max memory_allocated 29276.25634765625 
[2025-03-22 08:26:03 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 8 loss:0.7562870383262634 norm:0.0020613749511539936 max memory_allocated 29276.25634765625 
[2025-03-22 08:26:51 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 9 loss:0.7544794082641602 norm:0.001470041461288929 max memory_allocated 29276.25634765625 
[2025-03-22 08:27:39 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 10 loss:0.7533982396125793 norm:0.0012759699020534754 max memory_allocated 29276.25634765625 
[2025-03-22 08:28:27 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 11 loss:0.7525790333747864 norm:0.0012514666887000203 max memory_allocated 29276.25634765625 
[2025-03-22 08:29:15 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 12 loss:0.7519587278366089 norm:0.001218911143951118 max memory_allocated 29276.25634765625 
[2025-03-22 08:30:03 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 13 loss:0.7514685392379761 norm:0.0012053558602929115 max memory_allocated 29276.25634765625 
[2025-03-22 08:30:51 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 14 loss:0.7511218786239624 norm:0.0011774582089856267 max memory_allocated 29276.25634765625 
[2025-03-22 08:31:39 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 15 loss:0.7508100271224976 norm:0.0011659449664875865 max memory_allocated 29276.25634765625 
[2025-03-22 08:32:27 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 16 loss:0.7505120038986206 norm:0.0011590954381972551 max memory_allocated 29276.25634765625 
[2025-03-22 08:33:15 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 17 loss:0.750277042388916 norm:0.0011381616350263357 max memory_allocated 29276.25634765625 
[2025-03-22 08:34:03 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 18 loss:0.7500140070915222 norm:0.0011245342902839184 max memory_allocated 29276.25634765625 
[2025-03-22 08:34:51 root] (abq_llm_calibration_a.py 358): INFO layer 26 iter 19 loss:0.7498598694801331 norm:0.0011258457088842988 max memory_allocated 29276.25634765625 
[2025-03-22 08:35:06 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 27 ===
[2025-03-22 08:36:00 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 0 loss:0.848678469657898 norm:0.005406276322901249 max memory_allocated 29276.44384765625 
[2025-03-22 08:36:48 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 1 loss:0.840311586856842 norm:0.0031942131463438272 max memory_allocated 29276.44384765625 
[2025-03-22 08:37:36 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 2 loss:0.830142080783844 norm:0.0019457582384347916 max memory_allocated 29276.44384765625 
[2025-03-22 08:38:24 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 3 loss:0.8260363936424255 norm:0.0013915742747485638 max memory_allocated 29276.44384765625 
[2025-03-22 08:39:12 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 4 loss:0.8239158391952515 norm:0.0011653786059468985 max memory_allocated 29276.44384765625 
[2025-03-22 08:40:01 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 5 loss:0.8220373392105103 norm:0.0010162382386624813 max memory_allocated 29276.44384765625 
[2025-03-22 08:40:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 6 loss:0.8201431035995483 norm:0.0008542438736185431 max memory_allocated 29276.44384765625 
[2025-03-22 08:41:37 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 7 loss:0.8185920119285583 norm:0.0008015044732019305 max memory_allocated 29276.44384765625 
[2025-03-22 08:42:25 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 8 loss:0.8172428607940674 norm:0.0007736744591966271 max memory_allocated 29276.44384765625 
[2025-03-22 08:43:13 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 9 loss:0.8162020444869995 norm:0.0007531789597123861 max memory_allocated 29276.44384765625 
[2025-03-22 08:44:01 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 10 loss:0.815362811088562 norm:0.000745261088013649 max memory_allocated 29276.44384765625 
[2025-03-22 08:44:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 11 loss:0.8146183490753174 norm:0.0007194273639470339 max memory_allocated 29276.44384765625 
[2025-03-22 08:45:37 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 12 loss:0.8141368627548218 norm:0.0007153864135034382 max memory_allocated 29276.44384765625 
[2025-03-22 08:46:25 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 13 loss:0.8136327266693115 norm:0.0007144917035475373 max memory_allocated 29276.44384765625 
[2025-03-22 08:47:13 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 14 loss:0.813223123550415 norm:0.0006969640380702913 max memory_allocated 29276.44384765625 
[2025-03-22 08:48:01 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 15 loss:0.8128312230110168 norm:0.0006974670686759055 max memory_allocated 29276.44384765625 
[2025-03-22 08:48:49 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 16 loss:0.8124704957008362 norm:0.0006890444783493876 max memory_allocated 29276.44384765625 
[2025-03-22 08:49:37 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 17 loss:0.8121686577796936 norm:0.0006835251115262508 max memory_allocated 29276.44384765625 
[2025-03-22 08:50:25 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 18 loss:0.8119269013404846 norm:0.0006776662776246667 max memory_allocated 29276.44384765625 
[2025-03-22 08:51:13 root] (abq_llm_calibration_a.py 358): INFO layer 27 iter 19 loss:0.8117099404335022 norm:0.000682855024933815 max memory_allocated 29276.44384765625 
[2025-03-22 08:51:27 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 28 ===
[2025-03-22 08:52:21 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 0 loss:0.9329367280006409 norm:0.013821156695485115 max memory_allocated 29276.63134765625 
[2025-03-22 08:53:09 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 1 loss:0.9220728278160095 norm:0.008701621554791927 max memory_allocated 29276.63134765625 
[2025-03-22 08:53:57 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 2 loss:0.9111008644104004 norm:0.005930548068135977 max memory_allocated 29276.63134765625 
[2025-03-22 08:54:45 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 3 loss:0.9066186547279358 norm:0.0043710991740226746 max memory_allocated 29276.63134765625 
[2025-03-22 08:55:33 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 4 loss:0.9033317565917969 norm:0.0032885028049349785 max memory_allocated 29276.63134765625 
[2025-03-22 08:56:22 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 5 loss:0.9008221626281738 norm:0.0023880000226199627 max memory_allocated 29276.63134765625 
[2025-03-22 08:57:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 6 loss:0.8979731798171997 norm:0.0018917997367680073 max memory_allocated 29276.63134765625 
[2025-03-22 08:57:58 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 7 loss:0.8960839509963989 norm:0.001770120463334024 max memory_allocated 29276.63134765625 
[2025-03-22 08:58:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 8 loss:0.8941701650619507 norm:0.0014857954811304808 max memory_allocated 29276.63134765625 
[2025-03-22 08:59:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 9 loss:0.8927996158599854 norm:0.0014304665382951498 max memory_allocated 29276.63134765625 
[2025-03-22 09:00:23 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 10 loss:0.8916753530502319 norm:0.0013307954650372267 max memory_allocated 29276.63134765625 
[2025-03-22 09:01:11 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 11 loss:0.8911350965499878 norm:0.0011796613689512014 max memory_allocated 29276.63134765625 
[2025-03-22 09:01:59 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 12 loss:0.8906789422035217 norm:0.0011012449394911528 max memory_allocated 29276.63134765625 
[2025-03-22 09:02:47 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 13 loss:0.8903041481971741 norm:0.0009300084784626961 max memory_allocated 29276.63134765625 
[2025-03-22 09:03:35 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 14 loss:0.8896490931510925 norm:0.000928558234591037 max memory_allocated 29276.63134765625 
[2025-03-22 09:04:22 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 15 loss:0.8892201781272888 norm:0.000927038723602891 max memory_allocated 29276.63134765625 
[2025-03-22 09:05:10 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 16 loss:0.8889257311820984 norm:0.0009020636789500713 max memory_allocated 29276.63134765625 
[2025-03-22 09:05:58 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 17 loss:0.8886412382125854 norm:0.0008998336270451546 max memory_allocated 29276.63134765625 
[2025-03-22 09:06:46 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 18 loss:0.8882970809936523 norm:0.0009222868829965591 max memory_allocated 29276.63134765625 
[2025-03-22 09:07:34 root] (abq_llm_calibration_a.py 358): INFO layer 28 iter 19 loss:0.8879650235176086 norm:0.0009615004528313875 max memory_allocated 29276.63134765625 
[2025-03-22 09:07:48 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 29 ===
[2025-03-22 09:08:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 0 loss:1.004642367362976 norm:0.008355557918548584 max memory_allocated 29276.81884765625 
[2025-03-22 09:09:28 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 1 loss:0.9960840940475464 norm:0.005014665424823761 max memory_allocated 29276.81884765625 
[2025-03-22 09:10:16 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 2 loss:0.9844731092453003 norm:0.0027730907313525677 max memory_allocated 29276.81884765625 
[2025-03-22 09:11:04 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 3 loss:0.979846715927124 norm:0.002006996190175414 max memory_allocated 29276.81884765625 
[2025-03-22 09:11:52 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 4 loss:0.9772531986236572 norm:0.0015622362261638045 max memory_allocated 29276.81884765625 
[2025-03-22 09:12:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 5 loss:0.9748854041099548 norm:0.0011605510953813791 max memory_allocated 29276.81884765625 
[2025-03-22 09:13:28 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 6 loss:0.972777247428894 norm:0.0010293690720573068 max memory_allocated 29276.81884765625 
[2025-03-22 09:14:16 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 7 loss:0.9710794687271118 norm:0.0009615388116799295 max memory_allocated 29276.81884765625 
[2025-03-22 09:15:04 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 8 loss:0.9696199297904968 norm:0.0009108700323849916 max memory_allocated 29276.81884765625 
[2025-03-22 09:15:52 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 9 loss:0.9685311913490295 norm:0.0008754857699386775 max memory_allocated 29276.81884765625 
[2025-03-22 09:16:40 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 10 loss:0.9676686525344849 norm:0.000841136381495744 max memory_allocated 29276.81884765625 
[2025-03-22 09:17:28 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 11 loss:0.9670490026473999 norm:0.0008178778225556016 max memory_allocated 29276.81884765625 
[2025-03-22 09:18:16 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 12 loss:0.9664658308029175 norm:0.0008052608463913202 max memory_allocated 29276.81884765625 
[2025-03-22 09:19:03 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 13 loss:0.9659830331802368 norm:0.0007795349229127169 max memory_allocated 29276.81884765625 
[2025-03-22 09:19:51 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 14 loss:0.9655779004096985 norm:0.0007630486506968737 max memory_allocated 29276.81884765625 
[2025-03-22 09:20:39 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 15 loss:0.9652873873710632 norm:0.0007536199991591275 max memory_allocated 29276.81884765625 
[2025-03-22 09:21:27 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 16 loss:0.9650123119354248 norm:0.0007334578549489379 max memory_allocated 29276.81884765625 
[2025-03-22 09:22:14 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 17 loss:0.9647508859634399 norm:0.0007292706868611276 max memory_allocated 29276.81884765625 
[2025-03-22 09:23:02 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 18 loss:0.964509129524231 norm:0.0007150619640015066 max memory_allocated 29276.81884765625 
[2025-03-22 09:23:50 root] (abq_llm_calibration_a.py 358): INFO layer 29 iter 19 loss:0.9643087387084961 norm:0.0007024329388514161 max memory_allocated 29276.81884765625 
[2025-03-22 09:24:04 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 30 ===
[2025-03-22 09:24:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 0 loss:1.0981180667877197 norm:0.012035028077661991 max memory_allocated 29277.00634765625 
[2025-03-22 09:25:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 1 loss:1.0865294933319092 norm:0.007247891742736101 max memory_allocated 29277.00634765625 
[2025-03-22 09:26:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 2 loss:1.0720634460449219 norm:0.004656495526432991 max memory_allocated 29277.00634765625 
[2025-03-22 09:27:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 3 loss:1.0654923915863037 norm:0.003072934690862894 max memory_allocated 29277.00634765625 
[2025-03-22 09:28:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 4 loss:1.0621706247329712 norm:0.0017954278737306595 max memory_allocated 29277.00634765625 
[2025-03-22 09:28:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 5 loss:1.0596808195114136 norm:0.0014242925681173801 max memory_allocated 29277.00634765625 
[2025-03-22 09:29:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 6 loss:1.0574162006378174 norm:0.0013103745877742767 max memory_allocated 29277.00634765625 
[2025-03-22 09:30:33 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 7 loss:1.0553007125854492 norm:0.0012061864836141467 max memory_allocated 29277.00634765625 
[2025-03-22 09:31:21 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 8 loss:1.053629994392395 norm:0.001133681507781148 max memory_allocated 29277.00634765625 
[2025-03-22 09:32:08 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 9 loss:1.0523459911346436 norm:0.001077162683941424 max memory_allocated 29277.00634765625 
[2025-03-22 09:32:56 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 10 loss:1.051382064819336 norm:0.0010269185295328498 max memory_allocated 29277.00634765625 
[2025-03-22 09:33:44 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 11 loss:1.0505213737487793 norm:0.0009494578116573393 max memory_allocated 29277.00634765625 
[2025-03-22 09:34:32 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 12 loss:1.0497941970825195 norm:0.0009194861631840467 max memory_allocated 29277.00634765625 
[2025-03-22 09:35:20 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 13 loss:1.0492470264434814 norm:0.0008884861599653959 max memory_allocated 29277.00634765625 
[2025-03-22 09:36:07 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 14 loss:1.0487340688705444 norm:0.0008386627305299044 max memory_allocated 29277.00634765625 
[2025-03-22 09:36:55 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 15 loss:1.0482994318008423 norm:0.0008730887202546 max memory_allocated 29277.00634765625 
[2025-03-22 09:37:43 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 16 loss:1.047899603843689 norm:0.0008157795527949929 max memory_allocated 29277.00634765625 
[2025-03-22 09:38:31 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 17 loss:1.0475345849990845 norm:0.0008117608958855271 max memory_allocated 29277.00634765625 
[2025-03-22 09:39:19 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 18 loss:1.0472917556762695 norm:0.0007943591917864978 max memory_allocated 29277.00634765625 
[2025-03-22 09:40:07 root] (abq_llm_calibration_a.py 358): INFO layer 30 iter 19 loss:1.0469690561294556 norm:0.0007768709911033511 max memory_allocated 29277.00634765625 
[2025-03-22 09:40:20 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 31 ===
[2025-03-22 09:41:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 0 loss:1.1855013370513916 norm:0.010755605064332485 max memory_allocated 29277.19384765625 
[2025-03-22 09:42:00 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 1 loss:1.1749354600906372 norm:0.006669213995337486 max memory_allocated 29277.19384765625 
[2025-03-22 09:42:48 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 2 loss:1.1605013608932495 norm:0.0034640273079276085 max memory_allocated 29277.19384765625 
[2025-03-22 09:43:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 3 loss:1.1533046960830688 norm:0.002255393657833338 max memory_allocated 29277.19384765625 
[2025-03-22 09:44:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 4 loss:1.1491540670394897 norm:0.0012712408788502216 max memory_allocated 29277.19384765625 
[2025-03-22 09:45:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 5 loss:1.1459881067276 norm:0.0010423732455819845 max memory_allocated 29277.19384765625 
[2025-03-22 09:46:00 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 6 loss:1.1431881189346313 norm:0.0009494870901107788 max memory_allocated 29277.19384765625 
[2025-03-22 09:46:48 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 7 loss:1.1407841444015503 norm:0.0008788069826550782 max memory_allocated 29277.19384765625 
[2025-03-22 09:47:36 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 8 loss:1.1389801502227783 norm:0.0008494515786878765 max memory_allocated 29277.19384765625 
[2025-03-22 09:48:24 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 9 loss:1.137657642364502 norm:0.0008179197320714593 max memory_allocated 29277.19384765625 
[2025-03-22 09:49:12 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 10 loss:1.1366971731185913 norm:0.0008014023769646883 max memory_allocated 29277.19384765625 
[2025-03-22 09:50:00 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 11 loss:1.135921597480774 norm:0.0007712740916758776 max memory_allocated 29277.19384765625 
[2025-03-22 09:50:47 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 12 loss:1.135263204574585 norm:0.0007535551558248699 max memory_allocated 29277.19384765625 
[2025-03-22 09:51:35 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 13 loss:1.1347625255584717 norm:0.0007439286564476788 max memory_allocated 29277.19384765625 
[2025-03-22 09:52:23 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 14 loss:1.1342849731445312 norm:0.0007313602836802602 max memory_allocated 29277.19384765625 
[2025-03-22 09:53:11 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 15 loss:1.1339263916015625 norm:0.000716935726813972 max memory_allocated 29277.19384765625 
[2025-03-22 09:53:58 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 16 loss:1.1335973739624023 norm:0.0007104446995072067 max memory_allocated 29277.19384765625 
[2025-03-22 09:54:46 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 17 loss:1.133367657661438 norm:0.0007054514135234058 max memory_allocated 29277.19384765625 
[2025-03-22 09:55:34 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 18 loss:1.13311767578125 norm:0.0006950967363081872 max memory_allocated 29277.19384765625 
[2025-03-22 09:56:22 root] (abq_llm_calibration_a.py 358): INFO layer 31 iter 19 loss:1.132940649986267 norm:0.0006951740360818803 max memory_allocated 29277.19384765625 
[2025-03-22 09:56:36 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 32 ===
[2025-03-22 09:57:28 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 0 loss:1.2907862663269043 norm:0.016329586505889893 max memory_allocated 29277.38134765625 
[2025-03-22 09:58:16 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 1 loss:1.276666283607483 norm:0.009783197194337845 max memory_allocated 29277.38134765625 
[2025-03-22 09:59:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 2 loss:1.2598159313201904 norm:0.006237020716071129 max memory_allocated 29277.38134765625 
[2025-03-22 09:59:52 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 3 loss:1.25188148021698 norm:0.004296591505408287 max memory_allocated 29277.38134765625 
[2025-03-22 10:00:40 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 4 loss:1.246974229812622 norm:0.002970802830532193 max memory_allocated 29277.38134765625 
[2025-03-22 10:01:28 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 5 loss:1.2430264949798584 norm:0.00218851026147604 max memory_allocated 29277.38134765625 
[2025-03-22 10:02:16 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 6 loss:1.2395291328430176 norm:0.0011356973554939032 max memory_allocated 29277.38134765625 
[2025-03-22 10:03:04 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 7 loss:1.2369914054870605 norm:0.0011018022196367383 max memory_allocated 29277.38134765625 
[2025-03-22 10:03:52 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 8 loss:1.2351830005645752 norm:0.0010667378082871437 max memory_allocated 29277.38134765625 
[2025-03-22 10:04:39 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 9 loss:1.2339248657226562 norm:0.0010380330495536327 max memory_allocated 29277.38134765625 
[2025-03-22 10:05:27 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 10 loss:1.2329356670379639 norm:0.001009145169518888 max memory_allocated 29277.38134765625 
[2025-03-22 10:06:15 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 11 loss:1.2321873903274536 norm:0.0009720848174765706 max memory_allocated 29277.38134765625 
[2025-03-22 10:07:03 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 12 loss:1.231542944908142 norm:0.0009442554437555373 max memory_allocated 29277.38134765625 
[2025-03-22 10:07:50 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 13 loss:1.2310686111450195 norm:0.0009185943054035306 max memory_allocated 29277.38134765625 
[2025-03-22 10:08:38 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 14 loss:1.2305941581726074 norm:0.0009039139258675277 max memory_allocated 29277.38134765625 
[2025-03-22 10:09:26 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 15 loss:1.2302099466323853 norm:0.0008805488469079137 max memory_allocated 29277.38134765625 
[2025-03-22 10:10:14 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 16 loss:1.2299059629440308 norm:0.0008770329295657575 max memory_allocated 29277.38134765625 
[2025-03-22 10:11:02 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 17 loss:1.2296102046966553 norm:0.0008494469220750034 max memory_allocated 29277.38134765625 
[2025-03-22 10:11:50 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 18 loss:1.2293789386749268 norm:0.0008285314543172717 max memory_allocated 29277.38134765625 
[2025-03-22 10:12:38 root] (abq_llm_calibration_a.py 358): INFO layer 32 iter 19 loss:1.2290438413619995 norm:0.0008218857692554593 max memory_allocated 29277.38134765625 
[2025-03-22 10:12:51 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 33 ===
[2025-03-22 10:13:43 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 0 loss:1.3976945877075195 norm:0.012791084125638008 max memory_allocated 29277.56884765625 
[2025-03-22 10:14:31 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 1 loss:1.3841776847839355 norm:0.00914638303220272 max memory_allocated 29277.56884765625 
[2025-03-22 10:15:19 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 2 loss:1.3667043447494507 norm:0.006658375728875399 max memory_allocated 29277.56884765625 
[2025-03-22 10:16:07 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 3 loss:1.3585113286972046 norm:0.00507615739479661 max memory_allocated 29277.56884765625 
[2025-03-22 10:16:55 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 4 loss:1.3532445430755615 norm:0.0040557789616286755 max memory_allocated 29277.56884765625 
[2025-03-22 10:17:43 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 5 loss:1.3486058712005615 norm:0.003053483786061406 max memory_allocated 29277.56884765625 
[2025-03-22 10:18:31 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 6 loss:1.3446669578552246 norm:0.00236878776922822 max memory_allocated 29277.56884765625 
[2025-03-22 10:19:19 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 7 loss:1.3417264223098755 norm:0.0020783047657459974 max memory_allocated 29277.56884765625 
[2025-03-22 10:20:07 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 8 loss:1.3395493030548096 norm:0.0018851214554160833 max memory_allocated 29277.56884765625 
[2025-03-22 10:20:55 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 9 loss:1.337700366973877 norm:0.0016945241950452328 max memory_allocated 29277.56884765625 
[2025-03-22 10:21:42 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 10 loss:1.3361704349517822 norm:0.0015516870189458132 max memory_allocated 29277.56884765625 
[2025-03-22 10:22:30 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 11 loss:1.334815502166748 norm:0.0013544006505981088 max memory_allocated 29277.56884765625 
[2025-03-22 10:23:18 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 12 loss:1.33394455909729 norm:0.001241884776391089 max memory_allocated 29277.56884765625 
[2025-03-22 10:24:06 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 13 loss:1.3333090543746948 norm:0.0011341888457536697 max memory_allocated 29277.56884765625 
[2025-03-22 10:24:54 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 14 loss:1.3329931497573853 norm:0.0011019249213859439 max memory_allocated 29277.56884765625 
[2025-03-22 10:25:41 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 15 loss:1.3323297500610352 norm:0.0010547481942921877 max memory_allocated 29277.56884765625 
[2025-03-22 10:26:29 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 16 loss:1.3317996263504028 norm:0.0009737287764437497 max memory_allocated 29277.56884765625 
[2025-03-22 10:27:17 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 17 loss:1.3317731618881226 norm:0.0009606222156435251 max memory_allocated 29277.56884765625 
[2025-03-22 10:28:06 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 18 loss:1.3312194347381592 norm:0.000957665266469121 max memory_allocated 29277.56884765625 
[2025-03-22 10:28:54 root] (abq_llm_calibration_a.py 358): INFO layer 33 iter 19 loss:1.330816388130188 norm:0.0008695147698745131 max memory_allocated 29277.56884765625 
[2025-03-22 10:29:07 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 34 ===
[2025-03-22 10:29:59 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 0 loss:1.5234873294830322 norm:0.015079665929079056 max memory_allocated 29277.75634765625 
[2025-03-22 10:30:47 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 1 loss:1.5072256326675415 norm:0.0092230886220932 max memory_allocated 29277.75634765625 
[2025-03-22 10:31:35 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 2 loss:1.4879246950149536 norm:0.0060317739844322205 max memory_allocated 29277.75634765625 
[2025-03-22 10:32:23 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 3 loss:1.4789249897003174 norm:0.004352734424173832 max memory_allocated 29277.75634765625 
[2025-03-22 10:33:11 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 4 loss:1.4735016822814941 norm:0.003566279076039791 max memory_allocated 29277.75634765625 
[2025-03-22 10:33:59 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 5 loss:1.4689538478851318 norm:0.0028454160783439875 max memory_allocated 29277.75634765625 
[2025-03-22 10:34:47 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 6 loss:1.4647430181503296 norm:0.0022484282962977886 max memory_allocated 29277.75634765625 
[2025-03-22 10:35:35 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 7 loss:1.4614969491958618 norm:0.001937698689289391 max memory_allocated 29277.75634765625 
[2025-03-22 10:36:23 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 8 loss:1.4592409133911133 norm:0.0017686192877590656 max memory_allocated 29277.75634765625 
[2025-03-22 10:37:10 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 9 loss:1.4576297998428345 norm:0.0015952449757605791 max memory_allocated 29277.75634765625 
[2025-03-22 10:37:58 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 10 loss:1.4563839435577393 norm:0.0014424989931285381 max memory_allocated 29277.75634765625 
[2025-03-22 10:38:46 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 11 loss:1.4552171230316162 norm:0.0013257882092148066 max memory_allocated 29277.75634765625 
[2025-03-22 10:39:34 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 12 loss:1.4542862176895142 norm:0.0012402296997606754 max memory_allocated 29277.75634765625 
[2025-03-22 10:40:22 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 13 loss:1.4536120891571045 norm:0.0011633428512141109 max memory_allocated 29277.75634765625 
[2025-03-22 10:41:09 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 14 loss:1.4529893398284912 norm:0.001102452166378498 max memory_allocated 29277.75634765625 
[2025-03-22 10:41:57 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 15 loss:1.452459692955017 norm:0.0010519148781895638 max memory_allocated 29277.75634765625 
[2025-03-22 10:42:45 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 16 loss:1.451975703239441 norm:0.0009961152682080865 max memory_allocated 29277.75634765625 
[2025-03-22 10:43:33 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 17 loss:1.4516457319259644 norm:0.0008674756973050535 max memory_allocated 29277.75634765625 
[2025-03-22 10:44:22 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 18 loss:1.4511510133743286 norm:0.0008272524573840201 max memory_allocated 29277.75634765625 
[2025-03-22 10:45:10 root] (abq_llm_calibration_a.py 358): INFO layer 34 iter 19 loss:1.4508159160614014 norm:0.0008153961971402168 max memory_allocated 29277.75634765625 
[2025-03-22 10:45:23 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 35 ===
[2025-03-22 10:46:15 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 0 loss:1.6620476245880127 norm:0.008690913207828999 max memory_allocated 29277.94384765625 
[2025-03-22 10:47:03 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 1 loss:1.6445039510726929 norm:0.0060950396582484245 max memory_allocated 29277.94384765625 
[2025-03-22 10:47:51 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 2 loss:1.6229280233383179 norm:0.004185207653790712 max memory_allocated 29277.94384765625 
[2025-03-22 10:48:39 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 3 loss:1.6109347343444824 norm:0.0029250294901430607 max memory_allocated 29277.94384765625 
[2025-03-22 10:49:27 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 4 loss:1.6043448448181152 norm:0.0023766837548464537 max memory_allocated 29277.94384765625 
[2025-03-22 10:50:15 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 5 loss:1.5994601249694824 norm:0.002003880450502038 max memory_allocated 29277.94384765625 
[2025-03-22 10:51:02 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 6 loss:1.5950597524642944 norm:0.0017287537921220064 max memory_allocated 29277.94384765625 
[2025-03-22 10:51:50 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 7 loss:1.5916321277618408 norm:0.0015727248974144459 max memory_allocated 29277.94384765625 
[2025-03-22 10:52:38 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 8 loss:1.58907151222229 norm:0.0014257485745474696 max memory_allocated 29277.94384765625 
[2025-03-22 10:53:26 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 9 loss:1.5872201919555664 norm:0.001362791401334107 max memory_allocated 29277.94384765625 
[2025-03-22 10:54:13 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 10 loss:1.585895299911499 norm:0.0013081964571028948 max memory_allocated 29277.94384765625 
[2025-03-22 10:55:01 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 11 loss:1.5846656560897827 norm:0.0012500903103500605 max memory_allocated 29277.94384765625 
[2025-03-22 10:55:49 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 12 loss:1.5836906433105469 norm:0.0011977992253378034 max memory_allocated 29277.94384765625 
[2025-03-22 10:56:37 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 13 loss:1.5827845335006714 norm:0.0011368453269824386 max memory_allocated 29277.94384765625 
[2025-03-22 10:57:25 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 14 loss:1.5819487571716309 norm:0.0010666983434930444 max memory_allocated 29277.94384765625 
[2025-03-22 10:58:13 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 15 loss:1.5812286138534546 norm:0.0010321784066036344 max memory_allocated 29277.94384765625 
[2025-03-22 10:59:01 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 16 loss:1.5806667804718018 norm:0.0010013015707954764 max memory_allocated 29277.94384765625 
[2025-03-22 10:59:49 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 17 loss:1.580283761024475 norm:0.0009807030437514186 max memory_allocated 29277.94384765625 
[2025-03-22 11:00:37 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 18 loss:1.5798356533050537 norm:0.0009532218100503087 max memory_allocated 29277.94384765625 
[2025-03-22 11:01:25 root] (abq_llm_calibration_a.py 358): INFO layer 35 iter 19 loss:1.5793877840042114 norm:0.0009354877984151244 max memory_allocated 29277.94384765625 
[2025-03-22 11:01:39 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 36 ===
[2025-03-22 11:01:43 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:02:31 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 0 loss:1.8614718914031982 norm:0.058399032801389694 max memory_allocated 29279.28759765625 
[2025-03-22 11:03:19 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 1 loss:1.8356657028198242 norm:0.04584217816591263 max memory_allocated 29279.28759765625 
[2025-03-22 11:04:08 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 2 loss:1.8058953285217285 norm:0.03405731916427612 max memory_allocated 29279.28759765625 
[2025-03-22 11:04:56 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 3 loss:1.7890183925628662 norm:0.026585277169942856 max memory_allocated 29279.28759765625 
[2025-03-22 11:05:44 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 4 loss:1.7729345560073853 norm:0.01969928666949272 max memory_allocated 29279.28759765625 
[2025-03-22 11:06:32 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 5 loss:1.769076943397522 norm:0.023653021082282066 max memory_allocated 29279.28759765625 
[2025-03-22 11:07:21 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 6 loss:1.772842526435852 norm:0.01840333640575409 max memory_allocated 29279.28759765625 
[2025-03-22 11:08:09 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 7 loss:1.7683063745498657 norm:0.019310597330331802 max memory_allocated 29279.28759765625 
[2025-03-22 11:08:57 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 8 loss:1.7581915855407715 norm:0.017681436613202095 max memory_allocated 29279.28759765625 
[2025-03-22 11:09:45 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 9 loss:1.7563750743865967 norm:0.016090314835309982 max memory_allocated 29279.28759765625 
[2025-03-22 11:10:33 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 10 loss:1.7688815593719482 norm:0.02307160757482052 max memory_allocated 29279.28759765625 
[2025-03-22 11:11:21 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 11 loss:1.7701412439346313 norm:0.023472778499126434 max memory_allocated 29279.28759765625 
[2025-03-22 11:12:09 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 12 loss:1.7815990447998047 norm:0.02529795654118061 max memory_allocated 29279.28759765625 
[2025-03-22 11:12:57 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 13 loss:1.764992356300354 norm:0.02013644576072693 max memory_allocated 29279.28759765625 
[2025-03-22 11:13:45 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 14 loss:1.745621681213379 norm:0.008407454937696457 max memory_allocated 29279.28759765625 
[2025-03-22 11:14:33 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 15 loss:1.7429702281951904 norm:0.010219923220574856 max memory_allocated 29279.28759765625 
[2025-03-22 11:15:21 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 16 loss:1.7414278984069824 norm:0.008130406960844994 max memory_allocated 29279.28759765625 
[2025-03-22 11:16:09 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 17 loss:1.7392375469207764 norm:0.008915151469409466 max memory_allocated 29279.28759765625 
[2025-03-22 11:16:57 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 18 loss:1.737681269645691 norm:0.008633953519165516 max memory_allocated 29279.28759765625 
[2025-03-22 11:17:46 root] (abq_llm_calibration_a.py 358): INFO layer 36 iter 19 loss:1.736608624458313 norm:0.008524799719452858 max memory_allocated 29279.28759765625 
[2025-03-22 11:17:59 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 37 ===
[2025-03-22 11:18:03 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:18:51 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 0 loss:2.067960023880005 norm:0.07881742715835571 max memory_allocated 29279.47509765625 
[2025-03-22 11:19:39 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 1 loss:2.0374486446380615 norm:0.06286648660898209 max memory_allocated 29279.47509765625 
[2025-03-22 11:20:28 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 2 loss:2.002570390701294 norm:0.05088608339428902 max memory_allocated 29279.47509765625 
[2025-03-22 11:21:16 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 3 loss:1.9749001264572144 norm:0.038573965430259705 max memory_allocated 29279.47509765625 
[2025-03-22 11:22:04 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 4 loss:1.9628597497940063 norm:0.03327980637550354 max memory_allocated 29279.47509765625 
[2025-03-22 11:22:52 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 5 loss:1.953086495399475 norm:0.027741363272070885 max memory_allocated 29279.47509765625 
[2025-03-22 11:23:41 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 6 loss:1.9480211734771729 norm:0.026285726577043533 max memory_allocated 29279.47509765625 
[2025-03-22 11:24:29 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 7 loss:1.9498779773712158 norm:0.024985983967781067 max memory_allocated 29279.47509765625 
[2025-03-22 11:25:17 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 8 loss:1.9392180442810059 norm:0.019405707716941833 max memory_allocated 29279.47509765625 
[2025-03-22 11:26:05 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 9 loss:1.934692621231079 norm:0.018811773508787155 max memory_allocated 29279.47509765625 
[2025-03-22 11:26:53 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 10 loss:1.937199354171753 norm:0.019892092794179916 max memory_allocated 29279.47509765625 
[2025-03-22 11:27:41 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 11 loss:1.944085717201233 norm:0.017149675637483597 max memory_allocated 29279.47509765625 
[2025-03-22 11:28:29 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 12 loss:1.9293203353881836 norm:0.016978111118078232 max memory_allocated 29279.47509765625 
[2025-03-22 11:29:17 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 13 loss:1.9279539585113525 norm:0.014526096172630787 max memory_allocated 29279.47509765625 
[2025-03-22 11:30:05 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 14 loss:1.922520637512207 norm:0.012925565242767334 max memory_allocated 29279.47509765625 
[2025-03-22 11:30:53 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 15 loss:1.9206509590148926 norm:0.012776601128280163 max memory_allocated 29279.47509765625 
[2025-03-22 11:31:41 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 16 loss:1.9190820455551147 norm:0.01193988136947155 max memory_allocated 29279.47509765625 
[2025-03-22 11:32:29 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 17 loss:1.9179306030273438 norm:0.011093041859567165 max memory_allocated 29279.47509765625 
[2025-03-22 11:33:17 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 18 loss:1.9191057682037354 norm:0.014228994026780128 max memory_allocated 29279.47509765625 
[2025-03-22 11:34:06 root] (abq_llm_calibration_a.py 358): INFO layer 37 iter 19 loss:1.9188076257705688 norm:0.012289172038435936 max memory_allocated 29279.47509765625 
[2025-03-22 11:34:19 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 38 ===
[2025-03-22 11:34:23 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:35:11 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 0 loss:2.432377576828003 norm:0.12365896254777908 max memory_allocated 29279.66259765625 
[2025-03-22 11:35:59 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 1 loss:2.3833298683166504 norm:0.09148254990577698 max memory_allocated 29279.66259765625 
[2025-03-22 11:36:47 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 2 loss:2.3421778678894043 norm:0.08543898165225983 max memory_allocated 29279.66259765625 
[2025-03-22 11:37:36 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 3 loss:2.326390266418457 norm:0.0630575641989708 max memory_allocated 29279.66259765625 
[2025-03-22 11:38:24 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 4 loss:2.28387451171875 norm:0.06023507937788963 max memory_allocated 29279.66259765625 
[2025-03-22 11:39:12 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 5 loss:2.2785544395446777 norm:0.04634929075837135 max memory_allocated 29279.66259765625 
[2025-03-22 11:40:00 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 6 loss:2.2634449005126953 norm:0.043448302894830704 max memory_allocated 29279.66259765625 
[2025-03-22 11:40:49 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 7 loss:2.263498544692993 norm:0.04078067094087601 max memory_allocated 29279.66259765625 
[2025-03-22 11:41:37 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 8 loss:2.2513577938079834 norm:0.0415424183011055 max memory_allocated 29279.66259765625 
[2025-03-22 11:42:25 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 9 loss:2.3174498081207275 norm:0.05275142565369606 max memory_allocated 29279.66259765625 
[2025-03-22 11:43:14 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 10 loss:2.2490856647491455 norm:0.03953123837709427 max memory_allocated 29279.66259765625 
[2025-03-22 11:44:02 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 11 loss:2.2421460151672363 norm:0.037209652364254 max memory_allocated 29279.66259765625 
[2025-03-22 11:44:50 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 12 loss:2.236630439758301 norm:0.024286750704050064 max memory_allocated 29279.66259765625 
[2025-03-22 11:45:38 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 13 loss:2.2261574268341064 norm:0.025904132053256035 max memory_allocated 29279.66259765625 
[2025-03-22 11:46:26 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 14 loss:2.2220704555511475 norm:0.025888318195939064 max memory_allocated 29279.66259765625 
[2025-03-22 11:47:14 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 15 loss:2.2193615436553955 norm:0.025115855038166046 max memory_allocated 29279.66259765625 
[2025-03-22 11:48:03 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 16 loss:2.2166402339935303 norm:0.02450541779398918 max memory_allocated 29279.66259765625 
[2025-03-22 11:48:51 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 17 loss:2.214402437210083 norm:0.023766644299030304 max memory_allocated 29279.66259765625 
[2025-03-22 11:49:39 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 18 loss:2.212345600128174 norm:0.023016106337308884 max memory_allocated 29279.66259765625 
[2025-03-22 11:50:27 root] (abq_llm_calibration_a.py 358): INFO layer 38 iter 19 loss:2.2103490829467773 norm:0.022457432001829147 max memory_allocated 29279.66259765625 
[2025-03-22 11:50:40 root] (abq_llm_calibration_a.py 212): INFO === Start quantize layer 39 ===
[2025-03-22 11:50:44 root] (abq_llm_calibration_a.py 276): INFO use compensation vector
[2025-03-22 11:51:32 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 0 loss:3.4743266105651855 norm:0.355571448802948 max memory_allocated 29279.85009765625 
[2025-03-22 11:52:19 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 1 loss:3.2800943851470947 norm:0.2707982659339905 max memory_allocated 29279.85009765625 
[2025-03-22 11:53:07 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 2 loss:3.1735212802886963 norm:0.2106829434633255 max memory_allocated 29279.85009765625 
[2025-03-22 11:53:56 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 3 loss:3.0646896362304688 norm:0.16774362325668335 max memory_allocated 29279.85009765625 
[2025-03-22 11:54:44 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 4 loss:2.995640516281128 norm:0.12905505299568176 max memory_allocated 29279.85009765625 
[2025-03-22 11:55:32 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 5 loss:2.9583325386047363 norm:0.1045113354921341 max memory_allocated 29279.85009765625 
[2025-03-22 11:56:20 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 6 loss:2.9340972900390625 norm:0.09120532870292664 max memory_allocated 29279.85009765625 
[2025-03-22 11:57:08 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 7 loss:2.919217824935913 norm:0.07610216736793518 max memory_allocated 29279.85009765625 
[2025-03-22 11:57:56 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 8 loss:2.9083516597747803 norm:0.08031626790761948 max memory_allocated 29279.85009765625 
[2025-03-22 11:58:45 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 9 loss:2.9051642417907715 norm:0.06230119615793228 max memory_allocated 29279.85009765625 
[2025-03-22 11:59:33 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 10 loss:2.8763322830200195 norm:0.055283088237047195 max memory_allocated 29279.85009765625 
[2025-03-22 12:00:21 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 11 loss:2.8627407550811768 norm:0.05208275467157364 max memory_allocated 29279.85009765625 
[2025-03-22 12:01:10 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 12 loss:2.8529934883117676 norm:0.048827171325683594 max memory_allocated 29279.85009765625 
[2025-03-22 12:01:58 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 13 loss:2.850536823272705 norm:0.050202757120132446 max memory_allocated 29279.85009765625 
[2025-03-22 12:02:46 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 14 loss:2.8836240768432617 norm:0.061112575232982635 max memory_allocated 29279.85009765625 
[2025-03-22 12:03:34 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 15 loss:2.880824327468872 norm:0.06657060235738754 max memory_allocated 29279.85009765625 
[2025-03-22 12:04:22 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 16 loss:2.884425640106201 norm:0.05278058722615242 max memory_allocated 29279.85009765625 
[2025-03-22 12:05:10 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 17 loss:2.875208854675293 norm:0.0515744611620903 max memory_allocated 29279.85009765625 
[2025-03-22 12:05:58 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 18 loss:2.9648191928863525 norm:0.09750691801309586 max memory_allocated 29279.85009765625 
[2025-03-22 12:06:46 root] (abq_llm_calibration_a.py 358): INFO layer 39 iter 19 loss:2.9218225479125977 norm:0.08217093348503113 max memory_allocated 29279.85009765625 
[2025-03-22 12:07:00 root] (main_calibration_a.py 369): INFO 39122.86441159248
[2025-03-22 12:07:08 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-22 12:08:59 root] (main_calibration_a.py 158): INFO wikitext2 : 7.850029468536377
[2025-03-22 12:08:59 root] (main_calibration_a.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-22 12:11:50 root] (main_calibration_a.py 158): INFO c4 : 10.681294441223145
[2025-03-22 14:20:04 root] (main_calibration_a.py 169): INFO {'wikitext2': 7.850029468536377, 'c4': 10.681294441223145, 'results': {'winogrande': {'acc': 0.5737963693764798, 'acc_stderr': 0.013898585965412338}, 'boolq': {'acc': 0.6403669724770642, 'acc_stderr': 0.00839337808439905}, 'arc_challenge': {'acc': 0.3242320819112628, 'acc_stderr': 0.013678810399518829, 'acc_norm': 0.34897610921501704, 'acc_norm_stderr': 0.013928933461382504}, 'piqa': {'acc': 0.7067464635473341, 'acc_stderr': 0.01062181842110193, 'acc_norm': 0.6980413492927094, 'acc_norm_stderr': 0.010711732891588338}, 'arc_easy': {'acc': 0.6115319865319865, 'acc_stderr': 0.010001276044485224, 'acc_norm': 0.48737373737373735, 'acc_norm_stderr': 0.010256511718330584}, 'hellaswag': {'acc': 0.4970125473013344, 'acc_stderr': 0.0049896923443139935, 'acc_norm': 0.6471818362875921, 'acc_norm_stderr': 0.004768701562988878}}, 'versions': {'winogrande': 0, 'boolq': 1, 'arc_challenge': 0, 'piqa': 0, 'arc_easy': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-22 14:20:04 root] (main_calibration_a.py 172): INFO 32.42,61.15,64.04,49.70,70.67,57.38
