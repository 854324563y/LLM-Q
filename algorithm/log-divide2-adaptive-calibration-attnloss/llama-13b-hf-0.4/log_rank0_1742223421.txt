[2025-03-17 14:57:01 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-divide2-adaptive-calibration-attnloss/llama-13b-hf-0.4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide2-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.4.pkl', blocks_pkl='./log-divide2/llama-13b-hf-w4a4/llama-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-17 14:57:33 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-17 14:57:33 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-17 14:57:33 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-17 14:57:33 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide2-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.4.pkl
[2025-03-17 14:57:33 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide2/llama-13b-hf-w4a4/llama-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 8), (8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30), (30, 32), (32, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-17 14:57:33 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3, 4], [5], [6], [7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21], [22, 23], [24, 25], [26, 27], [28, 29], [30, 31], [32, 33], [34], [35], [36], [37], [38], [39]]
[2025-03-17 14:57:35 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-17 14:57:35 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 14:58:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.01228626724332571 norm:0.012274431996047497 max memory_allocated 44355.7939453125 
[2025-03-17 14:59:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.006841652560979128 norm:0.0064192526042461395 max memory_allocated 44355.7939453125 
[2025-03-17 14:59:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.004316836595535278 norm:0.004162510856986046 max memory_allocated 44355.7939453125 
[2025-03-17 15:00:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0035236093681305647 norm:0.0030657516326755285 max memory_allocated 44355.7939453125 
[2025-03-17 15:01:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0032379785552620888 norm:0.0025502261705696583 max memory_allocated 44355.7939453125 
[2025-03-17 15:01:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0030680429190397263 norm:0.0021557470317929983 max memory_allocated 44355.7939453125 
[2025-03-17 15:02:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.0029477751813828945 norm:0.0018402517307549715 max memory_allocated 44355.7939453125 
[2025-03-17 15:03:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.0028111692517995834 norm:0.001569821615703404 max memory_allocated 44355.7939453125 
[2025-03-17 15:03:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.0027361249085515738 norm:0.0014569833874702454 max memory_allocated 44355.7939453125 
[2025-03-17 15:04:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0026721390895545483 norm:0.0013115069596096873 max memory_allocated 44355.7939453125 
[2025-03-17 15:05:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.00264039752073586 norm:0.0011995892273262143 max memory_allocated 44355.7939453125 
[2025-03-17 15:05:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.002596708480268717 norm:0.0010808455990627408 max memory_allocated 44355.7939453125 
[2025-03-17 15:06:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002599494531750679 norm:0.0009940129239112139 max memory_allocated 44355.7939453125 
[2025-03-17 15:07:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0025390111841261387 norm:0.0008669111994095147 max memory_allocated 44355.7939453125 
[2025-03-17 15:07:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0025455616414546967 norm:0.0008150936919264495 max memory_allocated 44355.7939453125 
[2025-03-17 15:08:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002524977782741189 norm:0.0007868128595873713 max memory_allocated 44355.7939453125 
[2025-03-17 15:09:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.0025062558706849813 norm:0.0007579269586130977 max memory_allocated 44355.7939453125 
[2025-03-17 15:09:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.0024940413422882557 norm:0.000730000261683017 max memory_allocated 44355.7939453125 
[2025-03-17 15:10:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.002502705901861191 norm:0.0007105389377102256 max memory_allocated 44355.7939453125 
[2025-03-17 15:11:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0024953430984169245 norm:0.0006625511450693011 max memory_allocated 44355.7939453125 
[2025-03-17 15:12:05 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-17 15:12:05 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-17 15:12:05 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 15:12:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.022139625623822212 norm:0.01589668169617653 max memory_allocated 44355.7939453125 
[2025-03-17 15:13:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.012355457060039043 norm:0.009958520531654358 max memory_allocated 44355.7939453125 
[2025-03-17 15:14:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.008448625914752483 norm:0.006660887971520424 max memory_allocated 44355.7939453125 
[2025-03-17 15:14:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.007371725980192423 norm:0.004657278768718243 max memory_allocated 44355.7939453125 
[2025-03-17 15:15:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.006936215795576572 norm:0.0038929434958845377 max memory_allocated 44355.7939453125 
[2025-03-17 15:16:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.0066392309963703156 norm:0.003513803705573082 max memory_allocated 44355.7939453125 
[2025-03-17 15:16:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.00641984585672617 norm:0.0031911986880004406 max memory_allocated 44355.7939453125 
[2025-03-17 15:17:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.006244294345378876 norm:0.00293587613850832 max memory_allocated 44355.7939453125 
[2025-03-17 15:18:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.0060921162366867065 norm:0.002696189796552062 max memory_allocated 44355.7939453125 
[2025-03-17 15:18:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.005972902290523052 norm:0.0024550470989197493 max memory_allocated 44355.7939453125 
[2025-03-17 15:19:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.005840667523443699 norm:0.0022187060676515102 max memory_allocated 44355.7939453125 
[2025-03-17 15:20:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.0057340641506016254 norm:0.002026097383350134 max memory_allocated 44355.7939453125 
[2025-03-17 15:20:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0056426445953547955 norm:0.0018473027739673853 max memory_allocated 44355.7939453125 
[2025-03-17 15:21:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.00555648235604167 norm:0.001682480564340949 max memory_allocated 44355.7939453125 
[2025-03-17 15:22:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.005496652331203222 norm:0.001539930934086442 max memory_allocated 44355.7939453125 
[2025-03-17 15:22:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.005465368274599314 norm:0.0013971845619380474 max memory_allocated 44355.7939453125 
[2025-03-17 15:23:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.005402429960668087 norm:0.0012443345040082932 max memory_allocated 44355.7939453125 
[2025-03-17 15:24:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.005370797589421272 norm:0.0011143068550154567 max memory_allocated 44355.7939453125 
[2025-03-17 15:25:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.005346595775336027 norm:0.001024623867124319 max memory_allocated 44355.7939453125 
[2025-03-17 15:25:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.005326984450221062 norm:0.0010169313754886389 max memory_allocated 44355.7939453125 
[2025-03-17 15:26:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-17 15:26:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-17 15:26:34 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 15:27:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.01935388706624508 norm:0.009130174294114113 max memory_allocated 44355.7939453125 
[2025-03-17 15:27:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.01490760874003172 norm:0.006159904878586531 max memory_allocated 44355.7939453125 
[2025-03-17 15:28:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.01204361766576767 norm:0.005101717542856932 max memory_allocated 44355.7939453125 
[2025-03-17 15:29:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.011183010414242744 norm:0.004620993044227362 max memory_allocated 44355.7939453125 
[2025-03-17 15:30:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.010871196165680885 norm:0.004225876182317734 max memory_allocated 44355.7939453125 
[2025-03-17 15:30:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.010453596711158752 norm:0.0038654361851513386 max memory_allocated 44355.7939453125 
[2025-03-17 15:31:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.010040748864412308 norm:0.0036497958935797215 max memory_allocated 44355.7939453125 
[2025-03-17 15:32:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.009947407990694046 norm:0.003557051531970501 max memory_allocated 44355.7939453125 
[2025-03-17 15:32:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.01004802156239748 norm:0.003581894561648369 max memory_allocated 44355.7939453125 
[2025-03-17 15:33:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.009774720296263695 norm:0.0033408422023057938 max memory_allocated 44355.7939453125 
[2025-03-17 15:34:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.009657250717282295 norm:0.003228852292522788 max memory_allocated 44355.7939453125 
[2025-03-17 15:34:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.009701193310320377 norm:0.003297568531706929 max memory_allocated 44355.7939453125 
[2025-03-17 15:35:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.009903039783239365 norm:0.003322928911074996 max memory_allocated 44355.7939453125 
[2025-03-17 15:36:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.009639717638492584 norm:0.00306806736625731 max memory_allocated 44355.7939453125 
[2025-03-17 15:36:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.009742443449795246 norm:0.003253028029575944 max memory_allocated 44355.7939453125 
[2025-03-17 15:37:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.009708055295050144 norm:0.003110777586698532 max memory_allocated 44355.7939453125 
[2025-03-17 15:38:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.009826632216572762 norm:0.0031610080040991306 max memory_allocated 44355.7939453125 
[2025-03-17 15:38:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.00977671891450882 norm:0.0029799023177474737 max memory_allocated 44355.7939453125 
[2025-03-17 15:39:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.009664876386523247 norm:0.002713452558964491 max memory_allocated 44355.7939453125 
[2025-03-17 15:40:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.00958206970244646 norm:0.002617433201521635 max memory_allocated 44355.7939453125 
[2025-03-17 15:41:03 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-17 15:41:03 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3, 4] ===
[2025-03-17 15:42:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 0 loss:0.04759166017174721 norm:0.0049188206903636456 max memory_allocated 53571.974609375 
[2025-03-17 15:43:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 1 loss:0.03287508338689804 norm:0.0013151704333722591 max memory_allocated 53571.974609375 
[2025-03-17 15:45:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 2 loss:0.025086913257837296 norm:0.0006000254070386291 max memory_allocated 53571.974609375 
[2025-03-17 15:46:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 3 loss:0.022061040624976158 norm:0.00037008326034992933 max memory_allocated 53571.974609375 
[2025-03-17 15:47:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 4 loss:0.02052455022931099 norm:0.00030973638058640063 max memory_allocated 53571.974609375 
[2025-03-17 15:49:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 5 loss:0.019451290369033813 norm:0.0002692973939701915 max memory_allocated 53571.974609375 
[2025-03-17 15:50:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 6 loss:0.01879280060529709 norm:0.00028746298630721867 max memory_allocated 53571.974609375 
[2025-03-17 15:51:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 7 loss:0.018290778622031212 norm:0.00022357856505550444 max memory_allocated 53571.974609375 
[2025-03-17 15:53:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 8 loss:0.01802959479391575 norm:0.00021911915973760188 max memory_allocated 53571.974609375 
[2025-03-17 15:54:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 9 loss:0.01780761405825615 norm:0.0001866604434326291 max memory_allocated 53571.974609375 
[2025-03-17 15:55:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 10 loss:0.017732396721839905 norm:0.00022565211111214012 max memory_allocated 53571.974609375 
[2025-03-17 15:57:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 11 loss:0.017735429108142853 norm:0.00023664531181566417 max memory_allocated 53571.974609375 
[2025-03-17 15:58:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 12 loss:0.01763920858502388 norm:0.00018469066708348691 max memory_allocated 53571.974609375 
[2025-03-17 15:59:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 13 loss:0.01757531240582466 norm:0.00016832623805385083 max memory_allocated 53571.974609375 
[2025-03-17 16:01:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 14 loss:0.017491482198238373 norm:0.0001642107527004555 max memory_allocated 53571.974609375 
[2025-03-17 16:02:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 15 loss:0.017432069405913353 norm:0.00017579630366526544 max memory_allocated 53571.974609375 
[2025-03-17 16:03:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 16 loss:0.017454227432608604 norm:0.00019495468586683273 max memory_allocated 53571.974609375 
[2025-03-17 16:05:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 17 loss:0.01744115725159645 norm:0.00018809914763551205 max memory_allocated 53571.974609375 
[2025-03-17 16:06:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 18 loss:0.017391609027981758 norm:0.00018275569891557097 max memory_allocated 53571.974609375 
[2025-03-17 16:07:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3, 4]) iter 19 loss:0.01739441603422165 norm:0.0001880832714959979 max memory_allocated 53571.974609375 
[2025-03-17 16:09:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3, 4]
[2025-03-17 16:09:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [5] ===
[2025-03-17 16:10:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 0 loss:0.03688209876418114 norm:0.0034177121706306934 max memory_allocated 53571.974609375 
[2025-03-17 16:11:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 1 loss:0.02625393308699131 norm:0.0008619815926067531 max memory_allocated 53571.974609375 
[2025-03-17 16:11:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 2 loss:0.02107226848602295 norm:0.00040957529563456774 max memory_allocated 53571.974609375 
[2025-03-17 16:12:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 3 loss:0.019334465265274048 norm:0.0002679727622307837 max memory_allocated 53571.974609375 
[2025-03-17 16:13:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 4 loss:0.01838887855410576 norm:0.0002204097545472905 max memory_allocated 53571.974609375 
[2025-03-17 16:13:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 5 loss:0.017779888585209846 norm:0.00019710633205249906 max memory_allocated 53571.974609375 
[2025-03-17 16:14:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 6 loss:0.01740996725857258 norm:0.00016809871885925531 max memory_allocated 53571.974609375 
[2025-03-17 16:15:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 7 loss:0.017199736088514328 norm:0.00015511189121752977 max memory_allocated 53571.974609375 
[2025-03-17 16:15:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 8 loss:0.01707673817873001 norm:0.0001572393230162561 max memory_allocated 53571.974609375 
[2025-03-17 16:16:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 9 loss:0.01700948178768158 norm:0.00013126301928423345 max memory_allocated 53571.974609375 
[2025-03-17 16:17:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 10 loss:0.016949769109487534 norm:0.00012303059338591993 max memory_allocated 53571.974609375 
[2025-03-17 16:17:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 11 loss:0.016882797703146935 norm:0.0001076602129614912 max memory_allocated 53571.974609375 
[2025-03-17 16:18:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 12 loss:0.016868455335497856 norm:0.00012171286653028801 max memory_allocated 53571.974609375 
[2025-03-17 16:19:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 13 loss:0.016807083040475845 norm:0.00011353248555678874 max memory_allocated 53571.974609375 
[2025-03-17 16:19:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 14 loss:0.01677517592906952 norm:0.0001108139258576557 max memory_allocated 53571.974609375 
[2025-03-17 16:20:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 15 loss:0.016752438619732857 norm:9.59554745350033e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:21:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 16 loss:0.016727933660149574 norm:9.02432220755145e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:21:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 17 loss:0.016685429960489273 norm:8.685694774612784e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:22:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 18 loss:0.016695411875844002 norm:0.00010060312342830002 max memory_allocated 53571.974609375 
[2025-03-17 16:23:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [5]) iter 19 loss:0.016691258177161217 norm:9.930870874086395e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:24:07 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [5]
[2025-03-17 16:24:07 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [6] ===
[2025-03-17 16:24:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 0 loss:0.042337071150541306 norm:0.0009301186073571444 max memory_allocated 53571.974609375 
[2025-03-17 16:25:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 1 loss:0.03202503174543381 norm:0.0003668528806883842 max memory_allocated 53571.974609375 
[2025-03-17 16:26:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 2 loss:0.026180647313594818 norm:0.00026456339401192963 max memory_allocated 53571.974609375 
[2025-03-17 16:26:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 3 loss:0.024103980511426926 norm:0.00021165079670026898 max memory_allocated 53571.974609375 
[2025-03-17 16:27:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 4 loss:0.023121189326047897 norm:0.00023331589181907475 max memory_allocated 53571.974609375 
[2025-03-17 16:28:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 5 loss:0.02255352959036827 norm:0.0002625008055474609 max memory_allocated 53571.974609375 
[2025-03-17 16:28:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 6 loss:0.022281384095549583 norm:0.0002786665572784841 max memory_allocated 53571.974609375 
[2025-03-17 16:29:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 7 loss:0.021899763494729996 norm:0.0002493792271707207 max memory_allocated 53571.974609375 
[2025-03-17 16:30:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 8 loss:0.021738611161708832 norm:0.00021238118642941117 max memory_allocated 53571.974609375 
[2025-03-17 16:30:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 9 loss:0.02159847505390644 norm:0.00019427722145337611 max memory_allocated 53571.974609375 
[2025-03-17 16:31:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 10 loss:0.021513421088457108 norm:0.00013412476982921362 max memory_allocated 53571.974609375 
[2025-03-17 16:32:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 11 loss:0.021467939019203186 norm:0.00016089221753645688 max memory_allocated 53571.974609375 
[2025-03-17 16:32:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 12 loss:0.021467765793204308 norm:0.00019508463446982205 max memory_allocated 53571.974609375 
[2025-03-17 16:33:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 13 loss:0.021412748843431473 norm:0.00011630346125457436 max memory_allocated 53571.974609375 
[2025-03-17 16:34:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 14 loss:0.02175968512892723 norm:0.0004344347689766437 max memory_allocated 53571.974609375 
[2025-03-17 16:34:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 15 loss:0.021456044167280197 norm:0.0001361655886285007 max memory_allocated 53571.974609375 
[2025-03-17 16:35:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 16 loss:0.02137523889541626 norm:0.0001141088578151539 max memory_allocated 53571.974609375 
[2025-03-17 16:36:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 17 loss:0.02139490470290184 norm:0.00011814683966804296 max memory_allocated 53571.974609375 
[2025-03-17 16:37:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 18 loss:0.021600209176540375 norm:0.00017710572865325958 max memory_allocated 53571.974609375 
[2025-03-17 16:37:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6]) iter 19 loss:0.021484222263097763 norm:0.00020456669153645635 max memory_allocated 53571.974609375 
[2025-03-17 16:38:35 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [6]
[2025-03-17 16:38:36 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [7] ===
[2025-03-17 16:39:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 0 loss:0.03892850503325462 norm:0.00042987216147594154 max memory_allocated 53571.974609375 
[2025-03-17 16:40:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 1 loss:0.031717438250780106 norm:0.00021682614169549197 max memory_allocated 53571.974609375 
[2025-03-17 16:40:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 2 loss:0.026937279850244522 norm:0.00014941029075998813 max memory_allocated 53571.974609375 
[2025-03-17 16:41:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 3 loss:0.025267811492085457 norm:0.00012898950080852956 max memory_allocated 53571.974609375 
[2025-03-17 16:42:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 4 loss:0.024409765377640724 norm:0.00011560117854969576 max memory_allocated 53571.974609375 
[2025-03-17 16:42:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 5 loss:0.02388453297317028 norm:0.00011063866986660287 max memory_allocated 53571.974609375 
[2025-03-17 16:43:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 6 loss:0.02360152266919613 norm:0.00010756708797998726 max memory_allocated 53571.974609375 
[2025-03-17 16:44:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 7 loss:0.02343561314046383 norm:9.162221249425784e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:44:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 8 loss:0.023346146568655968 norm:9.095085260923952e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:45:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 9 loss:0.023299526423215866 norm:9.446084732189775e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:46:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 10 loss:0.023266784846782684 norm:9.34606505325064e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:46:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 11 loss:0.023197360336780548 norm:8.454429917037487e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:47:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 12 loss:0.023190151900053024 norm:8.521579729858786e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:48:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 13 loss:0.023176923394203186 norm:8.523112046532333e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:48:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 14 loss:0.02315741777420044 norm:8.607823110651225e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:49:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 15 loss:0.02312677726149559 norm:8.659253944642842e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:50:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 16 loss:0.023108910769224167 norm:8.734322182135656e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:50:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 17 loss:0.02311057783663273 norm:8.787455590208992e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:51:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 18 loss:0.02309541217982769 norm:8.216469723265618e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:52:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [7]) iter 19 loss:0.02310417778789997 norm:8.413582690991461e-05 max memory_allocated 53571.974609375 
[2025-03-17 16:53:00 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [7]
[2025-03-17 16:53:00 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [8, 9] ===
[2025-03-17 16:54:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 0 loss:0.0634857714176178 norm:0.00085799879161641 max memory_allocated 53572.365234375 
[2025-03-17 16:55:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 1 loss:0.04950805753469467 norm:0.0004039713239762932 max memory_allocated 53572.365234375 
[2025-03-17 16:57:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 2 loss:0.04078725725412369 norm:0.0002890524920076132 max memory_allocated 53572.365234375 
[2025-03-17 16:58:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 3 loss:0.037236958742141724 norm:0.00024333136389032006 max memory_allocated 53572.365234375 
[2025-03-17 16:59:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 4 loss:0.035345062613487244 norm:0.00022325615282170475 max memory_allocated 53572.365234375 
[2025-03-17 17:01:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 5 loss:0.034117117524147034 norm:0.00020162211148999631 max memory_allocated 53572.365234375 
[2025-03-17 17:02:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 6 loss:0.03330492600798607 norm:0.00018014620582107455 max memory_allocated 53572.365234375 
[2025-03-17 17:03:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 7 loss:0.03280974552035332 norm:0.00017153337830677629 max memory_allocated 53572.365234375 
[2025-03-17 17:05:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 8 loss:0.03249659016728401 norm:0.000171049076016061 max memory_allocated 53572.365234375 
[2025-03-17 17:06:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 9 loss:0.03230103850364685 norm:0.0001573529007146135 max memory_allocated 53572.365234375 
[2025-03-17 17:07:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 10 loss:0.032126717269420624 norm:0.00014945183647796512 max memory_allocated 53572.365234375 
[2025-03-17 17:09:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 11 loss:0.03203211724758148 norm:0.00015843051369301975 max memory_allocated 53572.365234375 
[2025-03-17 17:10:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 12 loss:0.03197433054447174 norm:0.00015263141540344805 max memory_allocated 53572.365234375 
[2025-03-17 17:11:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 13 loss:0.03190724179148674 norm:0.00015925370098557323 max memory_allocated 53572.365234375 
[2025-03-17 17:13:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 14 loss:0.031874436885118484 norm:0.00015997301670722663 max memory_allocated 53572.365234375 
[2025-03-17 17:14:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 15 loss:0.03183932602405548 norm:0.00015602385974489152 max memory_allocated 53572.365234375 
[2025-03-17 17:15:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 16 loss:0.031807105988264084 norm:0.00015980385069269687 max memory_allocated 53572.365234375 
[2025-03-17 17:17:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 17 loss:0.031782470643520355 norm:0.00015317271754611284 max memory_allocated 53572.365234375 
[2025-03-17 17:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 18 loss:0.03174760192632675 norm:0.00014887284487485886 max memory_allocated 53572.365234375 
[2025-03-17 17:19:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [8, 9]) iter 19 loss:0.031726837158203125 norm:0.0001514781324658543 max memory_allocated 53572.365234375 
[2025-03-17 17:21:36 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [8, 9]
[2025-03-17 17:21:36 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [10, 11] ===
[2025-03-17 17:23:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 0 loss:0.06656961143016815 norm:0.0006356821395456791 max memory_allocated 53573.521484375 
[2025-03-17 17:24:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 1 loss:0.05406879633665085 norm:0.0003532901464495808 max memory_allocated 53573.521484375 
[2025-03-17 17:25:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 2 loss:0.04522978141903877 norm:0.00023919450177345425 max memory_allocated 53573.521484375 
[2025-03-17 17:27:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 3 loss:0.041709937155246735 norm:0.00018586392980068922 max memory_allocated 53573.521484375 
[2025-03-17 17:28:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 4 loss:0.04001018404960632 norm:0.000160092007718049 max memory_allocated 53573.521484375 
[2025-03-17 17:29:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 5 loss:0.03890583664178848 norm:0.00014675201964564621 max memory_allocated 53573.521484375 
[2025-03-17 17:31:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 6 loss:0.03821414336562157 norm:0.00013732962543144822 max memory_allocated 53573.521484375 
[2025-03-17 17:32:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 7 loss:0.03783257305622101 norm:0.00013208533346187323 max memory_allocated 53573.521484375 
[2025-03-17 17:33:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 8 loss:0.03756590932607651 norm:0.00012687330308835953 max memory_allocated 53573.521484375 
[2025-03-17 17:35:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 9 loss:0.03742044046521187 norm:0.00012242156662978232 max memory_allocated 53573.521484375 
[2025-03-17 17:36:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 10 loss:0.037316665053367615 norm:0.00012019580753985792 max memory_allocated 53573.521484375 
[2025-03-17 17:37:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 11 loss:0.03720327094197273 norm:0.00011615544644882903 max memory_allocated 53573.521484375 
[2025-03-17 17:39:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 12 loss:0.037132374942302704 norm:0.00011267280206084251 max memory_allocated 53573.521484375 
[2025-03-17 17:40:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 13 loss:0.037109483033418655 norm:0.0001119239823310636 max memory_allocated 53573.521484375 
[2025-03-17 17:41:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 14 loss:0.03707461059093475 norm:0.00011086371523560956 max memory_allocated 53573.521484375 
[2025-03-17 17:43:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 15 loss:0.03701537102460861 norm:0.00010878950706683099 max memory_allocated 53573.521484375 
[2025-03-17 17:44:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 16 loss:0.03697090595960617 norm:0.00010869083052966744 max memory_allocated 53573.521484375 
[2025-03-17 17:45:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 17 loss:0.03697174787521362 norm:0.00010919557098532096 max memory_allocated 53573.521484375 
[2025-03-17 17:47:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 18 loss:0.03694802522659302 norm:0.00010896550520556048 max memory_allocated 53573.521484375 
[2025-03-17 17:48:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [10, 11]) iter 19 loss:0.03693961724638939 norm:0.00011108038597740233 max memory_allocated 53573.521484375 
[2025-03-17 17:50:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [10, 11]
[2025-03-17 17:50:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [12, 13] ===
[2025-03-17 17:51:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 0 loss:0.07362227141857147 norm:0.0005428144941106439 max memory_allocated 53573.677734375 
[2025-03-17 17:53:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 1 loss:0.06109243631362915 norm:0.0003242759557906538 max memory_allocated 53574.677734375 
[2025-03-17 17:54:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 2 loss:0.05122450739145279 norm:0.00022949563572183251 max memory_allocated 53574.677734375 
[2025-03-17 17:55:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 3 loss:0.047464754432439804 norm:0.00018794808420352638 max memory_allocated 53574.677734375 
[2025-03-17 17:57:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 4 loss:0.045655712485313416 norm:0.0001683723967289552 max memory_allocated 53574.677734375 
[2025-03-17 17:58:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 5 loss:0.04446803405880928 norm:0.0001550619927002117 max memory_allocated 53574.677734375 
[2025-03-17 17:59:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 6 loss:0.043765805661678314 norm:0.00014696706784889102 max memory_allocated 53574.677734375 
[2025-03-17 18:01:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 7 loss:0.04342598468065262 norm:0.0001424257643520832 max memory_allocated 53574.677734375 
[2025-03-17 18:02:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 8 loss:0.043143726885318756 norm:0.0001377949520247057 max memory_allocated 53574.677734375 
[2025-03-17 18:03:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 9 loss:0.04296553134918213 norm:0.00013807034702040255 max memory_allocated 53574.677734375 
[2025-03-17 18:05:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 10 loss:0.042805615812540054 norm:0.0001279191201319918 max memory_allocated 53574.677734375 
[2025-03-17 18:06:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 11 loss:0.042680904269218445 norm:0.00012531207175925374 max memory_allocated 53574.677734375 
[2025-03-17 18:07:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 12 loss:0.042583927512168884 norm:0.00012412316573318094 max memory_allocated 53574.677734375 
[2025-03-17 18:09:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 13 loss:0.042515307664871216 norm:0.0001223078725161031 max memory_allocated 53574.677734375 
[2025-03-17 18:10:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 14 loss:0.04246765375137329 norm:0.00012228151899762452 max memory_allocated 53574.677734375 
[2025-03-17 18:11:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 15 loss:0.04240841045975685 norm:0.00012031793448841199 max memory_allocated 53574.677734375 
[2025-03-17 18:13:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 16 loss:0.042346492409706116 norm:0.00011973386426689103 max memory_allocated 53574.677734375 
[2025-03-17 18:14:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 17 loss:0.04234430566430092 norm:0.00011909564636880532 max memory_allocated 53574.677734375 
[2025-03-17 18:15:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 18 loss:0.042323432862758636 norm:0.00011747267853934318 max memory_allocated 53574.677734375 
[2025-03-17 18:17:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [12, 13]) iter 19 loss:0.042288631200790405 norm:0.00011838414502562955 max memory_allocated 53574.677734375 
[2025-03-17 18:18:59 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [12, 13]
[2025-03-17 18:18:59 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [14, 15] ===
[2025-03-17 18:20:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 0 loss:0.08043034374713898 norm:0.0005823111860081553 max memory_allocated 53574.677734375 
[2025-03-17 18:21:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 1 loss:0.0678325816988945 norm:0.000321159022860229 max memory_allocated 53574.677734375 
[2025-03-17 18:23:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 2 loss:0.057914625853300095 norm:0.00023842354130465537 max memory_allocated 53574.677734375 
[2025-03-17 18:24:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 3 loss:0.05413120239973068 norm:0.00020353449508547783 max memory_allocated 53574.677734375 
[2025-03-17 18:25:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 4 loss:0.05200047418475151 norm:0.00018330280727241188 max memory_allocated 53574.677734375 
[2025-03-17 18:27:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 5 loss:0.05068518966436386 norm:0.00017140648560598493 max memory_allocated 53574.677734375 
[2025-03-17 18:28:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 6 loss:0.04989926517009735 norm:0.00015992602857295424 max memory_allocated 53574.677734375 
[2025-03-17 18:29:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 7 loss:0.049466297030448914 norm:0.00015409430488944054 max memory_allocated 53574.677734375 
[2025-03-17 18:31:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 8 loss:0.04914841800928116 norm:0.00014469504822045565 max memory_allocated 53574.677734375 
[2025-03-17 18:32:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 9 loss:0.048926111310720444 norm:0.00014008948346599936 max memory_allocated 53574.677734375 
[2025-03-17 18:33:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 10 loss:0.04874957352876663 norm:0.0001352909894194454 max memory_allocated 53574.677734375 
[2025-03-17 18:35:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 11 loss:0.048632167279720306 norm:0.00013202318223193288 max memory_allocated 53574.677734375 
[2025-03-17 18:36:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 12 loss:0.04852332919836044 norm:0.00013122183736413717 max memory_allocated 53574.677734375 
[2025-03-17 18:37:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 13 loss:0.048428021371364594 norm:0.00012654262536671013 max memory_allocated 53574.677734375 
[2025-03-17 18:39:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 14 loss:0.04834682121872902 norm:0.0001250015338882804 max memory_allocated 53574.677734375 
[2025-03-17 18:40:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 15 loss:0.04826606065034866 norm:0.00012246765254531056 max memory_allocated 53574.677734375 
[2025-03-17 18:41:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 16 loss:0.04822760075330734 norm:0.00012338004307821393 max memory_allocated 53574.677734375 
[2025-03-17 18:43:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 17 loss:0.04817604646086693 norm:0.00012256282207090408 max memory_allocated 53574.677734375 
[2025-03-17 18:44:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 18 loss:0.04813823848962784 norm:0.00012175148003734648 max memory_allocated 53574.677734375 
[2025-03-17 18:45:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [14, 15]) iter 19 loss:0.048104919493198395 norm:0.00012116860307287425 max memory_allocated 53574.677734375 
[2025-03-17 18:47:43 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [14, 15]
[2025-03-17 18:47:43 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [16, 17] ===
[2025-03-17 18:49:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 0 loss:0.08569924533367157 norm:0.00040875078411772847 max memory_allocated 53574.677734375 
[2025-03-17 18:50:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 1 loss:0.07402866333723068 norm:0.00024176185252144933 max memory_allocated 53574.677734375 
[2025-03-17 18:51:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 2 loss:0.06422127038240433 norm:0.00018315357738174498 max memory_allocated 53574.677734375 
[2025-03-17 18:53:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 3 loss:0.06079942360520363 norm:0.0001613261119928211 max memory_allocated 53574.677734375 
[2025-03-17 18:54:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 4 loss:0.05869528278708458 norm:0.00014954456128180027 max memory_allocated 53574.677734375 
[2025-03-17 18:55:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 5 loss:0.0574771948158741 norm:0.00014288854436017573 max memory_allocated 53574.677734375 
[2025-03-17 18:57:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 6 loss:0.05686061084270477 norm:0.00013403015327639878 max memory_allocated 53574.677734375 
[2025-03-17 18:58:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 7 loss:0.05652221292257309 norm:0.00013003210187889636 max memory_allocated 53574.677734375 
[2025-03-17 18:59:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 8 loss:0.05628737807273865 norm:0.00012344421702437103 max memory_allocated 53574.677734375 
[2025-03-17 19:01:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 9 loss:0.05612892284989357 norm:0.00011986595200141892 max memory_allocated 53574.677734375 
[2025-03-17 19:02:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 10 loss:0.055990736931562424 norm:0.0001137582803494297 max memory_allocated 53574.677734375 
[2025-03-17 19:03:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 11 loss:0.05585646629333496 norm:0.00011184650065843016 max memory_allocated 53574.677734375 
[2025-03-17 19:05:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 12 loss:0.05572764575481415 norm:0.00010898472100961953 max memory_allocated 53574.677734375 
[2025-03-17 19:06:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 13 loss:0.0556272491812706 norm:0.0001075283216778189 max memory_allocated 53574.677734375 
[2025-03-17 19:07:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 14 loss:0.05554921552538872 norm:0.00010636975639499724 max memory_allocated 53574.677734375 
[2025-03-17 19:09:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 15 loss:0.055487681180238724 norm:0.00010585429117782041 max memory_allocated 53574.677734375 
[2025-03-17 19:10:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 16 loss:0.055450186133384705 norm:0.00010564373224042356 max memory_allocated 53574.677734375 
[2025-03-17 19:11:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 17 loss:0.05538617819547653 norm:0.00010417135490570217 max memory_allocated 53574.677734375 
[2025-03-17 19:13:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 18 loss:0.0553302988409996 norm:0.0001037277834257111 max memory_allocated 53574.677734375 
[2025-03-17 19:14:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [16, 17]) iter 19 loss:0.055282700806856155 norm:0.00010287532495567575 max memory_allocated 53574.677734375 
[2025-03-17 19:16:29 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [16, 17]
[2025-03-17 19:16:29 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [18, 19] ===
[2025-03-17 19:17:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 0 loss:0.10442228615283966 norm:0.0005311096319928765 max memory_allocated 53574.677734375 
[2025-03-17 19:19:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 1 loss:0.09049728512763977 norm:0.000322185514960438 max memory_allocated 53574.677734375 
[2025-03-17 19:20:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 2 loss:0.07852905988693237 norm:0.0002257238229503855 max memory_allocated 53574.677734375 
[2025-03-17 19:21:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 3 loss:0.0744691714644432 norm:0.00019870417600031942 max memory_allocated 53574.677734375 
[2025-03-17 19:23:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 4 loss:0.07217034697532654 norm:0.0001878364128060639 max memory_allocated 53574.677734375 
[2025-03-17 19:24:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 5 loss:0.0709543451666832 norm:0.00017410492000635713 max memory_allocated 53574.677734375 
[2025-03-17 19:25:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 6 loss:0.07036145776510239 norm:0.00016425983631052077 max memory_allocated 53574.677734375 
[2025-03-17 19:27:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 7 loss:0.07001195847988129 norm:0.00015618337783962488 max memory_allocated 53574.677734375 
[2025-03-17 19:28:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 8 loss:0.06975492089986801 norm:0.0001489633577875793 max memory_allocated 53574.677734375 
[2025-03-17 19:30:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 9 loss:0.06950516998767853 norm:0.00014150050992611796 max memory_allocated 53574.677734375 
[2025-03-17 19:31:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 10 loss:0.06934837996959686 norm:0.00013818242587149143 max memory_allocated 53574.677734375 
[2025-03-17 19:32:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 11 loss:0.0692184716463089 norm:0.00013513854355551302 max memory_allocated 53574.677734375 
[2025-03-17 19:34:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 12 loss:0.06910169124603271 norm:0.0001336775894742459 max memory_allocated 53574.677734375 
[2025-03-17 19:35:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 13 loss:0.06896345317363739 norm:0.00013140036026015878 max memory_allocated 53574.677734375 
[2025-03-17 19:36:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 14 loss:0.06884583085775375 norm:0.0001292489469051361 max memory_allocated 53574.677734375 
[2025-03-17 19:38:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 15 loss:0.06873884797096252 norm:0.00012828558101318777 max memory_allocated 53574.677734375 
[2025-03-17 19:39:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 16 loss:0.06867343187332153 norm:0.0001269096537726 max memory_allocated 53574.677734375 
[2025-03-17 19:40:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 17 loss:0.06860198825597763 norm:0.00012430257629603148 max memory_allocated 53574.677734375 
[2025-03-17 19:42:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 18 loss:0.06854493170976639 norm:0.00012273419997654855 max memory_allocated 53574.677734375 
[2025-03-17 19:43:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [18, 19]) iter 19 loss:0.06848132610321045 norm:0.00012182045611552894 max memory_allocated 53574.677734375 
[2025-03-17 19:45:15 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [18, 19]
[2025-03-17 19:45:15 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [20, 21] ===
[2025-03-17 19:46:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 0 loss:0.1302015483379364 norm:0.0006268103024922311 max memory_allocated 53574.677734375 
[2025-03-17 19:48:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 1 loss:0.11222278326749802 norm:0.0003616440808400512 max memory_allocated 53574.677734375 
[2025-03-17 19:49:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 2 loss:0.09688965976238251 norm:0.0002368940768064931 max memory_allocated 53574.677734375 
[2025-03-17 19:50:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 3 loss:0.092198446393013 norm:0.00021402996208053082 max memory_allocated 53574.677734375 
[2025-03-17 19:52:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 4 loss:0.08959288150072098 norm:0.00019840312597807497 max memory_allocated 53574.677734375 
[2025-03-17 19:53:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 5 loss:0.08847822993993759 norm:0.00018694999744184315 max memory_allocated 53574.677734375 
[2025-03-17 19:54:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 6 loss:0.08789733052253723 norm:0.0001800354802981019 max memory_allocated 53574.677734375 
[2025-03-17 19:56:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 7 loss:0.08748337626457214 norm:0.0001673419465078041 max memory_allocated 53574.677734375 
[2025-03-17 19:57:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 8 loss:0.08716004341840744 norm:0.0001603521959623322 max memory_allocated 53574.677734375 
[2025-03-17 19:58:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 9 loss:0.08686216920614243 norm:0.00015484352479688823 max memory_allocated 53574.677734375 
[2025-03-17 20:00:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 10 loss:0.0866401195526123 norm:0.00015296999481506646 max memory_allocated 53574.677734375 
[2025-03-17 20:01:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 11 loss:0.08642677217721939 norm:0.0001482723164372146 max memory_allocated 53574.677734375 
[2025-03-17 20:02:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 12 loss:0.08623642474412918 norm:0.00014598414418287575 max memory_allocated 53574.677734375 
[2025-03-17 20:04:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 13 loss:0.08608388900756836 norm:0.0001413025165675208 max memory_allocated 53574.677734375 
[2025-03-17 20:05:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 14 loss:0.08594629913568497 norm:0.00013922866492066532 max memory_allocated 53574.677734375 
[2025-03-17 20:06:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 15 loss:0.08577370643615723 norm:0.0001364293711958453 max memory_allocated 53574.677734375 
[2025-03-17 20:08:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 16 loss:0.08566305786371231 norm:0.00013509707059711218 max memory_allocated 53574.677734375 
[2025-03-17 20:09:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 17 loss:0.08557053655385971 norm:0.0001320684386882931 max memory_allocated 53574.677734375 
[2025-03-17 20:10:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 18 loss:0.0854833573102951 norm:0.0001300223811995238 max memory_allocated 53574.677734375 
[2025-03-17 20:12:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [20, 21]) iter 19 loss:0.08540602773427963 norm:0.00012702701496891677 max memory_allocated 53574.677734375 
[2025-03-17 20:14:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [20, 21]
[2025-03-17 20:14:01 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [22, 23] ===
[2025-03-17 20:15:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 0 loss:0.1640862226486206 norm:0.0006799572147428989 max memory_allocated 53574.677734375 
[2025-03-17 20:16:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 1 loss:0.14325276017189026 norm:0.000412137305829674 max memory_allocated 53574.677734375 
[2025-03-17 20:18:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 2 loss:0.124815933406353 norm:0.000308497401420027 max memory_allocated 53574.677734375 
[2025-03-17 20:19:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 3 loss:0.11930987983942032 norm:0.0002977357362397015 max memory_allocated 53574.677734375 
[2025-03-17 20:20:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 4 loss:0.11645340919494629 norm:0.0002831903984770179 max memory_allocated 53574.677734375 
[2025-03-17 20:22:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 5 loss:0.11536571383476257 norm:0.000273234152700752 max memory_allocated 53574.677734375 
[2025-03-17 20:23:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 6 loss:0.1147344559431076 norm:0.00026596258976496756 max memory_allocated 53574.677734375 
[2025-03-17 20:24:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 7 loss:0.11418306827545166 norm:0.0002552199293859303 max memory_allocated 53574.677734375 
[2025-03-17 20:26:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 8 loss:0.11372390389442444 norm:0.00024938196293078363 max memory_allocated 53574.677734375 
[2025-03-17 20:27:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 9 loss:0.11332917213439941 norm:0.00024308371939696372 max memory_allocated 53574.677734375 
[2025-03-17 20:28:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 10 loss:0.11299306154251099 norm:0.0002392882015556097 max memory_allocated 53574.677734375 
[2025-03-17 20:30:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 11 loss:0.11271791160106659 norm:0.00023188896011561155 max memory_allocated 53574.677734375 
[2025-03-17 20:31:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 12 loss:0.11245463788509369 norm:0.0002278369211126119 max memory_allocated 53574.677734375 
[2025-03-17 20:32:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 13 loss:0.11223360896110535 norm:0.00022375815024133772 max memory_allocated 53574.677734375 
[2025-03-17 20:34:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 14 loss:0.11204469203948975 norm:0.0002192444953834638 max memory_allocated 53574.677734375 
[2025-03-17 20:35:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 15 loss:0.1118280291557312 norm:0.00021362265397328883 max memory_allocated 53574.677734375 
[2025-03-17 20:36:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 16 loss:0.11165538430213928 norm:0.00020950436010025442 max memory_allocated 53574.677734375 
[2025-03-17 20:38:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 17 loss:0.11148400604724884 norm:0.00020569213666021824 max memory_allocated 53574.677734375 
[2025-03-17 20:39:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 18 loss:0.11134830862283707 norm:0.00020519894314929843 max memory_allocated 53574.677734375 
[2025-03-17 20:40:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [22, 23]) iter 19 loss:0.11123086512088776 norm:0.00020204724569339305 max memory_allocated 53574.677734375 
[2025-03-17 20:42:45 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [22, 23]
[2025-03-17 20:42:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [24, 25] ===
[2025-03-17 20:44:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 0 loss:0.20087532699108124 norm:0.0006253274623304605 max memory_allocated 53574.677734375 
[2025-03-17 20:45:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 1 loss:0.177506223320961 norm:0.0004008091927971691 max memory_allocated 53574.677734375 
[2025-03-17 20:46:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 2 loss:0.1561795026063919 norm:0.00028468488017097116 max memory_allocated 53574.677734375 
[2025-03-17 20:48:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 3 loss:0.14980816841125488 norm:0.00026541954139247537 max memory_allocated 53574.677734375 
[2025-03-17 20:49:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 4 loss:0.14701825380325317 norm:0.00025683437706902623 max memory_allocated 53574.677734375 
[2025-03-17 20:50:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 5 loss:0.1459241807460785 norm:0.00024196751473937184 max memory_allocated 53574.677734375 
[2025-03-17 20:52:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 6 loss:0.14521145820617676 norm:0.00023570205667056143 max memory_allocated 53574.677734375 
[2025-03-17 20:53:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 7 loss:0.14466914534568787 norm:0.00023308608797378838 max memory_allocated 53574.677734375 
[2025-03-17 20:54:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 8 loss:0.14420104026794434 norm:0.00022849846573080868 max memory_allocated 53574.677734375 
[2025-03-17 20:56:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 9 loss:0.14378850162029266 norm:0.0002212740364484489 max memory_allocated 53574.677734375 
[2025-03-17 20:57:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 10 loss:0.1433688998222351 norm:0.00021520035807043314 max memory_allocated 53574.677734375 
[2025-03-17 20:58:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 11 loss:0.14302301406860352 norm:0.00021128689695615321 max memory_allocated 53574.677734375 
[2025-03-17 21:00:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 12 loss:0.14274846017360687 norm:0.00020944149582646787 max memory_allocated 53574.677734375 
[2025-03-17 21:01:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 13 loss:0.14247240126132965 norm:0.0002078602701658383 max memory_allocated 53574.677734375 
[2025-03-17 21:02:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 14 loss:0.1422252357006073 norm:0.00020192604279145598 max memory_allocated 53574.677734375 
[2025-03-17 21:04:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 15 loss:0.1420038491487503 norm:0.0002002682740567252 max memory_allocated 53574.677734375 
[2025-03-17 21:05:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 16 loss:0.14183621108531952 norm:0.00019900001643691212 max memory_allocated 53574.677734375 
[2025-03-17 21:06:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 17 loss:0.1416635811328888 norm:0.00019661607802845538 max memory_allocated 53574.677734375 
[2025-03-17 21:08:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 18 loss:0.14152777194976807 norm:0.00019481146591715515 max memory_allocated 53574.677734375 
[2025-03-17 21:09:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [24, 25]) iter 19 loss:0.1413756161928177 norm:0.00019106693798676133 max memory_allocated 53574.677734375 
[2025-03-17 21:11:29 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [24, 25]
[2025-03-17 21:11:29 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [26, 27] ===
[2025-03-17 21:12:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 0 loss:0.24651367962360382 norm:0.0006655470933765173 max memory_allocated 53574.677734375 
[2025-03-17 21:14:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 1 loss:0.21904711425304413 norm:0.00041933247121050954 max memory_allocated 53574.677734375 
[2025-03-17 21:15:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 2 loss:0.19333168864250183 norm:0.00028743152506649494 max memory_allocated 53574.677734375 
[2025-03-17 21:16:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 3 loss:0.18608003854751587 norm:0.0002694577560760081 max memory_allocated 53574.677734375 
[2025-03-17 21:18:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 4 loss:0.18327677249908447 norm:0.0002568401687312871 max memory_allocated 53574.677734375 
[2025-03-17 21:19:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 5 loss:0.1822497546672821 norm:0.00024495270918123424 max memory_allocated 53574.677734375 
[2025-03-17 21:20:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 6 loss:0.18153630197048187 norm:0.0002371591399423778 max memory_allocated 53574.677734375 
[2025-03-17 21:22:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 7 loss:0.18088039755821228 norm:0.0002288282848894596 max memory_allocated 53574.677734375 
[2025-03-17 21:23:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 8 loss:0.18038582801818848 norm:0.0002254278224427253 max memory_allocated 53574.677734375 
[2025-03-17 21:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 9 loss:0.1798909306526184 norm:0.0002179171278839931 max memory_allocated 53574.677734375 
[2025-03-17 21:26:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 10 loss:0.17948462069034576 norm:0.00021449345513246953 max memory_allocated 53574.677734375 
[2025-03-17 21:27:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 11 loss:0.17910334467887878 norm:0.00020872167078778148 max memory_allocated 53574.677734375 
[2025-03-17 21:29:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 12 loss:0.17878396809101105 norm:0.0002054396172752604 max memory_allocated 53574.677734375 
[2025-03-17 21:30:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 13 loss:0.17848245799541473 norm:0.00019956965115852654 max memory_allocated 53574.677734375 
[2025-03-17 21:31:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 14 loss:0.17820774018764496 norm:0.00019827194046229124 max memory_allocated 53574.677734375 
[2025-03-17 21:33:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 15 loss:0.17796891927719116 norm:0.00019875593716278672 max memory_allocated 53574.677734375 
[2025-03-17 21:34:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 16 loss:0.17777542769908905 norm:0.00019640462414827198 max memory_allocated 53574.677734375 
[2025-03-17 21:35:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 17 loss:0.17757637798786163 norm:0.0001936647604452446 max memory_allocated 53574.677734375 
[2025-03-17 21:37:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 18 loss:0.1773630529642105 norm:0.00019259545661043376 max memory_allocated 53574.677734375 
[2025-03-17 21:38:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [26, 27]) iter 19 loss:0.17718422412872314 norm:0.00019049554248340428 max memory_allocated 53574.677734375 
[2025-03-17 21:40:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 16, block: [26, 27]
[2025-03-17 21:40:13 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [28, 29] ===
[2025-03-17 21:41:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 0 loss:0.2957581877708435 norm:0.0008217634749598801 max memory_allocated 53574.677734375 
[2025-03-17 21:43:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 1 loss:0.26399779319763184 norm:0.0004555927589535713 max memory_allocated 53574.677734375 
[2025-03-17 21:44:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 2 loss:0.23427125811576843 norm:0.00033432396594434977 max memory_allocated 53574.677734375 
[2025-03-17 21:45:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 3 loss:0.22609259188175201 norm:0.00030620372854173183 max memory_allocated 53574.677734375 
[2025-03-17 21:47:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 4 loss:0.22347500920295715 norm:0.0002875525096897036 max memory_allocated 53574.677734375 
[2025-03-17 21:48:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 5 loss:0.2224307805299759 norm:0.0002706362574826926 max memory_allocated 53574.677734375 
[2025-03-17 21:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 6 loss:0.22163578867912292 norm:0.00026657478883862495 max memory_allocated 53574.677734375 
[2025-03-17 21:51:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 7 loss:0.22104771435260773 norm:0.00026042776880785823 max memory_allocated 53574.677734375 
[2025-03-17 21:52:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 8 loss:0.2204882949590683 norm:0.0002517588436603546 max memory_allocated 53574.677734375 
[2025-03-17 21:53:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 9 loss:0.2199905812740326 norm:0.0002424456033622846 max memory_allocated 53574.677734375 
[2025-03-17 21:55:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 10 loss:0.21959803998470306 norm:0.00024019570264499635 max memory_allocated 53574.677734375 
[2025-03-17 21:56:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 11 loss:0.21918392181396484 norm:0.0002503058931324631 max memory_allocated 53574.677734375 
[2025-03-17 21:57:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 12 loss:0.2187877595424652 norm:0.00023232234525494277 max memory_allocated 53574.677734375 
[2025-03-17 21:59:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 13 loss:0.2184412181377411 norm:0.00022883052588440478 max memory_allocated 53574.677734375 
[2025-03-17 22:00:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 14 loss:0.21814383566379547 norm:0.00022703938884660602 max memory_allocated 53574.677734375 
[2025-03-17 22:01:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 15 loss:0.2178804874420166 norm:0.00022554352472070605 max memory_allocated 53574.677734375 
[2025-03-17 22:03:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 16 loss:0.21765419840812683 norm:0.0002345781249459833 max memory_allocated 53574.677734375 
[2025-03-17 22:04:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 17 loss:0.21742761135101318 norm:0.00023414715542457998 max memory_allocated 53574.677734375 
[2025-03-17 22:05:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 18 loss:0.21720290184020996 norm:0.00023211106599774212 max memory_allocated 53574.677734375 
[2025-03-17 22:07:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [28, 29]) iter 19 loss:0.2169988602399826 norm:0.00026039849035441875 max memory_allocated 53574.677734375 
[2025-03-17 22:08:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 17, block: [28, 29]
[2025-03-17 22:08:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 18 with layers [30, 31] ===
[2025-03-17 22:10:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 0 loss:0.34669411182403564 norm:0.000631161849014461 max memory_allocated 53574.677734375 
[2025-03-17 22:11:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 1 loss:0.31379708647727966 norm:0.00042160163866356015 max memory_allocated 53574.677734375 
[2025-03-17 22:13:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 2 loss:0.28194743394851685 norm:0.0002805760013870895 max memory_allocated 53574.677734375 
[2025-03-17 22:14:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 3 loss:0.2731623649597168 norm:0.0002656250144354999 max memory_allocated 53574.677734375 
[2025-03-17 22:15:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 4 loss:0.27067893743515015 norm:0.0002527724427636713 max memory_allocated 53574.677734375 
[2025-03-17 22:17:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 5 loss:0.26956701278686523 norm:0.00025160732911899686 max memory_allocated 53574.677734375 
[2025-03-17 22:18:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 6 loss:0.26871350407600403 norm:0.00024541147286072373 max memory_allocated 53574.677734375 
[2025-03-17 22:19:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 7 loss:0.2680082321166992 norm:0.0002513690560590476 max memory_allocated 53574.677734375 
[2025-03-17 22:21:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 8 loss:0.2674052119255066 norm:0.00023482181131839752 max memory_allocated 53574.677734375 
[2025-03-17 22:22:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 9 loss:0.2668752670288086 norm:0.00023670270456932485 max memory_allocated 53574.677734375 
[2025-03-17 22:23:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 10 loss:0.266384482383728 norm:0.00023186388716567308 max memory_allocated 53574.677734375 
[2025-03-17 22:25:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 11 loss:0.26594823598861694 norm:0.00023276626598089933 max memory_allocated 53574.677734375 
[2025-03-17 22:26:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 12 loss:0.2655527889728546 norm:0.00023330732074100524 max memory_allocated 53574.677734375 
[2025-03-17 22:27:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 13 loss:0.265178918838501 norm:0.0002278071187902242 max memory_allocated 53574.677734375 
[2025-03-17 22:29:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 14 loss:0.2648908197879791 norm:0.000240542198298499 max memory_allocated 53574.677734375 
[2025-03-17 22:30:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 15 loss:0.2645871043205261 norm:0.00023779649927746505 max memory_allocated 53574.677734375 
[2025-03-17 22:31:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 16 loss:0.2643032670021057 norm:0.00022594004985876381 max memory_allocated 53574.677734375 
[2025-03-17 22:33:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 17 loss:0.2640734016895294 norm:0.00021736155031248927 max memory_allocated 53574.677734375 
[2025-03-17 22:34:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 18 loss:0.2638636529445648 norm:0.0002159835130441934 max memory_allocated 53574.677734375 
[2025-03-17 22:35:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [30, 31]) iter 19 loss:0.26363781094551086 norm:0.00022309445193968713 max memory_allocated 53574.677734375 
[2025-03-17 22:37:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 18, block: [30, 31]
[2025-03-17 22:37:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 19 with layers [32, 33] ===
[2025-03-17 22:39:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 0 loss:0.40524694323539734 norm:0.0008545634336769581 max memory_allocated 53574.677734375 
[2025-03-17 22:40:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 1 loss:0.36947929859161377 norm:0.0006846031174063683 max memory_allocated 53574.677734375 
[2025-03-17 22:41:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 2 loss:0.3350396156311035 norm:0.00042942154686897993 max memory_allocated 53574.677734375 
[2025-03-17 22:43:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 3 loss:0.32593703269958496 norm:0.0003108600794803351 max memory_allocated 53574.677734375 
[2025-03-17 22:44:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 4 loss:0.32354336977005005 norm:0.00028492003912106156 max memory_allocated 53574.677734375 
[2025-03-17 22:45:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 5 loss:0.32236483693122864 norm:0.000282662600511685 max memory_allocated 53574.677734375 
[2025-03-17 22:47:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 6 loss:0.3214416801929474 norm:0.000265660259174183 max memory_allocated 53574.677734375 
[2025-03-17 22:48:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 7 loss:0.32070380449295044 norm:0.0002639055601321161 max memory_allocated 53574.677734375 
[2025-03-17 22:49:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 8 loss:0.32004520297050476 norm:0.00026411289582028985 max memory_allocated 53574.677734375 
[2025-03-17 22:51:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 9 loss:0.31943827867507935 norm:0.0002668215020094067 max memory_allocated 53574.677734375 
[2025-03-17 22:52:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 10 loss:0.31892257928848267 norm:0.00026004345272667706 max memory_allocated 53574.677734375 
[2025-03-17 22:53:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 11 loss:0.3184430003166199 norm:0.00025607470888644457 max memory_allocated 53574.677734375 
[2025-03-17 22:55:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 12 loss:0.31805190443992615 norm:0.0002532446233090013 max memory_allocated 53574.677734375 
[2025-03-17 22:56:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 13 loss:0.31768015027046204 norm:0.00024841673439368606 max memory_allocated 53574.677734375 
[2025-03-17 22:57:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 14 loss:0.3173693120479584 norm:0.00025594583712518215 max memory_allocated 53574.677734375 
[2025-03-17 22:59:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 15 loss:0.31704258918762207 norm:0.0002491901395842433 max memory_allocated 53574.677734375 
[2025-03-17 23:00:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 16 loss:0.31674742698669434 norm:0.0002461305120959878 max memory_allocated 53574.677734375 
[2025-03-17 23:01:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 17 loss:0.3165102005004883 norm:0.00024700877838768065 max memory_allocated 53574.677734375 
[2025-03-17 23:03:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 18 loss:0.31625592708587646 norm:0.00024205821682699025 max memory_allocated 53574.677734375 
[2025-03-17 23:04:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [32, 33]) iter 19 loss:0.3160255253314972 norm:0.00024486990878358483 max memory_allocated 53574.677734375 
[2025-03-17 23:06:25 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 19, block: [32, 33]
[2025-03-17 23:06:25 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 20 with layers [34] ===
[2025-03-17 23:07:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 0 loss:0.3902134597301483 norm:0.0007263862644322217 max memory_allocated 53574.677734375 
[2025-03-17 23:07:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 1 loss:0.3692508339881897 norm:0.0004331252130214125 max memory_allocated 53574.677734375 
[2025-03-17 23:08:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 2 loss:0.3492782413959503 norm:0.0002352447045268491 max memory_allocated 53574.677734375 
[2025-03-17 23:09:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 3 loss:0.34459421038627625 norm:0.00020918893278576434 max memory_allocated 53574.677734375 
[2025-03-17 23:09:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 4 loss:0.34356313943862915 norm:0.00020295596914365888 max memory_allocated 53574.677734375 
[2025-03-17 23:10:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 5 loss:0.34302207827568054 norm:0.00019898844766430557 max memory_allocated 53574.677734375 
[2025-03-17 23:11:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 6 loss:0.3425178825855255 norm:0.0001906798133859411 max memory_allocated 53574.677734375 
[2025-03-17 23:11:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 7 loss:0.34209805727005005 norm:0.00018908813945017755 max memory_allocated 53574.677734375 
[2025-03-17 23:12:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 8 loss:0.34176963567733765 norm:0.00018439565610606223 max memory_allocated 53574.677734375 
[2025-03-17 23:13:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 9 loss:0.34146296977996826 norm:0.0001829174580052495 max memory_allocated 53574.677734375 
[2025-03-17 23:13:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 10 loss:0.34123843908309937 norm:0.00018708000425249338 max memory_allocated 53574.677734375 
[2025-03-17 23:14:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 11 loss:0.34102919697761536 norm:0.00018705827824305743 max memory_allocated 53574.677734375 
[2025-03-17 23:15:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 12 loss:0.34069544076919556 norm:0.00018568726954981685 max memory_allocated 53574.677734375 
[2025-03-17 23:15:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 13 loss:0.34049901366233826 norm:0.0001865696394816041 max memory_allocated 53574.677734375 
[2025-03-17 23:16:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 14 loss:0.34032994508743286 norm:0.00018183242355007678 max memory_allocated 53574.677734375 
[2025-03-17 23:17:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 15 loss:0.3402162194252014 norm:0.00017914382624439895 max memory_allocated 53574.677734375 
[2025-03-17 23:17:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 16 loss:0.34005099534988403 norm:0.00017749918333720416 max memory_allocated 53574.677734375 
[2025-03-17 23:18:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 17 loss:0.3398857116699219 norm:0.00017323368228971958 max memory_allocated 53574.677734375 
[2025-03-17 23:19:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 18 loss:0.3397701382637024 norm:0.0001735883706714958 max memory_allocated 53574.677734375 
[2025-03-17 23:19:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [34]) iter 19 loss:0.3397078514099121 norm:0.00017521608970128 max memory_allocated 53574.677734375 
[2025-03-17 23:20:51 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 20, block: [34]
[2025-03-17 23:20:52 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 21 with layers [35] ===
[2025-03-17 23:21:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 0 loss:0.4276869297027588 norm:0.0007936700130812824 max memory_allocated 53574.677734375 
[2025-03-17 23:22:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 1 loss:0.40524551272392273 norm:0.0004887107061222196 max memory_allocated 53574.677734375 
[2025-03-17 23:22:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 2 loss:0.3844242990016937 norm:0.0002860262175090611 max memory_allocated 53574.677734375 
[2025-03-17 23:23:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 3 loss:0.379809707403183 norm:0.00025904556969180703 max memory_allocated 53574.677734375 
[2025-03-17 23:24:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 4 loss:0.3787243068218231 norm:0.00025044550420716405 max memory_allocated 53574.677734375 
[2025-03-17 23:24:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 5 loss:0.37802571058273315 norm:0.00024091103114187717 max memory_allocated 53574.677734375 
[2025-03-17 23:25:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 6 loss:0.37745893001556396 norm:0.0002317910548299551 max memory_allocated 53574.677734375 
[2025-03-17 23:26:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 7 loss:0.3769574761390686 norm:0.00022046694357413799 max memory_allocated 53574.677734375 
[2025-03-17 23:26:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 8 loss:0.3765404224395752 norm:0.00021915772231295705 max memory_allocated 53574.677734375 
[2025-03-17 23:27:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 9 loss:0.3761829435825348 norm:0.00021633502910844982 max memory_allocated 53574.677734375 
[2025-03-17 23:28:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 10 loss:0.3758326768875122 norm:0.00020726486400235444 max memory_allocated 53574.677734375 
[2025-03-17 23:29:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 11 loss:0.37555521726608276 norm:0.00020571943605318666 max memory_allocated 53574.677734375 
[2025-03-17 23:29:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 12 loss:0.37529560923576355 norm:0.0002037710655713454 max memory_allocated 53574.677734375 
[2025-03-17 23:30:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 13 loss:0.37506985664367676 norm:0.00020301685435697436 max memory_allocated 53574.677734375 
[2025-03-17 23:31:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 14 loss:0.37484508752822876 norm:0.0001969201839528978 max memory_allocated 53574.677734375 
[2025-03-17 23:31:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 15 loss:0.3746725022792816 norm:0.00018892099615186453 max memory_allocated 53574.677734375 
[2025-03-17 23:32:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 16 loss:0.3744867146015167 norm:0.0001877721369965002 max memory_allocated 53574.677734375 
[2025-03-17 23:33:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 17 loss:0.37433671951293945 norm:0.00018744674162007868 max memory_allocated 53574.677734375 
[2025-03-17 23:33:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 18 loss:0.3742069602012634 norm:0.00018808164168149233 max memory_allocated 53574.677734375 
[2025-03-17 23:34:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [35]) iter 19 loss:0.3740631341934204 norm:0.00018550685490481555 max memory_allocated 53574.677734375 
[2025-03-17 23:35:18 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 21, block: [35]
[2025-03-17 23:35:18 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 22 with layers [36] ===
[2025-03-17 23:35:18 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 23:36:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 0 loss:0.47416427731513977 norm:0.013710560277104378 max memory_allocated 53574.677734375 
[2025-03-17 23:36:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 1 loss:0.448039174079895 norm:0.010470755398273468 max memory_allocated 53574.677734375 
[2025-03-17 23:37:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 2 loss:0.4239720106124878 norm:0.007736222818493843 max memory_allocated 53574.677734375 
[2025-03-17 23:38:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 3 loss:0.41905614733695984 norm:0.006479721050709486 max memory_allocated 53574.677734375 
[2025-03-17 23:38:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 4 loss:0.417612761259079 norm:0.005286606959998608 max memory_allocated 53574.677734375 
[2025-03-17 23:39:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 5 loss:0.41658490896224976 norm:0.004334401339292526 max memory_allocated 53574.677734375 
[2025-03-17 23:40:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 6 loss:0.4159207344055176 norm:0.003930442500859499 max memory_allocated 53574.677734375 
[2025-03-17 23:40:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 7 loss:0.415424644947052 norm:0.0039052371867001057 max memory_allocated 53574.677734375 
[2025-03-17 23:41:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 8 loss:0.41497161984443665 norm:0.0037109870463609695 max memory_allocated 53574.677734375 
[2025-03-17 23:42:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 9 loss:0.4146426320075989 norm:0.0037372817751020193 max memory_allocated 53574.677734375 
[2025-03-17 23:42:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 10 loss:0.41423386335372925 norm:0.0035509555600583553 max memory_allocated 53574.677734375 
[2025-03-17 23:43:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 11 loss:0.413891464471817 norm:0.0033581960014998913 max memory_allocated 53574.677734375 
[2025-03-17 23:44:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 12 loss:0.4135191738605499 norm:0.003260984318330884 max memory_allocated 53574.677734375 
[2025-03-17 23:44:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 13 loss:0.41324782371520996 norm:0.0031332410871982574 max memory_allocated 53574.677734375 
[2025-03-17 23:45:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 14 loss:0.41295310854911804 norm:0.0030607073567807674 max memory_allocated 53574.677734375 
[2025-03-17 23:46:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 15 loss:0.4127331078052521 norm:0.0029749118257313967 max memory_allocated 53574.677734375 
[2025-03-17 23:46:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 16 loss:0.4126058518886566 norm:0.0029772422276437283 max memory_allocated 53574.677734375 
[2025-03-17 23:47:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 17 loss:0.41242802143096924 norm:0.0029491987079381943 max memory_allocated 53574.677734375 
[2025-03-17 23:48:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 18 loss:0.4123050570487976 norm:0.002882355358451605 max memory_allocated 53574.677734375 
[2025-03-17 23:48:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [36]) iter 19 loss:0.41214725375175476 norm:0.0028455653227865696 max memory_allocated 53574.677734375 
[2025-03-17 23:49:49 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 22, block: [36]
[2025-03-17 23:49:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 23 with layers [37] ===
[2025-03-17 23:49:50 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 23:50:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 0 loss:0.5460407137870789 norm:0.020353220403194427 max memory_allocated 53574.677734375 
[2025-03-17 23:51:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 1 loss:0.5095992088317871 norm:0.014153117313981056 max memory_allocated 53574.677734375 
[2025-03-17 23:51:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 2 loss:0.478381872177124 norm:0.009934871457517147 max memory_allocated 53574.677734375 
[2025-03-17 23:52:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 3 loss:0.4719356894493103 norm:0.008246413432061672 max memory_allocated 53574.677734375 
[2025-03-17 23:53:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 4 loss:0.47004997730255127 norm:0.006942249834537506 max memory_allocated 53574.677734375 
[2025-03-17 23:53:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 5 loss:0.4686559736728668 norm:0.00584071222692728 max memory_allocated 53574.677734375 
[2025-03-17 23:54:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 6 loss:0.4676682949066162 norm:0.004903505090624094 max memory_allocated 53574.677734375 
[2025-03-17 23:55:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 7 loss:0.4669394791126251 norm:0.004501616582274437 max memory_allocated 53574.677734375 
[2025-03-17 23:55:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 8 loss:0.4664709270000458 norm:0.004639816004782915 max memory_allocated 53574.677734375 
[2025-03-17 23:56:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 9 loss:0.4659360945224762 norm:0.004504873882979155 max memory_allocated 53574.677734375 
[2025-03-17 23:57:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 10 loss:0.465400755405426 norm:0.004129224456846714 max memory_allocated 53574.677734375 
[2025-03-17 23:58:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 11 loss:0.4649479389190674 norm:0.003836010815575719 max memory_allocated 53574.677734375 
[2025-03-17 23:58:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 12 loss:0.46469393372535706 norm:0.003973638638854027 max memory_allocated 53574.677734375 
[2025-03-17 23:59:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 13 loss:0.4645787179470062 norm:0.003948081284761429 max memory_allocated 53574.677734375 
[2025-03-18 00:00:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 14 loss:0.464294970035553 norm:0.0038797948509454727 max memory_allocated 53574.677734375 
[2025-03-18 00:00:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 15 loss:0.46391013264656067 norm:0.0036742831580340862 max memory_allocated 53574.677734375 
[2025-03-18 00:01:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 16 loss:0.46379294991493225 norm:0.00366117712110281 max memory_allocated 53574.677734375 
[2025-03-18 00:02:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 17 loss:0.4635488986968994 norm:0.003709631273522973 max memory_allocated 53574.677734375 
[2025-03-18 00:02:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 18 loss:0.46327266097068787 norm:0.0035726476926356554 max memory_allocated 53574.677734375 
[2025-03-18 00:03:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [37]) iter 19 loss:0.4629664421081543 norm:0.00334439380094409 max memory_allocated 53574.677734375 
[2025-03-18 00:04:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 23, block: [37]
[2025-03-18 00:04:20 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 24 with layers [38] ===
[2025-03-18 00:04:20 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-18 00:05:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 0 loss:0.657733678817749 norm:0.03297491744160652 max memory_allocated 53574.677734375 
[2025-03-18 00:05:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 1 loss:0.611083984375 norm:0.022438788786530495 max memory_allocated 53574.677734375 
[2025-03-18 00:06:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 2 loss:0.5761222839355469 norm:0.015498585999011993 max memory_allocated 53574.677734375 
[2025-03-18 00:07:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 3 loss:0.5675163269042969 norm:0.013236969709396362 max memory_allocated 53574.677734375 
[2025-03-18 00:07:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 4 loss:0.5646185874938965 norm:0.011816581711173058 max memory_allocated 53574.677734375 
[2025-03-18 00:08:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 5 loss:0.5625004172325134 norm:0.010279659181833267 max memory_allocated 53574.677734375 
[2025-03-18 00:09:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 6 loss:0.5606029629707336 norm:0.008940822444856167 max memory_allocated 53574.677734375 
[2025-03-18 00:09:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 7 loss:0.5593149662017822 norm:0.008074180223047733 max memory_allocated 53574.677734375 
[2025-03-18 00:10:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 8 loss:0.5583258867263794 norm:0.007398632820695639 max memory_allocated 53574.677734375 
[2025-03-18 00:11:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 9 loss:0.5578346252441406 norm:0.007447626907378435 max memory_allocated 53574.677734375 
[2025-03-18 00:11:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 10 loss:0.5573309063911438 norm:0.007425995077937841 max memory_allocated 53574.677734375 
[2025-03-18 00:12:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 11 loss:0.5565751791000366 norm:0.006826236844062805 max memory_allocated 53574.677734375 
[2025-03-18 00:13:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 12 loss:0.5558002591133118 norm:0.006229689344763756 max memory_allocated 53574.677734375 
[2025-03-18 00:13:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 13 loss:0.555301308631897 norm:0.006156985182315111 max memory_allocated 53574.677734375 
[2025-03-18 00:14:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 14 loss:0.5549136400222778 norm:0.005978680215775967 max memory_allocated 53574.677734375 
[2025-03-18 00:15:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 15 loss:0.5545406341552734 norm:0.005785797722637653 max memory_allocated 53574.677734375 
[2025-03-18 00:15:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 16 loss:0.5544130206108093 norm:0.005866790656000376 max memory_allocated 53574.677734375 
[2025-03-18 00:16:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 17 loss:0.554348349571228 norm:0.0060480497777462006 max memory_allocated 53574.677734375 
[2025-03-18 00:17:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 18 loss:0.5543521046638489 norm:0.006100448314100504 max memory_allocated 53574.677734375 
[2025-03-18 00:17:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 24 (layers [38]) iter 19 loss:0.5539325475692749 norm:0.005671332124620676 max memory_allocated 53574.677734375 
[2025-03-18 00:18:50 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 24, block: [38]
[2025-03-18 00:18:50 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 25 with layers [39] ===
[2025-03-18 00:18:50 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-18 00:19:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 0 loss:1.0541062355041504 norm:0.09728644788265228 max memory_allocated 53574.677734375 
[2025-03-18 00:20:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 1 loss:0.9591954946517944 norm:0.06475852429866791 max memory_allocated 53574.677734375 
[2025-03-18 00:20:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 2 loss:0.8853137493133545 norm:0.037744250148534775 max memory_allocated 53574.677734375 
[2025-03-18 00:21:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 3 loss:0.8668674230575562 norm:0.03396878391504288 max memory_allocated 53574.677734375 
[2025-03-18 00:22:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 4 loss:0.8588975071907043 norm:0.03085065260529518 max memory_allocated 53574.677734375 
[2025-03-18 00:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 5 loss:0.8530123233795166 norm:0.0280134454369545 max memory_allocated 53574.677734375 
[2025-03-18 00:23:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 6 loss:0.849347710609436 norm:0.025823500007390976 max memory_allocated 53574.677734375 
[2025-03-18 00:24:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 7 loss:0.8460726737976074 norm:0.025893684476614 max memory_allocated 53574.677734375 
[2025-03-18 00:24:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 8 loss:0.8439000248908997 norm:0.0253886878490448 max memory_allocated 53574.677734375 
[2025-03-18 00:25:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 9 loss:0.8419273495674133 norm:0.024772271513938904 max memory_allocated 53574.677734375 
[2025-03-18 00:26:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 10 loss:0.8414725065231323 norm:0.022809144109487534 max memory_allocated 53574.677734375 
[2025-03-18 00:27:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 11 loss:0.8395355343818665 norm:0.02220461145043373 max memory_allocated 53574.677734375 
[2025-03-18 00:27:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 12 loss:0.8387011885643005 norm:0.022067900747060776 max memory_allocated 53574.677734375 
[2025-03-18 00:28:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 13 loss:0.8372059464454651 norm:0.021823888644576073 max memory_allocated 53574.677734375 
[2025-03-18 00:29:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 14 loss:0.8361043334007263 norm:0.021891215816140175 max memory_allocated 53574.677734375 
[2025-03-18 00:29:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 15 loss:0.8349990844726562 norm:0.021960286423563957 max memory_allocated 53574.677734375 
[2025-03-18 00:30:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 16 loss:0.8333225250244141 norm:0.0204699095338583 max memory_allocated 53574.677734375 
[2025-03-18 00:31:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 17 loss:0.8322331309318542 norm:0.019596880301833153 max memory_allocated 53574.677734375 
[2025-03-18 00:31:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 18 loss:0.8315646052360535 norm:0.019239600747823715 max memory_allocated 53574.677734375 
[2025-03-18 00:32:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 25 (layers [39]) iter 19 loss:0.8314506411552429 norm:0.019380461424589157 max memory_allocated 53574.677734375 
[2025-03-18 00:33:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 25, block: [39]
[2025-03-18 00:33:19 root] (main_calib_config3_attn.py 379): INFO 34545.583627939224
[2025-03-18 00:33:30 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_wikitext2_all.cache
[2025-03-18 00:34:46 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.192934989929199
[2025-03-18 00:34:46 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_llama_c4_all.cache
[2025-03-18 00:36:44 root] (main_calib_config3_attn.py 161): INFO c4 : 6.749094009399414
[2025-03-18 01:20:10 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.192934989929199, 'c4': 6.749094009399414, 'results': {'boolq': {'acc': 0.6737003058103975, 'acc_stderr': 0.008200385052427128}, 'winogrande': {'acc': 0.6985003946329913, 'acc_stderr': 0.012897628072546678}, 'arc_easy': {'acc': 0.7365319865319865, 'acc_stderr': 0.009039157374497711, 'acc_norm': 0.5888047138047138, 'acc_norm_stderr': 0.010096663811817685}, 'hellaswag': {'acc': 0.5845449113722366, 'acc_stderr': 0.004917931778593197, 'acc_norm': 0.7551284604660427, 'acc_norm_stderr': 0.004291321888122741}, 'arc_challenge': {'acc': 0.4300341296928328, 'acc_stderr': 0.014467631559137994, 'acc_norm': 0.4283276450511945, 'acc_norm_stderr': 0.01446049636759901}, 'piqa': {'acc': 0.7861806311207835, 'acc_stderr': 0.009565994206915602, 'acc_norm': 0.7845484221980413, 'acc_norm_stderr': 0.009592463115658114}}, 'versions': {'boolq': 1, 'winogrande': 0, 'arc_easy': 0, 'hellaswag': 0, 'arc_challenge': 0, 'piqa': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-18 01:20:10 root] (main_calib_config3_attn.py 175): INFO 43.00,73.65,67.37,58.45,78.62,69.85
