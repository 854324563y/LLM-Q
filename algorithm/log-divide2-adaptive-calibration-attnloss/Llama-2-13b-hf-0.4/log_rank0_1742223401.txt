[2025-03-17 14:56:41 root] (main_calib_config3_attn.py 283): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-divide2-adaptive-calibration-attnloss/Llama-2-13b-hf-0.4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-divide2-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.4.pkl', blocks_pkl='./log-divide2/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-17 14:56:42 root] (main_calib_config3_attn.py 350): INFO === start quantization ===
[2025-03-17 14:56:42 root] (main_calib_config3_attn.py 356): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-17 14:56:42 root] (abq_llm_calib_config3_attn.py 82): INFO Starting ...
[2025-03-17 14:56:42 root] (abq_llm_calib_config3_attn.py 89): INFO Loaded quant_map from log-divide2-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.4.pkl
[2025-03-17 14:56:42 root] (abq_llm_calib_config3_attn.py 96): INFO Loaded blocks from ./log-divide2/Llama-2-13b-hf-w4a4/Llama-2-13b-hf_blocks.pkl: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 6), (6, 8), (8, 10), (10, 12), (12, 14), (14, 16), (16, 18), (18, 20), (20, 22), (22, 24), (24, 26), (26, 28), (28, 30), (30, 32), (32, 34), (34, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-03-17 14:56:42 root] (abq_llm_calib_config3_attn.py 102): INFO Processed blocks: [[0], [1], [2], [3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21], [22, 23], [24, 25], [26, 27], [28, 29], [30, 31], [32, 33], [34, 35], [36], [37], [38], [39]]
[2025-03-17 14:56:44 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 0 with layers [0] ===
[2025-03-17 14:56:44 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 14:57:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 0 loss:0.010323721915483475 norm:0.014946574345231056 max memory_allocated 44358.7939453125 
[2025-03-17 14:58:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 1 loss:0.005699658766388893 norm:0.008258647285401821 max memory_allocated 44358.7939453125 
[2025-03-17 14:58:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 2 loss:0.003884692210704088 norm:0.005237219389528036 max memory_allocated 44358.7939453125 
[2025-03-17 14:59:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 3 loss:0.0032581498380750418 norm:0.004073811694979668 max memory_allocated 44358.7939453125 
[2025-03-17 15:00:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 4 loss:0.0030715144239366055 norm:0.0035019423812627792 max memory_allocated 44358.7939453125 
[2025-03-17 15:00:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 5 loss:0.0028920730110257864 norm:0.002865179907530546 max memory_allocated 44358.7939453125 
[2025-03-17 15:01:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 6 loss:0.002874556463211775 norm:0.002493254840373993 max memory_allocated 44358.7939453125 
[2025-03-17 15:02:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 7 loss:0.002789960475638509 norm:0.0020435629412531853 max memory_allocated 44358.7939453125 
[2025-03-17 15:02:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 8 loss:0.00268835318274796 norm:0.0018256905023008585 max memory_allocated 44358.7939453125 
[2025-03-17 15:03:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 9 loss:0.0025748922489583492 norm:0.0016920724883675575 max memory_allocated 44358.7939453125 
[2025-03-17 15:04:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 10 loss:0.0026289094239473343 norm:0.0015630859415978193 max memory_allocated 44358.7939453125 
[2025-03-17 15:04:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 11 loss:0.0025102468207478523 norm:0.0015044466126710176 max memory_allocated 44358.7939453125 
[2025-03-17 15:05:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 12 loss:0.002550604287534952 norm:0.0013970572035759687 max memory_allocated 44358.7939453125 
[2025-03-17 15:06:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 13 loss:0.0025005959905683994 norm:0.0012865593889728189 max memory_allocated 44358.7939453125 
[2025-03-17 15:06:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 14 loss:0.0025623603723943233 norm:0.0013490616111084819 max memory_allocated 44358.7939453125 
[2025-03-17 15:07:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 15 loss:0.002429389161989093 norm:0.0011444888077676296 max memory_allocated 44358.7939453125 
[2025-03-17 15:08:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 16 loss:0.002466819016262889 norm:0.0011540588457137346 max memory_allocated 44358.7939453125 
[2025-03-17 15:08:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 17 loss:0.00246571097522974 norm:0.0011418343055993319 max memory_allocated 44358.7939453125 
[2025-03-17 15:09:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 18 loss:0.002463587326928973 norm:0.0011645155027508736 max memory_allocated 44358.7939453125 
[2025-03-17 15:10:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 0 (layers [0]) iter 19 loss:0.0024297861382365227 norm:0.000997025053948164 max memory_allocated 44358.7939453125 
[2025-03-17 15:11:06 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 0, block: [0]
[2025-03-17 15:11:06 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 1 with layers [1] ===
[2025-03-17 15:11:11 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 15:11:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 0 loss:0.023174772039055824 norm:0.014597122557461262 max memory_allocated 44358.7939453125 
[2025-03-17 15:12:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 1 loss:0.01426510140299797 norm:0.008723451755940914 max memory_allocated 44358.7939453125 
[2025-03-17 15:13:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 2 loss:0.010302052833139896 norm:0.005979773588478565 max memory_allocated 44358.7939453125 
[2025-03-17 15:13:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 3 loss:0.009157965891063213 norm:0.00479549914598465 max memory_allocated 44358.7939453125 
[2025-03-17 15:14:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 4 loss:0.008694926276803017 norm:0.0040064649656414986 max memory_allocated 44358.7939453125 
[2025-03-17 15:15:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 5 loss:0.008436834439635277 norm:0.0034612405579537153 max memory_allocated 44358.7939453125 
[2025-03-17 15:15:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 6 loss:0.008240675553679466 norm:0.0030728676356375217 max memory_allocated 44358.7939453125 
[2025-03-17 15:16:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 7 loss:0.00813225843012333 norm:0.0026813477743417025 max memory_allocated 44358.7939453125 
[2025-03-17 15:17:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 8 loss:0.00799476820975542 norm:0.002302328823134303 max memory_allocated 44358.7939453125 
[2025-03-17 15:17:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 9 loss:0.007842025719583035 norm:0.001994355348870158 max memory_allocated 44358.7939453125 
[2025-03-17 15:18:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 10 loss:0.007722079288214445 norm:0.0017214968102052808 max memory_allocated 44358.7939453125 
[2025-03-17 15:19:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 11 loss:0.007632075343281031 norm:0.001562691992148757 max memory_allocated 44358.7939453125 
[2025-03-17 15:20:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 12 loss:0.0076034655794501305 norm:0.001489682705141604 max memory_allocated 44358.7939453125 
[2025-03-17 15:20:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 13 loss:0.007583592087030411 norm:0.0015626555541530252 max memory_allocated 44358.7939453125 
[2025-03-17 15:21:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 14 loss:0.007636805064976215 norm:0.0014097276143729687 max memory_allocated 44358.7939453125 
[2025-03-17 15:22:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 15 loss:0.007521842606365681 norm:0.0013956823386251926 max memory_allocated 44358.7939453125 
[2025-03-17 15:22:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 16 loss:0.007569389883428812 norm:0.0012344273272901773 max memory_allocated 44358.7939453125 
[2025-03-17 15:23:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 17 loss:0.007467172108590603 norm:0.0012915923725813627 max memory_allocated 44358.7939453125 
[2025-03-17 15:24:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 18 loss:0.00754505442455411 norm:0.0011837572092190385 max memory_allocated 44358.7939453125 
[2025-03-17 15:24:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 1 (layers [1]) iter 19 loss:0.007440377492457628 norm:0.0012108136434108019 max memory_allocated 44358.7939453125 
[2025-03-17 15:25:33 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 1, block: [1]
[2025-03-17 15:25:33 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 2 with layers [2] ===
[2025-03-17 15:25:36 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 15:26:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 0 loss:0.029852252453565598 norm:0.014195245690643787 max memory_allocated 44358.7939453125 
[2025-03-17 15:27:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 1 loss:0.02051466889679432 norm:0.009242987260222435 max memory_allocated 44358.7939453125 
[2025-03-17 15:27:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 2 loss:0.01570083387196064 norm:0.006097255274653435 max memory_allocated 44358.7939453125 
[2025-03-17 15:28:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 3 loss:0.014247966930270195 norm:0.004842390771955252 max memory_allocated 44358.7939453125 
[2025-03-17 15:29:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 4 loss:0.013643987476825714 norm:0.003970078658312559 max memory_allocated 44358.7939453125 
[2025-03-17 15:29:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 5 loss:0.013236429542303085 norm:0.00343128084205091 max memory_allocated 44358.7939453125 
[2025-03-17 15:30:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 6 loss:0.012895829044282436 norm:0.0029997588135302067 max memory_allocated 44358.7939453125 
[2025-03-17 15:31:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 7 loss:0.012623484246432781 norm:0.002647638786584139 max memory_allocated 44358.7939453125 
[2025-03-17 15:31:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 8 loss:0.01243017241358757 norm:0.002315588528290391 max memory_allocated 44358.7939453125 
[2025-03-17 15:32:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 9 loss:0.012293829582631588 norm:0.0019637099467217922 max memory_allocated 44358.7939453125 
[2025-03-17 15:33:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 10 loss:0.012182499282062054 norm:0.001689218683168292 max memory_allocated 44358.7939453125 
[2025-03-17 15:33:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 11 loss:0.012194829061627388 norm:0.0016289004124701023 max memory_allocated 44358.7939453125 
[2025-03-17 15:34:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 12 loss:0.012157576158642769 norm:0.0016141985543072224 max memory_allocated 44358.7939453125 
[2025-03-17 15:35:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 13 loss:0.012149511836469173 norm:0.0015849788906052709 max memory_allocated 44358.7939453125 
[2025-03-17 15:35:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 14 loss:0.012143515050411224 norm:0.0015529196243733168 max memory_allocated 44358.7939453125 
[2025-03-17 15:36:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 15 loss:0.012163687497377396 norm:0.0015619844198226929 max memory_allocated 44358.7939453125 
[2025-03-17 15:37:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 16 loss:0.01216251403093338 norm:0.0013411547988653183 max memory_allocated 44358.7939453125 
[2025-03-17 15:37:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 17 loss:0.012109652161598206 norm:0.0013820065651088953 max memory_allocated 44358.7939453125 
[2025-03-17 15:38:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 18 loss:0.012057051062583923 norm:0.0012276350753381848 max memory_allocated 44358.7939453125 
[2025-03-17 15:39:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 2 (layers [2]) iter 19 loss:0.012067765928804874 norm:0.0012922545429319143 max memory_allocated 44358.7939453125 
[2025-03-17 15:40:02 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 2, block: [2]
[2025-03-17 15:40:02 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 3 with layers [3] ===
[2025-03-17 15:40:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 0 loss:0.05643066391348839 norm:0.004323647357523441 max memory_allocated 44358.7939453125 
[2025-03-17 15:41:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 1 loss:0.042225662618875504 norm:0.011611750349402428 max memory_allocated 44358.7939453125 
[2025-03-17 15:42:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 2 loss:0.033886346966028214 norm:0.016783053055405617 max memory_allocated 44358.7939453125 
[2025-03-17 15:42:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 3 loss:0.03049589693546295 norm:0.002705453662201762 max memory_allocated 44358.7939453125 
[2025-03-17 15:43:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 4 loss:0.029928267002105713 norm:0.0032741958275437355 max memory_allocated 44358.7939453125 
[2025-03-17 15:44:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 5 loss:0.029116014018654823 norm:0.003895305562764406 max memory_allocated 44358.7939453125 
[2025-03-17 15:44:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 6 loss:0.02832934632897377 norm:0.002977911150082946 max memory_allocated 44358.7939453125 
[2025-03-17 15:45:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 7 loss:0.024583496153354645 norm:0.0016717083053663373 max memory_allocated 44358.7939453125 
[2025-03-17 15:46:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 8 loss:0.025004040449857712 norm:0.0016813665861263871 max memory_allocated 44358.7939453125 
[2025-03-17 15:46:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 9 loss:0.02621910348534584 norm:0.0023348298855125904 max memory_allocated 44358.7939453125 
[2025-03-17 15:47:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 10 loss:0.024162454530596733 norm:0.001507034059613943 max memory_allocated 44358.7939453125 
[2025-03-17 15:48:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 11 loss:0.02397041581571102 norm:0.0011574350064620376 max memory_allocated 44358.7939453125 
[2025-03-17 15:48:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 12 loss:0.0271993987262249 norm:0.003085625357925892 max memory_allocated 44358.7939453125 
[2025-03-17 15:49:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 13 loss:0.025392800569534302 norm:0.0024277286138385534 max memory_allocated 44358.7939453125 
[2025-03-17 15:50:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 14 loss:0.02290666289627552 norm:0.0007642631535418332 max memory_allocated 44358.7939453125 
[2025-03-17 15:50:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 15 loss:0.022853337228298187 norm:0.0008926787413656712 max memory_allocated 44358.7939453125 
[2025-03-17 15:51:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 16 loss:0.023070858791470528 norm:0.0008643179899081588 max memory_allocated 44358.7939453125 
[2025-03-17 15:52:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 17 loss:0.023450974375009537 norm:0.0009438563138246536 max memory_allocated 44358.7939453125 
[2025-03-17 15:52:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 18 loss:0.023431718349456787 norm:0.0008533108630217612 max memory_allocated 44358.7939453125 
[2025-03-17 15:53:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 3 (layers [3]) iter 19 loss:0.023501954972743988 norm:0.0010484835365787148 max memory_allocated 44358.7939453125 
[2025-03-17 15:54:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 3, block: [3]
[2025-03-17 15:54:24 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 4 with layers [4, 5] ===
[2025-03-17 15:55:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 0 loss:0.05074898153543472 norm:0.0011755134910345078 max memory_allocated 53577.052734375 
[2025-03-17 15:57:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 1 loss:0.037914689630270004 norm:0.0013046246021986008 max memory_allocated 53577.052734375 
[2025-03-17 15:58:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 2 loss:0.0310048907995224 norm:0.0006243850803002715 max memory_allocated 53577.052734375 
[2025-03-17 15:59:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 3 loss:0.028168633580207825 norm:0.0003266241983510554 max memory_allocated 53577.052734375 
[2025-03-17 16:01:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 4 loss:0.026672538369894028 norm:0.00029120894032530487 max memory_allocated 53577.052734375 
[2025-03-17 16:02:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 5 loss:0.025686010718345642 norm:0.000321019790135324 max memory_allocated 53577.052734375 
[2025-03-17 16:03:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 6 loss:0.024973293766379356 norm:0.000253420730587095 max memory_allocated 53577.052734375 
[2025-03-17 16:05:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 7 loss:0.024482499808073044 norm:0.00024259556084871292 max memory_allocated 53577.052734375 
[2025-03-17 16:06:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 8 loss:0.024199360981583595 norm:0.00022813185933046043 max memory_allocated 53577.052734375 
[2025-03-17 16:07:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 9 loss:0.024035120382905006 norm:0.00023550985497422516 max memory_allocated 53577.052734375 
[2025-03-17 16:09:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 10 loss:0.023900186643004417 norm:0.0002552127407398075 max memory_allocated 53577.052734375 
[2025-03-17 16:10:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 11 loss:0.02381717599928379 norm:0.0002453260531183332 max memory_allocated 53577.052734375 
[2025-03-17 16:11:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 12 loss:0.023785658180713654 norm:0.00025189577718265355 max memory_allocated 53577.052734375 
[2025-03-17 16:13:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 13 loss:0.023777157068252563 norm:0.00025990992435254157 max memory_allocated 53577.052734375 
[2025-03-17 16:14:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 14 loss:0.023730793967843056 norm:0.00023992726346477866 max memory_allocated 53577.052734375 
[2025-03-17 16:15:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 15 loss:0.023715218529105186 norm:0.0002486906305421144 max memory_allocated 53577.052734375 
[2025-03-17 16:17:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 16 loss:0.023683305829763412 norm:0.00024362960539292544 max memory_allocated 53577.052734375 
[2025-03-17 16:18:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 17 loss:0.023655720055103302 norm:0.00023435996263287961 max memory_allocated 53577.052734375 
[2025-03-17 16:19:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 18 loss:0.023633936420083046 norm:0.00021336495410650969 max memory_allocated 53577.052734375 
[2025-03-17 16:21:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 4 (layers [4, 5]) iter 19 loss:0.023633817210793495 norm:0.0002478013338986784 max memory_allocated 53577.052734375 
[2025-03-17 16:23:01 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 4, block: [4, 5]
[2025-03-17 16:23:01 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 5 with layers [6, 7] ===
[2025-03-17 16:24:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 0 loss:0.066550612449646 norm:0.001447132439352572 max memory_allocated 53578.208984375 
[2025-03-17 16:25:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 1 loss:0.047745879739522934 norm:0.0005576604744419456 max memory_allocated 53578.208984375 
[2025-03-17 16:27:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 2 loss:0.03798089548945427 norm:0.00035782973282039165 max memory_allocated 53578.208984375 
[2025-03-17 16:28:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 3 loss:0.034193720668554306 norm:0.0002849099109880626 max memory_allocated 53578.208984375 
[2025-03-17 16:29:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 4 loss:0.03229110687971115 norm:0.0002688418317120522 max memory_allocated 53578.208984375 
[2025-03-17 16:31:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 5 loss:0.030906111001968384 norm:0.0002365458058193326 max memory_allocated 53578.208984375 
[2025-03-17 16:32:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 6 loss:0.03002801723778248 norm:0.0002254347928101197 max memory_allocated 53578.208984375 
[2025-03-17 16:33:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 7 loss:0.029437890276312828 norm:0.0002181025774916634 max memory_allocated 53578.208984375 
[2025-03-17 16:35:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 8 loss:0.029036495834589005 norm:0.00020190805662423372 max memory_allocated 53578.208984375 
[2025-03-17 16:36:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 9 loss:0.02874431386590004 norm:0.00020221939485054463 max memory_allocated 53578.208984375 
[2025-03-17 16:37:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 10 loss:0.028518451377749443 norm:0.00020553474314510822 max memory_allocated 53578.208984375 
[2025-03-17 16:39:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 11 loss:0.028392454609274864 norm:0.0002084056322928518 max memory_allocated 53578.208984375 
[2025-03-17 16:40:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 12 loss:0.028282226994633675 norm:0.0002063155989162624 max memory_allocated 53578.208984375 
[2025-03-17 16:41:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 13 loss:0.028159374371170998 norm:0.00020235285046510398 max memory_allocated 53578.208984375 
[2025-03-17 16:43:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 14 loss:0.028091633692383766 norm:0.00021556402498390526 max memory_allocated 53578.208984375 
[2025-03-17 16:44:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 15 loss:0.02800884284079075 norm:0.0002141885634046048 max memory_allocated 53578.208984375 
[2025-03-17 16:45:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 16 loss:0.027965236455202103 norm:0.000194401916814968 max memory_allocated 53578.208984375 
[2025-03-17 16:47:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 17 loss:0.027918176725506783 norm:0.00020690665405709296 max memory_allocated 53578.208984375 
[2025-03-17 16:48:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 18 loss:0.027860108762979507 norm:0.00020492864132393152 max memory_allocated 53578.208984375 
[2025-03-17 16:49:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 5 (layers [6, 7]) iter 19 loss:0.027819732204079628 norm:0.00019342187442816794 max memory_allocated 53578.208984375 
[2025-03-17 16:51:34 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 5, block: [6, 7]
[2025-03-17 16:51:34 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 6 with layers [8, 9] ===
[2025-03-17 16:53:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 0 loss:0.08998489379882812 norm:0.0030500832945108414 max memory_allocated 53579.365234375 
[2025-03-17 16:54:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 1 loss:0.06036756560206413 norm:0.0009870905196294188 max memory_allocated 53579.365234375 
[2025-03-17 16:55:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 2 loss:0.047477301210165024 norm:0.0005290194530971348 max memory_allocated 53579.365234375 
[2025-03-17 16:57:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 3 loss:0.042297765612602234 norm:0.00042444688733667135 max memory_allocated 53579.365234375 
[2025-03-17 16:58:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 4 loss:0.03953497111797333 norm:0.00035615506931208074 max memory_allocated 53579.365234375 
[2025-03-17 16:59:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 5 loss:0.03768928349018097 norm:0.0003239526995457709 max memory_allocated 53579.365234375 
[2025-03-17 17:01:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 6 loss:0.036417197436094284 norm:0.00032169040059670806 max memory_allocated 53579.365234375 
[2025-03-17 17:02:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 7 loss:0.03557982295751572 norm:0.00031759298872202635 max memory_allocated 53579.365234375 
[2025-03-17 17:03:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 8 loss:0.034977905452251434 norm:0.00030423240968957543 max memory_allocated 53579.365234375 
[2025-03-17 17:05:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 9 loss:0.03457414358854294 norm:0.00028850120725110173 max memory_allocated 53579.365234375 
[2025-03-17 17:06:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 10 loss:0.034223031252622604 norm:0.00027224025689065456 max memory_allocated 53579.365234375 
[2025-03-17 17:07:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 11 loss:0.03397941589355469 norm:0.0002718203468248248 max memory_allocated 53579.365234375 
[2025-03-17 17:09:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 12 loss:0.03379436954855919 norm:0.0002637334109749645 max memory_allocated 53579.365234375 
[2025-03-17 17:10:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 13 loss:0.03364448621869087 norm:0.0002633437397889793 max memory_allocated 53579.365234375 
[2025-03-17 17:11:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 14 loss:0.033532269299030304 norm:0.0002602842287160456 max memory_allocated 53579.365234375 
[2025-03-17 17:13:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 15 loss:0.03343741595745087 norm:0.0002678996534086764 max memory_allocated 53579.365234375 
[2025-03-17 17:14:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 16 loss:0.03334149718284607 norm:0.0002525200543459505 max memory_allocated 53579.365234375 
[2025-03-17 17:15:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 17 loss:0.03320512920618057 norm:0.000264623697148636 max memory_allocated 53579.365234375 
[2025-03-17 17:17:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 18 loss:0.033113256096839905 norm:0.00024082025629468262 max memory_allocated 53579.365234375 
[2025-03-17 17:18:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 6 (layers [8, 9]) iter 19 loss:0.03303809091448784 norm:0.00023525365395471454 max memory_allocated 53579.365234375 
[2025-03-17 17:20:10 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 6, block: [8, 9]
[2025-03-17 17:20:10 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 7 with layers [10, 11] ===
[2025-03-17 17:21:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 0 loss:0.08122293651103973 norm:0.0014822200173512101 max memory_allocated 53579.365234375 
[2025-03-17 17:22:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 1 loss:0.061071086674928665 norm:0.0007287666667252779 max memory_allocated 53579.365234375 
[2025-03-17 17:24:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 2 loss:0.04950445890426636 norm:0.00045033253263682127 max memory_allocated 53579.365234375 
[2025-03-17 17:25:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 3 loss:0.04460606724023819 norm:0.00032467543496750295 max memory_allocated 53579.365234375 
[2025-03-17 17:26:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 4 loss:0.042226795107126236 norm:0.0002700554032344371 max memory_allocated 53579.365234375 
[2025-03-17 17:28:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 5 loss:0.04077249765396118 norm:0.00024304045655298978 max memory_allocated 53579.365234375 
[2025-03-17 17:29:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 6 loss:0.03986753523349762 norm:0.0002262771304231137 max memory_allocated 53579.365234375 
[2025-03-17 17:30:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 7 loss:0.03924727067351341 norm:0.00021236634347587824 max memory_allocated 53579.365234375 
[2025-03-17 17:32:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 8 loss:0.03884554281830788 norm:0.00019978440832346678 max memory_allocated 53579.365234375 
[2025-03-17 17:33:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 9 loss:0.03858392685651779 norm:0.00019331877410877496 max memory_allocated 53579.365234375 
[2025-03-17 17:34:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 10 loss:0.03841213136911392 norm:0.00019070271810051054 max memory_allocated 53579.365234375 
[2025-03-17 17:36:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 11 loss:0.03825637325644493 norm:0.00018513777467887849 max memory_allocated 53579.365234375 
[2025-03-17 17:37:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 12 loss:0.0381058007478714 norm:0.00017876124184112996 max memory_allocated 53579.365234375 
[2025-03-17 17:38:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 13 loss:0.03805907815694809 norm:0.00017758026660885662 max memory_allocated 53579.365234375 
[2025-03-17 17:40:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 14 loss:0.03796977177262306 norm:0.00017555872909724712 max memory_allocated 53579.365234375 
[2025-03-17 17:41:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 15 loss:0.03784430772066116 norm:0.00016588906873948872 max memory_allocated 53579.365234375 
[2025-03-17 17:42:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 16 loss:0.03778046369552612 norm:0.0001645148586248979 max memory_allocated 53579.365234375 
[2025-03-17 17:44:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 17 loss:0.03773360699415207 norm:0.00016405663336627185 max memory_allocated 53579.365234375 
[2025-03-17 17:45:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 18 loss:0.037676408886909485 norm:0.0001612515188753605 max memory_allocated 53579.365234375 
[2025-03-17 17:46:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 7 (layers [10, 11]) iter 19 loss:0.03762379288673401 norm:0.0001593316119397059 max memory_allocated 53579.365234375 
[2025-03-17 17:48:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 7, block: [10, 11]
[2025-03-17 17:48:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 8 with layers [12, 13] ===
[2025-03-17 17:50:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 0 loss:0.08675195276737213 norm:0.0016473073046654463 max memory_allocated 53580.677734375 
[2025-03-17 17:51:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 1 loss:0.06537923961877823 norm:0.0007462531793862581 max memory_allocated 53580.677734375 
[2025-03-17 17:52:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 2 loss:0.053425587713718414 norm:0.0004560010856948793 max memory_allocated 53580.677734375 
[2025-03-17 17:54:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 3 loss:0.048343390226364136 norm:0.00033272794098593295 max memory_allocated 53580.677734375 
[2025-03-17 17:55:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 4 loss:0.04578638821840286 norm:0.00027481449069455266 max memory_allocated 53580.677734375 
[2025-03-17 17:56:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 5 loss:0.044295765459537506 norm:0.00024815177312120795 max memory_allocated 53580.677734375 
[2025-03-17 17:58:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 6 loss:0.04324904829263687 norm:0.0002292308781761676 max memory_allocated 53580.677734375 
[2025-03-17 17:59:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 7 loss:0.042608361691236496 norm:0.00021634539007209241 max memory_allocated 53580.677734375 
[2025-03-17 18:00:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 8 loss:0.04213114082813263 norm:0.00020658809808082879 max memory_allocated 53580.677734375 
[2025-03-17 18:02:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 9 loss:0.04179421439766884 norm:0.00020160491112619638 max memory_allocated 53580.677734375 
[2025-03-17 18:03:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 10 loss:0.041582077741622925 norm:0.00019580867956392467 max memory_allocated 53580.677734375 
[2025-03-17 18:04:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 11 loss:0.04140030965209007 norm:0.00018462758453097194 max memory_allocated 53580.677734375 
[2025-03-17 18:06:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 12 loss:0.04122121259570122 norm:0.00017477398796472698 max memory_allocated 53580.677734375 
[2025-03-17 18:07:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 13 loss:0.041110385209321976 norm:0.00017106838640756905 max memory_allocated 53580.677734375 
[2025-03-17 18:08:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 14 loss:0.04100118577480316 norm:0.0001661319547565654 max memory_allocated 53580.677734375 
[2025-03-17 18:10:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 15 loss:0.04090185835957527 norm:0.00015950517263263464 max memory_allocated 53580.677734375 
[2025-03-17 18:11:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 16 loss:0.04082881659269333 norm:0.00015857029939070344 max memory_allocated 53580.677734375 
[2025-03-17 18:12:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 17 loss:0.040739357471466064 norm:0.00015289556176867336 max memory_allocated 53580.677734375 
[2025-03-17 18:14:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 18 loss:0.04068998247385025 norm:0.0001523591490695253 max memory_allocated 53580.677734375 
[2025-03-17 18:15:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 8 (layers [12, 13]) iter 19 loss:0.04062823951244354 norm:0.00015155474829953164 max memory_allocated 53580.677734375 
[2025-03-17 18:17:14 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 8, block: [12, 13]
[2025-03-17 18:17:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 9 with layers [14, 15] ===
[2025-03-17 18:18:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 0 loss:0.08418329060077667 norm:0.0014303050702437758 max memory_allocated 53580.677734375 
[2025-03-17 18:20:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 1 loss:0.06619983911514282 norm:0.0006789934705011547 max memory_allocated 53580.677734375 
[2025-03-17 18:21:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 2 loss:0.05491311475634575 norm:0.0004157262446824461 max memory_allocated 53580.677734375 
[2025-03-17 18:22:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 3 loss:0.04999319836497307 norm:0.00029490486485883594 max memory_allocated 53580.677734375 
[2025-03-17 18:24:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 4 loss:0.047702062875032425 norm:0.0002408449217909947 max memory_allocated 53580.677734375 
[2025-03-17 18:25:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 5 loss:0.04630717262625694 norm:0.0002146186016034335 max memory_allocated 53580.677734375 
[2025-03-17 18:26:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 6 loss:0.045365117490291595 norm:0.0002032141201198101 max memory_allocated 53580.677734375 
[2025-03-17 18:28:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 7 loss:0.044736284762620926 norm:0.00019115154282189906 max memory_allocated 53580.677734375 
[2025-03-17 18:29:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 8 loss:0.04429495334625244 norm:0.00018415974045637995 max memory_allocated 53580.677734375 
[2025-03-17 18:30:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 9 loss:0.044000718742609024 norm:0.00017690809909254313 max memory_allocated 53580.677734375 
[2025-03-17 18:32:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 10 loss:0.04378719627857208 norm:0.00017729407409206033 max memory_allocated 53580.677734375 
[2025-03-17 18:33:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 11 loss:0.04359275847673416 norm:0.00017419541836716235 max memory_allocated 53580.677734375 
[2025-03-17 18:34:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 12 loss:0.04341965168714523 norm:0.000165462406584993 max memory_allocated 53580.677734375 
[2025-03-17 18:36:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 13 loss:0.043294914066791534 norm:0.0001613517670193687 max memory_allocated 53580.677734375 
[2025-03-17 18:37:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 14 loss:0.043223388493061066 norm:0.00016114447498694062 max memory_allocated 53580.677734375 
[2025-03-17 18:38:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 15 loss:0.043146368116140366 norm:0.00015334525960497558 max memory_allocated 53580.677734375 
[2025-03-17 18:40:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 16 loss:0.04306105524301529 norm:0.00014936465595383197 max memory_allocated 53580.677734375 
[2025-03-17 18:41:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 17 loss:0.0429740771651268 norm:0.00014541625569108874 max memory_allocated 53580.677734375 
[2025-03-17 18:42:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 18 loss:0.04292913153767586 norm:0.00015187484677881002 max memory_allocated 53580.677734375 
[2025-03-17 18:44:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 9 (layers [14, 15]) iter 19 loss:0.042889513075351715 norm:0.00014885528071317822 max memory_allocated 53580.677734375 
[2025-03-17 18:45:45 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 9, block: [14, 15]
[2025-03-17 18:45:45 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 10 with layers [16, 17] ===
[2025-03-17 18:47:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 0 loss:0.08206147700548172 norm:0.001365921925753355 max memory_allocated 53580.677734375 
[2025-03-17 18:48:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 1 loss:0.06577485799789429 norm:0.0006021622102707624 max memory_allocated 53580.677734375 
[2025-03-17 18:49:52 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 2 loss:0.05527225136756897 norm:0.0003750715986825526 max memory_allocated 53580.677734375 
[2025-03-17 18:51:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 3 loss:0.051026128232479095 norm:0.0002699904434848577 max memory_allocated 53580.677734375 
[2025-03-17 18:52:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 4 loss:0.048902545124292374 norm:0.00022735039237886667 max memory_allocated 53580.677734375 
[2025-03-17 18:53:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 5 loss:0.04754824563860893 norm:0.0002081261045532301 max memory_allocated 53580.677734375 
[2025-03-17 18:55:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 6 loss:0.046616535633802414 norm:0.00019555700419005007 max memory_allocated 53580.677734375 
[2025-03-17 18:56:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 7 loss:0.045946259051561356 norm:0.00018262000230606645 max memory_allocated 53580.677734375 
[2025-03-17 18:57:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 8 loss:0.045529257506132126 norm:0.00017785212548915297 max memory_allocated 53580.677734375 
[2025-03-17 18:59:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 9 loss:0.04518365487456322 norm:0.0001666869648033753 max memory_allocated 53580.677734375 
[2025-03-17 19:00:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 10 loss:0.044934116303920746 norm:0.00015864057058934122 max memory_allocated 53580.677734375 
[2025-03-17 19:01:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 11 loss:0.044767528772354126 norm:0.0001552742614876479 max memory_allocated 53580.677734375 
[2025-03-17 19:03:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 12 loss:0.04458284750580788 norm:0.0001494635798735544 max memory_allocated 53580.677734375 
[2025-03-17 19:04:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 13 loss:0.044457610696554184 norm:0.00014665153867099434 max memory_allocated 53580.677734375 
[2025-03-17 19:05:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 14 loss:0.044328443706035614 norm:0.000142456847243011 max memory_allocated 53580.677734375 
[2025-03-17 19:07:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 15 loss:0.04422829672694206 norm:0.00013609147572424263 max memory_allocated 53580.677734375 
[2025-03-17 19:08:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 16 loss:0.044122014194726944 norm:0.00012966450594831258 max memory_allocated 53580.677734375 
[2025-03-17 19:09:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 17 loss:0.04405466839671135 norm:0.00012636282190214843 max memory_allocated 53580.677734375 
[2025-03-17 19:11:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 18 loss:0.04399530589580536 norm:0.0001228552428074181 max memory_allocated 53580.677734375 
[2025-03-17 19:12:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 10 (layers [16, 17]) iter 19 loss:0.043919965624809265 norm:0.0001184297725558281 max memory_allocated 53580.677734375 
[2025-03-17 19:14:26 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 10, block: [16, 17]
[2025-03-17 19:14:27 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 11 with layers [18, 19] ===
[2025-03-17 19:15:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 0 loss:0.07518789172172546 norm:0.0008781966171227396 max memory_allocated 53580.677734375 
[2025-03-17 19:17:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 1 loss:0.06334573030471802 norm:0.000392869085771963 max memory_allocated 53580.677734375 
[2025-03-17 19:18:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 2 loss:0.054924219846725464 norm:0.0002607680798973888 max memory_allocated 53580.677734375 
[2025-03-17 19:19:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 3 loss:0.05197093263268471 norm:0.00020677533757407218 max memory_allocated 53580.677734375 
[2025-03-17 19:21:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 4 loss:0.05017223581671715 norm:0.00017685556667856872 max memory_allocated 53580.677734375 
[2025-03-17 19:22:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 5 loss:0.04893884062767029 norm:0.00016421815962530673 max memory_allocated 53580.677734375 
[2025-03-17 19:23:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 6 loss:0.04809444397687912 norm:0.00015581140178255737 max memory_allocated 53580.677734375 
[2025-03-17 19:25:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 7 loss:0.04754652827978134 norm:0.00014472230395767838 max memory_allocated 53580.677734375 
[2025-03-17 19:26:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 8 loss:0.04721750691533089 norm:0.0001391688419971615 max memory_allocated 53580.677734375 
[2025-03-17 19:27:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 9 loss:0.04697597026824951 norm:0.00013144867261871696 max memory_allocated 53580.677734375 
[2025-03-17 19:29:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 10 loss:0.04680052399635315 norm:0.0001249180204467848 max memory_allocated 53580.677734375 
[2025-03-17 19:30:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 11 loss:0.04663398116827011 norm:0.00011995578825008124 max memory_allocated 53580.677734375 
[2025-03-17 19:31:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 12 loss:0.04650885984301567 norm:0.00011693855776684359 max memory_allocated 53580.677734375 
[2025-03-17 19:33:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 13 loss:0.04639628157019615 norm:0.00011335332237649709 max memory_allocated 53580.677734375 
[2025-03-17 19:34:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 14 loss:0.04632808268070221 norm:0.00011102095595560968 max memory_allocated 53580.677734375 
[2025-03-17 19:35:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 15 loss:0.04625334590673447 norm:0.0001081311929738149 max memory_allocated 53580.677734375 
[2025-03-17 19:37:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 16 loss:0.04617413878440857 norm:0.00010452941933181137 max memory_allocated 53580.677734375 
[2025-03-17 19:38:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 17 loss:0.04610973969101906 norm:0.00010165171988774091 max memory_allocated 53580.677734375 
[2025-03-17 19:39:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 18 loss:0.04604136198759079 norm:9.813180076889694e-05 max memory_allocated 53580.677734375 
[2025-03-17 19:41:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 11 (layers [18, 19]) iter 19 loss:0.04599883034825325 norm:9.608185064280406e-05 max memory_allocated 53580.677734375 
[2025-03-17 19:43:08 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 11, block: [18, 19]
[2025-03-17 19:43:08 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 12 with layers [20, 21] ===
[2025-03-17 19:44:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 0 loss:0.08343664556741714 norm:0.000874502002261579 max memory_allocated 53580.677734375 
[2025-03-17 19:45:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 1 loss:0.07127778977155685 norm:0.0003969262761529535 max memory_allocated 53580.677734375 
[2025-03-17 19:47:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 2 loss:0.06204364076256752 norm:0.0002713716239668429 max memory_allocated 53580.677734375 
[2025-03-17 19:48:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 3 loss:0.05882081016898155 norm:0.0002189501392422244 max memory_allocated 53580.677734375 
[2025-03-17 19:49:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 4 loss:0.05695585161447525 norm:0.00019155121117364615 max memory_allocated 53580.677734375 
[2025-03-17 19:51:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 5 loss:0.055622413754463196 norm:0.00017629821377340704 max memory_allocated 53580.677734375 
[2025-03-17 19:52:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 6 loss:0.054764535278081894 norm:0.00016633898485451937 max memory_allocated 53580.677734375 
[2025-03-17 19:53:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 7 loss:0.05420394986867905 norm:0.00015799938410054892 max memory_allocated 53580.677734375 
[2025-03-17 19:55:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 8 loss:0.053831908851861954 norm:0.00015104669728316367 max memory_allocated 53580.677734375 
[2025-03-17 19:56:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 9 loss:0.05356339365243912 norm:0.0001459526247344911 max memory_allocated 53580.677734375 
[2025-03-17 19:57:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 10 loss:0.05334868282079697 norm:0.00013873580610379577 max memory_allocated 53580.677734375 
[2025-03-17 19:59:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 11 loss:0.05319388955831528 norm:0.00013411682448349893 max memory_allocated 53580.677734375 
[2025-03-17 20:00:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 12 loss:0.053053054958581924 norm:0.000131124586914666 max memory_allocated 53580.677734375 
[2025-03-17 20:01:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 13 loss:0.052934400737285614 norm:0.00012772632180713117 max memory_allocated 53580.677734375 
[2025-03-17 20:03:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 14 loss:0.052819229662418365 norm:0.00012293571489863098 max memory_allocated 53580.677734375 
[2025-03-17 20:04:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 15 loss:0.05272736772894859 norm:0.00012030939251417294 max memory_allocated 53580.677734375 
[2025-03-17 20:05:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 16 loss:0.05261550843715668 norm:0.00011829950381070375 max memory_allocated 53580.677734375 
[2025-03-17 20:07:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 17 loss:0.05255315452814102 norm:0.00011778395128203556 max memory_allocated 53580.677734375 
[2025-03-17 20:08:37 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 18 loss:0.052499920129776 norm:0.00011331652785884216 max memory_allocated 53580.677734375 
[2025-03-17 20:09:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 12 (layers [20, 21]) iter 19 loss:0.052443161606788635 norm:0.0001099625660572201 max memory_allocated 53580.677734375 
[2025-03-17 20:11:52 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 12, block: [20, 21]
[2025-03-17 20:11:52 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 13 with layers [22, 23] ===
[2025-03-17 20:13:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 0 loss:0.08979785442352295 norm:0.0005346084362827241 max memory_allocated 53580.677734375 
[2025-03-17 20:14:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 1 loss:0.07908686995506287 norm:0.0003016719128936529 max memory_allocated 53580.677734375 
[2025-03-17 20:15:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 2 loss:0.06967554241418839 norm:0.00019816617714241147 max memory_allocated 53580.677734375 
[2025-03-17 20:17:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 3 loss:0.06703164428472519 norm:0.00016760405560489744 max memory_allocated 53580.677734375 
[2025-03-17 20:18:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 4 loss:0.06525352597236633 norm:0.00015059580618981272 max memory_allocated 53580.677734375 
[2025-03-17 20:19:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 5 loss:0.06390319019556046 norm:0.00014322313654702157 max memory_allocated 53580.677734375 
[2025-03-17 20:21:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 6 loss:0.06308244168758392 norm:0.00014632650709245354 max memory_allocated 53580.677734375 
[2025-03-17 20:22:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 7 loss:0.06263851374387741 norm:0.0001382376649416983 max memory_allocated 53580.677734375 
[2025-03-17 20:24:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 8 loss:0.062347739934921265 norm:0.0001335444103460759 max memory_allocated 53580.677734375 
[2025-03-17 20:25:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 9 loss:0.06210660934448242 norm:0.00012926719500683248 max memory_allocated 53580.677734375 
[2025-03-17 20:26:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 10 loss:0.061914097517728806 norm:0.0001294213579967618 max memory_allocated 53580.677734375 
[2025-03-17 20:28:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 11 loss:0.061746228486299515 norm:0.00012946207425557077 max memory_allocated 53580.677734375 
[2025-03-17 20:29:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 12 loss:0.06158345937728882 norm:0.00012080811575287953 max memory_allocated 53580.677734375 
[2025-03-17 20:30:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 13 loss:0.06144856661558151 norm:0.0001183183048851788 max memory_allocated 53580.677734375 
[2025-03-17 20:32:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 14 loss:0.061341531574726105 norm:0.00011535538214957342 max memory_allocated 53580.677734375 
[2025-03-17 20:33:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 15 loss:0.061242081224918365 norm:0.00011380738578736782 max memory_allocated 53580.677734375 
[2025-03-17 20:34:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 16 loss:0.06115255132317543 norm:0.00010738016135292128 max memory_allocated 53580.677734375 
[2025-03-17 20:36:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 17 loss:0.0610789880156517 norm:9.674921602709219e-05 max memory_allocated 53580.677734375 
[2025-03-17 20:37:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 18 loss:0.06103074923157692 norm:9.606152889318764e-05 max memory_allocated 53580.677734375 
[2025-03-17 20:38:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 13 (layers [22, 23]) iter 19 loss:0.060960691422224045 norm:9.576237061992288e-05 max memory_allocated 53580.677734375 
[2025-03-17 20:40:31 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 13, block: [22, 23]
[2025-03-17 20:40:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 14 with layers [24, 25] ===
[2025-03-17 20:41:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 0 loss:0.10332071036100388 norm:0.0005173560348339379 max memory_allocated 53580.677734375 
[2025-03-17 20:43:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 1 loss:0.09200043231248856 norm:0.00029671203810721636 max memory_allocated 53580.677734375 
[2025-03-17 20:44:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 2 loss:0.0816650316119194 norm:0.00020399066852405667 max memory_allocated 53580.677734375 
[2025-03-17 20:45:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 3 loss:0.0790441483259201 norm:0.00018303486285731196 max memory_allocated 53580.677734375 
[2025-03-17 20:47:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 4 loss:0.07713784277439117 norm:0.00016486433742102236 max memory_allocated 53580.677734375 
[2025-03-17 20:48:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 5 loss:0.0756678432226181 norm:0.00015285014524124563 max memory_allocated 53580.677734375 
[2025-03-17 20:49:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 6 loss:0.07490389049053192 norm:0.00014501711120828986 max memory_allocated 53580.677734375 
[2025-03-17 20:51:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 7 loss:0.07448911666870117 norm:0.00013820362801197916 max memory_allocated 53580.677734375 
[2025-03-17 20:52:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 8 loss:0.07418909668922424 norm:0.0001343620242550969 max memory_allocated 53580.677734375 
[2025-03-17 20:53:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 9 loss:0.07394756376743317 norm:0.0001275168760912493 max memory_allocated 53580.677734375 
[2025-03-17 20:55:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 10 loss:0.07374420762062073 norm:0.00012531218817457557 max memory_allocated 53580.677734375 
[2025-03-17 20:56:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 11 loss:0.0735737681388855 norm:0.0001246698957402259 max memory_allocated 53580.677734375 
[2025-03-17 20:58:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 12 loss:0.07340949028730392 norm:0.0001206991437356919 max memory_allocated 53580.677734375 
[2025-03-17 20:59:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 13 loss:0.07329007238149643 norm:0.00011673574044834822 max memory_allocated 53580.677734375 
[2025-03-17 21:00:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 14 loss:0.07316769659519196 norm:0.00011543299478944391 max memory_allocated 53580.677734375 
[2025-03-17 21:02:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 15 loss:0.07305604219436646 norm:0.00011172687663929537 max memory_allocated 53580.677734375 
[2025-03-17 21:03:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 16 loss:0.07294053584337234 norm:0.00010963407839881256 max memory_allocated 53580.677734375 
[2025-03-17 21:04:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 17 loss:0.07285334914922714 norm:0.00010870949336094782 max memory_allocated 53580.677734375 
[2025-03-17 21:06:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 18 loss:0.07277434319257736 norm:0.00010729047062341124 max memory_allocated 53580.677734375 
[2025-03-17 21:07:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 14 (layers [24, 25]) iter 19 loss:0.07269531488418579 norm:0.00010591067257337272 max memory_allocated 53580.677734375 
[2025-03-17 21:09:13 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 14, block: [24, 25]
[2025-03-17 21:09:14 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 15 with layers [26, 27] ===
[2025-03-17 21:10:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 0 loss:0.12112036347389221 norm:0.0005023081321269274 max memory_allocated 53580.677734375 
[2025-03-17 21:12:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 1 loss:0.10900424420833588 norm:0.0002970502828247845 max memory_allocated 53580.677734375 
[2025-03-17 21:13:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 2 loss:0.09741532802581787 norm:0.00021322436805348843 max memory_allocated 53580.677734375 
[2025-03-17 21:14:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 3 loss:0.09442611038684845 norm:0.000178796355612576 max memory_allocated 53580.677734375 
[2025-03-17 21:16:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 4 loss:0.09226348251104355 norm:0.00016947125550359488 max memory_allocated 53580.677734375 
[2025-03-17 21:17:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 5 loss:0.09070710837841034 norm:0.00015982103650458157 max memory_allocated 53580.677734375 
[2025-03-17 21:18:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 6 loss:0.08996330946683884 norm:0.0001535451883682981 max memory_allocated 53580.677734375 
[2025-03-17 21:20:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 7 loss:0.08953709900379181 norm:0.00014488963643088937 max memory_allocated 53580.677734375 
[2025-03-17 21:21:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 8 loss:0.089235819876194 norm:0.00013541450607590377 max memory_allocated 53580.677734375 
[2025-03-17 21:22:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 9 loss:0.08900491893291473 norm:0.00013625780411530286 max memory_allocated 53580.677734375 
[2025-03-17 21:24:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 10 loss:0.08877940475940704 norm:0.00013097601186018437 max memory_allocated 53580.677734375 
[2025-03-17 21:25:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 11 loss:0.08859122544527054 norm:0.00012973547563888133 max memory_allocated 53580.677734375 
[2025-03-17 21:26:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 12 loss:0.08840320259332657 norm:0.00012464981409721076 max memory_allocated 53580.677734375 
[2025-03-17 21:28:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 13 loss:0.08823294192552567 norm:0.00012069121294189245 max memory_allocated 53580.677734375 
[2025-03-17 21:29:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 14 loss:0.08811701834201813 norm:0.00011811172589659691 max memory_allocated 53580.677734375 
[2025-03-17 21:30:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 15 loss:0.08799507468938828 norm:0.0001179130922537297 max memory_allocated 53580.677734375 
[2025-03-17 21:32:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 16 loss:0.08786746859550476 norm:0.00011839707440230995 max memory_allocated 53580.677734375 
[2025-03-17 21:33:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 17 loss:0.08774890750646591 norm:0.00011310747504467145 max memory_allocated 53580.677734375 
[2025-03-17 21:34:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 18 loss:0.0876588225364685 norm:0.00011131237260997295 max memory_allocated 53580.677734375 
[2025-03-17 21:36:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 15 (layers [26, 27]) iter 19 loss:0.0875813215970993 norm:0.00011024963896488771 max memory_allocated 53580.677734375 
[2025-03-17 21:37:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 15, block: [26, 27]
[2025-03-17 21:37:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 16 with layers [28, 29] ===
[2025-03-17 21:39:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 0 loss:0.14164958894252777 norm:0.0005136622930876911 max memory_allocated 53580.677734375 
[2025-03-17 21:40:45 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 1 loss:0.128719300031662 norm:0.00034477683948352933 max memory_allocated 53580.677734375 
[2025-03-17 21:42:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 2 loss:0.1162237897515297 norm:0.00025801040465012193 max memory_allocated 53580.677734375 
[2025-03-17 21:43:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 3 loss:0.1129307746887207 norm:0.0002219615416834131 max memory_allocated 53580.677734375 
[2025-03-17 21:44:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 4 loss:0.11050830781459808 norm:0.00020500196842476726 max memory_allocated 53580.677734375 
[2025-03-17 21:46:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 5 loss:0.10898756235837936 norm:0.00019741395954042673 max memory_allocated 53580.677734375 
[2025-03-17 21:47:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 6 loss:0.10833071917295456 norm:0.00018205259402748197 max memory_allocated 53580.677734375 
[2025-03-17 21:48:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 7 loss:0.10791081190109253 norm:0.00017201504670083523 max memory_allocated 53580.677734375 
[2025-03-17 21:50:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 8 loss:0.10759510844945908 norm:0.00017596148245502263 max memory_allocated 53580.677734375 
[2025-03-17 21:51:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 9 loss:0.10733343660831451 norm:0.0001767222274793312 max memory_allocated 53580.677734375 
[2025-03-17 21:52:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 10 loss:0.10709090530872345 norm:0.00017397683404851705 max memory_allocated 53580.677734375 
[2025-03-17 21:54:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 11 loss:0.10686980187892914 norm:0.00015931982488837093 max memory_allocated 53580.677734375 
[2025-03-17 21:55:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 12 loss:0.10668002814054489 norm:0.00016728657647036016 max memory_allocated 53580.677734375 
[2025-03-17 21:56:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 13 loss:0.10650759190320969 norm:0.0001654421357670799 max memory_allocated 53580.677734375 
[2025-03-17 21:58:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 14 loss:0.1063612699508667 norm:0.00018299699877388775 max memory_allocated 53580.677734375 
[2025-03-17 21:59:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 15 loss:0.1062285527586937 norm:0.0001707731862552464 max memory_allocated 53580.677734375 
[2025-03-17 22:00:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 16 loss:0.10609076917171478 norm:0.00013024217332713306 max memory_allocated 53580.677734375 
[2025-03-17 22:02:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 17 loss:0.10597296059131622 norm:0.00015958449512254447 max memory_allocated 53580.677734375 
[2025-03-17 22:03:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 18 loss:0.10586578398942947 norm:0.00012430697097443044 max memory_allocated 53580.677734375 
[2025-03-17 22:04:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 16 (layers [28, 29]) iter 19 loss:0.10578377544879913 norm:0.0001276324619539082 max memory_allocated 53580.677734375 
[2025-03-17 22:06:42 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 16, block: [28, 29]
[2025-03-17 22:06:42 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 17 with layers [30, 31] ===
[2025-03-17 22:08:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 0 loss:0.17008982598781586 norm:0.0005625271005555987 max memory_allocated 53580.677734375 
[2025-03-17 22:09:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 1 loss:0.1543591022491455 norm:0.00032944034319370985 max memory_allocated 53580.677734375 
[2025-03-17 22:10:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 2 loss:0.13898542523384094 norm:0.0001992973411688581 max memory_allocated 53580.677734375 
[2025-03-17 22:12:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 3 loss:0.1348780244588852 norm:0.00016709609190002084 max memory_allocated 53580.677734375 
[2025-03-17 22:13:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 4 loss:0.1322162002325058 norm:0.00015573500422760844 max memory_allocated 53580.677734375 
[2025-03-17 22:14:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 5 loss:0.1309593766927719 norm:0.000146782273077406 max memory_allocated 53580.677734375 
[2025-03-17 22:16:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 6 loss:0.13043884932994843 norm:0.0001472553121857345 max memory_allocated 53580.677734375 
[2025-03-17 22:17:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 7 loss:0.13004830479621887 norm:0.00013744454190600663 max memory_allocated 53580.677734375 
[2025-03-17 22:18:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 8 loss:0.12971535325050354 norm:0.00013436046720016748 max memory_allocated 53580.677734375 
[2025-03-17 22:20:10 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 9 loss:0.12942886352539062 norm:0.00012995577708352357 max memory_allocated 53580.677734375 
[2025-03-17 22:21:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 10 loss:0.1291848123073578 norm:0.00012419097765814513 max memory_allocated 53580.677734375 
[2025-03-17 22:22:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 11 loss:0.12896963953971863 norm:0.00012180732301203534 max memory_allocated 53580.677734375 
[2025-03-17 22:24:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 12 loss:0.12877660989761353 norm:0.00011953149078181013 max memory_allocated 53580.677734375 
[2025-03-17 22:25:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 13 loss:0.12861120700836182 norm:0.0001159820967586711 max memory_allocated 53580.677734375 
[2025-03-17 22:26:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 14 loss:0.1284390687942505 norm:0.00011447087308624759 max memory_allocated 53580.677734375 
[2025-03-17 22:28:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 15 loss:0.12829090654850006 norm:0.00011061574332416058 max memory_allocated 53580.677734375 
[2025-03-17 22:29:31 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 16 loss:0.12815731763839722 norm:0.00010959881910821423 max memory_allocated 53580.677734375 
[2025-03-17 22:30:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 17 loss:0.1280430406332016 norm:0.0001084613031707704 max memory_allocated 53580.677734375 
[2025-03-17 22:32:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 18 loss:0.12792694568634033 norm:0.00011242114123888314 max memory_allocated 53580.677734375 
[2025-03-17 22:33:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 17 (layers [30, 31]) iter 19 loss:0.12782269716262817 norm:0.00011235788406338543 max memory_allocated 53580.677734375 
[2025-03-17 22:35:19 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 17, block: [30, 31]
[2025-03-17 22:35:19 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 18 with layers [32, 33] ===
[2025-03-17 22:36:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 0 loss:0.2013738751411438 norm:0.0005512342322617769 max memory_allocated 53580.677734375 
[2025-03-17 22:38:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 1 loss:0.1836068332195282 norm:0.0003401873109396547 max memory_allocated 53580.677734375 
[2025-03-17 22:39:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 2 loss:0.16652482748031616 norm:0.00021667344844900072 max memory_allocated 53580.677734375 
[2025-03-17 22:40:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 3 loss:0.16168135404586792 norm:0.0001821986079448834 max memory_allocated 53580.677734375 
[2025-03-17 22:42:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 4 loss:0.15881362557411194 norm:0.00017246014613192528 max memory_allocated 53580.677734375 
[2025-03-17 22:43:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 5 loss:0.15767157077789307 norm:0.00015757454093545675 max memory_allocated 53580.677734375 
[2025-03-17 22:44:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 6 loss:0.1570831835269928 norm:0.00014874521002639085 max memory_allocated 53580.677734375 
[2025-03-17 22:46:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 7 loss:0.15663538873195648 norm:0.00014150625793263316 max memory_allocated 53580.677734375 
[2025-03-17 22:47:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 8 loss:0.15625934302806854 norm:0.00013615010539069772 max memory_allocated 53580.677734375 
[2025-03-17 22:48:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 9 loss:0.1559402346611023 norm:0.00013318068522494286 max memory_allocated 53580.677734375 
[2025-03-17 22:50:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 10 loss:0.15563102066516876 norm:0.0001290119980694726 max memory_allocated 53580.677734375 
[2025-03-17 22:51:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 11 loss:0.1553851068019867 norm:0.00012437986151780933 max memory_allocated 53580.677734375 
[2025-03-17 22:52:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 12 loss:0.1551525890827179 norm:0.0001229399349540472 max memory_allocated 53580.677734375 
[2025-03-17 22:54:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 13 loss:0.15492986142635345 norm:0.00011823706881841645 max memory_allocated 53580.677734375 
[2025-03-17 22:55:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 14 loss:0.15474213659763336 norm:0.00011410809383960441 max memory_allocated 53580.677734375 
[2025-03-17 22:56:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 15 loss:0.1545901745557785 norm:0.0001156262296717614 max memory_allocated 53580.677734375 
[2025-03-17 22:58:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 16 loss:0.15445247292518616 norm:0.00011384978279238567 max memory_allocated 53580.677734375 
[2025-03-17 22:59:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 17 loss:0.15428239107131958 norm:0.00011105740122729912 max memory_allocated 53580.677734375 
[2025-03-17 23:00:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 18 loss:0.15416209399700165 norm:0.00011367395927663893 max memory_allocated 53580.677734375 
[2025-03-17 23:02:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 18 (layers [32, 33]) iter 19 loss:0.1540515124797821 norm:0.00011166672629769892 max memory_allocated 53580.677734375 
[2025-03-17 23:03:55 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 18, block: [32, 33]
[2025-03-17 23:03:55 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 19 with layers [34, 35] ===
[2025-03-17 23:05:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 0 loss:0.2431669384241104 norm:0.0005876830546185374 max memory_allocated 53581.396484375 
[2025-03-17 23:06:41 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 1 loss:0.2221703827381134 norm:0.0003585755475796759 max memory_allocated 53581.396484375 
[2025-03-17 23:08:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 2 loss:0.20341920852661133 norm:0.00024746923008933663 max memory_allocated 53581.396484375 
[2025-03-17 23:09:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 3 loss:0.1976536512374878 norm:0.00021496399131137878 max memory_allocated 53581.396484375 
[2025-03-17 23:10:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 4 loss:0.19474460184574127 norm:0.00020155339734628797 max memory_allocated 53581.396484375 
[2025-03-17 23:12:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 5 loss:0.193545863032341 norm:0.0001826892257668078 max memory_allocated 53581.396484375 
[2025-03-17 23:13:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 6 loss:0.19283606112003326 norm:0.00017209732322953641 max memory_allocated 53581.396484375 
[2025-03-17 23:14:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 7 loss:0.1922832429409027 norm:0.00016688898904249072 max memory_allocated 53581.396484375 
[2025-03-17 23:16:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 8 loss:0.1917860358953476 norm:0.0001593680353835225 max memory_allocated 53581.396484375 
[2025-03-17 23:17:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 9 loss:0.191331148147583 norm:0.00015322226681746542 max memory_allocated 53581.396484375 
[2025-03-17 23:18:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 10 loss:0.19095711410045624 norm:0.0001490265130996704 max memory_allocated 53581.396484375 
[2025-03-17 23:20:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 11 loss:0.19063350558280945 norm:0.0001439806801499799 max memory_allocated 53581.396484375 
[2025-03-17 23:21:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 12 loss:0.190335214138031 norm:0.00013920775381848216 max memory_allocated 53581.396484375 
[2025-03-17 23:22:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 13 loss:0.1900302916765213 norm:0.00013574995682574809 max memory_allocated 53581.396484375 
[2025-03-17 23:24:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 14 loss:0.1898043304681778 norm:0.00013284401211421937 max memory_allocated 53581.396484375 
[2025-03-17 23:25:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 15 loss:0.18958459794521332 norm:0.00013157670036889613 max memory_allocated 53581.396484375 
[2025-03-17 23:26:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 16 loss:0.18939515948295593 norm:0.00012734337360598147 max memory_allocated 53581.396484375 
[2025-03-17 23:28:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 17 loss:0.18919366598129272 norm:0.00012521029566414654 max memory_allocated 53581.396484375 
[2025-03-17 23:29:24 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 18 loss:0.18904253840446472 norm:0.00012403327855281532 max memory_allocated 53581.396484375 
[2025-03-17 23:30:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 19 (layers [34, 35]) iter 19 loss:0.18889248371124268 norm:0.00012216228060424328 max memory_allocated 53581.396484375 
[2025-03-17 23:32:31 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 19, block: [34, 35]
[2025-03-17 23:32:31 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 20 with layers [36] ===
[2025-03-17 23:32:32 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 23:33:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 0 loss:0.23877425491809845 norm:0.009217904880642891 max memory_allocated 53581.396484375 
[2025-03-17 23:33:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 1 loss:0.2248060554265976 norm:0.006879919208586216 max memory_allocated 53581.396484375 
[2025-03-17 23:34:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 2 loss:0.21332629024982452 norm:0.004927375819534063 max memory_allocated 53581.396484375 
[2025-03-17 23:35:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 3 loss:0.20967361330986023 norm:0.004033793229609728 max memory_allocated 53581.396484375 
[2025-03-17 23:35:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 4 loss:0.20813311636447906 norm:0.0032985692378133535 max memory_allocated 53581.396484375 
[2025-03-17 23:36:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 5 loss:0.20744526386260986 norm:0.0027020606212317944 max memory_allocated 53581.396484375 
[2025-03-17 23:37:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 6 loss:0.20695598423480988 norm:0.0023652450181543827 max memory_allocated 53581.396484375 
[2025-03-17 23:37:58 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 7 loss:0.20668551325798035 norm:0.0023391516879200935 max memory_allocated 53581.396484375 
[2025-03-17 23:38:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 8 loss:0.20636945962905884 norm:0.0022245405707508326 max memory_allocated 53581.396484375 
[2025-03-17 23:39:19 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 9 loss:0.20608623325824738 norm:0.002009625779464841 max memory_allocated 53581.396484375 
[2025-03-17 23:40:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 10 loss:0.20581594109535217 norm:0.0019784581381827593 max memory_allocated 53581.396484375 
[2025-03-17 23:40:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 11 loss:0.20558691024780273 norm:0.0018656258471310139 max memory_allocated 53581.396484375 
[2025-03-17 23:41:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 12 loss:0.2053755819797516 norm:0.0017911528702825308 max memory_allocated 53581.396484375 
[2025-03-17 23:42:01 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 13 loss:0.20514178276062012 norm:0.0016849467065185308 max memory_allocated 53581.396484375 
[2025-03-17 23:42:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 14 loss:0.2049730122089386 norm:0.001600885996595025 max memory_allocated 53581.396484375 
[2025-03-17 23:43:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 15 loss:0.20482727885246277 norm:0.0015583423664793372 max memory_allocated 53581.396484375 
[2025-03-17 23:44:02 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 16 loss:0.2047402560710907 norm:0.00163018808234483 max memory_allocated 53581.396484375 
[2025-03-17 23:44:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 17 loss:0.20461304485797882 norm:0.001585796126164496 max memory_allocated 53581.396484375 
[2025-03-17 23:45:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 18 loss:0.20447759330272675 norm:0.0014481120742857456 max memory_allocated 53581.396484375 
[2025-03-17 23:46:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 20 (layers [36]) iter 19 loss:0.2043924778699875 norm:0.0013879550388082862 max memory_allocated 53581.396484375 
[2025-03-17 23:46:58 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 20, block: [36]
[2025-03-17 23:46:58 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 21 with layers [37] ===
[2025-03-17 23:46:58 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-17 23:47:42 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 0 loss:0.2643323540687561 norm:0.009585616178810596 max memory_allocated 53581.396484375 
[2025-03-17 23:48:22 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 1 loss:0.24967998266220093 norm:0.007309495005756617 max memory_allocated 53581.396484375 
[2025-03-17 23:49:03 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 2 loss:0.2371746301651001 norm:0.005131221376359463 max memory_allocated 53581.396484375 
[2025-03-17 23:49:43 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 3 loss:0.23312678933143616 norm:0.004189767874777317 max memory_allocated 53581.396484375 
[2025-03-17 23:50:23 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 4 loss:0.23159940540790558 norm:0.0034619690850377083 max memory_allocated 53581.396484375 
[2025-03-17 23:51:04 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 5 loss:0.23088258504867554 norm:0.002920406172052026 max memory_allocated 53581.396484375 
[2025-03-17 23:51:44 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 6 loss:0.23036229610443115 norm:0.0024569393135607243 max memory_allocated 53581.396484375 
[2025-03-17 23:52:25 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 7 loss:0.2301046997308731 norm:0.0024357226211577654 max memory_allocated 53581.396484375 
[2025-03-17 23:53:05 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 8 loss:0.2298024594783783 norm:0.0024077906273305416 max memory_allocated 53581.396484375 
[2025-03-17 23:53:46 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 9 loss:0.2296021580696106 norm:0.002302216598764062 max memory_allocated 53581.396484375 
[2025-03-17 23:54:26 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 10 loss:0.22930842638015747 norm:0.002100651850923896 max memory_allocated 53581.396484375 
[2025-03-17 23:55:06 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 11 loss:0.22907491028308868 norm:0.002011626260355115 max memory_allocated 53581.396484375 
[2025-03-17 23:55:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 12 loss:0.22893798351287842 norm:0.0020575569942593575 max memory_allocated 53581.396484375 
[2025-03-17 23:56:27 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 13 loss:0.22876021265983582 norm:0.0019446087535470724 max memory_allocated 53581.396484375 
[2025-03-17 23:57:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 14 loss:0.2286253571510315 norm:0.0018983999034389853 max memory_allocated 53581.396484375 
[2025-03-17 23:57:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 15 loss:0.22854332625865936 norm:0.0019629383459687233 max memory_allocated 53581.396484375 
[2025-03-17 23:58:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 16 loss:0.22845202684402466 norm:0.001960502006113529 max memory_allocated 53581.396484375 
[2025-03-17 23:59:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 17 loss:0.22836512327194214 norm:0.0019300547428429127 max memory_allocated 53581.396484375 
[2025-03-17 23:59:49 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 18 loss:0.2283039689064026 norm:0.0019251920748502016 max memory_allocated 53581.396484375 
[2025-03-18 00:00:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 21 (layers [37]) iter 19 loss:0.22816643118858337 norm:0.001857678173109889 max memory_allocated 53581.396484375 
[2025-03-18 00:01:22 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 21, block: [37]
[2025-03-18 00:01:23 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 22 with layers [38] ===
[2025-03-18 00:01:23 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-18 00:02:07 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 0 loss:0.3096473813056946 norm:0.015133332461118698 max memory_allocated 53581.396484375 
[2025-03-18 00:02:47 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 1 loss:0.2903304994106293 norm:0.010710446164011955 max memory_allocated 53581.396484375 
[2025-03-18 00:03:28 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 2 loss:0.27566516399383545 norm:0.007283600978553295 max memory_allocated 53581.396484375 
[2025-03-18 00:04:08 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 3 loss:0.2711056172847748 norm:0.006102022714912891 max memory_allocated 53581.396484375 
[2025-03-18 00:04:48 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 4 loss:0.269401490688324 norm:0.0052408683113753796 max memory_allocated 53581.396484375 
[2025-03-18 00:05:29 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 5 loss:0.2685297429561615 norm:0.004571897443383932 max memory_allocated 53581.396484375 
[2025-03-18 00:06:09 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 6 loss:0.2678868770599365 norm:0.004080294631421566 max memory_allocated 53581.396484375 
[2025-03-18 00:06:50 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 7 loss:0.26743072271347046 norm:0.003826848231256008 max memory_allocated 53581.396484375 
[2025-03-18 00:07:30 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 8 loss:0.26705479621887207 norm:0.0036455688532441854 max memory_allocated 53581.396484375 
[2025-03-18 00:08:11 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 9 loss:0.26681748032569885 norm:0.0036546997725963593 max memory_allocated 53581.396484375 
[2025-03-18 00:08:51 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 10 loss:0.2667405903339386 norm:0.0037310991901904345 max memory_allocated 53581.396484375 
[2025-03-18 00:09:32 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 11 loss:0.2665152847766876 norm:0.003773844800889492 max memory_allocated 53581.396484375 
[2025-03-18 00:10:12 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 12 loss:0.26628464460372925 norm:0.0033700934145599604 max memory_allocated 53581.396484375 
[2025-03-18 00:10:53 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 13 loss:0.26629698276519775 norm:0.0035205786116421223 max memory_allocated 53581.396484375 
[2025-03-18 00:11:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 14 loss:0.2661033868789673 norm:0.003414152655750513 max memory_allocated 53581.396484375 
[2025-03-18 00:12:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 15 loss:0.2659095227718353 norm:0.0032135643996298313 max memory_allocated 53581.396484375 
[2025-03-18 00:12:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 16 loss:0.26578158140182495 norm:0.0030394934583455324 max memory_allocated 53581.396484375 
[2025-03-18 00:13:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 17 loss:0.26563936471939087 norm:0.002923174761235714 max memory_allocated 53581.396484375 
[2025-03-18 00:14:15 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 18 loss:0.2655283808708191 norm:0.0028511767741292715 max memory_allocated 53581.396484375 
[2025-03-18 00:14:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 22 (layers [38]) iter 19 loss:0.2655295729637146 norm:0.0028918820898979902 max memory_allocated 53581.396484375 
[2025-03-18 00:15:49 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 22, block: [38]
[2025-03-18 00:15:49 root] (abq_llm_calib_config3_attn.py 257): INFO === Start quantize block 23 with layers [39] ===
[2025-03-18 00:15:49 root] (abq_llm_calib_config3_attn.py 309): INFO use compensation vector
[2025-03-18 00:16:33 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 0 loss:0.44335290789604187 norm:0.04169600084424019 max memory_allocated 53581.396484375 
[2025-03-18 00:17:13 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 1 loss:0.3995729386806488 norm:0.02716636098921299 max memory_allocated 53581.396484375 
[2025-03-18 00:17:54 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 2 loss:0.36808472871780396 norm:0.017712097615003586 max memory_allocated 53581.396484375 
[2025-03-18 00:18:34 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 3 loss:0.3587595522403717 norm:0.01451765839010477 max memory_allocated 53581.396484375 
[2025-03-18 00:19:14 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 4 loss:0.3546184301376343 norm:0.012809592299163342 max memory_allocated 53581.396484375 
[2025-03-18 00:19:55 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 5 loss:0.3523685932159424 norm:0.011286710388958454 max memory_allocated 53581.396484375 
[2025-03-18 00:20:35 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 6 loss:0.35045716166496277 norm:0.010531888343393803 max memory_allocated 53581.396484375 
[2025-03-18 00:21:16 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 7 loss:0.3492427170276642 norm:0.009739527478814125 max memory_allocated 53581.396484375 
[2025-03-18 00:21:56 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 8 loss:0.3482745885848999 norm:0.009210608899593353 max memory_allocated 53581.396484375 
[2025-03-18 00:22:36 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 9 loss:0.3472284972667694 norm:0.008660349994897842 max memory_allocated 53581.396484375 
[2025-03-18 00:23:17 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 10 loss:0.34650227427482605 norm:0.00811711698770523 max memory_allocated 53581.396484375 
[2025-03-18 00:23:57 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 11 loss:0.3458060324192047 norm:0.007830032147467136 max memory_allocated 53581.396484375 
[2025-03-18 00:24:38 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 12 loss:0.34544503688812256 norm:0.00791251566261053 max memory_allocated 53581.396484375 
[2025-03-18 00:25:18 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 13 loss:0.345380574464798 norm:0.008099112659692764 max memory_allocated 53581.396484375 
[2025-03-18 00:25:59 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 14 loss:0.34508442878723145 norm:0.007504066918045282 max memory_allocated 53581.396484375 
[2025-03-18 00:26:39 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 15 loss:0.3448260426521301 norm:0.007273978088051081 max memory_allocated 53581.396484375 
[2025-03-18 00:27:20 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 16 loss:0.34435179829597473 norm:0.006806827150285244 max memory_allocated 53581.396484375 
[2025-03-18 00:28:00 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 17 loss:0.3437359035015106 norm:0.00636051781475544 max memory_allocated 53581.396484375 
[2025-03-18 00:28:40 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 18 loss:0.34335893392562866 norm:0.006442515179514885 max memory_allocated 53581.396484375 
[2025-03-18 00:29:21 root] (abq_llm_calib_config3_attn.py 464): INFO block 23 (layers [39]) iter 19 loss:0.3429354429244995 norm:0.006062519270926714 max memory_allocated 53581.396484375 
[2025-03-18 00:30:14 root] (abq_llm_calib_config3_attn.py 509): INFO Saving abq_parameters for block 23, block: [39]
[2025-03-18 00:30:14 root] (main_calib_config3_attn.py 379): INFO 34411.611666202545
[2025-03-18 00:30:25 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-03-18 00:31:42 root] (main_calib_config3_attn.py 161): INFO wikitext2 : 5.0068817138671875
[2025-03-18 00:31:42 root] (main_calib_config3_attn.py 117): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-03-18 00:33:40 root] (main_calib_config3_attn.py 161): INFO c4 : 6.631824016571045
[2025-03-18 00:33:50 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/hellaswag/512a66dd8b1b1643ab4a48aa4f150d04c91680da6a4096498a5e5f799623d5ae (last modified on Tue Feb 18 03:27:10 2025) since it couldn't be found locally at hellaswag., or remotely on the Hugging Face Hub.
[2025-03-18 01:18:53 root] (main_calib_config3_attn.py 172): INFO {'wikitext2': 5.0068817138671875, 'c4': 6.631824016571045, 'results': {'hellaswag': {'acc': 0.5914160525791675, 'acc_stderr': 0.004905674408614029, 'acc_norm': 0.7517426807408882, 'acc_norm_stderr': 0.004311189882238346}, 'arc_challenge': {'acc': 0.4377133105802048, 'acc_stderr': 0.014497573881108287, 'acc_norm': 0.4377133105802048, 'acc_norm_stderr': 0.014497573881108288}, 'winogrande': {'acc': 0.6874506708760852, 'acc_stderr': 0.013027563620748838}, 'piqa': {'acc': 0.7823721436343852, 'acc_stderr': 0.009627407474840876, 'acc_norm': 0.7872687704026116, 'acc_norm_stderr': 0.009548223123047346}, 'arc_easy': {'acc': 0.7218013468013468, 'acc_stderr': 0.009195059601583896, 'acc_norm': 0.5669191919191919, 'acc_norm_stderr': 0.010167478013701796}, 'boolq': {'acc': 0.6568807339449542, 'acc_stderr': 0.008303445777655934}}, 'versions': {'hellaswag': 0, 'arc_challenge': 0, 'winogrande': 0, 'piqa': 0, 'arc_easy': 0, 'boolq': 1}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-18 01:18:53 root] (main_calib_config3_attn.py 175): INFO 43.77,72.18,65.69,59.14,78.24,68.75
