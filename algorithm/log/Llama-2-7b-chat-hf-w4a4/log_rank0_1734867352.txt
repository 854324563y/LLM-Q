[2024-12-22 11:35:52 root] (main.py 265): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-chat-hf', cache_dir='./cache', output_dir='./log/Llama-2-7b-chat-hf-w4a4', save_dir='./quant/Llama-2-7b-chat-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2024-12-22 11:35:54 root] (main.py 331): INFO === start quantization ===
[2024-12-22 11:36:54 root] (abq_llm.py 62): INFO Starting ...
[2024-12-22 11:36:56 root] (abq_llm.py 208): INFO === Start quantize layer 0 ===
[2024-12-22 11:37:00 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-22 11:37:31 root] (abq_llm.py 321): INFO layer 0 iter 0 loss:0.06102924048900604 norm:0.03738400340080261 max memory_allocated 22886.16943359375 
[2024-12-22 11:38:02 root] (abq_llm.py 321): INFO layer 0 iter 1 loss:0.03717965632677078 norm:0.022742262110114098 max memory_allocated 22886.16943359375 
[2024-12-22 11:38:33 root] (abq_llm.py 321): INFO layer 0 iter 2 loss:0.02959926426410675 norm:0.018674591556191444 max memory_allocated 22886.16943359375 
[2024-12-22 11:39:05 root] (abq_llm.py 321): INFO layer 0 iter 3 loss:0.02623472735285759 norm:0.015616677701473236 max memory_allocated 22886.16943359375 
[2024-12-22 11:39:36 root] (abq_llm.py 321): INFO layer 0 iter 4 loss:0.024533074349164963 norm:0.012646907940506935 max memory_allocated 22886.16943359375 
[2024-12-22 11:40:07 root] (abq_llm.py 321): INFO layer 0 iter 5 loss:0.02376299351453781 norm:0.011132760904729366 max memory_allocated 22886.16943359375 
[2024-12-22 11:40:39 root] (abq_llm.py 321): INFO layer 0 iter 6 loss:0.023052604869008064 norm:0.009796020574867725 max memory_allocated 22886.16943359375 
[2024-12-22 11:41:10 root] (abq_llm.py 321): INFO layer 0 iter 7 loss:0.022481955587863922 norm:0.008883357048034668 max memory_allocated 22886.16943359375 
[2024-12-22 11:41:42 root] (abq_llm.py 321): INFO layer 0 iter 8 loss:0.022037528455257416 norm:0.007426197174936533 max memory_allocated 22886.16943359375 
[2024-12-22 11:42:13 root] (abq_llm.py 321): INFO layer 0 iter 9 loss:0.02162674255669117 norm:0.006561185233294964 max memory_allocated 22886.16943359375 
[2024-12-22 11:42:45 root] (abq_llm.py 321): INFO layer 0 iter 10 loss:0.02169867977499962 norm:0.007413978222757578 max memory_allocated 22886.16943359375 
[2024-12-22 11:43:16 root] (abq_llm.py 321): INFO layer 0 iter 11 loss:0.021321356296539307 norm:0.005920126102864742 max memory_allocated 22886.16943359375 
[2024-12-22 11:43:48 root] (abq_llm.py 321): INFO layer 0 iter 12 loss:0.021219532936811447 norm:0.005861320067197084 max memory_allocated 22886.16943359375 
[2024-12-22 11:44:19 root] (abq_llm.py 321): INFO layer 0 iter 13 loss:0.021191319450736046 norm:0.0057322317734360695 max memory_allocated 22886.16943359375 
[2024-12-22 11:44:51 root] (abq_llm.py 321): INFO layer 0 iter 14 loss:0.02105686627328396 norm:0.0053974660113453865 max memory_allocated 22886.16943359375 
[2024-12-22 11:45:22 root] (abq_llm.py 321): INFO layer 0 iter 15 loss:0.021058419719338417 norm:0.008272774517536163 max memory_allocated 22886.16943359375 
[2024-12-22 11:45:54 root] (abq_llm.py 321): INFO layer 0 iter 16 loss:0.02082189731299877 norm:0.004810340702533722 max memory_allocated 22886.16943359375 
[2024-12-22 11:46:25 root] (abq_llm.py 321): INFO layer 0 iter 17 loss:0.020655684173107147 norm:0.0047143446281552315 max memory_allocated 22886.16943359375 
[2024-12-22 11:46:57 root] (abq_llm.py 321): INFO layer 0 iter 18 loss:0.020857982337474823 norm:0.005241095554083586 max memory_allocated 22886.16943359375 
[2024-12-22 11:47:28 root] (abq_llm.py 321): INFO layer 0 iter 19 loss:0.02089810185134411 norm:0.0054184324108064175 max memory_allocated 22886.16943359375 
[2024-12-22 11:47:37 root] (abq_llm.py 208): INFO === Start quantize layer 1 ===
[2024-12-22 11:47:40 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-22 11:48:11 root] (abq_llm.py 321): INFO layer 1 iter 0 loss:0.21527087688446045 norm:0.05725619196891785 max memory_allocated 22887.84130859375 
[2024-12-22 11:48:43 root] (abq_llm.py 321): INFO layer 1 iter 1 loss:0.16028261184692383 norm:0.0430668368935585 max memory_allocated 22887.84130859375 
[2024-12-22 11:49:14 root] (abq_llm.py 321): INFO layer 1 iter 2 loss:0.13433489203453064 norm:0.036822136491537094 max memory_allocated 22887.84130859375 
[2024-12-22 11:49:46 root] (abq_llm.py 321): INFO layer 1 iter 3 loss:0.12401851266622543 norm:0.03343519568443298 max memory_allocated 22887.84130859375 
[2024-12-22 11:50:17 root] (abq_llm.py 321): INFO layer 1 iter 4 loss:0.1197931170463562 norm:0.03289642930030823 max memory_allocated 22887.84130859375 
[2024-12-22 11:50:49 root] (abq_llm.py 321): INFO layer 1 iter 5 loss:0.11514554917812347 norm:0.030045321211218834 max memory_allocated 22887.84130859375 
[2024-12-22 11:51:20 root] (abq_llm.py 321): INFO layer 1 iter 6 loss:0.11411751806735992 norm:0.03015744313597679 max memory_allocated 22887.84130859375 
[2024-12-22 11:51:52 root] (abq_llm.py 321): INFO layer 1 iter 7 loss:0.11292271316051483 norm:0.04159966856241226 max memory_allocated 22887.84130859375 
[2024-12-22 11:52:23 root] (abq_llm.py 321): INFO layer 1 iter 8 loss:0.11245995759963989 norm:0.028978364542126656 max memory_allocated 22887.84130859375 
[2024-12-22 11:52:55 root] (abq_llm.py 321): INFO layer 1 iter 9 loss:0.11029274761676788 norm:0.03422580286860466 max memory_allocated 22887.84130859375 
[2024-12-22 11:53:26 root] (abq_llm.py 321): INFO layer 1 iter 10 loss:0.10866733640432358 norm:0.026913674548268318 max memory_allocated 22887.84130859375 
[2024-12-22 11:53:58 root] (abq_llm.py 321): INFO layer 1 iter 11 loss:0.10793724656105042 norm:0.028108160942792892 max memory_allocated 22887.84130859375 
[2024-12-22 11:54:29 root] (abq_llm.py 321): INFO layer 1 iter 12 loss:0.1084604263305664 norm:0.02807510830461979 max memory_allocated 22887.84130859375 
[2024-12-22 11:55:01 root] (abq_llm.py 321): INFO layer 1 iter 13 loss:0.10816318541765213 norm:0.02775140292942524 max memory_allocated 22887.84130859375 
[2024-12-22 11:55:32 root] (abq_llm.py 321): INFO layer 1 iter 14 loss:0.10766912251710892 norm:0.027464386075735092 max memory_allocated 22887.84130859375 
[2024-12-22 11:56:04 root] (abq_llm.py 321): INFO layer 1 iter 15 loss:0.10745776444673538 norm:0.02654561586678028 max memory_allocated 22887.84130859375 
[2024-12-22 11:56:36 root] (abq_llm.py 321): INFO layer 1 iter 16 loss:0.10671398788690567 norm:0.02673337422311306 max memory_allocated 22887.84130859375 
[2024-12-22 11:57:07 root] (abq_llm.py 321): INFO layer 1 iter 17 loss:0.10560287535190582 norm:0.024545513093471527 max memory_allocated 22887.84130859375 
[2024-12-22 11:57:39 root] (abq_llm.py 321): INFO layer 1 iter 18 loss:0.10531341284513474 norm:0.026663053780794144 max memory_allocated 22887.84130859375 
[2024-12-22 11:58:10 root] (abq_llm.py 321): INFO layer 1 iter 19 loss:0.10579103976488113 norm:0.026931868866086006 max memory_allocated 22887.84130859375 
[2024-12-22 11:58:19 root] (abq_llm.py 208): INFO === Start quantize layer 2 ===
[2024-12-22 11:58:22 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-22 11:58:53 root] (abq_llm.py 321): INFO layer 2 iter 0 loss:0.20488505065441132 norm:0.05154317989945412 max memory_allocated 22889.51318359375 
[2024-12-22 11:59:25 root] (abq_llm.py 321): INFO layer 2 iter 1 loss:0.174517422914505 norm:0.029660169035196304 max memory_allocated 22889.51318359375 
[2024-12-22 11:59:57 root] (abq_llm.py 321): INFO layer 2 iter 2 loss:0.15742728114128113 norm:0.02064570039510727 max memory_allocated 22889.51318359375 
[2024-12-22 12:00:28 root] (abq_llm.py 321): INFO layer 2 iter 3 loss:0.149089977145195 norm:0.01602974347770214 max memory_allocated 22889.51318359375 
[2024-12-22 12:01:00 root] (abq_llm.py 321): INFO layer 2 iter 4 loss:0.14442291855812073 norm:0.012308460660278797 max memory_allocated 22889.51318359375 
[2024-12-22 12:01:31 root] (abq_llm.py 321): INFO layer 2 iter 5 loss:0.1412423849105835 norm:0.00955270603299141 max memory_allocated 22889.51318359375 
[2024-12-22 12:02:03 root] (abq_llm.py 321): INFO layer 2 iter 6 loss:0.13924573361873627 norm:0.007668088655918837 max memory_allocated 22889.51318359375 
[2024-12-22 12:02:35 root] (abq_llm.py 321): INFO layer 2 iter 7 loss:0.1380552053451538 norm:0.006823045201599598 max memory_allocated 22889.51318359375 
[2024-12-22 12:03:06 root] (abq_llm.py 321): INFO layer 2 iter 8 loss:0.1373138576745987 norm:0.006253982428461313 max memory_allocated 22889.51318359375 
[2024-12-22 12:03:38 root] (abq_llm.py 321): INFO layer 2 iter 9 loss:0.1368570625782013 norm:0.005959004163742065 max memory_allocated 22889.51318359375 
[2024-12-22 12:04:10 root] (abq_llm.py 321): INFO layer 2 iter 10 loss:0.13653133809566498 norm:0.005672458093613386 max memory_allocated 22889.51318359375 
[2024-12-22 12:04:41 root] (abq_llm.py 321): INFO layer 2 iter 11 loss:0.1363900899887085 norm:0.005555932410061359 max memory_allocated 22889.51318359375 
[2024-12-22 12:05:13 root] (abq_llm.py 321): INFO layer 2 iter 12 loss:0.13616128265857697 norm:0.005455432925373316 max memory_allocated 22889.51318359375 
[2024-12-22 12:05:44 root] (abq_llm.py 321): INFO layer 2 iter 13 loss:0.13595691323280334 norm:0.005312442779541016 max memory_allocated 22889.51318359375 
[2024-12-22 12:06:16 root] (abq_llm.py 321): INFO layer 2 iter 14 loss:0.1358591616153717 norm:0.0052244411781430244 max memory_allocated 22889.51318359375 
[2024-12-22 12:06:48 root] (abq_llm.py 321): INFO layer 2 iter 15 loss:0.13569918274879456 norm:0.00495324470102787 max memory_allocated 22889.51318359375 
[2024-12-22 12:07:19 root] (abq_llm.py 321): INFO layer 2 iter 16 loss:0.13556654751300812 norm:0.004702896345406771 max memory_allocated 22889.51318359375 
[2024-12-22 12:07:51 root] (abq_llm.py 321): INFO layer 2 iter 17 loss:0.13544143736362457 norm:0.004583104979246855 max memory_allocated 22889.51318359375 
[2024-12-22 12:08:22 root] (abq_llm.py 321): INFO layer 2 iter 18 loss:0.1354268193244934 norm:0.004688001237809658 max memory_allocated 22889.51318359375 
[2024-12-22 12:08:54 root] (abq_llm.py 321): INFO layer 2 iter 19 loss:0.13521626591682434 norm:0.004570996388792992 max memory_allocated 22889.51318359375 
[2024-12-22 12:09:03 root] (abq_llm.py 208): INFO === Start quantize layer 3 ===
[2024-12-22 12:09:37 root] (abq_llm.py 321): INFO layer 3 iter 0 loss:0.26920339465141296 norm:0.027582116425037384 max memory_allocated 22891.06982421875 
[2024-12-22 12:10:08 root] (abq_llm.py 321): INFO layer 3 iter 1 loss:0.23142598569393158 norm:0.010503783822059631 max memory_allocated 22891.06982421875 
[2024-12-22 12:10:40 root] (abq_llm.py 321): INFO layer 3 iter 2 loss:0.20830371975898743 norm:0.006055500824004412 max memory_allocated 22891.06982421875 
[2024-12-22 12:11:11 root] (abq_llm.py 321): INFO layer 3 iter 3 loss:0.19860628247261047 norm:0.004306526854634285 max memory_allocated 22891.06982421875 
[2024-12-22 12:11:43 root] (abq_llm.py 321): INFO layer 3 iter 4 loss:0.19456687569618225 norm:0.004070287570357323 max memory_allocated 22891.06982421875 
[2024-12-22 12:12:14 root] (abq_llm.py 321): INFO layer 3 iter 5 loss:0.19176092743873596 norm:0.003592434572055936 max memory_allocated 22891.06982421875 
[2024-12-22 12:12:46 root] (abq_llm.py 321): INFO layer 3 iter 6 loss:0.18975749611854553 norm:0.0031537471804767847 max memory_allocated 22891.06982421875 
[2024-12-22 12:13:17 root] (abq_llm.py 321): INFO layer 3 iter 7 loss:0.18853090703487396 norm:0.0030387723818421364 max memory_allocated 22891.06982421875 
[2024-12-22 12:13:49 root] (abq_llm.py 321): INFO layer 3 iter 8 loss:0.18786151707172394 norm:0.002971161622554064 max memory_allocated 22891.06982421875 
[2024-12-22 12:14:20 root] (abq_llm.py 321): INFO layer 3 iter 9 loss:0.1874580681324005 norm:0.0028648260049521923 max memory_allocated 22891.06982421875 
[2024-12-22 12:14:52 root] (abq_llm.py 321): INFO layer 3 iter 10 loss:0.18717332184314728 norm:0.0027157608419656754 max memory_allocated 22891.06982421875 
[2024-12-22 12:15:23 root] (abq_llm.py 321): INFO layer 3 iter 11 loss:0.1867990344762802 norm:0.002672553528100252 max memory_allocated 22891.06982421875 
[2024-12-22 12:15:55 root] (abq_llm.py 321): INFO layer 3 iter 12 loss:0.18675915896892548 norm:0.0026485943235456944 max memory_allocated 22891.06982421875 
[2024-12-22 12:16:26 root] (abq_llm.py 321): INFO layer 3 iter 13 loss:0.18656885623931885 norm:0.002649865346029401 max memory_allocated 22891.06982421875 
[2024-12-22 12:16:58 root] (abq_llm.py 321): INFO layer 3 iter 14 loss:0.18634675443172455 norm:0.0025447853840887547 max memory_allocated 22891.06982421875 
[2024-12-22 12:17:29 root] (abq_llm.py 321): INFO layer 3 iter 15 loss:0.18612056970596313 norm:0.0025922353379428387 max memory_allocated 22891.06982421875 
[2024-12-22 12:18:01 root] (abq_llm.py 321): INFO layer 3 iter 16 loss:0.18608355522155762 norm:0.0026362838689237833 max memory_allocated 22891.06982421875 
[2024-12-22 12:18:32 root] (abq_llm.py 321): INFO layer 3 iter 17 loss:0.18606996536254883 norm:0.0025386365596204996 max memory_allocated 22891.06982421875 
[2024-12-22 12:19:04 root] (abq_llm.py 321): INFO layer 3 iter 18 loss:0.1858179122209549 norm:0.0026376480236649513 max memory_allocated 22891.06982421875 
[2024-12-22 12:19:35 root] (abq_llm.py 321): INFO layer 3 iter 19 loss:0.18576714396476746 norm:0.0027098595164716244 max memory_allocated 22891.06982421875 
[2024-12-22 12:19:44 root] (abq_llm.py 208): INFO === Start quantize layer 4 ===
[2024-12-22 12:20:18 root] (abq_llm.py 321): INFO layer 4 iter 0 loss:0.3461564779281616 norm:0.043373752385377884 max memory_allocated 22892.74169921875 
[2024-12-22 12:20:50 root] (abq_llm.py 321): INFO layer 4 iter 1 loss:0.2940695881843567 norm:0.014459901489317417 max memory_allocated 22892.74169921875 
[2024-12-22 12:21:21 root] (abq_llm.py 321): INFO layer 4 iter 2 loss:0.25905248522758484 norm:0.007228851784020662 max memory_allocated 22892.74169921875 
[2024-12-22 12:21:53 root] (abq_llm.py 321): INFO layer 4 iter 3 loss:0.2463427186012268 norm:0.004979659337550402 max memory_allocated 22892.74169921875 
[2024-12-22 12:22:24 root] (abq_llm.py 321): INFO layer 4 iter 4 loss:0.24021217226982117 norm:0.003931532613933086 max memory_allocated 22892.74169921875 
[2024-12-22 12:22:56 root] (abq_llm.py 321): INFO layer 4 iter 5 loss:0.23668096959590912 norm:0.003575659357011318 max memory_allocated 22892.74169921875 
[2024-12-22 12:23:27 root] (abq_llm.py 321): INFO layer 4 iter 6 loss:0.23429742455482483 norm:0.0032325447537004948 max memory_allocated 22892.74169921875 
[2024-12-22 12:23:59 root] (abq_llm.py 321): INFO layer 4 iter 7 loss:0.2324620634317398 norm:0.003031083382666111 max memory_allocated 22892.74169921875 
[2024-12-22 12:24:30 root] (abq_llm.py 321): INFO layer 4 iter 8 loss:0.2312534898519516 norm:0.003029916901141405 max memory_allocated 22892.74169921875 
[2024-12-22 12:25:02 root] (abq_llm.py 321): INFO layer 4 iter 9 loss:0.23046506941318512 norm:0.0028694288339465857 max memory_allocated 22892.74169921875 
[2024-12-22 12:25:34 root] (abq_llm.py 321): INFO layer 4 iter 10 loss:0.22983774542808533 norm:0.0027644659858196974 max memory_allocated 22892.74169921875 
[2024-12-22 12:26:05 root] (abq_llm.py 321): INFO layer 4 iter 11 loss:0.22943896055221558 norm:0.0027331113815307617 max memory_allocated 22892.74169921875 
[2024-12-22 12:26:37 root] (abq_llm.py 321): INFO layer 4 iter 12 loss:0.22894006967544556 norm:0.0026544025167822838 max memory_allocated 22892.74169921875 
[2024-12-22 12:27:08 root] (abq_llm.py 321): INFO layer 4 iter 13 loss:0.2286837249994278 norm:0.002599588828161359 max memory_allocated 22892.74169921875 
[2024-12-22 12:27:40 root] (abq_llm.py 321): INFO layer 4 iter 14 loss:0.22832565009593964 norm:0.0025802110321819782 max memory_allocated 22892.74169921875 
[2024-12-22 12:28:11 root] (abq_llm.py 321): INFO layer 4 iter 15 loss:0.2280726432800293 norm:0.00253981351852417 max memory_allocated 22892.74169921875 
[2024-12-22 12:28:43 root] (abq_llm.py 321): INFO layer 4 iter 16 loss:0.22782209515571594 norm:0.002613002434372902 max memory_allocated 22892.74169921875 
[2024-12-22 12:29:14 root] (abq_llm.py 321): INFO layer 4 iter 17 loss:0.22759386897087097 norm:0.0025256224907934666 max memory_allocated 22892.74169921875 
[2024-12-22 12:29:46 root] (abq_llm.py 321): INFO layer 4 iter 18 loss:0.22765156626701355 norm:0.0024956115521490574 max memory_allocated 22892.74169921875 
[2024-12-22 12:30:17 root] (abq_llm.py 321): INFO layer 4 iter 19 loss:0.22766238451004028 norm:0.002505179261788726 max memory_allocated 22892.74169921875 
[2024-12-22 12:30:26 root] (abq_llm.py 208): INFO === Start quantize layer 5 ===
[2024-12-22 12:31:00 root] (abq_llm.py 321): INFO layer 5 iter 0 loss:0.36250633001327515 norm:0.02051350846886635 max memory_allocated 22894.41357421875 
[2024-12-22 12:31:32 root] (abq_llm.py 321): INFO layer 5 iter 1 loss:0.3228375315666199 norm:0.009204515255987644 max memory_allocated 22894.41357421875 
[2024-12-22 12:32:03 root] (abq_llm.py 321): INFO layer 5 iter 2 loss:0.29220661520957947 norm:0.005423814989626408 max memory_allocated 22894.41357421875 
[2024-12-22 12:32:35 root] (abq_llm.py 321): INFO layer 5 iter 3 loss:0.27958065271377563 norm:0.004290991462767124 max memory_allocated 22894.41357421875 
[2024-12-22 12:33:06 root] (abq_llm.py 321): INFO layer 5 iter 4 loss:0.27387887239456177 norm:0.003843293059617281 max memory_allocated 22894.41357421875 
[2024-12-22 12:33:38 root] (abq_llm.py 321): INFO layer 5 iter 5 loss:0.27027633786201477 norm:0.003644339507445693 max memory_allocated 22894.41357421875 
[2024-12-22 12:34:09 root] (abq_llm.py 321): INFO layer 5 iter 6 loss:0.26810935139656067 norm:0.003400729736313224 max memory_allocated 22894.41357421875 
[2024-12-22 12:34:41 root] (abq_llm.py 321): INFO layer 5 iter 7 loss:0.26676586270332336 norm:0.0031806339975446463 max memory_allocated 22894.41357421875 
[2024-12-22 12:35:12 root] (abq_llm.py 321): INFO layer 5 iter 8 loss:0.26566922664642334 norm:0.0030928351916372776 max memory_allocated 22894.41357421875 
[2024-12-22 12:35:44 root] (abq_llm.py 321): INFO layer 5 iter 9 loss:0.26516810059547424 norm:0.0030378466472029686 max memory_allocated 22894.41357421875 
[2024-12-22 12:36:15 root] (abq_llm.py 321): INFO layer 5 iter 10 loss:0.26464396715164185 norm:0.003021321026608348 max memory_allocated 22894.41357421875 
[2024-12-22 12:36:47 root] (abq_llm.py 321): INFO layer 5 iter 11 loss:0.2642185389995575 norm:0.0030773950275033712 max memory_allocated 22894.41357421875 
[2024-12-22 12:37:18 root] (abq_llm.py 321): INFO layer 5 iter 12 loss:0.26379328966140747 norm:0.0029738314915448427 max memory_allocated 22894.41357421875 
[2024-12-22 12:37:50 root] (abq_llm.py 321): INFO layer 5 iter 13 loss:0.2635442018508911 norm:0.0029437586199492216 max memory_allocated 22894.41357421875 
[2024-12-22 12:38:21 root] (abq_llm.py 321): INFO layer 5 iter 14 loss:0.2631293535232544 norm:0.0029460813384503126 max memory_allocated 22894.41357421875 
[2024-12-22 12:38:53 root] (abq_llm.py 321): INFO layer 5 iter 15 loss:0.26292258501052856 norm:0.0028676704969257116 max memory_allocated 22894.41357421875 
[2024-12-22 12:39:24 root] (abq_llm.py 321): INFO layer 5 iter 16 loss:0.26278647780418396 norm:0.0028728581964969635 max memory_allocated 22894.41357421875 
[2024-12-22 12:39:56 root] (abq_llm.py 321): INFO layer 5 iter 17 loss:0.2626821994781494 norm:0.0027998301666229963 max memory_allocated 22894.41357421875 
[2024-12-22 12:40:27 root] (abq_llm.py 321): INFO layer 5 iter 18 loss:0.26256805658340454 norm:0.002819565823301673 max memory_allocated 22894.41357421875 
[2024-12-22 12:40:59 root] (abq_llm.py 321): INFO layer 5 iter 19 loss:0.26241534948349 norm:0.0028704768046736717 max memory_allocated 22894.41357421875 
[2024-12-22 12:41:07 root] (abq_llm.py 208): INFO === Start quantize layer 6 ===
[2024-12-22 12:41:41 root] (abq_llm.py 321): INFO layer 6 iter 0 loss:0.4483346939086914 norm:0.04431675374507904 max memory_allocated 22896.08544921875 
[2024-12-22 12:42:13 root] (abq_llm.py 321): INFO layer 6 iter 1 loss:0.38674449920654297 norm:0.014945625327527523 max memory_allocated 22896.08544921875 
[2024-12-22 12:42:44 root] (abq_llm.py 321): INFO layer 6 iter 2 loss:0.35049116611480713 norm:0.008823059499263763 max memory_allocated 22896.08544921875 
[2024-12-22 12:43:16 root] (abq_llm.py 321): INFO layer 6 iter 3 loss:0.33156806230545044 norm:0.006204697303473949 max memory_allocated 22896.08544921875 
[2024-12-22 12:43:48 root] (abq_llm.py 321): INFO layer 6 iter 4 loss:0.3233017027378082 norm:0.0052245743572711945 max memory_allocated 22896.08544921875 
[2024-12-22 12:44:19 root] (abq_llm.py 321): INFO layer 6 iter 5 loss:0.3181132972240448 norm:0.004884006455540657 max memory_allocated 22896.08544921875 
[2024-12-22 12:44:51 root] (abq_llm.py 321): INFO layer 6 iter 6 loss:0.3147159516811371 norm:0.004533100873231888 max memory_allocated 22896.08544921875 
[2024-12-22 12:45:22 root] (abq_llm.py 321): INFO layer 6 iter 7 loss:0.31230396032333374 norm:0.004263435490429401 max memory_allocated 22896.08544921875 
[2024-12-22 12:45:54 root] (abq_llm.py 321): INFO layer 6 iter 8 loss:0.31066107749938965 norm:0.0041083358228206635 max memory_allocated 22896.08544921875 
[2024-12-22 12:46:25 root] (abq_llm.py 321): INFO layer 6 iter 9 loss:0.30917495489120483 norm:0.0038737028371542692 max memory_allocated 22896.08544921875 
[2024-12-22 12:46:57 root] (abq_llm.py 321): INFO layer 6 iter 10 loss:0.3083295524120331 norm:0.003731359029188752 max memory_allocated 22896.08544921875 
[2024-12-22 12:47:28 root] (abq_llm.py 321): INFO layer 6 iter 11 loss:0.30766358971595764 norm:0.003628451842814684 max memory_allocated 22896.08544921875 
[2024-12-22 12:48:00 root] (abq_llm.py 321): INFO layer 6 iter 12 loss:0.30732905864715576 norm:0.0036884010769426823 max memory_allocated 22896.08544921875 
[2024-12-22 12:48:31 root] (abq_llm.py 321): INFO layer 6 iter 13 loss:0.30704212188720703 norm:0.0035927484277635813 max memory_allocated 22896.08544921875 
[2024-12-22 12:49:03 root] (abq_llm.py 321): INFO layer 6 iter 14 loss:0.3067680597305298 norm:0.003527655266225338 max memory_allocated 22896.08544921875 
[2024-12-22 12:49:34 root] (abq_llm.py 321): INFO layer 6 iter 15 loss:0.306473046541214 norm:0.003508003894239664 max memory_allocated 22896.08544921875 
[2024-12-22 12:50:06 root] (abq_llm.py 321): INFO layer 6 iter 16 loss:0.30643805861473083 norm:0.0035840764176100492 max memory_allocated 22896.08544921875 
[2024-12-22 12:50:37 root] (abq_llm.py 321): INFO layer 6 iter 17 loss:0.30622154474258423 norm:0.003651465754956007 max memory_allocated 22896.08544921875 
[2024-12-22 12:51:09 root] (abq_llm.py 321): INFO layer 6 iter 18 loss:0.3060368299484253 norm:0.003558668540790677 max memory_allocated 22896.08544921875 
[2024-12-22 12:51:40 root] (abq_llm.py 321): INFO layer 6 iter 19 loss:0.30608901381492615 norm:0.0035163969732820988 max memory_allocated 22896.08544921875 
[2024-12-22 12:51:49 root] (abq_llm.py 208): INFO === Start quantize layer 7 ===
[2024-12-22 12:52:23 root] (abq_llm.py 321): INFO layer 7 iter 0 loss:0.4847843050956726 norm:0.038710132241249084 max memory_allocated 22897.75732421875 
[2024-12-22 12:52:55 root] (abq_llm.py 321): INFO layer 7 iter 1 loss:0.4200890362262726 norm:0.013244357891380787 max memory_allocated 22897.75732421875 
[2024-12-22 12:53:26 root] (abq_llm.py 321): INFO layer 7 iter 2 loss:0.38099366426467896 norm:0.008014613762497902 max memory_allocated 22897.75732421875 
[2024-12-22 12:53:58 root] (abq_llm.py 321): INFO layer 7 iter 3 loss:0.3634522259235382 norm:0.005828997120261192 max memory_allocated 22897.75732421875 
[2024-12-22 12:54:29 root] (abq_llm.py 321): INFO layer 7 iter 4 loss:0.3559114933013916 norm:0.005022841040045023 max memory_allocated 22897.75732421875 
[2024-12-22 12:55:01 root] (abq_llm.py 321): INFO layer 7 iter 5 loss:0.3509623408317566 norm:0.004453857894986868 max memory_allocated 22897.75732421875 
[2024-12-22 12:55:32 root] (abq_llm.py 321): INFO layer 7 iter 6 loss:0.34822559356689453 norm:0.0043478915467858315 max memory_allocated 22897.75732421875 
[2024-12-22 12:56:04 root] (abq_llm.py 321): INFO layer 7 iter 7 loss:0.34597909450531006 norm:0.0040928665548563 max memory_allocated 22897.75732421875 
[2024-12-22 12:56:35 root] (abq_llm.py 321): INFO layer 7 iter 8 loss:0.3442467451095581 norm:0.003988015931099653 max memory_allocated 22897.75732421875 
[2024-12-22 12:57:07 root] (abq_llm.py 321): INFO layer 7 iter 9 loss:0.3428630530834198 norm:0.0038142241537570953 max memory_allocated 22897.75732421875 
[2024-12-22 12:57:38 root] (abq_llm.py 321): INFO layer 7 iter 10 loss:0.3419368863105774 norm:0.0035741173196583986 max memory_allocated 22897.75732421875 
[2024-12-22 12:58:10 root] (abq_llm.py 321): INFO layer 7 iter 11 loss:0.34117668867111206 norm:0.0034725142177194357 max memory_allocated 22897.75732421875 
[2024-12-22 12:58:41 root] (abq_llm.py 321): INFO layer 7 iter 12 loss:0.3407757580280304 norm:0.003509806701913476 max memory_allocated 22897.75732421875 
[2024-12-22 12:59:13 root] (abq_llm.py 321): INFO layer 7 iter 13 loss:0.3402000367641449 norm:0.0033257019240409136 max memory_allocated 22897.75732421875 
[2024-12-22 12:59:44 root] (abq_llm.py 321): INFO layer 7 iter 14 loss:0.3398735821247101 norm:0.0033290167339146137 max memory_allocated 22897.75732421875 
[2024-12-22 13:00:16 root] (abq_llm.py 321): INFO layer 7 iter 15 loss:0.3395468294620514 norm:0.0032626017928123474 max memory_allocated 22897.75732421875 
[2024-12-22 13:00:47 root] (abq_llm.py 321): INFO layer 7 iter 16 loss:0.33926621079444885 norm:0.0031774654053151608 max memory_allocated 22897.75732421875 
[2024-12-22 13:01:19 root] (abq_llm.py 321): INFO layer 7 iter 17 loss:0.33898621797561646 norm:0.0030763207469135523 max memory_allocated 22897.75732421875 
[2024-12-22 13:01:50 root] (abq_llm.py 321): INFO layer 7 iter 18 loss:0.33879244327545166 norm:0.0030463829170912504 max memory_allocated 22897.75732421875 
[2024-12-22 13:02:22 root] (abq_llm.py 321): INFO layer 7 iter 19 loss:0.33893540501594543 norm:0.0030899893026798964 max memory_allocated 22897.75732421875 
[2024-12-22 13:02:31 root] (abq_llm.py 208): INFO === Start quantize layer 8 ===
[2024-12-22 13:03:05 root] (abq_llm.py 321): INFO layer 8 iter 0 loss:0.49749043583869934 norm:0.025637226179242134 max memory_allocated 22899.42919921875 
[2024-12-22 13:03:36 root] (abq_llm.py 321): INFO layer 8 iter 1 loss:0.44583410024642944 norm:0.010600966401398182 max memory_allocated 22899.42919921875 
[2024-12-22 13:04:08 root] (abq_llm.py 321): INFO layer 8 iter 2 loss:0.40613919496536255 norm:0.006261810194700956 max memory_allocated 22899.42919921875 
[2024-12-22 13:04:39 root] (abq_llm.py 321): INFO layer 8 iter 3 loss:0.3882461488246918 norm:0.004548888187855482 max memory_allocated 22899.42919921875 
[2024-12-22 13:05:11 root] (abq_llm.py 321): INFO layer 8 iter 4 loss:0.38081085681915283 norm:0.004242092836648226 max memory_allocated 22899.42919921875 
[2024-12-22 13:05:42 root] (abq_llm.py 321): INFO layer 8 iter 5 loss:0.37633761763572693 norm:0.003952122293412685 max memory_allocated 22899.42919921875 
[2024-12-22 13:06:14 root] (abq_llm.py 321): INFO layer 8 iter 6 loss:0.3731319010257721 norm:0.003691322635859251 max memory_allocated 22899.42919921875 
[2024-12-22 13:06:45 root] (abq_llm.py 321): INFO layer 8 iter 7 loss:0.3710024356842041 norm:0.003507812274619937 max memory_allocated 22899.42919921875 
[2024-12-22 13:07:17 root] (abq_llm.py 321): INFO layer 8 iter 8 loss:0.3696911036968231 norm:0.0033769335132092237 max memory_allocated 22899.42919921875 
[2024-12-22 13:07:48 root] (abq_llm.py 321): INFO layer 8 iter 9 loss:0.3685992956161499 norm:0.0032592597417533398 max memory_allocated 22899.42919921875 
[2024-12-22 13:08:20 root] (abq_llm.py 321): INFO layer 8 iter 10 loss:0.36777961254119873 norm:0.0031531681306660175 max memory_allocated 22899.42919921875 
[2024-12-22 13:08:51 root] (abq_llm.py 321): INFO layer 8 iter 11 loss:0.36722710728645325 norm:0.003108479082584381 max memory_allocated 22899.42919921875 
[2024-12-22 13:09:23 root] (abq_llm.py 321): INFO layer 8 iter 12 loss:0.3668578863143921 norm:0.00304485228843987 max memory_allocated 22899.42919921875 
[2024-12-22 13:09:54 root] (abq_llm.py 321): INFO layer 8 iter 13 loss:0.3664713203907013 norm:0.003053368767723441 max memory_allocated 22899.42919921875 
[2024-12-22 13:10:26 root] (abq_llm.py 321): INFO layer 8 iter 14 loss:0.3661309778690338 norm:0.0030247815884649754 max memory_allocated 22899.42919921875 
[2024-12-22 13:10:57 root] (abq_llm.py 321): INFO layer 8 iter 15 loss:0.36596688628196716 norm:0.003112827893346548 max memory_allocated 22899.42919921875 
[2024-12-22 13:11:29 root] (abq_llm.py 321): INFO layer 8 iter 16 loss:0.3658128082752228 norm:0.0030778776854276657 max memory_allocated 22899.42919921875 
[2024-12-22 13:12:01 root] (abq_llm.py 321): INFO layer 8 iter 17 loss:0.3656547963619232 norm:0.003002314595505595 max memory_allocated 22899.42919921875 
[2024-12-22 13:12:32 root] (abq_llm.py 321): INFO layer 8 iter 18 loss:0.36556047201156616 norm:0.0030795675702393055 max memory_allocated 22899.42919921875 
[2024-12-22 13:13:04 root] (abq_llm.py 321): INFO layer 8 iter 19 loss:0.3655063807964325 norm:0.0030067439656704664 max memory_allocated 22899.42919921875 
[2024-12-22 13:13:12 root] (abq_llm.py 208): INFO === Start quantize layer 9 ===
[2024-12-22 13:13:47 root] (abq_llm.py 321): INFO layer 9 iter 0 loss:0.5336577296257019 norm:0.03760136291384697 max memory_allocated 22901.10107421875 
[2024-12-22 13:14:18 root] (abq_llm.py 321): INFO layer 9 iter 1 loss:0.4674132466316223 norm:0.013030060566961765 max memory_allocated 22901.10107421875 
[2024-12-22 13:14:50 root] (abq_llm.py 321): INFO layer 9 iter 2 loss:0.42495936155319214 norm:0.006390259601175785 max memory_allocated 22901.10107421875 
[2024-12-22 13:15:21 root] (abq_llm.py 321): INFO layer 9 iter 3 loss:0.4075177311897278 norm:0.004328281152993441 max memory_allocated 22901.10107421875 
[2024-12-22 13:15:53 root] (abq_llm.py 321): INFO layer 9 iter 4 loss:0.4004060924053192 norm:0.003808649955317378 max memory_allocated 22901.10107421875 
[2024-12-22 13:16:24 root] (abq_llm.py 321): INFO layer 9 iter 5 loss:0.3956892788410187 norm:0.0035240016877651215 max memory_allocated 22901.10107421875 
[2024-12-22 13:16:56 root] (abq_llm.py 321): INFO layer 9 iter 6 loss:0.3921326696872711 norm:0.003253206145018339 max memory_allocated 22901.10107421875 
[2024-12-22 13:17:27 root] (abq_llm.py 321): INFO layer 9 iter 7 loss:0.38991478085517883 norm:0.0031488356180489063 max memory_allocated 22901.10107421875 
[2024-12-22 13:17:59 root] (abq_llm.py 321): INFO layer 9 iter 8 loss:0.3884384036064148 norm:0.0030287345871329308 max memory_allocated 22901.10107421875 
[2024-12-22 13:18:30 root] (abq_llm.py 321): INFO layer 9 iter 9 loss:0.38745835423469543 norm:0.0028890499379485846 max memory_allocated 22901.10107421875 
[2024-12-22 13:19:02 root] (abq_llm.py 321): INFO layer 9 iter 10 loss:0.3866969347000122 norm:0.0029277994763106108 max memory_allocated 22901.10107421875 
[2024-12-22 13:19:33 root] (abq_llm.py 321): INFO layer 9 iter 11 loss:0.38609784841537476 norm:0.0028185576666146517 max memory_allocated 22901.10107421875 
[2024-12-22 13:20:05 root] (abq_llm.py 321): INFO layer 9 iter 12 loss:0.385461688041687 norm:0.002806737320497632 max memory_allocated 22901.10107421875 
[2024-12-22 13:20:36 root] (abq_llm.py 321): INFO layer 9 iter 13 loss:0.3849746286869049 norm:0.002708718180656433 max memory_allocated 22901.10107421875 
[2024-12-22 13:21:08 root] (abq_llm.py 321): INFO layer 9 iter 14 loss:0.3846950829029083 norm:0.0026990545447915792 max memory_allocated 22901.10107421875 
[2024-12-22 13:21:39 root] (abq_llm.py 321): INFO layer 9 iter 15 loss:0.3843918740749359 norm:0.002674051094800234 max memory_allocated 22901.10107421875 
[2024-12-22 13:22:11 root] (abq_llm.py 321): INFO layer 9 iter 16 loss:0.38411128520965576 norm:0.0026192637160420418 max memory_allocated 22901.10107421875 
[2024-12-22 13:22:42 root] (abq_llm.py 321): INFO layer 9 iter 17 loss:0.38372695446014404 norm:0.0025883589405566454 max memory_allocated 22901.10107421875 
[2024-12-22 13:23:14 root] (abq_llm.py 321): INFO layer 9 iter 18 loss:0.38349664211273193 norm:0.0025772417429834604 max memory_allocated 22901.10107421875 
[2024-12-22 13:23:45 root] (abq_llm.py 321): INFO layer 9 iter 19 loss:0.3833645284175873 norm:0.0025006579235196114 max memory_allocated 22901.10107421875 
[2024-12-22 13:23:54 root] (abq_llm.py 208): INFO === Start quantize layer 10 ===
[2024-12-22 13:24:28 root] (abq_llm.py 321): INFO layer 10 iter 0 loss:0.5170170068740845 norm:0.02925707958638668 max memory_allocated 22902.77294921875 
[2024-12-22 13:25:00 root] (abq_llm.py 321): INFO layer 10 iter 1 loss:0.4732622504234314 norm:0.012292670086026192 max memory_allocated 22902.77294921875 
[2024-12-22 13:25:31 root] (abq_llm.py 321): INFO layer 10 iter 2 loss:0.43688109517097473 norm:0.006405945867300034 max memory_allocated 22902.77294921875 
[2024-12-22 13:26:03 root] (abq_llm.py 321): INFO layer 10 iter 3 loss:0.41689392924308777 norm:0.0035227960906922817 max memory_allocated 22902.77294921875 
[2024-12-22 13:26:34 root] (abq_llm.py 321): INFO layer 10 iter 4 loss:0.40857675671577454 norm:0.003056999295949936 max memory_allocated 22902.77294921875 
[2024-12-22 13:27:06 root] (abq_llm.py 321): INFO layer 10 iter 5 loss:0.403272807598114 norm:0.0027957800775766373 max memory_allocated 22902.77294921875 
[2024-12-22 13:27:37 root] (abq_llm.py 321): INFO layer 10 iter 6 loss:0.39970293641090393 norm:0.0026655704714357853 max memory_allocated 22902.77294921875 
[2024-12-22 13:28:09 root] (abq_llm.py 321): INFO layer 10 iter 7 loss:0.3973023295402527 norm:0.0025567414704710245 max memory_allocated 22902.77294921875 
[2024-12-22 13:28:41 root] (abq_llm.py 321): INFO layer 10 iter 8 loss:0.3955913484096527 norm:0.0024656064342707396 max memory_allocated 22902.77294921875 
[2024-12-22 13:29:12 root] (abq_llm.py 321): INFO layer 10 iter 9 loss:0.3944587707519531 norm:0.0024025333113968372 max memory_allocated 22902.77294921875 
[2024-12-22 13:29:44 root] (abq_llm.py 321): INFO layer 10 iter 10 loss:0.39366617798805237 norm:0.0023459340445697308 max memory_allocated 22902.77294921875 
[2024-12-22 13:30:15 root] (abq_llm.py 321): INFO layer 10 iter 11 loss:0.39291515946388245 norm:0.0023324789945036173 max memory_allocated 22902.77294921875 
[2024-12-22 13:30:47 root] (abq_llm.py 321): INFO layer 10 iter 12 loss:0.39245662093162537 norm:0.0023625220637768507 max memory_allocated 22902.77294921875 
[2024-12-22 13:31:18 root] (abq_llm.py 321): INFO layer 10 iter 13 loss:0.3920381963253021 norm:0.002358473604544997 max memory_allocated 22902.77294921875 
[2024-12-22 13:31:50 root] (abq_llm.py 321): INFO layer 10 iter 14 loss:0.3916550278663635 norm:0.002331443829461932 max memory_allocated 22902.77294921875 
[2024-12-22 13:32:21 root] (abq_llm.py 321): INFO layer 10 iter 15 loss:0.39139795303344727 norm:0.0022851931862533092 max memory_allocated 22902.77294921875 
[2024-12-22 13:32:53 root] (abq_llm.py 321): INFO layer 10 iter 16 loss:0.3912835121154785 norm:0.002314346143975854 max memory_allocated 22902.77294921875 
[2024-12-22 13:33:24 root] (abq_llm.py 321): INFO layer 10 iter 17 loss:0.39101046323776245 norm:0.0022825472988188267 max memory_allocated 22902.77294921875 
[2024-12-22 13:33:56 root] (abq_llm.py 321): INFO layer 10 iter 18 loss:0.3907836377620697 norm:0.0022621082607656717 max memory_allocated 22902.77294921875 
[2024-12-22 13:34:27 root] (abq_llm.py 321): INFO layer 10 iter 19 loss:0.3906211853027344 norm:0.002203222131356597 max memory_allocated 22902.77294921875 
[2024-12-22 13:34:36 root] (abq_llm.py 208): INFO === Start quantize layer 11 ===
[2024-12-22 13:35:10 root] (abq_llm.py 321): INFO layer 11 iter 0 loss:0.5104389190673828 norm:0.023563973605632782 max memory_allocated 22904.44482421875 
[2024-12-22 13:35:42 root] (abq_llm.py 321): INFO layer 11 iter 1 loss:0.462603896856308 norm:0.009081456810235977 max memory_allocated 22904.44482421875 
[2024-12-22 13:36:13 root] (abq_llm.py 321): INFO layer 11 iter 2 loss:0.4329492747783661 norm:0.006013810634613037 max memory_allocated 22904.44482421875 
[2024-12-22 13:36:45 root] (abq_llm.py 321): INFO layer 11 iter 3 loss:0.4177868068218231 norm:0.004466503392904997 max memory_allocated 22904.44482421875 
[2024-12-22 13:37:16 root] (abq_llm.py 321): INFO layer 11 iter 4 loss:0.41055336594581604 norm:0.0037541096098721027 max memory_allocated 22904.44482421875 
[2024-12-22 13:37:48 root] (abq_llm.py 321): INFO layer 11 iter 5 loss:0.40579891204833984 norm:0.003294270718470216 max memory_allocated 22904.44482421875 
[2024-12-22 13:38:19 root] (abq_llm.py 321): INFO layer 11 iter 6 loss:0.4024913012981415 norm:0.002912526950240135 max memory_allocated 22904.44482421875 
[2024-12-22 13:38:51 root] (abq_llm.py 321): INFO layer 11 iter 7 loss:0.40021640062332153 norm:0.0027589143719524145 max memory_allocated 22904.44482421875 
[2024-12-22 13:39:23 root] (abq_llm.py 321): INFO layer 11 iter 8 loss:0.39868754148483276 norm:0.002507789758965373 max memory_allocated 22904.44482421875 
[2024-12-22 13:39:54 root] (abq_llm.py 321): INFO layer 11 iter 9 loss:0.39750784635543823 norm:0.002449685474857688 max memory_allocated 22904.44482421875 
[2024-12-22 13:40:26 root] (abq_llm.py 321): INFO layer 11 iter 10 loss:0.3967367112636566 norm:0.002303595654666424 max memory_allocated 22904.44482421875 
[2024-12-22 13:40:57 root] (abq_llm.py 321): INFO layer 11 iter 11 loss:0.3962162733078003 norm:0.002306211506947875 max memory_allocated 22904.44482421875 
[2024-12-22 13:41:29 root] (abq_llm.py 321): INFO layer 11 iter 12 loss:0.39576560258865356 norm:0.002256158972159028 max memory_allocated 22904.44482421875 
[2024-12-22 13:42:00 root] (abq_llm.py 321): INFO layer 11 iter 13 loss:0.39527657628059387 norm:0.002248523524031043 max memory_allocated 22904.44482421875 
[2024-12-22 13:42:32 root] (abq_llm.py 321): INFO layer 11 iter 14 loss:0.39504462480545044 norm:0.0021819951944053173 max memory_allocated 22904.44482421875 
[2024-12-22 13:43:03 root] (abq_llm.py 321): INFO layer 11 iter 15 loss:0.3948473036289215 norm:0.0021471972577273846 max memory_allocated 22904.44482421875 
[2024-12-22 13:43:35 root] (abq_llm.py 321): INFO layer 11 iter 16 loss:0.3945847153663635 norm:0.002111124573275447 max memory_allocated 22904.44482421875 
[2024-12-22 13:44:06 root] (abq_llm.py 321): INFO layer 11 iter 17 loss:0.39443305134773254 norm:0.0021135697606951 max memory_allocated 22904.44482421875 
[2024-12-22 13:44:38 root] (abq_llm.py 321): INFO layer 11 iter 18 loss:0.3941967487335205 norm:0.0020462542306631804 max memory_allocated 22904.44482421875 
[2024-12-22 13:45:09 root] (abq_llm.py 321): INFO layer 11 iter 19 loss:0.39409735798835754 norm:0.002094032010063529 max memory_allocated 22904.44482421875 
[2024-12-22 13:45:18 root] (abq_llm.py 208): INFO === Start quantize layer 12 ===
[2024-12-22 13:45:52 root] (abq_llm.py 321): INFO layer 12 iter 0 loss:0.505518913269043 norm:0.013618705794215202 max memory_allocated 22906.11669921875 
[2024-12-22 13:46:24 root] (abq_llm.py 321): INFO layer 12 iter 1 loss:0.46644651889801025 norm:0.006210952531546354 max memory_allocated 22906.11669921875 
[2024-12-22 13:46:55 root] (abq_llm.py 321): INFO layer 12 iter 2 loss:0.4376477599143982 norm:0.003933242987841368 max memory_allocated 22906.11669921875 
[2024-12-22 13:47:27 root] (abq_llm.py 321): INFO layer 12 iter 3 loss:0.42312386631965637 norm:0.003055717796087265 max memory_allocated 22906.11669921875 
[2024-12-22 13:47:58 root] (abq_llm.py 321): INFO layer 12 iter 4 loss:0.4170471131801605 norm:0.002691238187253475 max memory_allocated 22906.11669921875 
[2024-12-22 13:48:30 root] (abq_llm.py 321): INFO layer 12 iter 5 loss:0.41305622458457947 norm:0.0024596117436885834 max memory_allocated 22906.11669921875 
[2024-12-22 13:49:01 root] (abq_llm.py 321): INFO layer 12 iter 6 loss:0.41050639748573303 norm:0.002358457073569298 max memory_allocated 22906.11669921875 
[2024-12-22 13:49:33 root] (abq_llm.py 321): INFO layer 12 iter 7 loss:0.40869802236557007 norm:0.0022625939454883337 max memory_allocated 22906.11669921875 
[2024-12-22 13:50:05 root] (abq_llm.py 321): INFO layer 12 iter 8 loss:0.40739431977272034 norm:0.002131296554580331 max memory_allocated 22906.11669921875 
[2024-12-22 13:50:36 root] (abq_llm.py 321): INFO layer 12 iter 9 loss:0.4064748287200928 norm:0.0021093813702464104 max memory_allocated 22906.11669921875 
[2024-12-22 13:51:08 root] (abq_llm.py 321): INFO layer 12 iter 10 loss:0.40584859251976013 norm:0.002050564158707857 max memory_allocated 22906.11669921875 
[2024-12-22 13:51:39 root] (abq_llm.py 321): INFO layer 12 iter 11 loss:0.40538907051086426 norm:0.002035456709563732 max memory_allocated 22906.11669921875 
[2024-12-22 13:52:11 root] (abq_llm.py 321): INFO layer 12 iter 12 loss:0.40503716468811035 norm:0.0020276554860174656 max memory_allocated 22906.11669921875 
[2024-12-22 13:52:42 root] (abq_llm.py 321): INFO layer 12 iter 13 loss:0.40474289655685425 norm:0.002017337130382657 max memory_allocated 22906.11669921875 
[2024-12-22 13:53:14 root] (abq_llm.py 321): INFO layer 12 iter 14 loss:0.40445348620414734 norm:0.0019557401537895203 max memory_allocated 22906.11669921875 
[2024-12-22 13:53:45 root] (abq_llm.py 321): INFO layer 12 iter 15 loss:0.40430212020874023 norm:0.001930388156324625 max memory_allocated 22906.11669921875 
[2024-12-22 13:54:17 root] (abq_llm.py 321): INFO layer 12 iter 16 loss:0.40413427352905273 norm:0.0018932258244603872 max memory_allocated 22906.11669921875 
[2024-12-22 13:54:48 root] (abq_llm.py 321): INFO layer 12 iter 17 loss:0.4040139615535736 norm:0.0018725378904491663 max memory_allocated 22906.11669921875 
[2024-12-22 13:55:20 root] (abq_llm.py 321): INFO layer 12 iter 18 loss:0.40390467643737793 norm:0.0018701305380091071 max memory_allocated 22906.11669921875 
[2024-12-22 13:55:51 root] (abq_llm.py 321): INFO layer 12 iter 19 loss:0.403785765171051 norm:0.0018747238209471107 max memory_allocated 22906.11669921875 
[2024-12-22 13:56:00 root] (abq_llm.py 208): INFO === Start quantize layer 13 ===
[2024-12-22 13:56:34 root] (abq_llm.py 321): INFO layer 13 iter 0 loss:0.5227159857749939 norm:0.04179782792925835 max memory_allocated 22907.78857421875 
[2024-12-22 13:57:06 root] (abq_llm.py 321): INFO layer 13 iter 1 loss:0.4726889133453369 norm:0.015236232429742813 max memory_allocated 22907.78857421875 
[2024-12-22 13:57:37 root] (abq_llm.py 321): INFO layer 13 iter 2 loss:0.4413394331932068 norm:0.008231015875935555 max memory_allocated 22907.78857421875 
[2024-12-22 13:58:09 root] (abq_llm.py 321): INFO layer 13 iter 3 loss:0.4245077669620514 norm:0.005388915538787842 max memory_allocated 22907.78857421875 
[2024-12-22 13:58:40 root] (abq_llm.py 321): INFO layer 13 iter 4 loss:0.4158898591995239 norm:0.0035875989124178886 max memory_allocated 22907.78857421875 
[2024-12-22 13:59:12 root] (abq_llm.py 321): INFO layer 13 iter 5 loss:0.41105690598487854 norm:0.003100557718425989 max memory_allocated 22907.78857421875 
[2024-12-22 13:59:43 root] (abq_llm.py 321): INFO layer 13 iter 6 loss:0.40789666771888733 norm:0.002951489994302392 max memory_allocated 22907.78857421875 
[2024-12-22 14:00:15 root] (abq_llm.py 321): INFO layer 13 iter 7 loss:0.40547865629196167 norm:0.0027376001235097647 max memory_allocated 22907.78857421875 
[2024-12-22 14:00:46 root] (abq_llm.py 321): INFO layer 13 iter 8 loss:0.4037131667137146 norm:0.0026369427796453238 max memory_allocated 22907.78857421875 
[2024-12-22 14:01:18 root] (abq_llm.py 321): INFO layer 13 iter 9 loss:0.40241265296936035 norm:0.0024445923045277596 max memory_allocated 22907.78857421875 
[2024-12-22 14:01:49 root] (abq_llm.py 321): INFO layer 13 iter 10 loss:0.4015262722969055 norm:0.0024187983945012093 max memory_allocated 22907.78857421875 
[2024-12-22 14:02:21 root] (abq_llm.py 321): INFO layer 13 iter 11 loss:0.40070396661758423 norm:0.0023454148322343826 max memory_allocated 22907.78857421875 
[2024-12-22 14:02:52 root] (abq_llm.py 321): INFO layer 13 iter 12 loss:0.40011075139045715 norm:0.0023692818358540535 max memory_allocated 22907.78857421875 
[2024-12-22 14:03:24 root] (abq_llm.py 321): INFO layer 13 iter 13 loss:0.39965522289276123 norm:0.0022474832367151976 max memory_allocated 22907.78857421875 
[2024-12-22 14:03:55 root] (abq_llm.py 321): INFO layer 13 iter 14 loss:0.3993333876132965 norm:0.0022237251978367567 max memory_allocated 22907.78857421875 
[2024-12-22 14:04:27 root] (abq_llm.py 321): INFO layer 13 iter 15 loss:0.39899370074272156 norm:0.0021572450641542673 max memory_allocated 22907.78857421875 
[2024-12-22 14:04:58 root] (abq_llm.py 321): INFO layer 13 iter 16 loss:0.3987446129322052 norm:0.0021129054948687553 max memory_allocated 22907.78857421875 
[2024-12-22 14:05:30 root] (abq_llm.py 321): INFO layer 13 iter 17 loss:0.3984861373901367 norm:0.002043958054855466 max memory_allocated 22907.78857421875 
[2024-12-22 14:06:02 root] (abq_llm.py 321): INFO layer 13 iter 18 loss:0.39830678701400757 norm:0.00200357916764915 max memory_allocated 22907.78857421875 
[2024-12-22 14:06:33 root] (abq_llm.py 321): INFO layer 13 iter 19 loss:0.3980521857738495 norm:0.001996960723772645 max memory_allocated 22907.78857421875 
[2024-12-22 14:06:42 root] (abq_llm.py 208): INFO === Start quantize layer 14 ===
[2024-12-22 14:07:18 root] (abq_llm.py 321): INFO layer 14 iter 0 loss:0.49783310294151306 norm:0.01633305475115776 max memory_allocated 22909.46044921875 
[2024-12-22 14:07:49 root] (abq_llm.py 321): INFO layer 14 iter 1 loss:0.45869261026382446 norm:0.006210631225258112 max memory_allocated 22909.46044921875 
[2024-12-22 14:08:21 root] (abq_llm.py 321): INFO layer 14 iter 2 loss:0.4328291714191437 norm:0.0036431325133889914 max memory_allocated 22909.46044921875 
[2024-12-22 14:08:52 root] (abq_llm.py 321): INFO layer 14 iter 3 loss:0.4209674596786499 norm:0.0026037292554974556 max memory_allocated 22909.46044921875 
[2024-12-22 14:09:24 root] (abq_llm.py 321): INFO layer 14 iter 4 loss:0.41555535793304443 norm:0.0022479521576315165 max memory_allocated 22909.46044921875 
[2024-12-22 14:09:56 root] (abq_llm.py 321): INFO layer 14 iter 5 loss:0.4119265079498291 norm:0.0020539413671940565 max memory_allocated 22909.46044921875 
[2024-12-22 14:10:27 root] (abq_llm.py 321): INFO layer 14 iter 6 loss:0.4091966152191162 norm:0.0019767379853874445 max memory_allocated 22909.46044921875 
[2024-12-22 14:10:59 root] (abq_llm.py 321): INFO layer 14 iter 7 loss:0.4073546826839447 norm:0.001904608798213303 max memory_allocated 22909.46044921875 
[2024-12-22 14:11:30 root] (abq_llm.py 321): INFO layer 14 iter 8 loss:0.4061218798160553 norm:0.0018439266132190824 max memory_allocated 22909.46044921875 
[2024-12-22 14:12:02 root] (abq_llm.py 321): INFO layer 14 iter 9 loss:0.4052170515060425 norm:0.0018512215465307236 max memory_allocated 22909.46044921875 
[2024-12-22 14:12:33 root] (abq_llm.py 321): INFO layer 14 iter 10 loss:0.40453040599823 norm:0.0017921860562637448 max memory_allocated 22909.46044921875 
[2024-12-22 14:13:05 root] (abq_llm.py 321): INFO layer 14 iter 11 loss:0.40390804409980774 norm:0.0017321007326245308 max memory_allocated 22909.46044921875 
[2024-12-22 14:13:36 root] (abq_llm.py 321): INFO layer 14 iter 12 loss:0.4034927487373352 norm:0.0017412672750651836 max memory_allocated 22909.46044921875 
[2024-12-22 14:14:08 root] (abq_llm.py 321): INFO layer 14 iter 13 loss:0.4030883312225342 norm:0.0017146642785519361 max memory_allocated 22909.46044921875 
[2024-12-22 14:14:39 root] (abq_llm.py 321): INFO layer 14 iter 14 loss:0.4027163088321686 norm:0.0017098793759942055 max memory_allocated 22909.46044921875 
[2024-12-22 14:15:11 root] (abq_llm.py 321): INFO layer 14 iter 15 loss:0.4023750126361847 norm:0.0017244883347302675 max memory_allocated 22909.46044921875 
[2024-12-22 14:15:43 root] (abq_llm.py 321): INFO layer 14 iter 16 loss:0.40215766429901123 norm:0.0016965731047093868 max memory_allocated 22909.46044921875 
[2024-12-22 14:16:14 root] (abq_llm.py 321): INFO layer 14 iter 17 loss:0.4019949436187744 norm:0.0016803046455606818 max memory_allocated 22909.46044921875 
[2024-12-22 14:16:46 root] (abq_llm.py 321): INFO layer 14 iter 18 loss:0.4017879366874695 norm:0.0016232881462201476 max memory_allocated 22909.46044921875 
[2024-12-22 14:17:17 root] (abq_llm.py 321): INFO layer 14 iter 19 loss:0.4016949236392975 norm:0.0016475114971399307 max memory_allocated 22909.46044921875 
[2024-12-22 14:17:26 root] (abq_llm.py 208): INFO === Start quantize layer 15 ===
[2024-12-22 14:18:00 root] (abq_llm.py 321): INFO layer 15 iter 0 loss:0.5351485013961792 norm:0.04702087864279747 max memory_allocated 22911.13232421875 
[2024-12-22 14:18:32 root] (abq_llm.py 321): INFO layer 15 iter 1 loss:0.4704093337059021 norm:0.016781024634838104 max memory_allocated 22911.13232421875 
[2024-12-22 14:19:03 root] (abq_llm.py 321): INFO layer 15 iter 2 loss:0.4333263337612152 norm:0.0072937156073749065 max memory_allocated 22911.13232421875 
[2024-12-22 14:19:35 root] (abq_llm.py 321): INFO layer 15 iter 3 loss:0.41842782497406006 norm:0.004949130583554506 max memory_allocated 22911.13232421875 
[2024-12-22 14:20:06 root] (abq_llm.py 321): INFO layer 15 iter 4 loss:0.41177961230278015 norm:0.004020894877612591 max memory_allocated 22911.13232421875 
[2024-12-22 14:20:38 root] (abq_llm.py 321): INFO layer 15 iter 5 loss:0.40766441822052 norm:0.003498022211715579 max memory_allocated 22911.13232421875 
[2024-12-22 14:21:09 root] (abq_llm.py 321): INFO layer 15 iter 6 loss:0.40428999066352844 norm:0.0030714436434209347 max memory_allocated 22911.13232421875 
[2024-12-22 14:21:41 root] (abq_llm.py 321): INFO layer 15 iter 7 loss:0.4019266963005066 norm:0.002837676089257002 max memory_allocated 22911.13232421875 
[2024-12-22 14:22:12 root] (abq_llm.py 321): INFO layer 15 iter 8 loss:0.4003187119960785 norm:0.002789079677313566 max memory_allocated 22911.13232421875 
[2024-12-22 14:22:44 root] (abq_llm.py 321): INFO layer 15 iter 9 loss:0.3992105722427368 norm:0.002672347240149975 max memory_allocated 22911.13232421875 
[2024-12-22 14:23:15 root] (abq_llm.py 321): INFO layer 15 iter 10 loss:0.3982151746749878 norm:0.0025703138671815395 max memory_allocated 22911.13232421875 
[2024-12-22 14:23:47 root] (abq_llm.py 321): INFO layer 15 iter 11 loss:0.39751651883125305 norm:0.002582857385277748 max memory_allocated 22911.13232421875 
[2024-12-22 14:24:18 root] (abq_llm.py 321): INFO layer 15 iter 12 loss:0.3969513475894928 norm:0.0025439911987632513 max memory_allocated 22911.13232421875 
[2024-12-22 14:24:50 root] (abq_llm.py 321): INFO layer 15 iter 13 loss:0.3964375853538513 norm:0.0024207516107708216 max memory_allocated 22911.13232421875 
[2024-12-22 14:25:21 root] (abq_llm.py 321): INFO layer 15 iter 14 loss:0.39603397250175476 norm:0.0023724909406155348 max memory_allocated 22911.13232421875 
[2024-12-22 14:25:53 root] (abq_llm.py 321): INFO layer 15 iter 15 loss:0.3957505524158478 norm:0.0023557983804494143 max memory_allocated 22911.13232421875 
[2024-12-22 14:26:24 root] (abq_llm.py 321): INFO layer 15 iter 16 loss:0.39548030495643616 norm:0.0023089456371963024 max memory_allocated 22911.13232421875 
[2024-12-22 14:26:56 root] (abq_llm.py 321): INFO layer 15 iter 17 loss:0.3952406942844391 norm:0.002211866667494178 max memory_allocated 22911.13232421875 
[2024-12-22 14:27:27 root] (abq_llm.py 321): INFO layer 15 iter 18 loss:0.3950181007385254 norm:0.002201055409386754 max memory_allocated 22911.13232421875 
[2024-12-22 14:27:59 root] (abq_llm.py 321): INFO layer 15 iter 19 loss:0.39476874470710754 norm:0.002226752694696188 max memory_allocated 22911.13232421875 
[2024-12-22 14:28:08 root] (abq_llm.py 208): INFO === Start quantize layer 16 ===
[2024-12-22 14:28:42 root] (abq_llm.py 321): INFO layer 16 iter 0 loss:0.5227388143539429 norm:0.04876899719238281 max memory_allocated 22912.80419921875 
[2024-12-22 14:29:13 root] (abq_llm.py 321): INFO layer 16 iter 1 loss:0.4677066206932068 norm:0.01810315065085888 max memory_allocated 22912.80419921875 
[2024-12-22 14:29:45 root] (abq_llm.py 321): INFO layer 16 iter 2 loss:0.4311113953590393 norm:0.007053237874060869 max memory_allocated 22912.80419921875 
[2024-12-22 14:30:16 root] (abq_llm.py 321): INFO layer 16 iter 3 loss:0.41594409942626953 norm:0.004048153292387724 max memory_allocated 22912.80419921875 
[2024-12-22 14:30:48 root] (abq_llm.py 321): INFO layer 16 iter 4 loss:0.4093700051307678 norm:0.0031016371212899685 max memory_allocated 22912.80419921875 
[2024-12-22 14:31:19 root] (abq_llm.py 321): INFO layer 16 iter 5 loss:0.4051043689250946 norm:0.0026171111967414618 max memory_allocated 22912.80419921875 
[2024-12-22 14:31:51 root] (abq_llm.py 321): INFO layer 16 iter 6 loss:0.4022838771343231 norm:0.002527642296627164 max memory_allocated 22912.80419921875 
[2024-12-22 14:32:22 root] (abq_llm.py 321): INFO layer 16 iter 7 loss:0.40011438727378845 norm:0.0023871762678027153 max memory_allocated 22912.80419921875 
[2024-12-22 14:32:54 root] (abq_llm.py 321): INFO layer 16 iter 8 loss:0.39851585030555725 norm:0.0023265087511390448 max memory_allocated 22912.80419921875 
[2024-12-22 14:33:25 root] (abq_llm.py 321): INFO layer 16 iter 9 loss:0.39717769622802734 norm:0.002235904335975647 max memory_allocated 22912.80419921875 
[2024-12-22 14:33:57 root] (abq_llm.py 321): INFO layer 16 iter 10 loss:0.39617466926574707 norm:0.0021797195076942444 max memory_allocated 22912.80419921875 
[2024-12-22 14:34:28 root] (abq_llm.py 321): INFO layer 16 iter 11 loss:0.39534494280815125 norm:0.002087658504024148 max memory_allocated 22912.80419921875 
[2024-12-22 14:35:00 root] (abq_llm.py 321): INFO layer 16 iter 12 loss:0.3946877121925354 norm:0.0020062036346644163 max memory_allocated 22912.80419921875 
[2024-12-22 14:35:31 root] (abq_llm.py 321): INFO layer 16 iter 13 loss:0.394126832485199 norm:0.0019552665762603283 max memory_allocated 22912.80419921875 
[2024-12-22 14:36:03 root] (abq_llm.py 321): INFO layer 16 iter 14 loss:0.39368587732315063 norm:0.0019187605939805508 max memory_allocated 22912.80419921875 
[2024-12-22 14:36:34 root] (abq_llm.py 321): INFO layer 16 iter 15 loss:0.3932722806930542 norm:0.001895868219435215 max memory_allocated 22912.80419921875 
[2024-12-22 14:37:06 root] (abq_llm.py 321): INFO layer 16 iter 16 loss:0.39298105239868164 norm:0.0018878731643781066 max memory_allocated 22912.80419921875 
[2024-12-22 14:37:37 root] (abq_llm.py 321): INFO layer 16 iter 17 loss:0.39268988370895386 norm:0.0018610252300277352 max memory_allocated 22912.80419921875 
[2024-12-22 14:38:09 root] (abq_llm.py 321): INFO layer 16 iter 18 loss:0.39252394437789917 norm:0.001828002859838307 max memory_allocated 22912.80419921875 
[2024-12-22 14:38:40 root] (abq_llm.py 321): INFO layer 16 iter 19 loss:0.3922536373138428 norm:0.001804178929887712 max memory_allocated 22912.80419921875 
[2024-12-22 14:38:49 root] (abq_llm.py 208): INFO === Start quantize layer 17 ===
[2024-12-22 14:39:23 root] (abq_llm.py 321): INFO layer 17 iter 0 loss:0.4958289861679077 norm:0.03250526636838913 max memory_allocated 22914.47607421875 
[2024-12-22 14:39:55 root] (abq_llm.py 321): INFO layer 17 iter 1 loss:0.4617388844490051 norm:0.014441408216953278 max memory_allocated 22914.47607421875 
[2024-12-22 14:40:26 root] (abq_llm.py 321): INFO layer 17 iter 2 loss:0.4315255582332611 norm:0.005934814922511578 max memory_allocated 22914.47607421875 
[2024-12-22 14:40:58 root] (abq_llm.py 321): INFO layer 17 iter 3 loss:0.41814368963241577 norm:0.0032642921432852745 max memory_allocated 22914.47607421875 
[2024-12-22 14:41:29 root] (abq_llm.py 321): INFO layer 17 iter 4 loss:0.4134500324726105 norm:0.0027141161262989044 max memory_allocated 22914.47607421875 
[2024-12-22 14:42:01 root] (abq_llm.py 321): INFO layer 17 iter 5 loss:0.4104015827178955 norm:0.002395638497546315 max memory_allocated 22914.47607421875 
[2024-12-22 14:42:32 root] (abq_llm.py 321): INFO layer 17 iter 6 loss:0.4083216190338135 norm:0.002280876040458679 max memory_allocated 22914.47607421875 
[2024-12-22 14:43:04 root] (abq_llm.py 321): INFO layer 17 iter 7 loss:0.4066607356071472 norm:0.0022124925162643194 max memory_allocated 22914.47607421875 
[2024-12-22 14:43:35 root] (abq_llm.py 321): INFO layer 17 iter 8 loss:0.4053880572319031 norm:0.0021436449605971575 max memory_allocated 22914.47607421875 
[2024-12-22 14:44:07 root] (abq_llm.py 321): INFO layer 17 iter 9 loss:0.4042791426181793 norm:0.0020866545382887125 max memory_allocated 22914.47607421875 
[2024-12-22 14:44:38 root] (abq_llm.py 321): INFO layer 17 iter 10 loss:0.4035070240497589 norm:0.0020513306371867657 max memory_allocated 22914.47607421875 
[2024-12-22 14:45:10 root] (abq_llm.py 321): INFO layer 17 iter 11 loss:0.40286797285079956 norm:0.0020221201702952385 max memory_allocated 22914.47607421875 
[2024-12-22 14:45:41 root] (abq_llm.py 321): INFO layer 17 iter 12 loss:0.4024297595024109 norm:0.0020257874857634306 max memory_allocated 22914.47607421875 
[2024-12-22 14:46:13 root] (abq_llm.py 321): INFO layer 17 iter 13 loss:0.4019947946071625 norm:0.001984451897442341 max memory_allocated 22914.47607421875 
[2024-12-22 14:46:44 root] (abq_llm.py 321): INFO layer 17 iter 14 loss:0.401593416929245 norm:0.0019630298484116793 max memory_allocated 22914.47607421875 
[2024-12-22 14:47:16 root] (abq_llm.py 321): INFO layer 17 iter 15 loss:0.40127015113830566 norm:0.00193124171346426 max memory_allocated 22914.47607421875 
[2024-12-22 14:47:47 root] (abq_llm.py 321): INFO layer 17 iter 16 loss:0.4009460210800171 norm:0.0019333369564265013 max memory_allocated 22914.47607421875 
[2024-12-22 14:48:19 root] (abq_llm.py 321): INFO layer 17 iter 17 loss:0.400712788105011 norm:0.0018919063732028008 max memory_allocated 22914.47607421875 
[2024-12-22 14:48:50 root] (abq_llm.py 321): INFO layer 17 iter 18 loss:0.4004982113838196 norm:0.0019261991837993264 max memory_allocated 22914.47607421875 
[2024-12-22 14:49:22 root] (abq_llm.py 321): INFO layer 17 iter 19 loss:0.40024879574775696 norm:0.0018777674995362759 max memory_allocated 22914.47607421875 
[2024-12-22 14:49:31 root] (abq_llm.py 208): INFO === Start quantize layer 18 ===
[2024-12-22 14:50:05 root] (abq_llm.py 321): INFO layer 18 iter 0 loss:0.517102062702179 norm:0.049408070743083954 max memory_allocated 22916.14794921875 
[2024-12-22 14:50:37 root] (abq_llm.py 321): INFO layer 18 iter 1 loss:0.4808385670185089 norm:0.01701994799077511 max memory_allocated 22916.14794921875 
[2024-12-22 14:51:08 root] (abq_llm.py 321): INFO layer 18 iter 2 loss:0.45410051941871643 norm:0.007464599795639515 max memory_allocated 22916.14794921875 
[2024-12-22 14:51:39 root] (abq_llm.py 321): INFO layer 18 iter 3 loss:0.4404130280017853 norm:0.004595296457409859 max memory_allocated 22916.14794921875 
[2024-12-22 14:52:11 root] (abq_llm.py 321): INFO layer 18 iter 4 loss:0.43354296684265137 norm:0.002659834921360016 max memory_allocated 22916.14794921875 
[2024-12-22 14:52:42 root] (abq_llm.py 321): INFO layer 18 iter 5 loss:0.43043041229248047 norm:0.0021360376849770546 max memory_allocated 22916.14794921875 
[2024-12-22 14:53:14 root] (abq_llm.py 321): INFO layer 18 iter 6 loss:0.42813459038734436 norm:0.0019586272537708282 max memory_allocated 22916.14794921875 
[2024-12-22 14:53:45 root] (abq_llm.py 321): INFO layer 18 iter 7 loss:0.4263410270214081 norm:0.0019529493292793632 max memory_allocated 22916.14794921875 
[2024-12-22 14:54:17 root] (abq_llm.py 321): INFO layer 18 iter 8 loss:0.4249451458454132 norm:0.0018586884252727032 max memory_allocated 22916.14794921875 
[2024-12-22 14:54:48 root] (abq_llm.py 321): INFO layer 18 iter 9 loss:0.4238971471786499 norm:0.0018226689426228404 max memory_allocated 22916.14794921875 
[2024-12-22 14:55:20 root] (abq_llm.py 321): INFO layer 18 iter 10 loss:0.4229961037635803 norm:0.0018102223984897137 max memory_allocated 22916.14794921875 
[2024-12-22 14:55:51 root] (abq_llm.py 321): INFO layer 18 iter 11 loss:0.4223310649394989 norm:0.0017633932875469327 max memory_allocated 22916.14794921875 
[2024-12-22 14:56:23 root] (abq_llm.py 321): INFO layer 18 iter 12 loss:0.42170873284339905 norm:0.0017309881513938308 max memory_allocated 22916.14794921875 
[2024-12-22 14:56:54 root] (abq_llm.py 321): INFO layer 18 iter 13 loss:0.42126336693763733 norm:0.001753324642777443 max memory_allocated 22916.14794921875 
[2024-12-22 14:57:26 root] (abq_llm.py 321): INFO layer 18 iter 14 loss:0.42089349031448364 norm:0.0017676455900073051 max memory_allocated 22916.14794921875 
[2024-12-22 14:57:57 root] (abq_llm.py 321): INFO layer 18 iter 15 loss:0.42051559686660767 norm:0.0017132560024037957 max memory_allocated 22916.14794921875 
[2024-12-22 14:58:29 root] (abq_llm.py 321): INFO layer 18 iter 16 loss:0.42019933462142944 norm:0.001727639464661479 max memory_allocated 22916.14794921875 
[2024-12-22 14:59:01 root] (abq_llm.py 321): INFO layer 18 iter 17 loss:0.41995441913604736 norm:0.001680913963355124 max memory_allocated 22916.14794921875 
[2024-12-22 14:59:32 root] (abq_llm.py 321): INFO layer 18 iter 18 loss:0.41966575384140015 norm:0.0016267304308712482 max memory_allocated 22916.14794921875 
[2024-12-22 15:00:04 root] (abq_llm.py 321): INFO layer 18 iter 19 loss:0.41949060559272766 norm:0.0016387979267165065 max memory_allocated 22916.14794921875 
[2024-12-22 15:00:12 root] (abq_llm.py 208): INFO === Start quantize layer 19 ===
[2024-12-22 15:00:47 root] (abq_llm.py 321): INFO layer 19 iter 0 loss:0.5190402269363403 norm:0.03337937220931053 max memory_allocated 22917.81982421875 
[2024-12-22 15:01:18 root] (abq_llm.py 321): INFO layer 19 iter 1 loss:0.4927978217601776 norm:0.013930666260421276 max memory_allocated 22917.81982421875 
[2024-12-22 15:01:49 root] (abq_llm.py 321): INFO layer 19 iter 2 loss:0.4700740873813629 norm:0.005801486782729626 max memory_allocated 22917.81982421875 
[2024-12-22 15:02:21 root] (abq_llm.py 321): INFO layer 19 iter 3 loss:0.4605068564414978 norm:0.003585489932447672 max memory_allocated 22917.81982421875 
[2024-12-22 15:02:52 root] (abq_llm.py 321): INFO layer 19 iter 4 loss:0.45660990476608276 norm:0.002962104743346572 max memory_allocated 22917.81982421875 
[2024-12-22 15:03:24 root] (abq_llm.py 321): INFO layer 19 iter 5 loss:0.4537654519081116 norm:0.0024508663918823004 max memory_allocated 22917.81982421875 
[2024-12-22 15:03:55 root] (abq_llm.py 321): INFO layer 19 iter 6 loss:0.4511890709400177 norm:0.0015901416772976518 max memory_allocated 22917.81982421875 
[2024-12-22 15:04:27 root] (abq_llm.py 321): INFO layer 19 iter 7 loss:0.4496105909347534 norm:0.0015639315824955702 max memory_allocated 22917.81982421875 
[2024-12-22 15:04:58 root] (abq_llm.py 321): INFO layer 19 iter 8 loss:0.44848355650901794 norm:0.0015134369023144245 max memory_allocated 22917.81982421875 
[2024-12-22 15:05:30 root] (abq_llm.py 321): INFO layer 19 iter 9 loss:0.4476451277732849 norm:0.0014937934465706348 max memory_allocated 22917.81982421875 
[2024-12-22 15:06:01 root] (abq_llm.py 321): INFO layer 19 iter 10 loss:0.44694048166275024 norm:0.001428331364877522 max memory_allocated 22917.81982421875 
[2024-12-22 15:06:33 root] (abq_llm.py 321): INFO layer 19 iter 11 loss:0.4463724195957184 norm:0.0013892941642552614 max memory_allocated 22917.81982421875 
[2024-12-22 15:07:04 root] (abq_llm.py 321): INFO layer 19 iter 12 loss:0.44599592685699463 norm:0.0013775448314845562 max memory_allocated 22917.81982421875 
[2024-12-22 15:07:36 root] (abq_llm.py 321): INFO layer 19 iter 13 loss:0.4456547498703003 norm:0.001340565737336874 max memory_allocated 22917.81982421875 
[2024-12-22 15:08:08 root] (abq_llm.py 321): INFO layer 19 iter 14 loss:0.44533011317253113 norm:0.0012993892887607217 max memory_allocated 22917.81982421875 
[2024-12-22 15:08:39 root] (abq_llm.py 321): INFO layer 19 iter 15 loss:0.44512832164764404 norm:0.0013138405047357082 max memory_allocated 22917.81982421875 
[2024-12-22 15:09:11 root] (abq_llm.py 321): INFO layer 19 iter 16 loss:0.4449387788772583 norm:0.0013172572944313288 max memory_allocated 22917.81982421875 
[2024-12-22 15:09:42 root] (abq_llm.py 321): INFO layer 19 iter 17 loss:0.44475096464157104 norm:0.0013081406941637397 max memory_allocated 22917.81982421875 
[2024-12-22 15:10:14 root] (abq_llm.py 321): INFO layer 19 iter 18 loss:0.44455909729003906 norm:0.0014092973433434963 max memory_allocated 22917.81982421875 
[2024-12-22 15:10:45 root] (abq_llm.py 321): INFO layer 19 iter 19 loss:0.44439709186553955 norm:0.0012881351867690682 max memory_allocated 22917.81982421875 
[2024-12-22 15:10:54 root] (abq_llm.py 208): INFO === Start quantize layer 20 ===
[2024-12-22 15:11:28 root] (abq_llm.py 321): INFO layer 20 iter 0 loss:0.5452885031700134 norm:0.021376099437475204 max memory_allocated 22919.49169921875 
[2024-12-22 15:12:00 root] (abq_llm.py 321): INFO layer 20 iter 1 loss:0.5245987176895142 norm:0.01126844808459282 max memory_allocated 22919.49169921875 
[2024-12-22 15:12:31 root] (abq_llm.py 321): INFO layer 20 iter 2 loss:0.5039595365524292 norm:0.006096161436289549 max memory_allocated 22919.49169921875 
[2024-12-22 15:13:03 root] (abq_llm.py 321): INFO layer 20 iter 3 loss:0.4939577281475067 norm:0.0040941983461380005 max memory_allocated 22919.49169921875 
[2024-12-22 15:13:34 root] (abq_llm.py 321): INFO layer 20 iter 4 loss:0.4889463186264038 norm:0.0026993483770638704 max memory_allocated 22919.49169921875 
[2024-12-22 15:14:06 root] (abq_llm.py 321): INFO layer 20 iter 5 loss:0.4855496883392334 norm:0.0022381539456546307 max memory_allocated 22919.49169921875 
[2024-12-22 15:14:37 root] (abq_llm.py 321): INFO layer 20 iter 6 loss:0.48315635323524475 norm:0.0021368935704231262 max memory_allocated 22919.49169921875 
[2024-12-22 15:15:09 root] (abq_llm.py 321): INFO layer 20 iter 7 loss:0.4814521372318268 norm:0.00205182540230453 max memory_allocated 22919.49169921875 
[2024-12-22 15:15:40 root] (abq_llm.py 321): INFO layer 20 iter 8 loss:0.48017728328704834 norm:0.00200927397236228 max memory_allocated 22919.49169921875 
[2024-12-22 15:16:12 root] (abq_llm.py 321): INFO layer 20 iter 9 loss:0.47915521264076233 norm:0.0019934556912630796 max memory_allocated 22919.49169921875 
[2024-12-22 15:16:43 root] (abq_llm.py 321): INFO layer 20 iter 10 loss:0.4784156382083893 norm:0.0019261949928477407 max memory_allocated 22919.49169921875 
[2024-12-22 15:17:15 root] (abq_llm.py 321): INFO layer 20 iter 11 loss:0.4777735471725464 norm:0.0018908025231212378 max memory_allocated 22919.49169921875 
[2024-12-22 15:17:47 root] (abq_llm.py 321): INFO layer 20 iter 12 loss:0.4772469997406006 norm:0.0018901991425082088 max memory_allocated 22919.49169921875 
[2024-12-22 15:18:18 root] (abq_llm.py 321): INFO layer 20 iter 13 loss:0.4768518805503845 norm:0.0018552553374320269 max memory_allocated 22919.49169921875 
[2024-12-22 15:18:50 root] (abq_llm.py 321): INFO layer 20 iter 14 loss:0.47648707032203674 norm:0.0018665832467377186 max memory_allocated 22919.49169921875 
[2024-12-22 15:19:21 root] (abq_llm.py 321): INFO layer 20 iter 15 loss:0.47616666555404663 norm:0.0018440443091094494 max memory_allocated 22919.49169921875 
[2024-12-22 15:19:53 root] (abq_llm.py 321): INFO layer 20 iter 16 loss:0.4758477509021759 norm:0.0018049984937533736 max memory_allocated 22919.49169921875 
[2024-12-22 15:20:24 root] (abq_llm.py 321): INFO layer 20 iter 17 loss:0.4756370186805725 norm:0.001781442086212337 max memory_allocated 22919.49169921875 
[2024-12-22 15:20:56 root] (abq_llm.py 321): INFO layer 20 iter 18 loss:0.475371390581131 norm:0.0017837522318586707 max memory_allocated 22919.49169921875 
[2024-12-22 15:21:27 root] (abq_llm.py 321): INFO layer 20 iter 19 loss:0.4751710295677185 norm:0.0017461561365053058 max memory_allocated 22919.49169921875 
[2024-12-22 15:21:36 root] (abq_llm.py 208): INFO === Start quantize layer 21 ===
[2024-12-22 15:22:10 root] (abq_llm.py 321): INFO layer 21 iter 0 loss:0.5689790844917297 norm:0.01753302663564682 max memory_allocated 22921.16357421875 
[2024-12-22 15:22:42 root] (abq_llm.py 321): INFO layer 21 iter 1 loss:0.5526411533355713 norm:0.008021678775548935 max memory_allocated 22921.16357421875 
[2024-12-22 15:23:13 root] (abq_llm.py 321): INFO layer 21 iter 2 loss:0.5362938642501831 norm:0.0034722299315035343 max memory_allocated 22921.16357421875 
[2024-12-22 15:23:45 root] (abq_llm.py 321): INFO layer 21 iter 3 loss:0.5287529230117798 norm:0.0017738606547936797 max memory_allocated 22921.16357421875 
[2024-12-22 15:24:16 root] (abq_llm.py 321): INFO layer 21 iter 4 loss:0.5258667469024658 norm:0.0015697049675509334 max memory_allocated 22921.16357421875 
[2024-12-22 15:24:48 root] (abq_llm.py 321): INFO layer 21 iter 5 loss:0.5237669944763184 norm:0.00148953462485224 max memory_allocated 22921.16357421875 
[2024-12-22 15:25:19 root] (abq_llm.py 321): INFO layer 21 iter 6 loss:0.5220118165016174 norm:0.0014439269434660673 max memory_allocated 22921.16357421875 
[2024-12-22 15:25:51 root] (abq_llm.py 321): INFO layer 21 iter 7 loss:0.5205785036087036 norm:0.00141293799970299 max memory_allocated 22921.16357421875 
[2024-12-22 15:26:22 root] (abq_llm.py 321): INFO layer 21 iter 8 loss:0.5194700360298157 norm:0.00137317122425884 max memory_allocated 22921.16357421875 
[2024-12-22 15:26:54 root] (abq_llm.py 321): INFO layer 21 iter 9 loss:0.5186548829078674 norm:0.001348199206404388 max memory_allocated 22921.16357421875 
[2024-12-22 15:27:25 root] (abq_llm.py 321): INFO layer 21 iter 10 loss:0.5180408954620361 norm:0.0013044321676716208 max memory_allocated 22921.16357421875 
[2024-12-22 15:27:57 root] (abq_llm.py 321): INFO layer 21 iter 11 loss:0.5174973607063293 norm:0.001269558328203857 max memory_allocated 22921.16357421875 
[2024-12-22 15:28:28 root] (abq_llm.py 321): INFO layer 21 iter 12 loss:0.5170326828956604 norm:0.0012452950468286872 max memory_allocated 22921.16357421875 
[2024-12-22 15:29:00 root] (abq_llm.py 321): INFO layer 21 iter 13 loss:0.5166596174240112 norm:0.0012191457208245993 max memory_allocated 22921.16357421875 
[2024-12-22 15:29:31 root] (abq_llm.py 321): INFO layer 21 iter 14 loss:0.5163947939872742 norm:0.0011979264672845602 max memory_allocated 22921.16357421875 
[2024-12-22 15:30:03 root] (abq_llm.py 321): INFO layer 21 iter 15 loss:0.5161935091018677 norm:0.0011908308370038867 max memory_allocated 22921.16357421875 
[2024-12-22 15:30:34 root] (abq_llm.py 321): INFO layer 21 iter 16 loss:0.516007661819458 norm:0.0011938451789319515 max memory_allocated 22921.16357421875 
[2024-12-22 15:31:06 root] (abq_llm.py 321): INFO layer 21 iter 17 loss:0.5158036351203918 norm:0.0011640260927379131 max memory_allocated 22921.16357421875 
[2024-12-22 15:31:37 root] (abq_llm.py 321): INFO layer 21 iter 18 loss:0.5157116651535034 norm:0.001164409564808011 max memory_allocated 22921.16357421875 
[2024-12-22 15:32:09 root] (abq_llm.py 321): INFO layer 21 iter 19 loss:0.5155880451202393 norm:0.0011585336178541183 max memory_allocated 22921.16357421875 
[2024-12-22 15:32:18 root] (abq_llm.py 208): INFO === Start quantize layer 22 ===
[2024-12-22 15:32:52 root] (abq_llm.py 321): INFO layer 22 iter 0 loss:0.6370443105697632 norm:0.013650339096784592 max memory_allocated 22922.83544921875 
[2024-12-22 15:33:23 root] (abq_llm.py 321): INFO layer 22 iter 1 loss:0.6192514896392822 norm:0.005859221797436476 max memory_allocated 22922.83544921875 
[2024-12-22 15:33:55 root] (abq_llm.py 321): INFO layer 22 iter 2 loss:0.6033028960227966 norm:0.003716181730851531 max memory_allocated 22922.83544921875 
[2024-12-22 15:34:26 root] (abq_llm.py 321): INFO layer 22 iter 3 loss:0.5952077507972717 norm:0.002878385130316019 max memory_allocated 22922.83544921875 
[2024-12-22 15:34:58 root] (abq_llm.py 321): INFO layer 22 iter 4 loss:0.5919762253761292 norm:0.002647130051627755 max memory_allocated 22922.83544921875 
[2024-12-22 15:35:30 root] (abq_llm.py 321): INFO layer 22 iter 5 loss:0.589669942855835 norm:0.0025315959937870502 max memory_allocated 22922.83544921875 
[2024-12-22 15:36:01 root] (abq_llm.py 321): INFO layer 22 iter 6 loss:0.5876767039299011 norm:0.002410236047580838 max memory_allocated 22922.83544921875 
[2024-12-22 15:36:33 root] (abq_llm.py 321): INFO layer 22 iter 7 loss:0.5861376523971558 norm:0.0024920993018895388 max memory_allocated 22922.83544921875 
[2024-12-22 15:37:04 root] (abq_llm.py 321): INFO layer 22 iter 8 loss:0.5849865674972534 norm:0.00242052273824811 max memory_allocated 22922.83544921875 
[2024-12-22 15:37:36 root] (abq_llm.py 321): INFO layer 22 iter 9 loss:0.5841091275215149 norm:0.002452525543048978 max memory_allocated 22922.83544921875 
[2024-12-22 15:38:07 root] (abq_llm.py 321): INFO layer 22 iter 10 loss:0.5834453701972961 norm:0.002522318856790662 max memory_allocated 22922.83544921875 
[2024-12-22 15:38:39 root] (abq_llm.py 321): INFO layer 22 iter 11 loss:0.5828811526298523 norm:0.002476345282047987 max memory_allocated 22922.83544921875 
[2024-12-22 15:39:10 root] (abq_llm.py 321): INFO layer 22 iter 12 loss:0.5823870897293091 norm:0.0023843056987971067 max memory_allocated 22922.83544921875 
[2024-12-22 15:39:42 root] (abq_llm.py 321): INFO layer 22 iter 13 loss:0.582054615020752 norm:0.002335264813154936 max memory_allocated 22922.83544921875 
[2024-12-22 15:40:13 root] (abq_llm.py 321): INFO layer 22 iter 14 loss:0.5817173719406128 norm:0.002325437031686306 max memory_allocated 22922.83544921875 
[2024-12-22 15:40:45 root] (abq_llm.py 321): INFO layer 22 iter 15 loss:0.5814818143844604 norm:0.002308113034814596 max memory_allocated 22922.83544921875 
[2024-12-22 15:41:16 root] (abq_llm.py 321): INFO layer 22 iter 16 loss:0.5813084840774536 norm:0.002308129332959652 max memory_allocated 22922.83544921875 
[2024-12-22 15:41:48 root] (abq_llm.py 321): INFO layer 22 iter 17 loss:0.5811466574668884 norm:0.002224663272500038 max memory_allocated 22922.83544921875 
[2024-12-22 15:42:19 root] (abq_llm.py 321): INFO layer 22 iter 18 loss:0.5809504985809326 norm:0.0022622067481279373 max memory_allocated 22922.83544921875 
[2024-12-22 15:42:51 root] (abq_llm.py 321): INFO layer 22 iter 19 loss:0.5807282328605652 norm:0.002240236848592758 max memory_allocated 22922.83544921875 
[2024-12-22 15:43:00 root] (abq_llm.py 208): INFO === Start quantize layer 23 ===
[2024-12-22 15:43:34 root] (abq_llm.py 321): INFO layer 23 iter 0 loss:0.6887627243995667 norm:0.01156007032841444 max memory_allocated 22924.50732421875 
[2024-12-22 15:44:05 root] (abq_llm.py 321): INFO layer 23 iter 1 loss:0.6759519577026367 norm:0.006491190753877163 max memory_allocated 22924.50732421875 
[2024-12-22 15:44:37 root] (abq_llm.py 321): INFO layer 23 iter 2 loss:0.6606342792510986 norm:0.00327876303344965 max memory_allocated 22924.50732421875 
[2024-12-22 15:45:08 root] (abq_llm.py 321): INFO layer 23 iter 3 loss:0.6535705327987671 norm:0.00180456240195781 max memory_allocated 22924.50732421875 
[2024-12-22 15:45:40 root] (abq_llm.py 321): INFO layer 23 iter 4 loss:0.650260329246521 norm:0.0015464464668184519 max memory_allocated 22924.50732421875 
[2024-12-22 15:46:11 root] (abq_llm.py 321): INFO layer 23 iter 5 loss:0.6474733352661133 norm:0.0012824761215597391 max memory_allocated 22924.50732421875 
[2024-12-22 15:46:43 root] (abq_llm.py 321): INFO layer 23 iter 6 loss:0.6453568935394287 norm:0.0012071887031197548 max memory_allocated 22924.50732421875 
[2024-12-22 15:47:14 root] (abq_llm.py 321): INFO layer 23 iter 7 loss:0.6437652707099915 norm:0.0011802063090726733 max memory_allocated 22924.50732421875 
[2024-12-22 15:47:46 root] (abq_llm.py 321): INFO layer 23 iter 8 loss:0.6427239179611206 norm:0.001135346945375204 max memory_allocated 22924.50732421875 
[2024-12-22 15:48:17 root] (abq_llm.py 321): INFO layer 23 iter 9 loss:0.6419749855995178 norm:0.0010973515454679728 max memory_allocated 22924.50732421875 
[2024-12-22 15:48:49 root] (abq_llm.py 321): INFO layer 23 iter 10 loss:0.6414150595664978 norm:0.0010692579671740532 max memory_allocated 22924.50732421875 
[2024-12-22 15:49:20 root] (abq_llm.py 321): INFO layer 23 iter 11 loss:0.6409157514572144 norm:0.0010571872116997838 max memory_allocated 22924.50732421875 
[2024-12-22 15:49:52 root] (abq_llm.py 321): INFO layer 23 iter 12 loss:0.6405515074729919 norm:0.001038663904182613 max memory_allocated 22924.50732421875 
[2024-12-22 15:50:23 root] (abq_llm.py 321): INFO layer 23 iter 13 loss:0.6403315663337708 norm:0.0010341873858124018 max memory_allocated 22924.50732421875 
[2024-12-22 15:50:55 root] (abq_llm.py 321): INFO layer 23 iter 14 loss:0.6401602029800415 norm:0.0010283266892656684 max memory_allocated 22924.50732421875 
[2024-12-22 15:51:26 root] (abq_llm.py 321): INFO layer 23 iter 15 loss:0.6399301886558533 norm:0.0010118097998201847 max memory_allocated 22924.50732421875 
[2024-12-22 15:51:58 root] (abq_llm.py 321): INFO layer 23 iter 16 loss:0.639790415763855 norm:0.0010068253614008427 max memory_allocated 22924.50732421875 
[2024-12-22 15:52:30 root] (abq_llm.py 321): INFO layer 23 iter 17 loss:0.6396925449371338 norm:0.0010117277270182967 max memory_allocated 22924.50732421875 
[2024-12-22 15:53:01 root] (abq_llm.py 321): INFO layer 23 iter 18 loss:0.6395227313041687 norm:0.0009980931645259261 max memory_allocated 22924.50732421875 
[2024-12-22 15:53:33 root] (abq_llm.py 321): INFO layer 23 iter 19 loss:0.6394438743591309 norm:0.0009968567173928022 max memory_allocated 22924.50732421875 
[2024-12-22 15:53:41 root] (abq_llm.py 208): INFO === Start quantize layer 24 ===
[2024-12-22 15:54:16 root] (abq_llm.py 321): INFO layer 24 iter 0 loss:0.7676946520805359 norm:0.01288268156349659 max memory_allocated 22926.17919921875 
[2024-12-22 15:54:47 root] (abq_llm.py 321): INFO layer 24 iter 1 loss:0.7504968047142029 norm:0.005784644279628992 max memory_allocated 22926.17919921875 
[2024-12-22 15:55:19 root] (abq_llm.py 321): INFO layer 24 iter 2 loss:0.7341805696487427 norm:0.003433848964050412 max memory_allocated 22926.17919921875 
[2024-12-22 15:55:50 root] (abq_llm.py 321): INFO layer 24 iter 3 loss:0.7255212068557739 norm:0.0027738576754927635 max memory_allocated 22926.17919921875 
[2024-12-22 15:56:22 root] (abq_llm.py 321): INFO layer 24 iter 4 loss:0.721840500831604 norm:0.002766815945506096 max memory_allocated 22926.17919921875 
[2024-12-22 15:56:53 root] (abq_llm.py 321): INFO layer 24 iter 5 loss:0.7190809845924377 norm:0.0026324009522795677 max memory_allocated 22926.17919921875 
[2024-12-22 15:57:25 root] (abq_llm.py 321): INFO layer 24 iter 6 loss:0.7167024612426758 norm:0.0025578055065125227 max memory_allocated 22926.17919921875 
[2024-12-22 15:57:56 root] (abq_llm.py 321): INFO layer 24 iter 7 loss:0.7150403261184692 norm:0.0024490971118211746 max memory_allocated 22926.17919921875 
[2024-12-22 15:58:28 root] (abq_llm.py 321): INFO layer 24 iter 8 loss:0.7138162851333618 norm:0.00239241449162364 max memory_allocated 22926.17919921875 
[2024-12-22 15:58:59 root] (abq_llm.py 321): INFO layer 24 iter 9 loss:0.7129141092300415 norm:0.0023891120217740536 max memory_allocated 22926.17919921875 
[2024-12-22 15:59:31 root] (abq_llm.py 321): INFO layer 24 iter 10 loss:0.7122663259506226 norm:0.0023604005109518766 max memory_allocated 22926.17919921875 
[2024-12-22 16:00:02 root] (abq_llm.py 321): INFO layer 24 iter 11 loss:0.7118373513221741 norm:0.0024033032823354006 max memory_allocated 22926.17919921875 
[2024-12-22 16:00:34 root] (abq_llm.py 321): INFO layer 24 iter 12 loss:0.7114199995994568 norm:0.002377481432631612 max memory_allocated 22926.17919921875 
[2024-12-22 16:01:05 root] (abq_llm.py 321): INFO layer 24 iter 13 loss:0.7110956907272339 norm:0.002393204951658845 max memory_allocated 22926.17919921875 
[2024-12-22 16:01:37 root] (abq_llm.py 321): INFO layer 24 iter 14 loss:0.7108185291290283 norm:0.0022879191674292088 max memory_allocated 22926.17919921875 
[2024-12-22 16:02:08 root] (abq_llm.py 321): INFO layer 24 iter 15 loss:0.7106114625930786 norm:0.002285171300172806 max memory_allocated 22926.17919921875 
[2024-12-22 16:02:40 root] (abq_llm.py 321): INFO layer 24 iter 16 loss:0.7103864550590515 norm:0.002231745980679989 max memory_allocated 22926.17919921875 
[2024-12-22 16:03:11 root] (abq_llm.py 321): INFO layer 24 iter 17 loss:0.7101964354515076 norm:0.0023340689949691296 max memory_allocated 22926.17919921875 
[2024-12-22 16:03:43 root] (abq_llm.py 321): INFO layer 24 iter 18 loss:0.7099863886833191 norm:0.0022495975717902184 max memory_allocated 22926.17919921875 
[2024-12-22 16:04:14 root] (abq_llm.py 321): INFO layer 24 iter 19 loss:0.7098553776741028 norm:0.002231899183243513 max memory_allocated 22926.17919921875 
[2024-12-22 16:04:23 root] (abq_llm.py 208): INFO === Start quantize layer 25 ===
[2024-12-22 16:04:57 root] (abq_llm.py 321): INFO layer 25 iter 0 loss:0.8702400922775269 norm:0.02694700099527836 max memory_allocated 22927.85107421875 
[2024-12-22 16:05:29 root] (abq_llm.py 321): INFO layer 25 iter 1 loss:0.8560169339179993 norm:0.01661885529756546 max memory_allocated 22927.85107421875 
[2024-12-22 16:06:00 root] (abq_llm.py 321): INFO layer 25 iter 2 loss:0.8378108739852905 norm:0.010780192911624908 max memory_allocated 22927.85107421875 
[2024-12-22 16:06:32 root] (abq_llm.py 321): INFO layer 25 iter 3 loss:0.828854501247406 norm:0.007178996689617634 max memory_allocated 22927.85107421875 
[2024-12-22 16:07:03 root] (abq_llm.py 321): INFO layer 25 iter 4 loss:0.8212709426879883 norm:0.004871871788054705 max memory_allocated 22927.85107421875 
[2024-12-22 16:07:35 root] (abq_llm.py 321): INFO layer 25 iter 5 loss:0.8187649250030518 norm:0.005466314032673836 max memory_allocated 22927.85107421875 
[2024-12-22 16:08:06 root] (abq_llm.py 321): INFO layer 25 iter 6 loss:0.8156150579452515 norm:0.004456918686628342 max memory_allocated 22927.85107421875 
[2024-12-22 16:08:37 root] (abq_llm.py 321): INFO layer 25 iter 7 loss:0.8114068508148193 norm:0.0034781985450536013 max memory_allocated 22927.85107421875 
[2024-12-22 16:09:09 root] (abq_llm.py 321): INFO layer 25 iter 8 loss:0.8106616139411926 norm:0.0036181253381073475 max memory_allocated 22927.85107421875 
[2024-12-22 16:09:40 root] (abq_llm.py 321): INFO layer 25 iter 9 loss:0.808560311794281 norm:0.00314033473841846 max memory_allocated 22927.85107421875 
[2024-12-22 16:10:12 root] (abq_llm.py 321): INFO layer 25 iter 10 loss:0.8079818487167358 norm:0.002993562025949359 max memory_allocated 22927.85107421875 
[2024-12-22 16:10:43 root] (abq_llm.py 321): INFO layer 25 iter 11 loss:0.8066234588623047 norm:0.002828325843438506 max memory_allocated 22927.85107421875 
[2024-12-22 16:11:15 root] (abq_llm.py 321): INFO layer 25 iter 12 loss:0.8067076206207275 norm:0.0028003205079585314 max memory_allocated 22927.85107421875 
[2024-12-22 16:11:46 root] (abq_llm.py 321): INFO layer 25 iter 13 loss:0.8051948547363281 norm:0.0025425702333450317 max memory_allocated 22927.85107421875 
[2024-12-22 16:12:18 root] (abq_llm.py 321): INFO layer 25 iter 14 loss:0.8047968149185181 norm:0.0024049272760748863 max memory_allocated 22927.85107421875 
[2024-12-22 16:12:49 root] (abq_llm.py 321): INFO layer 25 iter 15 loss:0.8042649626731873 norm:0.002416504081338644 max memory_allocated 22927.85107421875 
[2024-12-22 16:13:21 root] (abq_llm.py 321): INFO layer 25 iter 16 loss:0.8036217093467712 norm:0.0023079069796949625 max memory_allocated 22927.85107421875 
[2024-12-22 16:13:52 root] (abq_llm.py 321): INFO layer 25 iter 17 loss:0.8034616708755493 norm:0.002260136418044567 max memory_allocated 22927.85107421875 
[2024-12-22 16:14:24 root] (abq_llm.py 321): INFO layer 25 iter 18 loss:0.802408754825592 norm:0.0019699162803590298 max memory_allocated 22927.85107421875 
[2024-12-22 16:14:55 root] (abq_llm.py 321): INFO layer 25 iter 19 loss:0.8016504049301147 norm:0.0019069105619564652 max memory_allocated 22927.85107421875 
[2024-12-22 16:15:04 root] (abq_llm.py 208): INFO === Start quantize layer 26 ===
[2024-12-22 16:15:38 root] (abq_llm.py 321): INFO layer 26 iter 0 loss:0.9682135581970215 norm:0.00971728004515171 max memory_allocated 22929.52294921875 
[2024-12-22 16:16:10 root] (abq_llm.py 321): INFO layer 26 iter 1 loss:0.9481300711631775 norm:0.005205008666962385 max memory_allocated 22929.52294921875 
[2024-12-22 16:16:41 root] (abq_llm.py 321): INFO layer 26 iter 2 loss:0.9277818202972412 norm:0.0029747174121439457 max memory_allocated 22929.52294921875 
[2024-12-22 16:17:13 root] (abq_llm.py 321): INFO layer 26 iter 3 loss:0.9170824885368347 norm:0.002251031808555126 max memory_allocated 22929.52294921875 
[2024-12-22 16:17:44 root] (abq_llm.py 321): INFO layer 26 iter 4 loss:0.9124489426612854 norm:0.002094377763569355 max memory_allocated 22929.52294921875 
[2024-12-22 16:18:16 root] (abq_llm.py 321): INFO layer 26 iter 5 loss:0.9086644053459167 norm:0.0019936482422053814 max memory_allocated 22929.52294921875 
[2024-12-22 16:18:47 root] (abq_llm.py 321): INFO layer 26 iter 6 loss:0.9055923819541931 norm:0.0019135632319375873 max memory_allocated 22929.52294921875 
[2024-12-22 16:19:19 root] (abq_llm.py 321): INFO layer 26 iter 7 loss:0.9035094380378723 norm:0.0018754465272650123 max memory_allocated 22929.52294921875 
[2024-12-22 16:19:50 root] (abq_llm.py 321): INFO layer 26 iter 8 loss:0.9020891189575195 norm:0.0018611197592690587 max memory_allocated 22929.52294921875 
[2024-12-22 16:20:22 root] (abq_llm.py 321): INFO layer 26 iter 9 loss:0.9010132551193237 norm:0.0018515595002099872 max memory_allocated 22929.52294921875 
[2024-12-22 16:20:53 root] (abq_llm.py 321): INFO layer 26 iter 10 loss:0.9002788066864014 norm:0.0017598685808479786 max memory_allocated 22929.52294921875 
[2024-12-22 16:21:25 root] (abq_llm.py 321): INFO layer 26 iter 11 loss:0.899684488773346 norm:0.0017714258283376694 max memory_allocated 22929.52294921875 
[2024-12-22 16:21:56 root] (abq_llm.py 321): INFO layer 26 iter 12 loss:0.8992128372192383 norm:0.0017571602948009968 max memory_allocated 22929.52294921875 
[2024-12-22 16:22:28 root] (abq_llm.py 321): INFO layer 26 iter 13 loss:0.8987461924552917 norm:0.0017328823450952768 max memory_allocated 22929.52294921875 
[2024-12-22 16:22:59 root] (abq_llm.py 321): INFO layer 26 iter 14 loss:0.8984030485153198 norm:0.0017080187099054456 max memory_allocated 22929.52294921875 
[2024-12-22 16:23:31 root] (abq_llm.py 321): INFO layer 26 iter 15 loss:0.8981164693832397 norm:0.0017403180245310068 max memory_allocated 22929.52294921875 
[2024-12-22 16:24:02 root] (abq_llm.py 321): INFO layer 26 iter 16 loss:0.8978767395019531 norm:0.00177484261803329 max memory_allocated 22929.52294921875 
[2024-12-22 16:24:34 root] (abq_llm.py 321): INFO layer 26 iter 17 loss:0.8975717425346375 norm:0.0017375772586092353 max memory_allocated 22929.52294921875 
[2024-12-22 16:25:05 root] (abq_llm.py 321): INFO layer 26 iter 18 loss:0.8973981142044067 norm:0.001754157361574471 max memory_allocated 22929.52294921875 
[2024-12-22 16:25:37 root] (abq_llm.py 321): INFO layer 26 iter 19 loss:0.8971331715583801 norm:0.0017172632506117225 max memory_allocated 22929.52294921875 
[2024-12-22 16:25:46 root] (abq_llm.py 208): INFO === Start quantize layer 27 ===
[2024-12-22 16:26:20 root] (abq_llm.py 321): INFO layer 27 iter 0 loss:1.0988715887069702 norm:0.015529407188296318 max memory_allocated 22931.19482421875 
[2024-12-22 16:26:51 root] (abq_llm.py 321): INFO layer 27 iter 1 loss:1.0759570598602295 norm:0.007442005909979343 max memory_allocated 22931.19482421875 
[2024-12-22 16:27:23 root] (abq_llm.py 321): INFO layer 27 iter 2 loss:1.053433895111084 norm:0.004024384077638388 max memory_allocated 22931.19482421875 
[2024-12-22 16:27:54 root] (abq_llm.py 321): INFO layer 27 iter 3 loss:1.0410114526748657 norm:0.0028840438462793827 max memory_allocated 22931.19482421875 
[2024-12-22 16:28:25 root] (abq_llm.py 321): INFO layer 27 iter 4 loss:1.0348469018936157 norm:0.0024173937272280455 max memory_allocated 22931.19482421875 
[2024-12-22 16:28:57 root] (abq_llm.py 321): INFO layer 27 iter 5 loss:1.029733657836914 norm:0.002134953159838915 max memory_allocated 22931.19482421875 
[2024-12-22 16:29:28 root] (abq_llm.py 321): INFO layer 27 iter 6 loss:1.0260467529296875 norm:0.002076452597975731 max memory_allocated 22931.19482421875 
[2024-12-22 16:30:00 root] (abq_llm.py 321): INFO layer 27 iter 7 loss:1.0237220525741577 norm:0.0019507667748257518 max memory_allocated 22931.19482421875 
[2024-12-22 16:30:32 root] (abq_llm.py 321): INFO layer 27 iter 8 loss:1.022119402885437 norm:0.0017944719875231385 max memory_allocated 22931.19482421875 
[2024-12-22 16:31:03 root] (abq_llm.py 321): INFO layer 27 iter 9 loss:1.020989179611206 norm:0.001750118681229651 max memory_allocated 22931.19482421875 
[2024-12-22 16:31:35 root] (abq_llm.py 321): INFO layer 27 iter 10 loss:1.0201547145843506 norm:0.0017186171608045697 max memory_allocated 22931.19482421875 
[2024-12-22 16:32:06 root] (abq_llm.py 321): INFO layer 27 iter 11 loss:1.0195603370666504 norm:0.0017374727176502347 max memory_allocated 22931.19482421875 
[2024-12-22 16:32:38 root] (abq_llm.py 321): INFO layer 27 iter 12 loss:1.0192170143127441 norm:0.001732141012325883 max memory_allocated 22931.19482421875 
[2024-12-22 16:33:09 root] (abq_llm.py 321): INFO layer 27 iter 13 loss:1.0188183784484863 norm:0.0016967246774584055 max memory_allocated 22931.19482421875 
[2024-12-22 16:33:41 root] (abq_llm.py 321): INFO layer 27 iter 14 loss:1.0184283256530762 norm:0.0016800176817923784 max memory_allocated 22931.19482421875 
[2024-12-22 16:34:12 root] (abq_llm.py 321): INFO layer 27 iter 15 loss:1.0181022882461548 norm:0.001675354316830635 max memory_allocated 22931.19482421875 
[2024-12-22 16:34:44 root] (abq_llm.py 321): INFO layer 27 iter 16 loss:1.017817497253418 norm:0.001667617354542017 max memory_allocated 22931.19482421875 
[2024-12-22 16:35:15 root] (abq_llm.py 321): INFO layer 27 iter 17 loss:1.017545461654663 norm:0.001714007114060223 max memory_allocated 22931.19482421875 
[2024-12-22 16:35:47 root] (abq_llm.py 321): INFO layer 27 iter 18 loss:1.0173254013061523 norm:0.0016808221116662025 max memory_allocated 22931.19482421875 
[2024-12-22 16:36:18 root] (abq_llm.py 321): INFO layer 27 iter 19 loss:1.017195701599121 norm:0.001660169567912817 max memory_allocated 22931.19482421875 
[2024-12-22 16:36:27 root] (abq_llm.py 208): INFO === Start quantize layer 28 ===
[2024-12-22 16:36:30 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-22 16:37:01 root] (abq_llm.py 321): INFO layer 28 iter 0 loss:1.248297929763794 norm:0.03688034415245056 max memory_allocated 22932.98193359375 
[2024-12-22 16:37:33 root] (abq_llm.py 321): INFO layer 28 iter 1 loss:1.221667766571045 norm:0.026781871914863586 max memory_allocated 22932.98193359375 
[2024-12-22 16:38:04 root] (abq_llm.py 321): INFO layer 28 iter 2 loss:1.1927719116210938 norm:0.018372219055891037 max memory_allocated 22932.98193359375 
[2024-12-22 16:38:36 root] (abq_llm.py 321): INFO layer 28 iter 3 loss:1.1740336418151855 norm:0.014657312072813511 max memory_allocated 22932.98193359375 
[2024-12-22 16:39:08 root] (abq_llm.py 321): INFO layer 28 iter 4 loss:1.165990948677063 norm:0.012031462974846363 max memory_allocated 22932.98193359375 
[2024-12-22 16:39:39 root] (abq_llm.py 321): INFO layer 28 iter 5 loss:1.1609551906585693 norm:0.010677061975002289 max memory_allocated 22932.98193359375 
[2024-12-22 16:40:11 root] (abq_llm.py 321): INFO layer 28 iter 6 loss:1.1555242538452148 norm:0.00910239852964878 max memory_allocated 22932.98193359375 
[2024-12-22 16:40:43 root] (abq_llm.py 321): INFO layer 28 iter 7 loss:1.1518855094909668 norm:0.008786128833889961 max memory_allocated 22932.98193359375 
[2024-12-22 16:41:14 root] (abq_llm.py 321): INFO layer 28 iter 8 loss:1.1497113704681396 norm:0.008580172434449196 max memory_allocated 22932.98193359375 
[2024-12-22 16:41:46 root] (abq_llm.py 321): INFO layer 28 iter 9 loss:1.1485354900360107 norm:0.008641687221825123 max memory_allocated 22932.98193359375 
[2024-12-22 16:42:17 root] (abq_llm.py 321): INFO layer 28 iter 10 loss:1.147070288658142 norm:0.008429266512393951 max memory_allocated 22932.98193359375 
[2024-12-22 16:42:49 root] (abq_llm.py 321): INFO layer 28 iter 11 loss:1.1462377309799194 norm:0.008001289330422878 max memory_allocated 22932.98193359375 
[2024-12-22 16:43:21 root] (abq_llm.py 321): INFO layer 28 iter 12 loss:1.1456390619277954 norm:0.007682868279516697 max memory_allocated 22932.98193359375 
[2024-12-22 16:43:52 root] (abq_llm.py 321): INFO layer 28 iter 13 loss:1.1450767517089844 norm:0.0075932396575808525 max memory_allocated 22932.98193359375 
[2024-12-22 16:44:24 root] (abq_llm.py 321): INFO layer 28 iter 14 loss:1.144669771194458 norm:0.007398339454084635 max memory_allocated 22932.98193359375 
[2024-12-22 16:44:56 root] (abq_llm.py 321): INFO layer 28 iter 15 loss:1.1445358991622925 norm:0.007491764612495899 max memory_allocated 22932.98193359375 
[2024-12-22 16:45:27 root] (abq_llm.py 321): INFO layer 28 iter 16 loss:1.1442333459854126 norm:0.007446803152561188 max memory_allocated 22932.98193359375 
[2024-12-22 16:45:59 root] (abq_llm.py 321): INFO layer 28 iter 17 loss:1.1436419486999512 norm:0.007033814210444689 max memory_allocated 22932.98193359375 
[2024-12-22 16:46:31 root] (abq_llm.py 321): INFO layer 28 iter 18 loss:1.1434298753738403 norm:0.006844537332653999 max memory_allocated 22932.98193359375 
[2024-12-22 16:47:02 root] (abq_llm.py 321): INFO layer 28 iter 19 loss:1.1434197425842285 norm:0.006987705826759338 max memory_allocated 22932.98193359375 
[2024-12-22 16:47:11 root] (abq_llm.py 208): INFO === Start quantize layer 29 ===
[2024-12-22 16:47:14 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-22 16:47:45 root] (abq_llm.py 321): INFO layer 29 iter 0 loss:1.4047110080718994 norm:0.034773729741573334 max memory_allocated 22934.65380859375 
[2024-12-22 16:48:17 root] (abq_llm.py 321): INFO layer 29 iter 1 loss:1.3718825578689575 norm:0.027302244678139687 max memory_allocated 22934.65380859375 
[2024-12-22 16:48:49 root] (abq_llm.py 321): INFO layer 29 iter 2 loss:1.33976411819458 norm:0.020842989906668663 max memory_allocated 22934.65380859375 
[2024-12-22 16:49:20 root] (abq_llm.py 321): INFO layer 29 iter 3 loss:1.3180806636810303 norm:0.01694459095597267 max memory_allocated 22934.65380859375 
[2024-12-22 16:49:52 root] (abq_llm.py 321): INFO layer 29 iter 4 loss:1.3064854145050049 norm:0.014278806746006012 max memory_allocated 22934.65380859375 
[2024-12-22 16:50:23 root] (abq_llm.py 321): INFO layer 29 iter 5 loss:1.2991790771484375 norm:0.012119578197598457 max memory_allocated 22934.65380859375 
[2024-12-22 16:50:55 root] (abq_llm.py 321): INFO layer 29 iter 6 loss:1.2943978309631348 norm:0.011612930335104465 max memory_allocated 22934.65380859375 
[2024-12-22 16:51:27 root] (abq_llm.py 321): INFO layer 29 iter 7 loss:1.2914535999298096 norm:0.01156109943985939 max memory_allocated 22934.65380859375 
[2024-12-22 16:51:58 root] (abq_llm.py 321): INFO layer 29 iter 8 loss:1.289460301399231 norm:0.010802559554576874 max memory_allocated 22934.65380859375 
[2024-12-22 16:52:30 root] (abq_llm.py 321): INFO layer 29 iter 9 loss:1.287824034690857 norm:0.010422901250422001 max memory_allocated 22934.65380859375 
[2024-12-22 16:53:02 root] (abq_llm.py 321): INFO layer 29 iter 10 loss:1.2863911390304565 norm:0.010012823157012463 max memory_allocated 22934.65380859375 
[2024-12-22 16:53:33 root] (abq_llm.py 321): INFO layer 29 iter 11 loss:1.2852989435195923 norm:0.009923352859914303 max memory_allocated 22934.65380859375 
[2024-12-22 16:54:05 root] (abq_llm.py 321): INFO layer 29 iter 12 loss:1.2845228910446167 norm:0.009828604757785797 max memory_allocated 22934.65380859375 
[2024-12-22 16:54:37 root] (abq_llm.py 321): INFO layer 29 iter 13 loss:1.2835954427719116 norm:0.009484984911978245 max memory_allocated 22934.65380859375 
[2024-12-22 16:55:08 root] (abq_llm.py 321): INFO layer 29 iter 14 loss:1.2828559875488281 norm:0.00925455056130886 max memory_allocated 22934.65380859375 
[2024-12-22 16:55:40 root] (abq_llm.py 321): INFO layer 29 iter 15 loss:1.2822576761245728 norm:0.009318364784121513 max memory_allocated 22934.65380859375 
[2024-12-22 16:56:12 root] (abq_llm.py 321): INFO layer 29 iter 16 loss:1.2818949222564697 norm:0.00917069148272276 max memory_allocated 22934.65380859375 
[2024-12-22 16:56:43 root] (abq_llm.py 321): INFO layer 29 iter 17 loss:1.2813173532485962 norm:0.009147856384515762 max memory_allocated 22934.65380859375 
[2024-12-22 16:57:15 root] (abq_llm.py 321): INFO layer 29 iter 18 loss:1.2810935974121094 norm:0.008954224176704884 max memory_allocated 22934.65380859375 
[2024-12-22 16:57:47 root] (abq_llm.py 321): INFO layer 29 iter 19 loss:1.2806932926177979 norm:0.00865950994193554 max memory_allocated 22934.65380859375 
[2024-12-22 16:57:56 root] (abq_llm.py 208): INFO === Start quantize layer 30 ===
[2024-12-22 16:57:58 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-22 16:58:30 root] (abq_llm.py 321): INFO layer 30 iter 0 loss:4.046794891357422 norm:0.25176262855529785 max memory_allocated 22936.32568359375 
[2024-12-22 16:59:01 root] (abq_llm.py 321): INFO layer 30 iter 1 loss:3.6819863319396973 norm:0.40636974573135376 max memory_allocated 22936.32568359375 
[2024-12-22 16:59:33 root] (abq_llm.py 321): INFO layer 30 iter 2 loss:3.2994775772094727 norm:0.45943886041641235 max memory_allocated 22936.32568359375 
[2024-12-22 17:00:05 root] (abq_llm.py 321): INFO layer 30 iter 3 loss:2.828690528869629 norm:0.5869996547698975 max memory_allocated 22936.32568359375 
[2024-12-22 17:00:36 root] (abq_llm.py 321): INFO layer 30 iter 4 loss:2.5778439044952393 norm:0.8703458905220032 max memory_allocated 22936.32568359375 
[2024-12-22 17:01:08 root] (abq_llm.py 321): INFO layer 30 iter 5 loss:2.454584836959839 norm:0.7485101222991943 max memory_allocated 22936.32568359375 
[2024-12-22 17:01:39 root] (abq_llm.py 321): INFO layer 30 iter 6 loss:2.369795322418213 norm:0.5589407682418823 max memory_allocated 22936.32568359375 
[2024-12-22 17:02:11 root] (abq_llm.py 321): INFO layer 30 iter 7 loss:2.3160810470581055 norm:0.4862992763519287 max memory_allocated 22936.32568359375 
[2024-12-22 17:02:43 root] (abq_llm.py 321): INFO layer 30 iter 8 loss:2.2774930000305176 norm:0.48080432415008545 max memory_allocated 22936.32568359375 
[2024-12-22 17:03:14 root] (abq_llm.py 321): INFO layer 30 iter 9 loss:2.241896152496338 norm:0.4689197540283203 max memory_allocated 22936.32568359375 
[2024-12-22 17:03:46 root] (abq_llm.py 321): INFO layer 30 iter 10 loss:2.205721616744995 norm:0.42509934306144714 max memory_allocated 22936.32568359375 
[2024-12-22 17:04:17 root] (abq_llm.py 321): INFO layer 30 iter 11 loss:2.172548294067383 norm:0.39469918608665466 max memory_allocated 22936.32568359375 
[2024-12-22 17:04:49 root] (abq_llm.py 321): INFO layer 30 iter 12 loss:2.156071424484253 norm:0.42559272050857544 max memory_allocated 22936.32568359375 
[2024-12-22 17:05:21 root] (abq_llm.py 321): INFO layer 30 iter 13 loss:2.1532533168792725 norm:0.4615757465362549 max memory_allocated 22936.32568359375 
[2024-12-22 17:05:52 root] (abq_llm.py 321): INFO layer 30 iter 14 loss:2.1194610595703125 norm:0.3740805685520172 max memory_allocated 22936.32568359375 
[2024-12-22 17:06:24 root] (abq_llm.py 321): INFO layer 30 iter 15 loss:2.1004106998443604 norm:0.36216408014297485 max memory_allocated 22936.32568359375 
[2024-12-22 17:06:55 root] (abq_llm.py 321): INFO layer 30 iter 16 loss:2.0872607231140137 norm:0.4090670347213745 max memory_allocated 22936.32568359375 
[2024-12-22 17:07:27 root] (abq_llm.py 321): INFO layer 30 iter 17 loss:2.078235149383545 norm:0.39849019050598145 max memory_allocated 22936.32568359375 
[2024-12-22 17:07:59 root] (abq_llm.py 321): INFO layer 30 iter 18 loss:2.0667331218719482 norm:0.3737296462059021 max memory_allocated 22936.32568359375 
[2024-12-22 17:08:30 root] (abq_llm.py 321): INFO layer 30 iter 19 loss:2.0390095710754395 norm:0.3030984401702881 max memory_allocated 22936.32568359375 
[2024-12-22 17:08:39 root] (abq_llm.py 208): INFO === Start quantize layer 31 ===
[2024-12-22 17:08:42 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-22 17:09:13 root] (abq_llm.py 321): INFO layer 31 iter 0 loss:3.529139518737793 norm:0.547415018081665 max memory_allocated 22937.99755859375 
[2024-12-22 17:09:45 root] (abq_llm.py 321): INFO layer 31 iter 1 loss:3.184537887573242 norm:0.20611149072647095 max memory_allocated 22937.99755859375 
[2024-12-22 17:10:16 root] (abq_llm.py 321): INFO layer 31 iter 2 loss:3.021425485610962 norm:0.1504371017217636 max memory_allocated 22937.99755859375 
[2024-12-22 17:10:48 root] (abq_llm.py 321): INFO layer 31 iter 3 loss:2.9262540340423584 norm:0.12335918843746185 max memory_allocated 22937.99755859375 
[2024-12-22 17:11:19 root] (abq_llm.py 321): INFO layer 31 iter 4 loss:2.880206823348999 norm:0.11152904480695724 max memory_allocated 22937.99755859375 
[2024-12-22 17:11:51 root] (abq_llm.py 321): INFO layer 31 iter 5 loss:2.8477108478546143 norm:0.10368309915065765 max memory_allocated 22937.99755859375 
[2024-12-22 17:12:23 root] (abq_llm.py 321): INFO layer 31 iter 6 loss:2.8252787590026855 norm:0.09914291650056839 max memory_allocated 22937.99755859375 
[2024-12-22 17:12:54 root] (abq_llm.py 321): INFO layer 31 iter 7 loss:2.801180601119995 norm:0.09077157825231552 max memory_allocated 22937.99755859375 
[2024-12-22 17:13:26 root] (abq_llm.py 321): INFO layer 31 iter 8 loss:2.7825756072998047 norm:0.08846402913331985 max memory_allocated 22937.99755859375 
[2024-12-22 17:13:57 root] (abq_llm.py 321): INFO layer 31 iter 9 loss:2.7637529373168945 norm:0.08505608141422272 max memory_allocated 22937.99755859375 
[2024-12-22 17:14:29 root] (abq_llm.py 321): INFO layer 31 iter 10 loss:2.7500128746032715 norm:0.08424068987369537 max memory_allocated 22937.99755859375 
[2024-12-22 17:15:01 root] (abq_llm.py 321): INFO layer 31 iter 11 loss:2.739992380142212 norm:0.08265548199415207 max memory_allocated 22937.99755859375 
[2024-12-22 17:15:32 root] (abq_llm.py 321): INFO layer 31 iter 12 loss:2.7294065952301025 norm:0.08068639785051346 max memory_allocated 22937.99755859375 
[2024-12-22 17:16:04 root] (abq_llm.py 321): INFO layer 31 iter 13 loss:2.715869426727295 norm:0.0751611664891243 max memory_allocated 22937.99755859375 
[2024-12-22 17:16:36 root] (abq_llm.py 321): INFO layer 31 iter 14 loss:2.707656145095825 norm:0.07162722945213318 max memory_allocated 22937.99755859375 
[2024-12-22 17:17:07 root] (abq_llm.py 321): INFO layer 31 iter 15 loss:2.701519012451172 norm:0.06942582875490189 max memory_allocated 22937.99755859375 
[2024-12-22 17:17:39 root] (abq_llm.py 321): INFO layer 31 iter 16 loss:2.6983461380004883 norm:0.06864719837903976 max memory_allocated 22937.99755859375 
[2024-12-22 17:18:10 root] (abq_llm.py 321): INFO layer 31 iter 17 loss:2.691213607788086 norm:0.06842397153377533 max memory_allocated 22937.99755859375 
[2024-12-22 17:18:42 root] (abq_llm.py 321): INFO layer 31 iter 18 loss:2.68509578704834 norm:0.06468317657709122 max memory_allocated 22937.99755859375 
[2024-12-22 17:19:14 root] (abq_llm.py 321): INFO layer 31 iter 19 loss:2.6811461448669434 norm:0.06282984465360641 max memory_allocated 22937.99755859375 
[2024-12-22 17:19:23 root] (main.py 360): INFO 20608.778398275375
[2024-12-22 17:21:58 root] (main.py 158): INFO wikitext2 : 13.067985534667969
