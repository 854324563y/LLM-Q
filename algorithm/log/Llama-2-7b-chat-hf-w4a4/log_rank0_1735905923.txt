[2025-01-03 12:05:23 root] (main_divide_blocks.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-chat-hf', cache_dir='./cache', output_dir='./log/Llama-2-7b-chat-hf-w4a4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.2, similarity_threshold=0.999, sensitivity_threshold=0.5, max_block_size=3)
[2025-01-03 12:05:25 root] (main_divide_blocks.py 342): INFO === start quantization ===
[2025-01-03 12:05:26 root] (main_divide_blocks.py 348): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-01-03 12:05:26 root] (abq_llm_divide_blocks.py 61): INFO Starting ...
[2025-01-03 12:05:31 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-01-03 12:05:31 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000490830274023
[2025-01-03 12:05:31 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-01-03 12:05:32 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9365224923406329
[2025-01-03 12:05:32 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000876853581474
[2025-01-03 12:05:32 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-01-03 12:05:34 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.945068189076015
[2025-01-03 12:05:34 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001003331313034
[2025-01-03 12:05:34 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-01-03 12:05:35 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.998047241142818
[2025-01-03 12:05:36 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000936197431889
[2025-01-03 12:05:36 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-01-03 12:05:37 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9985637324196952
[2025-01-03 12:05:38 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000983058270517
[2025-01-03 12:05:38 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-01-03 12:05:39 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995886853763035
[2025-01-03 12:05:40 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001007354179442
[2025-01-03 12:05:40 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-01-03 12:05:42 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9971630828721183
[2025-01-03 12:05:42 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000925191478373
[2025-01-03 12:05:42 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-01-03 12:05:44 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999109166009086
[2025-01-03 12:05:44 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500092061272256
[2025-01-03 12:05:44 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-01-03 12:05:45 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999577522277832
[2025-01-03 12:05:46 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000937804594723
[2025-01-03 12:05:46 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-01-03 12:05:47 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996087721415928
[2025-01-03 12:05:47 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000953864179264
[2025-01-03 12:05:47 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-01-03 12:05:48 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998486042022705
[2025-01-03 12:05:48 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000956544574908
[2025-01-03 12:05:48 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-01-03 12:05:49 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.998076319694519
[2025-01-03 12:05:49 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500091346161906
[2025-01-03 12:05:49 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-01-03 12:05:50 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9991021752357483
[2025-01-03 12:05:51 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000942837419642
[2025-01-03 12:05:51 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-01-03 12:05:52 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995694586208889
[2025-01-03 12:05:52 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000943508495221
[2025-01-03 12:05:52 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-01-03 12:05:53 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999871483870915
[2025-01-03 12:05:53 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000935940097908
[2025-01-03 12:05:53 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-01-03 12:05:54 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999535356249128
[2025-01-03 12:05:54 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000947940224716
[2025-01-03 12:05:54 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-01-03 12:05:55 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999412110873631
[2025-01-03 12:05:56 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000955214240426
[2025-01-03 12:05:56 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-01-03 12:05:57 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999857519354139
[2025-01-03 12:05:57 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000947970663164
[2025-01-03 12:05:57 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-01-03 12:05:58 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9993765694754464
[2025-01-03 12:05:58 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000948428385326
[2025-01-03 12:05:58 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-01-03 12:05:59 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998676010540554
[2025-01-03 12:06:00 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000943848607768
[2025-01-03 12:06:00 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-01-03 12:06:01 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998610275132316
[2025-01-03 12:06:01 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000956644530451
[2025-01-03 12:06:01 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-01-03 12:06:02 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996094107627869
[2025-01-03 12:06:02 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000950630099376
[2025-01-03 12:06:02 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-01-03 12:06:03 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997915370123727
[2025-01-03 12:06:04 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000969061576352
[2025-01-03 12:06:04 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-01-03 12:06:05 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9992672204971313
[2025-01-03 12:06:06 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000987159477684
[2025-01-03 12:06:06 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-01-03 12:06:07 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9993519953319004
[2025-01-03 12:06:07 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000963136078269
[2025-01-03 12:06:07 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-01-03 12:06:08 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999281815120152
[2025-01-03 12:06:09 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000990316092525
[2025-01-03 12:06:09 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-01-03 12:06:10 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997429081371852
[2025-01-03 12:06:10 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000993887073134
[2025-01-03 12:06:10 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-01-03 12:06:11 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999108612537384
[2025-01-03 12:06:11 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001025115819753
[2025-01-03 12:06:11 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-01-03 12:06:12 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994618892669678
[2025-01-03 12:06:13 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001025278984136
[2025-01-03 12:06:13 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-01-03 12:06:14 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996175595692226
[2025-01-03 12:06:14 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001015481187296
[2025-01-03 12:06:14 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-01-03 12:06:15 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995589682034084
[2025-01-03 12:06:15 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001045052855374
[2025-01-03 12:06:15 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-01-03 12:06:16 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9965078830718994
[2025-01-03 12:06:16 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001033471263031
[2025-01-03 12:06:16 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 0 ===
[2025-01-03 12:06:28 root] (abq_llm_divide_blocks.py 278): INFO layer 0 loss_mean: 0.00022863886260893196
[2025-01-03 12:06:28 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 1 ===
[2025-01-03 12:06:38 root] (abq_llm_divide_blocks.py 278): INFO layer 1 loss_mean: 0.028571682050824165
[2025-01-03 12:06:38 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 2 ===
[2025-01-03 12:06:49 root] (abq_llm_divide_blocks.py 278): INFO layer 2 loss_mean: 0.002505195327103138
[2025-01-03 12:06:49 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 3 ===
[2025-01-03 12:07:00 root] (abq_llm_divide_blocks.py 278): INFO layer 3 loss_mean: 0.0019114165334030986
[2025-01-03 12:07:00 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 4 ===
[2025-01-03 12:07:11 root] (abq_llm_divide_blocks.py 278): INFO layer 4 loss_mean: 0.0011111360508948565
[2025-01-03 12:07:11 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 5 ===
[2025-01-03 12:07:21 root] (abq_llm_divide_blocks.py 278): INFO layer 5 loss_mean: 0.0015577609883621335
[2025-01-03 12:07:21 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 6 ===
[2025-01-03 12:07:32 root] (abq_llm_divide_blocks.py 278): INFO layer 6 loss_mean: 0.0052434587851166725
[2025-01-03 12:07:32 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 7 ===
[2025-01-03 12:07:42 root] (abq_llm_divide_blocks.py 278): INFO layer 7 loss_mean: 0.005836475640535355
[2025-01-03 12:07:42 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 8 ===
[2025-01-03 12:07:53 root] (abq_llm_divide_blocks.py 278): INFO layer 8 loss_mean: 0.006808248348534107
[2025-01-03 12:07:53 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 9 ===
[2025-01-03 12:08:04 root] (abq_llm_divide_blocks.py 278): INFO layer 9 loss_mean: 0.00669066933915019
[2025-01-03 12:08:04 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 10 ===
[2025-01-03 12:08:14 root] (abq_llm_divide_blocks.py 278): INFO layer 10 loss_mean: 0.007493246346712112
[2025-01-03 12:08:14 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 11 ===
[2025-01-03 12:08:25 root] (abq_llm_divide_blocks.py 278): INFO layer 11 loss_mean: 0.0072413282468914986
[2025-01-03 12:08:25 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 12 ===
[2025-01-03 12:08:36 root] (abq_llm_divide_blocks.py 278): INFO layer 12 loss_mean: 0.007276169955730438
[2025-01-03 12:08:36 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 13 ===
[2025-01-03 12:08:47 root] (abq_llm_divide_blocks.py 278): INFO layer 13 loss_mean: 0.009749066084623337
[2025-01-03 12:08:47 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 14 ===
[2025-01-03 12:08:57 root] (abq_llm_divide_blocks.py 278): INFO layer 14 loss_mean: 0.010395384393632412
[2025-01-03 12:08:57 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 15 ===
[2025-01-03 12:09:08 root] (abq_llm_divide_blocks.py 278): INFO layer 15 loss_mean: 0.016192426905035973
[2025-01-03 12:09:08 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 16 ===
[2025-01-03 12:09:19 root] (abq_llm_divide_blocks.py 278): INFO layer 16 loss_mean: 0.016168339177966118
[2025-01-03 12:09:19 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 17 ===
[2025-01-03 12:09:29 root] (abq_llm_divide_blocks.py 278): INFO layer 17 loss_mean: 0.016835102811455727
[2025-01-03 12:09:29 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 18 ===
[2025-01-03 12:09:40 root] (abq_llm_divide_blocks.py 278): INFO layer 18 loss_mean: 0.020924797281622887
[2025-01-03 12:09:40 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 19 ===
[2025-01-03 12:09:51 root] (abq_llm_divide_blocks.py 278): INFO layer 19 loss_mean: 0.018699804320931435
[2025-01-03 12:09:51 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 20 ===
[2025-01-03 12:10:01 root] (abq_llm_divide_blocks.py 278): INFO layer 20 loss_mean: 0.019195670261979103
[2025-01-03 12:10:01 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 21 ===
[2025-01-03 12:10:12 root] (abq_llm_divide_blocks.py 278): INFO layer 21 loss_mean: 0.020328950136899948
[2025-01-03 12:10:12 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 22 ===
[2025-01-03 12:10:23 root] (abq_llm_divide_blocks.py 278): INFO layer 22 loss_mean: 0.026913652196526527
[2025-01-03 12:10:23 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 23 ===
[2025-01-03 12:10:34 root] (abq_llm_divide_blocks.py 278): INFO layer 23 loss_mean: 0.023367004469037056
[2025-01-03 12:10:34 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 24 ===
[2025-01-03 12:10:44 root] (abq_llm_divide_blocks.py 278): INFO layer 24 loss_mean: 0.02865133062005043
[2025-01-03 12:10:44 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 25 ===
[2025-01-03 12:10:55 root] (abq_llm_divide_blocks.py 278): INFO layer 25 loss_mean: 0.035225462168455124
[2025-01-03 12:10:55 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 26 ===
[2025-01-03 12:11:06 root] (abq_llm_divide_blocks.py 278): INFO layer 26 loss_mean: 0.0428847037255764
[2025-01-03 12:11:06 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 27 ===
[2025-01-03 12:11:17 root] (abq_llm_divide_blocks.py 278): INFO layer 27 loss_mean: 0.043513406068086624
[2025-01-03 12:11:17 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 28 ===
[2025-01-03 12:11:27 root] (abq_llm_divide_blocks.py 278): INFO layer 28 loss_mean: 0.05851059779524803
[2025-01-03 12:11:27 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 29 ===
[2025-01-03 12:11:38 root] (abq_llm_divide_blocks.py 278): INFO layer 29 loss_mean: 0.08298107236623764
[2025-01-03 12:11:38 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 30 ===
[2025-01-03 12:11:50 root] (abq_llm_divide_blocks.py 278): INFO layer 30 loss_mean: 0.2804822325706482
[2025-01-03 12:11:50 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 31 ===
[2025-01-03 12:12:00 root] (abq_llm_divide_blocks.py 278): INFO layer 31 loss_mean: 0.5396586656570435
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-1: size=1, error_sum=0.0002, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 1-2: size=1, error_sum=0.0286, min_similarity=0.9365, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-3: size=1, error_sum=0.0025, min_similarity=0.9451, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 3-6: size=3, error_sum=0.0046, min_similarity=0.9986, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-9: size=3, error_sum=0.0179, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 9-11: size=2, error_sum=0.0142, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 11-14: size=3, error_sum=0.0243, min_similarity=0.9991, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 14-17: size=3, error_sum=0.0428, min_similarity=0.9994, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 17-20: size=3, error_sum=0.0565, min_similarity=0.9994, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 20-23: size=3, error_sum=0.0664, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 23-26: size=3, error_sum=0.0872, min_similarity=0.9993, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 26-29: size=3, error_sum=0.1449, min_similarity=0.9991, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 29-30: size=1, error_sum=0.0830, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-31: size=1, error_sum=0.2805, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 quantize.utils_divide] (utils_divide.py 110): INFO Block 31-32: size=1, error_sum=0.5397, min_similarity=0.9965, max_sensitivity_diff=0.0000
[2025-01-03 12:12:00 root] (abq_llm_divide_blocks.py 294): INFO blocks: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 17), (17, 20), (20, 23), (23, 26), (26, 29), (29, 30), (30, 31), (31, 32)]
[2025-01-03 12:12:01 root] (main_divide_blocks.py 371): INFO 395.3484320640564
