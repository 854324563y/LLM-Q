[2024-12-23 08:44:04 root] (main.py 265): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-chat-hf', cache_dir='./cache', output_dir='./log/Llama-2-13b-chat-hf-w4a4', save_dir='./quant/Llama-2-13b-chat-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2024-12-23 08:44:09 root] (main.py 331): INFO === start quantization ===
[2024-12-23 08:44:09 root] (main.py 337): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2024-12-23 08:44:11 root] (abq_llm.py 62): INFO Starting ...
[2024-12-23 08:44:13 root] (abq_llm.py 208): INFO === Start quantize layer 0 ===
[2024-12-23 08:44:16 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-23 08:45:00 root] (abq_llm.py 321): INFO layer 0 iter 0 loss:0.055781807750463486 norm:0.03818599134683609 max memory_allocated 29716.09814453125 
[2024-12-23 08:45:44 root] (abq_llm.py 321): INFO layer 0 iter 1 loss:0.033493392169475555 norm:0.02268759161233902 max memory_allocated 29716.09814453125 
[2024-12-23 08:46:27 root] (abq_llm.py 321): INFO layer 0 iter 2 loss:0.026436133310198784 norm:0.01851153001189232 max memory_allocated 29716.09814453125 
[2024-12-23 08:47:11 root] (abq_llm.py 321): INFO layer 0 iter 3 loss:0.023340709507465363 norm:0.014420959167182446 max memory_allocated 29716.09814453125 
[2024-12-23 08:47:54 root] (abq_llm.py 321): INFO layer 0 iter 4 loss:0.02196023054420948 norm:0.011673864908516407 max memory_allocated 29716.09814453125 
[2024-12-23 08:48:38 root] (abq_llm.py 321): INFO layer 0 iter 5 loss:0.021117206662893295 norm:0.010602550581097603 max memory_allocated 29716.09814453125 
[2024-12-23 08:49:21 root] (abq_llm.py 321): INFO layer 0 iter 6 loss:0.020645927637815475 norm:0.008947755210101604 max memory_allocated 29716.09814453125 
[2024-12-23 08:50:05 root] (abq_llm.py 321): INFO layer 0 iter 7 loss:0.02033878304064274 norm:0.007953697815537453 max memory_allocated 29716.09814453125 
[2024-12-23 08:50:49 root] (abq_llm.py 321): INFO layer 0 iter 8 loss:0.01994570903480053 norm:0.006965582258999348 max memory_allocated 29716.09814453125 
[2024-12-23 08:51:32 root] (abq_llm.py 321): INFO layer 0 iter 9 loss:0.019750472158193588 norm:0.0064665875397622585 max memory_allocated 29716.09814453125 
[2024-12-23 08:52:16 root] (abq_llm.py 321): INFO layer 0 iter 10 loss:0.019695110619068146 norm:0.005991073325276375 max memory_allocated 29716.09814453125 
[2024-12-23 08:52:59 root] (abq_llm.py 321): INFO layer 0 iter 11 loss:0.01957577094435692 norm:0.005807760637253523 max memory_allocated 29716.09814453125 
[2024-12-23 08:53:43 root] (abq_llm.py 321): INFO layer 0 iter 12 loss:0.019423585385084152 norm:0.00529328640550375 max memory_allocated 29716.09814453125 
[2024-12-23 08:54:26 root] (abq_llm.py 321): INFO layer 0 iter 13 loss:0.019397642463445663 norm:0.005194209050387144 max memory_allocated 29716.09814453125 
[2024-12-23 08:55:10 root] (abq_llm.py 321): INFO layer 0 iter 14 loss:0.019354861229658127 norm:0.005039381794631481 max memory_allocated 29716.09814453125 
[2024-12-23 08:55:54 root] (abq_llm.py 321): INFO layer 0 iter 15 loss:0.019376667216420174 norm:0.005107081960886717 max memory_allocated 29716.09814453125 
[2024-12-23 08:56:37 root] (abq_llm.py 321): INFO layer 0 iter 16 loss:0.01928701438009739 norm:0.00468676071614027 max memory_allocated 29716.09814453125 
[2024-12-23 08:57:21 root] (abq_llm.py 321): INFO layer 0 iter 17 loss:0.019239965826272964 norm:0.00466670747846365 max memory_allocated 29716.09814453125 
[2024-12-23 08:58:04 root] (abq_llm.py 321): INFO layer 0 iter 18 loss:0.019253263249993324 norm:0.004387349355965853 max memory_allocated 29716.09814453125 
[2024-12-23 08:58:48 root] (abq_llm.py 321): INFO layer 0 iter 19 loss:0.01935865543782711 norm:0.005318842828273773 max memory_allocated 29716.09814453125 
[2024-12-23 08:59:03 root] (abq_llm.py 208): INFO === Start quantize layer 1 ===
[2024-12-23 08:59:08 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-23 08:59:53 root] (abq_llm.py 321): INFO layer 1 iter 0 loss:0.13598978519439697 norm:0.02865474671125412 max memory_allocated 29716.16064453125 
[2024-12-23 09:00:37 root] (abq_llm.py 321): INFO layer 1 iter 1 loss:0.10479685664176941 norm:0.020834391936659813 max memory_allocated 29716.16064453125 
[2024-12-23 09:01:20 root] (abq_llm.py 321): INFO layer 1 iter 2 loss:0.09253599494695663 norm:0.01640961319208145 max memory_allocated 29716.16064453125 
[2024-12-23 09:02:04 root] (abq_llm.py 321): INFO layer 1 iter 3 loss:0.08735916018486023 norm:0.01309889741241932 max memory_allocated 29716.16064453125 
[2024-12-23 09:02:48 root] (abq_llm.py 321): INFO layer 1 iter 4 loss:0.08427058905363083 norm:0.010720641352236271 max memory_allocated 29716.16064453125 
[2024-12-23 09:03:31 root] (abq_llm.py 321): INFO layer 1 iter 5 loss:0.08240306377410889 norm:0.008877630345523357 max memory_allocated 29716.16064453125 
[2024-12-23 09:04:15 root] (abq_llm.py 321): INFO layer 1 iter 6 loss:0.08114904910326004 norm:0.0076584359630942345 max memory_allocated 29716.16064453125 
[2024-12-23 09:04:58 root] (abq_llm.py 321): INFO layer 1 iter 7 loss:0.08020977675914764 norm:0.007162583060562611 max memory_allocated 29716.16064453125 
[2024-12-23 09:05:42 root] (abq_llm.py 321): INFO layer 1 iter 8 loss:0.07960233092308044 norm:0.00661504827439785 max memory_allocated 29716.16064453125 
[2024-12-23 09:06:25 root] (abq_llm.py 321): INFO layer 1 iter 9 loss:0.0792476013302803 norm:0.006814149674028158 max memory_allocated 29716.16064453125 
[2024-12-23 09:07:09 root] (abq_llm.py 321): INFO layer 1 iter 10 loss:0.07890535891056061 norm:0.006715364754199982 max memory_allocated 29716.16064453125 
[2024-12-23 09:07:52 root] (abq_llm.py 321): INFO layer 1 iter 11 loss:0.07858415693044662 norm:0.006390838883817196 max memory_allocated 29716.16064453125 
[2024-12-23 09:08:36 root] (abq_llm.py 321): INFO layer 1 iter 12 loss:0.07835794985294342 norm:0.006064454559236765 max memory_allocated 29716.16064453125 
[2024-12-23 09:09:20 root] (abq_llm.py 321): INFO layer 1 iter 13 loss:0.07826317846775055 norm:0.006087892223149538 max memory_allocated 29716.16064453125 
[2024-12-23 09:10:03 root] (abq_llm.py 321): INFO layer 1 iter 14 loss:0.07811766862869263 norm:0.006068369839340448 max memory_allocated 29716.16064453125 
[2024-12-23 09:10:47 root] (abq_llm.py 321): INFO layer 1 iter 15 loss:0.0780780091881752 norm:0.0062329405918717384 max memory_allocated 29716.16064453125 
[2024-12-23 09:11:30 root] (abq_llm.py 321): INFO layer 1 iter 16 loss:0.07802684605121613 norm:0.006096154451370239 max memory_allocated 29716.16064453125 
[2024-12-23 09:12:14 root] (abq_llm.py 321): INFO layer 1 iter 17 loss:0.07790753245353699 norm:0.005974137224256992 max memory_allocated 29716.16064453125 
[2024-12-23 09:12:57 root] (abq_llm.py 321): INFO layer 1 iter 18 loss:0.07786235958337784 norm:0.005888236220926046 max memory_allocated 29716.16064453125 
[2024-12-23 09:13:41 root] (abq_llm.py 321): INFO layer 1 iter 19 loss:0.07790807634592056 norm:0.005910777021199465 max memory_allocated 29716.16064453125 
[2024-12-23 09:13:55 root] (abq_llm.py 208): INFO === Start quantize layer 2 ===
[2024-12-23 09:13:58 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-23 09:14:47 root] (abq_llm.py 321): INFO layer 2 iter 0 loss:0.18153686821460724 norm:0.02947165071964264 max memory_allocated 29718.22314453125 
[2024-12-23 09:15:30 root] (abq_llm.py 321): INFO layer 2 iter 1 loss:0.15744692087173462 norm:0.02191052958369255 max memory_allocated 29718.22314453125 
[2024-12-23 09:16:14 root] (abq_llm.py 321): INFO layer 2 iter 2 loss:0.14280575513839722 norm:0.015689758583903313 max memory_allocated 29718.22314453125 
[2024-12-23 09:16:57 root] (abq_llm.py 321): INFO layer 2 iter 3 loss:0.1362675130367279 norm:0.011447673663496971 max memory_allocated 29718.22314453125 
[2024-12-23 09:17:41 root] (abq_llm.py 321): INFO layer 2 iter 4 loss:0.13269756734371185 norm:0.008719785138964653 max memory_allocated 29718.22314453125 
[2024-12-23 09:18:25 root] (abq_llm.py 321): INFO layer 2 iter 5 loss:0.1304953694343567 norm:0.007445916999131441 max memory_allocated 29718.22314453125 
[2024-12-23 09:19:08 root] (abq_llm.py 321): INFO layer 2 iter 6 loss:0.1292971819639206 norm:0.006786668673157692 max memory_allocated 29718.22314453125 
[2024-12-23 09:19:52 root] (abq_llm.py 321): INFO layer 2 iter 7 loss:0.1282452642917633 norm:0.0063340081833302975 max memory_allocated 29718.22314453125 
[2024-12-23 09:20:35 root] (abq_llm.py 321): INFO layer 2 iter 8 loss:0.12762722373008728 norm:0.0061308955773711205 max memory_allocated 29718.22314453125 
[2024-12-23 09:21:19 root] (abq_llm.py 321): INFO layer 2 iter 9 loss:0.12707240879535675 norm:0.005939022172242403 max memory_allocated 29718.22314453125 
[2024-12-23 09:22:02 root] (abq_llm.py 321): INFO layer 2 iter 10 loss:0.1264583021402359 norm:0.0056970808655023575 max memory_allocated 29718.22314453125 
[2024-12-23 09:22:46 root] (abq_llm.py 321): INFO layer 2 iter 11 loss:0.12597912549972534 norm:0.005530834663659334 max memory_allocated 29718.22314453125 
[2024-12-23 09:23:30 root] (abq_llm.py 321): INFO layer 2 iter 12 loss:0.12578898668289185 norm:0.005499081686139107 max memory_allocated 29718.22314453125 
[2024-12-23 09:24:13 root] (abq_llm.py 321): INFO layer 2 iter 13 loss:0.12557056546211243 norm:0.005386818200349808 max memory_allocated 29718.22314453125 
[2024-12-23 09:24:57 root] (abq_llm.py 321): INFO layer 2 iter 14 loss:0.12533849477767944 norm:0.0052774338982999325 max memory_allocated 29718.22314453125 
[2024-12-23 09:25:40 root] (abq_llm.py 321): INFO layer 2 iter 15 loss:0.12530706822872162 norm:0.005147146061062813 max memory_allocated 29718.22314453125 
[2024-12-23 09:26:24 root] (abq_llm.py 321): INFO layer 2 iter 16 loss:0.12523013353347778 norm:0.005147747229784727 max memory_allocated 29718.22314453125 
[2024-12-23 09:27:08 root] (abq_llm.py 321): INFO layer 2 iter 17 loss:0.12511955201625824 norm:0.004999923519790173 max memory_allocated 29718.22314453125 
[2024-12-23 09:27:51 root] (abq_llm.py 321): INFO layer 2 iter 18 loss:0.12504161894321442 norm:0.00494998088106513 max memory_allocated 29718.22314453125 
[2024-12-23 09:28:35 root] (abq_llm.py 321): INFO layer 2 iter 19 loss:0.12497806549072266 norm:0.004828424658626318 max memory_allocated 29718.22314453125 
[2024-12-23 09:28:47 root] (abq_llm.py 208): INFO === Start quantize layer 3 ===
[2024-12-23 09:29:34 root] (abq_llm.py 321): INFO layer 3 iter 0 loss:0.44210928678512573 norm:0.08494840562343597 max memory_allocated 29722.14111328125 
[2024-12-23 09:30:17 root] (abq_llm.py 321): INFO layer 3 iter 1 loss:0.3851422965526581 norm:0.04735058918595314 max memory_allocated 29722.14111328125 
[2024-12-23 09:31:01 root] (abq_llm.py 321): INFO layer 3 iter 2 loss:0.3381718397140503 norm:0.02872297167778015 max memory_allocated 29722.14111328125 
[2024-12-23 09:31:44 root] (abq_llm.py 321): INFO layer 3 iter 3 loss:0.3143732249736786 norm:0.024886181578040123 max memory_allocated 29722.14111328125 
[2024-12-23 09:32:27 root] (abq_llm.py 321): INFO layer 3 iter 4 loss:0.3022063970565796 norm:0.022986052557826042 max memory_allocated 29722.14111328125 
[2024-12-23 09:33:11 root] (abq_llm.py 321): INFO layer 3 iter 5 loss:0.29027172923088074 norm:0.0223721731454134 max memory_allocated 29722.14111328125 
[2024-12-23 09:33:54 root] (abq_llm.py 321): INFO layer 3 iter 6 loss:0.28215739130973816 norm:0.020447902381420135 max memory_allocated 29722.14111328125 
[2024-12-23 09:34:38 root] (abq_llm.py 321): INFO layer 3 iter 7 loss:0.27843672037124634 norm:0.017798004671931267 max memory_allocated 29722.14111328125 
[2024-12-23 09:35:21 root] (abq_llm.py 321): INFO layer 3 iter 8 loss:0.2750813066959381 norm:0.016607588157057762 max memory_allocated 29722.14111328125 
[2024-12-23 09:36:04 root] (abq_llm.py 321): INFO layer 3 iter 9 loss:0.269751638174057 norm:0.015448475256562233 max memory_allocated 29722.14111328125 
[2024-12-23 09:36:48 root] (abq_llm.py 321): INFO layer 3 iter 10 loss:0.2676069140434265 norm:0.015143953263759613 max memory_allocated 29722.14111328125 
[2024-12-23 09:37:31 root] (abq_llm.py 321): INFO layer 3 iter 11 loss:0.26553648710250854 norm:0.014688888564705849 max memory_allocated 29722.14111328125 
[2024-12-23 09:38:15 root] (abq_llm.py 321): INFO layer 3 iter 12 loss:0.2658199667930603 norm:0.016045302152633667 max memory_allocated 29722.14111328125 
[2024-12-23 09:38:58 root] (abq_llm.py 321): INFO layer 3 iter 13 loss:0.2647189497947693 norm:0.013869605027139187 max memory_allocated 29722.14111328125 
[2024-12-23 09:39:41 root] (abq_llm.py 321): INFO layer 3 iter 14 loss:0.26437899470329285 norm:0.014011059887707233 max memory_allocated 29722.14111328125 
[2024-12-23 09:40:25 root] (abq_llm.py 321): INFO layer 3 iter 15 loss:0.2634381949901581 norm:0.014712374657392502 max memory_allocated 29722.14111328125 
[2024-12-23 09:41:08 root] (abq_llm.py 321): INFO layer 3 iter 16 loss:0.26230210065841675 norm:0.014761273749172688 max memory_allocated 29722.14111328125 
[2024-12-23 09:41:52 root] (abq_llm.py 321): INFO layer 3 iter 17 loss:0.2623633146286011 norm:0.015337338671088219 max memory_allocated 29722.14111328125 
[2024-12-23 09:42:35 root] (abq_llm.py 321): INFO layer 3 iter 18 loss:0.26137226819992065 norm:0.01527940109372139 max memory_allocated 29722.14111328125 
[2024-12-23 09:43:19 root] (abq_llm.py 321): INFO layer 3 iter 19 loss:0.26077091693878174 norm:0.014500461518764496 max memory_allocated 29722.14111328125 
[2024-12-23 09:43:31 root] (abq_llm.py 208): INFO === Start quantize layer 4 ===
[2024-12-23 09:44:17 root] (abq_llm.py 321): INFO layer 4 iter 0 loss:0.32917964458465576 norm:0.01819906383752823 max memory_allocated 29724.20361328125 
[2024-12-23 09:45:01 root] (abq_llm.py 321): INFO layer 4 iter 1 loss:0.30139097571372986 norm:0.0179862342774868 max memory_allocated 29724.20361328125 
[2024-12-23 09:45:44 root] (abq_llm.py 321): INFO layer 4 iter 2 loss:0.28267180919647217 norm:0.022572869434952736 max memory_allocated 29724.20361328125 
[2024-12-23 09:46:27 root] (abq_llm.py 321): INFO layer 4 iter 3 loss:0.27366912364959717 norm:0.016761504113674164 max memory_allocated 29724.20361328125 
[2024-12-23 09:47:11 root] (abq_llm.py 321): INFO layer 4 iter 4 loss:0.2694498598575592 norm:0.01838655397295952 max memory_allocated 29724.20361328125 
[2024-12-23 09:47:54 root] (abq_llm.py 321): INFO layer 4 iter 5 loss:0.26665374636650085 norm:0.06537199020385742 max memory_allocated 29724.20361328125 
[2024-12-23 09:48:38 root] (abq_llm.py 321): INFO layer 4 iter 6 loss:0.26387035846710205 norm:0.0056354268454015255 max memory_allocated 29724.20361328125 
[2024-12-23 09:49:21 root] (abq_llm.py 321): INFO layer 4 iter 7 loss:0.26225870847702026 norm:0.005215615499764681 max memory_allocated 29724.20361328125 
[2024-12-23 09:50:05 root] (abq_llm.py 321): INFO layer 4 iter 8 loss:0.2611403465270996 norm:0.00460355170071125 max memory_allocated 29724.20361328125 
[2024-12-23 09:50:48 root] (abq_llm.py 321): INFO layer 4 iter 9 loss:0.2602837383747101 norm:0.004597727674990892 max memory_allocated 29724.20361328125 
[2024-12-23 09:51:31 root] (abq_llm.py 321): INFO layer 4 iter 10 loss:0.2594207227230072 norm:0.004485361743718386 max memory_allocated 29724.20361328125 
[2024-12-23 09:52:15 root] (abq_llm.py 321): INFO layer 4 iter 11 loss:0.2582872211933136 norm:0.004156784154474735 max memory_allocated 29724.20361328125 
[2024-12-23 09:52:58 root] (abq_llm.py 321): INFO layer 4 iter 12 loss:0.2576737403869629 norm:0.004107921849936247 max memory_allocated 29724.20361328125 
[2024-12-23 09:53:42 root] (abq_llm.py 321): INFO layer 4 iter 13 loss:0.2571292519569397 norm:0.0038583907298743725 max memory_allocated 29724.20361328125 
[2024-12-23 09:54:25 root] (abq_llm.py 321): INFO layer 4 iter 14 loss:0.2567421793937683 norm:0.003820589277893305 max memory_allocated 29724.20361328125 
[2024-12-23 09:55:09 root] (abq_llm.py 321): INFO layer 4 iter 15 loss:0.25619158148765564 norm:0.0038164881989359856 max memory_allocated 29724.20361328125 
[2024-12-23 09:55:52 root] (abq_llm.py 321): INFO layer 4 iter 16 loss:0.2559036910533905 norm:0.003754667704924941 max memory_allocated 29724.20361328125 
[2024-12-23 09:56:35 root] (abq_llm.py 321): INFO layer 4 iter 17 loss:0.2555827796459198 norm:0.0037416096311062574 max memory_allocated 29724.20361328125 
[2024-12-23 09:57:19 root] (abq_llm.py 321): INFO layer 4 iter 18 loss:0.25518798828125 norm:0.0037052235566079617 max memory_allocated 29724.20361328125 
[2024-12-23 09:58:02 root] (abq_llm.py 321): INFO layer 4 iter 19 loss:0.25491422414779663 norm:0.0038174400106072426 max memory_allocated 29724.20361328125 
[2024-12-23 09:58:14 root] (abq_llm.py 208): INFO === Start quantize layer 5 ===
[2024-12-23 09:59:01 root] (abq_llm.py 321): INFO layer 5 iter 0 loss:0.3706560730934143 norm:0.019466374069452286 max memory_allocated 29726.26611328125 
[2024-12-23 09:59:44 root] (abq_llm.py 321): INFO layer 5 iter 1 loss:0.34257984161376953 norm:0.01047671027481556 max memory_allocated 29726.26611328125 
[2024-12-23 10:00:28 root] (abq_llm.py 321): INFO layer 5 iter 2 loss:0.32236248254776 norm:0.007536951452493668 max memory_allocated 29726.26611328125 
[2024-12-23 10:01:11 root] (abq_llm.py 321): INFO layer 5 iter 3 loss:0.31268924474716187 norm:0.006074949633330107 max memory_allocated 29726.26611328125 
[2024-12-23 10:01:55 root] (abq_llm.py 321): INFO layer 5 iter 4 loss:0.30760371685028076 norm:0.00516398623585701 max memory_allocated 29726.26611328125 
[2024-12-23 10:02:38 root] (abq_llm.py 321): INFO layer 5 iter 5 loss:0.3041701018810272 norm:0.004610909614712 max memory_allocated 29726.26611328125 
[2024-12-23 10:03:22 root] (abq_llm.py 321): INFO layer 5 iter 6 loss:0.30141976475715637 norm:0.004216914530843496 max memory_allocated 29726.26611328125 
[2024-12-23 10:04:05 root] (abq_llm.py 321): INFO layer 5 iter 7 loss:0.2991703748703003 norm:0.0038084506522864103 max memory_allocated 29726.26611328125 
[2024-12-23 10:04:48 root] (abq_llm.py 321): INFO layer 5 iter 8 loss:0.2975767254829407 norm:0.0037364810705184937 max memory_allocated 29726.26611328125 
[2024-12-23 10:05:32 root] (abq_llm.py 321): INFO layer 5 iter 9 loss:0.29626575112342834 norm:0.003680967725813389 max memory_allocated 29726.26611328125 
[2024-12-23 10:06:15 root] (abq_llm.py 321): INFO layer 5 iter 10 loss:0.2955746054649353 norm:0.0038689938373863697 max memory_allocated 29726.26611328125 
[2024-12-23 10:06:59 root] (abq_llm.py 321): INFO layer 5 iter 11 loss:0.29474684596061707 norm:0.003472265088930726 max memory_allocated 29726.26611328125 
[2024-12-23 10:07:42 root] (abq_llm.py 321): INFO layer 5 iter 12 loss:0.29453766345977783 norm:0.0037730825133621693 max memory_allocated 29726.26611328125 
[2024-12-23 10:08:25 root] (abq_llm.py 321): INFO layer 5 iter 13 loss:0.2938991189002991 norm:0.0034019812010228634 max memory_allocated 29726.26611328125 
[2024-12-23 10:09:09 root] (abq_llm.py 321): INFO layer 5 iter 14 loss:0.293418288230896 norm:0.003418892389163375 max memory_allocated 29726.26611328125 
[2024-12-23 10:09:52 root] (abq_llm.py 321): INFO layer 5 iter 15 loss:0.2930901348590851 norm:0.0033579866867512465 max memory_allocated 29726.26611328125 
[2024-12-23 10:10:36 root] (abq_llm.py 321): INFO layer 5 iter 16 loss:0.2927608788013458 norm:0.003219245932996273 max memory_allocated 29726.26611328125 
[2024-12-23 10:11:19 root] (abq_llm.py 321): INFO layer 5 iter 17 loss:0.2924850881099701 norm:0.0032716281712055206 max memory_allocated 29726.26611328125 
[2024-12-23 10:12:03 root] (abq_llm.py 321): INFO layer 5 iter 18 loss:0.29232487082481384 norm:0.003374616615474224 max memory_allocated 29726.26611328125 
[2024-12-23 10:12:46 root] (abq_llm.py 321): INFO layer 5 iter 19 loss:0.2919400930404663 norm:0.0031944464426487684 max memory_allocated 29726.26611328125 
[2024-12-23 10:12:58 root] (abq_llm.py 208): INFO === Start quantize layer 6 ===
[2024-12-23 10:13:45 root] (abq_llm.py 321): INFO layer 6 iter 0 loss:0.4182033836841583 norm:0.030540790408849716 max memory_allocated 29728.32861328125 
[2024-12-23 10:14:28 root] (abq_llm.py 321): INFO layer 6 iter 1 loss:0.37726423144340515 norm:0.011103307828307152 max memory_allocated 29728.32861328125 
[2024-12-23 10:15:12 root] (abq_llm.py 321): INFO layer 6 iter 2 loss:0.35126635432243347 norm:0.006350953597575426 max memory_allocated 29728.32861328125 
[2024-12-23 10:15:55 root] (abq_llm.py 321): INFO layer 6 iter 3 loss:0.33891481161117554 norm:0.004956341348588467 max memory_allocated 29728.32861328125 
[2024-12-23 10:16:38 root] (abq_llm.py 321): INFO layer 6 iter 4 loss:0.3337826728820801 norm:0.00442292308434844 max memory_allocated 29728.32861328125 
[2024-12-23 10:17:22 root] (abq_llm.py 321): INFO layer 6 iter 5 loss:0.3300619125366211 norm:0.004132759291678667 max memory_allocated 29728.32861328125 
[2024-12-23 10:18:05 root] (abq_llm.py 321): INFO layer 6 iter 6 loss:0.3272112011909485 norm:0.0037469009403139353 max memory_allocated 29728.32861328125 
[2024-12-23 10:18:49 root] (abq_llm.py 321): INFO layer 6 iter 7 loss:0.32514408230781555 norm:0.0036707560066133738 max memory_allocated 29728.32861328125 
[2024-12-23 10:19:32 root] (abq_llm.py 321): INFO layer 6 iter 8 loss:0.32376816868782043 norm:0.003548857057467103 max memory_allocated 29728.32861328125 
[2024-12-23 10:20:15 root] (abq_llm.py 321): INFO layer 6 iter 9 loss:0.322823166847229 norm:0.003472246928140521 max memory_allocated 29728.32861328125 
[2024-12-23 10:20:59 root] (abq_llm.py 321): INFO layer 6 iter 10 loss:0.32209470868110657 norm:0.003463392611593008 max memory_allocated 29728.32861328125 
[2024-12-23 10:21:42 root] (abq_llm.py 321): INFO layer 6 iter 11 loss:0.3216254711151123 norm:0.0033253899309784174 max memory_allocated 29728.32861328125 
[2024-12-23 10:22:26 root] (abq_llm.py 321): INFO layer 6 iter 12 loss:0.3211499750614166 norm:0.0032342677004635334 max memory_allocated 29728.32861328125 
[2024-12-23 10:23:09 root] (abq_llm.py 321): INFO layer 6 iter 13 loss:0.3207095265388489 norm:0.0031474793795496225 max memory_allocated 29728.32861328125 
[2024-12-23 10:23:53 root] (abq_llm.py 321): INFO layer 6 iter 14 loss:0.32022711634635925 norm:0.003100391011685133 max memory_allocated 29728.32861328125 
[2024-12-23 10:24:36 root] (abq_llm.py 321): INFO layer 6 iter 15 loss:0.31987088918685913 norm:0.003036769572645426 max memory_allocated 29728.32861328125 
[2024-12-23 10:25:19 root] (abq_llm.py 321): INFO layer 6 iter 16 loss:0.31959280371665955 norm:0.003062762785702944 max memory_allocated 29728.32861328125 
[2024-12-23 10:26:03 root] (abq_llm.py 321): INFO layer 6 iter 17 loss:0.3194316029548645 norm:0.0030813042540103197 max memory_allocated 29728.32861328125 
[2024-12-23 10:26:46 root] (abq_llm.py 321): INFO layer 6 iter 18 loss:0.31926268339157104 norm:0.0030182679183781147 max memory_allocated 29728.32861328125 
[2024-12-23 10:27:30 root] (abq_llm.py 321): INFO layer 6 iter 19 loss:0.3189844489097595 norm:0.0029703325126320124 max memory_allocated 29728.32861328125 
[2024-12-23 10:27:42 root] (abq_llm.py 208): INFO === Start quantize layer 7 ===
[2024-12-23 10:28:28 root] (abq_llm.py 321): INFO layer 7 iter 0 loss:0.4644956588745117 norm:0.021859435364603996 max memory_allocated 29730.39111328125 
[2024-12-23 10:29:12 root] (abq_llm.py 321): INFO layer 7 iter 1 loss:0.4247933030128479 norm:0.009613389149308205 max memory_allocated 29730.39111328125 
[2024-12-23 10:29:55 root] (abq_llm.py 321): INFO layer 7 iter 2 loss:0.3927866220474243 norm:0.006797230336815119 max memory_allocated 29730.39111328125 
[2024-12-23 10:30:39 root] (abq_llm.py 321): INFO layer 7 iter 3 loss:0.3772500157356262 norm:0.005341595504432917 max memory_allocated 29730.39111328125 
[2024-12-23 10:31:22 root] (abq_llm.py 321): INFO layer 7 iter 4 loss:0.3711753487586975 norm:0.004797669593244791 max memory_allocated 29730.39111328125 
[2024-12-23 10:32:05 root] (abq_llm.py 321): INFO layer 7 iter 5 loss:0.36712026596069336 norm:0.004363375715911388 max memory_allocated 29730.39111328125 
[2024-12-23 10:32:49 root] (abq_llm.py 321): INFO layer 7 iter 6 loss:0.3641974925994873 norm:0.004473281558603048 max memory_allocated 29730.39111328125 
[2024-12-23 10:33:32 root] (abq_llm.py 321): INFO layer 7 iter 7 loss:0.3621884286403656 norm:0.004385585431009531 max memory_allocated 29730.39111328125 
[2024-12-23 10:34:16 root] (abq_llm.py 321): INFO layer 7 iter 8 loss:0.3600957989692688 norm:0.004301866516470909 max memory_allocated 29730.39111328125 
[2024-12-23 10:34:59 root] (abq_llm.py 321): INFO layer 7 iter 9 loss:0.3585176467895508 norm:0.004115690011531115 max memory_allocated 29730.39111328125 
[2024-12-23 10:35:43 root] (abq_llm.py 321): INFO layer 7 iter 10 loss:0.35752159357070923 norm:0.003824485931545496 max memory_allocated 29730.39111328125 
[2024-12-23 10:36:26 root] (abq_llm.py 321): INFO layer 7 iter 11 loss:0.35683706402778625 norm:0.003713182406499982 max memory_allocated 29730.39111328125 
[2024-12-23 10:37:09 root] (abq_llm.py 321): INFO layer 7 iter 12 loss:0.35629576444625854 norm:0.0038388206157833338 max memory_allocated 29730.39111328125 
[2024-12-23 10:37:53 root] (abq_llm.py 321): INFO layer 7 iter 13 loss:0.35586899518966675 norm:0.0038462986703962088 max memory_allocated 29730.39111328125 
[2024-12-23 10:38:36 root] (abq_llm.py 321): INFO layer 7 iter 14 loss:0.3555634915828705 norm:0.003654362866654992 max memory_allocated 29730.39111328125 
[2024-12-23 10:39:20 root] (abq_llm.py 321): INFO layer 7 iter 15 loss:0.35533034801483154 norm:0.0035594890359789133 max memory_allocated 29730.39111328125 
[2024-12-23 10:40:03 root] (abq_llm.py 321): INFO layer 7 iter 16 loss:0.35505223274230957 norm:0.0034380012657493353 max memory_allocated 29730.39111328125 
[2024-12-23 10:40:47 root] (abq_llm.py 321): INFO layer 7 iter 17 loss:0.3548305630683899 norm:0.003370929043740034 max memory_allocated 29730.39111328125 
[2024-12-23 10:41:30 root] (abq_llm.py 321): INFO layer 7 iter 18 loss:0.35465919971466064 norm:0.003347645280882716 max memory_allocated 29730.39111328125 
[2024-12-23 10:42:13 root] (abq_llm.py 321): INFO layer 7 iter 19 loss:0.3543899953365326 norm:0.003356111701577902 max memory_allocated 29730.39111328125 
[2024-12-23 10:42:26 root] (abq_llm.py 208): INFO === Start quantize layer 8 ===
[2024-12-23 10:43:12 root] (abq_llm.py 321): INFO layer 8 iter 0 loss:0.5079703330993652 norm:0.04306967556476593 max memory_allocated 29732.45361328125 
[2024-12-23 10:43:56 root] (abq_llm.py 321): INFO layer 8 iter 1 loss:0.46341389417648315 norm:0.021039767190814018 max memory_allocated 29732.45361328125 
[2024-12-23 10:44:39 root] (abq_llm.py 321): INFO layer 8 iter 2 loss:0.42176204919815063 norm:0.008078338578343391 max memory_allocated 29732.45361328125 
[2024-12-23 10:45:22 root] (abq_llm.py 321): INFO layer 8 iter 3 loss:0.4049968123435974 norm:0.005755985155701637 max memory_allocated 29732.45361328125 
[2024-12-23 10:46:06 root] (abq_llm.py 321): INFO layer 8 iter 4 loss:0.39792704582214355 norm:0.0050425040535628796 max memory_allocated 29732.45361328125 
[2024-12-23 10:46:49 root] (abq_llm.py 321): INFO layer 8 iter 5 loss:0.3931613564491272 norm:0.004706642124801874 max memory_allocated 29732.45361328125 
[2024-12-23 10:47:33 root] (abq_llm.py 321): INFO layer 8 iter 6 loss:0.3894452750682831 norm:0.004503689706325531 max memory_allocated 29732.45361328125 
[2024-12-23 10:48:16 root] (abq_llm.py 321): INFO layer 8 iter 7 loss:0.3864646553993225 norm:0.00426600594073534 max memory_allocated 29732.45361328125 
[2024-12-23 10:49:00 root] (abq_llm.py 321): INFO layer 8 iter 8 loss:0.3846018314361572 norm:0.004020151682198048 max memory_allocated 29732.45361328125 
[2024-12-23 10:49:43 root] (abq_llm.py 321): INFO layer 8 iter 9 loss:0.38315680623054504 norm:0.0037080361507833004 max memory_allocated 29732.45361328125 
[2024-12-23 10:50:26 root] (abq_llm.py 321): INFO layer 8 iter 10 loss:0.3821564316749573 norm:0.003638738999143243 max memory_allocated 29732.45361328125 
[2024-12-23 10:51:10 root] (abq_llm.py 321): INFO layer 8 iter 11 loss:0.3812076449394226 norm:0.0034116541501134634 max memory_allocated 29732.45361328125 
[2024-12-23 10:51:53 root] (abq_llm.py 321): INFO layer 8 iter 12 loss:0.3805295526981354 norm:0.0033081506844609976 max memory_allocated 29732.45361328125 
[2024-12-23 10:52:37 root] (abq_llm.py 321): INFO layer 8 iter 13 loss:0.3800436854362488 norm:0.003164831316098571 max memory_allocated 29732.45361328125 
[2024-12-23 10:53:20 root] (abq_llm.py 321): INFO layer 8 iter 14 loss:0.3796604573726654 norm:0.0031112823635339737 max memory_allocated 29732.45361328125 
[2024-12-23 10:54:04 root] (abq_llm.py 321): INFO layer 8 iter 15 loss:0.3793320655822754 norm:0.0031025575008243322 max memory_allocated 29732.45361328125 
[2024-12-23 10:54:47 root] (abq_llm.py 321): INFO layer 8 iter 16 loss:0.37891218066215515 norm:0.003119625383988023 max memory_allocated 29732.45361328125 
[2024-12-23 10:55:30 root] (abq_llm.py 321): INFO layer 8 iter 17 loss:0.37861961126327515 norm:0.003011917695403099 max memory_allocated 29732.45361328125 
[2024-12-23 10:56:14 root] (abq_llm.py 321): INFO layer 8 iter 18 loss:0.3784550130367279 norm:0.002974078990519047 max memory_allocated 29732.45361328125 
[2024-12-23 10:56:57 root] (abq_llm.py 321): INFO layer 8 iter 19 loss:0.3782983720302582 norm:0.0029205549508333206 max memory_allocated 29732.45361328125 
[2024-12-23 10:57:10 root] (abq_llm.py 208): INFO === Start quantize layer 9 ===
[2024-12-23 10:57:56 root] (abq_llm.py 321): INFO layer 9 iter 0 loss:0.5713772773742676 norm:0.05149505287408829 max memory_allocated 29734.51611328125 
[2024-12-23 10:58:40 root] (abq_llm.py 321): INFO layer 9 iter 1 loss:0.5079066157341003 norm:0.017225489020347595 max memory_allocated 29734.51611328125 
[2024-12-23 10:59:23 root] (abq_llm.py 321): INFO layer 9 iter 2 loss:0.4641406536102295 norm:0.009984451346099377 max memory_allocated 29734.51611328125 
[2024-12-23 11:00:06 root] (abq_llm.py 321): INFO layer 9 iter 3 loss:0.4408436119556427 norm:0.007188962772488594 max memory_allocated 29734.51611328125 
[2024-12-23 11:00:50 root] (abq_llm.py 321): INFO layer 9 iter 4 loss:0.4320949912071228 norm:0.006084658671170473 max memory_allocated 29734.51611328125 
[2024-12-23 11:01:33 root] (abq_llm.py 321): INFO layer 9 iter 5 loss:0.42655572295188904 norm:0.005426859483122826 max memory_allocated 29734.51611328125 
[2024-12-23 11:02:17 root] (abq_llm.py 321): INFO layer 9 iter 6 loss:0.42235279083251953 norm:0.004882472567260265 max memory_allocated 29734.51611328125 
[2024-12-23 11:03:00 root] (abq_llm.py 321): INFO layer 9 iter 7 loss:0.4192507863044739 norm:0.004546529613435268 max memory_allocated 29734.51611328125 
[2024-12-23 11:03:43 root] (abq_llm.py 321): INFO layer 9 iter 8 loss:0.41678184270858765 norm:0.004666345193982124 max memory_allocated 29734.51611328125 
[2024-12-23 11:04:27 root] (abq_llm.py 321): INFO layer 9 iter 9 loss:0.41506683826446533 norm:0.004380378872156143 max memory_allocated 29734.51611328125 
[2024-12-23 11:05:10 root] (abq_llm.py 321): INFO layer 9 iter 10 loss:0.413939893245697 norm:0.0038785538636147976 max memory_allocated 29734.51611328125 
[2024-12-23 11:05:54 root] (abq_llm.py 321): INFO layer 9 iter 11 loss:0.41307997703552246 norm:0.0036073834635317326 max memory_allocated 29734.51611328125 
[2024-12-23 11:06:37 root] (abq_llm.py 321): INFO layer 9 iter 12 loss:0.412489652633667 norm:0.003589686704799533 max memory_allocated 29734.51611328125 
[2024-12-23 11:07:21 root] (abq_llm.py 321): INFO layer 9 iter 13 loss:0.41195279359817505 norm:0.003475814824923873 max memory_allocated 29734.51611328125 
[2024-12-23 11:08:04 root] (abq_llm.py 321): INFO layer 9 iter 14 loss:0.4113308787345886 norm:0.0033640339970588684 max memory_allocated 29734.51611328125 
[2024-12-23 11:08:47 root] (abq_llm.py 321): INFO layer 9 iter 15 loss:0.41075313091278076 norm:0.003372497158125043 max memory_allocated 29734.51611328125 
[2024-12-23 11:09:31 root] (abq_llm.py 321): INFO layer 9 iter 16 loss:0.41043365001678467 norm:0.003241749946027994 max memory_allocated 29734.51611328125 
[2024-12-23 11:10:14 root] (abq_llm.py 321): INFO layer 9 iter 17 loss:0.4101520776748657 norm:0.003175476798787713 max memory_allocated 29734.51611328125 
[2024-12-23 11:10:58 root] (abq_llm.py 321): INFO layer 9 iter 18 loss:0.4100889265537262 norm:0.003177197650074959 max memory_allocated 29734.51611328125 
[2024-12-23 11:11:41 root] (abq_llm.py 321): INFO layer 9 iter 19 loss:0.4099687933921814 norm:0.003310411935672164 max memory_allocated 29734.51611328125 
[2024-12-23 11:11:53 root] (abq_llm.py 208): INFO === Start quantize layer 10 ===
[2024-12-23 11:12:40 root] (abq_llm.py 321): INFO layer 10 iter 0 loss:0.5420224666595459 norm:0.03033851832151413 max memory_allocated 29736.57861328125 
[2024-12-23 11:13:23 root] (abq_llm.py 321): INFO layer 10 iter 1 loss:0.5021653175354004 norm:0.012424028478562832 max memory_allocated 29736.57861328125 
[2024-12-23 11:14:07 root] (abq_llm.py 321): INFO layer 10 iter 2 loss:0.47040748596191406 norm:0.007336926646530628 max memory_allocated 29736.57861328125 
[2024-12-23 11:14:50 root] (abq_llm.py 321): INFO layer 10 iter 3 loss:0.4533178210258484 norm:0.005234745796769857 max memory_allocated 29736.57861328125 
[2024-12-23 11:15:34 root] (abq_llm.py 321): INFO layer 10 iter 4 loss:0.44621801376342773 norm:0.0045736320316791534 max memory_allocated 29736.57861328125 
[2024-12-23 11:16:17 root] (abq_llm.py 321): INFO layer 10 iter 5 loss:0.4417390823364258 norm:0.004296860657632351 max memory_allocated 29736.57861328125 
[2024-12-23 11:17:01 root] (abq_llm.py 321): INFO layer 10 iter 6 loss:0.4385228157043457 norm:0.0038494509644806385 max memory_allocated 29736.57861328125 
[2024-12-23 11:17:44 root] (abq_llm.py 321): INFO layer 10 iter 7 loss:0.4363403916358948 norm:0.0036184601485729218 max memory_allocated 29736.57861328125 
[2024-12-23 11:18:27 root] (abq_llm.py 321): INFO layer 10 iter 8 loss:0.43461138010025024 norm:0.003452041419222951 max memory_allocated 29736.57861328125 
[2024-12-23 11:19:11 root] (abq_llm.py 321): INFO layer 10 iter 9 loss:0.4333050847053528 norm:0.0034512353595346212 max memory_allocated 29736.57861328125 
[2024-12-23 11:19:54 root] (abq_llm.py 321): INFO layer 10 iter 10 loss:0.4322799742221832 norm:0.0032481523230671883 max memory_allocated 29736.57861328125 
[2024-12-23 11:20:38 root] (abq_llm.py 321): INFO layer 10 iter 11 loss:0.4314844012260437 norm:0.00317668030038476 max memory_allocated 29736.57861328125 
[2024-12-23 11:21:21 root] (abq_llm.py 321): INFO layer 10 iter 12 loss:0.43072575330734253 norm:0.0030930712819099426 max memory_allocated 29736.57861328125 
[2024-12-23 11:22:05 root] (abq_llm.py 321): INFO layer 10 iter 13 loss:0.43023186922073364 norm:0.003115506377071142 max memory_allocated 29736.57861328125 
[2024-12-23 11:22:48 root] (abq_llm.py 321): INFO layer 10 iter 14 loss:0.4297525882720947 norm:0.003138264873996377 max memory_allocated 29736.57861328125 
[2024-12-23 11:23:32 root] (abq_llm.py 321): INFO layer 10 iter 15 loss:0.42938345670700073 norm:0.002918809885159135 max memory_allocated 29736.57861328125 
[2024-12-23 11:24:15 root] (abq_llm.py 321): INFO layer 10 iter 16 loss:0.42900121212005615 norm:0.002873524557799101 max memory_allocated 29736.57861328125 
[2024-12-23 11:24:58 root] (abq_llm.py 321): INFO layer 10 iter 17 loss:0.42872804403305054 norm:0.0027800793759524822 max memory_allocated 29736.57861328125 
[2024-12-23 11:25:42 root] (abq_llm.py 321): INFO layer 10 iter 18 loss:0.42854318022727966 norm:0.0026846060063689947 max memory_allocated 29736.57861328125 
[2024-12-23 11:26:25 root] (abq_llm.py 321): INFO layer 10 iter 19 loss:0.42831742763519287 norm:0.002685842104256153 max memory_allocated 29736.57861328125 
[2024-12-23 11:26:38 root] (abq_llm.py 208): INFO === Start quantize layer 11 ===
[2024-12-23 11:27:24 root] (abq_llm.py 321): INFO layer 11 iter 0 loss:0.5564373135566711 norm:0.026245834305882454 max memory_allocated 29738.64111328125 
[2024-12-23 11:28:08 root] (abq_llm.py 321): INFO layer 11 iter 1 loss:0.5155070424079895 norm:0.010451806709170341 max memory_allocated 29738.64111328125 
[2024-12-23 11:28:51 root] (abq_llm.py 321): INFO layer 11 iter 2 loss:0.4843982458114624 norm:0.0063227703794837 max memory_allocated 29738.64111328125 
[2024-12-23 11:29:34 root] (abq_llm.py 321): INFO layer 11 iter 3 loss:0.46799618005752563 norm:0.004417440854012966 max memory_allocated 29738.64111328125 
[2024-12-23 11:30:18 root] (abq_llm.py 321): INFO layer 11 iter 4 loss:0.46134260296821594 norm:0.0035901565570384264 max memory_allocated 29738.64111328125 
[2024-12-23 11:31:01 root] (abq_llm.py 321): INFO layer 11 iter 5 loss:0.45690256357192993 norm:0.0031089121475815773 max memory_allocated 29738.64111328125 
[2024-12-23 11:31:45 root] (abq_llm.py 321): INFO layer 11 iter 6 loss:0.4536110758781433 norm:0.0028464847709983587 max memory_allocated 29738.64111328125 
[2024-12-23 11:32:28 root] (abq_llm.py 321): INFO layer 11 iter 7 loss:0.4512692093849182 norm:0.0026970284525305033 max memory_allocated 29738.64111328125 
[2024-12-23 11:33:12 root] (abq_llm.py 321): INFO layer 11 iter 8 loss:0.44958293437957764 norm:0.0026124780997633934 max memory_allocated 29738.64111328125 
[2024-12-23 11:33:55 root] (abq_llm.py 321): INFO layer 11 iter 9 loss:0.4482797086238861 norm:0.0026219533756375313 max memory_allocated 29738.64111328125 
[2024-12-23 11:34:38 root] (abq_llm.py 321): INFO layer 11 iter 10 loss:0.44726619124412537 norm:0.0026404941454529762 max memory_allocated 29738.64111328125 
[2024-12-23 11:35:22 root] (abq_llm.py 321): INFO layer 11 iter 11 loss:0.44666215777397156 norm:0.002543077804148197 max memory_allocated 29738.64111328125 
[2024-12-23 11:36:05 root] (abq_llm.py 321): INFO layer 11 iter 12 loss:0.44615551829338074 norm:0.002520861802622676 max memory_allocated 29738.64111328125 
[2024-12-23 11:36:49 root] (abq_llm.py 321): INFO layer 11 iter 13 loss:0.44572386145591736 norm:0.002534859348088503 max memory_allocated 29738.64111328125 
[2024-12-23 11:37:32 root] (abq_llm.py 321): INFO layer 11 iter 14 loss:0.44539904594421387 norm:0.002459712326526642 max memory_allocated 29738.64111328125 
[2024-12-23 11:38:16 root] (abq_llm.py 321): INFO layer 11 iter 15 loss:0.4450894892215729 norm:0.002381244208663702 max memory_allocated 29738.64111328125 
[2024-12-23 11:38:59 root] (abq_llm.py 321): INFO layer 11 iter 16 loss:0.4447095990180969 norm:0.0022857324220240116 max memory_allocated 29738.64111328125 
[2024-12-23 11:39:42 root] (abq_llm.py 321): INFO layer 11 iter 17 loss:0.4445396959781647 norm:0.002348866779357195 max memory_allocated 29738.64111328125 
[2024-12-23 11:40:26 root] (abq_llm.py 321): INFO layer 11 iter 18 loss:0.44428715109825134 norm:0.0022462159395217896 max memory_allocated 29738.64111328125 
[2024-12-23 11:41:09 root] (abq_llm.py 321): INFO layer 11 iter 19 loss:0.44410622119903564 norm:0.0021642930805683136 max memory_allocated 29738.64111328125 
[2024-12-23 11:41:21 root] (abq_llm.py 208): INFO === Start quantize layer 12 ===
[2024-12-23 11:42:08 root] (abq_llm.py 321): INFO layer 12 iter 0 loss:0.5686809420585632 norm:0.02453528344631195 max memory_allocated 29740.70361328125 
[2024-12-23 11:42:52 root] (abq_llm.py 321): INFO layer 12 iter 1 loss:0.5266762375831604 norm:0.009175201877951622 max memory_allocated 29740.70361328125 
[2024-12-23 11:43:35 root] (abq_llm.py 321): INFO layer 12 iter 2 loss:0.49378907680511475 norm:0.00638556107878685 max memory_allocated 29740.70361328125 
[2024-12-23 11:44:18 root] (abq_llm.py 321): INFO layer 12 iter 3 loss:0.47509893774986267 norm:0.004356559365987778 max memory_allocated 29740.70361328125 
[2024-12-23 11:45:02 root] (abq_llm.py 321): INFO layer 12 iter 4 loss:0.4680616855621338 norm:0.0033885689917951822 max memory_allocated 29740.70361328125 
[2024-12-23 11:45:45 root] (abq_llm.py 321): INFO layer 12 iter 5 loss:0.463869571685791 norm:0.0031614010222256184 max memory_allocated 29740.70361328125 
[2024-12-23 11:46:29 root] (abq_llm.py 321): INFO layer 12 iter 6 loss:0.46085092425346375 norm:0.0029272304382175207 max memory_allocated 29740.70361328125 
[2024-12-23 11:47:12 root] (abq_llm.py 321): INFO layer 12 iter 7 loss:0.45867738127708435 norm:0.0027689924463629723 max memory_allocated 29740.70361328125 
[2024-12-23 11:47:55 root] (abq_llm.py 321): INFO layer 12 iter 8 loss:0.45707428455352783 norm:0.002709073480218649 max memory_allocated 29740.70361328125 
[2024-12-23 11:48:39 root] (abq_llm.py 321): INFO layer 12 iter 9 loss:0.4559866487979889 norm:0.002744014374911785 max memory_allocated 29740.70361328125 
[2024-12-23 11:49:22 root] (abq_llm.py 321): INFO layer 12 iter 10 loss:0.4551407992839813 norm:0.0027104218024760485 max memory_allocated 29740.70361328125 
[2024-12-23 11:50:06 root] (abq_llm.py 321): INFO layer 12 iter 11 loss:0.45442524552345276 norm:0.0026614638045430183 max memory_allocated 29740.70361328125 
[2024-12-23 11:50:49 root] (abq_llm.py 321): INFO layer 12 iter 12 loss:0.4539428651332855 norm:0.002615432720631361 max memory_allocated 29740.70361328125 
[2024-12-23 11:51:33 root] (abq_llm.py 321): INFO layer 12 iter 13 loss:0.4535701870918274 norm:0.0025090319104492664 max memory_allocated 29740.70361328125 
[2024-12-23 11:52:16 root] (abq_llm.py 321): INFO layer 12 iter 14 loss:0.45315322279930115 norm:0.002479624468833208 max memory_allocated 29740.70361328125 
[2024-12-23 11:52:59 root] (abq_llm.py 321): INFO layer 12 iter 15 loss:0.45281362533569336 norm:0.0025872820988297462 max memory_allocated 29740.70361328125 
[2024-12-23 11:53:43 root] (abq_llm.py 321): INFO layer 12 iter 16 loss:0.4524742066860199 norm:0.002532016485929489 max memory_allocated 29740.70361328125 
[2024-12-23 11:54:26 root] (abq_llm.py 321): INFO layer 12 iter 17 loss:0.4522032141685486 norm:0.0024944848846644163 max memory_allocated 29740.70361328125 
[2024-12-23 11:55:10 root] (abq_llm.py 321): INFO layer 12 iter 18 loss:0.45200201869010925 norm:0.002367227105423808 max memory_allocated 29740.70361328125 
[2024-12-23 11:55:53 root] (abq_llm.py 321): INFO layer 12 iter 19 loss:0.45190176367759705 norm:0.002318532904610038 max memory_allocated 29740.70361328125 
[2024-12-23 11:56:06 root] (abq_llm.py 208): INFO === Start quantize layer 13 ===
[2024-12-23 11:56:52 root] (abq_llm.py 321): INFO layer 13 iter 0 loss:0.5666972398757935 norm:0.030759243294596672 max memory_allocated 29742.76611328125 
[2024-12-23 11:57:35 root] (abq_llm.py 321): INFO layer 13 iter 1 loss:0.5297451615333557 norm:0.011761856265366077 max memory_allocated 29742.76611328125 
[2024-12-23 11:58:19 root] (abq_llm.py 321): INFO layer 13 iter 2 loss:0.5029030442237854 norm:0.0070835077203810215 max memory_allocated 29742.76611328125 
[2024-12-23 11:59:03 root] (abq_llm.py 321): INFO layer 13 iter 3 loss:0.4844653308391571 norm:0.004327218979597092 max memory_allocated 29742.76611328125 
[2024-12-23 11:59:47 root] (abq_llm.py 321): INFO layer 13 iter 4 loss:0.4771644175052643 norm:0.0035304673947393894 max memory_allocated 29742.76611328125 
[2024-12-23 12:00:30 root] (abq_llm.py 321): INFO layer 13 iter 5 loss:0.4729885756969452 norm:0.00321699446067214 max memory_allocated 29742.76611328125 
[2024-12-23 12:01:14 root] (abq_llm.py 321): INFO layer 13 iter 6 loss:0.4698881208896637 norm:0.0031736150849610567 max memory_allocated 29742.76611328125 
[2024-12-23 12:01:57 root] (abq_llm.py 321): INFO layer 13 iter 7 loss:0.4676293730735779 norm:0.0030458681285381317 max memory_allocated 29742.76611328125 
[2024-12-23 12:02:40 root] (abq_llm.py 321): INFO layer 13 iter 8 loss:0.46597373485565186 norm:0.002855453873053193 max memory_allocated 29742.76611328125 
[2024-12-23 12:03:24 root] (abq_llm.py 321): INFO layer 13 iter 9 loss:0.46470844745635986 norm:0.0027681586798280478 max memory_allocated 29742.76611328125 
[2024-12-23 12:04:07 root] (abq_llm.py 321): INFO layer 13 iter 10 loss:0.46370866894721985 norm:0.0027458469849079847 max memory_allocated 29742.76611328125 
[2024-12-23 12:04:51 root] (abq_llm.py 321): INFO layer 13 iter 11 loss:0.462822824716568 norm:0.002665912266820669 max memory_allocated 29742.76611328125 
[2024-12-23 12:05:34 root] (abq_llm.py 321): INFO layer 13 iter 12 loss:0.46214568614959717 norm:0.0026210949290543795 max memory_allocated 29742.76611328125 
[2024-12-23 12:06:17 root] (abq_llm.py 321): INFO layer 13 iter 13 loss:0.4616971015930176 norm:0.0026580796111375093 max memory_allocated 29742.76611328125 
[2024-12-23 12:07:01 root] (abq_llm.py 321): INFO layer 13 iter 14 loss:0.46137332916259766 norm:0.0025980721693485975 max memory_allocated 29742.76611328125 
[2024-12-23 12:07:44 root] (abq_llm.py 321): INFO layer 13 iter 15 loss:0.46096813678741455 norm:0.002505912911146879 max memory_allocated 29742.76611328125 
[2024-12-23 12:08:28 root] (abq_llm.py 321): INFO layer 13 iter 16 loss:0.4607420861721039 norm:0.0024489862844347954 max memory_allocated 29742.76611328125 
[2024-12-23 12:09:11 root] (abq_llm.py 321): INFO layer 13 iter 17 loss:0.4604952931404114 norm:0.002384084975346923 max memory_allocated 29742.76611328125 
[2024-12-23 12:09:55 root] (abq_llm.py 321): INFO layer 13 iter 18 loss:0.46026739478111267 norm:0.002413257723674178 max memory_allocated 29742.76611328125 
[2024-12-23 12:10:38 root] (abq_llm.py 321): INFO layer 13 iter 19 loss:0.46007588505744934 norm:0.002417703391984105 max memory_allocated 29742.76611328125 
[2024-12-23 12:10:50 root] (abq_llm.py 208): INFO === Start quantize layer 14 ===
[2024-12-23 12:11:37 root] (abq_llm.py 321): INFO layer 14 iter 0 loss:0.5675784349441528 norm:0.01872871071100235 max memory_allocated 29744.82861328125 
[2024-12-23 12:12:20 root] (abq_llm.py 321): INFO layer 14 iter 1 loss:0.5356070399284363 norm:0.009197711013257504 max memory_allocated 29744.82861328125 
[2024-12-23 12:13:04 root] (abq_llm.py 321): INFO layer 14 iter 2 loss:0.5110035538673401 norm:0.006269252393394709 max memory_allocated 29744.82861328125 
[2024-12-23 12:13:47 root] (abq_llm.py 321): INFO layer 14 iter 3 loss:0.4949582815170288 norm:0.00375948054715991 max memory_allocated 29744.82861328125 
[2024-12-23 12:14:31 root] (abq_llm.py 321): INFO layer 14 iter 4 loss:0.48816871643066406 norm:0.0029736512806266546 max memory_allocated 29744.82861328125 
[2024-12-23 12:15:14 root] (abq_llm.py 321): INFO layer 14 iter 5 loss:0.4840690493583679 norm:0.0026233734097331762 max memory_allocated 29744.82861328125 
[2024-12-23 12:15:57 root] (abq_llm.py 321): INFO layer 14 iter 6 loss:0.48121562600135803 norm:0.002421413082629442 max memory_allocated 29744.82861328125 
[2024-12-23 12:16:41 root] (abq_llm.py 321): INFO layer 14 iter 7 loss:0.4790135324001312 norm:0.0022568530403077602 max memory_allocated 29744.82861328125 
[2024-12-23 12:17:24 root] (abq_llm.py 321): INFO layer 14 iter 8 loss:0.47751736640930176 norm:0.0022329625207930803 max memory_allocated 29744.82861328125 
[2024-12-23 12:18:08 root] (abq_llm.py 321): INFO layer 14 iter 9 loss:0.4764300584793091 norm:0.0021405788138508797 max memory_allocated 29744.82861328125 
[2024-12-23 12:18:51 root] (abq_llm.py 321): INFO layer 14 iter 10 loss:0.4755786657333374 norm:0.0020932029001414776 max memory_allocated 29744.82861328125 
[2024-12-23 12:19:35 root] (abq_llm.py 321): INFO layer 14 iter 11 loss:0.4749706983566284 norm:0.0020713103003799915 max memory_allocated 29744.82861328125 
[2024-12-23 12:20:18 root] (abq_llm.py 321): INFO layer 14 iter 12 loss:0.4743938744068146 norm:0.0020844698883593082 max memory_allocated 29744.82861328125 
[2024-12-23 12:21:01 root] (abq_llm.py 321): INFO layer 14 iter 13 loss:0.4738865792751312 norm:0.0020877933129668236 max memory_allocated 29744.82861328125 
[2024-12-23 12:21:45 root] (abq_llm.py 321): INFO layer 14 iter 14 loss:0.47346287965774536 norm:0.002143389545381069 max memory_allocated 29744.82861328125 
[2024-12-23 12:22:28 root] (abq_llm.py 321): INFO layer 14 iter 15 loss:0.47315970063209534 norm:0.0020960960537195206 max memory_allocated 29744.82861328125 
[2024-12-23 12:23:12 root] (abq_llm.py 321): INFO layer 14 iter 16 loss:0.4729480445384979 norm:0.0020263802725821733 max memory_allocated 29744.82861328125 
[2024-12-23 12:23:55 root] (abq_llm.py 321): INFO layer 14 iter 17 loss:0.47274473309516907 norm:0.002010371768847108 max memory_allocated 29744.82861328125 
[2024-12-23 12:24:39 root] (abq_llm.py 321): INFO layer 14 iter 18 loss:0.47259700298309326 norm:0.001988754840567708 max memory_allocated 29744.82861328125 
[2024-12-23 12:25:22 root] (abq_llm.py 321): INFO layer 14 iter 19 loss:0.4725315570831299 norm:0.002028865972533822 max memory_allocated 29744.82861328125 
[2024-12-23 12:25:34 root] (abq_llm.py 208): INFO === Start quantize layer 15 ===
[2024-12-23 12:26:21 root] (abq_llm.py 321): INFO layer 15 iter 0 loss:0.5657101273536682 norm:0.019697006791830063 max memory_allocated 29746.89111328125 
[2024-12-23 12:27:04 root] (abq_llm.py 321): INFO layer 15 iter 1 loss:0.5360770225524902 norm:0.010379099287092686 max memory_allocated 29746.89111328125 
[2024-12-23 12:27:48 root] (abq_llm.py 321): INFO layer 15 iter 2 loss:0.5140610337257385 norm:0.007140223402529955 max memory_allocated 29746.89111328125 
[2024-12-23 12:28:31 root] (abq_llm.py 321): INFO layer 15 iter 3 loss:0.49842655658721924 norm:0.005170631222426891 max memory_allocated 29746.89111328125 
[2024-12-23 12:29:15 root] (abq_llm.py 321): INFO layer 15 iter 4 loss:0.49107033014297485 norm:0.004141765646636486 max memory_allocated 29746.89111328125 
[2024-12-23 12:29:58 root] (abq_llm.py 321): INFO layer 15 iter 5 loss:0.4867490828037262 norm:0.0036668903194367886 max memory_allocated 29746.89111328125 
[2024-12-23 12:30:41 root] (abq_llm.py 321): INFO layer 15 iter 6 loss:0.4835189878940582 norm:0.003408062271773815 max memory_allocated 29746.89111328125 
[2024-12-23 12:31:25 root] (abq_llm.py 321): INFO layer 15 iter 7 loss:0.48102283477783203 norm:0.003056017216295004 max memory_allocated 29746.89111328125 
[2024-12-23 12:32:08 root] (abq_llm.py 321): INFO layer 15 iter 8 loss:0.478849321603775 norm:0.0028271512128412724 max memory_allocated 29746.89111328125 
[2024-12-23 12:32:52 root] (abq_llm.py 321): INFO layer 15 iter 9 loss:0.47697532176971436 norm:0.0026133758947253227 max memory_allocated 29746.89111328125 
[2024-12-23 12:33:35 root] (abq_llm.py 321): INFO layer 15 iter 10 loss:0.47572943568229675 norm:0.0025338721461594105 max memory_allocated 29746.89111328125 
[2024-12-23 12:34:19 root] (abq_llm.py 321): INFO layer 15 iter 11 loss:0.4748901128768921 norm:0.0024858436081558466 max memory_allocated 29746.89111328125 
[2024-12-23 12:35:02 root] (abq_llm.py 321): INFO layer 15 iter 12 loss:0.4743521809577942 norm:0.0024742763489484787 max memory_allocated 29746.89111328125 
[2024-12-23 12:35:45 root] (abq_llm.py 321): INFO layer 15 iter 13 loss:0.47380703687667847 norm:0.0023830225691199303 max memory_allocated 29746.89111328125 
[2024-12-23 12:36:29 root] (abq_llm.py 321): INFO layer 15 iter 14 loss:0.47335267066955566 norm:0.002346979919821024 max memory_allocated 29746.89111328125 
[2024-12-23 12:37:12 root] (abq_llm.py 321): INFO layer 15 iter 15 loss:0.47285571694374084 norm:0.002375079784542322 max memory_allocated 29746.89111328125 
[2024-12-23 12:37:56 root] (abq_llm.py 321): INFO layer 15 iter 16 loss:0.4723891019821167 norm:0.0023049914743751287 max memory_allocated 29746.89111328125 
[2024-12-23 12:38:39 root] (abq_llm.py 321): INFO layer 15 iter 17 loss:0.4721415340900421 norm:0.002271064091473818 max memory_allocated 29746.89111328125 
[2024-12-23 12:39:22 root] (abq_llm.py 321): INFO layer 15 iter 18 loss:0.471938818693161 norm:0.00224740756675601 max memory_allocated 29746.89111328125 
[2024-12-23 12:40:06 root] (abq_llm.py 321): INFO layer 15 iter 19 loss:0.4716944396495819 norm:0.0022209116723388433 max memory_allocated 29746.89111328125 
[2024-12-23 12:40:18 root] (abq_llm.py 208): INFO === Start quantize layer 16 ===
[2024-12-23 12:41:05 root] (abq_llm.py 321): INFO layer 16 iter 0 loss:0.5874248147010803 norm:0.045000553131103516 max memory_allocated 29748.95361328125 
[2024-12-23 12:41:48 root] (abq_llm.py 321): INFO layer 16 iter 1 loss:0.5479717254638672 norm:0.016830334439873695 max memory_allocated 29748.95361328125 
[2024-12-23 12:42:31 root] (abq_llm.py 321): INFO layer 16 iter 2 loss:0.5220773816108704 norm:0.009123899973928928 max memory_allocated 29748.95361328125 
[2024-12-23 12:43:15 root] (abq_llm.py 321): INFO layer 16 iter 3 loss:0.5068109035491943 norm:0.005841213744133711 max memory_allocated 29748.95361328125 
[2024-12-23 12:43:58 root] (abq_llm.py 321): INFO layer 16 iter 4 loss:0.4997806251049042 norm:0.004459465388208628 max memory_allocated 29748.95361328125 
[2024-12-23 12:44:42 root] (abq_llm.py 321): INFO layer 16 iter 5 loss:0.4960903525352478 norm:0.004285931587219238 max memory_allocated 29748.95361328125 
[2024-12-23 12:45:25 root] (abq_llm.py 321): INFO layer 16 iter 6 loss:0.4932137131690979 norm:0.003962840884923935 max memory_allocated 29748.95361328125 
[2024-12-23 12:46:09 root] (abq_llm.py 321): INFO layer 16 iter 7 loss:0.49081721901893616 norm:0.0034816779661923647 max memory_allocated 29748.95361328125 
[2024-12-23 12:46:52 root] (abq_llm.py 321): INFO layer 16 iter 8 loss:0.4886997640132904 norm:0.0032007291447371244 max memory_allocated 29748.95361328125 
[2024-12-23 12:47:35 root] (abq_llm.py 321): INFO layer 16 iter 9 loss:0.487238347530365 norm:0.002979096956551075 max memory_allocated 29748.95361328125 
[2024-12-23 12:48:19 root] (abq_llm.py 321): INFO layer 16 iter 10 loss:0.486168771982193 norm:0.0028228634037077427 max memory_allocated 29748.95361328125 
[2024-12-23 12:49:02 root] (abq_llm.py 321): INFO layer 16 iter 11 loss:0.4852830767631531 norm:0.002707171719521284 max memory_allocated 29748.95361328125 
[2024-12-23 12:49:46 root] (abq_llm.py 321): INFO layer 16 iter 12 loss:0.4845370650291443 norm:0.00259387562982738 max memory_allocated 29748.95361328125 
[2024-12-23 12:50:29 root] (abq_llm.py 321): INFO layer 16 iter 13 loss:0.48393961787223816 norm:0.002517223823815584 max memory_allocated 29748.95361328125 
[2024-12-23 12:51:12 root] (abq_llm.py 321): INFO layer 16 iter 14 loss:0.4834405183792114 norm:0.0024968592915683985 max memory_allocated 29748.95361328125 
[2024-12-23 12:51:56 root] (abq_llm.py 321): INFO layer 16 iter 15 loss:0.4829815626144409 norm:0.0024114602711051702 max memory_allocated 29748.95361328125 
[2024-12-23 12:52:39 root] (abq_llm.py 321): INFO layer 16 iter 16 loss:0.48253360390663147 norm:0.0023451908491551876 max memory_allocated 29748.95361328125 
[2024-12-23 12:53:23 root] (abq_llm.py 321): INFO layer 16 iter 17 loss:0.4822978973388672 norm:0.0023103507701307535 max memory_allocated 29748.95361328125 
[2024-12-23 12:54:06 root] (abq_llm.py 321): INFO layer 16 iter 18 loss:0.4819452166557312 norm:0.002266370691359043 max memory_allocated 29748.95361328125 
[2024-12-23 12:54:50 root] (abq_llm.py 321): INFO layer 16 iter 19 loss:0.48156067728996277 norm:0.0022015392314642668 max memory_allocated 29748.95361328125 
[2024-12-23 12:55:02 root] (abq_llm.py 208): INFO === Start quantize layer 17 ===
[2024-12-23 12:55:48 root] (abq_llm.py 321): INFO layer 17 iter 0 loss:0.5740894079208374 norm:0.03569412603974342 max memory_allocated 29751.01611328125 
[2024-12-23 12:56:32 root] (abq_llm.py 321): INFO layer 17 iter 1 loss:0.5406116247177124 norm:0.013593270443379879 max memory_allocated 29751.01611328125 
[2024-12-23 12:57:15 root] (abq_llm.py 321): INFO layer 17 iter 2 loss:0.5200574994087219 norm:0.007679962087422609 max memory_allocated 29751.01611328125 
[2024-12-23 12:57:59 root] (abq_llm.py 321): INFO layer 17 iter 3 loss:0.5067911148071289 norm:0.004575859755277634 max memory_allocated 29751.01611328125 
[2024-12-23 12:58:42 root] (abq_llm.py 321): INFO layer 17 iter 4 loss:0.500956118106842 norm:0.0035417540930211544 max memory_allocated 29751.01611328125 
[2024-12-23 12:59:25 root] (abq_llm.py 321): INFO layer 17 iter 5 loss:0.49739521741867065 norm:0.0032643380109220743 max memory_allocated 29751.01611328125 
[2024-12-23 13:00:09 root] (abq_llm.py 321): INFO layer 17 iter 6 loss:0.4946953356266022 norm:0.003109742421656847 max memory_allocated 29751.01611328125 
[2024-12-23 13:00:52 root] (abq_llm.py 321): INFO layer 17 iter 7 loss:0.49266791343688965 norm:0.0029992288909852505 max memory_allocated 29751.01611328125 
[2024-12-23 13:01:36 root] (abq_llm.py 321): INFO layer 17 iter 8 loss:0.49087417125701904 norm:0.0027395172510296106 max memory_allocated 29751.01611328125 
[2024-12-23 13:02:19 root] (abq_llm.py 321): INFO layer 17 iter 9 loss:0.4895186424255371 norm:0.0026208143681287766 max memory_allocated 29751.01611328125 
[2024-12-23 13:03:02 root] (abq_llm.py 321): INFO layer 17 iter 10 loss:0.4884120225906372 norm:0.00246237451210618 max memory_allocated 29751.01611328125 
[2024-12-23 13:03:46 root] (abq_llm.py 321): INFO layer 17 iter 11 loss:0.4875023365020752 norm:0.002336313249543309 max memory_allocated 29751.01611328125 
[2024-12-23 13:04:29 root] (abq_llm.py 321): INFO layer 17 iter 12 loss:0.48682448267936707 norm:0.002209801459684968 max memory_allocated 29751.01611328125 
[2024-12-23 13:05:13 root] (abq_llm.py 321): INFO layer 17 iter 13 loss:0.486173152923584 norm:0.002173499669879675 max memory_allocated 29751.01611328125 
[2024-12-23 13:05:56 root] (abq_llm.py 321): INFO layer 17 iter 14 loss:0.4857247471809387 norm:0.002152751898393035 max memory_allocated 29751.01611328125 
[2024-12-23 13:06:40 root] (abq_llm.py 321): INFO layer 17 iter 15 loss:0.4853717088699341 norm:0.0021712486632168293 max memory_allocated 29751.01611328125 
[2024-12-23 13:07:23 root] (abq_llm.py 321): INFO layer 17 iter 16 loss:0.48490452766418457 norm:0.002125862054526806 max memory_allocated 29751.01611328125 
[2024-12-23 13:08:06 root] (abq_llm.py 321): INFO layer 17 iter 17 loss:0.4845159351825714 norm:0.0021361636463552713 max memory_allocated 29751.01611328125 
[2024-12-23 13:08:50 root] (abq_llm.py 321): INFO layer 17 iter 18 loss:0.4842458963394165 norm:0.0021271677687764168 max memory_allocated 29751.01611328125 
[2024-12-23 13:09:33 root] (abq_llm.py 321): INFO layer 17 iter 19 loss:0.4840654730796814 norm:0.002071442548185587 max memory_allocated 29751.01611328125 
[2024-12-23 13:09:45 root] (abq_llm.py 208): INFO === Start quantize layer 18 ===
[2024-12-23 13:10:32 root] (abq_llm.py 321): INFO layer 18 iter 0 loss:0.5720833539962769 norm:0.03847985342144966 max memory_allocated 29753.07861328125 
[2024-12-23 13:11:15 root] (abq_llm.py 321): INFO layer 18 iter 1 loss:0.5419446229934692 norm:0.01627444475889206 max memory_allocated 29753.07861328125 
[2024-12-23 13:11:59 root] (abq_llm.py 321): INFO layer 18 iter 2 loss:0.5234875082969666 norm:0.009520485997200012 max memory_allocated 29753.07861328125 
[2024-12-23 13:12:42 root] (abq_llm.py 321): INFO layer 18 iter 3 loss:0.5122422575950623 norm:0.005878392141312361 max memory_allocated 29753.07861328125 
[2024-12-23 13:13:26 root] (abq_llm.py 321): INFO layer 18 iter 4 loss:0.5069301724433899 norm:0.004347315523773432 max memory_allocated 29753.07861328125 
[2024-12-23 13:14:09 root] (abq_llm.py 321): INFO layer 18 iter 5 loss:0.5040295124053955 norm:0.003681322792544961 max memory_allocated 29753.07861328125 
[2024-12-23 13:14:52 root] (abq_llm.py 321): INFO layer 18 iter 6 loss:0.5017799735069275 norm:0.0033003902062773705 max memory_allocated 29753.07861328125 
[2024-12-23 13:15:36 root] (abq_llm.py 321): INFO layer 18 iter 7 loss:0.49984824657440186 norm:0.002949787536635995 max memory_allocated 29753.07861328125 
[2024-12-23 13:16:19 root] (abq_llm.py 321): INFO layer 18 iter 8 loss:0.49830397963523865 norm:0.0027949491050094366 max memory_allocated 29753.07861328125 
[2024-12-23 13:17:03 root] (abq_llm.py 321): INFO layer 18 iter 9 loss:0.4970948398113251 norm:0.002666295040398836 max memory_allocated 29753.07861328125 
[2024-12-23 13:17:46 root] (abq_llm.py 321): INFO layer 18 iter 10 loss:0.49613842368125916 norm:0.0025586322881281376 max memory_allocated 29753.07861328125 
[2024-12-23 13:18:29 root] (abq_llm.py 321): INFO layer 18 iter 11 loss:0.4953972101211548 norm:0.0024776484351605177 max memory_allocated 29753.07861328125 
[2024-12-23 13:19:13 root] (abq_llm.py 321): INFO layer 18 iter 12 loss:0.4946576952934265 norm:0.0023486479185521603 max memory_allocated 29753.07861328125 
[2024-12-23 13:19:56 root] (abq_llm.py 321): INFO layer 18 iter 13 loss:0.49400192499160767 norm:0.002306920476257801 max memory_allocated 29753.07861328125 
[2024-12-23 13:20:40 root] (abq_llm.py 321): INFO layer 18 iter 14 loss:0.49341094493865967 norm:0.002175640780478716 max memory_allocated 29753.07861328125 
[2024-12-23 13:21:23 root] (abq_llm.py 321): INFO layer 18 iter 15 loss:0.4929945170879364 norm:0.0021035028621554375 max memory_allocated 29753.07861328125 
[2024-12-23 13:22:06 root] (abq_llm.py 321): INFO layer 18 iter 16 loss:0.4927213490009308 norm:0.0020482754334807396 max memory_allocated 29753.07861328125 
[2024-12-23 13:22:50 root] (abq_llm.py 321): INFO layer 18 iter 17 loss:0.492498517036438 norm:0.0020463920664042234 max memory_allocated 29753.07861328125 
[2024-12-23 13:23:33 root] (abq_llm.py 321): INFO layer 18 iter 18 loss:0.49222826957702637 norm:0.002010950120165944 max memory_allocated 29753.07861328125 
[2024-12-23 13:24:17 root] (abq_llm.py 321): INFO layer 18 iter 19 loss:0.49204370379447937 norm:0.0020092525519430637 max memory_allocated 29753.07861328125 
[2024-12-23 13:24:29 root] (abq_llm.py 208): INFO === Start quantize layer 19 ===
[2024-12-23 13:25:16 root] (abq_llm.py 321): INFO layer 19 iter 0 loss:0.577392578125 norm:0.026960255578160286 max memory_allocated 29755.14111328125 
[2024-12-23 13:25:59 root] (abq_llm.py 321): INFO layer 19 iter 1 loss:0.5542812347412109 norm:0.013070514425635338 max memory_allocated 29755.14111328125 
[2024-12-23 13:26:42 root] (abq_llm.py 321): INFO layer 19 iter 2 loss:0.5357481241226196 norm:0.007778138853609562 max memory_allocated 29755.14111328125 
[2024-12-23 13:27:26 root] (abq_llm.py 321): INFO layer 19 iter 3 loss:0.5247020125389099 norm:0.004923270549625158 max memory_allocated 29755.14111328125 
[2024-12-23 13:28:09 root] (abq_llm.py 321): INFO layer 19 iter 4 loss:0.5201659798622131 norm:0.003915912937372923 max memory_allocated 29755.14111328125 
[2024-12-23 13:28:53 root] (abq_llm.py 321): INFO layer 19 iter 5 loss:0.5173421502113342 norm:0.0033281093928962946 max memory_allocated 29755.14111328125 
[2024-12-23 13:29:36 root] (abq_llm.py 321): INFO layer 19 iter 6 loss:0.5150858163833618 norm:0.0029114815406501293 max memory_allocated 29755.14111328125 
[2024-12-23 13:30:19 root] (abq_llm.py 321): INFO layer 19 iter 7 loss:0.5131127834320068 norm:0.00276012159883976 max memory_allocated 29755.14111328125 
[2024-12-23 13:31:03 root] (abq_llm.py 321): INFO layer 19 iter 8 loss:0.5114277601242065 norm:0.0025124596431851387 max memory_allocated 29755.14111328125 
[2024-12-23 13:31:46 root] (abq_llm.py 321): INFO layer 19 iter 9 loss:0.5102001428604126 norm:0.00235679280012846 max memory_allocated 29755.14111328125 
[2024-12-23 13:32:30 root] (abq_llm.py 321): INFO layer 19 iter 10 loss:0.5092937350273132 norm:0.0022906512022018433 max memory_allocated 29755.14111328125 
[2024-12-23 13:33:13 root] (abq_llm.py 321): INFO layer 19 iter 11 loss:0.5084922313690186 norm:0.00219333847053349 max memory_allocated 29755.14111328125 
[2024-12-23 13:33:56 root] (abq_llm.py 321): INFO layer 19 iter 12 loss:0.5078922510147095 norm:0.00222349283285439 max memory_allocated 29755.14111328125 
[2024-12-23 13:34:40 root] (abq_llm.py 321): INFO layer 19 iter 13 loss:0.5072525143623352 norm:0.0021332113537937403 max memory_allocated 29755.14111328125 
[2024-12-23 13:35:23 root] (abq_llm.py 321): INFO layer 19 iter 14 loss:0.5068936347961426 norm:0.002143500605598092 max memory_allocated 29755.14111328125 
[2024-12-23 13:36:07 root] (abq_llm.py 321): INFO layer 19 iter 15 loss:0.506537914276123 norm:0.0020965873263776302 max memory_allocated 29755.14111328125 
[2024-12-23 13:36:50 root] (abq_llm.py 321): INFO layer 19 iter 16 loss:0.5062528848648071 norm:0.002130217617377639 max memory_allocated 29755.14111328125 
[2024-12-23 13:37:34 root] (abq_llm.py 321): INFO layer 19 iter 17 loss:0.5059834122657776 norm:0.0021056453697383404 max memory_allocated 29755.14111328125 
[2024-12-23 13:38:17 root] (abq_llm.py 321): INFO layer 19 iter 18 loss:0.5056995153427124 norm:0.002024783054366708 max memory_allocated 29755.14111328125 
[2024-12-23 13:39:00 root] (abq_llm.py 321): INFO layer 19 iter 19 loss:0.5055184364318848 norm:0.002021356485784054 max memory_allocated 29755.14111328125 
[2024-12-23 13:39:15 root] (abq_llm.py 208): INFO === Start quantize layer 20 ===
[2024-12-23 13:40:02 root] (abq_llm.py 321): INFO layer 20 iter 0 loss:0.592790424823761 norm:0.029940906912088394 max memory_allocated 29757.20361328125 
[2024-12-23 13:40:45 root] (abq_llm.py 321): INFO layer 20 iter 1 loss:0.5694344639778137 norm:0.013423364609479904 max memory_allocated 29757.20361328125 
[2024-12-23 13:41:29 root] (abq_llm.py 321): INFO layer 20 iter 2 loss:0.5531531572341919 norm:0.008475842885673046 max memory_allocated 29757.20361328125 
[2024-12-23 13:42:12 root] (abq_llm.py 321): INFO layer 20 iter 3 loss:0.5422972440719604 norm:0.005822508130222559 max memory_allocated 29757.20361328125 
[2024-12-23 13:42:56 root] (abq_llm.py 321): INFO layer 20 iter 4 loss:0.5377325415611267 norm:0.004741710610687733 max memory_allocated 29757.20361328125 
[2024-12-23 13:43:39 root] (abq_llm.py 321): INFO layer 20 iter 5 loss:0.5348072648048401 norm:0.003994357772171497 max memory_allocated 29757.20361328125 
[2024-12-23 13:44:22 root] (abq_llm.py 321): INFO layer 20 iter 6 loss:0.5325279235839844 norm:0.0036560564767569304 max memory_allocated 29757.20361328125 
[2024-12-23 13:45:06 root] (abq_llm.py 321): INFO layer 20 iter 7 loss:0.530674159526825 norm:0.003327898448333144 max memory_allocated 29757.20361328125 
[2024-12-23 13:45:49 root] (abq_llm.py 321): INFO layer 20 iter 8 loss:0.5292017459869385 norm:0.003011730033904314 max memory_allocated 29757.20361328125 
[2024-12-23 13:46:33 root] (abq_llm.py 321): INFO layer 20 iter 9 loss:0.5279721617698669 norm:0.00275957933627069 max memory_allocated 29757.20361328125 
[2024-12-23 13:47:16 root] (abq_llm.py 321): INFO layer 20 iter 10 loss:0.526872992515564 norm:0.0025329734198749065 max memory_allocated 29757.20361328125 
[2024-12-23 13:48:00 root] (abq_llm.py 321): INFO layer 20 iter 11 loss:0.5261631608009338 norm:0.0023962981067597866 max memory_allocated 29757.20361328125 
[2024-12-23 13:48:43 root] (abq_llm.py 321): INFO layer 20 iter 12 loss:0.5254642963409424 norm:0.002263403497636318 max memory_allocated 29757.20361328125 
[2024-12-23 13:49:26 root] (abq_llm.py 321): INFO layer 20 iter 13 loss:0.5249043107032776 norm:0.0021679787896573544 max memory_allocated 29757.20361328125 
[2024-12-23 13:50:10 root] (abq_llm.py 321): INFO layer 20 iter 14 loss:0.5244476199150085 norm:0.002119935117661953 max memory_allocated 29757.20361328125 
[2024-12-23 13:50:53 root] (abq_llm.py 321): INFO layer 20 iter 15 loss:0.5239881873130798 norm:0.002074463991448283 max memory_allocated 29757.20361328125 
[2024-12-23 13:51:37 root] (abq_llm.py 321): INFO layer 20 iter 16 loss:0.5236502289772034 norm:0.0020577507093548775 max memory_allocated 29757.20361328125 
[2024-12-23 13:52:20 root] (abq_llm.py 321): INFO layer 20 iter 17 loss:0.5233491659164429 norm:0.0020633148960769176 max memory_allocated 29757.20361328125 
[2024-12-23 13:53:03 root] (abq_llm.py 321): INFO layer 20 iter 18 loss:0.5231505632400513 norm:0.0020260000601410866 max memory_allocated 29757.20361328125 
[2024-12-23 13:53:47 root] (abq_llm.py 321): INFO layer 20 iter 19 loss:0.5228824615478516 norm:0.0019549827557057142 max memory_allocated 29757.20361328125 
[2024-12-23 13:54:00 root] (abq_llm.py 208): INFO === Start quantize layer 21 ===
[2024-12-23 13:54:51 root] (abq_llm.py 321): INFO layer 21 iter 0 loss:0.6124707460403442 norm:0.027748918160796165 max memory_allocated 29759.26611328125 
[2024-12-23 13:55:34 root] (abq_llm.py 321): INFO layer 21 iter 1 loss:0.5921035408973694 norm:0.01303031574934721 max memory_allocated 29759.26611328125 
[2024-12-23 13:56:18 root] (abq_llm.py 321): INFO layer 21 iter 2 loss:0.5749537348747253 norm:0.007460897322744131 max memory_allocated 29759.26611328125 
[2024-12-23 13:57:01 root] (abq_llm.py 321): INFO layer 21 iter 3 loss:0.563797116279602 norm:0.004503648728132248 max memory_allocated 29759.26611328125 
[2024-12-23 13:57:45 root] (abq_llm.py 321): INFO layer 21 iter 4 loss:0.5592003464698792 norm:0.003785054199397564 max memory_allocated 29759.26611328125 
[2024-12-23 13:58:28 root] (abq_llm.py 321): INFO layer 21 iter 5 loss:0.5561987161636353 norm:0.0033179190941154957 max memory_allocated 29759.26611328125 
[2024-12-23 13:59:11 root] (abq_llm.py 321): INFO layer 21 iter 6 loss:0.554203987121582 norm:0.0031081661581993103 max memory_allocated 29759.26611328125 
[2024-12-23 13:59:55 root] (abq_llm.py 321): INFO layer 21 iter 7 loss:0.5524175763130188 norm:0.0029156592208892107 max memory_allocated 29759.26611328125 
[2024-12-23 14:00:38 root] (abq_llm.py 321): INFO layer 21 iter 8 loss:0.550926923751831 norm:0.0027830777689814568 max memory_allocated 29759.26611328125 
[2024-12-23 14:01:22 root] (abq_llm.py 321): INFO layer 21 iter 9 loss:0.5496124029159546 norm:0.002551110927015543 max memory_allocated 29759.26611328125 
[2024-12-23 14:02:05 root] (abq_llm.py 321): INFO layer 21 iter 10 loss:0.548603355884552 norm:0.0024069698993116617 max memory_allocated 29759.26611328125 
[2024-12-23 14:02:48 root] (abq_llm.py 321): INFO layer 21 iter 11 loss:0.5477690696716309 norm:0.0022904151119291782 max memory_allocated 29759.26611328125 
[2024-12-23 14:03:32 root] (abq_llm.py 321): INFO layer 21 iter 12 loss:0.5470540523529053 norm:0.002259007655084133 max memory_allocated 29759.26611328125 
[2024-12-23 14:04:15 root] (abq_llm.py 321): INFO layer 21 iter 13 loss:0.5463684797286987 norm:0.0021827491000294685 max memory_allocated 29759.26611328125 
[2024-12-23 14:04:59 root] (abq_llm.py 321): INFO layer 21 iter 14 loss:0.5458741784095764 norm:0.0020893157925456762 max memory_allocated 29759.26611328125 
[2024-12-23 14:05:42 root] (abq_llm.py 321): INFO layer 21 iter 15 loss:0.5454167127609253 norm:0.0020337982568889856 max memory_allocated 29759.26611328125 
[2024-12-23 14:06:26 root] (abq_llm.py 321): INFO layer 21 iter 16 loss:0.5451058745384216 norm:0.0019919537007808685 max memory_allocated 29759.26611328125 
[2024-12-23 14:07:09 root] (abq_llm.py 321): INFO layer 21 iter 17 loss:0.5448163747787476 norm:0.0019640796817839146 max memory_allocated 29759.26611328125 
[2024-12-23 14:07:52 root] (abq_llm.py 321): INFO layer 21 iter 18 loss:0.5445017218589783 norm:0.0019264265429228544 max memory_allocated 29759.26611328125 
[2024-12-23 14:08:36 root] (abq_llm.py 321): INFO layer 21 iter 19 loss:0.5442736744880676 norm:0.0019265161827206612 max memory_allocated 29759.26611328125 
[2024-12-23 14:08:50 root] (abq_llm.py 208): INFO === Start quantize layer 22 ===
[2024-12-23 14:09:43 root] (abq_llm.py 321): INFO layer 22 iter 0 loss:0.6222693920135498 norm:0.011537195183336735 max memory_allocated 29761.32861328125 
[2024-12-23 14:10:27 root] (abq_llm.py 321): INFO layer 22 iter 1 loss:0.608244001865387 norm:0.006538698449730873 max memory_allocated 29761.32861328125 
[2024-12-23 14:11:10 root] (abq_llm.py 321): INFO layer 22 iter 2 loss:0.5945897102355957 norm:0.004031072836369276 max memory_allocated 29761.32861328125 
[2024-12-23 14:11:53 root] (abq_llm.py 321): INFO layer 22 iter 3 loss:0.5870411396026611 norm:0.002819701563566923 max memory_allocated 29761.32861328125 
[2024-12-23 14:12:37 root] (abq_llm.py 321): INFO layer 22 iter 4 loss:0.5831865668296814 norm:0.002228621393442154 max memory_allocated 29761.32861328125 
[2024-12-23 14:13:20 root] (abq_llm.py 321): INFO layer 22 iter 5 loss:0.5809382200241089 norm:0.0019840856548398733 max memory_allocated 29761.32861328125 
[2024-12-23 14:14:03 root] (abq_llm.py 321): INFO layer 22 iter 6 loss:0.5790332555770874 norm:0.0018687164410948753 max memory_allocated 29761.32861328125 
[2024-12-23 14:14:47 root] (abq_llm.py 321): INFO layer 22 iter 7 loss:0.5774580836296082 norm:0.0017598390113562346 max memory_allocated 29761.32861328125 
[2024-12-23 14:15:30 root] (abq_llm.py 321): INFO layer 22 iter 8 loss:0.5760809779167175 norm:0.0016290690982714295 max memory_allocated 29761.32861328125 
[2024-12-23 14:16:14 root] (abq_llm.py 321): INFO layer 22 iter 9 loss:0.5750101208686829 norm:0.0015667378902435303 max memory_allocated 29761.32861328125 
[2024-12-23 14:16:57 root] (abq_llm.py 321): INFO layer 22 iter 10 loss:0.5740754008293152 norm:0.001536297844722867 max memory_allocated 29761.32861328125 
[2024-12-23 14:17:41 root] (abq_llm.py 321): INFO layer 22 iter 11 loss:0.5733984112739563 norm:0.014413499273359776 max memory_allocated 29761.32861328125 
[2024-12-23 14:18:24 root] (abq_llm.py 321): INFO layer 22 iter 12 loss:0.5727673768997192 norm:0.0014564904849976301 max memory_allocated 29761.32861328125 
[2024-12-23 14:19:07 root] (abq_llm.py 321): INFO layer 22 iter 13 loss:0.5722602009773254 norm:0.001424924237653613 max memory_allocated 29761.32861328125 
[2024-12-23 14:19:51 root] (abq_llm.py 321): INFO layer 22 iter 14 loss:0.5718634724617004 norm:0.001369920326396823 max memory_allocated 29761.32861328125 
[2024-12-23 14:20:34 root] (abq_llm.py 321): INFO layer 22 iter 15 loss:0.5714988708496094 norm:0.0013707139296457171 max memory_allocated 29761.32861328125 
[2024-12-23 14:21:18 root] (abq_llm.py 321): INFO layer 22 iter 16 loss:0.5711833238601685 norm:0.001344843883998692 max memory_allocated 29761.32861328125 
[2024-12-23 14:22:01 root] (abq_llm.py 321): INFO layer 22 iter 17 loss:0.5709176659584045 norm:0.001308185514062643 max memory_allocated 29761.32861328125 
[2024-12-23 14:22:44 root] (abq_llm.py 321): INFO layer 22 iter 18 loss:0.5706713199615479 norm:0.0012818509712815285 max memory_allocated 29761.32861328125 
[2024-12-23 14:23:28 root] (abq_llm.py 321): INFO layer 22 iter 19 loss:0.5704318284988403 norm:0.0012649556156247854 max memory_allocated 29761.32861328125 
[2024-12-23 14:23:40 root] (abq_llm.py 208): INFO === Start quantize layer 23 ===
[2024-12-23 14:24:27 root] (abq_llm.py 321): INFO layer 23 iter 0 loss:0.6596143245697021 norm:0.023537758737802505 max memory_allocated 29763.39111328125 
[2024-12-23 14:25:10 root] (abq_llm.py 321): INFO layer 23 iter 1 loss:0.6464364528656006 norm:0.012562463991343975 max memory_allocated 29763.39111328125 
[2024-12-23 14:25:54 root] (abq_llm.py 321): INFO layer 23 iter 2 loss:0.6330769062042236 norm:0.007136945612728596 max memory_allocated 29763.39111328125 
[2024-12-23 14:26:37 root] (abq_llm.py 321): INFO layer 23 iter 3 loss:0.6261823177337646 norm:0.005008268635720015 max memory_allocated 29763.39111328125 
[2024-12-23 14:27:20 root] (abq_llm.py 321): INFO layer 23 iter 4 loss:0.6223267316818237 norm:0.0035111650358885527 max memory_allocated 29763.39111328125 
[2024-12-23 14:28:04 root] (abq_llm.py 321): INFO layer 23 iter 5 loss:0.6198766231536865 norm:0.0030007141176611185 max memory_allocated 29763.39111328125 
[2024-12-23 14:28:47 root] (abq_llm.py 321): INFO layer 23 iter 6 loss:0.617854654788971 norm:0.0026200164575129747 max memory_allocated 29763.39111328125 
[2024-12-23 14:29:31 root] (abq_llm.py 321): INFO layer 23 iter 7 loss:0.6160637140274048 norm:0.002364016603678465 max memory_allocated 29763.39111328125 
[2024-12-23 14:30:14 root] (abq_llm.py 321): INFO layer 23 iter 8 loss:0.6146001219749451 norm:0.0021641983184963465 max memory_allocated 29763.39111328125 
[2024-12-23 14:30:57 root] (abq_llm.py 321): INFO layer 23 iter 9 loss:0.6134504079818726 norm:0.002020792802795768 max memory_allocated 29763.39111328125 
[2024-12-23 14:31:41 root] (abq_llm.py 321): INFO layer 23 iter 10 loss:0.6125060319900513 norm:0.0018694624304771423 max memory_allocated 29763.39111328125 
[2024-12-23 14:32:24 root] (abq_llm.py 321): INFO layer 23 iter 11 loss:0.6117222309112549 norm:0.0017557810060679913 max memory_allocated 29763.39111328125 
[2024-12-23 14:33:08 root] (abq_llm.py 321): INFO layer 23 iter 12 loss:0.6111271977424622 norm:0.0016557322815060616 max memory_allocated 29763.39111328125 
[2024-12-23 14:33:51 root] (abq_llm.py 321): INFO layer 23 iter 13 loss:0.6105289459228516 norm:0.0015604651998728514 max memory_allocated 29763.39111328125 
[2024-12-23 14:34:34 root] (abq_llm.py 321): INFO layer 23 iter 14 loss:0.6100874543190002 norm:0.0015075564151629806 max memory_allocated 29763.39111328125 
[2024-12-23 14:35:18 root] (abq_llm.py 321): INFO layer 23 iter 15 loss:0.6096712350845337 norm:0.0014592153020203114 max memory_allocated 29763.39111328125 
[2024-12-23 14:36:01 root] (abq_llm.py 321): INFO layer 23 iter 16 loss:0.60926353931427 norm:0.0014018245274201035 max memory_allocated 29763.39111328125 
[2024-12-23 14:36:45 root] (abq_llm.py 321): INFO layer 23 iter 17 loss:0.6089807152748108 norm:0.0013591381721198559 max memory_allocated 29763.39111328125 
[2024-12-23 14:37:28 root] (abq_llm.py 321): INFO layer 23 iter 18 loss:0.6086921691894531 norm:0.0013489311095327139 max memory_allocated 29763.39111328125 
[2024-12-23 14:38:12 root] (abq_llm.py 321): INFO layer 23 iter 19 loss:0.6082165837287903 norm:0.0012335729552432895 max memory_allocated 29763.39111328125 
[2024-12-23 14:38:24 root] (abq_llm.py 208): INFO === Start quantize layer 24 ===
[2024-12-23 14:39:10 root] (abq_llm.py 321): INFO layer 24 iter 0 loss:0.6979906558990479 norm:0.011292671784758568 max memory_allocated 29765.45361328125 
[2024-12-23 14:39:54 root] (abq_llm.py 321): INFO layer 24 iter 1 loss:0.6875336766242981 norm:0.0072257607243955135 max memory_allocated 29765.45361328125 
[2024-12-23 14:40:37 root] (abq_llm.py 321): INFO layer 24 iter 2 loss:0.6759583353996277 norm:0.005465949885547161 max memory_allocated 29765.45361328125 
[2024-12-23 14:41:21 root] (abq_llm.py 321): INFO layer 24 iter 3 loss:0.6688828468322754 norm:0.004119585268199444 max memory_allocated 29765.45361328125 
[2024-12-23 14:42:04 root] (abq_llm.py 321): INFO layer 24 iter 4 loss:0.6652411222457886 norm:0.0031344026792794466 max memory_allocated 29765.45361328125 
[2024-12-23 14:42:47 root] (abq_llm.py 321): INFO layer 24 iter 5 loss:0.662696361541748 norm:0.0027520873118191957 max memory_allocated 29765.45361328125 
[2024-12-23 14:43:31 root] (abq_llm.py 321): INFO layer 24 iter 6 loss:0.6609299182891846 norm:0.0025405155029147863 max memory_allocated 29765.45361328125 
[2024-12-23 14:44:14 root] (abq_llm.py 321): INFO layer 24 iter 7 loss:0.6592043042182922 norm:0.0023953113704919815 max memory_allocated 29765.45361328125 
[2024-12-23 14:44:58 root] (abq_llm.py 321): INFO layer 24 iter 8 loss:0.657720685005188 norm:0.002268906682729721 max memory_allocated 29765.45361328125 
[2024-12-23 14:45:41 root] (abq_llm.py 321): INFO layer 24 iter 9 loss:0.6566399335861206 norm:0.002155919559299946 max memory_allocated 29765.45361328125 
[2024-12-23 14:46:24 root] (abq_llm.py 321): INFO layer 24 iter 10 loss:0.6557716131210327 norm:0.002082537394016981 max memory_allocated 29765.45361328125 
[2024-12-23 14:47:08 root] (abq_llm.py 321): INFO layer 24 iter 11 loss:0.654966413974762 norm:0.001980115193873644 max memory_allocated 29765.45361328125 
[2024-12-23 14:47:51 root] (abq_llm.py 321): INFO layer 24 iter 12 loss:0.6543974876403809 norm:0.0019228864694014192 max memory_allocated 29765.45361328125 
[2024-12-23 14:48:35 root] (abq_llm.py 321): INFO layer 24 iter 13 loss:0.6539912819862366 norm:0.0019211574690416455 max memory_allocated 29765.45361328125 
[2024-12-23 14:49:18 root] (abq_llm.py 321): INFO layer 24 iter 14 loss:0.6535549163818359 norm:0.0018570186803117394 max memory_allocated 29765.45361328125 
[2024-12-23 14:50:02 root] (abq_llm.py 321): INFO layer 24 iter 15 loss:0.6532248854637146 norm:0.0018483013845980167 max memory_allocated 29765.45361328125 
[2024-12-23 14:50:45 root] (abq_llm.py 321): INFO layer 24 iter 16 loss:0.6529511213302612 norm:0.0018413679208606482 max memory_allocated 29765.45361328125 
[2024-12-23 14:51:28 root] (abq_llm.py 321): INFO layer 24 iter 17 loss:0.6527050733566284 norm:0.0017881259554997087 max memory_allocated 29765.45361328125 
[2024-12-23 14:52:12 root] (abq_llm.py 321): INFO layer 24 iter 18 loss:0.6524874567985535 norm:0.001787065528333187 max memory_allocated 29765.45361328125 
[2024-12-23 14:52:55 root] (abq_llm.py 321): INFO layer 24 iter 19 loss:0.6522261500358582 norm:0.0017615072429180145 max memory_allocated 29765.45361328125 
[2024-12-23 14:53:08 root] (abq_llm.py 208): INFO === Start quantize layer 25 ===
[2024-12-23 14:53:54 root] (abq_llm.py 321): INFO layer 25 iter 0 loss:0.7416554093360901 norm:0.007255637086927891 max memory_allocated 29767.51611328125 
[2024-12-23 14:54:38 root] (abq_llm.py 321): INFO layer 25 iter 1 loss:0.7316533923149109 norm:0.0045496150851249695 max memory_allocated 29767.51611328125 
[2024-12-23 14:55:21 root] (abq_llm.py 321): INFO layer 25 iter 2 loss:0.7209376692771912 norm:0.0033573934342712164 max memory_allocated 29767.51611328125 
[2024-12-23 14:56:04 root] (abq_llm.py 321): INFO layer 25 iter 3 loss:0.715419352054596 norm:0.0026143123395740986 max memory_allocated 29767.51611328125 
[2024-12-23 14:56:48 root] (abq_llm.py 321): INFO layer 25 iter 4 loss:0.712749719619751 norm:0.0022339210845530033 max memory_allocated 29767.51611328125 
[2024-12-23 14:57:31 root] (abq_llm.py 321): INFO layer 25 iter 5 loss:0.7108198404312134 norm:0.002012083074077964 max memory_allocated 29767.51611328125 
[2024-12-23 14:58:15 root] (abq_llm.py 321): INFO layer 25 iter 6 loss:0.708862841129303 norm:0.0018125423230230808 max memory_allocated 29767.51611328125 
[2024-12-23 14:58:58 root] (abq_llm.py 321): INFO layer 25 iter 7 loss:0.7072988152503967 norm:0.0016900321934372187 max memory_allocated 29767.51611328125 
[2024-12-23 14:59:42 root] (abq_llm.py 321): INFO layer 25 iter 8 loss:0.705946683883667 norm:0.0016106795519590378 max memory_allocated 29767.51611328125 
[2024-12-23 15:00:25 root] (abq_llm.py 321): INFO layer 25 iter 9 loss:0.7048924565315247 norm:0.0015749374870210886 max memory_allocated 29767.51611328125 
[2024-12-23 15:01:08 root] (abq_llm.py 321): INFO layer 25 iter 10 loss:0.7040656805038452 norm:0.0015099701704457402 max memory_allocated 29767.51611328125 
[2024-12-23 15:01:52 root] (abq_llm.py 321): INFO layer 25 iter 11 loss:0.7034307718276978 norm:0.001477776444517076 max memory_allocated 29767.51611328125 
[2024-12-23 15:02:35 root] (abq_llm.py 321): INFO layer 25 iter 12 loss:0.7028409242630005 norm:0.0014263713965192437 max memory_allocated 29767.51611328125 
[2024-12-23 15:03:19 root] (abq_llm.py 321): INFO layer 25 iter 13 loss:0.7024025321006775 norm:0.0014134555822238326 max memory_allocated 29767.51611328125 
[2024-12-23 15:04:02 root] (abq_llm.py 321): INFO layer 25 iter 14 loss:0.7019867300987244 norm:0.0013611279428005219 max memory_allocated 29767.51611328125 
[2024-12-23 15:04:46 root] (abq_llm.py 321): INFO layer 25 iter 15 loss:0.7016467452049255 norm:0.0013627667212858796 max memory_allocated 29767.51611328125 
[2024-12-23 15:05:29 root] (abq_llm.py 321): INFO layer 25 iter 16 loss:0.7013746500015259 norm:0.0013274351367726922 max memory_allocated 29767.51611328125 
[2024-12-23 15:06:12 root] (abq_llm.py 321): INFO layer 25 iter 17 loss:0.701157808303833 norm:0.001302419463172555 max memory_allocated 29767.51611328125 
[2024-12-23 15:06:56 root] (abq_llm.py 321): INFO layer 25 iter 18 loss:0.7009249329566956 norm:0.001294339308515191 max memory_allocated 29767.51611328125 
[2024-12-23 15:07:39 root] (abq_llm.py 321): INFO layer 25 iter 19 loss:0.7007225751876831 norm:0.0012904491741210222 max memory_allocated 29767.51611328125 
[2024-12-23 15:07:52 root] (abq_llm.py 208): INFO === Start quantize layer 26 ===
[2024-12-23 15:08:38 root] (abq_llm.py 321): INFO layer 26 iter 0 loss:0.8096437454223633 norm:0.01769828237593174 max memory_allocated 29769.57861328125 
[2024-12-23 15:09:22 root] (abq_llm.py 321): INFO layer 26 iter 1 loss:0.7980096340179443 norm:0.01053616777062416 max memory_allocated 29769.57861328125 
[2024-12-23 15:10:05 root] (abq_llm.py 321): INFO layer 26 iter 2 loss:0.7855856418609619 norm:0.006836698390543461 max memory_allocated 29769.57861328125 
[2024-12-23 15:10:48 root] (abq_llm.py 321): INFO layer 26 iter 3 loss:0.7783724665641785 norm:0.004339816048741341 max memory_allocated 29769.57861328125 
[2024-12-23 15:11:32 root] (abq_llm.py 321): INFO layer 26 iter 4 loss:0.7744195461273193 norm:0.003214579075574875 max memory_allocated 29769.57861328125 
[2024-12-23 15:12:15 root] (abq_llm.py 321): INFO layer 26 iter 5 loss:0.7716811299324036 norm:0.0025176196359097958 max memory_allocated 29769.57861328125 
[2024-12-23 15:12:59 root] (abq_llm.py 321): INFO layer 26 iter 6 loss:0.7695207595825195 norm:0.002388507593423128 max memory_allocated 29769.57861328125 
[2024-12-23 15:13:42 root] (abq_llm.py 321): INFO layer 26 iter 7 loss:0.7676333785057068 norm:0.0022383546456694603 max memory_allocated 29769.57861328125 
[2024-12-23 15:14:25 root] (abq_llm.py 321): INFO layer 26 iter 8 loss:0.7661213278770447 norm:0.002094652969390154 max memory_allocated 29769.57861328125 
[2024-12-23 15:15:09 root] (abq_llm.py 321): INFO layer 26 iter 9 loss:0.7649710178375244 norm:0.001971119549125433 max memory_allocated 29769.57861328125 
[2024-12-23 15:15:52 root] (abq_llm.py 321): INFO layer 26 iter 10 loss:0.764062225818634 norm:0.0018874749075621367 max memory_allocated 29769.57861328125 
[2024-12-23 15:16:36 root] (abq_llm.py 321): INFO layer 26 iter 11 loss:0.7632954120635986 norm:0.0017998424591496587 max memory_allocated 29769.57861328125 
[2024-12-23 15:17:19 root] (abq_llm.py 321): INFO layer 26 iter 12 loss:0.7627328634262085 norm:0.0017528720200061798 max memory_allocated 29769.57861328125 
[2024-12-23 15:18:03 root] (abq_llm.py 321): INFO layer 26 iter 13 loss:0.7622697353363037 norm:0.001741707674227655 max memory_allocated 29769.57861328125 
[2024-12-23 15:18:46 root] (abq_llm.py 321): INFO layer 26 iter 14 loss:0.7618527412414551 norm:0.001675605308264494 max memory_allocated 29769.57861328125 
[2024-12-23 15:19:30 root] (abq_llm.py 321): INFO layer 26 iter 15 loss:0.7615424394607544 norm:0.001648628618568182 max memory_allocated 29769.57861328125 
[2024-12-23 15:20:13 root] (abq_llm.py 321): INFO layer 26 iter 16 loss:0.761215329170227 norm:0.0016104974783957005 max memory_allocated 29769.57861328125 
[2024-12-23 15:20:56 root] (abq_llm.py 321): INFO layer 26 iter 17 loss:0.7608916759490967 norm:0.001590547151863575 max memory_allocated 29769.57861328125 
[2024-12-23 15:21:40 root] (abq_llm.py 321): INFO layer 26 iter 18 loss:0.7606562376022339 norm:0.0015683294041082263 max memory_allocated 29769.57861328125 
[2024-12-23 15:22:23 root] (abq_llm.py 321): INFO layer 26 iter 19 loss:0.7604522705078125 norm:0.0015637991018593311 max memory_allocated 29769.57861328125 
[2024-12-23 15:22:35 root] (abq_llm.py 208): INFO === Start quantize layer 27 ===
[2024-12-23 15:23:22 root] (abq_llm.py 321): INFO layer 27 iter 0 loss:0.8614093661308289 norm:0.006279800087213516 max memory_allocated 29771.64111328125 
[2024-12-23 15:24:06 root] (abq_llm.py 321): INFO layer 27 iter 1 loss:0.852901816368103 norm:0.0038128900341689587 max memory_allocated 29771.64111328125 
[2024-12-23 15:24:49 root] (abq_llm.py 321): INFO layer 27 iter 2 loss:0.8421546220779419 norm:0.0024425331503152847 max memory_allocated 29771.64111328125 
[2024-12-23 15:25:32 root] (abq_llm.py 321): INFO layer 27 iter 3 loss:0.8368076682090759 norm:0.0018456733087077737 max memory_allocated 29771.64111328125 
[2024-12-23 15:26:16 root] (abq_llm.py 321): INFO layer 27 iter 4 loss:0.8341420888900757 norm:0.0016534199239686131 max memory_allocated 29771.64111328125 
[2024-12-23 15:26:59 root] (abq_llm.py 321): INFO layer 27 iter 5 loss:0.8321319818496704 norm:0.0015564396744593978 max memory_allocated 29771.64111328125 
[2024-12-23 15:27:43 root] (abq_llm.py 321): INFO layer 27 iter 6 loss:0.830151379108429 norm:0.0014962444547563791 max memory_allocated 29771.64111328125 
[2024-12-23 15:28:26 root] (abq_llm.py 321): INFO layer 27 iter 7 loss:0.8283925652503967 norm:0.0014713048003613949 max memory_allocated 29771.64111328125 
[2024-12-23 15:29:09 root] (abq_llm.py 321): INFO layer 27 iter 8 loss:0.8269021511077881 norm:0.00139030406717211 max memory_allocated 29771.64111328125 
[2024-12-23 15:29:53 root] (abq_llm.py 321): INFO layer 27 iter 9 loss:0.8256137371063232 norm:0.0013338468270376325 max memory_allocated 29771.64111328125 
[2024-12-23 15:30:36 root] (abq_llm.py 321): INFO layer 27 iter 10 loss:0.8245548009872437 norm:0.0012658950872719288 max memory_allocated 29771.64111328125 
[2024-12-23 15:31:20 root] (abq_llm.py 321): INFO layer 27 iter 11 loss:0.8238428235054016 norm:0.0012384706642478704 max memory_allocated 29771.64111328125 
[2024-12-23 15:32:03 root] (abq_llm.py 321): INFO layer 27 iter 12 loss:0.8232457041740417 norm:0.001233002170920372 max memory_allocated 29771.64111328125 
[2024-12-23 15:32:47 root] (abq_llm.py 321): INFO layer 27 iter 13 loss:0.8228312730789185 norm:0.001213669078424573 max memory_allocated 29771.64111328125 
[2024-12-23 15:33:30 root] (abq_llm.py 321): INFO layer 27 iter 14 loss:0.8224362730979919 norm:0.001180207938887179 max memory_allocated 29771.64111328125 
[2024-12-23 15:34:14 root] (abq_llm.py 321): INFO layer 27 iter 15 loss:0.822029709815979 norm:0.0011598668061196804 max memory_allocated 29771.64111328125 
[2024-12-23 15:34:57 root] (abq_llm.py 321): INFO layer 27 iter 16 loss:0.8217270374298096 norm:0.00114670698530972 max memory_allocated 29771.64111328125 
[2024-12-23 15:35:40 root] (abq_llm.py 321): INFO layer 27 iter 17 loss:0.8214298486709595 norm:0.0011279508471488953 max memory_allocated 29771.64111328125 
[2024-12-23 15:36:24 root] (abq_llm.py 321): INFO layer 27 iter 18 loss:0.821214497089386 norm:0.001115551684051752 max memory_allocated 29771.64111328125 
[2024-12-23 15:37:07 root] (abq_llm.py 321): INFO layer 27 iter 19 loss:0.8210023641586304 norm:0.0011084545403718948 max memory_allocated 29771.64111328125 
[2024-12-23 15:37:20 root] (abq_llm.py 208): INFO === Start quantize layer 28 ===
[2024-12-23 15:38:06 root] (abq_llm.py 321): INFO layer 28 iter 0 loss:0.955722451210022 norm:0.01728113181889057 max memory_allocated 29773.70361328125 
[2024-12-23 15:38:50 root] (abq_llm.py 321): INFO layer 28 iter 1 loss:0.9440081119537354 norm:0.011043035425245762 max memory_allocated 29773.70361328125 
[2024-12-23 15:39:33 root] (abq_llm.py 321): INFO layer 28 iter 2 loss:0.9291574954986572 norm:0.0072308811359107494 max memory_allocated 29773.70361328125 
[2024-12-23 15:40:16 root] (abq_llm.py 321): INFO layer 28 iter 3 loss:0.9207109212875366 norm:0.0051495046354830265 max memory_allocated 29773.70361328125 
[2024-12-23 15:41:00 root] (abq_llm.py 321): INFO layer 28 iter 4 loss:0.9164308309555054 norm:0.004263734444975853 max memory_allocated 29773.70361328125 
[2024-12-23 15:41:43 root] (abq_llm.py 321): INFO layer 28 iter 5 loss:0.9135180115699768 norm:0.0040404605679214 max memory_allocated 29773.70361328125 
[2024-12-23 15:42:27 root] (abq_llm.py 321): INFO layer 28 iter 6 loss:0.911065936088562 norm:0.0039859795942902565 max memory_allocated 29773.70361328125 
[2024-12-23 15:43:10 root] (abq_llm.py 321): INFO layer 28 iter 7 loss:0.9089747071266174 norm:0.0038786635268479586 max memory_allocated 29773.70361328125 
[2024-12-23 15:43:54 root] (abq_llm.py 321): INFO layer 28 iter 8 loss:0.9074702858924866 norm:0.003760293126106262 max memory_allocated 29773.70361328125 
[2024-12-23 15:44:37 root] (abq_llm.py 321): INFO layer 28 iter 9 loss:0.9062100648880005 norm:0.0035583614371716976 max memory_allocated 29773.70361328125 
[2024-12-23 15:45:20 root] (abq_llm.py 321): INFO layer 28 iter 10 loss:0.9052566289901733 norm:0.0034164749085903168 max memory_allocated 29773.70361328125 
[2024-12-23 15:46:04 root] (abq_llm.py 321): INFO layer 28 iter 11 loss:0.9043581485748291 norm:0.0033407039009034634 max memory_allocated 29773.70361328125 
[2024-12-23 15:46:47 root] (abq_llm.py 321): INFO layer 28 iter 12 loss:0.9035463333129883 norm:0.003431193297728896 max memory_allocated 29773.70361328125 
[2024-12-23 15:47:31 root] (abq_llm.py 321): INFO layer 28 iter 13 loss:0.9030240774154663 norm:0.0032773353159427643 max memory_allocated 29773.70361328125 
[2024-12-23 15:48:14 root] (abq_llm.py 321): INFO layer 28 iter 14 loss:0.902503490447998 norm:0.002999763935804367 max memory_allocated 29773.70361328125 
[2024-12-23 15:48:58 root] (abq_llm.py 321): INFO layer 28 iter 15 loss:0.9018685817718506 norm:0.003139341250061989 max memory_allocated 29773.70361328125 
[2024-12-23 15:49:41 root] (abq_llm.py 321): INFO layer 28 iter 16 loss:0.9015867710113525 norm:0.0030528169590979815 max memory_allocated 29773.70361328125 
[2024-12-23 15:50:24 root] (abq_llm.py 321): INFO layer 28 iter 17 loss:0.9013665318489075 norm:0.0029137684032320976 max memory_allocated 29773.70361328125 
[2024-12-23 15:51:08 root] (abq_llm.py 321): INFO layer 28 iter 18 loss:0.9011962413787842 norm:0.0030579499434679747 max memory_allocated 29773.70361328125 
[2024-12-23 15:51:51 root] (abq_llm.py 321): INFO layer 28 iter 19 loss:0.9012327790260315 norm:0.0030253645963966846 max memory_allocated 29773.70361328125 
[2024-12-23 15:52:04 root] (abq_llm.py 208): INFO === Start quantize layer 29 ===
[2024-12-23 15:52:50 root] (abq_llm.py 321): INFO layer 29 iter 0 loss:1.0264512300491333 norm:0.007013482507318258 max memory_allocated 29775.76611328125 
[2024-12-23 15:53:34 root] (abq_llm.py 321): INFO layer 29 iter 1 loss:1.0160443782806396 norm:0.0035904175601899624 max memory_allocated 29775.76611328125 
[2024-12-23 15:54:17 root] (abq_llm.py 321): INFO layer 29 iter 2 loss:1.0037909746170044 norm:0.002420727862045169 max memory_allocated 29775.76611328125 
[2024-12-23 15:55:00 root] (abq_llm.py 321): INFO layer 29 iter 3 loss:0.9972623586654663 norm:0.001922694151289761 max memory_allocated 29775.76611328125 
[2024-12-23 15:55:44 root] (abq_llm.py 321): INFO layer 29 iter 4 loss:0.9940075278282166 norm:0.0016549101565033197 max memory_allocated 29775.76611328125 
[2024-12-23 15:56:27 root] (abq_llm.py 321): INFO layer 29 iter 5 loss:0.9915525317192078 norm:0.001525326631963253 max memory_allocated 29775.76611328125 
[2024-12-23 15:57:11 root] (abq_llm.py 321): INFO layer 29 iter 6 loss:0.9891006350517273 norm:0.0013916577445343137 max memory_allocated 29775.76611328125 
[2024-12-23 15:57:54 root] (abq_llm.py 321): INFO layer 29 iter 7 loss:0.9870079755783081 norm:0.0013178191147744656 max memory_allocated 29775.76611328125 
[2024-12-23 15:58:38 root] (abq_llm.py 321): INFO layer 29 iter 8 loss:0.9854249954223633 norm:0.0012357021914795041 max memory_allocated 29775.76611328125 
[2024-12-23 15:59:21 root] (abq_llm.py 321): INFO layer 29 iter 9 loss:0.9842219948768616 norm:0.001215162337757647 max memory_allocated 29775.76611328125 
[2024-12-23 16:00:04 root] (abq_llm.py 321): INFO layer 29 iter 10 loss:0.9833579659461975 norm:0.0011729007819667459 max memory_allocated 29775.76611328125 
[2024-12-23 16:00:48 root] (abq_llm.py 321): INFO layer 29 iter 11 loss:0.982671320438385 norm:0.0011285791406407952 max memory_allocated 29775.76611328125 
[2024-12-23 16:01:31 root] (abq_llm.py 321): INFO layer 29 iter 12 loss:0.9821043014526367 norm:0.0011029678862541914 max memory_allocated 29775.76611328125 
[2024-12-23 16:02:15 root] (abq_llm.py 321): INFO layer 29 iter 13 loss:0.9816379547119141 norm:0.0010975166223943233 max memory_allocated 29775.76611328125 
[2024-12-23 16:02:58 root] (abq_llm.py 321): INFO layer 29 iter 14 loss:0.981202244758606 norm:0.0010908684926107526 max memory_allocated 29775.76611328125 
[2024-12-23 16:03:42 root] (abq_llm.py 321): INFO layer 29 iter 15 loss:0.9808682203292847 norm:0.0010889777913689613 max memory_allocated 29775.76611328125 
[2024-12-23 16:04:25 root] (abq_llm.py 321): INFO layer 29 iter 16 loss:0.9805537462234497 norm:0.0010722822044044733 max memory_allocated 29775.76611328125 
[2024-12-23 16:05:08 root] (abq_llm.py 321): INFO layer 29 iter 17 loss:0.9803284406661987 norm:0.001081037800759077 max memory_allocated 29775.76611328125 
[2024-12-23 16:05:52 root] (abq_llm.py 321): INFO layer 29 iter 18 loss:0.9800379276275635 norm:0.001057776273228228 max memory_allocated 29775.76611328125 
[2024-12-23 16:06:35 root] (abq_llm.py 321): INFO layer 29 iter 19 loss:0.9798443913459778 norm:0.001035944209434092 max memory_allocated 29775.76611328125 
[2024-12-23 16:06:48 root] (abq_llm.py 208): INFO === Start quantize layer 30 ===
[2024-12-23 16:07:35 root] (abq_llm.py 321): INFO layer 30 iter 0 loss:1.127824306488037 norm:0.008637262508273125 max memory_allocated 29777.82861328125 
[2024-12-23 16:08:18 root] (abq_llm.py 321): INFO layer 30 iter 1 loss:1.1148208379745483 norm:0.005140478722751141 max memory_allocated 29777.82861328125 
[2024-12-23 16:09:01 root] (abq_llm.py 321): INFO layer 30 iter 2 loss:1.098989486694336 norm:0.0032511905301362276 max memory_allocated 29777.82861328125 
[2024-12-23 16:09:45 root] (abq_llm.py 321): INFO layer 30 iter 3 loss:1.0895450115203857 norm:0.002214238280430436 max memory_allocated 29777.82861328125 
[2024-12-23 16:10:28 root] (abq_llm.py 321): INFO layer 30 iter 4 loss:1.0850039720535278 norm:0.0019118757918477058 max memory_allocated 29777.82861328125 
[2024-12-23 16:11:12 root] (abq_llm.py 321): INFO layer 30 iter 5 loss:1.0819637775421143 norm:0.0017382990336045623 max memory_allocated 29777.82861328125 
[2024-12-23 16:11:55 root] (abq_llm.py 321): INFO layer 30 iter 6 loss:1.0794447660446167 norm:0.0016235667280852795 max memory_allocated 29777.82861328125 
[2024-12-23 16:12:39 root] (abq_llm.py 321): INFO layer 30 iter 7 loss:1.0773377418518066 norm:0.0015547984512522817 max memory_allocated 29777.82861328125 
[2024-12-23 16:13:22 root] (abq_llm.py 321): INFO layer 30 iter 8 loss:1.075671911239624 norm:0.0014750947011634707 max memory_allocated 29777.82861328125 
[2024-12-23 16:14:05 root] (abq_llm.py 321): INFO layer 30 iter 9 loss:1.074507713317871 norm:0.0014129517367109656 max memory_allocated 29777.82861328125 
[2024-12-23 16:14:49 root] (abq_llm.py 321): INFO layer 30 iter 10 loss:1.0735608339309692 norm:0.0013613725313916802 max memory_allocated 29777.82861328125 
[2024-12-23 16:15:32 root] (abq_llm.py 321): INFO layer 30 iter 11 loss:1.0728881359100342 norm:0.0013275675009936094 max memory_allocated 29777.82861328125 
[2024-12-23 16:16:16 root] (abq_llm.py 321): INFO layer 30 iter 12 loss:1.0722994804382324 norm:0.0012759470846503973 max memory_allocated 29777.82861328125 
[2024-12-23 16:16:59 root] (abq_llm.py 321): INFO layer 30 iter 13 loss:1.0717837810516357 norm:0.0012504304759204388 max memory_allocated 29777.82861328125 
[2024-12-23 16:17:43 root] (abq_llm.py 321): INFO layer 30 iter 14 loss:1.0712969303131104 norm:0.0012086705537512898 max memory_allocated 29777.82861328125 
[2024-12-23 16:18:26 root] (abq_llm.py 321): INFO layer 30 iter 15 loss:1.0709059238433838 norm:0.0011966869933530688 max memory_allocated 29777.82861328125 
[2024-12-23 16:19:10 root] (abq_llm.py 321): INFO layer 30 iter 16 loss:1.0706210136413574 norm:0.0011794816236943007 max memory_allocated 29777.82861328125 
[2024-12-23 16:19:53 root] (abq_llm.py 321): INFO layer 30 iter 17 loss:1.0703051090240479 norm:0.001159563777036965 max memory_allocated 29777.82861328125 
[2024-12-23 16:20:36 root] (abq_llm.py 321): INFO layer 30 iter 18 loss:1.070002794265747 norm:0.001134590245783329 max memory_allocated 29777.82861328125 
[2024-12-23 16:21:20 root] (abq_llm.py 321): INFO layer 30 iter 19 loss:1.069784164428711 norm:0.001121903071179986 max memory_allocated 29777.82861328125 
[2024-12-23 16:21:32 root] (abq_llm.py 208): INFO === Start quantize layer 31 ===
[2024-12-23 16:22:19 root] (abq_llm.py 321): INFO layer 31 iter 0 loss:1.2238497734069824 norm:0.0071982210502028465 max memory_allocated 29779.89111328125 
[2024-12-23 16:23:02 root] (abq_llm.py 321): INFO layer 31 iter 1 loss:1.212851881980896 norm:0.004999514669179916 max memory_allocated 29779.89111328125 
[2024-12-23 16:23:46 root] (abq_llm.py 321): INFO layer 31 iter 2 loss:1.198393702507019 norm:0.0034782961010932922 max memory_allocated 29779.89111328125 
[2024-12-23 16:24:29 root] (abq_llm.py 321): INFO layer 31 iter 3 loss:1.188058614730835 norm:0.002261724090203643 max memory_allocated 29779.89111328125 
[2024-12-23 16:25:12 root] (abq_llm.py 321): INFO layer 31 iter 4 loss:1.182558536529541 norm:0.001876218942925334 max memory_allocated 29779.89111328125 
[2024-12-23 16:25:56 root] (abq_llm.py 321): INFO layer 31 iter 5 loss:1.1788899898529053 norm:0.0016580920200794935 max memory_allocated 29779.89111328125 
[2024-12-23 16:26:39 root] (abq_llm.py 321): INFO layer 31 iter 6 loss:1.175990343093872 norm:0.0015346378786489367 max memory_allocated 29779.89111328125 
[2024-12-23 16:27:23 root] (abq_llm.py 321): INFO layer 31 iter 7 loss:1.1735906600952148 norm:0.0014591688523069024 max memory_allocated 29779.89111328125 
[2024-12-23 16:28:06 root] (abq_llm.py 321): INFO layer 31 iter 8 loss:1.1719086170196533 norm:0.001430773758329451 max memory_allocated 29779.89111328125 
[2024-12-23 16:28:50 root] (abq_llm.py 321): INFO layer 31 iter 9 loss:1.1706013679504395 norm:0.001371837337501347 max memory_allocated 29779.89111328125 
[2024-12-23 16:29:33 root] (abq_llm.py 321): INFO layer 31 iter 10 loss:1.1697146892547607 norm:0.0013374746777117252 max memory_allocated 29779.89111328125 
[2024-12-23 16:30:16 root] (abq_llm.py 321): INFO layer 31 iter 11 loss:1.1688947677612305 norm:0.0013168449513614178 max memory_allocated 29779.89111328125 
[2024-12-23 16:31:00 root] (abq_llm.py 321): INFO layer 31 iter 12 loss:1.1682835817337036 norm:0.0012716728961095214 max memory_allocated 29779.89111328125 
[2024-12-23 16:31:43 root] (abq_llm.py 321): INFO layer 31 iter 13 loss:1.167823314666748 norm:0.0012500992743298411 max memory_allocated 29779.89111328125 
[2024-12-23 16:32:27 root] (abq_llm.py 321): INFO layer 31 iter 14 loss:1.1674015522003174 norm:0.0012250271392986178 max memory_allocated 29779.89111328125 
[2024-12-23 16:33:10 root] (abq_llm.py 321): INFO layer 31 iter 15 loss:1.1669707298278809 norm:0.0011937093222513795 max memory_allocated 29779.89111328125 
[2024-12-23 16:33:54 root] (abq_llm.py 321): INFO layer 31 iter 16 loss:1.16665518283844 norm:0.0011880777310580015 max memory_allocated 29779.89111328125 
[2024-12-23 16:34:37 root] (abq_llm.py 321): INFO layer 31 iter 17 loss:1.1663471460342407 norm:0.001157583435997367 max memory_allocated 29779.89111328125 
[2024-12-23 16:35:20 root] (abq_llm.py 321): INFO layer 31 iter 18 loss:1.1660740375518799 norm:0.001143704168498516 max memory_allocated 29779.89111328125 
[2024-12-23 16:36:04 root] (abq_llm.py 321): INFO layer 31 iter 19 loss:1.1658117771148682 norm:0.0011344992090016603 max memory_allocated 29779.89111328125 
[2024-12-23 16:36:16 root] (abq_llm.py 208): INFO === Start quantize layer 32 ===
[2024-12-23 16:37:03 root] (abq_llm.py 321): INFO layer 32 iter 0 loss:1.3418917655944824 norm:0.012280385009944439 max memory_allocated 29781.95361328125 
[2024-12-23 16:37:46 root] (abq_llm.py 321): INFO layer 32 iter 1 loss:1.3263708353042603 norm:0.0067916130647063255 max memory_allocated 29781.95361328125 
[2024-12-23 16:38:30 root] (abq_llm.py 321): INFO layer 32 iter 2 loss:1.3082455396652222 norm:0.004295458551496267 max memory_allocated 29781.95361328125 
[2024-12-23 16:39:13 root] (abq_llm.py 321): INFO layer 32 iter 3 loss:1.296369194984436 norm:0.00308237224817276 max memory_allocated 29781.95361328125 
[2024-12-23 16:39:56 root] (abq_llm.py 321): INFO layer 32 iter 4 loss:1.289935827255249 norm:0.0025365501642227173 max memory_allocated 29781.95361328125 
[2024-12-23 16:40:40 root] (abq_llm.py 321): INFO layer 32 iter 5 loss:1.2859959602355957 norm:0.002142510609701276 max memory_allocated 29781.95361328125 
[2024-12-23 16:41:23 root] (abq_llm.py 321): INFO layer 32 iter 6 loss:1.2827229499816895 norm:0.0020312496926635504 max memory_allocated 29781.95361328125 
[2024-12-23 16:42:07 root] (abq_llm.py 321): INFO layer 32 iter 7 loss:1.2799901962280273 norm:0.002035049721598625 max memory_allocated 29781.95361328125 
[2024-12-23 16:42:50 root] (abq_llm.py 321): INFO layer 32 iter 8 loss:1.278175950050354 norm:0.001995573751628399 max memory_allocated 29781.95361328125 
[2024-12-23 16:43:33 root] (abq_llm.py 321): INFO layer 32 iter 9 loss:1.276928186416626 norm:0.001955004408955574 max memory_allocated 29781.95361328125 
[2024-12-23 16:44:17 root] (abq_llm.py 321): INFO layer 32 iter 10 loss:1.2759220600128174 norm:0.0019007353112101555 max memory_allocated 29781.95361328125 
[2024-12-23 16:45:00 root] (abq_llm.py 321): INFO layer 32 iter 11 loss:1.2751898765563965 norm:0.0017918165540322661 max memory_allocated 29781.95361328125 
[2024-12-23 16:45:44 root] (abq_llm.py 321): INFO layer 32 iter 12 loss:1.2746692895889282 norm:0.0017796065658330917 max memory_allocated 29781.95361328125 
[2024-12-23 16:46:27 root] (abq_llm.py 321): INFO layer 32 iter 13 loss:1.274111270904541 norm:0.001746218535117805 max memory_allocated 29781.95361328125 
[2024-12-23 16:47:11 root] (abq_llm.py 321): INFO layer 32 iter 14 loss:1.2736672163009644 norm:0.001722589135169983 max memory_allocated 29781.95361328125 
[2024-12-23 16:47:54 root] (abq_llm.py 321): INFO layer 32 iter 15 loss:1.2732319831848145 norm:0.0017005768604576588 max memory_allocated 29781.95361328125 
[2024-12-23 16:48:38 root] (abq_llm.py 321): INFO layer 32 iter 16 loss:1.2728832960128784 norm:0.0016472709830850363 max memory_allocated 29781.95361328125 
[2024-12-23 16:49:21 root] (abq_llm.py 321): INFO layer 32 iter 17 loss:1.2726213932037354 norm:0.0016349074430763721 max memory_allocated 29781.95361328125 
[2024-12-23 16:50:04 root] (abq_llm.py 321): INFO layer 32 iter 18 loss:1.2723454236984253 norm:0.0015726559795439243 max memory_allocated 29781.95361328125 
[2024-12-23 16:50:48 root] (abq_llm.py 321): INFO layer 32 iter 19 loss:1.2720544338226318 norm:0.0015457496047019958 max memory_allocated 29781.95361328125 
[2024-12-23 16:51:00 root] (abq_llm.py 208): INFO === Start quantize layer 33 ===
[2024-12-23 16:51:47 root] (abq_llm.py 321): INFO layer 33 iter 0 loss:1.464322566986084 norm:0.020716214552521706 max memory_allocated 29784.01611328125 
[2024-12-23 16:52:30 root] (abq_llm.py 321): INFO layer 33 iter 1 loss:1.4484392404556274 norm:0.013646341860294342 max memory_allocated 29784.01611328125 
[2024-12-23 16:53:14 root] (abq_llm.py 321): INFO layer 33 iter 2 loss:1.4289124011993408 norm:0.009307635016739368 max memory_allocated 29784.01611328125 
[2024-12-23 16:53:57 root] (abq_llm.py 321): INFO layer 33 iter 3 loss:1.4166439771652222 norm:0.006564824841916561 max memory_allocated 29784.01611328125 
[2024-12-23 16:54:40 root] (abq_llm.py 321): INFO layer 33 iter 4 loss:1.4089231491088867 norm:0.0049695041961967945 max memory_allocated 29784.01611328125 
[2024-12-23 16:55:24 root] (abq_llm.py 321): INFO layer 33 iter 5 loss:1.4036821126937866 norm:0.004183812998235226 max memory_allocated 29784.01611328125 
[2024-12-23 16:56:07 root] (abq_llm.py 321): INFO layer 33 iter 6 loss:1.399115800857544 norm:0.0037740617990493774 max memory_allocated 29784.01611328125 
[2024-12-23 16:56:51 root] (abq_llm.py 321): INFO layer 33 iter 7 loss:1.3958053588867188 norm:0.0033944351598620415 max memory_allocated 29784.01611328125 
[2024-12-23 16:57:34 root] (abq_llm.py 321): INFO layer 33 iter 8 loss:1.3935892581939697 norm:0.003107016207650304 max memory_allocated 29784.01611328125 
[2024-12-23 16:58:18 root] (abq_llm.py 321): INFO layer 33 iter 9 loss:1.3918203115463257 norm:0.002814285224303603 max memory_allocated 29784.01611328125 
[2024-12-23 16:59:01 root] (abq_llm.py 321): INFO layer 33 iter 10 loss:1.3903449773788452 norm:0.0025996810290962458 max memory_allocated 29784.01611328125 
[2024-12-23 16:59:44 root] (abq_llm.py 321): INFO layer 33 iter 11 loss:1.3877661228179932 norm:0.0022137288469821215 max memory_allocated 29784.01611328125 
[2024-12-23 17:00:28 root] (abq_llm.py 321): INFO layer 33 iter 12 loss:1.3873100280761719 norm:0.002294971374794841 max memory_allocated 29784.01611328125 
[2024-12-23 17:01:11 root] (abq_llm.py 321): INFO layer 33 iter 13 loss:1.3869370222091675 norm:0.0020546962041407824 max memory_allocated 29784.01611328125 
[2024-12-23 17:01:55 root] (abq_llm.py 321): INFO layer 33 iter 14 loss:1.3860929012298584 norm:0.0019230179022997618 max memory_allocated 29784.01611328125 
[2024-12-23 17:02:38 root] (abq_llm.py 321): INFO layer 33 iter 15 loss:1.3852075338363647 norm:0.001828618929721415 max memory_allocated 29784.01611328125 
[2024-12-23 17:03:22 root] (abq_llm.py 321): INFO layer 33 iter 16 loss:1.3846455812454224 norm:0.0017810591962188482 max memory_allocated 29784.01611328125 
[2024-12-23 17:04:05 root] (abq_llm.py 321): INFO layer 33 iter 17 loss:1.3841649293899536 norm:0.0017487081931903958 max memory_allocated 29784.01611328125 
[2024-12-23 17:04:49 root] (abq_llm.py 321): INFO layer 33 iter 18 loss:1.3836359977722168 norm:0.0016913242870941758 max memory_allocated 29784.01611328125 
[2024-12-23 17:05:32 root] (abq_llm.py 321): INFO layer 33 iter 19 loss:1.383193016052246 norm:0.0016687155002728105 max memory_allocated 29784.01611328125 
[2024-12-23 17:05:44 root] (abq_llm.py 208): INFO === Start quantize layer 34 ===
[2024-12-23 17:06:31 root] (abq_llm.py 321): INFO layer 34 iter 0 loss:1.5921748876571655 norm:0.010510347783565521 max memory_allocated 29786.07861328125 
[2024-12-23 17:07:14 root] (abq_llm.py 321): INFO layer 34 iter 1 loss:1.5742374658584595 norm:0.006371063180267811 max memory_allocated 29786.07861328125 
[2024-12-23 17:07:58 root] (abq_llm.py 321): INFO layer 34 iter 2 loss:1.5535495281219482 norm:0.004532609600573778 max memory_allocated 29786.07861328125 
[2024-12-23 17:08:41 root] (abq_llm.py 321): INFO layer 34 iter 3 loss:1.5414707660675049 norm:0.003388776211068034 max memory_allocated 29786.07861328125 
[2024-12-23 17:09:25 root] (abq_llm.py 321): INFO layer 34 iter 4 loss:1.534223198890686 norm:0.002725206781178713 max memory_allocated 29786.07861328125 
[2024-12-23 17:10:08 root] (abq_llm.py 321): INFO layer 34 iter 5 loss:1.5282880067825317 norm:0.002514848718419671 max memory_allocated 29786.07861328125 
[2024-12-23 17:10:52 root] (abq_llm.py 321): INFO layer 34 iter 6 loss:1.523565649986267 norm:0.0024227332323789597 max memory_allocated 29786.07861328125 
[2024-12-23 17:11:35 root] (abq_llm.py 321): INFO layer 34 iter 7 loss:1.5200142860412598 norm:0.002337007550522685 max memory_allocated 29786.07861328125 
[2024-12-23 17:12:18 root] (abq_llm.py 321): INFO layer 34 iter 8 loss:1.5177313089370728 norm:0.0022579890210181475 max memory_allocated 29786.07861328125 
[2024-12-23 17:13:02 root] (abq_llm.py 321): INFO layer 34 iter 9 loss:1.516296625137329 norm:0.0021624749060720205 max memory_allocated 29786.07861328125 
[2024-12-23 17:13:45 root] (abq_llm.py 321): INFO layer 34 iter 10 loss:1.5150885581970215 norm:0.002094060881063342 max memory_allocated 29786.07861328125 
[2024-12-23 17:14:29 root] (abq_llm.py 321): INFO layer 34 iter 11 loss:1.5141503810882568 norm:0.002013476099818945 max memory_allocated 29786.07861328125 
[2024-12-23 17:15:12 root] (abq_llm.py 321): INFO layer 34 iter 12 loss:1.5134222507476807 norm:0.001993619604036212 max memory_allocated 29786.07861328125 
[2024-12-23 17:15:56 root] (abq_llm.py 321): INFO layer 34 iter 13 loss:1.5127140283584595 norm:0.0019220687681809068 max memory_allocated 29786.07861328125 
[2024-12-23 17:16:39 root] (abq_llm.py 321): INFO layer 34 iter 14 loss:1.5122095346450806 norm:0.001914454624056816 max memory_allocated 29786.07861328125 
[2024-12-23 17:17:23 root] (abq_llm.py 321): INFO layer 34 iter 15 loss:1.5116550922393799 norm:0.001893004053272307 max memory_allocated 29786.07861328125 
[2024-12-23 17:18:06 root] (abq_llm.py 321): INFO layer 34 iter 16 loss:1.511226773262024 norm:0.0018524772021919489 max memory_allocated 29786.07861328125 
[2024-12-23 17:18:49 root] (abq_llm.py 321): INFO layer 34 iter 17 loss:1.5108540058135986 norm:0.001828731968998909 max memory_allocated 29786.07861328125 
[2024-12-23 17:19:33 root] (abq_llm.py 321): INFO layer 34 iter 18 loss:1.510543942451477 norm:0.0018341742688789964 max memory_allocated 29786.07861328125 
[2024-12-23 17:20:16 root] (abq_llm.py 321): INFO layer 34 iter 19 loss:1.5102298259735107 norm:0.0018147473456338048 max memory_allocated 29786.07861328125 
[2024-12-23 17:20:29 root] (abq_llm.py 208): INFO === Start quantize layer 35 ===
[2024-12-23 17:21:15 root] (abq_llm.py 321): INFO layer 35 iter 0 loss:1.7451953887939453 norm:0.00887344405055046 max memory_allocated 29788.14111328125 
[2024-12-23 17:21:59 root] (abq_llm.py 321): INFO layer 35 iter 1 loss:1.7230499982833862 norm:0.006581120193004608 max memory_allocated 29788.14111328125 
[2024-12-23 17:22:42 root] (abq_llm.py 321): INFO layer 35 iter 2 loss:1.6991997957229614 norm:0.004929277580231428 max memory_allocated 29788.14111328125 
[2024-12-23 17:23:25 root] (abq_llm.py 321): INFO layer 35 iter 3 loss:1.6852178573608398 norm:0.003909186460077763 max memory_allocated 29788.14111328125 
[2024-12-23 17:24:09 root] (abq_llm.py 321): INFO layer 35 iter 4 loss:1.676405668258667 norm:0.0033793188631534576 max memory_allocated 29788.14111328125 
[2024-12-23 17:24:52 root] (abq_llm.py 321): INFO layer 35 iter 5 loss:1.669105052947998 norm:0.0030118441209197044 max memory_allocated 29788.14111328125 
[2024-12-23 17:25:36 root] (abq_llm.py 321): INFO layer 35 iter 6 loss:1.6633697748184204 norm:0.0028238981030881405 max memory_allocated 29788.14111328125 
[2024-12-23 17:26:19 root] (abq_llm.py 321): INFO layer 35 iter 7 loss:1.6593055725097656 norm:0.0025677585508674383 max memory_allocated 29788.14111328125 
[2024-12-23 17:27:03 root] (abq_llm.py 321): INFO layer 35 iter 8 loss:1.6569602489471436 norm:0.002488760743290186 max memory_allocated 29788.14111328125 
[2024-12-23 17:27:46 root] (abq_llm.py 321): INFO layer 35 iter 9 loss:1.655113935470581 norm:0.0023436788469552994 max memory_allocated 29788.14111328125 
[2024-12-23 17:28:29 root] (abq_llm.py 321): INFO layer 35 iter 10 loss:1.6535385847091675 norm:0.0022290037013590336 max memory_allocated 29788.14111328125 
[2024-12-23 17:29:13 root] (abq_llm.py 321): INFO layer 35 iter 11 loss:1.6523016691207886 norm:0.002141358098015189 max memory_allocated 29788.14111328125 
[2024-12-23 17:29:56 root] (abq_llm.py 321): INFO layer 35 iter 12 loss:1.6513646841049194 norm:0.002063083229586482 max memory_allocated 29788.14111328125 
