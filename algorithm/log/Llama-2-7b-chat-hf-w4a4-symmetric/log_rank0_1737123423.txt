[2025-01-17 14:17:03 root] (main.py 265): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-chat-hf', cache_dir='./cache', output_dir='./log/Llama-2-7b-chat-hf-w4a4-symmetric', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=True, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2025-01-17 14:17:07 root] (main.py 332): INFO === start quantization ===
[2025-01-17 14:17:07 root] (main.py 338): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-01-17 14:17:07 root] (abq_llm.py 62): INFO Starting ...
[2025-01-17 14:17:15 root] (abq_llm.py 212): INFO === Start quantize layer 0 ===
[2025-01-17 14:17:19 root] (abq_llm.py 268): INFO use compensation vector
[2025-01-17 14:17:49 root] (abq_llm.py 328): INFO layer 0 iter 0 loss:0.07575738430023193 norm:0.04652697592973709 max memory_allocated 22886.49365234375 
[2025-01-17 14:18:20 root] (abq_llm.py 328): INFO layer 0 iter 1 loss:0.047432348132133484 norm:0.027069779112935066 max memory_allocated 22886.49365234375 
[2025-01-17 14:18:50 root] (abq_llm.py 328): INFO layer 0 iter 2 loss:0.03799058496952057 norm:0.020888937637209892 max memory_allocated 22886.49365234375 
[2025-01-17 14:19:21 root] (abq_llm.py 328): INFO layer 0 iter 3 loss:0.03248177096247673 norm:0.017487075179815292 max memory_allocated 22886.49365234375 
[2025-01-17 14:19:52 root] (abq_llm.py 328): INFO layer 0 iter 4 loss:0.02967996522784233 norm:0.014844636432826519 max memory_allocated 22886.49365234375 
[2025-01-17 14:20:23 root] (abq_llm.py 328): INFO layer 0 iter 5 loss:0.02826526015996933 norm:0.011698193848133087 max memory_allocated 22886.49365234375 
[2025-01-17 14:20:54 root] (abq_llm.py 328): INFO layer 0 iter 6 loss:0.02735116332769394 norm:0.010303623043000698 max memory_allocated 22886.49365234375 
[2025-01-17 14:21:25 root] (abq_llm.py 328): INFO layer 0 iter 7 loss:0.02687300555408001 norm:0.00929272174835205 max memory_allocated 22886.49365234375 
[2025-01-17 14:21:56 root] (abq_llm.py 328): INFO layer 0 iter 8 loss:0.0261891707777977 norm:0.009107029065489769 max memory_allocated 22886.49365234375 
[2025-01-17 14:22:27 root] (abq_llm.py 328): INFO layer 0 iter 9 loss:0.02594621479511261 norm:0.007901793345808983 max memory_allocated 22886.49365234375 
[2025-01-17 14:22:58 root] (abq_llm.py 328): INFO layer 0 iter 10 loss:0.02574080228805542 norm:0.006856614723801613 max memory_allocated 22886.49365234375 
[2025-01-17 14:23:30 root] (abq_llm.py 328): INFO layer 0 iter 11 loss:0.025602418929338455 norm:0.006901747081428766 max memory_allocated 22886.49365234375 
[2025-01-17 14:24:01 root] (abq_llm.py 328): INFO layer 0 iter 12 loss:0.02520539052784443 norm:0.005850198678672314 max memory_allocated 22886.49365234375 
[2025-01-17 14:24:32 root] (abq_llm.py 328): INFO layer 0 iter 13 loss:0.025315895676612854 norm:0.00699717877432704 max memory_allocated 22886.49365234375 
[2025-01-17 14:25:03 root] (abq_llm.py 328): INFO layer 0 iter 14 loss:0.02521127462387085 norm:0.008276954293251038 max memory_allocated 22886.49365234375 
[2025-01-17 14:25:34 root] (abq_llm.py 328): INFO layer 0 iter 15 loss:0.025024408474564552 norm:0.006108174100518227 max memory_allocated 22886.49365234375 
[2025-01-17 14:26:05 root] (abq_llm.py 328): INFO layer 0 iter 16 loss:0.025190068408846855 norm:0.00915838684886694 max memory_allocated 22886.49365234375 
[2025-01-17 14:26:36 root] (abq_llm.py 328): INFO layer 0 iter 17 loss:0.025210676714777946 norm:0.007030193693935871 max memory_allocated 22886.49365234375 
[2025-01-17 14:27:07 root] (abq_llm.py 328): INFO layer 0 iter 18 loss:0.024898922070860863 norm:0.005656518507748842 max memory_allocated 22886.49365234375 
[2025-01-17 14:27:38 root] (abq_llm.py 328): INFO layer 0 iter 19 loss:0.024747325107455254 norm:0.005763473454862833 max memory_allocated 22886.49365234375 
[2025-01-17 14:27:47 root] (abq_llm.py 212): INFO === Start quantize layer 1 ===
[2025-01-17 14:27:57 root] (abq_llm.py 268): INFO use compensation vector
[2025-01-17 14:28:28 root] (abq_llm.py 328): INFO layer 1 iter 0 loss:0.30381813645362854 norm:0.09814110398292542 max memory_allocated 22888.16552734375 
[2025-01-17 14:28:58 root] (abq_llm.py 328): INFO layer 1 iter 1 loss:0.22636404633522034 norm:0.08341685682535172 max memory_allocated 22888.16552734375 
[2025-01-17 14:29:29 root] (abq_llm.py 328): INFO layer 1 iter 2 loss:0.18870893120765686 norm:0.07356563210487366 max memory_allocated 22888.16552734375 
[2025-01-17 14:30:00 root] (abq_llm.py 328): INFO layer 1 iter 3 loss:0.16911505162715912 norm:0.061954617500305176 max memory_allocated 22888.16552734375 
[2025-01-17 14:30:31 root] (abq_llm.py 328): INFO layer 1 iter 4 loss:0.15726691484451294 norm:0.05724077299237251 max memory_allocated 22888.16552734375 
[2025-01-17 14:31:02 root] (abq_llm.py 328): INFO layer 1 iter 5 loss:0.15085788071155548 norm:0.053563304245471954 max memory_allocated 22888.16552734375 
[2025-01-17 14:31:33 root] (abq_llm.py 328): INFO layer 1 iter 6 loss:0.14545971155166626 norm:0.05183827131986618 max memory_allocated 22888.16552734375 
[2025-01-17 14:32:04 root] (abq_llm.py 328): INFO layer 1 iter 7 loss:0.14234118163585663 norm:0.050571296364068985 max memory_allocated 22888.16552734375 
[2025-01-17 14:32:35 root] (abq_llm.py 328): INFO layer 1 iter 8 loss:0.13929486274719238 norm:0.0444672554731369 max memory_allocated 22888.16552734375 
[2025-01-17 14:33:07 root] (abq_llm.py 328): INFO layer 1 iter 9 loss:0.13699427247047424 norm:0.04501738026738167 max memory_allocated 22888.16552734375 
[2025-01-17 14:33:38 root] (abq_llm.py 328): INFO layer 1 iter 10 loss:0.13352921605110168 norm:0.04123703017830849 max memory_allocated 22888.16552734375 
[2025-01-17 14:34:09 root] (abq_llm.py 328): INFO layer 1 iter 11 loss:0.1317143738269806 norm:0.04559247940778732 max memory_allocated 22888.16552734375 
[2025-01-17 14:34:40 root] (abq_llm.py 328): INFO layer 1 iter 12 loss:0.13149221241474152 norm:0.04074054956436157 max memory_allocated 22888.16552734375 
[2025-01-17 14:35:11 root] (abq_llm.py 328): INFO layer 1 iter 13 loss:0.13146331906318665 norm:0.041724614799022675 max memory_allocated 22888.16552734375 
[2025-01-17 14:35:42 root] (abq_llm.py 328): INFO layer 1 iter 14 loss:0.12989842891693115 norm:0.039593178778886795 max memory_allocated 22888.16552734375 
[2025-01-17 14:36:13 root] (abq_llm.py 328): INFO layer 1 iter 15 loss:0.13077612221240997 norm:0.04040085896849632 max memory_allocated 22888.16552734375 
[2025-01-17 14:36:44 root] (abq_llm.py 328): INFO layer 1 iter 16 loss:0.12979058921337128 norm:0.0409943088889122 max memory_allocated 22888.16552734375 
[2025-01-17 14:37:15 root] (abq_llm.py 328): INFO layer 1 iter 17 loss:0.12901659309864044 norm:0.05382656678557396 max memory_allocated 22888.16552734375 
[2025-01-17 14:37:46 root] (abq_llm.py 328): INFO layer 1 iter 18 loss:0.12916402518749237 norm:0.03918727487325668 max memory_allocated 22888.16552734375 
[2025-01-17 14:38:17 root] (abq_llm.py 328): INFO layer 1 iter 19 loss:0.12768076360225677 norm:0.038117311894893646 max memory_allocated 22888.16552734375 
[2025-01-17 14:38:26 root] (abq_llm.py 212): INFO === Start quantize layer 2 ===
[2025-01-17 14:38:34 root] (abq_llm.py 268): INFO use compensation vector
[2025-01-17 14:39:05 root] (abq_llm.py 328): INFO layer 2 iter 0 loss:0.3328998386859894 norm:0.20078521966934204 max memory_allocated 22889.83740234375 
[2025-01-17 14:39:36 root] (abq_llm.py 328): INFO layer 2 iter 1 loss:0.24265597760677338 norm:0.05725694075226784 max memory_allocated 22889.83740234375 
[2025-01-17 14:40:07 root] (abq_llm.py 328): INFO layer 2 iter 2 loss:0.2157168984413147 norm:0.03898532688617706 max memory_allocated 22889.83740234375 
[2025-01-17 14:40:38 root] (abq_llm.py 328): INFO layer 2 iter 3 loss:0.19858506321907043 norm:0.02778974175453186 max memory_allocated 22889.83740234375 
[2025-01-17 14:41:09 root] (abq_llm.py 328): INFO layer 2 iter 4 loss:0.1894068717956543 norm:0.021789487451314926 max memory_allocated 22889.83740234375 
[2025-01-17 14:41:40 root] (abq_llm.py 328): INFO layer 2 iter 5 loss:0.18298578262329102 norm:0.017344987019896507 max memory_allocated 22889.83740234375 
[2025-01-17 14:42:12 root] (abq_llm.py 328): INFO layer 2 iter 6 loss:0.17833919823169708 norm:0.023752981796860695 max memory_allocated 22889.83740234375 
[2025-01-17 14:42:43 root] (abq_llm.py 328): INFO layer 2 iter 7 loss:0.17504549026489258 norm:0.0127031235024333 max memory_allocated 22889.83740234375 
[2025-01-17 14:43:14 root] (abq_llm.py 328): INFO layer 2 iter 8 loss:0.17200523614883423 norm:0.011221475899219513 max memory_allocated 22889.83740234375 
[2025-01-17 14:43:45 root] (abq_llm.py 328): INFO layer 2 iter 9 loss:0.16980376839637756 norm:0.010168561711907387 max memory_allocated 22889.83740234375 
[2025-01-17 14:44:16 root] (abq_llm.py 328): INFO layer 2 iter 10 loss:0.16826187074184418 norm:0.00949225015938282 max memory_allocated 22889.83740234375 
[2025-01-17 14:44:47 root] (abq_llm.py 328): INFO layer 2 iter 11 loss:0.16717304289340973 norm:0.009029146283864975 max memory_allocated 22889.83740234375 
[2025-01-17 14:45:18 root] (abq_llm.py 328): INFO layer 2 iter 12 loss:0.1662731170654297 norm:0.008930081501603127 max memory_allocated 22889.83740234375 
[2025-01-17 14:45:49 root] (abq_llm.py 328): INFO layer 2 iter 13 loss:0.16535799205303192 norm:0.008434596471488476 max memory_allocated 22889.83740234375 
[2025-01-17 14:46:20 root] (abq_llm.py 328): INFO layer 2 iter 14 loss:0.16460415720939636 norm:0.008301468566060066 max memory_allocated 22889.83740234375 
[2025-01-17 14:46:52 root] (abq_llm.py 328): INFO layer 2 iter 15 loss:0.1642179787158966 norm:0.007937713526189327 max memory_allocated 22889.83740234375 
[2025-01-17 14:47:23 root] (abq_llm.py 328): INFO layer 2 iter 16 loss:0.16359326243400574 norm:0.007641105446964502 max memory_allocated 22889.83740234375 
[2025-01-17 14:47:54 root] (abq_llm.py 328): INFO layer 2 iter 17 loss:0.16295817494392395 norm:0.00768722640350461 max memory_allocated 22889.83740234375 
[2025-01-17 14:48:25 root] (abq_llm.py 328): INFO layer 2 iter 18 loss:0.16243863105773926 norm:0.007218396291136742 max memory_allocated 22889.83740234375 
[2025-01-17 14:48:56 root] (abq_llm.py 328): INFO layer 2 iter 19 loss:0.1619592010974884 norm:0.006713410373777151 max memory_allocated 22889.83740234375 
[2025-01-17 14:49:05 root] (abq_llm.py 212): INFO === Start quantize layer 3 ===
[2025-01-17 14:49:42 root] (abq_llm.py 328): INFO layer 3 iter 0 loss:0.4256141781806946 norm:0.16085858643054962 max memory_allocated 22891.39404296875 
[2025-01-17 14:50:13 root] (abq_llm.py 328): INFO layer 3 iter 1 loss:0.3172011077404022 norm:0.04003196954727173 max memory_allocated 22891.39404296875 
[2025-01-17 14:50:44 root] (abq_llm.py 328): INFO layer 3 iter 2 loss:0.2791649401187897 norm:0.02267424948513508 max memory_allocated 22891.39404296875 
[2025-01-17 14:51:15 root] (abq_llm.py 328): INFO layer 3 iter 3 loss:0.2563249468803406 norm:0.014422250911593437 max memory_allocated 22891.39404296875 
[2025-01-17 14:51:46 root] (abq_llm.py 328): INFO layer 3 iter 4 loss:0.24542078375816345 norm:0.009918197989463806 max memory_allocated 22891.39404296875 
[2025-01-17 14:52:17 root] (abq_llm.py 328): INFO layer 3 iter 5 loss:0.2388119101524353 norm:0.007616801653057337 max memory_allocated 22891.39404296875 
[2025-01-17 14:52:48 root] (abq_llm.py 328): INFO layer 3 iter 6 loss:0.23470422625541687 norm:0.006562485359609127 max memory_allocated 22891.39404296875 
[2025-01-17 14:53:19 root] (abq_llm.py 328): INFO layer 3 iter 7 loss:0.23174910247325897 norm:0.005872281733900309 max memory_allocated 22891.39404296875 
[2025-01-17 14:53:50 root] (abq_llm.py 328): INFO layer 3 iter 8 loss:0.2297208309173584 norm:0.0053984299302101135 max memory_allocated 22891.39404296875 
[2025-01-17 14:54:21 root] (abq_llm.py 328): INFO layer 3 iter 9 loss:0.22817304730415344 norm:0.004906936548650265 max memory_allocated 22891.39404296875 
[2025-01-17 14:54:52 root] (abq_llm.py 328): INFO layer 3 iter 10 loss:0.22694599628448486 norm:0.004630262963473797 max memory_allocated 22891.39404296875 
[2025-01-17 14:55:23 root] (abq_llm.py 328): INFO layer 3 iter 11 loss:0.22607678174972534 norm:0.004491700790822506 max memory_allocated 22891.39404296875 
[2025-01-17 14:55:54 root] (abq_llm.py 328): INFO layer 3 iter 12 loss:0.22497108578681946 norm:0.004318886436522007 max memory_allocated 22891.39404296875 
[2025-01-17 14:56:25 root] (abq_llm.py 328): INFO layer 3 iter 13 loss:0.22457300126552582 norm:0.0042419699020683765 max memory_allocated 22891.39404296875 
[2025-01-17 14:56:56 root] (abq_llm.py 328): INFO layer 3 iter 14 loss:0.22390449047088623 norm:0.004088669549673796 max memory_allocated 22891.39404296875 
[2025-01-17 14:57:27 root] (abq_llm.py 328): INFO layer 3 iter 15 loss:0.22310975193977356 norm:0.004050605930387974 max memory_allocated 22891.39404296875 
[2025-01-17 14:57:58 root] (abq_llm.py 328): INFO layer 3 iter 16 loss:0.22245138883590698 norm:0.003932558000087738 max memory_allocated 22891.39404296875 
[2025-01-17 14:58:29 root] (abq_llm.py 328): INFO layer 3 iter 17 loss:0.2219325304031372 norm:0.0036772042512893677 max memory_allocated 22891.39404296875 
[2025-01-17 14:59:00 root] (abq_llm.py 328): INFO layer 3 iter 18 loss:0.22135737538337708 norm:0.003455570200458169 max memory_allocated 22891.39404296875 
[2025-01-17 14:59:31 root] (abq_llm.py 328): INFO layer 3 iter 19 loss:0.22090765833854675 norm:0.003301884513348341 max memory_allocated 22891.39404296875 
[2025-01-17 14:59:40 root] (abq_llm.py 212): INFO === Start quantize layer 4 ===
[2025-01-17 15:00:19 root] (abq_llm.py 328): INFO layer 4 iter 0 loss:0.5376523733139038 norm:0.17151600122451782 max memory_allocated 22893.06591796875 
[2025-01-17 15:00:50 root] (abq_llm.py 328): INFO layer 4 iter 1 loss:0.4199233651161194 norm:0.05046354606747627 max memory_allocated 22893.06591796875 
[2025-01-17 15:01:21 root] (abq_llm.py 328): INFO layer 4 iter 2 loss:0.36705437302589417 norm:0.026287177577614784 max memory_allocated 22893.06591796875 
[2025-01-17 15:01:52 root] (abq_llm.py 328): INFO layer 4 iter 3 loss:0.33729657530784607 norm:0.017179610207676888 max memory_allocated 22893.06591796875 
[2025-01-17 15:02:23 root] (abq_llm.py 328): INFO layer 4 iter 4 loss:0.31834959983825684 norm:0.011114874854683876 max memory_allocated 22893.06591796875 
[2025-01-17 15:02:54 root] (abq_llm.py 328): INFO layer 4 iter 5 loss:0.3067687153816223 norm:0.008118693716824055 max memory_allocated 22893.06591796875 
[2025-01-17 15:03:25 root] (abq_llm.py 328): INFO layer 4 iter 6 loss:0.2991694509983063 norm:0.006483331322669983 max memory_allocated 22893.06591796875 
[2025-01-17 15:03:56 root] (abq_llm.py 328): INFO layer 4 iter 7 loss:0.2943439781665802 norm:0.0057586608454585075 max memory_allocated 22893.06591796875 
[2025-01-17 15:04:27 root] (abq_llm.py 328): INFO layer 4 iter 8 loss:0.2906959354877472 norm:0.0052796476520597935 max memory_allocated 22893.06591796875 
[2025-01-17 15:04:58 root] (abq_llm.py 328): INFO layer 4 iter 9 loss:0.2882930338382721 norm:0.004822931252419949 max memory_allocated 22893.06591796875 
[2025-01-17 15:05:29 root] (abq_llm.py 328): INFO layer 4 iter 10 loss:0.2864442467689514 norm:0.00459359772503376 max memory_allocated 22893.06591796875 
[2025-01-17 15:06:00 root] (abq_llm.py 328): INFO layer 4 iter 11 loss:0.2847403585910797 norm:0.004236868117004633 max memory_allocated 22893.06591796875 
[2025-01-17 15:06:31 root] (abq_llm.py 328): INFO layer 4 iter 12 loss:0.2833687663078308 norm:0.0038884298410266638 max memory_allocated 22893.06591796875 
[2025-01-17 15:07:02 root] (abq_llm.py 328): INFO layer 4 iter 13 loss:0.28238344192504883 norm:0.0038671032525599003 max memory_allocated 22893.06591796875 
[2025-01-17 15:07:33 root] (abq_llm.py 328): INFO layer 4 iter 14 loss:0.2815191447734833 norm:0.003773199860006571 max memory_allocated 22893.06591796875 
[2025-01-17 15:08:05 root] (abq_llm.py 328): INFO layer 4 iter 15 loss:0.28083133697509766 norm:0.003668759483844042 max memory_allocated 22893.06591796875 
[2025-01-17 15:08:36 root] (abq_llm.py 328): INFO layer 4 iter 16 loss:0.2802068889141083 norm:0.0035685477778315544 max memory_allocated 22893.06591796875 
[2025-01-17 15:09:07 root] (abq_llm.py 328): INFO layer 4 iter 17 loss:0.27962130308151245 norm:0.00361876143142581 max memory_allocated 22893.06591796875 
[2025-01-17 15:09:38 root] (abq_llm.py 328): INFO layer 4 iter 18 loss:0.2790466845035553 norm:0.0036413189955055714 max memory_allocated 22893.06591796875 
[2025-01-17 15:10:09 root] (abq_llm.py 328): INFO layer 4 iter 19 loss:0.27843353152275085 norm:0.0038089673034846783 max memory_allocated 22893.06591796875 
[2025-01-17 15:10:18 root] (abq_llm.py 212): INFO === Start quantize layer 5 ===
[2025-01-17 15:10:55 root] (abq_llm.py 328): INFO layer 5 iter 0 loss:0.5822184085845947 norm:0.10375326126813889 max memory_allocated 22894.73779296875 
[2025-01-17 15:11:26 root] (abq_llm.py 328): INFO layer 5 iter 1 loss:0.47783854603767395 norm:0.04247578606009483 max memory_allocated 22894.73779296875 
[2025-01-17 15:11:57 root] (abq_llm.py 328): INFO layer 5 iter 2 loss:0.419212281703949 norm:0.020866859704256058 max memory_allocated 22894.73779296875 
[2025-01-17 15:12:28 root] (abq_llm.py 328): INFO layer 5 iter 3 loss:0.38784968852996826 norm:0.014707151800394058 max memory_allocated 22894.73779296875 
[2025-01-17 15:12:59 root] (abq_llm.py 328): INFO layer 5 iter 4 loss:0.3684210181236267 norm:0.009893235750496387 max memory_allocated 22894.73779296875 
[2025-01-17 15:13:30 root] (abq_llm.py 328): INFO layer 5 iter 5 loss:0.3584865629673004 norm:0.007906440645456314 max memory_allocated 22894.73779296875 
[2025-01-17 15:14:01 root] (abq_llm.py 328): INFO layer 5 iter 6 loss:0.3512563407421112 norm:0.006793879438191652 max memory_allocated 22894.73779296875 
[2025-01-17 15:14:32 root] (abq_llm.py 328): INFO layer 5 iter 7 loss:0.34654098749160767 norm:0.005645828787237406 max memory_allocated 22894.73779296875 
[2025-01-17 15:15:03 root] (abq_llm.py 328): INFO layer 5 iter 8 loss:0.3431546092033386 norm:0.005167274735867977 max memory_allocated 22894.73779296875 
[2025-01-17 15:15:34 root] (abq_llm.py 328): INFO layer 5 iter 9 loss:0.3407948315143585 norm:0.0047538382932543755 max memory_allocated 22894.73779296875 
[2025-01-17 15:16:05 root] (abq_llm.py 328): INFO layer 5 iter 10 loss:0.3386361002922058 norm:0.004778495989739895 max memory_allocated 22894.73779296875 
[2025-01-17 15:16:36 root] (abq_llm.py 328): INFO layer 5 iter 11 loss:0.3369732201099396 norm:0.00467568589374423 max memory_allocated 22894.73779296875 
[2025-01-17 15:17:07 root] (abq_llm.py 328): INFO layer 5 iter 12 loss:0.33548325300216675 norm:0.004528684075921774 max memory_allocated 22894.73779296875 
[2025-01-17 15:17:38 root] (abq_llm.py 328): INFO layer 5 iter 13 loss:0.3345869183540344 norm:0.0045966156758368015 max memory_allocated 22894.73779296875 
[2025-01-17 15:18:09 root] (abq_llm.py 328): INFO layer 5 iter 14 loss:0.3337746262550354 norm:0.004324753303080797 max memory_allocated 22894.73779296875 
[2025-01-17 15:18:40 root] (abq_llm.py 328): INFO layer 5 iter 15 loss:0.33302703499794006 norm:0.004124090075492859 max memory_allocated 22894.73779296875 
[2025-01-17 15:19:11 root] (abq_llm.py 328): INFO layer 5 iter 16 loss:0.3321872055530548 norm:0.003947576507925987 max memory_allocated 22894.73779296875 
[2025-01-17 15:19:42 root] (abq_llm.py 328): INFO layer 5 iter 17 loss:0.33165243268013 norm:0.003989221528172493 max memory_allocated 22894.73779296875 
[2025-01-17 15:20:13 root] (abq_llm.py 328): INFO layer 5 iter 18 loss:0.33109626173973083 norm:0.003990917466580868 max memory_allocated 22894.73779296875 
[2025-01-17 15:20:45 root] (abq_llm.py 328): INFO layer 5 iter 19 loss:0.33056920766830444 norm:0.003927102778106928 max memory_allocated 22894.73779296875 
[2025-01-17 15:20:53 root] (abq_llm.py 212): INFO === Start quantize layer 6 ===
[2025-01-17 15:21:33 root] (abq_llm.py 328): INFO layer 6 iter 0 loss:0.7604950666427612 norm:0.14894455671310425 max memory_allocated 22896.40966796875 
[2025-01-17 15:22:04 root] (abq_llm.py 328): INFO layer 6 iter 1 loss:0.6004273891448975 norm:0.057515066117048264 max memory_allocated 22896.40966796875 
[2025-01-17 15:22:35 root] (abq_llm.py 328): INFO layer 6 iter 2 loss:0.5215432643890381 norm:0.03048289194703102 max memory_allocated 22896.40966796875 
[2025-01-17 15:23:06 root] (abq_llm.py 328): INFO layer 6 iter 3 loss:0.4826185703277588 norm:0.020141903311014175 max memory_allocated 22896.40966796875 
[2025-01-17 15:23:37 root] (abq_llm.py 328): INFO layer 6 iter 4 loss:0.45364946126937866 norm:0.014338240027427673 max memory_allocated 22896.40966796875 
[2025-01-17 15:24:08 root] (abq_llm.py 328): INFO layer 6 iter 5 loss:0.43631818890571594 norm:0.010987183079123497 max memory_allocated 22896.40966796875 
[2025-01-17 15:24:39 root] (abq_llm.py 328): INFO layer 6 iter 6 loss:0.4264276325702667 norm:0.009226891212165356 max memory_allocated 22896.40966796875 
[2025-01-17 15:25:10 root] (abq_llm.py 328): INFO layer 6 iter 7 loss:0.41885408759117126 norm:0.008346941322088242 max memory_allocated 22896.40966796875 
[2025-01-17 15:25:41 root] (abq_llm.py 328): INFO layer 6 iter 8 loss:0.41349127888679504 norm:0.007897384464740753 max memory_allocated 22896.40966796875 
[2025-01-17 15:26:13 root] (abq_llm.py 328): INFO layer 6 iter 9 loss:0.408894419670105 norm:0.007416597567498684 max memory_allocated 22896.40966796875 
[2025-01-17 15:26:44 root] (abq_llm.py 328): INFO layer 6 iter 10 loss:0.40556102991104126 norm:0.007140093483030796 max memory_allocated 22896.40966796875 
[2025-01-17 15:27:15 root] (abq_llm.py 328): INFO layer 6 iter 11 loss:0.4024747908115387 norm:0.006559767294675112 max memory_allocated 22896.40966796875 
[2025-01-17 15:27:46 root] (abq_llm.py 328): INFO layer 6 iter 12 loss:0.4003139138221741 norm:0.006347264628857374 max memory_allocated 22896.40966796875 
[2025-01-17 15:28:17 root] (abq_llm.py 328): INFO layer 6 iter 13 loss:0.3981681168079376 norm:0.006066753529012203 max memory_allocated 22896.40966796875 
[2025-01-17 15:28:48 root] (abq_llm.py 328): INFO layer 6 iter 14 loss:0.39645054936408997 norm:0.005741865839809179 max memory_allocated 22896.40966796875 
[2025-01-17 15:29:19 root] (abq_llm.py 328): INFO layer 6 iter 15 loss:0.3951316475868225 norm:0.005806473549455404 max memory_allocated 22896.40966796875 
[2025-01-17 15:29:50 root] (abq_llm.py 328): INFO layer 6 iter 16 loss:0.39367976784706116 norm:0.005572811234742403 max memory_allocated 22896.40966796875 
[2025-01-17 15:30:21 root] (abq_llm.py 328): INFO layer 6 iter 17 loss:0.39284589886665344 norm:0.005423889495432377 max memory_allocated 22896.40966796875 
[2025-01-17 15:30:52 root] (abq_llm.py 328): INFO layer 6 iter 18 loss:0.39187562465667725 norm:0.005226624198257923 max memory_allocated 22896.40966796875 
[2025-01-17 15:31:23 root] (abq_llm.py 328): INFO layer 6 iter 19 loss:0.3910614848136902 norm:0.005109766032546759 max memory_allocated 22896.40966796875 
[2025-01-17 15:31:32 root] (abq_llm.py 212): INFO === Start quantize layer 7 ===
[2025-01-17 15:32:11 root] (abq_llm.py 328): INFO layer 7 iter 0 loss:0.8568649888038635 norm:0.13048456609249115 max memory_allocated 22898.08154296875 
[2025-01-17 15:32:42 root] (abq_llm.py 328): INFO layer 7 iter 1 loss:0.6617158651351929 norm:0.0615544468164444 max memory_allocated 22898.08154296875 
[2025-01-17 15:33:13 root] (abq_llm.py 328): INFO layer 7 iter 2 loss:0.5720364451408386 norm:0.03111998550593853 max memory_allocated 22898.08154296875 
[2025-01-17 15:33:44 root] (abq_llm.py 328): INFO layer 7 iter 3 loss:0.5223963260650635 norm:0.019933709874749184 max memory_allocated 22898.08154296875 
[2025-01-17 15:34:15 root] (abq_llm.py 328): INFO layer 7 iter 4 loss:0.4930526614189148 norm:0.014862500131130219 max memory_allocated 22898.08154296875 
[2025-01-17 15:34:46 root] (abq_llm.py 328): INFO layer 7 iter 5 loss:0.4773772060871124 norm:0.011289658956229687 max memory_allocated 22898.08154296875 
[2025-01-17 15:35:17 root] (abq_llm.py 328): INFO layer 7 iter 6 loss:0.4682897925376892 norm:0.010015730746090412 max memory_allocated 22898.08154296875 
[2025-01-17 15:35:48 root] (abq_llm.py 328): INFO layer 7 iter 7 loss:0.4619700312614441 norm:0.008835912682116032 max memory_allocated 22898.08154296875 
[2025-01-17 15:36:19 root] (abq_llm.py 328): INFO layer 7 iter 8 loss:0.45748528838157654 norm:0.008036380633711815 max memory_allocated 22898.08154296875 
[2025-01-17 15:36:50 root] (abq_llm.py 328): INFO layer 7 iter 9 loss:0.45380541682243347 norm:0.007317668758332729 max memory_allocated 22898.08154296875 
[2025-01-17 15:37:21 root] (abq_llm.py 328): INFO layer 7 iter 10 loss:0.4509105682373047 norm:0.00697309011593461 max memory_allocated 22898.08154296875 
[2025-01-17 15:37:52 root] (abq_llm.py 328): INFO layer 7 iter 11 loss:0.44829002022743225 norm:0.006722534075379372 max memory_allocated 22898.08154296875 
[2025-01-17 15:38:23 root] (abq_llm.py 328): INFO layer 7 iter 12 loss:0.4451509416103363 norm:0.006397409830242395 max memory_allocated 22898.08154296875 
[2025-01-17 15:38:54 root] (abq_llm.py 328): INFO layer 7 iter 13 loss:0.44345349073410034 norm:0.00648413086310029 max memory_allocated 22898.08154296875 
[2025-01-17 15:39:25 root] (abq_llm.py 328): INFO layer 7 iter 14 loss:0.4413863718509674 norm:0.006192860193550587 max memory_allocated 22898.08154296875 
[2025-01-17 15:39:56 root] (abq_llm.py 328): INFO layer 7 iter 15 loss:0.4400594234466553 norm:0.006468938197940588 max memory_allocated 22898.08154296875 
[2025-01-17 15:40:28 root] (abq_llm.py 328): INFO layer 7 iter 16 loss:0.4386807680130005 norm:0.006465455051511526 max memory_allocated 22898.08154296875 
[2025-01-17 15:40:59 root] (abq_llm.py 328): INFO layer 7 iter 17 loss:0.43745607137680054 norm:0.006498738192021847 max memory_allocated 22898.08154296875 
[2025-01-17 15:41:30 root] (abq_llm.py 328): INFO layer 7 iter 18 loss:0.43652939796447754 norm:0.006411546375602484 max memory_allocated 22898.08154296875 
[2025-01-17 15:42:01 root] (abq_llm.py 328): INFO layer 7 iter 19 loss:0.4355289340019226 norm:0.006418614182621241 max memory_allocated 22898.08154296875 
[2025-01-17 15:42:10 root] (abq_llm.py 212): INFO === Start quantize layer 8 ===
[2025-01-17 15:42:47 root] (abq_llm.py 328): INFO layer 8 iter 0 loss:0.8045791983604431 norm:0.08252155780792236 max memory_allocated 22899.75341796875 
[2025-01-17 15:43:18 root] (abq_llm.py 328): INFO layer 8 iter 1 loss:0.6701212525367737 norm:0.03366965055465698 max memory_allocated 22899.75341796875 
[2025-01-17 15:43:49 root] (abq_llm.py 328): INFO layer 8 iter 2 loss:0.5998105406761169 norm:0.022475244477391243 max memory_allocated 22899.75341796875 
[2025-01-17 15:44:20 root] (abq_llm.py 328): INFO layer 8 iter 3 loss:0.5543262958526611 norm:0.014715452678501606 max memory_allocated 22899.75341796875 
[2025-01-17 15:44:51 root] (abq_llm.py 328): INFO layer 8 iter 4 loss:0.52164226770401 norm:0.00908332597464323 max memory_allocated 22899.75341796875 
[2025-01-17 15:45:22 root] (abq_llm.py 328): INFO layer 8 iter 5 loss:0.5061352252960205 norm:0.007166344206780195 max memory_allocated 22899.75341796875 
[2025-01-17 15:45:53 root] (abq_llm.py 328): INFO layer 8 iter 6 loss:0.4980428218841553 norm:0.006764122284948826 max memory_allocated 22899.75341796875 
[2025-01-17 15:46:24 root] (abq_llm.py 328): INFO layer 8 iter 7 loss:0.49206042289733887 norm:0.006207671482115984 max memory_allocated 22899.75341796875 
[2025-01-17 15:46:55 root] (abq_llm.py 328): INFO layer 8 iter 8 loss:0.48785313963890076 norm:0.005812705494463444 max memory_allocated 22899.75341796875 
[2025-01-17 15:47:27 root] (abq_llm.py 328): INFO layer 8 iter 9 loss:0.48457276821136475 norm:0.005558826960623264 max memory_allocated 22899.75341796875 
[2025-01-17 15:47:58 root] (abq_llm.py 328): INFO layer 8 iter 10 loss:0.48173099756240845 norm:0.005358733702450991 max memory_allocated 22899.75341796875 
[2025-01-17 15:48:29 root] (abq_llm.py 328): INFO layer 8 iter 11 loss:0.47988975048065186 norm:0.005264307837933302 max memory_allocated 22899.75341796875 
[2025-01-17 15:49:00 root] (abq_llm.py 328): INFO layer 8 iter 12 loss:0.47816604375839233 norm:0.0050552054308354855 max memory_allocated 22899.75341796875 
[2025-01-17 15:49:31 root] (abq_llm.py 328): INFO layer 8 iter 13 loss:0.47657305002212524 norm:0.004905025474727154 max memory_allocated 22899.75341796875 
[2025-01-17 15:50:02 root] (abq_llm.py 328): INFO layer 8 iter 14 loss:0.47533372044563293 norm:0.004739111755043268 max memory_allocated 22899.75341796875 
[2025-01-17 15:50:33 root] (abq_llm.py 328): INFO layer 8 iter 15 loss:0.47435814142227173 norm:0.004669828340411186 max memory_allocated 22899.75341796875 
[2025-01-17 15:51:04 root] (abq_llm.py 328): INFO layer 8 iter 16 loss:0.47345489263534546 norm:0.004580828361213207 max memory_allocated 22899.75341796875 
[2025-01-17 15:51:35 root] (abq_llm.py 328): INFO layer 8 iter 17 loss:0.4728374779224396 norm:0.004868871066719294 max memory_allocated 22899.75341796875 
[2025-01-17 15:52:06 root] (abq_llm.py 328): INFO layer 8 iter 18 loss:0.47210103273391724 norm:0.0046995654702186584 max memory_allocated 22899.75341796875 
[2025-01-17 15:52:37 root] (abq_llm.py 328): INFO layer 8 iter 19 loss:0.4713563621044159 norm:0.004720230586826801 max memory_allocated 22899.75341796875 
[2025-01-17 15:52:46 root] (abq_llm.py 212): INFO === Start quantize layer 9 ===
[2025-01-17 15:53:23 root] (abq_llm.py 328): INFO layer 9 iter 0 loss:0.8265296816825867 norm:0.08271534740924835 max memory_allocated 22901.42529296875 
[2025-01-17 15:53:54 root] (abq_llm.py 328): INFO layer 9 iter 1 loss:0.7111796140670776 norm:0.04109848290681839 max memory_allocated 22901.42529296875 
[2025-01-17 15:54:25 root] (abq_llm.py 328): INFO layer 9 iter 2 loss:0.6433794498443604 norm:0.024824216961860657 max memory_allocated 22901.42529296875 
[2025-01-17 15:54:56 root] (abq_llm.py 328): INFO layer 9 iter 3 loss:0.5949772000312805 norm:0.016423247754573822 max memory_allocated 22901.42529296875 
[2025-01-17 15:55:27 root] (abq_llm.py 328): INFO layer 9 iter 4 loss:0.5565205216407776 norm:0.010853145271539688 max memory_allocated 22901.42529296875 
[2025-01-17 15:55:58 root] (abq_llm.py 328): INFO layer 9 iter 5 loss:0.5371350049972534 norm:0.00812344066798687 max memory_allocated 22901.42529296875 
[2025-01-17 15:56:29 root] (abq_llm.py 328): INFO layer 9 iter 6 loss:0.5259665250778198 norm:0.006502766627818346 max memory_allocated 22901.42529296875 
[2025-01-17 15:57:00 root] (abq_llm.py 328): INFO layer 9 iter 7 loss:0.5195157527923584 norm:0.005771630443632603 max memory_allocated 22901.42529296875 
[2025-01-17 15:57:31 root] (abq_llm.py 328): INFO layer 9 iter 8 loss:0.5147503018379211 norm:0.00545027619227767 max memory_allocated 22901.42529296875 
[2025-01-17 15:58:02 root] (abq_llm.py 328): INFO layer 9 iter 9 loss:0.5111172795295715 norm:0.004987477324903011 max memory_allocated 22901.42529296875 
[2025-01-17 15:58:34 root] (abq_llm.py 328): INFO layer 9 iter 10 loss:0.5071590542793274 norm:0.004502024035900831 max memory_allocated 22901.42529296875 
[2025-01-17 15:59:05 root] (abq_llm.py 328): INFO layer 9 iter 11 loss:0.5047175884246826 norm:0.004455818794667721 max memory_allocated 22901.42529296875 
[2025-01-17 15:59:36 root] (abq_llm.py 328): INFO layer 9 iter 12 loss:0.5024359822273254 norm:0.004249251447618008 max memory_allocated 22901.42529296875 
[2025-01-17 16:00:07 root] (abq_llm.py 328): INFO layer 9 iter 13 loss:0.5005796551704407 norm:0.004103060346096754 max memory_allocated 22901.42529296875 
[2025-01-17 16:00:38 root] (abq_llm.py 328): INFO layer 9 iter 14 loss:0.49912548065185547 norm:0.003986735362559557 max memory_allocated 22901.42529296875 
[2025-01-17 16:01:09 root] (abq_llm.py 328): INFO layer 9 iter 15 loss:0.49780887365341187 norm:0.0039583900943398476 max memory_allocated 22901.42529296875 
[2025-01-17 16:01:40 root] (abq_llm.py 328): INFO layer 9 iter 16 loss:0.4969269931316376 norm:0.003957759123295546 max memory_allocated 22901.42529296875 
[2025-01-17 16:02:11 root] (abq_llm.py 328): INFO layer 9 iter 17 loss:0.4958207905292511 norm:0.0038252505473792553 max memory_allocated 22901.42529296875 
[2025-01-17 16:02:42 root] (abq_llm.py 328): INFO layer 9 iter 18 loss:0.49478641152381897 norm:0.0037273899652063847 max memory_allocated 22901.42529296875 
[2025-01-17 16:03:13 root] (abq_llm.py 328): INFO layer 9 iter 19 loss:0.4939819872379303 norm:0.003755207173526287 max memory_allocated 22901.42529296875 
[2025-01-17 16:03:22 root] (abq_llm.py 212): INFO === Start quantize layer 10 ===
[2025-01-17 16:03:59 root] (abq_llm.py 328): INFO layer 10 iter 0 loss:0.7638086080551147 norm:0.07738365232944489 max memory_allocated 22903.09716796875 
[2025-01-17 16:04:30 root] (abq_llm.py 328): INFO layer 10 iter 1 loss:0.6800438165664673 norm:0.027292488142848015 max memory_allocated 22903.09716796875 
[2025-01-17 16:05:01 root] (abq_llm.py 328): INFO layer 10 iter 2 loss:0.6307346820831299 norm:0.016132276505231857 max memory_allocated 22903.09716796875 
[2025-01-17 16:05:32 root] (abq_llm.py 328): INFO layer 10 iter 3 loss:0.5937788486480713 norm:0.011354155838489532 max memory_allocated 22903.09716796875 
[2025-01-17 16:06:03 root] (abq_llm.py 328): INFO layer 10 iter 4 loss:0.5676337480545044 norm:0.008858706802129745 max memory_allocated 22903.09716796875 
[2025-01-17 16:06:34 root] (abq_llm.py 328): INFO layer 10 iter 5 loss:0.5512502789497375 norm:0.00712902145460248 max memory_allocated 22903.09716796875 
[2025-01-17 16:07:05 root] (abq_llm.py 328): INFO layer 10 iter 6 loss:0.5415400266647339 norm:0.006209968589246273 max memory_allocated 22903.09716796875 
[2025-01-17 16:07:36 root] (abq_llm.py 328): INFO layer 10 iter 7 loss:0.5344211459159851 norm:0.005752543453127146 max memory_allocated 22903.09716796875 
[2025-01-17 16:08:07 root] (abq_llm.py 328): INFO layer 10 iter 8 loss:0.5267623662948608 norm:0.004357721656560898 max memory_allocated 22903.09716796875 
[2025-01-17 16:08:38 root] (abq_llm.py 328): INFO layer 10 iter 9 loss:0.5218236446380615 norm:0.004051270894706249 max memory_allocated 22903.09716796875 
[2025-01-17 16:09:09 root] (abq_llm.py 328): INFO layer 10 iter 10 loss:0.5186066627502441 norm:0.0038924843538552523 max memory_allocated 22903.09716796875 
[2025-01-17 16:09:40 root] (abq_llm.py 328): INFO layer 10 iter 11 loss:0.5157687664031982 norm:0.0037424443289637566 max memory_allocated 22903.09716796875 
[2025-01-17 16:10:11 root] (abq_llm.py 328): INFO layer 10 iter 12 loss:0.5134199261665344 norm:0.003731445176526904 max memory_allocated 22903.09716796875 
[2025-01-17 16:10:42 root] (abq_llm.py 328): INFO layer 10 iter 13 loss:0.5115236043930054 norm:0.003617066191509366 max memory_allocated 22903.09716796875 
[2025-01-17 16:11:14 root] (abq_llm.py 328): INFO layer 10 iter 14 loss:0.5099619626998901 norm:0.0035443317610770464 max memory_allocated 22903.09716796875 
[2025-01-17 16:11:45 root] (abq_llm.py 328): INFO layer 10 iter 15 loss:0.5088414549827576 norm:0.0034666634164750576 max memory_allocated 22903.09716796875 
[2025-01-17 16:12:16 root] (abq_llm.py 328): INFO layer 10 iter 16 loss:0.5079667568206787 norm:0.0034495098516345024 max memory_allocated 22903.09716796875 
[2025-01-17 16:12:47 root] (abq_llm.py 328): INFO layer 10 iter 17 loss:0.5071854591369629 norm:0.0034146441612392664 max memory_allocated 22903.09716796875 
[2025-01-17 16:13:18 root] (abq_llm.py 328): INFO layer 10 iter 18 loss:0.5063300132751465 norm:0.0033840942196547985 max memory_allocated 22903.09716796875 
[2025-01-17 16:13:49 root] (abq_llm.py 328): INFO layer 10 iter 19 loss:0.5055255889892578 norm:0.0033314600586891174 max memory_allocated 22903.09716796875 
[2025-01-17 16:13:58 root] (abq_llm.py 212): INFO === Start quantize layer 11 ===
[2025-01-17 16:14:40 root] (abq_llm.py 328): INFO layer 11 iter 0 loss:0.7907429933547974 norm:0.06255167722702026 max memory_allocated 22904.76904296875 
[2025-01-17 16:15:11 root] (abq_llm.py 328): INFO layer 11 iter 1 loss:0.6834351420402527 norm:0.032067518681287766 max memory_allocated 22904.76904296875 
[2025-01-17 16:15:42 root] (abq_llm.py 328): INFO layer 11 iter 2 loss:0.6237689256668091 norm:0.019582046195864677 max memory_allocated 22904.76904296875 
[2025-01-17 16:16:13 root] (abq_llm.py 328): INFO layer 11 iter 3 loss:0.58680260181427 norm:0.013416728936135769 max memory_allocated 22904.76904296875 
[2025-01-17 16:16:44 root] (abq_llm.py 328): INFO layer 11 iter 4 loss:0.560509979724884 norm:0.009762275032699108 max memory_allocated 22904.76904296875 
[2025-01-17 16:17:15 root] (abq_llm.py 328): INFO layer 11 iter 5 loss:0.5456233024597168 norm:0.007368797902017832 max memory_allocated 22904.76904296875 
[2025-01-17 16:17:46 root] (abq_llm.py 328): INFO layer 11 iter 6 loss:0.5363906621932983 norm:0.0060196141712367535 max memory_allocated 22904.76904296875 
[2025-01-17 16:18:18 root] (abq_llm.py 328): INFO layer 11 iter 7 loss:0.5298994779586792 norm:0.005328376777470112 max memory_allocated 22904.76904296875 
[2025-01-17 16:18:49 root] (abq_llm.py 328): INFO layer 11 iter 8 loss:0.5253289937973022 norm:0.00474643474444747 max memory_allocated 22904.76904296875 
[2025-01-17 16:19:20 root] (abq_llm.py 328): INFO layer 11 iter 9 loss:0.521949052810669 norm:0.004378272220492363 max memory_allocated 22904.76904296875 
[2025-01-17 16:19:51 root] (abq_llm.py 328): INFO layer 11 iter 10 loss:0.5194679498672485 norm:0.004267007578164339 max memory_allocated 22904.76904296875 
[2025-01-17 16:20:22 root] (abq_llm.py 328): INFO layer 11 iter 11 loss:0.5169854164123535 norm:0.004128275439143181 max memory_allocated 22904.76904296875 
[2025-01-17 16:20:53 root] (abq_llm.py 328): INFO layer 11 iter 12 loss:0.515313982963562 norm:0.003960879519581795 max memory_allocated 22904.76904296875 
[2025-01-17 16:21:24 root] (abq_llm.py 328): INFO layer 11 iter 13 loss:0.5137965083122253 norm:0.0038439666386693716 max memory_allocated 22904.76904296875 
[2025-01-17 16:21:55 root] (abq_llm.py 328): INFO layer 11 iter 14 loss:0.5125272274017334 norm:0.003701696638017893 max memory_allocated 22904.76904296875 
[2025-01-17 16:22:26 root] (abq_llm.py 328): INFO layer 11 iter 15 loss:0.5116246938705444 norm:0.0036709001287817955 max memory_allocated 22904.76904296875 
[2025-01-17 16:22:57 root] (abq_llm.py 328): INFO layer 11 iter 16 loss:0.510555624961853 norm:0.0034023174084722996 max memory_allocated 22904.76904296875 
[2025-01-17 16:23:29 root] (abq_llm.py 328): INFO layer 11 iter 17 loss:0.5096818804740906 norm:0.003305180463939905 max memory_allocated 22904.76904296875 
[2025-01-17 16:24:00 root] (abq_llm.py 328): INFO layer 11 iter 18 loss:0.5089969038963318 norm:0.0032456035260111094 max memory_allocated 22904.76904296875 
[2025-01-17 16:24:31 root] (abq_llm.py 328): INFO layer 11 iter 19 loss:0.5082309246063232 norm:0.00319785438477993 max memory_allocated 22904.76904296875 
[2025-01-17 16:24:40 root] (abq_llm.py 212): INFO === Start quantize layer 12 ===
[2025-01-17 16:25:22 root] (abq_llm.py 328): INFO layer 12 iter 0 loss:0.7704225182533264 norm:0.0404873788356781 max memory_allocated 22906.44091796875 
[2025-01-17 16:25:53 root] (abq_llm.py 328): INFO layer 12 iter 1 loss:0.673314094543457 norm:0.018092341721057892 max memory_allocated 22906.44091796875 
[2025-01-17 16:26:24 root] (abq_llm.py 328): INFO layer 12 iter 2 loss:0.6254968047142029 norm:0.011762303300201893 max memory_allocated 22906.44091796875 
[2025-01-17 16:26:55 root] (abq_llm.py 328): INFO layer 12 iter 3 loss:0.5937219858169556 norm:0.008367033675312996 max memory_allocated 22906.44091796875 
[2025-01-17 16:27:26 root] (abq_llm.py 328): INFO layer 12 iter 4 loss:0.5691539645195007 norm:0.006313936319202185 max memory_allocated 22906.44091796875 
[2025-01-17 16:27:57 root] (abq_llm.py 328): INFO layer 12 iter 5 loss:0.5548099279403687 norm:0.004904480651021004 max memory_allocated 22906.44091796875 
[2025-01-17 16:28:28 root] (abq_llm.py 328): INFO layer 12 iter 6 loss:0.5462257266044617 norm:0.004198918119072914 max memory_allocated 22906.44091796875 
[2025-01-17 16:28:59 root] (abq_llm.py 328): INFO layer 12 iter 7 loss:0.5407124161720276 norm:0.003789223963394761 max memory_allocated 22906.44091796875 
[2025-01-17 16:29:30 root] (abq_llm.py 328): INFO layer 12 iter 8 loss:0.5370288491249084 norm:0.0034870842937380075 max memory_allocated 22906.44091796875 
[2025-01-17 16:30:01 root] (abq_llm.py 328): INFO layer 12 iter 9 loss:0.5341394543647766 norm:0.003297789255157113 max memory_allocated 22906.44091796875 
[2025-01-17 16:30:32 root] (abq_llm.py 328): INFO layer 12 iter 10 loss:0.531762421131134 norm:0.003158081090077758 max memory_allocated 22906.44091796875 
[2025-01-17 16:31:03 root] (abq_llm.py 328): INFO layer 12 iter 11 loss:0.5298013687133789 norm:0.0030832148622721434 max memory_allocated 22906.44091796875 
[2025-01-17 16:31:35 root] (abq_llm.py 328): INFO layer 12 iter 12 loss:0.5284538865089417 norm:0.0030427391175180674 max memory_allocated 22906.44091796875 
[2025-01-17 16:32:06 root] (abq_llm.py 328): INFO layer 12 iter 13 loss:0.527125895023346 norm:0.0030851552728563547 max memory_allocated 22906.44091796875 
[2025-01-17 16:32:37 root] (abq_llm.py 328): INFO layer 12 iter 14 loss:0.5260741710662842 norm:0.00301389885134995 max memory_allocated 22906.44091796875 
[2025-01-17 16:33:08 root] (abq_llm.py 328): INFO layer 12 iter 15 loss:0.5249995589256287 norm:0.002909203991293907 max memory_allocated 22906.44091796875 
[2025-01-17 16:33:39 root] (abq_llm.py 328): INFO layer 12 iter 16 loss:0.5241194367408752 norm:0.0027432958595454693 max memory_allocated 22906.44091796875 
[2025-01-17 16:34:10 root] (abq_llm.py 328): INFO layer 12 iter 17 loss:0.5233974456787109 norm:0.002721151104196906 max memory_allocated 22906.44091796875 
[2025-01-17 16:34:41 root] (abq_llm.py 328): INFO layer 12 iter 18 loss:0.5227165222167969 norm:0.0026874791365116835 max memory_allocated 22906.44091796875 
[2025-01-17 16:35:12 root] (abq_llm.py 328): INFO layer 12 iter 19 loss:0.5222495198249817 norm:0.0025935762096196413 max memory_allocated 22906.44091796875 
[2025-01-17 16:35:21 root] (abq_llm.py 212): INFO === Start quantize layer 13 ===
[2025-01-17 16:36:00 root] (abq_llm.py 328): INFO layer 13 iter 0 loss:0.7862226366996765 norm:0.09996906667947769 max memory_allocated 22908.11279296875 
[2025-01-17 16:36:31 root] (abq_llm.py 328): INFO layer 13 iter 1 loss:0.7032421827316284 norm:0.05126301944255829 max memory_allocated 22908.11279296875 
[2025-01-17 16:37:02 root] (abq_llm.py 328): INFO layer 13 iter 2 loss:0.6403544545173645 norm:0.024186251685023308 max memory_allocated 22908.11279296875 
[2025-01-17 16:37:33 root] (abq_llm.py 328): INFO layer 13 iter 3 loss:0.6034079194068909 norm:0.015004637651145458 max memory_allocated 22908.11279296875 
[2025-01-17 16:38:04 root] (abq_llm.py 328): INFO layer 13 iter 4 loss:0.5760784149169922 norm:0.010569553822278976 max memory_allocated 22908.11279296875 
[2025-01-17 16:38:35 root] (abq_llm.py 328): INFO layer 13 iter 5 loss:0.5595430135726929 norm:0.008379378356039524 max memory_allocated 22908.11279296875 
[2025-01-17 16:39:06 root] (abq_llm.py 328): INFO layer 13 iter 6 loss:0.5490149855613708 norm:0.006872397847473621 max memory_allocated 22908.11279296875 
[2025-01-17 16:39:37 root] (abq_llm.py 328): INFO layer 13 iter 7 loss:0.5419031977653503 norm:0.005621463526040316 max memory_allocated 22908.11279296875 
[2025-01-17 16:40:08 root] (abq_llm.py 328): INFO layer 13 iter 8 loss:0.5366734266281128 norm:0.005173671990633011 max memory_allocated 22908.11279296875 
[2025-01-17 16:40:40 root] (abq_llm.py 328): INFO layer 13 iter 9 loss:0.5323176383972168 norm:0.004597688093781471 max memory_allocated 22908.11279296875 
[2025-01-17 16:41:11 root] (abq_llm.py 328): INFO layer 13 iter 10 loss:0.5290106534957886 norm:0.004128172527998686 max memory_allocated 22908.11279296875 
[2025-01-17 16:41:42 root] (abq_llm.py 328): INFO layer 13 iter 11 loss:0.5265275239944458 norm:0.003889016341418028 max memory_allocated 22908.11279296875 
[2025-01-17 16:42:13 root] (abq_llm.py 328): INFO layer 13 iter 12 loss:0.5246350169181824 norm:0.0037867226637899876 max memory_allocated 22908.11279296875 
[2025-01-17 16:42:44 root] (abq_llm.py 328): INFO layer 13 iter 13 loss:0.5228291749954224 norm:0.003590910229831934 max memory_allocated 22908.11279296875 
[2025-01-17 16:43:15 root] (abq_llm.py 328): INFO layer 13 iter 14 loss:0.5215566158294678 norm:0.003572497982531786 max memory_allocated 22908.11279296875 
[2025-01-17 16:43:46 root] (abq_llm.py 328): INFO layer 13 iter 15 loss:0.5204352736473083 norm:0.003478812985122204 max memory_allocated 22908.11279296875 
[2025-01-17 16:44:17 root] (abq_llm.py 328): INFO layer 13 iter 16 loss:0.5192632079124451 norm:0.003341920906677842 max memory_allocated 22908.11279296875 
[2025-01-17 16:44:48 root] (abq_llm.py 328): INFO layer 13 iter 17 loss:0.518196702003479 norm:0.003297247923910618 max memory_allocated 22908.11279296875 
[2025-01-17 16:45:19 root] (abq_llm.py 328): INFO layer 13 iter 18 loss:0.5173085331916809 norm:0.0032522506080567837 max memory_allocated 22908.11279296875 
[2025-01-17 16:45:50 root] (abq_llm.py 328): INFO layer 13 iter 19 loss:0.5165688991546631 norm:0.0032524066045880318 max memory_allocated 22908.11279296875 
[2025-01-17 16:45:59 root] (abq_llm.py 212): INFO === Start quantize layer 14 ===
[2025-01-17 16:46:36 root] (abq_llm.py 328): INFO layer 14 iter 0 loss:0.7287155985832214 norm:0.03912462666630745 max memory_allocated 22909.78466796875 
[2025-01-17 16:47:07 root] (abq_llm.py 328): INFO layer 14 iter 1 loss:0.6597414612770081 norm:0.021381037309765816 max memory_allocated 22909.78466796875 
[2025-01-17 16:47:38 root] (abq_llm.py 328): INFO layer 14 iter 2 loss:0.6147591471672058 norm:0.012559179216623306 max memory_allocated 22909.78466796875 
[2025-01-17 16:48:09 root] (abq_llm.py 328): INFO layer 14 iter 3 loss:0.583513081073761 norm:0.00840398296713829 max memory_allocated 22909.78466796875 
[2025-01-17 16:48:40 root] (abq_llm.py 328): INFO layer 14 iter 4 loss:0.5612668395042419 norm:0.0055071208626031876 max memory_allocated 22909.78466796875 
[2025-01-17 16:49:11 root] (abq_llm.py 328): INFO layer 14 iter 5 loss:0.5488924980163574 norm:0.004008630756288767 max memory_allocated 22909.78466796875 
[2025-01-17 16:49:42 root] (abq_llm.py 328): INFO layer 14 iter 6 loss:0.5418093800544739 norm:0.0033746659755706787 max memory_allocated 22909.78466796875 
[2025-01-17 16:50:13 root] (abq_llm.py 328): INFO layer 14 iter 7 loss:0.5371509194374084 norm:0.0030354587361216545 max memory_allocated 22909.78466796875 
[2025-01-17 16:50:44 root] (abq_llm.py 328): INFO layer 14 iter 8 loss:0.5336939096450806 norm:0.0029444110114127398 max memory_allocated 22909.78466796875 
[2025-01-17 16:51:15 root] (abq_llm.py 328): INFO layer 14 iter 9 loss:0.5309380888938904 norm:0.0028056837618350983 max memory_allocated 22909.78466796875 
[2025-01-17 16:51:46 root] (abq_llm.py 328): INFO layer 14 iter 10 loss:0.5288621187210083 norm:0.0027135293930768967 max memory_allocated 22909.78466796875 
[2025-01-17 16:52:17 root] (abq_llm.py 328): INFO layer 14 iter 11 loss:0.5272305011749268 norm:0.0026525910943746567 max memory_allocated 22909.78466796875 
[2025-01-17 16:52:48 root] (abq_llm.py 328): INFO layer 14 iter 12 loss:0.5257816910743713 norm:0.0025455027353018522 max memory_allocated 22909.78466796875 
[2025-01-17 16:53:19 root] (abq_llm.py 328): INFO layer 14 iter 13 loss:0.5245782136917114 norm:0.002471291460096836 max memory_allocated 22909.78466796875 
[2025-01-17 16:53:50 root] (abq_llm.py 328): INFO layer 14 iter 14 loss:0.5234052538871765 norm:0.0023644836619496346 max memory_allocated 22909.78466796875 
[2025-01-17 16:54:22 root] (abq_llm.py 328): INFO layer 14 iter 15 loss:0.5225691795349121 norm:0.0023873772006481886 max memory_allocated 22909.78466796875 
[2025-01-17 16:54:53 root] (abq_llm.py 328): INFO layer 14 iter 16 loss:0.5218685865402222 norm:0.002362614031881094 max memory_allocated 22909.78466796875 
[2025-01-17 16:55:24 root] (abq_llm.py 328): INFO layer 14 iter 17 loss:0.5212472081184387 norm:0.0023546849843114614 max memory_allocated 22909.78466796875 
[2025-01-17 16:55:55 root] (abq_llm.py 328): INFO layer 14 iter 18 loss:0.52074134349823 norm:0.002362922765314579 max memory_allocated 22909.78466796875 
[2025-01-17 16:56:26 root] (abq_llm.py 328): INFO layer 14 iter 19 loss:0.5204129219055176 norm:0.002358704339712858 max memory_allocated 22909.78466796875 
[2025-01-17 16:56:35 root] (abq_llm.py 212): INFO === Start quantize layer 15 ===
[2025-01-17 16:57:14 root] (abq_llm.py 328): INFO layer 15 iter 0 loss:0.7774556875228882 norm:0.07402442395687103 max memory_allocated 22911.45654296875 
[2025-01-17 16:57:45 root] (abq_llm.py 328): INFO layer 15 iter 1 loss:0.6784799098968506 norm:0.03906452655792236 max memory_allocated 22911.45654296875 
[2025-01-17 16:58:16 root] (abq_llm.py 328): INFO layer 15 iter 2 loss:0.6207026839256287 norm:0.026473406702280045 max memory_allocated 22911.45654296875 
[2025-01-17 16:58:47 root] (abq_llm.py 328): INFO layer 15 iter 3 loss:0.5836654305458069 norm:0.017965536564588547 max memory_allocated 22911.45654296875 
[2025-01-17 16:59:18 root] (abq_llm.py 328): INFO layer 15 iter 4 loss:0.5569326877593994 norm:0.010255626402795315 max memory_allocated 22911.45654296875 
[2025-01-17 16:59:49 root] (abq_llm.py 328): INFO layer 15 iter 5 loss:0.5446416139602661 norm:0.007812968455255032 max memory_allocated 22911.45654296875 
[2025-01-17 17:00:20 root] (abq_llm.py 328): INFO layer 15 iter 6 loss:0.5369192957878113 norm:0.006167527753859758 max memory_allocated 22911.45654296875 
[2025-01-17 17:00:51 root] (abq_llm.py 328): INFO layer 15 iter 7 loss:0.5323441624641418 norm:0.005731251090764999 max memory_allocated 22911.45654296875 
[2025-01-17 17:01:22 root] (abq_llm.py 328): INFO layer 15 iter 8 loss:0.5282890200614929 norm:0.004961969330906868 max memory_allocated 22911.45654296875 
[2025-01-17 17:01:53 root] (abq_llm.py 328): INFO layer 15 iter 9 loss:0.5250308513641357 norm:0.00416503706946969 max memory_allocated 22911.45654296875 
[2025-01-17 17:02:24 root] (abq_llm.py 328): INFO layer 15 iter 10 loss:0.5219337940216064 norm:0.003458583727478981 max memory_allocated 22911.45654296875 
[2025-01-17 17:02:55 root] (abq_llm.py 328): INFO layer 15 iter 11 loss:0.5195803046226501 norm:0.003440957749262452 max memory_allocated 22911.45654296875 
[2025-01-17 17:03:26 root] (abq_llm.py 328): INFO layer 15 iter 12 loss:0.517566442489624 norm:0.003267099382355809 max memory_allocated 22911.45654296875 
[2025-01-17 17:03:57 root] (abq_llm.py 328): INFO layer 15 iter 13 loss:0.5162240266799927 norm:0.0032760612666606903 max memory_allocated 22911.45654296875 
[2025-01-17 17:04:28 root] (abq_llm.py 328): INFO layer 15 iter 14 loss:0.5150781869888306 norm:0.0032097294460982084 max memory_allocated 22911.45654296875 
[2025-01-17 17:04:59 root] (abq_llm.py 328): INFO layer 15 iter 15 loss:0.5140323042869568 norm:0.003173351753503084 max memory_allocated 22911.45654296875 
[2025-01-17 17:05:30 root] (abq_llm.py 328): INFO layer 15 iter 16 loss:0.5133083462715149 norm:0.003052108222618699 max memory_allocated 22911.45654296875 
[2025-01-17 17:06:01 root] (abq_llm.py 328): INFO layer 15 iter 17 loss:0.5125901699066162 norm:0.0029694652184844017 max memory_allocated 22911.45654296875 
[2025-01-17 17:06:32 root] (abq_llm.py 328): INFO layer 15 iter 18 loss:0.5120490193367004 norm:0.0029431022703647614 max memory_allocated 22911.45654296875 
[2025-01-17 17:07:03 root] (abq_llm.py 328): INFO layer 15 iter 19 loss:0.5114316940307617 norm:0.002869861666113138 max memory_allocated 22911.45654296875 
[2025-01-17 17:07:12 root] (abq_llm.py 212): INFO === Start quantize layer 16 ===
[2025-01-17 17:07:48 root] (abq_llm.py 328): INFO layer 16 iter 0 loss:0.730510413646698 norm:0.070978544652462 max memory_allocated 22913.12841796875 
[2025-01-17 17:08:19 root] (abq_llm.py 328): INFO layer 16 iter 1 loss:0.6617386937141418 norm:0.03731533885002136 max memory_allocated 22913.12841796875 
[2025-01-17 17:08:50 root] (abq_llm.py 328): INFO layer 16 iter 2 loss:0.6207027435302734 norm:0.023256761953234673 max memory_allocated 22913.12841796875 
[2025-01-17 17:09:21 root] (abq_llm.py 328): INFO layer 16 iter 3 loss:0.5843002796173096 norm:0.014323742128908634 max memory_allocated 22913.12841796875 
[2025-01-17 17:09:52 root] (abq_llm.py 328): INFO layer 16 iter 4 loss:0.5566185116767883 norm:0.008596998639404774 max memory_allocated 22913.12841796875 
[2025-01-17 17:10:23 root] (abq_llm.py 328): INFO layer 16 iter 5 loss:0.5412092208862305 norm:0.005774691700935364 max memory_allocated 22913.12841796875 
[2025-01-17 17:10:54 root] (abq_llm.py 328): INFO layer 16 iter 6 loss:0.5332913398742676 norm:0.004703776445239782 max memory_allocated 22913.12841796875 
[2025-01-17 17:11:25 root] (abq_llm.py 328): INFO layer 16 iter 7 loss:0.5278111100196838 norm:0.004109946079552174 max memory_allocated 22913.12841796875 
[2025-01-17 17:11:56 root] (abq_llm.py 328): INFO layer 16 iter 8 loss:0.5245779752731323 norm:0.0038911295123398304 max memory_allocated 22913.12841796875 
[2025-01-17 17:12:27 root] (abq_llm.py 328): INFO layer 16 iter 9 loss:0.5213756561279297 norm:0.003501678118482232 max memory_allocated 22913.12841796875 
[2025-01-17 17:12:58 root] (abq_llm.py 328): INFO layer 16 iter 10 loss:0.5186434984207153 norm:0.0032672048546373844 max memory_allocated 22913.12841796875 
[2025-01-17 17:13:29 root] (abq_llm.py 328): INFO layer 16 iter 11 loss:0.5167714357376099 norm:0.003113636514171958 max memory_allocated 22913.12841796875 
[2025-01-17 17:14:00 root] (abq_llm.py 328): INFO layer 16 iter 12 loss:0.5150347948074341 norm:0.002991416724398732 max memory_allocated 22913.12841796875 
[2025-01-17 17:14:31 root] (abq_llm.py 328): INFO layer 16 iter 13 loss:0.5136563777923584 norm:0.002925875596702099 max memory_allocated 22913.12841796875 
[2025-01-17 17:15:02 root] (abq_llm.py 328): INFO layer 16 iter 14 loss:0.5122637152671814 norm:0.0027762798126786947 max memory_allocated 22913.12841796875 
[2025-01-17 17:15:33 root] (abq_llm.py 328): INFO layer 16 iter 15 loss:0.5112541317939758 norm:0.002727370709180832 max memory_allocated 22913.12841796875 
[2025-01-17 17:16:04 root] (abq_llm.py 328): INFO layer 16 iter 16 loss:0.5102080702781677 norm:0.0026050389278680086 max memory_allocated 22913.12841796875 
[2025-01-17 17:16:35 root] (abq_llm.py 328): INFO layer 16 iter 17 loss:0.5094780921936035 norm:0.0025157351046800613 max memory_allocated 22913.12841796875 
[2025-01-17 17:17:06 root] (abq_llm.py 328): INFO layer 16 iter 18 loss:0.5087849497795105 norm:0.00246645649895072 max memory_allocated 22913.12841796875 
[2025-01-17 17:17:37 root] (abq_llm.py 328): INFO layer 16 iter 19 loss:0.5081860423088074 norm:0.0024193869903683662 max memory_allocated 22913.12841796875 
[2025-01-17 17:17:46 root] (abq_llm.py 212): INFO === Start quantize layer 17 ===
[2025-01-17 17:18:24 root] (abq_llm.py 328): INFO layer 17 iter 0 loss:0.6824842691421509 norm:0.05970492586493492 max memory_allocated 22914.80029296875 
[2025-01-17 17:18:55 root] (abq_llm.py 328): INFO layer 17 iter 1 loss:0.6273543834686279 norm:0.02947816625237465 max memory_allocated 22914.80029296875 
[2025-01-17 17:19:26 root] (abq_llm.py 328): INFO layer 17 iter 2 loss:0.5961005091667175 norm:0.018705114722251892 max memory_allocated 22914.80029296875 
[2025-01-17 17:19:57 root] (abq_llm.py 328): INFO layer 17 iter 3 loss:0.5696204900741577 norm:0.011455515399575233 max memory_allocated 22914.80029296875 
[2025-01-17 17:20:27 root] (abq_llm.py 328): INFO layer 17 iter 4 loss:0.5493472814559937 norm:0.006087807938456535 max memory_allocated 22914.80029296875 
[2025-01-17 17:20:58 root] (abq_llm.py 328): INFO layer 17 iter 5 loss:0.540564775466919 norm:0.0045994240790605545 max memory_allocated 22914.80029296875 
[2025-01-17 17:21:29 root] (abq_llm.py 328): INFO layer 17 iter 6 loss:0.536016583442688 norm:0.0037793144583702087 max memory_allocated 22914.80029296875 
[2025-01-17 17:22:00 root] (abq_llm.py 328): INFO layer 17 iter 7 loss:0.5328904390335083 norm:0.0033503081649541855 max memory_allocated 22914.80029296875 
[2025-01-17 17:22:32 root] (abq_llm.py 328): INFO layer 17 iter 8 loss:0.5303587913513184 norm:0.0031188279390335083 max memory_allocated 22914.80029296875 
[2025-01-17 17:23:03 root] (abq_llm.py 328): INFO layer 17 iter 9 loss:0.5280094146728516 norm:0.0029022726230323315 max memory_allocated 22914.80029296875 
[2025-01-17 17:23:34 root] (abq_llm.py 328): INFO layer 17 iter 10 loss:0.5261294841766357 norm:0.00273739080876112 max memory_allocated 22914.80029296875 
[2025-01-17 17:24:05 root] (abq_llm.py 328): INFO layer 17 iter 11 loss:0.5248166918754578 norm:0.0026555247604846954 max memory_allocated 22914.80029296875 
[2025-01-17 17:24:36 root] (abq_llm.py 328): INFO layer 17 iter 12 loss:0.5235894918441772 norm:0.002611653646454215 max memory_allocated 22914.80029296875 
[2025-01-17 17:25:07 root] (abq_llm.py 328): INFO layer 17 iter 13 loss:0.5227450728416443 norm:0.002632805611938238 max memory_allocated 22914.80029296875 
[2025-01-17 17:25:38 root] (abq_llm.py 328): INFO layer 17 iter 14 loss:0.521699845790863 norm:0.0025267372839152813 max memory_allocated 22914.80029296875 
[2025-01-17 17:26:09 root] (abq_llm.py 328): INFO layer 17 iter 15 loss:0.5209020376205444 norm:0.0023606522008776665 max memory_allocated 22914.80029296875 
[2025-01-17 17:26:40 root] (abq_llm.py 328): INFO layer 17 iter 16 loss:0.5202578902244568 norm:0.002331939758732915 max memory_allocated 22914.80029296875 
[2025-01-17 17:27:11 root] (abq_llm.py 328): INFO layer 17 iter 17 loss:0.51963210105896 norm:0.002286751288920641 max memory_allocated 22914.80029296875 
[2025-01-17 17:27:42 root] (abq_llm.py 328): INFO layer 17 iter 18 loss:0.5190156102180481 norm:0.002230414655059576 max memory_allocated 22914.80029296875 
[2025-01-17 17:28:13 root] (abq_llm.py 328): INFO layer 17 iter 19 loss:0.5185547471046448 norm:0.0022424026392400265 max memory_allocated 22914.80029296875 
[2025-01-17 17:28:22 root] (abq_llm.py 212): INFO === Start quantize layer 18 ===
[2025-01-17 17:28:59 root] (abq_llm.py 328): INFO layer 18 iter 0 loss:0.7185067534446716 norm:0.09109240770339966 max memory_allocated 22916.47216796875 
[2025-01-17 17:29:30 root] (abq_llm.py 328): INFO layer 18 iter 1 loss:0.6583642363548279 norm:0.037981417030096054 max memory_allocated 22916.47216796875 
[2025-01-17 17:30:01 root] (abq_llm.py 328): INFO layer 18 iter 2 loss:0.6237667798995972 norm:0.022091036662459373 max memory_allocated 22916.47216796875 
[2025-01-17 17:30:32 root] (abq_llm.py 328): INFO layer 18 iter 3 loss:0.5988772511482239 norm:0.014865057542920113 max memory_allocated 22916.47216796875 
[2025-01-17 17:31:03 root] (abq_llm.py 328): INFO layer 18 iter 4 loss:0.5801091194152832 norm:0.009461015462875366 max memory_allocated 22916.47216796875 
[2025-01-17 17:31:34 root] (abq_llm.py 328): INFO layer 18 iter 5 loss:0.5695568919181824 norm:0.00702615175396204 max memory_allocated 22916.47216796875 
[2025-01-17 17:32:05 root] (abq_llm.py 328): INFO layer 18 iter 6 loss:0.5633954405784607 norm:0.005710633937269449 max memory_allocated 22916.47216796875 
[2025-01-17 17:32:36 root] (abq_llm.py 328): INFO layer 18 iter 7 loss:0.559319019317627 norm:0.004379361402243376 max memory_allocated 22916.47216796875 
[2025-01-17 17:33:07 root] (abq_llm.py 328): INFO layer 18 iter 8 loss:0.5564336180686951 norm:0.003944025840610266 max memory_allocated 22916.47216796875 
[2025-01-17 17:33:38 root] (abq_llm.py 328): INFO layer 18 iter 9 loss:0.5541378855705261 norm:0.0037673956248909235 max memory_allocated 22916.47216796875 
[2025-01-17 17:34:09 root] (abq_llm.py 328): INFO layer 18 iter 10 loss:0.5519638061523438 norm:0.0030437521636486053 max memory_allocated 22916.47216796875 
[2025-01-17 17:34:40 root] (abq_llm.py 328): INFO layer 18 iter 11 loss:0.5502243041992188 norm:0.0027334834448993206 max memory_allocated 22916.47216796875 
[2025-01-17 17:35:11 root] (abq_llm.py 328): INFO layer 18 iter 12 loss:0.5490682721138 norm:0.002722733421251178 max memory_allocated 22916.47216796875 
[2025-01-17 17:35:42 root] (abq_llm.py 328): INFO layer 18 iter 13 loss:0.5478581190109253 norm:0.0024730416480451822 max memory_allocated 22916.47216796875 
[2025-01-17 17:36:13 root] (abq_llm.py 328): INFO layer 18 iter 14 loss:0.5466623306274414 norm:0.002325474750250578 max memory_allocated 22916.47216796875 
[2025-01-17 17:36:44 root] (abq_llm.py 328): INFO layer 18 iter 15 loss:0.5456315875053406 norm:0.0021903247106820345 max memory_allocated 22916.47216796875 
[2025-01-17 17:37:15 root] (abq_llm.py 328): INFO layer 18 iter 16 loss:0.5448508858680725 norm:0.0021304034162312746 max memory_allocated 22916.47216796875 
[2025-01-17 17:37:46 root] (abq_llm.py 328): INFO layer 18 iter 17 loss:0.5441944003105164 norm:0.002109312918037176 max memory_allocated 22916.47216796875 
[2025-01-17 17:38:17 root] (abq_llm.py 328): INFO layer 18 iter 18 loss:0.543518602848053 norm:0.0020624976605176926 max memory_allocated 22916.47216796875 
[2025-01-17 17:38:48 root] (abq_llm.py 328): INFO layer 18 iter 19 loss:0.5428776741027832 norm:0.002068926114588976 max memory_allocated 22916.47216796875 
[2025-01-17 17:38:57 root] (abq_llm.py 212): INFO === Start quantize layer 19 ===
[2025-01-17 17:39:35 root] (abq_llm.py 328): INFO layer 19 iter 0 loss:0.7248059511184692 norm:0.09207207709550858 max memory_allocated 22918.14404296875 
[2025-01-17 17:40:06 root] (abq_llm.py 328): INFO layer 19 iter 1 loss:0.6766830086708069 norm:0.04270957410335541 max memory_allocated 22918.14404296875 
[2025-01-17 17:40:37 root] (abq_llm.py 328): INFO layer 19 iter 2 loss:0.6428244113922119 norm:0.022966567426919937 max memory_allocated 22918.14404296875 
[2025-01-17 17:41:08 root] (abq_llm.py 328): INFO layer 19 iter 3 loss:0.6211693286895752 norm:0.014705829322338104 max memory_allocated 22918.14404296875 
[2025-01-17 17:41:39 root] (abq_llm.py 328): INFO layer 19 iter 4 loss:0.6051548719406128 norm:0.008542080409824848 max memory_allocated 22918.14404296875 
[2025-01-17 17:42:10 root] (abq_llm.py 328): INFO layer 19 iter 5 loss:0.5966069102287292 norm:0.006081202067434788 max memory_allocated 22918.14404296875 
[2025-01-17 17:42:41 root] (abq_llm.py 328): INFO layer 19 iter 6 loss:0.5918905735015869 norm:0.004955286160111427 max memory_allocated 22918.14404296875 
[2025-01-17 17:43:12 root] (abq_llm.py 328): INFO layer 19 iter 7 loss:0.5886279344558716 norm:0.004262404516339302 max memory_allocated 22918.14404296875 
[2025-01-17 17:43:43 root] (abq_llm.py 328): INFO layer 19 iter 8 loss:0.5860300064086914 norm:0.003799253609031439 max memory_allocated 22918.14404296875 
[2025-01-17 17:44:14 root] (abq_llm.py 328): INFO layer 19 iter 9 loss:0.5838610529899597 norm:0.0033529563806951046 max memory_allocated 22918.14404296875 
[2025-01-17 17:44:45 root] (abq_llm.py 328): INFO layer 19 iter 10 loss:0.582105278968811 norm:0.003088522469624877 max memory_allocated 22918.14404296875 
[2025-01-17 17:45:16 root] (abq_llm.py 328): INFO layer 19 iter 11 loss:0.5806703567504883 norm:0.0028631286695599556 max memory_allocated 22918.14404296875 
[2025-01-17 17:45:47 root] (abq_llm.py 328): INFO layer 19 iter 12 loss:0.5794999599456787 norm:0.0026655131950974464 max memory_allocated 22918.14404296875 
[2025-01-17 17:46:18 root] (abq_llm.py 328): INFO layer 19 iter 13 loss:0.5785863399505615 norm:0.002531814156100154 max memory_allocated 22918.14404296875 
[2025-01-17 17:46:49 root] (abq_llm.py 328): INFO layer 19 iter 14 loss:0.577704131603241 norm:0.0023911921307444572 max memory_allocated 22918.14404296875 
[2025-01-17 17:47:21 root] (abq_llm.py 328): INFO layer 19 iter 15 loss:0.57694011926651 norm:0.0023208491038531065 max memory_allocated 22918.14404296875 
[2025-01-17 17:47:52 root] (abq_llm.py 328): INFO layer 19 iter 16 loss:0.5762531161308289 norm:0.0022483239881694317 max memory_allocated 22918.14404296875 
[2025-01-17 17:48:23 root] (abq_llm.py 328): INFO layer 19 iter 17 loss:0.5756785869598389 norm:0.002129761502146721 max memory_allocated 22918.14404296875 
[2025-01-17 17:48:54 root] (abq_llm.py 328): INFO layer 19 iter 18 loss:0.5750623941421509 norm:0.002049556467682123 max memory_allocated 22918.14404296875 
[2025-01-17 17:49:25 root] (abq_llm.py 328): INFO layer 19 iter 19 loss:0.5745225548744202 norm:0.001982663292437792 max memory_allocated 22918.14404296875 
[2025-01-17 17:49:34 root] (abq_llm.py 212): INFO === Start quantize layer 20 ===
[2025-01-17 17:50:10 root] (abq_llm.py 328): INFO layer 20 iter 0 loss:0.72585529088974 norm:0.03446874022483826 max memory_allocated 22919.81591796875 
[2025-01-17 17:50:41 root] (abq_llm.py 328): INFO layer 20 iter 1 loss:0.6923362016677856 norm:0.019029708579182625 max memory_allocated 22919.81591796875 
[2025-01-17 17:51:12 root] (abq_llm.py 328): INFO layer 20 iter 2 loss:0.6720334887504578 norm:0.012611120007932186 max memory_allocated 22919.81591796875 
[2025-01-17 17:51:43 root] (abq_llm.py 328): INFO layer 20 iter 3 loss:0.6537777185440063 norm:0.008086140267550945 max memory_allocated 22919.81591796875 
[2025-01-17 17:52:14 root] (abq_llm.py 328): INFO layer 20 iter 4 loss:0.6416634321212769 norm:0.0057347542606294155 max memory_allocated 22919.81591796875 
[2025-01-17 17:52:45 root] (abq_llm.py 328): INFO layer 20 iter 5 loss:0.6340566873550415 norm:0.004090145695954561 max memory_allocated 22919.81591796875 
[2025-01-17 17:53:16 root] (abq_llm.py 328): INFO layer 20 iter 6 loss:0.6297105550765991 norm:0.0032624751329421997 max memory_allocated 22919.81591796875 
[2025-01-17 17:53:47 root] (abq_llm.py 328): INFO layer 20 iter 7 loss:0.626293420791626 norm:0.002868212293833494 max memory_allocated 22919.81591796875 
[2025-01-17 17:54:18 root] (abq_llm.py 328): INFO layer 20 iter 8 loss:0.6237273216247559 norm:0.002739749848842621 max memory_allocated 22919.81591796875 
[2025-01-17 17:54:49 root] (abq_llm.py 328): INFO layer 20 iter 9 loss:0.6216211318969727 norm:0.0026362608186900616 max memory_allocated 22919.81591796875 
[2025-01-17 17:55:20 root] (abq_llm.py 328): INFO layer 20 iter 10 loss:0.6199101209640503 norm:0.0025352586526423693 max memory_allocated 22919.81591796875 
[2025-01-17 17:55:51 root] (abq_llm.py 328): INFO layer 20 iter 11 loss:0.6185119152069092 norm:0.0024925433099269867 max memory_allocated 22919.81591796875 
[2025-01-17 17:56:22 root] (abq_llm.py 328): INFO layer 20 iter 12 loss:0.6173456907272339 norm:0.002427395898848772 max memory_allocated 22919.81591796875 
[2025-01-17 17:56:53 root] (abq_llm.py 328): INFO layer 20 iter 13 loss:0.616385817527771 norm:0.0024154544807970524 max memory_allocated 22919.81591796875 
[2025-01-17 17:57:24 root] (abq_llm.py 328): INFO layer 20 iter 14 loss:0.6155506372451782 norm:0.0023855899926275015 max memory_allocated 22919.81591796875 
[2025-01-17 17:57:56 root] (abq_llm.py 328): INFO layer 20 iter 15 loss:0.6146925091743469 norm:0.0023354939185082912 max memory_allocated 22919.81591796875 
[2025-01-17 17:58:27 root] (abq_llm.py 328): INFO layer 20 iter 16 loss:0.6140109300613403 norm:0.002298528328537941 max memory_allocated 22919.81591796875 
[2025-01-17 17:58:58 root] (abq_llm.py 328): INFO layer 20 iter 17 loss:0.6134255528450012 norm:0.00229075038805604 max memory_allocated 22919.81591796875 
[2025-01-17 17:59:29 root] (abq_llm.py 328): INFO layer 20 iter 18 loss:0.6128650307655334 norm:0.002252790378406644 max memory_allocated 22919.81591796875 
[2025-01-17 18:00:00 root] (abq_llm.py 328): INFO layer 20 iter 19 loss:0.6123759746551514 norm:0.00224364479072392 max memory_allocated 22919.81591796875 
[2025-01-17 18:00:09 root] (abq_llm.py 212): INFO === Start quantize layer 21 ===
[2025-01-17 18:00:46 root] (abq_llm.py 328): INFO layer 21 iter 0 loss:0.7424756288528442 norm:0.032456740736961365 max memory_allocated 22921.48779296875 
[2025-01-17 18:01:17 root] (abq_llm.py 328): INFO layer 21 iter 1 loss:0.7180553674697876 norm:0.016065426170825958 max memory_allocated 22921.48779296875 
[2025-01-17 18:01:48 root] (abq_llm.py 328): INFO layer 21 iter 2 loss:0.701653003692627 norm:0.008724983781576157 max memory_allocated 22921.48779296875 
[2025-01-17 18:02:19 root] (abq_llm.py 328): INFO layer 21 iter 3 loss:0.6878461837768555 norm:0.004943311680108309 max memory_allocated 22921.48779296875 
[2025-01-17 18:02:50 root] (abq_llm.py 328): INFO layer 21 iter 4 loss:0.6801649928092957 norm:0.0032051969319581985 max memory_allocated 22921.48779296875 
[2025-01-17 18:03:21 root] (abq_llm.py 328): INFO layer 21 iter 5 loss:0.6757845282554626 norm:0.002181953750550747 max memory_allocated 22921.48779296875 
[2025-01-17 18:03:52 root] (abq_llm.py 328): INFO layer 21 iter 6 loss:0.6730825901031494 norm:0.0018928302451968193 max memory_allocated 22921.48779296875 
[2025-01-17 18:04:23 root] (abq_llm.py 328): INFO layer 21 iter 7 loss:0.6710765361785889 norm:0.0017372984439134598 max memory_allocated 22921.48779296875 
[2025-01-17 18:04:54 root] (abq_llm.py 328): INFO layer 21 iter 8 loss:0.6694685816764832 norm:0.00163990817964077 max memory_allocated 22921.48779296875 
[2025-01-17 18:05:25 root] (abq_llm.py 328): INFO layer 21 iter 9 loss:0.6682299971580505 norm:0.0016256124945357442 max memory_allocated 22921.48779296875 
[2025-01-17 18:05:56 root] (abq_llm.py 328): INFO layer 21 iter 10 loss:0.667077362537384 norm:0.0016267572063952684 max memory_allocated 22921.48779296875 
[2025-01-17 18:06:27 root] (abq_llm.py 328): INFO layer 21 iter 11 loss:0.6660290360450745 norm:0.0016015898436307907 max memory_allocated 22921.48779296875 
[2025-01-17 18:06:58 root] (abq_llm.py 328): INFO layer 21 iter 12 loss:0.6650025248527527 norm:0.0015640509082004428 max memory_allocated 22921.48779296875 
[2025-01-17 18:07:29 root] (abq_llm.py 328): INFO layer 21 iter 13 loss:0.6641718149185181 norm:0.0015429677441716194 max memory_allocated 22921.48779296875 
[2025-01-17 18:08:01 root] (abq_llm.py 328): INFO layer 21 iter 14 loss:0.6634812951087952 norm:0.0015048969071358442 max memory_allocated 22921.48779296875 
[2025-01-17 18:08:32 root] (abq_llm.py 328): INFO layer 21 iter 15 loss:0.6628424525260925 norm:0.0015003938460722566 max memory_allocated 22921.48779296875 
[2025-01-17 18:09:03 root] (abq_llm.py 328): INFO layer 21 iter 16 loss:0.6623363494873047 norm:0.0014679833548143506 max memory_allocated 22921.48779296875 
[2025-01-17 18:09:34 root] (abq_llm.py 328): INFO layer 21 iter 17 loss:0.6618585586547852 norm:0.0014770377893000841 max memory_allocated 22921.48779296875 
[2025-01-17 18:10:05 root] (abq_llm.py 328): INFO layer 21 iter 18 loss:0.6614065170288086 norm:0.0014767450047656894 max memory_allocated 22921.48779296875 
[2025-01-17 18:10:36 root] (abq_llm.py 328): INFO layer 21 iter 19 loss:0.6610446572303772 norm:0.0014773928560316563 max memory_allocated 22921.48779296875 
[2025-01-17 18:10:45 root] (abq_llm.py 212): INFO === Start quantize layer 22 ===
[2025-01-17 18:11:22 root] (abq_llm.py 328): INFO layer 22 iter 0 loss:0.8498351573944092 norm:0.02468707412481308 max memory_allocated 22923.15966796875 
[2025-01-17 18:11:53 root] (abq_llm.py 328): INFO layer 22 iter 1 loss:0.8115623593330383 norm:0.011618342250585556 max memory_allocated 22923.15966796875 
[2025-01-17 18:12:24 root] (abq_llm.py 328): INFO layer 22 iter 2 loss:0.7909482717514038 norm:0.008182051591575146 max memory_allocated 22923.15966796875 
[2025-01-17 18:12:55 root] (abq_llm.py 328): INFO layer 22 iter 3 loss:0.7771944403648376 norm:0.00582203408703208 max memory_allocated 22923.15966796875 
[2025-01-17 18:13:26 root] (abq_llm.py 328): INFO layer 22 iter 4 loss:0.7672719359397888 norm:0.004369919653981924 max memory_allocated 22923.15966796875 
[2025-01-17 18:13:57 root] (abq_llm.py 328): INFO layer 22 iter 5 loss:0.762062668800354 norm:0.003660182934254408 max memory_allocated 22923.15966796875 
[2025-01-17 18:14:28 root] (abq_llm.py 328): INFO layer 22 iter 6 loss:0.7587546110153198 norm:0.0032539283856749535 max memory_allocated 22923.15966796875 
[2025-01-17 18:14:59 root] (abq_llm.py 328): INFO layer 22 iter 7 loss:0.756488025188446 norm:0.0030488374177366495 max memory_allocated 22923.15966796875 
[2025-01-17 18:15:30 root] (abq_llm.py 328): INFO layer 22 iter 8 loss:0.7545679807662964 norm:0.002932707080617547 max memory_allocated 22923.15966796875 
[2025-01-17 18:16:01 root] (abq_llm.py 328): INFO layer 22 iter 9 loss:0.7528344988822937 norm:0.0027500158175826073 max memory_allocated 22923.15966796875 
[2025-01-17 18:16:32 root] (abq_llm.py 328): INFO layer 22 iter 10 loss:0.7513821721076965 norm:0.0026441500522196293 max memory_allocated 22923.15966796875 
[2025-01-17 18:17:03 root] (abq_llm.py 328): INFO layer 22 iter 11 loss:0.7500869631767273 norm:0.0026956682559102774 max memory_allocated 22923.15966796875 
[2025-01-17 18:17:34 root] (abq_llm.py 328): INFO layer 22 iter 12 loss:0.7490125298500061 norm:0.0026740171015262604 max memory_allocated 22923.15966796875 
[2025-01-17 18:18:05 root] (abq_llm.py 328): INFO layer 22 iter 13 loss:0.7480557560920715 norm:0.002757973037660122 max memory_allocated 22923.15966796875 
[2025-01-17 18:18:37 root] (abq_llm.py 328): INFO layer 22 iter 14 loss:0.747240424156189 norm:0.0027230626437813044 max memory_allocated 22923.15966796875 
[2025-01-17 18:19:08 root] (abq_llm.py 328): INFO layer 22 iter 15 loss:0.7465912103652954 norm:0.002640283200889826 max memory_allocated 22923.15966796875 
[2025-01-17 18:19:39 root] (abq_llm.py 328): INFO layer 22 iter 16 loss:0.7459578514099121 norm:0.0026612235233187675 max memory_allocated 22923.15966796875 
[2025-01-17 18:20:10 root] (abq_llm.py 328): INFO layer 22 iter 17 loss:0.7455018162727356 norm:0.0027463892474770546 max memory_allocated 22923.15966796875 
[2025-01-17 18:20:41 root] (abq_llm.py 328): INFO layer 22 iter 18 loss:0.7450661659240723 norm:0.002685059793293476 max memory_allocated 22923.15966796875 
[2025-01-17 18:21:12 root] (abq_llm.py 328): INFO layer 22 iter 19 loss:0.7445157170295715 norm:0.0026533850468695164 max memory_allocated 22923.15966796875 
[2025-01-17 18:21:21 root] (abq_llm.py 212): INFO === Start quantize layer 23 ===
[2025-01-17 18:21:57 root] (abq_llm.py 328): INFO layer 23 iter 0 loss:0.9010520577430725 norm:0.024596087634563446 max memory_allocated 22924.83154296875 
[2025-01-17 18:22:28 root] (abq_llm.py 328): INFO layer 23 iter 1 loss:0.8729004263877869 norm:0.012384261935949326 max memory_allocated 22924.83154296875 
[2025-01-17 18:22:59 root] (abq_llm.py 328): INFO layer 23 iter 2 loss:0.8559009432792664 norm:0.00726861460134387 max memory_allocated 22924.83154296875 
[2025-01-17 18:23:30 root] (abq_llm.py 328): INFO layer 23 iter 3 loss:0.8431832194328308 norm:0.004872786812484264 max memory_allocated 22924.83154296875 
[2025-01-17 18:24:01 root] (abq_llm.py 328): INFO layer 23 iter 4 loss:0.8355621695518494 norm:0.0028437136206775904 max memory_allocated 22924.83154296875 
[2025-01-17 18:24:32 root] (abq_llm.py 328): INFO layer 23 iter 5 loss:0.8307413458824158 norm:0.002038175705820322 max memory_allocated 22924.83154296875 
[2025-01-17 18:25:03 root] (abq_llm.py 328): INFO layer 23 iter 6 loss:0.8277649283409119 norm:0.0018343237461522222 max memory_allocated 22924.83154296875 
[2025-01-17 18:25:34 root] (abq_llm.py 328): INFO layer 23 iter 7 loss:0.8257490396499634 norm:0.001742191263474524 max memory_allocated 22924.83154296875 
[2025-01-17 18:26:05 root] (abq_llm.py 328): INFO layer 23 iter 8 loss:0.8239037990570068 norm:0.0016482651699334383 max memory_allocated 22924.83154296875 
[2025-01-17 18:26:37 root] (abq_llm.py 328): INFO layer 23 iter 9 loss:0.8222984075546265 norm:0.001607081270776689 max memory_allocated 22924.83154296875 
[2025-01-17 18:27:08 root] (abq_llm.py 328): INFO layer 23 iter 10 loss:0.8208434581756592 norm:0.0015263615641742945 max memory_allocated 22924.83154296875 
[2025-01-17 18:27:39 root] (abq_llm.py 328): INFO layer 23 iter 11 loss:0.8197193741798401 norm:0.0015212609432637691 max memory_allocated 22924.83154296875 
[2025-01-17 18:28:10 root] (abq_llm.py 328): INFO layer 23 iter 12 loss:0.8187782168388367 norm:0.001502455910667777 max memory_allocated 22924.83154296875 
[2025-01-17 18:28:41 root] (abq_llm.py 328): INFO layer 23 iter 13 loss:0.8179774284362793 norm:0.0014856546185910702 max memory_allocated 22924.83154296875 
[2025-01-17 18:29:12 root] (abq_llm.py 328): INFO layer 23 iter 14 loss:0.8172242045402527 norm:0.0014561405405402184 max memory_allocated 22924.83154296875 
[2025-01-17 18:29:43 root] (abq_llm.py 328): INFO layer 23 iter 15 loss:0.8165338039398193 norm:0.001398800639435649 max memory_allocated 22924.83154296875 
[2025-01-17 18:30:14 root] (abq_llm.py 328): INFO layer 23 iter 16 loss:0.8159514665603638 norm:0.0013707493199035525 max memory_allocated 22924.83154296875 
[2025-01-17 18:30:45 root] (abq_llm.py 328): INFO layer 23 iter 17 loss:0.8155098557472229 norm:0.0013721571303904057 max memory_allocated 22924.83154296875 
[2025-01-17 18:31:16 root] (abq_llm.py 328): INFO layer 23 iter 18 loss:0.8151128888130188 norm:0.0013665860751643777 max memory_allocated 22924.83154296875 
[2025-01-17 18:31:47 root] (abq_llm.py 328): INFO layer 23 iter 19 loss:0.8147857785224915 norm:0.0013599515659734607 max memory_allocated 22924.83154296875 
[2025-01-17 18:31:56 root] (abq_llm.py 212): INFO === Start quantize layer 24 ===
[2025-01-17 18:32:32 root] (abq_llm.py 328): INFO layer 24 iter 0 loss:1.0020018815994263 norm:0.02658062055706978 max memory_allocated 22926.50341796875 
[2025-01-17 18:33:03 root] (abq_llm.py 328): INFO layer 24 iter 1 loss:0.9697189331054688 norm:0.014239267446100712 max memory_allocated 22926.50341796875 
[2025-01-17 18:33:34 root] (abq_llm.py 328): INFO layer 24 iter 2 loss:0.9493530988693237 norm:0.007976528257131577 max memory_allocated 22926.50341796875 
[2025-01-17 18:34:05 root] (abq_llm.py 328): INFO layer 24 iter 3 loss:0.9365459680557251 norm:0.005956362001597881 max memory_allocated 22926.50341796875 
[2025-01-17 18:34:36 root] (abq_llm.py 328): INFO layer 24 iter 4 loss:0.9261864423751831 norm:0.004166982136666775 max memory_allocated 22926.50341796875 
[2025-01-17 18:35:07 root] (abq_llm.py 328): INFO layer 24 iter 5 loss:0.9203023314476013 norm:0.003560835961252451 max memory_allocated 22926.50341796875 
[2025-01-17 18:35:38 root] (abq_llm.py 328): INFO layer 24 iter 6 loss:0.9168552160263062 norm:0.0034879075828939676 max memory_allocated 22926.50341796875 
[2025-01-17 18:36:09 root] (abq_llm.py 328): INFO layer 24 iter 7 loss:0.9144240021705627 norm:0.003456416539847851 max memory_allocated 22926.50341796875 
[2025-01-17 18:36:40 root] (abq_llm.py 328): INFO layer 24 iter 8 loss:0.9119697213172913 norm:0.003586556762456894 max memory_allocated 22926.50341796875 
[2025-01-17 18:37:11 root] (abq_llm.py 328): INFO layer 24 iter 9 loss:0.9099396467208862 norm:0.00350094772875309 max memory_allocated 22926.50341796875 
[2025-01-17 18:37:42 root] (abq_llm.py 328): INFO layer 24 iter 10 loss:0.9081896543502808 norm:0.003533552400767803 max memory_allocated 22926.50341796875 
[2025-01-17 18:38:13 root] (abq_llm.py 328): INFO layer 24 iter 11 loss:0.9065502285957336 norm:0.003551169065758586 max memory_allocated 22926.50341796875 
[2025-01-17 18:38:44 root] (abq_llm.py 328): INFO layer 24 iter 12 loss:0.9052895903587341 norm:0.0034892575349658728 max memory_allocated 22926.50341796875 
[2025-01-17 18:39:15 root] (abq_llm.py 328): INFO layer 24 iter 13 loss:0.9042940735816956 norm:0.0034593609161674976 max memory_allocated 22926.50341796875 
[2025-01-17 18:39:46 root] (abq_llm.py 328): INFO layer 24 iter 14 loss:0.9034196734428406 norm:0.0034714331850409508 max memory_allocated 22926.50341796875 
[2025-01-17 18:40:18 root] (abq_llm.py 328): INFO layer 24 iter 15 loss:0.9026826024055481 norm:0.0033926316536962986 max memory_allocated 22926.50341796875 
[2025-01-17 18:40:49 root] (abq_llm.py 328): INFO layer 24 iter 16 loss:0.9020593166351318 norm:0.0034286186564713717 max memory_allocated 22926.50341796875 
[2025-01-17 18:41:20 root] (abq_llm.py 328): INFO layer 24 iter 17 loss:0.9015634059906006 norm:0.0033878767862915993 max memory_allocated 22926.50341796875 
[2025-01-17 18:41:51 root] (abq_llm.py 328): INFO layer 24 iter 18 loss:0.9011586308479309 norm:0.003355395747348666 max memory_allocated 22926.50341796875 
[2025-01-17 18:42:22 root] (abq_llm.py 328): INFO layer 24 iter 19 loss:0.9007411599159241 norm:0.003372069913893938 max memory_allocated 22926.50341796875 
[2025-01-17 18:42:31 root] (abq_llm.py 212): INFO === Start quantize layer 25 ===
[2025-01-17 18:43:10 root] (abq_llm.py 328): INFO layer 25 iter 0 loss:1.1213974952697754 norm:0.0549817718565464 max memory_allocated 22928.17529296875 
[2025-01-17 18:43:41 root] (abq_llm.py 328): INFO layer 25 iter 1 loss:1.0966315269470215 norm:0.03600854054093361 max memory_allocated 22928.17529296875 
[2025-01-17 18:44:12 root] (abq_llm.py 328): INFO layer 25 iter 2 loss:1.0787407159805298 norm:0.02527701109647751 max memory_allocated 22928.17529296875 
[2025-01-17 18:44:43 root] (abq_llm.py 328): INFO layer 25 iter 3 loss:1.0619428157806396 norm:0.01753324083983898 max memory_allocated 22928.17529296875 
[2025-01-17 18:45:14 root] (abq_llm.py 328): INFO layer 25 iter 4 loss:1.0518925189971924 norm:0.012602336704730988 max memory_allocated 22928.17529296875 
[2025-01-17 18:45:45 root] (abq_llm.py 328): INFO layer 25 iter 5 loss:1.0445870161056519 norm:0.0075020091608166695 max memory_allocated 22928.17529296875 
[2025-01-17 18:46:16 root] (abq_llm.py 328): INFO layer 25 iter 6 loss:1.036280632019043 norm:0.005842928774654865 max memory_allocated 22928.17529296875 
[2025-01-17 18:46:47 root] (abq_llm.py 328): INFO layer 25 iter 7 loss:1.03263258934021 norm:0.005706442054361105 max memory_allocated 22928.17529296875 
[2025-01-17 18:47:18 root] (abq_llm.py 328): INFO layer 25 iter 8 loss:1.0297784805297852 norm:0.005563279613852501 max memory_allocated 22928.17529296875 
[2025-01-17 18:47:49 root] (abq_llm.py 328): INFO layer 25 iter 9 loss:1.0285013914108276 norm:0.006016748026013374 max memory_allocated 22928.17529296875 
[2025-01-17 18:48:20 root] (abq_llm.py 328): INFO layer 25 iter 10 loss:1.0285143852233887 norm:0.005967434030026197 max memory_allocated 22928.17529296875 
[2025-01-17 18:48:51 root] (abq_llm.py 328): INFO layer 25 iter 11 loss:1.0245846509933472 norm:0.004210954532027245 max memory_allocated 22928.17529296875 
[2025-01-17 18:49:22 root] (abq_llm.py 328): INFO layer 25 iter 12 loss:1.0217055082321167 norm:0.004124639555811882 max memory_allocated 22928.17529296875 
[2025-01-17 18:49:53 root] (abq_llm.py 328): INFO layer 25 iter 13 loss:1.0217503309249878 norm:0.004776828922331333 max memory_allocated 22928.17529296875 
[2025-01-17 18:50:24 root] (abq_llm.py 328): INFO layer 25 iter 14 loss:1.0225549936294556 norm:0.004570948891341686 max memory_allocated 22928.17529296875 
[2025-01-17 18:50:55 root] (abq_llm.py 328): INFO layer 25 iter 15 loss:1.019322395324707 norm:0.0034957858733832836 max memory_allocated 22928.17529296875 
[2025-01-17 18:51:26 root] (abq_llm.py 328): INFO layer 25 iter 16 loss:1.0170427560806274 norm:0.0033597624860703945 max memory_allocated 22928.17529296875 
[2025-01-17 18:51:57 root] (abq_llm.py 328): INFO layer 25 iter 17 loss:1.0176191329956055 norm:0.003950171172618866 max memory_allocated 22928.17529296875 
[2025-01-17 18:52:28 root] (abq_llm.py 328): INFO layer 25 iter 18 loss:1.018557071685791 norm:0.003693607170134783 max memory_allocated 22928.17529296875 
[2025-01-17 18:52:59 root] (abq_llm.py 328): INFO layer 25 iter 19 loss:1.015840768814087 norm:0.003104252740740776 max memory_allocated 22928.17529296875 
[2025-01-17 18:53:08 root] (abq_llm.py 212): INFO === Start quantize layer 26 ===
[2025-01-17 18:53:45 root] (abq_llm.py 328): INFO layer 26 iter 0 loss:1.2485629320144653 norm:0.017806237563490868 max memory_allocated 22929.84716796875 
[2025-01-17 18:54:16 root] (abq_llm.py 328): INFO layer 26 iter 1 loss:1.2116414308547974 norm:0.009362115524709225 max memory_allocated 22929.84716796875 
[2025-01-17 18:54:47 root] (abq_llm.py 328): INFO layer 26 iter 2 loss:1.1903386116027832 norm:0.006611578166484833 max memory_allocated 22929.84716796875 
[2025-01-17 18:55:18 root] (abq_llm.py 328): INFO layer 26 iter 3 loss:1.1741095781326294 norm:0.004777256399393082 max memory_allocated 22929.84716796875 
[2025-01-17 18:55:49 root] (abq_llm.py 328): INFO layer 26 iter 4 loss:1.1619848012924194 norm:0.0033613438718020916 max memory_allocated 22929.84716796875 
[2025-01-17 18:56:20 root] (abq_llm.py 328): INFO layer 26 iter 5 loss:1.1547367572784424 norm:0.0028019766323268414 max memory_allocated 22929.84716796875 
[2025-01-17 18:56:51 root] (abq_llm.py 328): INFO layer 26 iter 6 loss:1.150194525718689 norm:0.0025766135659068823 max memory_allocated 22929.84716796875 
[2025-01-17 18:57:22 root] (abq_llm.py 328): INFO layer 26 iter 7 loss:1.1469120979309082 norm:0.0024747198913246393 max memory_allocated 22929.84716796875 
[2025-01-17 18:57:53 root] (abq_llm.py 328): INFO layer 26 iter 8 loss:1.144091248512268 norm:0.002421223558485508 max memory_allocated 22929.84716796875 
[2025-01-17 18:58:24 root] (abq_llm.py 328): INFO layer 26 iter 9 loss:1.1415576934814453 norm:0.0024050704669207335 max memory_allocated 22929.84716796875 
[2025-01-17 18:58:55 root] (abq_llm.py 328): INFO layer 26 iter 10 loss:1.1394898891448975 norm:0.0023246065247803926 max memory_allocated 22929.84716796875 
[2025-01-17 18:59:26 root] (abq_llm.py 328): INFO layer 26 iter 11 loss:1.1376678943634033 norm:0.0022884525824338198 max memory_allocated 22929.84716796875 
[2025-01-17 18:59:57 root] (abq_llm.py 328): INFO layer 26 iter 12 loss:1.1360423564910889 norm:0.0022793542593717575 max memory_allocated 22929.84716796875 
[2025-01-17 19:00:28 root] (abq_llm.py 328): INFO layer 26 iter 13 loss:1.134735345840454 norm:0.00234478572383523 max memory_allocated 22929.84716796875 
[2025-01-17 19:00:59 root] (abq_llm.py 328): INFO layer 26 iter 14 loss:1.1336169242858887 norm:0.0023206397891044617 max memory_allocated 22929.84716796875 
[2025-01-17 19:01:30 root] (abq_llm.py 328): INFO layer 26 iter 15 loss:1.1326289176940918 norm:0.0022763232700526714 max memory_allocated 22929.84716796875 
[2025-01-17 19:02:02 root] (abq_llm.py 328): INFO layer 26 iter 16 loss:1.1318045854568481 norm:0.0022430948447436094 max memory_allocated 22929.84716796875 
[2025-01-17 19:02:33 root] (abq_llm.py 328): INFO layer 26 iter 17 loss:1.1311017274856567 norm:0.0022399239242076874 max memory_allocated 22929.84716796875 
[2025-01-17 19:03:04 root] (abq_llm.py 328): INFO layer 26 iter 18 loss:1.1305091381072998 norm:0.002237428445369005 max memory_allocated 22929.84716796875 
[2025-01-17 19:03:35 root] (abq_llm.py 328): INFO layer 26 iter 19 loss:1.1300134658813477 norm:0.002220738213509321 max memory_allocated 22929.84716796875 
[2025-01-17 19:03:44 root] (abq_llm.py 212): INFO === Start quantize layer 27 ===
[2025-01-17 19:04:21 root] (abq_llm.py 328): INFO layer 27 iter 0 loss:1.4326552152633667 norm:0.03732217103242874 max memory_allocated 22931.51904296875 
[2025-01-17 19:04:52 root] (abq_llm.py 328): INFO layer 27 iter 1 loss:1.377585768699646 norm:0.017510775476694107 max memory_allocated 22931.51904296875 
[2025-01-17 19:05:23 root] (abq_llm.py 328): INFO layer 27 iter 2 loss:1.344062089920044 norm:0.00976492092013359 max memory_allocated 22931.51904296875 
[2025-01-17 19:05:54 root] (abq_llm.py 328): INFO layer 27 iter 3 loss:1.3242003917694092 norm:0.006685622502118349 max memory_allocated 22931.51904296875 
[2025-01-17 19:06:25 root] (abq_llm.py 328): INFO layer 27 iter 4 loss:1.3110079765319824 norm:0.004704698920249939 max memory_allocated 22931.51904296875 
[2025-01-17 19:06:56 root] (abq_llm.py 328): INFO layer 27 iter 5 loss:1.3027735948562622 norm:0.0036966041661798954 max memory_allocated 22931.51904296875 
[2025-01-17 19:07:27 root] (abq_llm.py 328): INFO layer 27 iter 6 loss:1.297589659690857 norm:0.0031682078260928392 max memory_allocated 22931.51904296875 
[2025-01-17 19:07:59 root] (abq_llm.py 328): INFO layer 27 iter 7 loss:1.2936278581619263 norm:0.002845559734851122 max memory_allocated 22931.51904296875 
[2025-01-17 19:08:30 root] (abq_llm.py 328): INFO layer 27 iter 8 loss:1.2898385524749756 norm:0.0025051154661923647 max memory_allocated 22931.51904296875 
[2025-01-17 19:09:01 root] (abq_llm.py 328): INFO layer 27 iter 9 loss:1.2863881587982178 norm:0.002317588310688734 max memory_allocated 22931.51904296875 
[2025-01-17 19:09:32 root] (abq_llm.py 328): INFO layer 27 iter 10 loss:1.2838730812072754 norm:0.0022734608501195908 max memory_allocated 22931.51904296875 
[2025-01-17 19:10:03 root] (abq_llm.py 328): INFO layer 27 iter 11 loss:1.281562328338623 norm:0.0021205442026257515 max memory_allocated 22931.51904296875 
[2025-01-17 19:10:34 root] (abq_llm.py 328): INFO layer 27 iter 12 loss:1.2794479131698608 norm:0.0020325835794210434 max memory_allocated 22931.51904296875 
[2025-01-17 19:11:05 root] (abq_llm.py 328): INFO layer 27 iter 13 loss:1.2779338359832764 norm:0.001977156847715378 max memory_allocated 22931.51904296875 
[2025-01-17 19:11:36 root] (abq_llm.py 328): INFO layer 27 iter 14 loss:1.2765841484069824 norm:0.0020218563731759787 max memory_allocated 22931.51904296875 
[2025-01-17 19:12:07 root] (abq_llm.py 328): INFO layer 27 iter 15 loss:1.2754325866699219 norm:0.0020176456309854984 max memory_allocated 22931.51904296875 
[2025-01-17 19:12:38 root] (abq_llm.py 328): INFO layer 27 iter 16 loss:1.2743005752563477 norm:0.0019918226171284914 max memory_allocated 22931.51904296875 
[2025-01-17 19:13:09 root] (abq_llm.py 328): INFO layer 27 iter 17 loss:1.2735399007797241 norm:0.0019693532958626747 max memory_allocated 22931.51904296875 
[2025-01-17 19:13:40 root] (abq_llm.py 328): INFO layer 27 iter 18 loss:1.2728577852249146 norm:0.0019487484823912382 max memory_allocated 22931.51904296875 
[2025-01-17 19:14:11 root] (abq_llm.py 328): INFO layer 27 iter 19 loss:1.2723644971847534 norm:0.00194045330863446 max memory_allocated 22931.51904296875 
[2025-01-17 19:14:20 root] (abq_llm.py 212): INFO === Start quantize layer 28 ===
[2025-01-17 19:14:27 root] (abq_llm.py 268): INFO use compensation vector
[2025-01-17 19:14:58 root] (abq_llm.py 328): INFO layer 28 iter 0 loss:1.5876845121383667 norm:0.05794385075569153 max memory_allocated 22933.30615234375 
[2025-01-17 19:15:29 root] (abq_llm.py 328): INFO layer 28 iter 1 loss:1.5439115762710571 norm:0.040119558572769165 max memory_allocated 22933.30615234375 
[2025-01-17 19:16:00 root] (abq_llm.py 328): INFO layer 28 iter 2 loss:1.5112311840057373 norm:0.028409169986844063 max memory_allocated 22933.30615234375 
[2025-01-17 19:16:31 root] (abq_llm.py 328): INFO layer 28 iter 3 loss:1.4900791645050049 norm:0.021490080282092094 max memory_allocated 22933.30615234375 
[2025-01-17 19:17:02 root] (abq_llm.py 328): INFO layer 28 iter 4 loss:1.4722685813903809 norm:0.016878820955753326 max memory_allocated 22933.30615234375 
[2025-01-17 19:17:34 root] (abq_llm.py 328): INFO layer 28 iter 5 loss:1.458653450012207 norm:0.0134755689650774 max memory_allocated 22933.30615234375 
[2025-01-17 19:18:05 root] (abq_llm.py 328): INFO layer 28 iter 6 loss:1.4503962993621826 norm:0.011915098875761032 max memory_allocated 22933.30615234375 
[2025-01-17 19:18:36 root] (abq_llm.py 328): INFO layer 28 iter 7 loss:1.4453043937683105 norm:0.010876394808292389 max memory_allocated 22933.30615234375 
[2025-01-17 19:19:07 root] (abq_llm.py 328): INFO layer 28 iter 8 loss:1.4416120052337646 norm:0.011004858650267124 max memory_allocated 22933.30615234375 
[2025-01-17 19:19:38 root] (abq_llm.py 328): INFO layer 28 iter 9 loss:1.4389841556549072 norm:0.011310082860291004 max memory_allocated 22933.30615234375 
[2025-01-17 19:20:09 root] (abq_llm.py 328): INFO layer 28 iter 10 loss:1.4363842010498047 norm:0.011066320352256298 max memory_allocated 22933.30615234375 
[2025-01-17 19:20:40 root] (abq_llm.py 328): INFO layer 28 iter 11 loss:1.4339861869812012 norm:0.010588183999061584 max memory_allocated 22933.30615234375 
[2025-01-17 19:21:11 root] (abq_llm.py 328): INFO layer 28 iter 12 loss:1.4317960739135742 norm:0.010485043749213219 max memory_allocated 22933.30615234375 
[2025-01-17 19:21:43 root] (abq_llm.py 328): INFO layer 28 iter 13 loss:1.4298280477523804 norm:0.010114885866641998 max memory_allocated 22933.30615234375 
[2025-01-17 19:22:14 root] (abq_llm.py 328): INFO layer 28 iter 14 loss:1.42828369140625 norm:0.009921473450958729 max memory_allocated 22933.30615234375 
[2025-01-17 19:22:45 root] (abq_llm.py 328): INFO layer 28 iter 15 loss:1.427026629447937 norm:0.00998331606388092 max memory_allocated 22933.30615234375 
[2025-01-17 19:23:16 root] (abq_llm.py 328): INFO layer 28 iter 16 loss:1.4256043434143066 norm:0.009652683511376381 max memory_allocated 22933.30615234375 
[2025-01-17 19:23:47 root] (abq_llm.py 328): INFO layer 28 iter 17 loss:1.4246118068695068 norm:0.009892350994050503 max memory_allocated 22933.30615234375 
[2025-01-17 19:24:18 root] (abq_llm.py 328): INFO layer 28 iter 18 loss:1.4234223365783691 norm:0.009465916082262993 max memory_allocated 22933.30615234375 
[2025-01-17 19:24:49 root] (abq_llm.py 328): INFO layer 28 iter 19 loss:1.4227224588394165 norm:0.00973633024841547 max memory_allocated 22933.30615234375 
[2025-01-17 19:24:58 root] (abq_llm.py 212): INFO === Start quantize layer 29 ===
[2025-01-17 19:25:04 root] (abq_llm.py 268): INFO use compensation vector
[2025-01-17 19:25:35 root] (abq_llm.py 328): INFO layer 29 iter 0 loss:1.7875244617462158 norm:0.0529559850692749 max memory_allocated 22934.97802734375 
[2025-01-17 19:26:06 root] (abq_llm.py 328): INFO layer 29 iter 1 loss:1.7283823490142822 norm:0.04095716401934624 max memory_allocated 22934.97802734375 
[2025-01-17 19:26:37 root] (abq_llm.py 328): INFO layer 29 iter 2 loss:1.6918621063232422 norm:0.031132111325860023 max memory_allocated 22934.97802734375 
[2025-01-17 19:27:08 root] (abq_llm.py 328): INFO layer 29 iter 3 loss:1.6633880138397217 norm:0.023739995434880257 max memory_allocated 22934.97802734375 
[2025-01-17 19:27:39 root] (abq_llm.py 328): INFO layer 29 iter 4 loss:1.6415435075759888 norm:0.019321151077747345 max memory_allocated 22934.97802734375 
[2025-01-17 19:28:10 root] (abq_llm.py 328): INFO layer 29 iter 5 loss:1.625152826309204 norm:0.016007911413908005 max memory_allocated 22934.97802734375 
[2025-01-17 19:28:41 root] (abq_llm.py 328): INFO layer 29 iter 6 loss:1.6158732175827026 norm:0.014477751217782497 max memory_allocated 22934.97802734375 
[2025-01-17 19:29:12 root] (abq_llm.py 328): INFO layer 29 iter 7 loss:1.6102429628372192 norm:0.01384790800511837 max memory_allocated 22934.97802734375 
[2025-01-17 19:29:44 root] (abq_llm.py 328): INFO layer 29 iter 8 loss:1.605250597000122 norm:0.013733193278312683 max memory_allocated 22934.97802734375 
[2025-01-17 19:30:15 root] (abq_llm.py 328): INFO layer 29 iter 9 loss:1.6018682718276978 norm:0.014295229688286781 max memory_allocated 22934.97802734375 
[2025-01-17 19:30:46 root] (abq_llm.py 328): INFO layer 29 iter 10 loss:1.5987348556518555 norm:0.0139620928093791 max memory_allocated 22934.97802734375 
[2025-01-17 19:31:17 root] (abq_llm.py 328): INFO layer 29 iter 11 loss:1.5955185890197754 norm:0.012941941618919373 max memory_allocated 22934.97802734375 
[2025-01-17 19:31:48 root] (abq_llm.py 328): INFO layer 29 iter 12 loss:1.59348726272583 norm:0.013058382086455822 max memory_allocated 22934.97802734375 
[2025-01-17 19:32:19 root] (abq_llm.py 328): INFO layer 29 iter 13 loss:1.5912517309188843 norm:0.012964061461389065 max memory_allocated 22934.97802734375 
[2025-01-17 19:32:50 root] (abq_llm.py 328): INFO layer 29 iter 14 loss:1.589730978012085 norm:0.013347719796001911 max memory_allocated 22934.97802734375 
[2025-01-17 19:33:21 root] (abq_llm.py 328): INFO layer 29 iter 15 loss:1.5874862670898438 norm:0.01226844359189272 max memory_allocated 22934.97802734375 
[2025-01-17 19:33:53 root] (abq_llm.py 328): INFO layer 29 iter 16 loss:1.5855822563171387 norm:0.011760180816054344 max memory_allocated 22934.97802734375 
[2025-01-17 19:34:24 root] (abq_llm.py 328): INFO layer 29 iter 17 loss:1.5839381217956543 norm:0.010948492214083672 max memory_allocated 22934.97802734375 
[2025-01-17 19:34:55 root] (abq_llm.py 328): INFO layer 29 iter 18 loss:1.582969069480896 norm:0.011432992294430733 max memory_allocated 22934.97802734375 
[2025-01-17 19:35:26 root] (abq_llm.py 328): INFO layer 29 iter 19 loss:1.5815407037734985 norm:0.011810221709311008 max memory_allocated 22934.97802734375 
[2025-01-17 19:35:35 root] (abq_llm.py 212): INFO === Start quantize layer 30 ===
[2025-01-17 19:35:41 root] (abq_llm.py 268): INFO use compensation vector
[2025-01-17 19:36:12 root] (abq_llm.py 328): INFO layer 30 iter 0 loss:3.991644859313965 norm:0.2349534034729004 max memory_allocated 22936.64990234375 
[2025-01-17 19:36:43 root] (abq_llm.py 328): INFO layer 30 iter 1 loss:3.7330048084259033 norm:0.24828436970710754 max memory_allocated 22936.64990234375 
[2025-01-17 19:37:14 root] (abq_llm.py 328): INFO layer 30 iter 2 loss:3.3242955207824707 norm:0.22699688374996185 max memory_allocated 22936.64990234375 
[2025-01-17 19:37:45 root] (abq_llm.py 328): INFO layer 30 iter 3 loss:3.052687883377075 norm:1.245333194732666 max memory_allocated 22936.64990234375 
[2025-01-17 19:38:16 root] (abq_llm.py 328): INFO layer 30 iter 4 loss:2.7951526641845703 norm:0.6522814035415649 max memory_allocated 22936.64990234375 
[2025-01-17 19:38:47 root] (abq_llm.py 328): INFO layer 30 iter 5 loss:2.687976360321045 norm:0.35457149147987366 max memory_allocated 22936.64990234375 
[2025-01-17 19:39:19 root] (abq_llm.py 328): INFO layer 30 iter 6 loss:2.6443915367126465 norm:0.30802276730537415 max memory_allocated 22936.64990234375 
[2025-01-17 19:39:50 root] (abq_llm.py 328): INFO layer 30 iter 7 loss:2.6294503211975098 norm:0.34189289808273315 max memory_allocated 22936.64990234375 
[2025-01-17 19:40:21 root] (abq_llm.py 328): INFO layer 30 iter 8 loss:2.563854455947876 norm:0.2902003526687622 max memory_allocated 22936.64990234375 
[2025-01-17 19:40:52 root] (abq_llm.py 328): INFO layer 30 iter 9 loss:2.534881591796875 norm:0.2919413447380066 max memory_allocated 22936.64990234375 
[2025-01-17 19:41:23 root] (abq_llm.py 328): INFO layer 30 iter 10 loss:2.4936392307281494 norm:0.29708316922187805 max memory_allocated 22936.64990234375 
[2025-01-17 19:41:54 root] (abq_llm.py 328): INFO layer 30 iter 11 loss:2.504483699798584 norm:0.2969222664833069 max memory_allocated 22936.64990234375 
[2025-01-17 19:42:25 root] (abq_llm.py 328): INFO layer 30 iter 12 loss:2.4635372161865234 norm:0.280910462141037 max memory_allocated 22936.64990234375 
[2025-01-17 19:42:57 root] (abq_llm.py 328): INFO layer 30 iter 13 loss:2.4546010494232178 norm:0.2849369943141937 max memory_allocated 22936.64990234375 
[2025-01-17 19:43:28 root] (abq_llm.py 328): INFO layer 30 iter 14 loss:2.4310100078582764 norm:0.28452038764953613 max memory_allocated 22936.64990234375 
[2025-01-17 19:43:59 root] (abq_llm.py 328): INFO layer 30 iter 15 loss:2.41774845123291 norm:0.3091094493865967 max memory_allocated 22936.64990234375 
[2025-01-17 19:44:30 root] (abq_llm.py 328): INFO layer 30 iter 16 loss:2.4153077602386475 norm:0.3441319167613983 max memory_allocated 22936.64990234375 
[2025-01-17 19:45:01 root] (abq_llm.py 328): INFO layer 30 iter 17 loss:2.436572313308716 norm:0.277921199798584 max memory_allocated 22936.64990234375 
[2025-01-17 19:45:32 root] (abq_llm.py 328): INFO layer 30 iter 18 loss:2.3782808780670166 norm:0.27318331599235535 max memory_allocated 22936.64990234375 
[2025-01-17 19:46:03 root] (abq_llm.py 328): INFO layer 30 iter 19 loss:2.3589978218078613 norm:0.2600215673446655 max memory_allocated 22936.64990234375 
[2025-01-17 19:46:12 root] (abq_llm.py 212): INFO === Start quantize layer 31 ===
[2025-01-17 19:46:17 root] (abq_llm.py 268): INFO use compensation vector
[2025-01-17 19:46:48 root] (abq_llm.py 328): INFO layer 31 iter 0 loss:4.613603591918945 norm:0.9793047904968262 max memory_allocated 22938.32177734375 
[2025-01-17 19:47:19 root] (abq_llm.py 328): INFO layer 31 iter 1 loss:4.019101619720459 norm:0.4603283107280731 max memory_allocated 22938.32177734375 
[2025-01-17 19:47:50 root] (abq_llm.py 328): INFO layer 31 iter 2 loss:3.8406147956848145 norm:0.3334486186504364 max memory_allocated 22938.32177734375 
[2025-01-17 19:48:22 root] (abq_llm.py 328): INFO layer 31 iter 3 loss:3.7131664752960205 norm:0.2690986394882202 max memory_allocated 22938.32177734375 
[2025-01-17 19:48:53 root] (abq_llm.py 328): INFO layer 31 iter 4 loss:3.6174166202545166 norm:0.2236592173576355 max memory_allocated 22938.32177734375 
[2025-01-17 19:49:24 root] (abq_llm.py 328): INFO layer 31 iter 5 loss:3.546143054962158 norm:0.19215738773345947 max memory_allocated 22938.32177734375 
[2025-01-17 19:49:55 root] (abq_llm.py 328): INFO layer 31 iter 6 loss:3.4940531253814697 norm:0.17430253326892853 max memory_allocated 22938.32177734375 
[2025-01-17 19:50:26 root] (abq_llm.py 328): INFO layer 31 iter 7 loss:3.456376791000366 norm:0.16216489672660828 max memory_allocated 22938.32177734375 
[2025-01-17 19:50:57 root] (abq_llm.py 328): INFO layer 31 iter 8 loss:3.4254536628723145 norm:0.15701183676719666 max memory_allocated 22938.32177734375 
[2025-01-17 19:51:28 root] (abq_llm.py 328): INFO layer 31 iter 9 loss:3.3971004486083984 norm:0.14185093343257904 max memory_allocated 22938.32177734375 
[2025-01-17 19:51:59 root] (abq_llm.py 328): INFO layer 31 iter 10 loss:3.3816449642181396 norm:0.13383378088474274 max memory_allocated 22938.32177734375 
[2025-01-17 19:52:31 root] (abq_llm.py 328): INFO layer 31 iter 11 loss:3.349323034286499 norm:0.12159281224012375 max memory_allocated 22938.32177734375 
[2025-01-17 19:53:02 root] (abq_llm.py 328): INFO layer 31 iter 12 loss:3.336320638656616 norm:0.11555090546607971 max memory_allocated 22938.32177734375 
[2025-01-17 19:53:33 root] (abq_llm.py 328): INFO layer 31 iter 13 loss:3.3216347694396973 norm:0.1154014840722084 max memory_allocated 22938.32177734375 
[2025-01-17 19:54:04 root] (abq_llm.py 328): INFO layer 31 iter 14 loss:3.309285879135132 norm:0.10763948410749435 max memory_allocated 22938.32177734375 
[2025-01-17 19:54:35 root] (abq_llm.py 328): INFO layer 31 iter 15 loss:3.293475389480591 norm:0.10099455714225769 max memory_allocated 22938.32177734375 
[2025-01-17 19:55:06 root] (abq_llm.py 328): INFO layer 31 iter 16 loss:3.2802789211273193 norm:0.09562557935714722 max memory_allocated 22938.32177734375 
[2025-01-17 19:55:37 root] (abq_llm.py 328): INFO layer 31 iter 17 loss:3.2736024856567383 norm:0.09528565406799316 max memory_allocated 22938.32177734375 
[2025-01-17 19:56:08 root] (abq_llm.py 328): INFO layer 31 iter 18 loss:3.26222825050354 norm:0.0953616350889206 max memory_allocated 22938.32177734375 
[2025-01-17 19:56:40 root] (abq_llm.py 328): INFO layer 31 iter 19 loss:3.2597312927246094 norm:0.09032964706420898 max memory_allocated 22938.32177734375 
[2025-01-17 19:56:49 root] (main.py 361): INFO 20382.11444211006
[2025-01-17 19:56:58 root] (main.py 114): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-01-17 19:58:08 root] (main.py 158): INFO wikitext2 : 18.122297286987305
[2025-01-17 19:58:08 root] (main.py 114): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-01-17 19:59:57 root] (main.py 158): INFO c4 : 24.06520652770996
[2025-01-17 21:22:01 root] (main.py 169): INFO {'wikitext2': 18.122297286987305, 'c4': 24.06520652770996, 'results': {'winogrande': {'acc': 0.5098658247829518, 'acc_stderr': 0.014049749833367592}, 'hellaswag': {'acc': 0.380601473809998, 'acc_stderr': 0.004845424524764025, 'acc_norm': 0.4647480581557459, 'acc_norm_stderr': 0.004977364364795593}}, 'versions': {'winogrande': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
