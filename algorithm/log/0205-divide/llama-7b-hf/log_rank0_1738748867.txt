[2025-02-05 09:47:47 root] (main_divide_blocks.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log/0205-divide/llama-7b-hf', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.2, similarity_threshold=0.999, sensitivity_threshold=0.01, max_block_size=3)
[2025-02-05 09:47:56 root] (main_divide_blocks.py 342): INFO === start quantization ===
[2025-02-05 09:47:56 root] (main_divide_blocks.py 348): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-05 09:47:56 root] (abq_llm_divide_blocks.py 61): INFO Starting ...
[2025-02-05 09:47:59 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-02-05 09:47:59 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001154010965533
[2025-02-05 09:47:59 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-02-05 09:48:00 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9814749445234027
[2025-02-05 09:48:00 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001379167804874
[2025-02-05 09:48:00 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-02-05 09:48:01 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9810646431786674
[2025-02-05 09:48:01 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001615941957291
[2025-02-05 09:48:01 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-02-05 09:48:02 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9854647261755807
[2025-02-05 09:48:02 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001369514917368
[2025-02-05 09:48:02 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-02-05 09:48:03 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997801950999669
[2025-02-05 09:48:03 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001373833304946
[2025-02-05 09:48:03 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-02-05 09:48:04 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9992428507123675
[2025-02-05 09:48:04 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500131731671684
[2025-02-05 09:48:04 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-02-05 09:48:05 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997492943491254
[2025-02-05 09:48:06 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001343899681799
[2025-02-05 09:48:06 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-02-05 09:48:07 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995527608054025
[2025-02-05 09:48:07 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500132350727577
[2025-02-05 09:48:07 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-02-05 09:48:08 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997730425425938
[2025-02-05 09:48:08 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001298084344663
[2025-02-05 09:48:08 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-02-05 09:48:09 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997132165091378
[2025-02-05 09:48:09 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001260918180803
[2025-02-05 09:48:09 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-02-05 09:48:10 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994315930775234
[2025-02-05 09:48:10 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001285443480294
[2025-02-05 09:48:10 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-02-05 09:48:11 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9988973311015538
[2025-02-05 09:48:12 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500134654595051
[2025-02-05 09:48:12 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-02-05 09:48:13 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9990161572183881
[2025-02-05 09:48:13 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001288511552888
[2025-02-05 09:48:13 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-02-05 09:48:14 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994031531470162
[2025-02-05 09:48:14 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001288158781224
[2025-02-05 09:48:14 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-02-05 09:48:15 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999086260795593
[2025-02-05 09:48:15 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001295739746476
[2025-02-05 09:48:15 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-02-05 09:48:16 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999410169465202
[2025-02-05 09:48:16 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001302741224353
[2025-02-05 09:48:16 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-02-05 09:48:18 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.998806927885328
[2025-02-05 09:48:18 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001327174462092
[2025-02-05 09:48:18 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-02-05 09:48:19 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999774728502546
[2025-02-05 09:48:19 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001312793871722
[2025-02-05 09:48:19 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-02-05 09:48:20 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998028108051845
[2025-02-05 09:48:20 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001298660310468
[2025-02-05 09:48:20 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-02-05 09:48:21 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9993489980697632
[2025-02-05 09:48:21 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001310690011053
[2025-02-05 09:48:21 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-02-05 09:48:22 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995535697255816
[2025-02-05 09:48:23 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001345538817727
[2025-02-05 09:48:23 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-02-05 09:48:24 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994866847991943
[2025-02-05 09:48:24 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001320280190265
[2025-02-05 09:48:24 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-02-05 09:48:25 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997681549617222
[2025-02-05 09:48:25 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001336308058348
[2025-02-05 09:48:25 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-02-05 09:48:26 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9990618995257786
[2025-02-05 09:48:26 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001333392420191
[2025-02-05 09:48:26 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-02-05 09:48:27 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998582431248256
[2025-02-05 09:48:27 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001349191965602
[2025-02-05 09:48:27 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-02-05 09:48:28 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994744317872184
[2025-02-05 09:48:29 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001382841127924
[2025-02-05 09:48:29 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-02-05 09:48:30 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996551530701774
[2025-02-05 09:48:30 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001396510568222
[2025-02-05 09:48:30 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-02-05 09:48:31 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998681034360614
[2025-02-05 09:48:31 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001414904156435
[2025-02-05 09:48:31 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-02-05 09:48:32 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996085677828107
[2025-02-05 09:48:32 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001404596638638
[2025-02-05 09:48:32 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-02-05 09:48:33 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996689302580697
[2025-02-05 09:48:33 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001427607784069
[2025-02-05 09:48:33 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-02-05 09:48:34 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998160345213754
[2025-02-05 09:48:35 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001433475785197
[2025-02-05 09:48:35 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-02-05 09:48:36 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9957310387066433
[2025-02-05 09:48:36 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001402993125806
[2025-02-05 09:48:36 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 0 ===
[2025-02-05 09:48:49 root] (abq_llm_divide_blocks.py 278): INFO layer 0 loss_mean: 0.0012097605504095554
[2025-02-05 09:48:49 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 1 ===
[2025-02-05 09:49:01 root] (abq_llm_divide_blocks.py 278): INFO layer 1 loss_mean: 0.0025464221835136414
[2025-02-05 09:49:01 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 2 ===
[2025-02-05 09:49:13 root] (abq_llm_divide_blocks.py 278): INFO layer 2 loss_mean: 0.247551828622818
[2025-02-05 09:49:13 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 3 ===
[2025-02-05 09:49:25 root] (abq_llm_divide_blocks.py 278): INFO layer 3 loss_mean: 0.0022920933552086353
[2025-02-05 09:49:25 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 4 ===
[2025-02-05 09:49:37 root] (abq_llm_divide_blocks.py 278): INFO layer 4 loss_mean: 0.009956404566764832
[2025-02-05 09:49:37 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 5 ===
[2025-02-05 09:49:49 root] (abq_llm_divide_blocks.py 278): INFO layer 5 loss_mean: 0.008886303752660751
[2025-02-05 09:49:49 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 6 ===
