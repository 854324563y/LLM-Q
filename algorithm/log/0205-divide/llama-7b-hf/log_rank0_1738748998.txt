[2025-02-05 09:49:58 root] (main_divide_blocks.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log/0205-divide/llama-7b-hf', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.2, similarity_threshold=0.999, sensitivity_threshold=0.01, max_block_size=3)
[2025-02-05 09:50:06 root] (main_divide_blocks.py 342): INFO === start quantization ===
[2025-02-05 09:50:06 root] (main_divide_blocks.py 348): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-05 09:50:06 root] (abq_llm_divide_blocks.py 61): INFO Starting ...
[2025-02-05 09:50:09 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-02-05 09:50:09 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001154010965533
[2025-02-05 09:50:09 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-02-05 09:50:10 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9814749445234027
[2025-02-05 09:50:10 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001379167804874
[2025-02-05 09:50:10 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-02-05 09:50:11 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9810646431786674
[2025-02-05 09:50:11 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001615941957291
[2025-02-05 09:50:11 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-02-05 09:50:12 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9854647261755807
[2025-02-05 09:50:12 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001369514917368
[2025-02-05 09:50:12 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-02-05 09:50:13 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997801950999669
[2025-02-05 09:50:13 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001373833304946
[2025-02-05 09:50:13 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-02-05 09:50:14 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9992428507123675
[2025-02-05 09:50:14 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500131731671684
[2025-02-05 09:50:14 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-02-05 09:50:15 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997492943491254
[2025-02-05 09:50:16 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001343899681799
[2025-02-05 09:50:16 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-02-05 09:50:17 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995527608054025
[2025-02-05 09:50:17 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500132350727577
[2025-02-05 09:50:17 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-02-05 09:50:18 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997730425425938
[2025-02-05 09:50:18 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001298084344663
[2025-02-05 09:50:18 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-02-05 09:50:19 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997132165091378
[2025-02-05 09:50:19 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001260918180803
[2025-02-05 09:50:19 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-02-05 09:50:20 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994315930775234
[2025-02-05 09:50:20 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001285443480294
[2025-02-05 09:50:20 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-02-05 09:50:21 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9988973311015538
[2025-02-05 09:50:21 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500134654595051
[2025-02-05 09:50:21 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-02-05 09:50:22 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9990161572183881
[2025-02-05 09:50:23 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001288511552888
[2025-02-05 09:50:23 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-02-05 09:50:24 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994031531470162
[2025-02-05 09:50:24 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001288158781224
[2025-02-05 09:50:24 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-02-05 09:50:25 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999086260795593
[2025-02-05 09:50:25 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001295739746476
[2025-02-05 09:50:25 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-02-05 09:50:26 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999410169465202
[2025-02-05 09:50:26 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001302741224353
[2025-02-05 09:50:26 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-02-05 09:50:27 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.998806927885328
[2025-02-05 09:50:27 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001327174462092
[2025-02-05 09:50:27 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-02-05 09:50:28 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999774728502546
[2025-02-05 09:50:28 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001312793871722
[2025-02-05 09:50:28 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-02-05 09:50:29 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998028108051845
[2025-02-05 09:50:30 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001298660310468
[2025-02-05 09:50:30 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-02-05 09:50:31 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9993489980697632
[2025-02-05 09:50:31 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001310690011053
[2025-02-05 09:50:31 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-02-05 09:50:32 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995535697255816
[2025-02-05 09:50:32 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001345538817727
[2025-02-05 09:50:32 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-02-05 09:50:33 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994866847991943
[2025-02-05 09:50:33 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001320280190265
[2025-02-05 09:50:33 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-02-05 09:50:34 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997681549617222
[2025-02-05 09:50:35 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001336308058348
[2025-02-05 09:50:35 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-02-05 09:50:35 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9990618995257786
[2025-02-05 09:50:36 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001333392420191
[2025-02-05 09:50:36 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-02-05 09:50:37 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998582431248256
[2025-02-05 09:50:37 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001349191965602
[2025-02-05 09:50:37 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-02-05 09:50:38 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994744317872184
[2025-02-05 09:50:38 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001382841127924
[2025-02-05 09:50:38 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-02-05 09:50:39 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996551530701774
[2025-02-05 09:50:39 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001396510568222
[2025-02-05 09:50:39 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-02-05 09:50:40 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998681034360614
[2025-02-05 09:50:40 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001414904156435
[2025-02-05 09:50:40 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-02-05 09:50:41 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996085677828107
[2025-02-05 09:50:42 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001404596638638
[2025-02-05 09:50:42 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-02-05 09:50:43 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996689302580697
[2025-02-05 09:50:43 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001427607784069
[2025-02-05 09:50:43 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-02-05 09:50:44 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998160345213754
[2025-02-05 09:50:44 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001433475785197
[2025-02-05 09:50:44 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-02-05 09:50:45 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9957310387066433
[2025-02-05 09:50:45 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001402993125806
[2025-02-05 09:50:45 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 0 ===
[2025-02-05 09:50:58 root] (abq_llm_divide_blocks.py 278): INFO layer 0 loss_mean: 0.0012097605504095554
[2025-02-05 09:50:58 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 1 ===
[2025-02-05 09:51:09 root] (abq_llm_divide_blocks.py 278): INFO layer 1 loss_mean: 0.0025464221835136414
[2025-02-05 09:51:09 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 2 ===
[2025-02-05 09:51:21 root] (abq_llm_divide_blocks.py 278): INFO layer 2 loss_mean: 0.247551828622818
[2025-02-05 09:51:21 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 3 ===
[2025-02-05 09:51:32 root] (abq_llm_divide_blocks.py 278): INFO layer 3 loss_mean: 0.0022920933552086353
[2025-02-05 09:51:32 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 4 ===
[2025-02-05 09:51:44 root] (abq_llm_divide_blocks.py 278): INFO layer 4 loss_mean: 0.009956404566764832
[2025-02-05 09:51:44 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 5 ===
[2025-02-05 09:51:55 root] (abq_llm_divide_blocks.py 278): INFO layer 5 loss_mean: 0.008886303752660751
[2025-02-05 09:51:55 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 6 ===
[2025-02-05 09:52:07 root] (abq_llm_divide_blocks.py 278): INFO layer 6 loss_mean: 0.008100233972072601
[2025-02-05 09:52:07 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 7 ===
[2025-02-05 09:52:18 root] (abq_llm_divide_blocks.py 278): INFO layer 7 loss_mean: 0.011704066768288612
[2025-02-05 09:52:18 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 8 ===
[2025-02-05 09:52:30 root] (abq_llm_divide_blocks.py 278): INFO layer 8 loss_mean: 0.013185225427150726
[2025-02-05 09:52:30 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 9 ===
[2025-02-05 09:52:41 root] (abq_llm_divide_blocks.py 278): INFO layer 9 loss_mean: 0.01704569160938263
[2025-02-05 09:52:41 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 10 ===
[2025-02-05 09:52:52 root] (abq_llm_divide_blocks.py 278): INFO layer 10 loss_mean: 0.021331528201699257
[2025-02-05 09:52:52 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 11 ===
[2025-02-05 09:53:04 root] (abq_llm_divide_blocks.py 278): INFO layer 11 loss_mean: 0.024459155276417732
[2025-02-05 09:53:04 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 12 ===
[2025-02-05 09:53:15 root] (abq_llm_divide_blocks.py 278): INFO layer 12 loss_mean: 0.02500472031533718
[2025-02-05 09:53:15 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 13 ===
[2025-02-05 09:53:27 root] (abq_llm_divide_blocks.py 278): INFO layer 13 loss_mean: 0.029626592993736267
[2025-02-05 09:53:27 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 14 ===
[2025-02-05 09:53:38 root] (abq_llm_divide_blocks.py 278): INFO layer 14 loss_mean: 0.04032508656382561
[2025-02-05 09:53:38 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 15 ===
[2025-02-05 09:53:49 root] (abq_llm_divide_blocks.py 278): INFO layer 15 loss_mean: 0.041023582220077515
[2025-02-05 09:53:49 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 16 ===
[2025-02-05 09:54:01 root] (abq_llm_divide_blocks.py 278): INFO layer 16 loss_mean: 0.06302522867918015
[2025-02-05 09:54:01 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 17 ===
[2025-02-05 09:54:12 root] (abq_llm_divide_blocks.py 278): INFO layer 17 loss_mean: 0.059119030833244324
[2025-02-05 09:54:12 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 18 ===
[2025-02-05 09:54:24 root] (abq_llm_divide_blocks.py 278): INFO layer 18 loss_mean: 0.057874131947755814
[2025-02-05 09:54:24 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 19 ===
[2025-02-05 09:54:35 root] (abq_llm_divide_blocks.py 278): INFO layer 19 loss_mean: 0.05911997705698013
[2025-02-05 09:54:35 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 20 ===
[2025-02-05 09:54:47 root] (abq_llm_divide_blocks.py 278): INFO layer 20 loss_mean: 0.0739775151014328
[2025-02-05 09:54:47 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 21 ===
[2025-02-05 09:54:58 root] (abq_llm_divide_blocks.py 278): INFO layer 21 loss_mean: 0.07325029373168945
[2025-02-05 09:54:58 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 22 ===
[2025-02-05 09:55:10 root] (abq_llm_divide_blocks.py 278): INFO layer 22 loss_mean: 0.07932534068822861
[2025-02-05 09:55:10 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 23 ===
[2025-02-05 09:55:21 root] (abq_llm_divide_blocks.py 278): INFO layer 23 loss_mean: 0.09173044562339783
[2025-02-05 09:55:21 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 24 ===
[2025-02-05 09:55:33 root] (abq_llm_divide_blocks.py 278): INFO layer 24 loss_mean: 0.10137145221233368
[2025-02-05 09:55:33 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 25 ===
[2025-02-05 09:55:44 root] (abq_llm_divide_blocks.py 278): INFO layer 25 loss_mean: 0.1106717437505722
[2025-02-05 09:55:44 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 26 ===
[2025-02-05 09:55:55 root] (abq_llm_divide_blocks.py 278): INFO layer 26 loss_mean: 0.1326705366373062
[2025-02-05 09:55:55 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 27 ===
[2025-02-05 09:56:07 root] (abq_llm_divide_blocks.py 278): INFO layer 27 loss_mean: 0.16148196160793304
[2025-02-05 09:56:07 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 28 ===
[2025-02-05 09:56:18 root] (abq_llm_divide_blocks.py 278): INFO layer 28 loss_mean: 0.23193392157554626
[2025-02-05 09:56:18 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 29 ===
[2025-02-05 09:56:30 root] (abq_llm_divide_blocks.py 278): INFO layer 29 loss_mean: 0.31383469700813293
[2025-02-05 09:56:30 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 30 ===
[2025-02-05 09:56:41 root] (abq_llm_divide_blocks.py 278): INFO layer 30 loss_mean: 1.0643645524978638
[2025-02-05 09:56:41 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 31 ===
[2025-02-05 09:56:52 root] (abq_llm_divide_blocks.py 278): INFO layer 31 loss_mean: 2.111400842666626
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-1: size=1, error_sum=0.0012, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 1-2: size=1, error_sum=0.0025, min_similarity=0.9815, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-3: size=1, error_sum=0.2476, min_similarity=0.9811, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 3-6: size=3, error_sum=0.0211, min_similarity=0.9992, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-9: size=3, error_sum=0.0330, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 9-11: size=2, error_sum=0.0384, min_similarity=0.9994, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 11-14: size=3, error_sum=0.0791, min_similarity=0.9990, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 14-16: size=2, error_sum=0.0813, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 16-19: size=3, error_sum=0.1800, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 19-21: size=2, error_sum=0.1331, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 21-23: size=2, error_sum=0.1526, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 23-25: size=2, error_sum=0.1931, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 25-26: size=1, error_sum=0.1107, min_similarity=0.9995, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 26-27: size=1, error_sum=0.1327, min_similarity=0.9997, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 27-28: size=1, error_sum=0.1615, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 28-29: size=1, error_sum=0.2319, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 29-30: size=1, error_sum=0.3138, min_similarity=0.9997, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-31: size=1, error_sum=1.0644, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 110): INFO Block 31-32: size=1, error_sum=2.1114, min_similarity=0.9957, max_sensitivity_diff=0.0000
[2025-02-05 09:56:52 root] (abq_llm_divide_blocks.py 294): INFO blocks: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 11), (11, 14), (14, 16), (16, 19), (19, 21), (21, 23), (23, 25), (25, 26), (26, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32)]
[2025-02-05 09:56:53 root] (main_divide_blocks.py 371): INFO 406.288108587265
