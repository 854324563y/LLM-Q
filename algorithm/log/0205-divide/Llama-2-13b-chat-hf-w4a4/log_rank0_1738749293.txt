[2025-02-05 09:54:53 root] (main_divide_blocks.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-chat-hf', cache_dir='./cache', output_dir='./log/0205-divide/Llama-2-13b-chat-hf-w4a4', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.2, similarity_threshold=0.999, sensitivity_threshold=0.01, max_block_size=3)
[2025-02-05 09:54:56 root] (main_divide_blocks.py 342): INFO === start quantization ===
[2025-02-05 09:54:56 root] (main_divide_blocks.py 348): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-05 09:54:56 root] (abq_llm_divide_blocks.py 61): INFO Starting ...
[2025-02-05 09:55:09 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-02-05 09:55:11 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000398189408968
[2025-02-05 09:55:11 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-02-05 09:55:13 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9135928324290684
[2025-02-05 09:55:13 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000680024458328
[2025-02-05 09:55:13 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-02-05 09:55:16 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9694740431649345
[2025-02-05 09:55:16 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000866983082378
[2025-02-05 09:55:16 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-02-05 09:55:19 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9986620885985238
[2025-02-05 09:55:19 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000918136995431
[2025-02-05 09:55:19 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-02-05 09:55:21 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9989175711359296
[2025-02-05 09:55:22 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000862853861181
[2025-02-05 09:55:22 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-02-05 09:55:24 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996176958084106
[2025-02-05 09:55:25 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000850608891576
[2025-02-05 09:55:25 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-02-05 09:55:27 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994363614491054
[2025-02-05 09:55:28 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000870686240142
[2025-02-05 09:55:28 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-02-05 09:55:30 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998588562011719
[2025-02-05 09:55:30 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000886869181862
[2025-02-05 09:55:30 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-02-05 09:55:33 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998424308640617
[2025-02-05 09:55:33 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000875583122656
[2025-02-05 09:55:33 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-02-05 09:55:36 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998841285705566
[2025-02-05 09:55:36 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000870901890164
[2025-02-05 09:55:36 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-02-05 09:55:39 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998882157461983
[2025-02-05 09:55:39 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000870765312129
[2025-02-05 09:55:39 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-02-05 09:55:42 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996410182544163
[2025-02-05 09:55:42 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000893909077045
[2025-02-05 09:55:42 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-02-05 09:55:45 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995631234986442
[2025-02-05 09:55:45 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000876287981029
[2025-02-05 09:55:45 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-02-05 09:55:48 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994033745356968
[2025-02-05 09:55:48 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000862194524315
[2025-02-05 09:55:48 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-02-05 09:55:51 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994947910308838
[2025-02-05 09:55:51 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000875270605735
[2025-02-05 09:55:51 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-02-05 09:55:54 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995928406715393
[2025-02-05 09:55:55 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000883308387045
[2025-02-05 09:55:55 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-02-05 09:55:57 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999485952513558
[2025-02-05 09:55:58 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000877563606035
[2025-02-05 09:55:58 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-02-05 09:56:00 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999198402677264
[2025-02-05 09:56:01 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000875834637951
[2025-02-05 09:56:01 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-02-05 09:56:03 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996755719184875
[2025-02-05 09:56:04 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000886171797457
[2025-02-05 09:56:04 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-02-05 09:56:06 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999713659286499
[2025-02-05 09:56:06 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000868810253022
[2025-02-05 09:56:06 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-02-05 09:56:09 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999053137642997
[2025-02-05 09:56:10 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000878435154513
[2025-02-05 09:56:10 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-02-05 09:56:12 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996265258107867
[2025-02-05 09:56:13 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000874598250103
[2025-02-05 09:56:13 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-02-05 09:56:15 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999066948890686
[2025-02-05 09:56:16 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500088621444808
[2025-02-05 09:56:16 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-02-05 09:56:18 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998137695448739
[2025-02-05 09:56:19 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000875030090647
[2025-02-05 09:56:19 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-02-05 09:56:21 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999223862375531
[2025-02-05 09:56:22 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000882095198068
[2025-02-05 09:56:22 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-02-05 09:56:25 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998070597648621
[2025-02-05 09:56:25 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000888791899856
[2025-02-05 09:56:25 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-02-05 09:56:28 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999918418271201
[2025-02-05 09:56:28 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000897883940357
[2025-02-05 09:56:28 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-02-05 09:56:31 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999935405594962
[2025-02-05 09:56:31 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000897022184281
[2025-02-05 09:56:31 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-02-05 09:56:34 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999742933682033
[2025-02-05 09:56:34 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000899066689846
[2025-02-05 09:56:34 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-02-05 09:56:37 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998492853982108
[2025-02-05 09:56:37 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000910369245187
[2025-02-05 09:56:37 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-02-05 09:56:40 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9994845816067287
[2025-02-05 09:56:40 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000920124664852
[2025-02-05 09:56:40 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-02-05 09:56:43 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996834908212934
[2025-02-05 09:56:44 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000917806925264
[2025-02-05 09:56:44 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 32 similarity and sensitivity ===
[2025-02-05 09:56:46 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999538004398346
[2025-02-05 09:56:47 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000918196893711
[2025-02-05 09:56:47 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 33 similarity and sensitivity ===
[2025-02-05 09:56:50 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998883179255894
[2025-02-05 09:56:50 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.50009132784328
[2025-02-05 09:56:50 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 34 similarity and sensitivity ===
[2025-02-05 09:56:52 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996332100459507
[2025-02-05 09:56:53 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000911903552304
[2025-02-05 09:56:53 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 35 similarity and sensitivity ===
[2025-02-05 09:56:55 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999587961605617
[2025-02-05 09:56:56 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000911571150192
[2025-02-05 09:56:56 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 36 similarity and sensitivity ===
[2025-02-05 09:56:58 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998700278145927
[2025-02-05 09:56:59 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000923536572264
[2025-02-05 09:56:59 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 37 similarity and sensitivity ===
[2025-02-05 09:57:02 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9991466062409537
[2025-02-05 09:57:02 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.50009082241017
[2025-02-05 09:57:02 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 38 similarity and sensitivity ===
[2025-02-05 09:57:05 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9993736488478524
[2025-02-05 09:57:05 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500092997678718
[2025-02-05 09:57:05 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 39 similarity and sensitivity ===
[2025-02-05 09:57:08 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9974761775561741
[2025-02-05 09:57:08 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5000915086240036
[2025-02-05 09:57:08 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 0 ===
[2025-02-05 09:57:24 root] (abq_llm_divide_blocks.py 278): INFO layer 0 loss_mean: 0.000959381926804781
[2025-02-05 09:57:24 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 1 ===
[2025-02-05 09:57:38 root] (abq_llm_divide_blocks.py 278): INFO layer 1 loss_mean: 0.0016495169838890433
[2025-02-05 09:57:38 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 2 ===
[2025-02-05 09:57:53 root] (abq_llm_divide_blocks.py 278): INFO layer 2 loss_mean: 0.0027094988618046045
[2025-02-05 09:57:53 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 3 ===
[2025-02-05 09:58:08 root] (abq_llm_divide_blocks.py 278): INFO layer 3 loss_mean: 0.05304201692342758
[2025-02-05 09:58:08 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 4 ===
[2025-02-05 09:58:23 root] (abq_llm_divide_blocks.py 278): INFO layer 4 loss_mean: 0.0014048560988157988
[2025-02-05 09:58:23 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 5 ===
[2025-02-05 09:58:38 root] (abq_llm_divide_blocks.py 278): INFO layer 5 loss_mean: 0.0021762626711279154
[2025-02-05 09:58:39 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 6 ===
[2025-02-05 09:58:54 root] (abq_llm_divide_blocks.py 278): INFO layer 6 loss_mean: 0.0027514216490089893
[2025-02-05 09:58:54 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 7 ===
[2025-02-05 09:59:09 root] (abq_llm_divide_blocks.py 278): INFO layer 7 loss_mean: 0.004389768000692129
[2025-02-05 09:59:09 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 8 ===
[2025-02-05 09:59:24 root] (abq_llm_divide_blocks.py 278): INFO layer 8 loss_mean: 0.006612609606236219
[2025-02-05 09:59:24 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 9 ===
[2025-02-05 09:59:39 root] (abq_llm_divide_blocks.py 278): INFO layer 9 loss_mean: 0.010297320783138275
[2025-02-05 09:59:39 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 10 ===
[2025-02-05 09:59:54 root] (abq_llm_divide_blocks.py 278): INFO layer 10 loss_mean: 0.008371757343411446
[2025-02-05 09:59:54 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 11 ===
[2025-02-05 10:00:09 root] (abq_llm_divide_blocks.py 278): INFO layer 11 loss_mean: 0.009550116956233978
[2025-02-05 10:00:09 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 12 ===
[2025-02-05 10:00:24 root] (abq_llm_divide_blocks.py 278): INFO layer 12 loss_mean: 0.013559093698859215
[2025-02-05 10:00:24 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 13 ===
[2025-02-05 10:00:39 root] (abq_llm_divide_blocks.py 278): INFO layer 13 loss_mean: 0.013427786529064178
[2025-02-05 10:00:39 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 14 ===
[2025-02-05 10:00:55 root] (abq_llm_divide_blocks.py 278): INFO layer 14 loss_mean: 0.013896209187805653
[2025-02-05 10:00:55 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 15 ===
[2025-02-05 10:01:10 root] (abq_llm_divide_blocks.py 278): INFO layer 15 loss_mean: 0.016910504549741745
[2025-02-05 10:01:10 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 16 ===
[2025-02-05 10:01:25 root] (abq_llm_divide_blocks.py 278): INFO layer 16 loss_mean: 0.020768901333212852
[2025-02-05 10:01:25 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 17 ===
[2025-02-05 10:01:40 root] (abq_llm_divide_blocks.py 278): INFO layer 17 loss_mean: 0.023888397961854935
[2025-02-05 10:01:40 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 18 ===
[2025-02-05 10:01:55 root] (abq_llm_divide_blocks.py 278): INFO layer 18 loss_mean: 0.028076153248548508
[2025-02-05 10:01:55 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 19 ===
[2025-02-05 10:02:10 root] (abq_llm_divide_blocks.py 278): INFO layer 19 loss_mean: 0.026723647490143776
[2025-02-05 10:02:10 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 20 ===
[2025-02-05 10:02:25 root] (abq_llm_divide_blocks.py 278): INFO layer 20 loss_mean: 0.03223009034991264
[2025-02-05 10:02:25 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 21 ===
[2025-02-05 10:02:41 root] (abq_llm_divide_blocks.py 278): INFO layer 21 loss_mean: 0.03416736051440239
[2025-02-05 10:02:41 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 22 ===
[2025-02-05 10:02:56 root] (abq_llm_divide_blocks.py 278): INFO layer 22 loss_mean: 0.034770458936691284
[2025-02-05 10:02:56 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 23 ===
[2025-02-05 10:03:11 root] (abq_llm_divide_blocks.py 278): INFO layer 23 loss_mean: 0.03457963094115257
[2025-02-05 10:03:11 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 24 ===
[2025-02-05 10:03:26 root] (abq_llm_divide_blocks.py 278): INFO layer 24 loss_mean: 0.03188752382993698
[2025-02-05 10:03:26 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 25 ===
[2025-02-05 10:03:41 root] (abq_llm_divide_blocks.py 278): INFO layer 25 loss_mean: 0.030319280922412872
[2025-02-05 10:03:41 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 26 ===
[2025-02-05 10:03:56 root] (abq_llm_divide_blocks.py 278): INFO layer 26 loss_mean: 0.036490388214588165
[2025-02-05 10:03:56 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 27 ===
[2025-02-05 10:04:12 root] (abq_llm_divide_blocks.py 278): INFO layer 27 loss_mean: 0.031500451266765594
[2025-02-05 10:04:12 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 28 ===
[2025-02-05 10:04:27 root] (abq_llm_divide_blocks.py 278): INFO layer 28 loss_mean: 0.04073040932416916
[2025-02-05 10:04:28 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 29 ===
[2025-02-05 10:04:43 root] (abq_llm_divide_blocks.py 278): INFO layer 29 loss_mean: 0.03504515439271927
[2025-02-05 10:04:43 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 30 ===
[2025-02-05 10:04:58 root] (abq_llm_divide_blocks.py 278): INFO layer 30 loss_mean: 0.04160061851143837
[2025-02-05 10:04:58 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 31 ===
[2025-02-05 10:05:13 root] (abq_llm_divide_blocks.py 278): INFO layer 31 loss_mean: 0.041470762342214584
[2025-02-05 10:05:13 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 32 ===
[2025-02-05 10:05:28 root] (abq_llm_divide_blocks.py 278): INFO layer 32 loss_mean: 0.05150238052010536
[2025-02-05 10:05:28 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 33 ===
[2025-02-05 10:05:44 root] (abq_llm_divide_blocks.py 278): INFO layer 33 loss_mean: 0.059540919959545135
[2025-02-05 10:05:44 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 34 ===
[2025-02-05 10:05:59 root] (abq_llm_divide_blocks.py 278): INFO layer 34 loss_mean: 0.0721321627497673
[2025-02-05 10:05:59 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 35 ===
[2025-02-05 10:06:14 root] (abq_llm_divide_blocks.py 278): INFO layer 35 loss_mean: 0.09205029904842377
[2025-02-05 10:06:14 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 36 ===
[2025-02-05 10:06:29 root] (abq_llm_divide_blocks.py 278): INFO layer 36 loss_mean: 0.14995115995407104
[2025-02-05 10:06:29 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 37 ===
[2025-02-05 10:06:44 root] (abq_llm_divide_blocks.py 278): INFO layer 37 loss_mean: 0.2620185613632202
[2025-02-05 10:06:45 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 38 ===
[2025-02-05 10:07:00 root] (abq_llm_divide_blocks.py 278): INFO layer 38 loss_mean: 0.338545560836792
[2025-02-05 10:07:00 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 39 ===
[2025-02-05 10:07:15 root] (abq_llm_divide_blocks.py 278): INFO layer 39 loss_mean: 1.4358463287353516
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-1: size=1, error_sum=0.0010, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 1-2: size=1, error_sum=0.0016, min_similarity=0.9136, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-3: size=1, error_sum=0.0027, min_similarity=0.9695, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 3-6: size=3, error_sum=0.0566, min_similarity=0.9989, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-9: size=3, error_sum=0.0138, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 9-12: size=3, error_sum=0.0282, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 12-15: size=3, error_sum=0.0409, min_similarity=0.9994, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 15-18: size=3, error_sum=0.0616, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 18-21: size=3, error_sum=0.0870, min_similarity=0.9997, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 21-24: size=3, error_sum=0.1035, min_similarity=0.9991, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 24-27: size=3, error_sum=0.0987, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 27-30: size=3, error_sum=0.1073, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-33: size=3, error_sum=0.1346, min_similarity=0.9995, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 33-35: size=2, error_sum=0.1317, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 35-36: size=1, error_sum=0.0921, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 36-37: size=1, error_sum=0.1500, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 37-38: size=1, error_sum=0.2620, min_similarity=0.9991, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 38-39: size=1, error_sum=0.3385, min_similarity=0.9994, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 quantize.utils_divide] (utils_divide.py 110): INFO Block 39-40: size=1, error_sum=1.4358, min_similarity=0.9975, max_sensitivity_diff=0.0000
[2025-02-05 10:07:15 root] (abq_llm_divide_blocks.py 294): INFO blocks: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 9), (9, 12), (12, 15), (15, 18), (18, 21), (21, 24), (24, 27), (27, 30), (30, 33), (33, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-02-05 10:07:15 root] (main_divide_blocks.py 371): INFO 739.341805934906
