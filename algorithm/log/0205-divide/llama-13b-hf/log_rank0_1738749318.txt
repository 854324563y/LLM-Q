[2025-02-05 09:55:18 root] (main_divide_blocks.py 274): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log/0205-divide/llama-13b-hf', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=10, let=False, lwc=False, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, error_threshold=0.2, similarity_threshold=0.999, sensitivity_threshold=0.01, max_block_size=3)
[2025-02-05 10:02:47 root] (main_divide_blocks.py 342): INFO === start quantization ===
[2025-02-05 10:02:47 root] (main_divide_blocks.py 348): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-02-05 10:02:47 root] (abq_llm_divide_blocks.py 61): INFO Starting ...
[2025-02-05 10:02:49 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 0 similarity and sensitivity ===
[2025-02-05 10:02:50 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500089599859584
[2025-02-05 10:02:50 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 1 similarity and sensitivity ===
[2025-02-05 10:02:51 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9615949051720756
[2025-02-05 10:02:51 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001250170093238
[2025-02-05 10:02:51 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 2 similarity and sensitivity ===
[2025-02-05 10:02:53 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.981979250907898
[2025-02-05 10:02:53 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001436235020391
[2025-02-05 10:02:53 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 3 similarity and sensitivity ===
[2025-02-05 10:02:55 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9900599632944379
[2025-02-05 10:02:55 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001256476532221
[2025-02-05 10:02:55 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 4 similarity and sensitivity ===
[2025-02-05 10:02:57 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999608039855957
[2025-02-05 10:02:57 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001254175741523
[2025-02-05 10:02:57 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 5 similarity and sensitivity ===
[2025-02-05 10:02:59 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996611731392997
[2025-02-05 10:02:59 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001212771410396
[2025-02-05 10:02:59 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 6 similarity and sensitivity ===
[2025-02-05 10:03:01 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998395698411124
[2025-02-05 10:03:01 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001237344422614
[2025-02-05 10:03:01 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 7 similarity and sensitivity ===
[2025-02-05 10:03:02 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9992820194789341
[2025-02-05 10:03:03 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001239694978784
[2025-02-05 10:03:03 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 8 similarity and sensitivity ===
[2025-02-05 10:03:04 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9989871723311288
[2025-02-05 10:03:05 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001194695688616
[2025-02-05 10:03:05 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 9 similarity and sensitivity ===
[2025-02-05 10:03:06 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.99907933814185
[2025-02-05 10:03:06 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001236503249206
[2025-02-05 10:03:06 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 10 similarity and sensitivity ===
[2025-02-05 10:03:08 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996614541326251
[2025-02-05 10:03:08 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001220883764843
[2025-02-05 10:03:08 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 11 similarity and sensitivity ===
[2025-02-05 10:03:10 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999208705765861
[2025-02-05 10:03:10 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001232175561852
[2025-02-05 10:03:10 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 12 similarity and sensitivity ===
[2025-02-05 10:03:12 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999057565416608
[2025-02-05 10:03:12 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001242278146266
[2025-02-05 10:03:12 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 13 similarity and sensitivity ===
[2025-02-05 10:03:14 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9992532985551017
[2025-02-05 10:03:14 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001219478668372
[2025-02-05 10:03:14 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 14 similarity and sensitivity ===
[2025-02-05 10:03:15 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998063615390232
[2025-02-05 10:03:16 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001227576030184
[2025-02-05 10:03:16 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 15 similarity and sensitivity ===
[2025-02-05 10:03:17 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998798029763358
[2025-02-05 10:03:18 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500122239373592
[2025-02-05 10:03:18 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 16 similarity and sensitivity ===
[2025-02-05 10:03:20 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997897403580802
[2025-02-05 10:03:20 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001222505422903
[2025-02-05 10:03:20 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 17 similarity and sensitivity ===
[2025-02-05 10:03:22 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999437417302813
[2025-02-05 10:03:22 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500123254715226
[2025-02-05 10:03:22 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 18 similarity and sensitivity ===
[2025-02-05 10:03:23 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997287307466779
[2025-02-05 10:03:24 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001219176813854
[2025-02-05 10:03:24 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 19 similarity and sensitivity ===
[2025-02-05 10:03:25 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998310378619603
[2025-02-05 10:03:26 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001214441324684
[2025-02-05 10:03:26 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 20 similarity and sensitivity ===
[2025-02-05 10:03:27 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998751538140433
[2025-02-05 10:03:28 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001223804559609
[2025-02-05 10:03:28 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 21 similarity and sensitivity ===
[2025-02-05 10:03:29 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9995440074375698
[2025-02-05 10:03:29 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001223224603456
[2025-02-05 10:03:29 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 22 similarity and sensitivity ===
[2025-02-05 10:03:31 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999141778264727
[2025-02-05 10:03:31 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001217706245202
[2025-02-05 10:03:31 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 23 similarity and sensitivity ===
[2025-02-05 10:03:33 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996859942163739
[2025-02-05 10:03:33 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001229617299969
[2025-02-05 10:03:33 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 24 similarity and sensitivity ===
[2025-02-05 10:03:35 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998900975499835
[2025-02-05 10:03:35 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001212554189812
[2025-02-05 10:03:35 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 25 similarity and sensitivity ===
[2025-02-05 10:03:37 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998655489512852
[2025-02-05 10:03:37 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001221549663561
[2025-02-05 10:03:37 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 26 similarity and sensitivity ===
[2025-02-05 10:03:39 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999262179647174
[2025-02-05 10:03:39 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500122459891847
[2025-02-05 10:03:39 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 27 similarity and sensitivity ===
[2025-02-05 10:03:40 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999001026153564
[2025-02-05 10:03:41 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001227541100391
[2025-02-05 10:03:41 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 28 similarity and sensitivity ===
[2025-02-05 10:03:42 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997256653649467
[2025-02-05 10:03:43 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001223321858058
[2025-02-05 10:03:43 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 29 similarity and sensitivity ===
[2025-02-05 10:03:44 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999829820224217
[2025-02-05 10:03:44 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001236927832489
[2025-02-05 10:03:44 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 30 similarity and sensitivity ===
[2025-02-05 10:03:46 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999498214040484
[2025-02-05 10:03:46 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001235620543595
[2025-02-05 10:03:46 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 31 similarity and sensitivity ===
[2025-02-05 10:03:48 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998759201594761
[2025-02-05 10:03:48 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500123784250138
[2025-02-05 10:03:48 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 32 similarity and sensitivity ===
[2025-02-05 10:03:50 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999594432967049
[2025-02-05 10:03:50 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001241972112231
[2025-02-05 10:03:50 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 33 similarity and sensitivity ===
[2025-02-05 10:03:51 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9999618870871407
[2025-02-05 10:03:52 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001252765819019
[2025-02-05 10:03:52 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 34 similarity and sensitivity ===
[2025-02-05 10:03:53 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.999891860144479
[2025-02-05 10:03:54 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500126420474624
[2025-02-05 10:03:54 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 35 similarity and sensitivity ===
[2025-02-05 10:03:55 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997418693133763
[2025-02-05 10:03:55 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001241574051891
[2025-02-05 10:03:55 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 36 similarity and sensitivity ===
[2025-02-05 10:03:57 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9996395707130432
[2025-02-05 10:03:57 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001245335420312
[2025-02-05 10:03:57 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 37 similarity and sensitivity ===
[2025-02-05 10:03:59 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9998987827982221
[2025-02-05 10:03:59 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.50012544531561
[2025-02-05 10:03:59 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 38 similarity and sensitivity ===
[2025-02-05 10:04:01 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9997256909097944
[2025-02-05 10:04:01 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.5001252154559621
[2025-02-05 10:04:01 root] (abq_llm_divide_blocks.py 217): INFO === Start compute layer 39 similarity and sensitivity ===
[2025-02-05 10:04:02 quantize.utils_divide] (utils_divide.py 255): INFO Layer similarity: 0.9977353811264038
[2025-02-05 10:04:03 quantize.utils_divide] (utils_divide.py 306): INFO Hessian sensitivity: 0.500119685285713
[2025-02-05 10:04:03 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 0 ===
[2025-02-05 10:04:18 root] (abq_llm_divide_blocks.py 278): INFO layer 0 loss_mean: 0.0021795970387756824
[2025-02-05 10:04:18 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 1 ===
[2025-02-05 10:04:33 root] (abq_llm_divide_blocks.py 278): INFO layer 1 loss_mean: 0.004646058194339275
[2025-02-05 10:04:33 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 2 ===
[2025-02-05 10:04:48 root] (abq_llm_divide_blocks.py 278): INFO layer 2 loss_mean: 1.2611465454101562
[2025-02-05 10:04:48 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 3 ===
[2025-02-05 10:05:03 root] (abq_llm_divide_blocks.py 278): INFO layer 3 loss_mean: 0.005112253595143557
[2025-02-05 10:05:03 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 4 ===
[2025-02-05 10:05:17 root] (abq_llm_divide_blocks.py 278): INFO layer 4 loss_mean: 0.012168350629508495
[2025-02-05 10:05:17 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 5 ===
[2025-02-05 10:05:32 root] (abq_llm_divide_blocks.py 278): INFO layer 5 loss_mean: 0.01755613088607788
[2025-02-05 10:05:32 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 6 ===
[2025-02-05 10:05:48 root] (abq_llm_divide_blocks.py 278): INFO layer 6 loss_mean: 2.991795539855957
[2025-02-05 10:05:48 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 7 ===
[2025-02-05 10:06:03 root] (abq_llm_divide_blocks.py 278): INFO layer 7 loss_mean: 0.012549489736557007
[2025-02-05 10:06:03 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 8 ===
[2025-02-05 10:06:18 root] (abq_llm_divide_blocks.py 278): INFO layer 8 loss_mean: 0.014674169942736626
[2025-02-05 10:06:18 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 9 ===
[2025-02-05 10:06:33 root] (abq_llm_divide_blocks.py 278): INFO layer 9 loss_mean: 0.01600712724030018
[2025-02-05 10:06:33 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 10 ===
[2025-02-05 10:06:48 root] (abq_llm_divide_blocks.py 278): INFO layer 10 loss_mean: 0.020644238218665123
[2025-02-05 10:06:48 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 11 ===
[2025-02-05 10:07:03 root] (abq_llm_divide_blocks.py 278): INFO layer 11 loss_mean: 0.02370295114815235
[2025-02-05 10:07:03 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 12 ===
[2025-02-05 10:07:18 root] (abq_llm_divide_blocks.py 278): INFO layer 12 loss_mean: 0.026408614590764046
[2025-02-05 10:07:18 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 13 ===
[2025-02-05 10:07:34 root] (abq_llm_divide_blocks.py 278): INFO layer 13 loss_mean: 0.030762219801545143
[2025-02-05 10:07:34 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 14 ===
[2025-02-05 10:07:49 root] (abq_llm_divide_blocks.py 278): INFO layer 14 loss_mean: 0.040674060583114624
[2025-02-05 10:07:49 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 15 ===
[2025-02-05 10:08:04 root] (abq_llm_divide_blocks.py 278): INFO layer 15 loss_mean: 0.03686928004026413
[2025-02-05 10:08:04 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 16 ===
[2025-02-05 10:08:19 root] (abq_llm_divide_blocks.py 278): INFO layer 16 loss_mean: 0.046858105808496475
[2025-02-05 10:08:19 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 17 ===
[2025-02-05 10:08:34 root] (abq_llm_divide_blocks.py 278): INFO layer 17 loss_mean: 0.05634862184524536
[2025-02-05 10:08:34 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 18 ===
[2025-02-05 10:08:50 root] (abq_llm_divide_blocks.py 278): INFO layer 18 loss_mean: 0.06484071910381317
[2025-02-05 10:08:50 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 19 ===
[2025-02-05 10:09:05 root] (abq_llm_divide_blocks.py 278): INFO layer 19 loss_mean: 0.07021848112344742
[2025-02-05 10:09:05 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 20 ===
[2025-02-05 10:09:20 root] (abq_llm_divide_blocks.py 278): INFO layer 20 loss_mean: 0.08611972630023956
[2025-02-05 10:09:20 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 21 ===
[2025-02-05 10:09:35 root] (abq_llm_divide_blocks.py 278): INFO layer 21 loss_mean: 0.09225122630596161
[2025-02-05 10:09:35 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 22 ===
[2025-02-05 10:09:50 root] (abq_llm_divide_blocks.py 278): INFO layer 22 loss_mean: 0.09806659817695618
[2025-02-05 10:09:50 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 23 ===
[2025-02-05 10:10:06 root] (abq_llm_divide_blocks.py 278): INFO layer 23 loss_mean: 0.09001991152763367
[2025-02-05 10:10:06 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 24 ===
[2025-02-05 10:10:21 root] (abq_llm_divide_blocks.py 278): INFO layer 24 loss_mean: 0.09221549332141876
[2025-02-05 10:10:21 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 25 ===
[2025-02-05 10:10:36 root] (abq_llm_divide_blocks.py 278): INFO layer 25 loss_mean: 0.09701679646968842
[2025-02-05 10:10:36 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 26 ===
[2025-02-05 10:10:51 root] (abq_llm_divide_blocks.py 278): INFO layer 26 loss_mean: 0.10018263757228851
[2025-02-05 10:10:51 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 27 ===
[2025-02-05 10:11:06 root] (abq_llm_divide_blocks.py 278): INFO layer 27 loss_mean: 0.10164619237184525
[2025-02-05 10:11:06 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 28 ===
[2025-02-05 10:11:22 root] (abq_llm_divide_blocks.py 278): INFO layer 28 loss_mean: 0.12604254484176636
[2025-02-05 10:11:22 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 29 ===
[2025-02-05 10:11:38 root] (abq_llm_divide_blocks.py 278): INFO layer 29 loss_mean: 0.10693687945604324
[2025-02-05 10:11:38 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 30 ===
[2025-02-05 10:11:53 root] (abq_llm_divide_blocks.py 278): INFO layer 30 loss_mean: 0.1091114729642868
[2025-02-05 10:11:53 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 31 ===
[2025-02-05 10:12:08 root] (abq_llm_divide_blocks.py 278): INFO layer 31 loss_mean: 0.12570832669734955
[2025-02-05 10:12:08 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 32 ===
[2025-02-05 10:12:23 root] (abq_llm_divide_blocks.py 278): INFO layer 32 loss_mean: 0.12892572581768036
[2025-02-05 10:12:23 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 33 ===
[2025-02-05 10:12:38 root] (abq_llm_divide_blocks.py 278): INFO layer 33 loss_mean: 0.1463436335325241
[2025-02-05 10:12:38 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 34 ===
[2025-02-05 10:12:54 root] (abq_llm_divide_blocks.py 278): INFO layer 34 loss_mean: 0.2039874941110611
[2025-02-05 10:12:54 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 35 ===
[2025-02-05 10:13:09 root] (abq_llm_divide_blocks.py 278): INFO layer 35 loss_mean: 0.22893798351287842
[2025-02-05 10:13:09 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 36 ===
[2025-02-05 10:13:24 root] (abq_llm_divide_blocks.py 278): INFO layer 36 loss_mean: 0.29811736941337585
[2025-02-05 10:13:24 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 37 ===
[2025-02-05 10:13:39 root] (abq_llm_divide_blocks.py 278): INFO layer 37 loss_mean: 0.4629914164543152
[2025-02-05 10:13:39 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 38 ===
[2025-02-05 10:13:55 root] (abq_llm_divide_blocks.py 278): INFO layer 38 loss_mean: 1.6973999738693237
[2025-02-05 10:13:55 root] (abq_llm_divide_blocks.py 232): INFO === Start quantize layer 39 ===
[2025-02-05 10:14:10 root] (abq_llm_divide_blocks.py 278): INFO layer 39 loss_mean: 8.008544921875
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 0-1: size=1, error_sum=0.0022, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 1-2: size=1, error_sum=0.0046, min_similarity=0.9616, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 2-3: size=1, error_sum=1.2611, min_similarity=0.9820, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 3-6: size=3, error_sum=0.0348, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 6-7: size=1, error_sum=2.9918, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 7-10: size=3, error_sum=0.0432, min_similarity=0.9990, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 10-13: size=3, error_sum=0.0708, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 13-16: size=3, error_sum=0.1083, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 16-19: size=3, error_sum=0.1680, min_similarity=0.9997, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 19-21: size=2, error_sum=0.1563, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 21-23: size=2, error_sum=0.1903, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 23-25: size=2, error_sum=0.1822, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 25-27: size=2, error_sum=0.1972, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 27-28: size=1, error_sum=0.1016, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 28-29: size=1, error_sum=0.1260, min_similarity=0.9997, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 29-30: size=1, error_sum=0.1069, min_similarity=0.9998, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 30-31: size=1, error_sum=0.1091, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 31-32: size=1, error_sum=0.1257, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 32-33: size=1, error_sum=0.1289, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 33-34: size=1, error_sum=0.1463, min_similarity=1.0000, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 34-35: size=1, error_sum=0.2040, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 35-36: size=1, error_sum=0.2289, min_similarity=0.9997, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 36-37: size=1, error_sum=0.2981, min_similarity=0.9996, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 37-38: size=1, error_sum=0.4630, min_similarity=0.9999, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 38-39: size=1, error_sum=1.6974, min_similarity=0.9997, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 quantize.utils_divide] (utils_divide.py 110): INFO Block 39-40: size=1, error_sum=8.0085, min_similarity=0.9977, max_sensitivity_diff=0.0000
[2025-02-05 10:14:10 root] (abq_llm_divide_blocks.py 294): INFO blocks: [(0, 1), (1, 2), (2, 3), (3, 6), (6, 7), (7, 10), (10, 13), (13, 16), (16, 19), (19, 21), (21, 23), (23, 25), (25, 27), (27, 28), (28, 29), (29, 30), (30, 31), (31, 32), (32, 33), (33, 34), (34, 35), (35, 36), (36, 37), (37, 38), (38, 39), (39, 40)]
[2025-02-05 10:14:10 root] (main_divide_blocks.py 371): INFO 683.3009433746338
