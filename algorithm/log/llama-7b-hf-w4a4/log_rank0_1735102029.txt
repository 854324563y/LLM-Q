[2024-12-25 04:47:09 root] (main.py 265): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log/llama-7b-hf-w4a4', save_dir='./quant/llama-7b-hf-w4a4', resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None)
[2024-12-25 04:47:17 root] (main.py 331): INFO === start quantization ===
[2024-12-25 04:47:18 root] (main.py 337): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2024-12-25 04:47:19 root] (abq_llm.py 62): INFO Starting ...
[2024-12-25 04:47:21 root] (abq_llm.py 208): INFO === Start quantize layer 0 ===
[2024-12-25 04:47:25 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-25 04:47:55 root] (abq_llm.py 321): INFO layer 0 iter 0 loss:0.06279340386390686 norm:0.02897413820028305 max memory_allocated 22883.16943359375 
[2024-12-25 04:48:27 root] (abq_llm.py 321): INFO layer 0 iter 1 loss:0.04337942972779274 norm:0.018674146384000778 max memory_allocated 22883.16943359375 
[2024-12-25 04:48:58 root] (abq_llm.py 321): INFO layer 0 iter 2 loss:0.0359891802072525 norm:0.014663781970739365 max memory_allocated 22883.16943359375 
[2024-12-25 04:49:30 root] (abq_llm.py 321): INFO layer 0 iter 3 loss:0.03272775560617447 norm:0.01183001883327961 max memory_allocated 22883.16943359375 
[2024-12-25 04:50:02 root] (abq_llm.py 321): INFO layer 0 iter 4 loss:0.030908633023500443 norm:0.009688710793852806 max memory_allocated 22883.16943359375 
[2024-12-25 04:50:33 root] (abq_llm.py 321): INFO layer 0 iter 5 loss:0.02979016862809658 norm:0.008132163435220718 max memory_allocated 22883.16943359375 
[2024-12-25 04:51:05 root] (abq_llm.py 321): INFO layer 0 iter 6 loss:0.029150575399398804 norm:0.006904603913426399 max memory_allocated 22883.16943359375 
[2024-12-25 04:51:37 root] (abq_llm.py 321): INFO layer 0 iter 7 loss:0.028656281530857086 norm:0.0059634605422616005 max memory_allocated 22883.16943359375 
[2024-12-25 04:52:08 root] (abq_llm.py 321): INFO layer 0 iter 8 loss:0.028299061581492424 norm:0.005197977181524038 max memory_allocated 22883.16943359375 
[2024-12-25 04:52:40 root] (abq_llm.py 321): INFO layer 0 iter 9 loss:0.02802509069442749 norm:0.004591106437146664 max memory_allocated 22883.16943359375 
[2024-12-25 04:53:12 root] (abq_llm.py 321): INFO layer 0 iter 10 loss:0.027911517769098282 norm:0.004247494041919708 max memory_allocated 22883.16943359375 
[2024-12-25 04:53:44 root] (abq_llm.py 321): INFO layer 0 iter 11 loss:0.027816202491521835 norm:0.003972142469137907 max memory_allocated 22883.16943359375 
[2024-12-25 04:54:15 root] (abq_llm.py 321): INFO layer 0 iter 12 loss:0.027680441737174988 norm:0.003958640620112419 max memory_allocated 22883.16943359375 
[2024-12-25 04:54:47 root] (abq_llm.py 321): INFO layer 0 iter 13 loss:0.027582036331295967 norm:0.0038182539865374565 max memory_allocated 22883.16943359375 
[2024-12-25 04:55:19 root] (abq_llm.py 321): INFO layer 0 iter 14 loss:0.02747463621199131 norm:0.003659020410850644 max memory_allocated 22883.16943359375 
[2024-12-25 04:55:51 root] (abq_llm.py 321): INFO layer 0 iter 15 loss:0.027485575526952744 norm:0.003537909360602498 max memory_allocated 22883.16943359375 
[2024-12-25 04:56:22 root] (abq_llm.py 321): INFO layer 0 iter 16 loss:0.02747456170618534 norm:0.003567194566130638 max memory_allocated 22883.16943359375 
[2024-12-25 04:56:54 root] (abq_llm.py 321): INFO layer 0 iter 17 loss:0.02743474207818508 norm:0.0035627465695142746 max memory_allocated 22883.16943359375 
[2024-12-25 04:57:26 root] (abq_llm.py 321): INFO layer 0 iter 18 loss:0.02741941809654236 norm:0.0035965251736342907 max memory_allocated 22883.16943359375 
[2024-12-25 04:57:57 root] (abq_llm.py 321): INFO layer 0 iter 19 loss:0.027426552027463913 norm:0.003582945093512535 max memory_allocated 22883.16943359375 
[2024-12-25 04:58:06 root] (abq_llm.py 208): INFO === Start quantize layer 1 ===
[2024-12-25 04:58:09 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-25 04:58:41 root] (abq_llm.py 321): INFO layer 1 iter 0 loss:0.204918771982193 norm:0.07930779457092285 max memory_allocated 22884.84130859375 
[2024-12-25 04:59:12 root] (abq_llm.py 321): INFO layer 1 iter 1 loss:0.12062753736972809 norm:0.02563714049756527 max memory_allocated 22884.84130859375 
[2024-12-25 04:59:44 root] (abq_llm.py 321): INFO layer 1 iter 2 loss:0.09979591518640518 norm:0.020684493705630302 max memory_allocated 22884.84130859375 
[2024-12-25 05:00:16 root] (abq_llm.py 321): INFO layer 1 iter 3 loss:0.08922699093818665 norm:0.01659882627427578 max memory_allocated 22884.84130859375 
[2024-12-25 05:00:47 root] (abq_llm.py 321): INFO layer 1 iter 4 loss:0.08383821696043015 norm:0.013828946277499199 max memory_allocated 22884.84130859375 
[2024-12-25 05:01:19 root] (abq_llm.py 321): INFO layer 1 iter 5 loss:0.0803535133600235 norm:0.011966206133365631 max memory_allocated 22884.84130859375 
[2024-12-25 05:01:51 root] (abq_llm.py 321): INFO layer 1 iter 6 loss:0.0779251903295517 norm:0.010895217768847942 max memory_allocated 22884.84130859375 
[2024-12-25 05:02:23 root] (abq_llm.py 321): INFO layer 1 iter 7 loss:0.07638580352067947 norm:0.009787493385374546 max memory_allocated 22884.84130859375 
[2024-12-25 05:02:54 root] (abq_llm.py 321): INFO layer 1 iter 8 loss:0.07517645508050919 norm:0.008779410272836685 max memory_allocated 22884.84130859375 
[2024-12-25 05:03:26 root] (abq_llm.py 321): INFO layer 1 iter 9 loss:0.07405014336109161 norm:0.007806889712810516 max memory_allocated 22884.84130859375 
[2024-12-25 05:03:58 root] (abq_llm.py 321): INFO layer 1 iter 10 loss:0.07325783371925354 norm:0.007040702737867832 max memory_allocated 22884.84130859375 
[2024-12-25 05:04:30 root] (abq_llm.py 321): INFO layer 1 iter 11 loss:0.07257123291492462 norm:0.00628503505140543 max memory_allocated 22884.84130859375 
[2024-12-25 05:05:01 root] (abq_llm.py 321): INFO layer 1 iter 12 loss:0.0720701813697815 norm:0.005827771965414286 max memory_allocated 22884.84130859375 
[2024-12-25 05:05:33 root] (abq_llm.py 321): INFO layer 1 iter 13 loss:0.07170239090919495 norm:0.005461066961288452 max memory_allocated 22884.84130859375 
[2024-12-25 05:06:05 root] (abq_llm.py 321): INFO layer 1 iter 14 loss:0.07131976634263992 norm:0.0051666125655174255 max memory_allocated 22884.84130859375 
[2024-12-25 05:06:37 root] (abq_llm.py 321): INFO layer 1 iter 15 loss:0.07099349796772003 norm:0.004897658713161945 max memory_allocated 22884.84130859375 
[2024-12-25 05:07:08 root] (abq_llm.py 321): INFO layer 1 iter 16 loss:0.07083852589130402 norm:0.004744655452668667 max memory_allocated 22884.84130859375 
[2024-12-25 05:07:40 root] (abq_llm.py 321): INFO layer 1 iter 17 loss:0.07062354683876038 norm:0.0046463655307888985 max memory_allocated 22884.84130859375 
[2024-12-25 05:08:12 root] (abq_llm.py 321): INFO layer 1 iter 18 loss:0.07047051936388016 norm:0.004395216703414917 max memory_allocated 22884.84130859375 
[2024-12-25 05:08:44 root] (abq_llm.py 321): INFO layer 1 iter 19 loss:0.0703129991889 norm:0.004313342273235321 max memory_allocated 22884.84130859375 
[2024-12-25 05:08:52 root] (abq_llm.py 208): INFO === Start quantize layer 2 ===
[2024-12-25 05:08:55 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-25 05:09:27 root] (abq_llm.py 321): INFO layer 2 iter 0 loss:0.2475273609161377 norm:0.037658680230379105 max memory_allocated 22886.51318359375 
[2024-12-25 05:09:59 root] (abq_llm.py 321): INFO layer 2 iter 1 loss:0.19719479978084564 norm:0.032192204147577286 max memory_allocated 22886.51318359375 
[2024-12-25 05:10:30 root] (abq_llm.py 321): INFO layer 2 iter 2 loss:0.17646251618862152 norm:0.018341194838285446 max memory_allocated 22886.51318359375 
[2024-12-25 05:11:02 root] (abq_llm.py 321): INFO layer 2 iter 3 loss:0.1667158007621765 norm:0.016993099823594093 max memory_allocated 22886.51318359375 
[2024-12-25 05:11:34 root] (abq_llm.py 321): INFO layer 2 iter 4 loss:0.1599583625793457 norm:0.015358753502368927 max memory_allocated 22886.51318359375 
[2024-12-25 05:12:06 root] (abq_llm.py 321): INFO layer 2 iter 5 loss:0.1548759490251541 norm:0.014411824755370617 max memory_allocated 22886.51318359375 
[2024-12-25 05:12:37 root] (abq_llm.py 321): INFO layer 2 iter 6 loss:0.15182340145111084 norm:0.014322400093078613 max memory_allocated 22886.51318359375 
[2024-12-25 05:13:09 root] (abq_llm.py 321): INFO layer 2 iter 7 loss:0.14626283943653107 norm:0.014000109396874905 max memory_allocated 22886.51318359375 
[2024-12-25 05:13:41 root] (abq_llm.py 321): INFO layer 2 iter 8 loss:0.1462123692035675 norm:0.01306826714426279 max memory_allocated 22886.51318359375 
[2024-12-25 05:14:13 root] (abq_llm.py 321): INFO layer 2 iter 9 loss:0.14526501297950745 norm:0.012690524570643902 max memory_allocated 22886.51318359375 
[2024-12-25 05:14:45 root] (abq_llm.py 321): INFO layer 2 iter 10 loss:0.14409668743610382 norm:0.013735792599618435 max memory_allocated 22886.51318359375 
[2024-12-25 05:15:16 root] (abq_llm.py 321): INFO layer 2 iter 11 loss:0.14267921447753906 norm:0.012043998576700687 max memory_allocated 22886.51318359375 
[2024-12-25 05:15:48 root] (abq_llm.py 321): INFO layer 2 iter 12 loss:0.14157484471797943 norm:0.011934420093894005 max memory_allocated 22886.51318359375 
[2024-12-25 05:16:20 root] (abq_llm.py 321): INFO layer 2 iter 13 loss:0.14013350009918213 norm:0.01198508869856596 max memory_allocated 22886.51318359375 
[2024-12-25 05:16:52 root] (abq_llm.py 321): INFO layer 2 iter 14 loss:0.13952895998954773 norm:0.010919267311692238 max memory_allocated 22886.51318359375 
[2024-12-25 05:17:24 root] (abq_llm.py 321): INFO layer 2 iter 15 loss:0.14108286798000336 norm:0.01109769381582737 max memory_allocated 22886.51318359375 
[2024-12-25 05:17:55 root] (abq_llm.py 321): INFO layer 2 iter 16 loss:0.14033786952495575 norm:0.010897057130932808 max memory_allocated 22886.51318359375 
[2024-12-25 05:18:27 root] (abq_llm.py 321): INFO layer 2 iter 17 loss:0.1405211240053177 norm:0.010804995894432068 max memory_allocated 22886.51318359375 
[2024-12-25 05:18:59 root] (abq_llm.py 321): INFO layer 2 iter 18 loss:0.14038898050785065 norm:0.011174021288752556 max memory_allocated 22886.51318359375 
[2024-12-25 05:19:31 root] (abq_llm.py 321): INFO layer 2 iter 19 loss:0.1378163993358612 norm:0.010494688525795937 max memory_allocated 22886.51318359375 
[2024-12-25 05:19:40 root] (abq_llm.py 208): INFO === Start quantize layer 3 ===
[2024-12-25 05:20:14 root] (abq_llm.py 321): INFO layer 3 iter 0 loss:0.23593097925186157 norm:0.04731607437133789 max memory_allocated 22888.06982421875 
[2024-12-25 05:20:46 root] (abq_llm.py 321): INFO layer 3 iter 1 loss:0.18910129368305206 norm:0.012590965256094933 max memory_allocated 22888.06982421875 
[2024-12-25 05:21:17 root] (abq_llm.py 321): INFO layer 3 iter 2 loss:0.1668892502784729 norm:0.006158553063869476 max memory_allocated 22888.06982421875 
[2024-12-25 05:21:49 root] (abq_llm.py 321): INFO layer 3 iter 3 loss:0.15765529870986938 norm:0.003904825309291482 max memory_allocated 22888.06982421875 
[2024-12-25 05:22:21 root] (abq_llm.py 321): INFO layer 3 iter 4 loss:0.15309807658195496 norm:0.0030955930706113577 max memory_allocated 22888.06982421875 
[2024-12-25 05:22:52 root] (abq_llm.py 321): INFO layer 3 iter 5 loss:0.150684654712677 norm:0.0028201378881931305 max memory_allocated 22888.06982421875 
[2024-12-25 05:23:24 root] (abq_llm.py 321): INFO layer 3 iter 6 loss:0.1488260179758072 norm:0.0025562536902725697 max memory_allocated 22888.06982421875 
[2024-12-25 05:23:56 root] (abq_llm.py 321): INFO layer 3 iter 7 loss:0.14757969975471497 norm:0.002366382163017988 max memory_allocated 22888.06982421875 
[2024-12-25 05:24:27 root] (abq_llm.py 321): INFO layer 3 iter 8 loss:0.14658468961715698 norm:0.002319733612239361 max memory_allocated 22888.06982421875 
[2024-12-25 05:24:59 root] (abq_llm.py 321): INFO layer 3 iter 9 loss:0.145779550075531 norm:0.0022241142578423023 max memory_allocated 22888.06982421875 
[2024-12-25 05:25:31 root] (abq_llm.py 321): INFO layer 3 iter 10 loss:0.14530184864997864 norm:0.0021960751619189978 max memory_allocated 22888.06982421875 
[2024-12-25 05:26:02 root] (abq_llm.py 321): INFO layer 3 iter 11 loss:0.1447521299123764 norm:0.0021032122895121574 max memory_allocated 22888.06982421875 
[2024-12-25 05:26:34 root] (abq_llm.py 321): INFO layer 3 iter 12 loss:0.1444282829761505 norm:0.0021062062587589025 max memory_allocated 22888.06982421875 
[2024-12-25 05:27:06 root] (abq_llm.py 321): INFO layer 3 iter 13 loss:0.1442301720380783 norm:0.0021143713966012 max memory_allocated 22888.06982421875 
[2024-12-25 05:27:37 root] (abq_llm.py 321): INFO layer 3 iter 14 loss:0.14410047233104706 norm:0.002080540405586362 max memory_allocated 22888.06982421875 
[2024-12-25 05:28:09 root] (abq_llm.py 321): INFO layer 3 iter 15 loss:0.1439446359872818 norm:0.0021036071702837944 max memory_allocated 22888.06982421875 
[2024-12-25 05:28:41 root] (abq_llm.py 321): INFO layer 3 iter 16 loss:0.14365839958190918 norm:0.002107288921251893 max memory_allocated 22888.06982421875 
[2024-12-25 05:29:12 root] (abq_llm.py 321): INFO layer 3 iter 17 loss:0.14361004531383514 norm:0.0020546913146972656 max memory_allocated 22888.06982421875 
[2024-12-25 05:29:44 root] (abq_llm.py 321): INFO layer 3 iter 18 loss:0.1436050534248352 norm:0.0020152959041297436 max memory_allocated 22888.06982421875 
[2024-12-25 05:30:16 root] (abq_llm.py 321): INFO layer 3 iter 19 loss:0.143617182970047 norm:0.001964107621461153 max memory_allocated 22888.06982421875 
[2024-12-25 05:30:25 root] (abq_llm.py 208): INFO === Start quantize layer 4 ===
[2024-12-25 05:30:59 root] (abq_llm.py 321): INFO layer 4 iter 0 loss:0.29295963048934937 norm:0.04950183257460594 max memory_allocated 22889.74169921875 
[2024-12-25 05:31:31 root] (abq_llm.py 321): INFO layer 4 iter 1 loss:0.2396443784236908 norm:0.016632579267024994 max memory_allocated 22889.74169921875 
[2024-12-25 05:32:02 root] (abq_llm.py 321): INFO layer 4 iter 2 loss:0.2105647325515747 norm:0.0073110307566821575 max memory_allocated 22889.74169921875 
[2024-12-25 05:32:34 root] (abq_llm.py 321): INFO layer 4 iter 3 loss:0.1980009526014328 norm:0.004130410961806774 max memory_allocated 22889.74169921875 
[2024-12-25 05:33:06 root] (abq_llm.py 321): INFO layer 4 iter 4 loss:0.19246479868888855 norm:0.0032225761096924543 max memory_allocated 22889.74169921875 
[2024-12-25 05:33:37 root] (abq_llm.py 321): INFO layer 4 iter 5 loss:0.18901565670967102 norm:0.002968789543956518 max memory_allocated 22889.74169921875 
[2024-12-25 05:34:09 root] (abq_llm.py 321): INFO layer 4 iter 6 loss:0.18717947602272034 norm:0.0028019717428833246 max memory_allocated 22889.74169921875 
[2024-12-25 05:34:41 root] (abq_llm.py 321): INFO layer 4 iter 7 loss:0.18612776696681976 norm:0.0028143590316176414 max memory_allocated 22889.74169921875 
[2024-12-25 05:35:12 root] (abq_llm.py 321): INFO layer 4 iter 8 loss:0.1852525770664215 norm:0.0025934502482414246 max memory_allocated 22889.74169921875 
[2024-12-25 05:35:44 root] (abq_llm.py 321): INFO layer 4 iter 9 loss:0.1845596581697464 norm:0.0025832538958638906 max memory_allocated 22889.74169921875 
[2024-12-25 05:36:16 root] (abq_llm.py 321): INFO layer 4 iter 10 loss:0.18409837782382965 norm:0.002420311328023672 max memory_allocated 22889.74169921875 
[2024-12-25 05:36:47 root] (abq_llm.py 321): INFO layer 4 iter 11 loss:0.18379627168178558 norm:0.0024964697659015656 max memory_allocated 22889.74169921875 
[2024-12-25 05:37:19 root] (abq_llm.py 321): INFO layer 4 iter 12 loss:0.18359392881393433 norm:0.00249157240614295 max memory_allocated 22889.74169921875 
[2024-12-25 05:37:51 root] (abq_llm.py 321): INFO layer 4 iter 13 loss:0.18330487608909607 norm:0.002573964651674032 max memory_allocated 22889.74169921875 
[2024-12-25 05:38:22 root] (abq_llm.py 321): INFO layer 4 iter 14 loss:0.18303151428699493 norm:0.002338642254471779 max memory_allocated 22889.74169921875 
[2024-12-25 05:38:54 root] (abq_llm.py 321): INFO layer 4 iter 15 loss:0.18285757303237915 norm:0.002376557793468237 max memory_allocated 22889.74169921875 
[2024-12-25 05:39:26 root] (abq_llm.py 321): INFO layer 4 iter 16 loss:0.18277709186077118 norm:0.002276928164064884 max memory_allocated 22889.74169921875 
[2024-12-25 05:39:57 root] (abq_llm.py 321): INFO layer 4 iter 17 loss:0.18270736932754517 norm:0.002298013074323535 max memory_allocated 22889.74169921875 
[2024-12-25 05:40:29 root] (abq_llm.py 321): INFO layer 4 iter 18 loss:0.18265661597251892 norm:0.0022673443891108036 max memory_allocated 22889.74169921875 
[2024-12-25 05:41:01 root] (abq_llm.py 321): INFO layer 4 iter 19 loss:0.18272824585437775 norm:0.002266954630613327 max memory_allocated 22889.74169921875 
[2024-12-25 05:41:10 root] (abq_llm.py 208): INFO === Start quantize layer 5 ===
[2024-12-25 05:41:44 root] (abq_llm.py 321): INFO layer 5 iter 0 loss:0.3395272195339203 norm:0.06293964385986328 max memory_allocated 22891.41357421875 
[2024-12-25 05:42:16 root] (abq_llm.py 321): INFO layer 5 iter 1 loss:0.27496740221977234 norm:0.01984710432589054 max memory_allocated 22891.41357421875 
[2024-12-25 05:42:47 root] (abq_llm.py 321): INFO layer 5 iter 2 loss:0.24156169593334198 norm:0.009281126782298088 max memory_allocated 22891.41357421875 
[2024-12-25 05:43:19 root] (abq_llm.py 321): INFO layer 5 iter 3 loss:0.22732675075531006 norm:0.005607765633612871 max memory_allocated 22891.41357421875 
[2024-12-25 05:43:51 root] (abq_llm.py 321): INFO layer 5 iter 4 loss:0.22110088169574738 norm:0.00468840217217803 max memory_allocated 22891.41357421875 
[2024-12-25 05:44:22 root] (abq_llm.py 321): INFO layer 5 iter 5 loss:0.2171105593442917 norm:0.004203787539154291 max memory_allocated 22891.41357421875 
[2024-12-25 05:44:54 root] (abq_llm.py 321): INFO layer 5 iter 6 loss:0.214300736784935 norm:0.0038655572570860386 max memory_allocated 22891.41357421875 
[2024-12-25 05:45:26 root] (abq_llm.py 321): INFO layer 5 iter 7 loss:0.2124439924955368 norm:0.0036663201171904802 max memory_allocated 22891.41357421875 
[2024-12-25 05:45:57 root] (abq_llm.py 321): INFO layer 5 iter 8 loss:0.21116548776626587 norm:0.0034882440231740475 max memory_allocated 22891.41357421875 
[2024-12-25 05:46:29 root] (abq_llm.py 321): INFO layer 5 iter 9 loss:0.2102261483669281 norm:0.00340854050591588 max memory_allocated 22891.41357421875 
[2024-12-25 05:47:01 root] (abq_llm.py 321): INFO layer 5 iter 10 loss:0.20954588055610657 norm:0.003382198978215456 max memory_allocated 22891.41357421875 
[2024-12-25 05:47:32 root] (abq_llm.py 321): INFO layer 5 iter 11 loss:0.20891043543815613 norm:0.0031462418846786022 max memory_allocated 22891.41357421875 
[2024-12-25 05:48:04 root] (abq_llm.py 321): INFO layer 5 iter 12 loss:0.2085111141204834 norm:0.0030480483546853065 max memory_allocated 22891.41357421875 
[2024-12-25 05:48:36 root] (abq_llm.py 321): INFO layer 5 iter 13 loss:0.2083677351474762 norm:0.0030456664972007275 max memory_allocated 22891.41357421875 
[2024-12-25 05:49:07 root] (abq_llm.py 321): INFO layer 5 iter 14 loss:0.20806922018527985 norm:0.0029608062468469143 max memory_allocated 22891.41357421875 
[2024-12-25 05:49:39 root] (abq_llm.py 321): INFO layer 5 iter 15 loss:0.2079940289258957 norm:0.0029308386147022247 max memory_allocated 22891.41357421875 
[2024-12-25 05:50:11 root] (abq_llm.py 321): INFO layer 5 iter 16 loss:0.207846999168396 norm:0.0029769495595246553 max memory_allocated 22891.41357421875 
[2024-12-25 05:50:43 root] (abq_llm.py 321): INFO layer 5 iter 17 loss:0.20795422792434692 norm:0.003043699311092496 max memory_allocated 22891.41357421875 
[2024-12-25 05:51:14 root] (abq_llm.py 321): INFO layer 5 iter 18 loss:0.20797881484031677 norm:0.0031520966440439224 max memory_allocated 22891.41357421875 
[2024-12-25 05:51:46 root] (abq_llm.py 321): INFO layer 5 iter 19 loss:0.20776645839214325 norm:0.0030826767906546593 max memory_allocated 22891.41357421875 
[2024-12-25 05:51:55 root] (abq_llm.py 208): INFO === Start quantize layer 6 ===
[2024-12-25 05:52:29 root] (abq_llm.py 321): INFO layer 6 iter 0 loss:0.358346164226532 norm:0.06028319522738457 max memory_allocated 22893.08544921875 
[2024-12-25 05:53:01 root] (abq_llm.py 321): INFO layer 6 iter 1 loss:0.30934303998947144 norm:0.024485601112246513 max memory_allocated 22893.08544921875 
[2024-12-25 05:53:33 root] (abq_llm.py 321): INFO layer 6 iter 2 loss:0.27000555396080017 norm:0.009499065577983856 max memory_allocated 22893.08544921875 
[2024-12-25 05:54:05 root] (abq_llm.py 321): INFO layer 6 iter 3 loss:0.25432389974594116 norm:0.006041876971721649 max memory_allocated 22893.08544921875 
[2024-12-25 05:54:36 root] (abq_llm.py 321): INFO layer 6 iter 4 loss:0.24842479825019836 norm:0.004938814323395491 max memory_allocated 22893.08544921875 
[2024-12-25 05:55:08 root] (abq_llm.py 321): INFO layer 6 iter 5 loss:0.24483883380889893 norm:0.004375736694782972 max memory_allocated 22893.08544921875 
[2024-12-25 05:55:40 root] (abq_llm.py 321): INFO layer 6 iter 6 loss:0.24249199032783508 norm:0.004035386722534895 max memory_allocated 22893.08544921875 
[2024-12-25 05:56:11 root] (abq_llm.py 321): INFO layer 6 iter 7 loss:0.24083785712718964 norm:0.00393718620762229 max memory_allocated 22893.08544921875 
[2024-12-25 05:56:43 root] (abq_llm.py 321): INFO layer 6 iter 8 loss:0.2397834211587906 norm:0.003911824896931648 max memory_allocated 22893.08544921875 
[2024-12-25 05:57:15 root] (abq_llm.py 321): INFO layer 6 iter 9 loss:0.2390245497226715 norm:0.0037403404712677 max memory_allocated 22893.08544921875 
[2024-12-25 05:57:46 root] (abq_llm.py 321): INFO layer 6 iter 10 loss:0.23840981721878052 norm:0.0038343232590705156 max memory_allocated 22893.08544921875 
[2024-12-25 05:58:18 root] (abq_llm.py 321): INFO layer 6 iter 11 loss:0.23795180022716522 norm:0.0037815698888152838 max memory_allocated 22893.08544921875 
[2024-12-25 05:58:50 root] (abq_llm.py 321): INFO layer 6 iter 12 loss:0.23748552799224854 norm:0.003673308063298464 max memory_allocated 22893.08544921875 
[2024-12-25 05:59:21 root] (abq_llm.py 321): INFO layer 6 iter 13 loss:0.23715710639953613 norm:0.003525372827425599 max memory_allocated 22893.08544921875 
[2024-12-25 05:59:53 root] (abq_llm.py 321): INFO layer 6 iter 14 loss:0.23684202134609222 norm:0.0034772485960274935 max memory_allocated 22893.08544921875 
[2024-12-25 06:00:25 root] (abq_llm.py 321): INFO layer 6 iter 15 loss:0.2368268370628357 norm:0.0034850689116865396 max memory_allocated 22893.08544921875 
[2024-12-25 06:00:56 root] (abq_llm.py 321): INFO layer 6 iter 16 loss:0.23648642003536224 norm:0.003411461366340518 max memory_allocated 22893.08544921875 
[2024-12-25 06:01:28 root] (abq_llm.py 321): INFO layer 6 iter 17 loss:0.23641668260097504 norm:0.003447231138125062 max memory_allocated 22893.08544921875 
[2024-12-25 06:02:00 root] (abq_llm.py 321): INFO layer 6 iter 18 loss:0.23650816082954407 norm:0.0035230352077633142 max memory_allocated 22893.08544921875 
[2024-12-25 06:02:32 root] (abq_llm.py 321): INFO layer 6 iter 19 loss:0.2364921271800995 norm:0.0033822087571024895 max memory_allocated 22893.08544921875 
[2024-12-25 06:02:40 root] (abq_llm.py 208): INFO === Start quantize layer 7 ===
[2024-12-25 06:03:15 root] (abq_llm.py 321): INFO layer 7 iter 0 loss:0.39847254753112793 norm:0.04289574921131134 max memory_allocated 22894.75732421875 
[2024-12-25 06:03:47 root] (abq_llm.py 321): INFO layer 7 iter 1 loss:0.34172049164772034 norm:0.01774252951145172 max memory_allocated 22894.75732421875 
[2024-12-25 06:04:18 root] (abq_llm.py 321): INFO layer 7 iter 2 loss:0.30181121826171875 norm:0.0107814846560359 max memory_allocated 22894.75732421875 
[2024-12-25 06:04:50 root] (abq_llm.py 321): INFO layer 7 iter 3 loss:0.28457823395729065 norm:0.005381928291171789 max memory_allocated 22894.75732421875 
[2024-12-25 06:05:22 root] (abq_llm.py 321): INFO layer 7 iter 4 loss:0.2775191068649292 norm:0.004523447714745998 max memory_allocated 22894.75732421875 
[2024-12-25 06:05:54 root] (abq_llm.py 321): INFO layer 7 iter 5 loss:0.27330952882766724 norm:0.004039239604026079 max memory_allocated 22894.75732421875 
[2024-12-25 06:06:25 root] (abq_llm.py 321): INFO layer 7 iter 6 loss:0.27068400382995605 norm:0.003853046800941229 max memory_allocated 22894.75732421875 
[2024-12-25 06:06:57 root] (abq_llm.py 321): INFO layer 7 iter 7 loss:0.2690112590789795 norm:0.0036187656223773956 max memory_allocated 22894.75732421875 
[2024-12-25 06:07:29 root] (abq_llm.py 321): INFO layer 7 iter 8 loss:0.2678339183330536 norm:0.003510881680995226 max memory_allocated 22894.75732421875 
[2024-12-25 06:08:00 root] (abq_llm.py 321): INFO layer 7 iter 9 loss:0.2670215666294098 norm:0.0032599163241684437 max memory_allocated 22894.75732421875 
[2024-12-25 06:08:32 root] (abq_llm.py 321): INFO layer 7 iter 10 loss:0.26674583554267883 norm:0.003145629307255149 max memory_allocated 22894.75732421875 
[2024-12-25 06:09:04 root] (abq_llm.py 321): INFO layer 7 iter 11 loss:0.2661677300930023 norm:0.003264139173552394 max memory_allocated 22894.75732421875 
[2024-12-25 06:09:36 root] (abq_llm.py 321): INFO layer 7 iter 12 loss:0.26562613248825073 norm:0.0030801056418567896 max memory_allocated 22894.75732421875 
[2024-12-25 06:10:07 root] (abq_llm.py 321): INFO layer 7 iter 13 loss:0.26532959938049316 norm:0.0030018771067261696 max memory_allocated 22894.75732421875 
[2024-12-25 06:10:39 root] (abq_llm.py 321): INFO layer 7 iter 14 loss:0.26509448885917664 norm:0.0030362606048583984 max memory_allocated 22894.75732421875 
[2024-12-25 06:11:11 root] (abq_llm.py 321): INFO layer 7 iter 15 loss:0.2648468315601349 norm:0.0029554746579378843 max memory_allocated 22894.75732421875 
[2024-12-25 06:11:43 root] (abq_llm.py 321): INFO layer 7 iter 16 loss:0.26473256945610046 norm:0.00292166112922132 max memory_allocated 22894.75732421875 
[2024-12-25 06:12:14 root] (abq_llm.py 321): INFO layer 7 iter 17 loss:0.26467639207839966 norm:0.0029752112459391356 max memory_allocated 22894.75732421875 
[2024-12-25 06:12:46 root] (abq_llm.py 321): INFO layer 7 iter 18 loss:0.2646557092666626 norm:0.0029283780604600906 max memory_allocated 22894.75732421875 
[2024-12-25 06:13:18 root] (abq_llm.py 321): INFO layer 7 iter 19 loss:0.26461976766586304 norm:0.002865128917619586 max memory_allocated 22894.75732421875 
[2024-12-25 06:13:27 root] (abq_llm.py 208): INFO === Start quantize layer 8 ===
[2024-12-25 06:14:01 root] (abq_llm.py 321): INFO layer 8 iter 0 loss:0.40952372550964355 norm:0.03872651979327202 max memory_allocated 22896.42919921875 
[2024-12-25 06:14:33 root] (abq_llm.py 321): INFO layer 8 iter 1 loss:0.3561631739139557 norm:0.013481736183166504 max memory_allocated 22896.42919921875 
[2024-12-25 06:15:04 root] (abq_llm.py 321): INFO layer 8 iter 2 loss:0.32209134101867676 norm:0.007169629912823439 max memory_allocated 22896.42919921875 
[2024-12-25 06:15:36 root] (abq_llm.py 321): INFO layer 8 iter 3 loss:0.3054977059364319 norm:0.004622858017683029 max memory_allocated 22896.42919921875 
[2024-12-25 06:16:08 root] (abq_llm.py 321): INFO layer 8 iter 4 loss:0.29885581135749817 norm:0.003865316743031144 max memory_allocated 22896.42919921875 
[2024-12-25 06:16:39 root] (abq_llm.py 321): INFO layer 8 iter 5 loss:0.29468321800231934 norm:0.0035301968455314636 max memory_allocated 22896.42919921875 
[2024-12-25 06:17:11 root] (abq_llm.py 321): INFO layer 8 iter 6 loss:0.2918036878108978 norm:0.003350678365677595 max memory_allocated 22896.42919921875 
[2024-12-25 06:17:43 root] (abq_llm.py 321): INFO layer 8 iter 7 loss:0.2897696793079376 norm:0.0032707287464290857 max memory_allocated 22896.42919921875 
[2024-12-25 06:18:15 root] (abq_llm.py 321): INFO layer 8 iter 8 loss:0.28850340843200684 norm:0.0031278880778700113 max memory_allocated 22896.42919921875 
[2024-12-25 06:18:46 root] (abq_llm.py 321): INFO layer 8 iter 9 loss:0.2874261438846588 norm:0.002955593168735504 max memory_allocated 22896.42919921875 
[2024-12-25 06:19:18 root] (abq_llm.py 321): INFO layer 8 iter 10 loss:0.286818265914917 norm:0.0028820522129535675 max memory_allocated 22896.42919921875 
[2024-12-25 06:19:50 root] (abq_llm.py 321): INFO layer 8 iter 11 loss:0.28635793924331665 norm:0.0028410572558641434 max memory_allocated 22896.42919921875 
[2024-12-25 06:20:22 root] (abq_llm.py 321): INFO layer 8 iter 12 loss:0.2859726548194885 norm:0.002717173658311367 max memory_allocated 22896.42919921875 
[2024-12-25 06:20:53 root] (abq_llm.py 321): INFO layer 8 iter 13 loss:0.2856362760066986 norm:0.002775861183181405 max memory_allocated 22896.42919921875 
[2024-12-25 06:21:25 root] (abq_llm.py 321): INFO layer 8 iter 14 loss:0.28528180718421936 norm:0.0026526451110839844 max memory_allocated 22896.42919921875 
[2024-12-25 06:21:57 root] (abq_llm.py 321): INFO layer 8 iter 15 loss:0.28509721159935 norm:0.0025955059099942446 max memory_allocated 22896.42919921875 
[2024-12-25 06:22:28 root] (abq_llm.py 321): INFO layer 8 iter 16 loss:0.28498026728630066 norm:0.0025672833435237408 max memory_allocated 22896.42919921875 
[2024-12-25 06:23:00 root] (abq_llm.py 321): INFO layer 8 iter 17 loss:0.2848453223705292 norm:0.0025113276205956936 max memory_allocated 22896.42919921875 
[2024-12-25 06:23:32 root] (abq_llm.py 321): INFO layer 8 iter 18 loss:0.28470438718795776 norm:0.0025207740254700184 max memory_allocated 22896.42919921875 
[2024-12-25 06:24:03 root] (abq_llm.py 321): INFO layer 8 iter 19 loss:0.28447824716567993 norm:0.0025001270696520805 max memory_allocated 22896.42919921875 
[2024-12-25 06:24:12 root] (abq_llm.py 208): INFO === Start quantize layer 9 ===
[2024-12-25 06:24:47 root] (abq_llm.py 321): INFO layer 9 iter 0 loss:0.4434939920902252 norm:0.03961317986249924 max memory_allocated 22898.10107421875 
[2024-12-25 06:25:18 root] (abq_llm.py 321): INFO layer 9 iter 1 loss:0.3927597999572754 norm:0.01780400238931179 max memory_allocated 22898.10107421875 
[2024-12-25 06:25:50 root] (abq_llm.py 321): INFO layer 9 iter 2 loss:0.3497576117515564 norm:0.007565549109131098 max memory_allocated 22898.10107421875 
[2024-12-25 06:26:22 root] (abq_llm.py 321): INFO layer 9 iter 3 loss:0.3304364085197449 norm:0.0047475057654082775 max memory_allocated 22898.10107421875 
[2024-12-25 06:26:54 root] (abq_llm.py 321): INFO layer 9 iter 4 loss:0.32303279638290405 norm:0.00390359153971076 max memory_allocated 22898.10107421875 
[2024-12-25 06:27:25 root] (abq_llm.py 321): INFO layer 9 iter 5 loss:0.3186415433883667 norm:0.003496099030598998 max memory_allocated 22898.10107421875 
[2024-12-25 06:27:57 root] (abq_llm.py 321): INFO layer 9 iter 6 loss:0.3153187334537506 norm:0.0031760153360664845 max memory_allocated 22898.10107421875 
[2024-12-25 06:28:29 root] (abq_llm.py 321): INFO layer 9 iter 7 loss:0.3129407465457916 norm:0.0030028701294213533 max memory_allocated 22898.10107421875 
[2024-12-25 06:29:00 root] (abq_llm.py 321): INFO layer 9 iter 8 loss:0.31152403354644775 norm:0.002886862261220813 max memory_allocated 22898.10107421875 
[2024-12-25 06:29:32 root] (abq_llm.py 321): INFO layer 9 iter 9 loss:0.3105509281158447 norm:0.002725330414250493 max memory_allocated 22898.10107421875 
[2024-12-25 06:30:04 root] (abq_llm.py 321): INFO layer 9 iter 10 loss:0.30969002842903137 norm:0.0027113964315503836 max memory_allocated 22898.10107421875 
[2024-12-25 06:30:35 root] (abq_llm.py 321): INFO layer 9 iter 11 loss:0.3090674877166748 norm:0.0026307774242013693 max memory_allocated 22898.10107421875 
[2024-12-25 06:31:07 root] (abq_llm.py 321): INFO layer 9 iter 12 loss:0.3084750771522522 norm:0.0026466508861631155 max memory_allocated 22898.10107421875 
[2024-12-25 06:31:39 root] (abq_llm.py 321): INFO layer 9 iter 13 loss:0.3080713450908661 norm:0.002525920979678631 max memory_allocated 22898.10107421875 
[2024-12-25 06:32:11 root] (abq_llm.py 321): INFO layer 9 iter 14 loss:0.3078352212905884 norm:0.002438334980979562 max memory_allocated 22898.10107421875 
[2024-12-25 06:32:42 root] (abq_llm.py 321): INFO layer 9 iter 15 loss:0.30767321586608887 norm:0.0024377028457820415 max memory_allocated 22898.10107421875 
[2024-12-25 06:33:14 root] (abq_llm.py 321): INFO layer 9 iter 16 loss:0.30752044916152954 norm:0.0024269581772387028 max memory_allocated 22898.10107421875 
[2024-12-25 06:33:46 root] (abq_llm.py 321): INFO layer 9 iter 17 loss:0.3074510991573334 norm:0.0024343717377632856 max memory_allocated 22898.10107421875 
[2024-12-25 06:34:17 root] (abq_llm.py 321): INFO layer 9 iter 18 loss:0.30730578303337097 norm:0.002496084664016962 max memory_allocated 22898.10107421875 
[2024-12-25 06:34:49 root] (abq_llm.py 321): INFO layer 9 iter 19 loss:0.3071887791156769 norm:0.002418593969196081 max memory_allocated 22898.10107421875 
[2024-12-25 06:34:58 root] (abq_llm.py 208): INFO === Start quantize layer 10 ===
[2024-12-25 06:35:32 root] (abq_llm.py 321): INFO layer 10 iter 0 loss:0.4296134412288666 norm:0.02262760140001774 max memory_allocated 22899.77294921875 
[2024-12-25 06:36:04 root] (abq_llm.py 321): INFO layer 10 iter 1 loss:0.3919268250465393 norm:0.011150840669870377 max memory_allocated 22899.77294921875 
[2024-12-25 06:36:36 root] (abq_llm.py 321): INFO layer 10 iter 2 loss:0.3584814965724945 norm:0.00534428283572197 max memory_allocated 22899.77294921875 
[2024-12-25 06:37:07 root] (abq_llm.py 321): INFO layer 10 iter 3 loss:0.3430699110031128 norm:0.0036323564127087593 max memory_allocated 22899.77294921875 
[2024-12-25 06:37:39 root] (abq_llm.py 321): INFO layer 10 iter 4 loss:0.3369653820991516 norm:0.0031302575953304768 max memory_allocated 22899.77294921875 
[2024-12-25 06:38:11 root] (abq_llm.py 321): INFO layer 10 iter 5 loss:0.3335024416446686 norm:0.002923305844888091 max memory_allocated 22899.77294921875 
[2024-12-25 06:38:43 root] (abq_llm.py 321): INFO layer 10 iter 6 loss:0.3310140073299408 norm:0.002775641158223152 max memory_allocated 22899.77294921875 
[2024-12-25 06:39:14 root] (abq_llm.py 321): INFO layer 10 iter 7 loss:0.32948043942451477 norm:0.002637529280036688 max memory_allocated 22899.77294921875 
[2024-12-25 06:39:46 root] (abq_llm.py 321): INFO layer 10 iter 8 loss:0.3283625543117523 norm:0.0025081485509872437 max memory_allocated 22899.77294921875 
[2024-12-25 06:40:18 root] (abq_llm.py 321): INFO layer 10 iter 9 loss:0.32746943831443787 norm:0.0025201146490871906 max memory_allocated 22899.77294921875 
[2024-12-25 06:40:49 root] (abq_llm.py 321): INFO layer 10 iter 10 loss:0.3268505334854126 norm:0.002521418733522296 max memory_allocated 22899.77294921875 
[2024-12-25 06:41:21 root] (abq_llm.py 321): INFO layer 10 iter 11 loss:0.3262343108654022 norm:0.0025738240219652653 max memory_allocated 22899.77294921875 
[2024-12-25 06:41:53 root] (abq_llm.py 321): INFO layer 10 iter 12 loss:0.3257332444190979 norm:0.0024516962002962828 max memory_allocated 22899.77294921875 
[2024-12-25 06:42:24 root] (abq_llm.py 321): INFO layer 10 iter 13 loss:0.32536083459854126 norm:0.002275003120303154 max memory_allocated 22899.77294921875 
[2024-12-25 06:42:56 root] (abq_llm.py 321): INFO layer 10 iter 14 loss:0.32507410645484924 norm:0.0022527333348989487 max memory_allocated 22899.77294921875 
[2024-12-25 06:43:28 root] (abq_llm.py 321): INFO layer 10 iter 15 loss:0.32493406534194946 norm:0.0022324793972074986 max memory_allocated 22899.77294921875 
[2024-12-25 06:44:00 root] (abq_llm.py 321): INFO layer 10 iter 16 loss:0.32476070523262024 norm:0.002176309237256646 max memory_allocated 22899.77294921875 
[2024-12-25 06:44:31 root] (abq_llm.py 321): INFO layer 10 iter 17 loss:0.32461899518966675 norm:0.002207919955253601 max memory_allocated 22899.77294921875 
[2024-12-25 06:45:03 root] (abq_llm.py 321): INFO layer 10 iter 18 loss:0.324486643075943 norm:0.0021896720863878727 max memory_allocated 22899.77294921875 
[2024-12-25 06:45:35 root] (abq_llm.py 321): INFO layer 10 iter 19 loss:0.3243858218193054 norm:0.0022004516795277596 max memory_allocated 22899.77294921875 
[2024-12-25 06:45:44 root] (abq_llm.py 208): INFO === Start quantize layer 11 ===
[2024-12-25 06:46:18 root] (abq_llm.py 321): INFO layer 11 iter 0 loss:0.45088666677474976 norm:0.028862515464425087 max memory_allocated 22901.44482421875 
[2024-12-25 06:46:50 root] (abq_llm.py 321): INFO layer 11 iter 1 loss:0.41160956025123596 norm:0.012635542079806328 max memory_allocated 22901.44482421875 
[2024-12-25 06:47:22 root] (abq_llm.py 321): INFO layer 11 iter 2 loss:0.37832579016685486 norm:0.0063344864174723625 max memory_allocated 22901.44482421875 
[2024-12-25 06:47:53 root] (abq_llm.py 321): INFO layer 11 iter 3 loss:0.36147212982177734 norm:0.0037089113611727953 max memory_allocated 22901.44482421875 
[2024-12-25 06:48:25 root] (abq_llm.py 321): INFO layer 11 iter 4 loss:0.3545776307582855 norm:0.003027664264664054 max memory_allocated 22901.44482421875 
[2024-12-25 06:48:57 root] (abq_llm.py 321): INFO layer 11 iter 5 loss:0.3505215048789978 norm:0.002748220693320036 max memory_allocated 22901.44482421875 
[2024-12-25 06:49:29 root] (abq_llm.py 321): INFO layer 11 iter 6 loss:0.34771528840065 norm:0.0025509262923151255 max memory_allocated 22901.44482421875 
[2024-12-25 06:50:00 root] (abq_llm.py 321): INFO layer 11 iter 7 loss:0.3458334803581238 norm:0.002390379086136818 max memory_allocated 22901.44482421875 
[2024-12-25 06:50:32 root] (abq_llm.py 321): INFO layer 11 iter 8 loss:0.34454455971717834 norm:0.0023336021695286036 max memory_allocated 22901.44482421875 
[2024-12-25 06:51:04 root] (abq_llm.py 321): INFO layer 11 iter 9 loss:0.3436949551105499 norm:0.0022722952999174595 max memory_allocated 22901.44482421875 
[2024-12-25 06:51:36 root] (abq_llm.py 321): INFO layer 11 iter 10 loss:0.34315288066864014 norm:0.002306204754859209 max memory_allocated 22901.44482421875 
[2024-12-25 06:52:07 root] (abq_llm.py 321): INFO layer 11 iter 11 loss:0.34266749024391174 norm:0.002367145847529173 max memory_allocated 22901.44482421875 
[2024-12-25 06:52:39 root] (abq_llm.py 321): INFO layer 11 iter 12 loss:0.3421672284603119 norm:0.0023889774456620216 max memory_allocated 22901.44482421875 
[2024-12-25 06:53:11 root] (abq_llm.py 321): INFO layer 11 iter 13 loss:0.3415995240211487 norm:0.0024105014745146036 max memory_allocated 22901.44482421875 
[2024-12-25 06:53:43 root] (abq_llm.py 321): INFO layer 11 iter 14 loss:0.34127137064933777 norm:0.0023470274172723293 max memory_allocated 22901.44482421875 
[2024-12-25 06:54:15 root] (abq_llm.py 321): INFO layer 11 iter 15 loss:0.3408861756324768 norm:0.0023520507384091616 max memory_allocated 22901.44482421875 
[2024-12-25 06:54:46 root] (abq_llm.py 321): INFO layer 11 iter 16 loss:0.34069040417671204 norm:0.0023291299585253 max memory_allocated 22901.44482421875 
[2024-12-25 06:55:18 root] (abq_llm.py 321): INFO layer 11 iter 17 loss:0.34044942259788513 norm:0.00228523975238204 max memory_allocated 22901.44482421875 
[2024-12-25 06:55:50 root] (abq_llm.py 321): INFO layer 11 iter 18 loss:0.34036263823509216 norm:0.002258799970149994 max memory_allocated 22901.44482421875 
[2024-12-25 06:56:22 root] (abq_llm.py 321): INFO layer 11 iter 19 loss:0.34014105796813965 norm:0.0021803525742143393 max memory_allocated 22901.44482421875 
[2024-12-25 06:56:31 root] (abq_llm.py 208): INFO === Start quantize layer 12 ===
[2024-12-25 06:57:05 root] (abq_llm.py 321): INFO layer 12 iter 0 loss:0.4466050863265991 norm:0.024413246661424637 max memory_allocated 22903.11669921875 
[2024-12-25 06:57:37 root] (abq_llm.py 321): INFO layer 12 iter 1 loss:0.41654592752456665 norm:0.012212731875479221 max memory_allocated 22903.11669921875 
[2024-12-25 06:58:09 root] (abq_llm.py 321): INFO layer 12 iter 2 loss:0.3917437791824341 norm:0.007138506509363651 max memory_allocated 22903.11669921875 
[2024-12-25 06:58:40 root] (abq_llm.py 321): INFO layer 12 iter 3 loss:0.3767625391483307 norm:0.00482466584071517 max memory_allocated 22903.11669921875 
[2024-12-25 06:59:12 root] (abq_llm.py 321): INFO layer 12 iter 4 loss:0.3699304461479187 norm:0.003997447434812784 max memory_allocated 22903.11669921875 
[2024-12-25 06:59:44 root] (abq_llm.py 321): INFO layer 12 iter 5 loss:0.3656090497970581 norm:0.0034520127810537815 max memory_allocated 22903.11669921875 
[2024-12-25 07:00:16 root] (abq_llm.py 321): INFO layer 12 iter 6 loss:0.36264997720718384 norm:0.0031681889668107033 max memory_allocated 22903.11669921875 
[2024-12-25 07:00:47 root] (abq_llm.py 321): INFO layer 12 iter 7 loss:0.3607284426689148 norm:0.0030793992336839437 max memory_allocated 22903.11669921875 
[2024-12-25 07:01:19 root] (abq_llm.py 321): INFO layer 12 iter 8 loss:0.3591012954711914 norm:0.0029060342349112034 max memory_allocated 22903.11669921875 
[2024-12-25 07:01:51 root] (abq_llm.py 321): INFO layer 12 iter 9 loss:0.35780420899391174 norm:0.0027259672060608864 max memory_allocated 22903.11669921875 
[2024-12-25 07:02:23 root] (abq_llm.py 321): INFO layer 12 iter 10 loss:0.3568037152290344 norm:0.002640585647895932 max memory_allocated 22903.11669921875 
[2024-12-25 07:02:54 root] (abq_llm.py 321): INFO layer 12 iter 11 loss:0.3560318350791931 norm:0.002649320987984538 max memory_allocated 22903.11669921875 
[2024-12-25 07:03:26 root] (abq_llm.py 321): INFO layer 12 iter 12 loss:0.35530489683151245 norm:0.002601630985736847 max memory_allocated 22903.11669921875 
[2024-12-25 07:03:58 root] (abq_llm.py 321): INFO layer 12 iter 13 loss:0.3547514081001282 norm:0.0025521472562104464 max memory_allocated 22903.11669921875 
[2024-12-25 07:04:30 root] (abq_llm.py 321): INFO layer 12 iter 14 loss:0.35433799028396606 norm:0.0024993228726089 max memory_allocated 22903.11669921875 
[2024-12-25 07:05:02 root] (abq_llm.py 321): INFO layer 12 iter 15 loss:0.35388439893722534 norm:0.002463254379108548 max memory_allocated 22903.11669921875 
[2024-12-25 07:05:33 root] (abq_llm.py 321): INFO layer 12 iter 16 loss:0.35353532433509827 norm:0.002380796242505312 max memory_allocated 22903.11669921875 
[2024-12-25 07:06:05 root] (abq_llm.py 321): INFO layer 12 iter 17 loss:0.3532072901725769 norm:0.0023632096126675606 max memory_allocated 22903.11669921875 
[2024-12-25 07:06:37 root] (abq_llm.py 321): INFO layer 12 iter 18 loss:0.35292187333106995 norm:0.002310108859091997 max memory_allocated 22903.11669921875 
[2024-12-25 07:07:09 root] (abq_llm.py 321): INFO layer 12 iter 19 loss:0.35261988639831543 norm:0.0022601312957704067 max memory_allocated 22903.11669921875 
[2024-12-25 07:07:18 root] (abq_llm.py 208): INFO === Start quantize layer 13 ===
[2024-12-25 07:07:52 root] (abq_llm.py 321): INFO layer 13 iter 0 loss:0.44959259033203125 norm:0.019950877875089645 max memory_allocated 22904.78857421875 
[2024-12-25 07:08:24 root] (abq_llm.py 321): INFO layer 13 iter 1 loss:0.419231116771698 norm:0.008069364354014397 max memory_allocated 22904.78857421875 
[2024-12-25 07:08:55 root] (abq_llm.py 321): INFO layer 13 iter 2 loss:0.39683088660240173 norm:0.0052057597786188126 max memory_allocated 22904.78857421875 
[2024-12-25 07:09:27 root] (abq_llm.py 321): INFO layer 13 iter 3 loss:0.38384681940078735 norm:0.0034831271041184664 max memory_allocated 22904.78857421875 
[2024-12-25 07:09:59 root] (abq_llm.py 321): INFO layer 13 iter 4 loss:0.37818092107772827 norm:0.003366681979969144 max memory_allocated 22904.78857421875 
[2024-12-25 07:10:31 root] (abq_llm.py 321): INFO layer 13 iter 5 loss:0.3751629590988159 norm:0.0025338437408208847 max memory_allocated 22904.78857421875 
[2024-12-25 07:11:02 root] (abq_llm.py 321): INFO layer 13 iter 6 loss:0.3726545572280884 norm:0.0024619183968752623 max memory_allocated 22904.78857421875 
[2024-12-25 07:11:34 root] (abq_llm.py 321): INFO layer 13 iter 7 loss:0.37086236476898193 norm:0.0025810017250478268 max memory_allocated 22904.78857421875 
[2024-12-25 07:12:06 root] (abq_llm.py 321): INFO layer 13 iter 8 loss:0.3696690797805786 norm:0.0023067982401698828 max memory_allocated 22904.78857421875 
[2024-12-25 07:12:38 root] (abq_llm.py 321): INFO layer 13 iter 9 loss:0.3687988519668579 norm:0.0024523702450096607 max memory_allocated 22904.78857421875 
[2024-12-25 07:13:09 root] (abq_llm.py 321): INFO layer 13 iter 10 loss:0.36796820163726807 norm:0.0023356007877737284 max memory_allocated 22904.78857421875 
[2024-12-25 07:13:41 root] (abq_llm.py 321): INFO layer 13 iter 11 loss:0.36736491322517395 norm:0.0024532978422939777 max memory_allocated 22904.78857421875 
[2024-12-25 07:14:13 root] (abq_llm.py 321): INFO layer 13 iter 12 loss:0.366802453994751 norm:0.002400539815425873 max memory_allocated 22904.78857421875 
[2024-12-25 07:14:44 root] (abq_llm.py 321): INFO layer 13 iter 13 loss:0.36650845408439636 norm:0.002407328225672245 max memory_allocated 22904.78857421875 
[2024-12-25 07:15:16 root] (abq_llm.py 321): INFO layer 13 iter 14 loss:0.3661946654319763 norm:0.002295831451192498 max memory_allocated 22904.78857421875 
[2024-12-25 07:15:48 root] (abq_llm.py 321): INFO layer 13 iter 15 loss:0.36601540446281433 norm:0.0022403080947697163 max memory_allocated 22904.78857421875 
[2024-12-25 07:16:20 root] (abq_llm.py 321): INFO layer 13 iter 16 loss:0.3657892346382141 norm:0.0023146062158048153 max memory_allocated 22904.78857421875 
[2024-12-25 07:16:51 root] (abq_llm.py 321): INFO layer 13 iter 17 loss:0.36529475450515747 norm:0.0022937392350286245 max memory_allocated 22904.78857421875 
[2024-12-25 07:17:23 root] (abq_llm.py 321): INFO layer 13 iter 18 loss:0.36483991146087646 norm:0.00217634835280478 max memory_allocated 22904.78857421875 
[2024-12-25 07:17:55 root] (abq_llm.py 321): INFO layer 13 iter 19 loss:0.3646436929702759 norm:0.0020951125770807266 max memory_allocated 22904.78857421875 
[2024-12-25 07:18:04 root] (abq_llm.py 208): INFO === Start quantize layer 14 ===
[2024-12-25 07:18:38 root] (abq_llm.py 321): INFO layer 14 iter 0 loss:0.5037075877189636 norm:0.044848084449768066 max memory_allocated 22906.46044921875 
[2024-12-25 07:19:10 root] (abq_llm.py 321): INFO layer 14 iter 1 loss:0.46729183197021484 norm:0.021228589117527008 max memory_allocated 22906.46044921875 
[2024-12-25 07:19:41 root] (abq_llm.py 321): INFO layer 14 iter 2 loss:0.43555283546447754 norm:0.012454941868782043 max memory_allocated 22906.46044921875 
[2024-12-25 07:20:13 root] (abq_llm.py 321): INFO layer 14 iter 3 loss:0.4167513847351074 norm:0.007524264510720968 max memory_allocated 22906.46044921875 
[2024-12-25 07:20:45 root] (abq_llm.py 321): INFO layer 14 iter 4 loss:0.40887606143951416 norm:0.005839141551405191 max memory_allocated 22906.46044921875 
[2024-12-25 07:21:16 root] (abq_llm.py 321): INFO layer 14 iter 5 loss:0.40415066480636597 norm:0.004853368736803532 max memory_allocated 22906.46044921875 
[2024-12-25 07:21:48 root] (abq_llm.py 321): INFO layer 14 iter 6 loss:0.40062153339385986 norm:0.004210802260786295 max memory_allocated 22906.46044921875 
[2024-12-25 07:22:20 root] (abq_llm.py 321): INFO layer 14 iter 7 loss:0.3982372283935547 norm:0.0038077114149928093 max memory_allocated 22906.46044921875 
[2024-12-25 07:22:52 root] (abq_llm.py 321): INFO layer 14 iter 8 loss:0.3962331712245941 norm:0.0034171296283602715 max memory_allocated 22906.46044921875 
[2024-12-25 07:23:23 root] (abq_llm.py 321): INFO layer 14 iter 9 loss:0.3947291970252991 norm:0.003136902814731002 max memory_allocated 22906.46044921875 
[2024-12-25 07:23:55 root] (abq_llm.py 321): INFO layer 14 iter 10 loss:0.3935948610305786 norm:0.0030325669795274734 max memory_allocated 22906.46044921875 
[2024-12-25 07:24:27 root] (abq_llm.py 321): INFO layer 14 iter 11 loss:0.3924451470375061 norm:0.0028194559272378683 max memory_allocated 22906.46044921875 
[2024-12-25 07:24:58 root] (abq_llm.py 321): INFO layer 14 iter 12 loss:0.3918606638908386 norm:0.002774799009785056 max memory_allocated 22906.46044921875 
[2024-12-25 07:25:30 root] (abq_llm.py 321): INFO layer 14 iter 13 loss:0.3911363482475281 norm:0.0026428133714944124 max memory_allocated 22906.46044921875 
[2024-12-25 07:26:02 root] (abq_llm.py 321): INFO layer 14 iter 14 loss:0.3905685544013977 norm:0.002526681637391448 max memory_allocated 22906.46044921875 
[2024-12-25 07:26:34 root] (abq_llm.py 321): INFO layer 14 iter 15 loss:0.3901936411857605 norm:0.0025372139643877745 max memory_allocated 22906.46044921875 
[2024-12-25 07:27:05 root] (abq_llm.py 321): INFO layer 14 iter 16 loss:0.389896422624588 norm:0.002487350255250931 max memory_allocated 22906.46044921875 
[2024-12-25 07:27:37 root] (abq_llm.py 321): INFO layer 14 iter 17 loss:0.3894994258880615 norm:0.002389596775174141 max memory_allocated 22906.46044921875 
[2024-12-25 07:28:09 root] (abq_llm.py 321): INFO layer 14 iter 18 loss:0.38922274112701416 norm:0.0023443347308784723 max memory_allocated 22906.46044921875 
[2024-12-25 07:28:41 root] (abq_llm.py 321): INFO layer 14 iter 19 loss:0.3890259563922882 norm:0.002305450849235058 max memory_allocated 22906.46044921875 
[2024-12-25 07:28:50 root] (abq_llm.py 208): INFO === Start quantize layer 15 ===
[2024-12-25 07:29:24 root] (abq_llm.py 321): INFO layer 15 iter 0 loss:0.49865108728408813 norm:0.02651764638721943 max memory_allocated 22908.13232421875 
[2024-12-25 07:29:56 root] (abq_llm.py 321): INFO layer 15 iter 1 loss:0.4727078378200531 norm:0.011774051934480667 max memory_allocated 22908.13232421875 
[2024-12-25 07:30:27 root] (abq_llm.py 321): INFO layer 15 iter 2 loss:0.45186150074005127 norm:0.007466907612979412 max memory_allocated 22908.13232421875 
[2024-12-25 07:30:59 root] (abq_llm.py 321): INFO layer 15 iter 3 loss:0.4379722774028778 norm:0.004156571812927723 max memory_allocated 22908.13232421875 
[2024-12-25 07:31:31 root] (abq_llm.py 321): INFO layer 15 iter 4 loss:0.431860089302063 norm:0.0035840095952153206 max memory_allocated 22908.13232421875 
[2024-12-25 07:32:03 root] (abq_llm.py 321): INFO layer 15 iter 5 loss:0.4277220070362091 norm:0.0029577873647212982 max memory_allocated 22908.13232421875 
[2024-12-25 07:32:34 root] (abq_llm.py 321): INFO layer 15 iter 6 loss:0.42511600255966187 norm:0.0029228366911411285 max memory_allocated 22908.13232421875 
[2024-12-25 07:33:06 root] (abq_llm.py 321): INFO layer 15 iter 7 loss:0.4232250452041626 norm:0.002902054460719228 max memory_allocated 22908.13232421875 
[2024-12-25 07:33:38 root] (abq_llm.py 321): INFO layer 15 iter 8 loss:0.4216880798339844 norm:0.0027754756156355143 max memory_allocated 22908.13232421875 
[2024-12-25 07:34:09 root] (abq_llm.py 321): INFO layer 15 iter 9 loss:0.42057931423187256 norm:0.0028307349421083927 max memory_allocated 22908.13232421875 
[2024-12-25 07:34:41 root] (abq_llm.py 321): INFO layer 15 iter 10 loss:0.41981837153434753 norm:0.0027452432550489902 max memory_allocated 22908.13232421875 
[2024-12-25 07:35:13 root] (abq_llm.py 321): INFO layer 15 iter 11 loss:0.41915327310562134 norm:0.002662966027855873 max memory_allocated 22908.13232421875 
[2024-12-25 07:35:44 root] (abq_llm.py 321): INFO layer 15 iter 12 loss:0.4186076521873474 norm:0.0026410603895783424 max memory_allocated 22908.13232421875 
[2024-12-25 07:36:16 root] (abq_llm.py 321): INFO layer 15 iter 13 loss:0.4181172251701355 norm:0.002665545791387558 max memory_allocated 22908.13232421875 
[2024-12-25 07:36:48 root] (abq_llm.py 321): INFO layer 15 iter 14 loss:0.41766107082366943 norm:0.002669018227607012 max memory_allocated 22908.13232421875 
[2024-12-25 07:37:19 root] (abq_llm.py 321): INFO layer 15 iter 15 loss:0.41726988554000854 norm:0.002585581736639142 max memory_allocated 22908.13232421875 
[2024-12-25 07:37:51 root] (abq_llm.py 321): INFO layer 15 iter 16 loss:0.416885107755661 norm:0.002514204243198037 max memory_allocated 22908.13232421875 
[2024-12-25 07:38:23 root] (abq_llm.py 321): INFO layer 15 iter 17 loss:0.4165925085544586 norm:0.0025322979781776667 max memory_allocated 22908.13232421875 
[2024-12-25 07:38:55 root] (abq_llm.py 321): INFO layer 15 iter 18 loss:0.4163970351219177 norm:0.0025229440070688725 max memory_allocated 22908.13232421875 
[2024-12-25 07:39:26 root] (abq_llm.py 321): INFO layer 15 iter 19 loss:0.4161153733730316 norm:0.0025540057104080915 max memory_allocated 22908.13232421875 
[2024-12-25 07:39:35 root] (abq_llm.py 208): INFO === Start quantize layer 16 ===
[2024-12-25 07:40:10 root] (abq_llm.py 321): INFO layer 16 iter 0 loss:0.5675709247589111 norm:0.03417902812361717 max memory_allocated 22909.80419921875 
[2024-12-25 07:40:41 root] (abq_llm.py 321): INFO layer 16 iter 1 loss:0.5292236804962158 norm:0.016133807599544525 max memory_allocated 22909.80419921875 
[2024-12-25 07:41:13 root] (abq_llm.py 321): INFO layer 16 iter 2 loss:0.5001195073127747 norm:0.009640363045036793 max memory_allocated 22909.80419921875 
[2024-12-25 07:41:45 root] (abq_llm.py 321): INFO layer 16 iter 3 loss:0.48235219717025757 norm:0.005826158449053764 max memory_allocated 22909.80419921875 
[2024-12-25 07:42:16 root] (abq_llm.py 321): INFO layer 16 iter 4 loss:0.47446906566619873 norm:0.004408855922520161 max memory_allocated 22909.80419921875 
[2024-12-25 07:42:48 root] (abq_llm.py 321): INFO layer 16 iter 5 loss:0.4700750708580017 norm:0.004009679891169071 max memory_allocated 22909.80419921875 
[2024-12-25 07:43:20 root] (abq_llm.py 321): INFO layer 16 iter 6 loss:0.4668351411819458 norm:0.003803092520684004 max memory_allocated 22909.80419921875 
[2024-12-25 07:43:51 root] (abq_llm.py 321): INFO layer 16 iter 7 loss:0.4643624424934387 norm:0.0035669878125190735 max memory_allocated 22909.80419921875 
[2024-12-25 07:44:23 root] (abq_llm.py 321): INFO layer 16 iter 8 loss:0.46261855959892273 norm:0.0033963152673095465 max memory_allocated 22909.80419921875 
[2024-12-25 07:44:55 root] (abq_llm.py 321): INFO layer 16 iter 9 loss:0.46117597818374634 norm:0.0032665706239640713 max memory_allocated 22909.80419921875 
[2024-12-25 07:45:27 root] (abq_llm.py 321): INFO layer 16 iter 10 loss:0.4599663019180298 norm:0.0031427815556526184 max memory_allocated 22909.80419921875 
[2024-12-25 07:45:58 root] (abq_llm.py 321): INFO layer 16 iter 11 loss:0.4589653015136719 norm:0.0030062138102948666 max memory_allocated 22909.80419921875 
[2024-12-25 07:46:30 root] (abq_llm.py 321): INFO layer 16 iter 12 loss:0.4580938220024109 norm:0.0029333047568798065 max memory_allocated 22909.80419921875 
[2024-12-25 07:47:02 root] (abq_llm.py 321): INFO layer 16 iter 13 loss:0.45746633410453796 norm:0.0028596660122275352 max memory_allocated 22909.80419921875 
[2024-12-25 07:47:34 root] (abq_llm.py 321): INFO layer 16 iter 14 loss:0.4570022225379944 norm:0.0028335792012512684 max memory_allocated 22909.80419921875 
[2024-12-25 07:48:05 root] (abq_llm.py 321): INFO layer 16 iter 15 loss:0.4565632939338684 norm:0.002817674307152629 max memory_allocated 22909.80419921875 
[2024-12-25 07:48:37 root] (abq_llm.py 321): INFO layer 16 iter 16 loss:0.45607802271842957 norm:0.0027461459394544363 max memory_allocated 22909.80419921875 
[2024-12-25 07:49:09 root] (abq_llm.py 321): INFO layer 16 iter 17 loss:0.4557068645954132 norm:0.002778722671791911 max memory_allocated 22909.80419921875 
[2024-12-25 07:49:40 root] (abq_llm.py 321): INFO layer 16 iter 18 loss:0.45547881722450256 norm:0.0027381989639252424 max memory_allocated 22909.80419921875 
[2024-12-25 07:50:12 root] (abq_llm.py 321): INFO layer 16 iter 19 loss:0.4551772475242615 norm:0.00266076042316854 max memory_allocated 22909.80419921875 
[2024-12-25 07:50:21 root] (abq_llm.py 208): INFO === Start quantize layer 17 ===
[2024-12-25 07:50:55 root] (abq_llm.py 321): INFO layer 17 iter 0 loss:0.6076964139938354 norm:0.03139744699001312 max memory_allocated 22911.47607421875 
[2024-12-25 07:51:27 root] (abq_llm.py 321): INFO layer 17 iter 1 loss:0.5738297700881958 norm:0.012891007587313652 max memory_allocated 22911.47607421875 
[2024-12-25 07:51:59 root] (abq_llm.py 321): INFO layer 17 iter 2 loss:0.5476987361907959 norm:0.007284635677933693 max memory_allocated 22911.47607421875 
[2024-12-25 07:52:30 root] (abq_llm.py 321): INFO layer 17 iter 3 loss:0.5321316719055176 norm:0.004529287107288837 max memory_allocated 22911.47607421875 
[2024-12-25 07:53:02 root] (abq_llm.py 321): INFO layer 17 iter 4 loss:0.5249674320220947 norm:0.0033885359298437834 max memory_allocated 22911.47607421875 
[2024-12-25 07:53:34 root] (abq_llm.py 321): INFO layer 17 iter 5 loss:0.5210157632827759 norm:0.0031067878007888794 max memory_allocated 22911.47607421875 
[2024-12-25 07:54:05 root] (abq_llm.py 321): INFO layer 17 iter 6 loss:0.5181263089179993 norm:0.0029356139712035656 max memory_allocated 22911.47607421875 
[2024-12-25 07:54:37 root] (abq_llm.py 321): INFO layer 17 iter 7 loss:0.5156913995742798 norm:0.0027081742882728577 max memory_allocated 22911.47607421875 
[2024-12-25 07:55:09 root] (abq_llm.py 321): INFO layer 17 iter 8 loss:0.5141177773475647 norm:0.002666408196091652 max memory_allocated 22911.47607421875 
[2024-12-25 07:55:40 root] (abq_llm.py 321): INFO layer 17 iter 9 loss:0.5128064155578613 norm:0.0025912451092153788 max memory_allocated 22911.47607421875 
[2024-12-25 07:56:12 root] (abq_llm.py 321): INFO layer 17 iter 10 loss:0.5116843581199646 norm:0.0025268890894949436 max memory_allocated 22911.47607421875 
[2024-12-25 07:56:44 root] (abq_llm.py 321): INFO layer 17 iter 11 loss:0.5108458399772644 norm:0.0024241942446678877 max memory_allocated 22911.47607421875 
[2024-12-25 07:57:15 root] (abq_llm.py 321): INFO layer 17 iter 12 loss:0.5100783705711365 norm:0.00235917535610497 max memory_allocated 22911.47607421875 
[2024-12-25 07:57:47 root] (abq_llm.py 321): INFO layer 17 iter 13 loss:0.5095725059509277 norm:0.002312928903847933 max memory_allocated 22911.47607421875 
[2024-12-25 07:58:19 root] (abq_llm.py 321): INFO layer 17 iter 14 loss:0.5091710686683655 norm:0.0023254842963069677 max memory_allocated 22911.47607421875 
[2024-12-25 07:58:50 root] (abq_llm.py 321): INFO layer 17 iter 15 loss:0.5086786150932312 norm:0.0023089812602847815 max memory_allocated 22911.47607421875 
[2024-12-25 07:59:22 root] (abq_llm.py 321): INFO layer 17 iter 16 loss:0.508185625076294 norm:0.002267474541440606 max memory_allocated 22911.47607421875 
[2024-12-25 07:59:54 root] (abq_llm.py 321): INFO layer 17 iter 17 loss:0.5078209638595581 norm:0.002245968207716942 max memory_allocated 22911.47607421875 
[2024-12-25 08:00:25 root] (abq_llm.py 321): INFO layer 17 iter 18 loss:0.5075757503509521 norm:0.002223153365775943 max memory_allocated 22911.47607421875 
[2024-12-25 08:00:57 root] (abq_llm.py 321): INFO layer 17 iter 19 loss:0.5072432160377502 norm:0.002212649444118142 max memory_allocated 22911.47607421875 
[2024-12-25 08:01:06 root] (abq_llm.py 208): INFO === Start quantize layer 18 ===
[2024-12-25 08:01:41 root] (abq_llm.py 321): INFO layer 18 iter 0 loss:0.6707438230514526 norm:0.031857553869485855 max memory_allocated 22913.14794921875 
[2024-12-25 08:02:12 root] (abq_llm.py 321): INFO layer 18 iter 1 loss:0.6404669880867004 norm:0.015072209760546684 max memory_allocated 22913.14794921875 
[2024-12-25 08:02:44 root] (abq_llm.py 321): INFO layer 18 iter 2 loss:0.6129271984100342 norm:0.008480602875351906 max memory_allocated 22913.14794921875 
[2024-12-25 08:03:15 root] (abq_llm.py 321): INFO layer 18 iter 3 loss:0.598896324634552 norm:0.005385932512581348 max memory_allocated 22913.14794921875 
[2024-12-25 08:03:47 root] (abq_llm.py 321): INFO layer 18 iter 4 loss:0.5925317406654358 norm:0.0041395919397473335 max memory_allocated 22913.14794921875 
[2024-12-25 08:04:19 root] (abq_llm.py 321): INFO layer 18 iter 5 loss:0.5892188549041748 norm:0.0037963700015097857 max memory_allocated 22913.14794921875 
[2024-12-25 08:04:51 root] (abq_llm.py 321): INFO layer 18 iter 6 loss:0.5863805413246155 norm:0.0034510688856244087 max memory_allocated 22913.14794921875 
[2024-12-25 08:05:22 root] (abq_llm.py 321): INFO layer 18 iter 7 loss:0.5842968225479126 norm:0.0032237940467894077 max memory_allocated 22913.14794921875 
[2024-12-25 08:05:54 root] (abq_llm.py 321): INFO layer 18 iter 8 loss:0.5826491117477417 norm:0.003033694578334689 max memory_allocated 22913.14794921875 
[2024-12-25 08:06:26 root] (abq_llm.py 321): INFO layer 18 iter 9 loss:0.5812191963195801 norm:0.002838944783434272 max memory_allocated 22913.14794921875 
[2024-12-25 08:06:57 root] (abq_llm.py 321): INFO layer 18 iter 10 loss:0.5797100067138672 norm:0.002430116757750511 max memory_allocated 22913.14794921875 
[2024-12-25 08:07:29 root] (abq_llm.py 321): INFO layer 18 iter 11 loss:0.5788534879684448 norm:0.002372763119637966 max memory_allocated 22913.14794921875 
[2024-12-25 08:08:01 root] (abq_llm.py 321): INFO layer 18 iter 12 loss:0.5782591700553894 norm:0.002376392250880599 max memory_allocated 22913.14794921875 
[2024-12-25 08:08:32 root] (abq_llm.py 321): INFO layer 18 iter 13 loss:0.577788233757019 norm:0.0022973036393523216 max memory_allocated 22913.14794921875 
[2024-12-25 08:09:04 root] (abq_llm.py 321): INFO layer 18 iter 14 loss:0.5773829221725464 norm:0.002339374041184783 max memory_allocated 22913.14794921875 
[2024-12-25 08:09:36 root] (abq_llm.py 321): INFO layer 18 iter 15 loss:0.5770241022109985 norm:0.0023130925837904215 max memory_allocated 22913.14794921875 
[2024-12-25 08:10:08 root] (abq_llm.py 321): INFO layer 18 iter 16 loss:0.5767611861228943 norm:0.0022711753845214844 max memory_allocated 22913.14794921875 
[2024-12-25 08:10:39 root] (abq_llm.py 321): INFO layer 18 iter 17 loss:0.5765727162361145 norm:0.0022847733926028013 max memory_allocated 22913.14794921875 
[2024-12-25 08:11:11 root] (abq_llm.py 321): INFO layer 18 iter 18 loss:0.5763676762580872 norm:0.0023486206773668528 max memory_allocated 22913.14794921875 
[2024-12-25 08:11:43 root] (abq_llm.py 321): INFO layer 18 iter 19 loss:0.5760344862937927 norm:0.0023815236054360867 max memory_allocated 22913.14794921875 
[2024-12-25 08:11:52 root] (abq_llm.py 208): INFO === Start quantize layer 19 ===
[2024-12-25 08:12:26 root] (abq_llm.py 321): INFO layer 19 iter 0 loss:0.7440803647041321 norm:0.026068828999996185 max memory_allocated 22914.81982421875 
[2024-12-25 08:12:58 root] (abq_llm.py 321): INFO layer 19 iter 1 loss:0.7196457982063293 norm:0.015115266665816307 max memory_allocated 22914.81982421875 
[2024-12-25 08:13:29 root] (abq_llm.py 321): INFO layer 19 iter 2 loss:0.6987513303756714 norm:0.009641267359256744 max memory_allocated 22914.81982421875 
[2024-12-25 08:14:01 root] (abq_llm.py 321): INFO layer 19 iter 3 loss:0.6870553493499756 norm:0.006844589486718178 max memory_allocated 22914.81982421875 
[2024-12-25 08:14:33 root] (abq_llm.py 321): INFO layer 19 iter 4 loss:0.6813986301422119 norm:0.005611715838313103 max memory_allocated 22914.81982421875 
[2024-12-25 08:15:05 root] (abq_llm.py 321): INFO layer 19 iter 5 loss:0.6775009036064148 norm:0.004579027183353901 max memory_allocated 22914.81982421875 
[2024-12-25 08:15:36 root] (abq_llm.py 321): INFO layer 19 iter 6 loss:0.6746467351913452 norm:0.004018945619463921 max memory_allocated 22914.81982421875 
[2024-12-25 08:16:08 root] (abq_llm.py 321): INFO layer 19 iter 7 loss:0.6723508238792419 norm:0.0035792947746813297 max memory_allocated 22914.81982421875 
[2024-12-25 08:16:40 root] (abq_llm.py 321): INFO layer 19 iter 8 loss:0.6706728935241699 norm:0.003293168032541871 max memory_allocated 22914.81982421875 
[2024-12-25 08:17:12 root] (abq_llm.py 321): INFO layer 19 iter 9 loss:0.6690548658370972 norm:0.00294516165740788 max memory_allocated 22914.81982421875 
[2024-12-25 08:17:43 root] (abq_llm.py 321): INFO layer 19 iter 10 loss:0.6674139499664307 norm:0.0026789712719619274 max memory_allocated 22914.81982421875 
[2024-12-25 08:18:15 root] (abq_llm.py 321): INFO layer 19 iter 11 loss:0.6659783124923706 norm:0.002453741617500782 max memory_allocated 22914.81982421875 
[2024-12-25 08:18:47 root] (abq_llm.py 321): INFO layer 19 iter 12 loss:0.6653118133544922 norm:0.002429310930892825 max memory_allocated 22914.81982421875 
[2024-12-25 08:19:18 root] (abq_llm.py 321): INFO layer 19 iter 13 loss:0.6647435426712036 norm:0.002422371646389365 max memory_allocated 22914.81982421875 
[2024-12-25 08:19:50 root] (abq_llm.py 321): INFO layer 19 iter 14 loss:0.6643452644348145 norm:0.0024509739596396685 max memory_allocated 22914.81982421875 
[2024-12-25 08:20:22 root] (abq_llm.py 321): INFO layer 19 iter 15 loss:0.6639263033866882 norm:0.002537328517064452 max memory_allocated 22914.81982421875 
[2024-12-25 08:20:54 root] (abq_llm.py 321): INFO layer 19 iter 16 loss:0.6635228991508484 norm:0.002502395072951913 max memory_allocated 22914.81982421875 
[2024-12-25 08:21:25 root] (abq_llm.py 321): INFO layer 19 iter 17 loss:0.6631181836128235 norm:0.0024888659827411175 max memory_allocated 22914.81982421875 
[2024-12-25 08:21:57 root] (abq_llm.py 321): INFO layer 19 iter 18 loss:0.6628473401069641 norm:0.0024564615450799465 max memory_allocated 22914.81982421875 
[2024-12-25 08:22:29 root] (abq_llm.py 321): INFO layer 19 iter 19 loss:0.6627287268638611 norm:0.002455285983160138 max memory_allocated 22914.81982421875 
[2024-12-25 08:22:38 root] (abq_llm.py 208): INFO === Start quantize layer 20 ===
[2024-12-25 08:23:12 root] (abq_llm.py 321): INFO layer 20 iter 0 loss:0.8824484348297119 norm:0.029167640954256058 max memory_allocated 22916.49169921875 
[2024-12-25 08:23:44 root] (abq_llm.py 321): INFO layer 20 iter 1 loss:0.8538532257080078 norm:0.01607707515358925 max memory_allocated 22916.49169921875 
[2024-12-25 08:24:15 root] (abq_llm.py 321): INFO layer 20 iter 2 loss:0.827120304107666 norm:0.009543225169181824 max memory_allocated 22916.49169921875 
[2024-12-25 08:24:47 root] (abq_llm.py 321): INFO layer 20 iter 3 loss:0.8112962245941162 norm:0.0062874420545995235 max memory_allocated 22916.49169921875 
[2024-12-25 08:25:19 root] (abq_llm.py 321): INFO layer 20 iter 4 loss:0.8052196502685547 norm:0.005100535694509745 max memory_allocated 22916.49169921875 
[2024-12-25 08:25:51 root] (abq_llm.py 321): INFO layer 20 iter 5 loss:0.8010056018829346 norm:0.00441847974434495 max memory_allocated 22916.49169921875 
[2024-12-25 08:26:22 root] (abq_llm.py 321): INFO layer 20 iter 6 loss:0.7979306578636169 norm:0.003986193332821131 max memory_allocated 22916.49169921875 
[2024-12-25 08:26:54 root] (abq_llm.py 321): INFO layer 20 iter 7 loss:0.7958152294158936 norm:0.0037409919314086437 max memory_allocated 22916.49169921875 
[2024-12-25 08:27:26 root] (abq_llm.py 321): INFO layer 20 iter 8 loss:0.7934266328811646 norm:0.002913551637902856 max memory_allocated 22916.49169921875 
[2024-12-25 08:27:57 root] (abq_llm.py 321): INFO layer 20 iter 9 loss:0.7918606996536255 norm:0.0026999013498425484 max memory_allocated 22916.49169921875 
[2024-12-25 08:28:29 root] (abq_llm.py 321): INFO layer 20 iter 10 loss:0.7910292148590088 norm:0.0026628756895661354 max memory_allocated 22916.49169921875 
[2024-12-25 08:29:01 root] (abq_llm.py 321): INFO layer 20 iter 11 loss:0.790270209312439 norm:0.002614472759887576 max memory_allocated 22916.49169921875 
[2024-12-25 08:29:33 root] (abq_llm.py 321): INFO layer 20 iter 12 loss:0.7895557284355164 norm:0.002570288022980094 max memory_allocated 22916.49169921875 
[2024-12-25 08:30:04 root] (abq_llm.py 321): INFO layer 20 iter 13 loss:0.7889744639396667 norm:0.0025400537997484207 max memory_allocated 22916.49169921875 
[2024-12-25 08:30:36 root] (abq_llm.py 321): INFO layer 20 iter 14 loss:0.7885748147964478 norm:0.002529378980398178 max memory_allocated 22916.49169921875 
[2024-12-25 08:31:08 root] (abq_llm.py 321): INFO layer 20 iter 15 loss:0.7881485223770142 norm:0.0025169537402689457 max memory_allocated 22916.49169921875 
[2024-12-25 08:31:39 root] (abq_llm.py 321): INFO layer 20 iter 16 loss:0.7877451181411743 norm:0.0025289375334978104 max memory_allocated 22916.49169921875 
[2024-12-25 08:32:11 root] (abq_llm.py 321): INFO layer 20 iter 17 loss:0.7873549461364746 norm:0.002513596322387457 max memory_allocated 22916.49169921875 
[2024-12-25 08:32:43 root] (abq_llm.py 321): INFO layer 20 iter 18 loss:0.7871436476707458 norm:0.0025050113908946514 max memory_allocated 22916.49169921875 
[2024-12-25 08:33:14 root] (abq_llm.py 321): INFO layer 20 iter 19 loss:0.7869841456413269 norm:0.0025114763993769884 max memory_allocated 22916.49169921875 
[2024-12-25 08:33:23 root] (abq_llm.py 208): INFO === Start quantize layer 21 ===
[2024-12-25 08:33:58 root] (abq_llm.py 321): INFO layer 21 iter 0 loss:0.9994101524353027 norm:0.012568732723593712 max memory_allocated 22918.16357421875 
[2024-12-25 08:34:29 root] (abq_llm.py 321): INFO layer 21 iter 1 loss:0.9830148220062256 norm:0.008257372304797173 max memory_allocated 22918.16357421875 
[2024-12-25 08:35:01 root] (abq_llm.py 321): INFO layer 21 iter 2 loss:0.961851179599762 norm:0.0057953838258981705 max memory_allocated 22918.16357421875 
[2024-12-25 08:35:33 root] (abq_llm.py 321): INFO layer 21 iter 3 loss:0.9504673480987549 norm:0.004184299148619175 max memory_allocated 22918.16357421875 
[2024-12-25 08:36:04 root] (abq_llm.py 321): INFO layer 21 iter 4 loss:0.9442815780639648 norm:0.0035229341592639685 max memory_allocated 22918.16357421875 
[2024-12-25 08:36:36 root] (abq_llm.py 321): INFO layer 21 iter 5 loss:0.9411795735359192 norm:0.0034400601871311665 max memory_allocated 22918.16357421875 
[2024-12-25 08:37:08 root] (abq_llm.py 321): INFO layer 21 iter 6 loss:0.9389626383781433 norm:0.003410113975405693 max memory_allocated 22918.16357421875 
[2024-12-25 08:37:39 root] (abq_llm.py 321): INFO layer 21 iter 7 loss:0.9375219345092773 norm:0.003395977895706892 max memory_allocated 22918.16357421875 
[2024-12-25 08:38:11 root] (abq_llm.py 321): INFO layer 21 iter 8 loss:0.9360409379005432 norm:0.003259051823988557 max memory_allocated 22918.16357421875 
[2024-12-25 08:38:43 root] (abq_llm.py 321): INFO layer 21 iter 9 loss:0.9352283477783203 norm:0.0033346465788781643 max memory_allocated 22918.16357421875 
[2024-12-25 08:39:14 root] (abq_llm.py 321): INFO layer 21 iter 10 loss:0.9343287944793701 norm:0.003353062318637967 max memory_allocated 22918.16357421875 
[2024-12-25 08:39:46 root] (abq_llm.py 321): INFO layer 21 iter 11 loss:0.9335303902626038 norm:0.003200436942279339 max memory_allocated 22918.16357421875 
[2024-12-25 08:40:18 root] (abq_llm.py 321): INFO layer 21 iter 12 loss:0.933290958404541 norm:0.00325823575258255 max memory_allocated 22918.16357421875 
[2024-12-25 08:40:49 root] (abq_llm.py 321): INFO layer 21 iter 13 loss:0.9325200319290161 norm:0.0031645139679312706 max memory_allocated 22918.16357421875 
[2024-12-25 08:41:21 root] (abq_llm.py 321): INFO layer 21 iter 14 loss:0.9319313764572144 norm:0.0030677018221467733 max memory_allocated 22918.16357421875 
[2024-12-25 08:41:53 root] (abq_llm.py 321): INFO layer 21 iter 15 loss:0.9318653345108032 norm:0.0030590551905333996 max memory_allocated 22918.16357421875 
[2024-12-25 08:42:24 root] (abq_llm.py 321): INFO layer 21 iter 16 loss:0.9315515756607056 norm:0.00322650745511055 max memory_allocated 22918.16357421875 
[2024-12-25 08:42:56 root] (abq_llm.py 321): INFO layer 21 iter 17 loss:0.9308391213417053 norm:0.00295297522097826 max memory_allocated 22918.16357421875 
[2024-12-25 08:43:28 root] (abq_llm.py 321): INFO layer 21 iter 18 loss:0.9306601881980896 norm:0.0029829794075340033 max memory_allocated 22918.16357421875 
[2024-12-25 08:43:59 root] (abq_llm.py 321): INFO layer 21 iter 19 loss:0.9304409027099609 norm:0.0029815875459462404 max memory_allocated 22918.16357421875 
[2024-12-25 08:44:08 root] (abq_llm.py 208): INFO === Start quantize layer 22 ===
[2024-12-25 08:44:43 root] (abq_llm.py 321): INFO layer 22 iter 0 loss:1.1425862312316895 norm:0.013347914442420006 max memory_allocated 22919.83544921875 
[2024-12-25 08:45:14 root] (abq_llm.py 321): INFO layer 22 iter 1 loss:1.1215358972549438 norm:0.008569047786295414 max memory_allocated 22919.83544921875 
[2024-12-25 08:45:46 root] (abq_llm.py 321): INFO layer 22 iter 2 loss:1.1001033782958984 norm:0.005654746666550636 max memory_allocated 22919.83544921875 
[2024-12-25 08:46:18 root] (abq_llm.py 321): INFO layer 22 iter 3 loss:1.0909351110458374 norm:0.004565618000924587 max memory_allocated 22919.83544921875 
[2024-12-25 08:46:49 root] (abq_llm.py 321): INFO layer 22 iter 4 loss:1.0858924388885498 norm:0.003912158310413361 max memory_allocated 22919.83544921875 
[2024-12-25 08:47:21 root] (abq_llm.py 321): INFO layer 22 iter 5 loss:1.0824253559112549 norm:0.003839940531179309 max memory_allocated 22919.83544921875 
[2024-12-25 08:47:53 root] (abq_llm.py 321): INFO layer 22 iter 6 loss:1.0798051357269287 norm:0.0037646598648279905 max memory_allocated 22919.83544921875 
[2024-12-25 08:48:24 root] (abq_llm.py 321): INFO layer 22 iter 7 loss:1.0777567625045776 norm:0.0034722359851002693 max memory_allocated 22919.83544921875 
[2024-12-25 08:48:56 root] (abq_llm.py 321): INFO layer 22 iter 8 loss:1.0763756036758423 norm:0.003398638218641281 max memory_allocated 22919.83544921875 
[2024-12-25 08:49:28 root] (abq_llm.py 321): INFO layer 22 iter 9 loss:1.0752590894699097 norm:0.0032845870591700077 max memory_allocated 22919.83544921875 
[2024-12-25 08:49:59 root] (abq_llm.py 321): INFO layer 22 iter 10 loss:1.0744342803955078 norm:0.003144234884530306 max memory_allocated 22919.83544921875 
[2024-12-25 08:50:31 root] (abq_llm.py 321): INFO layer 22 iter 11 loss:1.0738751888275146 norm:0.003103813622146845 max memory_allocated 22919.83544921875 
[2024-12-25 08:51:03 root] (abq_llm.py 321): INFO layer 22 iter 12 loss:1.0733602046966553 norm:0.003141166875138879 max memory_allocated 22919.83544921875 
[2024-12-25 08:51:34 root] (abq_llm.py 321): INFO layer 22 iter 13 loss:1.0728158950805664 norm:0.0031144055537879467 max memory_allocated 22919.83544921875 
[2024-12-25 08:52:06 root] (abq_llm.py 321): INFO layer 22 iter 14 loss:1.0724866390228271 norm:0.0029920702800154686 max memory_allocated 22919.83544921875 
[2024-12-25 08:52:37 root] (abq_llm.py 321): INFO layer 22 iter 15 loss:1.072292685508728 norm:0.0030313306488096714 max memory_allocated 22919.83544921875 
[2024-12-25 08:53:09 root] (abq_llm.py 321): INFO layer 22 iter 16 loss:1.0721244812011719 norm:0.0031440388411283493 max memory_allocated 22919.83544921875 
[2024-12-25 08:53:41 root] (abq_llm.py 321): INFO layer 22 iter 17 loss:1.071887493133545 norm:0.003062531352043152 max memory_allocated 22919.83544921875 
[2024-12-25 08:54:12 root] (abq_llm.py 321): INFO layer 22 iter 18 loss:1.071697473526001 norm:0.0030916829127818346 max memory_allocated 22919.83544921875 
[2024-12-25 08:54:44 root] (abq_llm.py 321): INFO layer 22 iter 19 loss:1.071567177772522 norm:0.0029306665528565645 max memory_allocated 22919.83544921875 
[2024-12-25 08:54:53 root] (abq_llm.py 208): INFO === Start quantize layer 23 ===
[2024-12-25 08:55:27 root] (abq_llm.py 321): INFO layer 23 iter 0 loss:1.3396923542022705 norm:0.022247541695833206 max memory_allocated 22921.50732421875 
[2024-12-25 08:55:59 root] (abq_llm.py 321): INFO layer 23 iter 1 loss:1.3140976428985596 norm:0.015274171717464924 max memory_allocated 22921.50732421875 
[2024-12-25 08:56:30 root] (abq_llm.py 321): INFO layer 23 iter 2 loss:1.285314917564392 norm:0.00936293788254261 max memory_allocated 22921.50732421875 
[2024-12-25 08:57:02 root] (abq_llm.py 321): INFO layer 23 iter 3 loss:1.271060824394226 norm:0.0063459486700594425 max memory_allocated 22921.50732421875 
[2024-12-25 08:57:34 root] (abq_llm.py 321): INFO layer 23 iter 4 loss:1.2632304430007935 norm:0.0052963742054998875 max memory_allocated 22921.50732421875 
[2024-12-25 08:58:05 root] (abq_llm.py 321): INFO layer 23 iter 5 loss:1.2584377527236938 norm:0.004915925208479166 max memory_allocated 22921.50732421875 
[2024-12-25 08:58:37 root] (abq_llm.py 321): INFO layer 23 iter 6 loss:1.2549723386764526 norm:0.004506198689341545 max memory_allocated 22921.50732421875 
[2024-12-25 08:59:09 root] (abq_llm.py 321): INFO layer 23 iter 7 loss:1.252699375152588 norm:0.004512152634561062 max memory_allocated 22921.50732421875 
[2024-12-25 08:59:40 root] (abq_llm.py 321): INFO layer 23 iter 8 loss:1.251212239265442 norm:0.004759552888572216 max memory_allocated 22921.50732421875 
[2024-12-25 09:00:12 root] (abq_llm.py 321): INFO layer 23 iter 9 loss:1.249969244003296 norm:0.004389401059597731 max memory_allocated 22921.50732421875 
[2024-12-25 09:00:44 root] (abq_llm.py 321): INFO layer 23 iter 10 loss:1.2488809823989868 norm:0.004276962485164404 max memory_allocated 22921.50732421875 
[2024-12-25 09:01:15 root] (abq_llm.py 321): INFO layer 23 iter 11 loss:1.2483325004577637 norm:0.004249602556228638 max memory_allocated 22921.50732421875 
[2024-12-25 09:01:47 root] (abq_llm.py 321): INFO layer 23 iter 12 loss:1.2478127479553223 norm:0.004316512495279312 max memory_allocated 22921.50732421875 
[2024-12-25 09:02:19 root] (abq_llm.py 321): INFO layer 23 iter 13 loss:1.2472628355026245 norm:0.004202791955322027 max memory_allocated 22921.50732421875 
[2024-12-25 09:02:50 root] (abq_llm.py 321): INFO layer 23 iter 14 loss:1.2468512058258057 norm:0.004304674454033375 max memory_allocated 22921.50732421875 
[2024-12-25 09:03:22 root] (abq_llm.py 321): INFO layer 23 iter 15 loss:1.2465779781341553 norm:0.00431473832577467 max memory_allocated 22921.50732421875 
[2024-12-25 09:03:54 root] (abq_llm.py 321): INFO layer 23 iter 16 loss:1.2461570501327515 norm:0.004259391222149134 max memory_allocated 22921.50732421875 
[2024-12-25 09:04:25 root] (abq_llm.py 321): INFO layer 23 iter 17 loss:1.2459405660629272 norm:0.004160213749855757 max memory_allocated 22921.50732421875 
[2024-12-25 09:04:57 root] (abq_llm.py 321): INFO layer 23 iter 18 loss:1.2456531524658203 norm:0.004145796876400709 max memory_allocated 22921.50732421875 
[2024-12-25 09:05:29 root] (abq_llm.py 321): INFO layer 23 iter 19 loss:1.2455086708068848 norm:0.004243142902851105 max memory_allocated 22921.50732421875 
[2024-12-25 09:05:38 root] (abq_llm.py 208): INFO === Start quantize layer 24 ===
[2024-12-25 09:06:12 root] (abq_llm.py 321): INFO layer 24 iter 0 loss:1.5050045251846313 norm:0.02914462611079216 max memory_allocated 22923.17919921875 
[2024-12-25 09:06:44 root] (abq_llm.py 321): INFO layer 24 iter 1 loss:1.4813220500946045 norm:0.02414146438241005 max memory_allocated 22923.17919921875 
[2024-12-25 09:07:15 root] (abq_llm.py 321): INFO layer 24 iter 2 loss:1.4556190967559814 norm:0.018092628568410873 max memory_allocated 22923.17919921875 
[2024-12-25 09:07:47 root] (abq_llm.py 321): INFO layer 24 iter 3 loss:1.4429634809494019 norm:0.012911087833344936 max memory_allocated 22923.17919921875 
[2024-12-25 09:08:19 root] (abq_llm.py 321): INFO layer 24 iter 4 loss:1.4354583024978638 norm:0.009294474497437477 max memory_allocated 22923.17919921875 
[2024-12-25 09:08:50 root] (abq_llm.py 321): INFO layer 24 iter 5 loss:1.43064546585083 norm:0.00804043561220169 max memory_allocated 22923.17919921875 
[2024-12-25 09:09:22 root] (abq_llm.py 321): INFO layer 24 iter 6 loss:1.4270724058151245 norm:0.0070040724240243435 max memory_allocated 22923.17919921875 
[2024-12-25 09:09:54 root] (abq_llm.py 321): INFO layer 24 iter 7 loss:1.4243414402008057 norm:0.0063390666618943214 max memory_allocated 22923.17919921875 
[2024-12-25 09:10:25 root] (abq_llm.py 321): INFO layer 24 iter 8 loss:1.4224134683609009 norm:0.00579411443322897 max memory_allocated 22923.17919921875 
[2024-12-25 09:10:57 root] (abq_llm.py 321): INFO layer 24 iter 9 loss:1.4209154844284058 norm:0.005414322484284639 max memory_allocated 22923.17919921875 
[2024-12-25 09:11:29 root] (abq_llm.py 321): INFO layer 24 iter 10 loss:1.4196431636810303 norm:0.004999804310500622 max memory_allocated 22923.17919921875 
[2024-12-25 09:12:00 root] (abq_llm.py 321): INFO layer 24 iter 11 loss:1.4184796810150146 norm:0.004594826605170965 max memory_allocated 22923.17919921875 
[2024-12-25 09:12:32 root] (abq_llm.py 321): INFO layer 24 iter 12 loss:1.4173775911331177 norm:0.004234383814036846 max memory_allocated 22923.17919921875 
[2024-12-25 09:13:04 root] (abq_llm.py 321): INFO layer 24 iter 13 loss:1.4166427850723267 norm:0.00412405701354146 max memory_allocated 22923.17919921875 
[2024-12-25 09:13:35 root] (abq_llm.py 321): INFO layer 24 iter 14 loss:1.4158823490142822 norm:0.003790163202211261 max memory_allocated 22923.17919921875 
[2024-12-25 09:14:07 root] (abq_llm.py 321): INFO layer 24 iter 15 loss:1.4153393507003784 norm:0.0036766892299056053 max memory_allocated 22923.17919921875 
[2024-12-25 09:14:39 root] (abq_llm.py 321): INFO layer 24 iter 16 loss:1.4145615100860596 norm:0.0033698296174407005 max memory_allocated 22923.17919921875 
[2024-12-25 09:15:11 root] (abq_llm.py 321): INFO layer 24 iter 17 loss:1.4138683080673218 norm:0.00326259876601398 max memory_allocated 22923.17919921875 
[2024-12-25 09:15:42 root] (abq_llm.py 321): INFO layer 24 iter 18 loss:1.4135143756866455 norm:0.0031854554545134306 max memory_allocated 22923.17919921875 
[2024-12-25 09:16:14 root] (abq_llm.py 321): INFO layer 24 iter 19 loss:1.413149118423462 norm:0.0031375675462186337 max memory_allocated 22923.17919921875 
[2024-12-25 09:16:23 root] (abq_llm.py 208): INFO === Start quantize layer 25 ===
[2024-12-25 09:16:57 root] (abq_llm.py 321): INFO layer 25 iter 0 loss:1.6890321969985962 norm:0.010752148926258087 max memory_allocated 22924.85107421875 
[2024-12-25 09:17:29 root] (abq_llm.py 321): INFO layer 25 iter 1 loss:1.6667311191558838 norm:0.006118835881352425 max memory_allocated 22924.85107421875 
[2024-12-25 09:18:01 root] (abq_llm.py 321): INFO layer 25 iter 2 loss:1.6427637338638306 norm:0.003951117396354675 max memory_allocated 22924.85107421875 
[2024-12-25 09:18:32 root] (abq_llm.py 321): INFO layer 25 iter 3 loss:1.6318682432174683 norm:0.003006185404956341 max memory_allocated 22924.85107421875 
[2024-12-25 09:19:04 root] (abq_llm.py 321): INFO layer 25 iter 4 loss:1.6260286569595337 norm:0.0027152823749929667 max memory_allocated 22924.85107421875 
[2024-12-25 09:19:36 root] (abq_llm.py 321): INFO layer 25 iter 5 loss:1.6215791702270508 norm:0.002613503485918045 max memory_allocated 22924.85107421875 
[2024-12-25 09:20:07 root] (abq_llm.py 321): INFO layer 25 iter 6 loss:1.6183525323867798 norm:0.002519145840778947 max memory_allocated 22924.85107421875 
[2024-12-25 09:20:39 root] (abq_llm.py 321): INFO layer 25 iter 7 loss:1.6161017417907715 norm:0.0025018558371812105 max memory_allocated 22924.85107421875 
[2024-12-25 09:21:11 root] (abq_llm.py 321): INFO layer 25 iter 8 loss:1.6145663261413574 norm:0.0024773364420980215 max memory_allocated 22924.85107421875 
[2024-12-25 09:21:42 root] (abq_llm.py 321): INFO layer 25 iter 9 loss:1.6133828163146973 norm:0.0024572929833084345 max memory_allocated 22924.85107421875 
[2024-12-25 09:22:14 root] (abq_llm.py 321): INFO layer 25 iter 10 loss:1.6124424934387207 norm:0.002434510737657547 max memory_allocated 22924.85107421875 
[2024-12-25 09:22:46 root] (abq_llm.py 321): INFO layer 25 iter 11 loss:1.6118091344833374 norm:0.0024317523930221796 max memory_allocated 22924.85107421875 
[2024-12-25 09:23:18 root] (abq_llm.py 321): INFO layer 25 iter 12 loss:1.6111446619033813 norm:0.002386897336691618 max memory_allocated 22924.85107421875 
[2024-12-25 09:23:49 root] (abq_llm.py 321): INFO layer 25 iter 13 loss:1.6105709075927734 norm:0.002348554553464055 max memory_allocated 22924.85107421875 
[2024-12-25 09:24:21 root] (abq_llm.py 321): INFO layer 25 iter 14 loss:1.610127568244934 norm:0.00236009550280869 max memory_allocated 22924.85107421875 
[2024-12-25 09:24:53 root] (abq_llm.py 321): INFO layer 25 iter 15 loss:1.609680414199829 norm:0.0023823457304388285 max memory_allocated 22924.85107421875 
[2024-12-25 09:25:24 root] (abq_llm.py 321): INFO layer 25 iter 16 loss:1.6093480587005615 norm:0.002387969521805644 max memory_allocated 22924.85107421875 
[2024-12-25 09:25:56 root] (abq_llm.py 321): INFO layer 25 iter 17 loss:1.609107255935669 norm:0.0023912570904940367 max memory_allocated 22924.85107421875 
[2024-12-25 09:26:28 root] (abq_llm.py 321): INFO layer 25 iter 18 loss:1.6088483333587646 norm:0.0023710313253104687 max memory_allocated 22924.85107421875 
[2024-12-25 09:26:59 root] (abq_llm.py 321): INFO layer 25 iter 19 loss:1.6085729598999023 norm:0.0023854391183704138 max memory_allocated 22924.85107421875 
[2024-12-25 09:27:08 root] (abq_llm.py 208): INFO === Start quantize layer 26 ===
[2024-12-25 09:27:42 root] (abq_llm.py 321): INFO layer 26 iter 0 loss:1.9197860956192017 norm:0.0280610341578722 max memory_allocated 22926.52294921875 
[2024-12-25 09:28:14 root] (abq_llm.py 321): INFO layer 26 iter 1 loss:1.887524962425232 norm:0.014359650202095509 max memory_allocated 22926.52294921875 
[2024-12-25 09:28:46 root] (abq_llm.py 321): INFO layer 26 iter 2 loss:1.8565081357955933 norm:0.007404247298836708 max memory_allocated 22926.52294921875 
[2024-12-25 09:29:17 root] (abq_llm.py 321): INFO layer 26 iter 3 loss:1.8412854671478271 norm:0.004438023082911968 max memory_allocated 22926.52294921875 
[2024-12-25 09:29:49 root] (abq_llm.py 321): INFO layer 26 iter 4 loss:1.833622694015503 norm:0.0035960187669843435 max memory_allocated 22926.52294921875 
[2024-12-25 09:30:21 root] (abq_llm.py 321): INFO layer 26 iter 5 loss:1.827920913696289 norm:0.0031419454608112574 max memory_allocated 22926.52294921875 
[2024-12-25 09:30:52 root] (abq_llm.py 321): INFO layer 26 iter 6 loss:1.8237698078155518 norm:0.002876501064747572 max memory_allocated 22926.52294921875 
[2024-12-25 09:31:24 root] (abq_llm.py 321): INFO layer 26 iter 7 loss:1.8207955360412598 norm:0.0026530632749199867 max memory_allocated 22926.52294921875 
[2024-12-25 09:31:56 root] (abq_llm.py 321): INFO layer 26 iter 8 loss:1.8187302350997925 norm:0.0024773685727268457 max memory_allocated 22926.52294921875 
[2024-12-25 09:32:27 root] (abq_llm.py 321): INFO layer 26 iter 9 loss:1.8174433708190918 norm:0.0024554533883929253 max memory_allocated 22926.52294921875 
[2024-12-25 09:32:59 root] (abq_llm.py 321): INFO layer 26 iter 10 loss:1.8163836002349854 norm:0.002376927761361003 max memory_allocated 22926.52294921875 
[2024-12-25 09:33:31 root] (abq_llm.py 321): INFO layer 26 iter 11 loss:1.8154630661010742 norm:0.002317711478099227 max memory_allocated 22926.52294921875 
[2024-12-25 09:34:02 root] (abq_llm.py 321): INFO layer 26 iter 12 loss:1.8148642778396606 norm:0.002302961191162467 max memory_allocated 22926.52294921875 
[2024-12-25 09:34:34 root] (abq_llm.py 321): INFO layer 26 iter 13 loss:1.8142805099487305 norm:0.0022708801552653313 max memory_allocated 22926.52294921875 
[2024-12-25 09:35:06 root] (abq_llm.py 321): INFO layer 26 iter 14 loss:1.8136534690856934 norm:0.0022298647090792656 max memory_allocated 22926.52294921875 
[2024-12-25 09:35:37 root] (abq_llm.py 321): INFO layer 26 iter 15 loss:1.8132458925247192 norm:0.0022505582310259342 max memory_allocated 22926.52294921875 
[2024-12-25 09:36:09 root] (abq_llm.py 321): INFO layer 26 iter 16 loss:1.8129609823226929 norm:0.0022216520737856627 max memory_allocated 22926.52294921875 
[2024-12-25 09:36:41 root] (abq_llm.py 321): INFO layer 26 iter 17 loss:1.8126616477966309 norm:0.0022256257943809032 max memory_allocated 22926.52294921875 
[2024-12-25 09:37:12 root] (abq_llm.py 321): INFO layer 26 iter 18 loss:1.8124428987503052 norm:0.002242189599201083 max memory_allocated 22926.52294921875 
[2024-12-25 09:37:44 root] (abq_llm.py 321): INFO layer 26 iter 19 loss:1.8120306730270386 norm:0.0022083576768636703 max memory_allocated 22926.52294921875 
[2024-12-25 09:37:53 root] (abq_llm.py 208): INFO === Start quantize layer 27 ===
[2024-12-25 09:38:27 root] (abq_llm.py 321): INFO layer 27 iter 0 loss:2.130185127258301 norm:0.02517911046743393 max memory_allocated 22928.19482421875 
[2024-12-25 09:38:59 root] (abq_llm.py 321): INFO layer 27 iter 1 loss:2.093999147415161 norm:0.010725433006882668 max memory_allocated 22928.19482421875 
[2024-12-25 09:39:31 root] (abq_llm.py 321): INFO layer 27 iter 2 loss:2.063885450363159 norm:0.0070342449471354485 max memory_allocated 22928.19482421875 
[2024-12-25 09:40:02 root] (abq_llm.py 321): INFO layer 27 iter 3 loss:2.047288656234741 norm:0.004227799363434315 max memory_allocated 22928.19482421875 
[2024-12-25 09:40:34 root] (abq_llm.py 321): INFO layer 27 iter 4 loss:2.0395731925964355 norm:0.003998251631855965 max memory_allocated 22928.19482421875 
[2024-12-25 09:41:06 root] (abq_llm.py 321): INFO layer 27 iter 5 loss:2.034193277359009 norm:0.003929158207029104 max memory_allocated 22928.19482421875 
[2024-12-25 09:41:38 root] (abq_llm.py 321): INFO layer 27 iter 6 loss:2.030524492263794 norm:0.0039992923848330975 max memory_allocated 22928.19482421875 
[2024-12-25 09:42:09 root] (abq_llm.py 321): INFO layer 27 iter 7 loss:2.0276429653167725 norm:0.003907279577106237 max memory_allocated 22928.19482421875 
[2024-12-25 09:42:41 root] (abq_llm.py 321): INFO layer 27 iter 8 loss:2.025874137878418 norm:0.003927402198314667 max memory_allocated 22928.19482421875 
[2024-12-25 09:43:13 root] (abq_llm.py 321): INFO layer 27 iter 9 loss:2.0245838165283203 norm:0.00386602315120399 max memory_allocated 22928.19482421875 
[2024-12-25 09:43:44 root] (abq_llm.py 321): INFO layer 27 iter 10 loss:2.0234766006469727 norm:0.003971196711063385 max memory_allocated 22928.19482421875 
[2024-12-25 09:44:16 root] (abq_llm.py 321): INFO layer 27 iter 11 loss:2.022599697113037 norm:0.004010559059679508 max memory_allocated 22928.19482421875 
[2024-12-25 09:44:48 root] (abq_llm.py 321): INFO layer 27 iter 12 loss:2.02181077003479 norm:0.003919658716768026 max memory_allocated 22928.19482421875 
[2024-12-25 09:45:19 root] (abq_llm.py 321): INFO layer 27 iter 13 loss:2.021249294281006 norm:0.003800464328378439 max memory_allocated 22928.19482421875 
[2024-12-25 09:45:51 root] (abq_llm.py 321): INFO layer 27 iter 14 loss:2.0206634998321533 norm:0.0037743879947811365 max memory_allocated 22928.19482421875 
[2024-12-25 09:46:23 root] (abq_llm.py 321): INFO layer 27 iter 15 loss:2.0201096534729004 norm:0.0036502848379313946 max memory_allocated 22928.19482421875 
[2024-12-25 09:46:55 root] (abq_llm.py 321): INFO layer 27 iter 16 loss:2.019723415374756 norm:0.0035583057906478643 max memory_allocated 22928.19482421875 
[2024-12-25 09:47:26 root] (abq_llm.py 321): INFO layer 27 iter 17 loss:2.0193495750427246 norm:0.003556895535439253 max memory_allocated 22928.19482421875 
[2024-12-25 09:47:58 root] (abq_llm.py 321): INFO layer 27 iter 18 loss:2.018975257873535 norm:0.003553817979991436 max memory_allocated 22928.19482421875 
[2024-12-25 09:48:30 root] (abq_llm.py 321): INFO layer 27 iter 19 loss:2.0186452865600586 norm:0.003594414098188281 max memory_allocated 22928.19482421875 
[2024-12-25 09:48:39 root] (abq_llm.py 208): INFO === Start quantize layer 28 ===
[2024-12-25 09:48:41 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-25 09:49:13 root] (abq_llm.py 321): INFO layer 28 iter 0 loss:2.4369471073150635 norm:0.05152785778045654 max memory_allocated 22929.98193359375 
[2024-12-25 09:49:45 root] (abq_llm.py 321): INFO layer 28 iter 1 loss:2.4040400981903076 norm:0.04499499499797821 max memory_allocated 22929.98193359375 
[2024-12-25 09:50:17 root] (abq_llm.py 321): INFO layer 28 iter 2 loss:2.3709588050842285 norm:0.0360703319311142 max memory_allocated 22929.98193359375 
[2024-12-25 09:50:48 root] (abq_llm.py 321): INFO layer 28 iter 3 loss:2.3510193824768066 norm:0.029883967712521553 max memory_allocated 22929.98193359375 
[2024-12-25 09:51:20 root] (abq_llm.py 321): INFO layer 28 iter 4 loss:2.339543581008911 norm:0.025579486042261124 max memory_allocated 22929.98193359375 
[2024-12-25 09:51:52 root] (abq_llm.py 321): INFO layer 28 iter 5 loss:2.330500364303589 norm:0.022219834849238396 max memory_allocated 22929.98193359375 
[2024-12-25 09:52:24 root] (abq_llm.py 321): INFO layer 28 iter 6 loss:2.3235666751861572 norm:0.019749756902456284 max memory_allocated 22929.98193359375 
[2024-12-25 09:52:55 root] (abq_llm.py 321): INFO layer 28 iter 7 loss:2.318286657333374 norm:0.01789313741028309 max memory_allocated 22929.98193359375 
[2024-12-25 09:53:27 root] (abq_llm.py 321): INFO layer 28 iter 8 loss:2.3142616748809814 norm:0.017193250358104706 max memory_allocated 22929.98193359375 
[2024-12-25 09:53:59 root] (abq_llm.py 321): INFO layer 28 iter 9 loss:2.3105921745300293 norm:0.016472065821290016 max memory_allocated 22929.98193359375 
[2024-12-25 09:54:31 root] (abq_llm.py 321): INFO layer 28 iter 10 loss:2.307542324066162 norm:0.016497157514095306 max memory_allocated 22929.98193359375 
[2024-12-25 09:55:03 root] (abq_llm.py 321): INFO layer 28 iter 11 loss:2.30511474609375 norm:0.016436483711004257 max memory_allocated 22929.98193359375 
[2024-12-25 09:55:34 root] (abq_llm.py 321): INFO layer 28 iter 12 loss:2.3023993968963623 norm:0.016447599977254868 max memory_allocated 22929.98193359375 
[2024-12-25 09:56:06 root] (abq_llm.py 321): INFO layer 28 iter 13 loss:2.2996826171875 norm:0.015496351756155491 max memory_allocated 22929.98193359375 
[2024-12-25 09:56:38 root] (abq_llm.py 321): INFO layer 28 iter 14 loss:2.2973973751068115 norm:0.014938835054636002 max memory_allocated 22929.98193359375 
[2024-12-25 09:57:10 root] (abq_llm.py 321): INFO layer 28 iter 15 loss:2.295596122741699 norm:0.014376594685018063 max memory_allocated 22929.98193359375 
[2024-12-25 09:57:42 root] (abq_llm.py 321): INFO layer 28 iter 16 loss:2.294006824493408 norm:0.014370898716151714 max memory_allocated 22929.98193359375 
[2024-12-25 09:58:13 root] (abq_llm.py 321): INFO layer 28 iter 17 loss:2.2925376892089844 norm:0.014133600518107414 max memory_allocated 22929.98193359375 
[2024-12-25 09:58:45 root] (abq_llm.py 321): INFO layer 28 iter 18 loss:2.2911412715911865 norm:0.014027943834662437 max memory_allocated 22929.98193359375 
[2024-12-25 09:59:17 root] (abq_llm.py 321): INFO layer 28 iter 19 loss:2.289996862411499 norm:0.014164596796035767 max memory_allocated 22929.98193359375 
[2024-12-25 09:59:26 root] (abq_llm.py 208): INFO === Start quantize layer 29 ===
[2024-12-25 09:59:29 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-25 10:00:00 root] (abq_llm.py 321): INFO layer 29 iter 0 loss:2.7608273029327393 norm:0.0497569739818573 max memory_allocated 22931.65380859375 
[2024-12-25 10:00:32 root] (abq_llm.py 321): INFO layer 29 iter 1 loss:2.7163281440734863 norm:0.041729636490345 max memory_allocated 22931.65380859375 
[2024-12-25 10:01:04 root] (abq_llm.py 321): INFO layer 29 iter 2 loss:2.669044017791748 norm:0.03249981999397278 max memory_allocated 22931.65380859375 
[2024-12-25 10:01:36 root] (abq_llm.py 321): INFO layer 29 iter 3 loss:2.641706943511963 norm:0.02682618424296379 max memory_allocated 22931.65380859375 
[2024-12-25 10:02:07 root] (abq_llm.py 321): INFO layer 29 iter 4 loss:2.628258228302002 norm:0.022727657109498978 max memory_allocated 22931.65380859375 
[2024-12-25 10:02:39 root] (abq_llm.py 321): INFO layer 29 iter 5 loss:2.6189944744110107 norm:0.019346933811903 max memory_allocated 22931.65380859375 
[2024-12-25 10:03:11 root] (abq_llm.py 321): INFO layer 29 iter 6 loss:2.612924575805664 norm:0.016783569008111954 max memory_allocated 22931.65380859375 
[2024-12-25 10:03:43 root] (abq_llm.py 321): INFO layer 29 iter 7 loss:2.608849287033081 norm:0.0152892442420125 max memory_allocated 22931.65380859375 
[2024-12-25 10:04:15 root] (abq_llm.py 321): INFO layer 29 iter 8 loss:2.606020927429199 norm:0.014167469926178455 max memory_allocated 22931.65380859375 
[2024-12-25 10:04:46 root] (abq_llm.py 321): INFO layer 29 iter 9 loss:2.6037237644195557 norm:0.013647467829287052 max memory_allocated 22931.65380859375 
[2024-12-25 10:05:18 root] (abq_llm.py 321): INFO layer 29 iter 10 loss:2.6020333766937256 norm:0.01359458640217781 max memory_allocated 22931.65380859375 
[2024-12-25 10:05:50 root] (abq_llm.py 321): INFO layer 29 iter 11 loss:2.6008784770965576 norm:0.013901805505156517 max memory_allocated 22931.65380859375 
[2024-12-25 10:06:22 root] (abq_llm.py 321): INFO layer 29 iter 12 loss:2.5993564128875732 norm:0.013640901073813438 max memory_allocated 22931.65380859375 
[2024-12-25 10:06:54 root] (abq_llm.py 321): INFO layer 29 iter 13 loss:2.598311185836792 norm:0.01345951110124588 max memory_allocated 22931.65380859375 
[2024-12-25 10:07:26 root] (abq_llm.py 321): INFO layer 29 iter 14 loss:2.596705436706543 norm:0.012871751561760902 max memory_allocated 22931.65380859375 
[2024-12-25 10:07:58 root] (abq_llm.py 321): INFO layer 29 iter 15 loss:2.5960543155670166 norm:0.012739594094455242 max memory_allocated 22931.65380859375 
[2024-12-25 10:08:29 root] (abq_llm.py 321): INFO layer 29 iter 16 loss:2.5953028202056885 norm:0.012713521718978882 max memory_allocated 22931.65380859375 
[2024-12-25 10:09:01 root] (abq_llm.py 321): INFO layer 29 iter 17 loss:2.5945141315460205 norm:0.012425163760781288 max memory_allocated 22931.65380859375 
[2024-12-25 10:09:33 root] (abq_llm.py 321): INFO layer 29 iter 18 loss:2.5939431190490723 norm:0.01220712810754776 max memory_allocated 22931.65380859375 
[2024-12-25 10:10:05 root] (abq_llm.py 321): INFO layer 29 iter 19 loss:2.593569278717041 norm:0.012300600297749043 max memory_allocated 22931.65380859375 
[2024-12-25 10:10:14 root] (abq_llm.py 208): INFO === Start quantize layer 30 ===
[2024-12-25 10:10:17 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-25 10:10:48 root] (abq_llm.py 321): INFO layer 30 iter 0 loss:3.629441022872925 norm:0.10363862663507462 max memory_allocated 22933.32568359375 
[2024-12-25 10:11:20 root] (abq_llm.py 321): INFO layer 30 iter 1 loss:3.534224510192871 norm:0.07969458401203156 max memory_allocated 22933.32568359375 
[2024-12-25 10:11:52 root] (abq_llm.py 321): INFO layer 30 iter 2 loss:3.3865551948547363 norm:0.04018231853842735 max memory_allocated 22933.32568359375 
[2024-12-25 10:12:24 root] (abq_llm.py 321): INFO layer 30 iter 3 loss:3.3180606365203857 norm:0.0351630300283432 max memory_allocated 22933.32568359375 
[2024-12-25 10:12:56 root] (abq_llm.py 321): INFO layer 30 iter 4 loss:3.287466526031494 norm:0.03334309533238411 max memory_allocated 22933.32568359375 
[2024-12-25 10:13:27 root] (abq_llm.py 321): INFO layer 30 iter 5 loss:3.273158550262451 norm:0.03436211869120598 max memory_allocated 22933.32568359375 
[2024-12-25 10:13:59 root] (abq_llm.py 321): INFO layer 30 iter 6 loss:3.252871513366699 norm:0.033512189984321594 max memory_allocated 22933.32568359375 
[2024-12-25 10:14:31 root] (abq_llm.py 321): INFO layer 30 iter 7 loss:3.244318962097168 norm:0.0341217964887619 max memory_allocated 22933.32568359375 
[2024-12-25 10:15:03 root] (abq_llm.py 321): INFO layer 30 iter 8 loss:3.2375292778015137 norm:0.037582941353321075 max memory_allocated 22933.32568359375 
[2024-12-25 10:15:35 root] (abq_llm.py 321): INFO layer 30 iter 9 loss:3.2254204750061035 norm:0.03635076805949211 max memory_allocated 22933.32568359375 
[2024-12-25 10:16:06 root] (abq_llm.py 321): INFO layer 30 iter 10 loss:3.2190394401550293 norm:0.03719433769583702 max memory_allocated 22933.32568359375 
[2024-12-25 10:16:38 root] (abq_llm.py 321): INFO layer 30 iter 11 loss:3.2117791175842285 norm:0.037849001586437225 max memory_allocated 22933.32568359375 
[2024-12-25 10:17:10 root] (abq_llm.py 321): INFO layer 30 iter 12 loss:3.2091593742370605 norm:0.03609998896718025 max memory_allocated 22933.32568359375 
[2024-12-25 10:17:42 root] (abq_llm.py 321): INFO layer 30 iter 13 loss:3.205629587173462 norm:0.03781528025865555 max memory_allocated 22933.32568359375 
[2024-12-25 10:18:14 root] (abq_llm.py 321): INFO layer 30 iter 14 loss:3.207291841506958 norm:0.039708204567432404 max memory_allocated 22933.32568359375 
[2024-12-25 10:18:46 root] (abq_llm.py 321): INFO layer 30 iter 15 loss:3.2014596462249756 norm:0.03860616683959961 max memory_allocated 22933.32568359375 
[2024-12-25 10:19:17 root] (abq_llm.py 321): INFO layer 30 iter 16 loss:3.201781749725342 norm:0.04136558622121811 max memory_allocated 22933.32568359375 
[2024-12-25 10:19:49 root] (abq_llm.py 321): INFO layer 30 iter 17 loss:3.1973180770874023 norm:0.041176535189151764 max memory_allocated 22933.32568359375 
[2024-12-25 10:20:21 root] (abq_llm.py 321): INFO layer 30 iter 18 loss:3.192993402481079 norm:0.03964199125766754 max memory_allocated 22933.32568359375 
[2024-12-25 10:20:53 root] (abq_llm.py 321): INFO layer 30 iter 19 loss:3.195589065551758 norm:0.040393952280282974 max memory_allocated 22933.32568359375 
[2024-12-25 10:21:02 root] (abq_llm.py 208): INFO === Start quantize layer 31 ===
[2024-12-25 10:21:04 root] (abq_llm.py 262): INFO use compensation vector
[2024-12-25 10:21:36 root] (abq_llm.py 321): INFO layer 31 iter 0 loss:6.712438106536865 norm:0.41616082191467285 max memory_allocated 22934.99755859375 
[2024-12-25 10:22:08 root] (abq_llm.py 321): INFO layer 31 iter 1 loss:6.220781326293945 norm:0.366110622882843 max memory_allocated 22934.99755859375 
[2024-12-25 10:22:40 root] (abq_llm.py 321): INFO layer 31 iter 2 loss:5.922754764556885 norm:0.303568959236145 max memory_allocated 22934.99755859375 
[2024-12-25 10:23:11 root] (abq_llm.py 321): INFO layer 31 iter 3 loss:5.750722408294678 norm:0.2880893349647522 max memory_allocated 22934.99755859375 
[2024-12-25 10:23:43 root] (abq_llm.py 321): INFO layer 31 iter 4 loss:5.64717960357666 norm:0.26010066270828247 max memory_allocated 22934.99755859375 
[2024-12-25 10:24:15 root] (abq_llm.py 321): INFO layer 31 iter 5 loss:5.56069803237915 norm:0.24925264716148376 max memory_allocated 22934.99755859375 
[2024-12-25 10:24:47 root] (abq_llm.py 321): INFO layer 31 iter 6 loss:5.5043158531188965 norm:0.23890827596187592 max memory_allocated 22934.99755859375 
[2024-12-25 10:25:18 root] (abq_llm.py 321): INFO layer 31 iter 7 loss:5.464020252227783 norm:0.2300734519958496 max memory_allocated 22934.99755859375 
[2024-12-25 10:25:50 root] (abq_llm.py 321): INFO layer 31 iter 8 loss:5.430308818817139 norm:0.21836553514003754 max memory_allocated 22934.99755859375 
[2024-12-25 10:26:22 root] (abq_llm.py 321): INFO layer 31 iter 9 loss:5.405428409576416 norm:0.2141766995191574 max memory_allocated 22934.99755859375 
[2024-12-25 10:26:54 root] (abq_llm.py 321): INFO layer 31 iter 10 loss:5.380684852600098 norm:0.20621436834335327 max memory_allocated 22934.99755859375 
[2024-12-25 10:27:25 root] (abq_llm.py 321): INFO layer 31 iter 11 loss:5.358700275421143 norm:0.19759643077850342 max memory_allocated 22934.99755859375 
[2024-12-25 10:27:57 root] (abq_llm.py 321): INFO layer 31 iter 12 loss:5.34015417098999 norm:0.18897053599357605 max memory_allocated 22934.99755859375 
[2024-12-25 10:28:29 root] (abq_llm.py 321): INFO layer 31 iter 13 loss:5.3272223472595215 norm:0.18237030506134033 max memory_allocated 22934.99755859375 
[2024-12-25 10:29:01 root] (abq_llm.py 321): INFO layer 31 iter 14 loss:5.315487861633301 norm:0.18124768137931824 max memory_allocated 22934.99755859375 
[2024-12-25 10:29:32 root] (abq_llm.py 321): INFO layer 31 iter 15 loss:5.307560443878174 norm:0.1833178997039795 max memory_allocated 22934.99755859375 
[2024-12-25 10:30:04 root] (abq_llm.py 321): INFO layer 31 iter 16 loss:5.29649543762207 norm:0.1772639900445938 max memory_allocated 22934.99755859375 
[2024-12-25 10:30:36 root] (abq_llm.py 321): INFO layer 31 iter 17 loss:5.28692626953125 norm:0.1731988489627838 max memory_allocated 22934.99755859375 
[2024-12-25 10:31:08 root] (abq_llm.py 321): INFO layer 31 iter 18 loss:5.282835960388184 norm:0.17931751906871796 max memory_allocated 22934.99755859375 
[2024-12-25 10:31:39 root] (abq_llm.py 321): INFO layer 31 iter 19 loss:5.273675441741943 norm:0.16806122660636902 max memory_allocated 22934.99755859375 
[2024-12-25 10:31:49 root] (main.py 360): INFO 20671.21073460579
[2024-12-25 10:34:23 root] (main.py 158): INFO wikitext2 : 8.610572814941406
[2024-12-25 10:36:59 root] (main.py 158): INFO c4 : 12.158793449401855
