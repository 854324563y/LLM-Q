[2025-02-28 13:28:37 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/Llama-2-13b-hf_0.8', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.8.pkl', scale_calibration=True, compensation_calibration=True)
[2025-02-28 13:28:46 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-02-28 13:28:46 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-28 13:28:47 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-02-28 13:28:47 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.8.pkl
[2025-02-28 13:28:59 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-02-28 13:29:04 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:29:48 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 0 loss:0.006509814877063036 norm:0.01168204564601183 max memory_allocated 29271.02001953125 
[2025-02-28 13:30:34 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 1 loss:0.003579376731067896 norm:0.006682621780782938 max memory_allocated 29271.02001953125 
[2025-02-28 13:31:19 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 2 loss:0.0029958924278616905 norm:0.005195687524974346 max memory_allocated 29271.02001953125 
[2025-02-28 13:32:05 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 3 loss:0.00269558047875762 norm:0.004146229475736618 max memory_allocated 29271.02001953125 
[2025-02-28 13:32:50 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 4 loss:0.002493839943781495 norm:0.0034108080435544252 max memory_allocated 29271.02001953125 
[2025-02-28 13:33:36 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 5 loss:0.002383796265348792 norm:0.002825790084898472 max memory_allocated 29271.02001953125 
[2025-02-28 13:34:22 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 6 loss:0.0022070189006626606 norm:0.0022159649524837732 max memory_allocated 29271.02001953125 
[2025-02-28 13:35:07 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 7 loss:0.002154208952561021 norm:0.001996826846152544 max memory_allocated 29271.02001953125 
[2025-02-28 13:35:53 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 8 loss:0.002218365203589201 norm:0.0018851794302463531 max memory_allocated 29271.02001953125 
[2025-02-28 13:36:39 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 9 loss:0.002118071774020791 norm:0.001612937543541193 max memory_allocated 29271.02001953125 
[2025-02-28 13:37:24 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 10 loss:0.0020931661128997803 norm:0.001412676996551454 max memory_allocated 29271.02001953125 
[2025-02-28 13:38:10 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 11 loss:0.002091404516249895 norm:0.0014193210517987609 max memory_allocated 29271.02001953125 
[2025-02-28 13:38:55 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 12 loss:0.0019615450873970985 norm:0.0011253771372139454 max memory_allocated 29271.02001953125 
[2025-02-28 13:39:41 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 13 loss:0.001980625791475177 norm:0.0010509464191272855 max memory_allocated 29271.02001953125 
[2025-02-28 13:40:27 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 14 loss:0.001983711263164878 norm:0.0010247613536193967 max memory_allocated 29271.02001953125 
[2025-02-28 13:41:12 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 15 loss:0.001959980931133032 norm:0.0009997168090194464 max memory_allocated 29271.02001953125 
[2025-02-28 13:41:58 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 16 loss:0.00192761211656034 norm:0.000930356327444315 max memory_allocated 29271.02001953125 
[2025-02-28 13:42:44 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 17 loss:0.00194416637532413 norm:0.0009498932049609721 max memory_allocated 29271.02001953125 
[2025-02-28 13:43:29 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 18 loss:0.0020899823866784573 norm:0.0010334181133657694 max memory_allocated 29271.02001953125 
[2025-02-28 13:44:15 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 19 loss:0.0020113401114940643 norm:0.0009914740221574903 max memory_allocated 29271.02001953125 
[2025-02-28 13:44:28 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-02-28 13:44:35 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:45:20 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 0 loss:0.01091067586094141 norm:0.00870194286108017 max memory_allocated 29271.02001953125 
[2025-02-28 13:46:06 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 1 loss:0.00707039050757885 norm:0.006002229638397694 max memory_allocated 29271.02001953125 
[2025-02-28 13:46:52 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 2 loss:0.006043137516826391 norm:0.004200561437755823 max memory_allocated 29271.02001953125 
[2025-02-28 13:47:37 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 3 loss:0.005600554868578911 norm:0.0032603915315121412 max memory_allocated 29271.02001953125 
[2025-02-28 13:48:23 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 4 loss:0.005325964652001858 norm:0.0027050829958170652 max memory_allocated 29271.02001953125 
[2025-02-28 13:49:09 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 5 loss:0.0051630111411213875 norm:0.0023090741597115993 max memory_allocated 29271.02001953125 
[2025-02-28 13:49:55 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 6 loss:0.005052081309258938 norm:0.0020480414386838675 max memory_allocated 29271.02001953125 
[2025-02-28 13:50:40 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 7 loss:0.004949213471263647 norm:0.0018173912540078163 max memory_allocated 29271.02001953125 
[2025-02-28 13:51:26 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 8 loss:0.0048724268563091755 norm:0.0016209043096750975 max memory_allocated 29271.02001953125 
[2025-02-28 13:52:12 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 9 loss:0.004813541192561388 norm:0.00144797726534307 max memory_allocated 29271.02001953125 
[2025-02-28 13:52:57 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 10 loss:0.004746149759739637 norm:0.0012811619089916348 max memory_allocated 29271.02001953125 
[2025-02-28 13:53:43 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 11 loss:0.0046791634522378445 norm:0.0011467074509710073 max memory_allocated 29271.02001953125 
[2025-02-28 13:54:29 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 12 loss:0.004621288739144802 norm:0.0010485246311873198 max memory_allocated 29271.02001953125 
[2025-02-28 13:55:15 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 13 loss:0.004587431438267231 norm:0.0009971135295927525 max memory_allocated 29271.02001953125 
[2025-02-28 13:56:00 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 14 loss:0.0045510693453252316 norm:0.0010136087657883763 max memory_allocated 29271.02001953125 
[2025-02-28 13:56:46 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 15 loss:0.004539852030575275 norm:0.0010328436037525535 max memory_allocated 29271.02001953125 
[2025-02-28 13:57:32 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 16 loss:0.0045263138599693775 norm:0.0009811249328777194 max memory_allocated 29271.02001953125 
[2025-02-28 13:58:17 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 17 loss:0.0044860332272946835 norm:0.0008951636264100671 max memory_allocated 29271.02001953125 
[2025-02-28 13:59:03 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 18 loss:0.0044787852093577385 norm:0.0008607705240137875 max memory_allocated 29271.02001953125 
[2025-02-28 13:59:49 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 19 loss:0.0044500320218503475 norm:0.0008300914196297526 max memory_allocated 29271.02001953125 
[2025-02-28 14:00:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-02-28 14:00:06 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 14:00:52 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 0 loss:0.014778630807995796 norm:0.007338872645050287 max memory_allocated 29271.02001953125 
[2025-02-28 14:01:37 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 1 loss:0.011991826817393303 norm:0.005750743206590414 max memory_allocated 29271.02001953125 
[2025-02-28 14:02:23 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 2 loss:0.010887917131185532 norm:0.004261977970600128 max memory_allocated 29271.02001953125 
[2025-02-28 14:03:09 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 3 loss:0.010206000879406929 norm:0.0033524897880852222 max memory_allocated 29271.02001953125 
[2025-02-28 14:03:55 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 4 loss:0.009807800874114037 norm:0.0027547336649149656 max memory_allocated 29271.02001953125 
[2025-02-28 14:04:40 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 5 loss:0.009554708376526833 norm:0.0023434264585375786 max memory_allocated 29271.02001953125 
[2025-02-28 14:05:26 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 6 loss:0.00926182884722948 norm:0.0019494248554110527 max memory_allocated 29271.02001953125 
[2025-02-28 14:06:12 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 7 loss:0.009099870920181274 norm:0.0017665221821516752 max memory_allocated 29271.02001953125 
[2025-02-28 14:06:58 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 8 loss:0.008951106108725071 norm:0.0016377426218241453 max memory_allocated 29271.02001953125 
[2025-02-28 14:07:43 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 9 loss:0.008790811523795128 norm:0.0014880603412166238 max memory_allocated 29271.02001953125 
[2025-02-28 14:08:29 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 10 loss:0.008717905730009079 norm:0.0013565734261646867 max memory_allocated 29271.02001953125 
[2025-02-28 14:09:15 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 11 loss:0.008689779788255692 norm:0.0013330965302884579 max memory_allocated 29271.02001953125 
[2025-02-28 14:10:01 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 12 loss:0.008700912818312645 norm:0.0013664127327501774 max memory_allocated 29271.02001953125 
[2025-02-28 14:10:46 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 13 loss:0.008731456473469734 norm:0.0013741827569901943 max memory_allocated 29271.02001953125 
[2025-02-28 14:11:32 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 14 loss:0.00875383522361517 norm:0.0012893808307126164 max memory_allocated 29271.02001953125 
[2025-02-28 14:12:18 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 15 loss:0.008732855319976807 norm:0.0012758579105138779 max memory_allocated 29271.02001953125 
[2025-02-28 14:13:04 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 16 loss:0.008829078637063503 norm:0.001238871249370277 max memory_allocated 29271.02001953125 
[2025-02-28 14:13:49 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 17 loss:0.008659575134515762 norm:0.0011028883745893836 max memory_allocated 29271.02001953125 
[2025-02-28 14:14:35 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 18 loss:0.0086580291390419 norm:0.0011003059335052967 max memory_allocated 29271.02001953125 
[2025-02-28 14:15:21 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 19 loss:0.008688258938491344 norm:0.0011504258727654815 max memory_allocated 29271.02001953125 
[2025-02-28 14:15:34 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-02-28 14:16:23 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 0 loss:0.12099918723106384 norm:0.017212659120559692 max memory_allocated 29271.02001953125 
[2025-02-28 14:17:09 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 1 loss:0.08652342110872269 norm:0.01044552493840456 max memory_allocated 29271.02001953125 
[2025-02-28 14:17:54 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 2 loss:0.06457538902759552 norm:0.009801231324672699 max memory_allocated 29271.02001953125 
[2025-02-28 14:18:40 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 3 loss:0.057154033333063126 norm:0.009273352101445198 max memory_allocated 29271.02001953125 
[2025-02-28 14:19:26 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 4 loss:0.04794510453939438 norm:0.0073730554431676865 max memory_allocated 29271.02001953125 
[2025-02-28 14:20:11 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 5 loss:0.04491777345538139 norm:0.006166982464492321 max memory_allocated 29271.02001953125 
[2025-02-28 14:20:57 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 6 loss:0.04315586015582085 norm:0.006050290074199438 max memory_allocated 29271.02001953125 
[2025-02-28 14:21:42 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 7 loss:0.03466327488422394 norm:0.005162802990525961 max memory_allocated 29271.02001953125 
[2025-02-28 14:22:28 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 8 loss:0.03395356237888336 norm:0.004705952480435371 max memory_allocated 29271.02001953125 
[2025-02-28 14:23:13 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 9 loss:0.032214727252721786 norm:0.004515315871685743 max memory_allocated 29271.02001953125 
[2025-02-28 14:23:59 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 10 loss:0.03232055902481079 norm:0.004393090959638357 max memory_allocated 29271.02001953125 
[2025-02-28 14:24:45 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 11 loss:0.0324888676404953 norm:0.004442770034074783 max memory_allocated 29271.02001953125 
[2025-02-28 14:25:30 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 12 loss:0.030689751729369164 norm:0.003955571912229061 max memory_allocated 29271.02001953125 
[2025-02-28 14:26:16 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 13 loss:0.03088066354393959 norm:0.004345601890236139 max memory_allocated 29271.02001953125 
[2025-02-28 14:27:01 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 14 loss:0.029579084366559982 norm:0.0035952336620539427 max memory_allocated 29271.02001953125 
[2025-02-28 14:27:47 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 15 loss:0.029215479269623756 norm:0.0034541990607976913 max memory_allocated 29271.02001953125 
[2025-02-28 14:28:32 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 16 loss:0.028880655765533447 norm:0.003837701864540577 max memory_allocated 29271.02001953125 
[2025-02-28 14:29:18 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 17 loss:0.02703212946653366 norm:0.0024419398978352547 max memory_allocated 29271.02001953125 
[2025-02-28 14:30:03 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 18 loss:0.027714598923921585 norm:0.0028654271736741066 max memory_allocated 29271.02001953125 
[2025-02-28 14:30:49 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 19 loss:0.029175670817494392 norm:0.0031976511236280203 max memory_allocated 29271.02001953125 
[2025-02-28 14:31:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-02-28 14:31:52 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 0 loss:0.03104313090443611 norm:0.0008079182007350028 max memory_allocated 29271.62548828125 
[2025-02-28 14:32:37 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 1 loss:0.026622483506798744 norm:0.0003371039929334074 max memory_allocated 29271.62548828125 
[2025-02-28 14:33:23 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 2 loss:0.024985533207654953 norm:0.00020036078058183193 max memory_allocated 29271.62548828125 
[2025-02-28 14:34:08 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 3 loss:0.024292131885886192 norm:0.00016398628940805793 max memory_allocated 29271.62548828125 
[2025-02-28 14:34:54 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 4 loss:0.023735051974654198 norm:0.00014213311078492552 max memory_allocated 29271.62548828125 
[2025-02-28 14:35:40 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 5 loss:0.023305818438529968 norm:0.00012965689529664814 max memory_allocated 29271.62548828125 
[2025-02-28 14:36:25 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 6 loss:0.02299819327890873 norm:0.00012306722055654973 max memory_allocated 29271.62548828125 
[2025-02-28 14:37:11 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 7 loss:0.02280345745384693 norm:0.00011354542948538437 max memory_allocated 29271.62548828125 
[2025-02-28 14:37:56 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 8 loss:0.0227207038551569 norm:0.00011548446491360664 max memory_allocated 29271.62548828125 
[2025-02-28 14:38:42 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 9 loss:0.022645117715001106 norm:0.00011397172056604177 max memory_allocated 29271.62548828125 
[2025-02-28 14:39:28 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 10 loss:0.022632891312241554 norm:0.00011822620581369847 max memory_allocated 29271.62548828125 
[2025-02-28 14:40:13 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 11 loss:0.02259313315153122 norm:0.00011062272824347019 max memory_allocated 29271.62548828125 
[2025-02-28 14:40:59 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 12 loss:0.022532835602760315 norm:0.00010408902016934007 max memory_allocated 29271.62548828125 
[2025-02-28 14:41:44 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 13 loss:0.022508986294269562 norm:0.0001018941547954455 max memory_allocated 29271.62548828125 
[2025-02-28 14:42:30 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 14 loss:0.02248430997133255 norm:0.00010187534644501284 max memory_allocated 29271.62548828125 
[2025-02-28 14:43:16 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 15 loss:0.02248232066631317 norm:0.0001067895645974204 max memory_allocated 29271.62548828125 
[2025-02-28 14:44:01 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 16 loss:0.022477811202406883 norm:0.00010421890328871086 max memory_allocated 29271.62548828125 
[2025-02-28 14:44:47 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 17 loss:0.022452667355537415 norm:0.00010387822112534195 max memory_allocated 29271.62548828125 
[2025-02-28 14:45:33 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 18 loss:0.022446809336543083 norm:0.00010873279825318605 max memory_allocated 29271.62548828125 
[2025-02-28 14:46:18 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 19 loss:0.022444501519203186 norm:0.0001043929296429269 max memory_allocated 29271.62548828125 
[2025-02-28 14:46:32 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-02-28 14:47:21 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 0 loss:0.03396603465080261 norm:0.001083349110558629 max memory_allocated 29271.62548828125 
[2025-02-28 14:48:06 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 1 loss:0.028268033638596535 norm:0.00042653633863665164 max memory_allocated 29271.62548828125 
[2025-02-28 14:48:52 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 2 loss:0.02655160427093506 norm:0.0002429987653158605 max memory_allocated 29271.62548828125 
[2025-02-28 14:49:38 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 3 loss:0.02572432905435562 norm:0.0001803322957130149 max memory_allocated 29271.62548828125 
[2025-02-28 14:50:23 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 4 loss:0.02510356530547142 norm:0.0001519210491096601 max memory_allocated 29271.62548828125 
[2025-02-28 14:51:09 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 5 loss:0.024624474346637726 norm:0.00013361242599785328 max memory_allocated 29271.62548828125 
[2025-02-28 14:51:54 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 6 loss:0.02434024214744568 norm:0.00012355386570561677 max memory_allocated 29271.62548828125 
[2025-02-28 14:52:40 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 7 loss:0.024182993918657303 norm:0.00011556901881704107 max memory_allocated 29271.62548828125 
[2025-02-28 14:53:26 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 8 loss:0.02407911978662014 norm:0.00011438236106187105 max memory_allocated 29271.62548828125 
[2025-02-28 14:54:11 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 9 loss:0.024011073634028435 norm:0.00010433291754452512 max memory_allocated 29271.62548828125 
[2025-02-28 14:54:57 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 10 loss:0.023961948230862617 norm:0.00010303496674168855 max memory_allocated 29271.62548828125 
[2025-02-28 14:55:43 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 11 loss:0.02392539381980896 norm:0.00010436592856422067 max memory_allocated 29271.62548828125 
[2025-02-28 14:56:28 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 12 loss:0.02389698475599289 norm:0.00010122591629624367 max memory_allocated 29271.62548828125 
[2025-02-28 14:57:14 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 13 loss:0.0238798800855875 norm:0.00010535811452427879 max memory_allocated 29271.62548828125 
[2025-02-28 14:58:00 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 14 loss:0.02386164292693138 norm:9.604723163647577e-05 max memory_allocated 29271.62548828125 
[2025-02-28 14:58:45 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 15 loss:0.023853901773691177 norm:0.00010484670201549307 max memory_allocated 29271.62548828125 
[2025-02-28 14:59:31 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 16 loss:0.02384362183511257 norm:0.00010622321133268997 max memory_allocated 29271.62548828125 
[2025-02-28 15:00:17 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 17 loss:0.023816216737031937 norm:0.00010288116027368233 max memory_allocated 29271.62548828125 
[2025-02-28 15:01:02 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 18 loss:0.023806257173419 norm:0.00010114235192304477 max memory_allocated 29271.62548828125 
[2025-02-28 15:01:48 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 19 loss:0.023801900446414948 norm:9.947815851774067e-05 max memory_allocated 29271.62548828125 
[2025-02-28 15:02:01 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-02-28 15:02:51 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 0 loss:0.036583397537469864 norm:0.0013145728735253215 max memory_allocated 29271.62548828125 
[2025-02-28 15:03:36 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 1 loss:0.031021760776638985 norm:0.0005546407774090767 max memory_allocated 29271.62548828125 
[2025-02-28 15:04:22 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 2 loss:0.029008183628320694 norm:0.00032960358657874167 max memory_allocated 29271.62548828125 
[2025-02-28 15:05:08 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 3 loss:0.02790495753288269 norm:0.00022920069750398397 max memory_allocated 29271.62548828125 
[2025-02-28 15:05:53 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 4 loss:0.027134092524647713 norm:0.0001867459068307653 max memory_allocated 29271.62548828125 
[2025-02-28 15:06:39 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 5 loss:0.026624266058206558 norm:0.0001616623339941725 max memory_allocated 29271.62548828125 
[2025-02-28 15:07:25 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 6 loss:0.026338137686252594 norm:0.0001509972644271329 max memory_allocated 29271.62548828125 
[2025-02-28 15:08:11 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 7 loss:0.026203028857707977 norm:0.00014058357919566333 max memory_allocated 29271.62548828125 
[2025-02-28 15:08:56 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 8 loss:0.026139168068766594 norm:0.00013510057760868222 max memory_allocated 29271.62548828125 
[2025-02-28 15:09:42 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 9 loss:0.026062704622745514 norm:0.0001354442210868001 max memory_allocated 29271.62548828125 
[2025-02-28 15:10:28 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 10 loss:0.026018500328063965 norm:0.000136874383315444 max memory_allocated 29271.62548828125 
[2025-02-28 15:11:13 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 11 loss:0.025986483320593834 norm:0.00013199450040701777 max memory_allocated 29271.62548828125 
[2025-02-28 15:11:59 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 12 loss:0.025947701185941696 norm:0.00013246781600173563 max memory_allocated 29271.62548828125 
[2025-02-28 15:12:45 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 13 loss:0.025918003171682358 norm:0.0001339662412647158 max memory_allocated 29271.62548828125 
[2025-02-28 15:13:30 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 14 loss:0.025887001305818558 norm:0.00012718966172542423 max memory_allocated 29271.62548828125 
[2025-02-28 15:14:16 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 15 loss:0.025875557214021683 norm:0.0001326365309068933 max memory_allocated 29271.62548828125 
[2025-02-28 15:15:02 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 16 loss:0.025849055498838425 norm:0.00012627629621420056 max memory_allocated 29271.62548828125 
[2025-02-28 15:15:48 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 17 loss:0.025845807045698166 norm:0.00013508422125596553 max memory_allocated 29271.62548828125 
[2025-02-28 15:16:33 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 18 loss:0.02585870586335659 norm:0.00013643354759551585 max memory_allocated 29271.62548828125 
[2025-02-28 15:17:19 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 19 loss:0.025853492319583893 norm:0.00013677621609531343 max memory_allocated 29271.62548828125 
[2025-02-28 15:17:32 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-02-28 15:18:22 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 0 loss:0.044491395354270935 norm:0.00133105821441859 max memory_allocated 29271.62548828125 
[2025-02-28 15:19:07 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 1 loss:0.03665144741535187 norm:0.0006028550560586154 max memory_allocated 29271.62548828125 
[2025-02-28 15:19:53 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 2 loss:0.033780086785554886 norm:0.00036114262184128165 max memory_allocated 29271.62548828125 
[2025-02-28 15:20:39 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 3 loss:0.032547663897275925 norm:0.0002755551540758461 max memory_allocated 29271.62548828125 
[2025-02-28 15:21:24 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 4 loss:0.03169090673327446 norm:0.00023568759206682444 max memory_allocated 29271.62548828125 
[2025-02-28 15:22:10 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 5 loss:0.031149405986070633 norm:0.00021394634677562863 max memory_allocated 29271.62548828125 
[2025-02-28 15:22:56 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 6 loss:0.030797088518738747 norm:0.00019984180107712746 max memory_allocated 29271.62548828125 
[2025-02-28 15:23:41 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 7 loss:0.03057512640953064 norm:0.00019375178089831024 max memory_allocated 29271.62548828125 
[2025-02-28 15:24:27 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 8 loss:0.030421670526266098 norm:0.00019241437257733196 max memory_allocated 29271.62548828125 
[2025-02-28 15:25:13 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 9 loss:0.030336817726492882 norm:0.00018980482127517462 max memory_allocated 29271.62548828125 
[2025-02-28 15:25:59 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 10 loss:0.030295994132757187 norm:0.00017728336388245225 max memory_allocated 29271.62548828125 
[2025-02-28 15:26:44 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 11 loss:0.030196476727724075 norm:0.00017526822921354324 max memory_allocated 29271.62548828125 
[2025-02-28 15:27:30 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 12 loss:0.030129801481962204 norm:0.0001673741644481197 max memory_allocated 29271.62548828125 
[2025-02-28 15:28:16 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 13 loss:0.03012300282716751 norm:0.00017367109830956906 max memory_allocated 29271.62548828125 
[2025-02-28 15:29:01 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 14 loss:0.030120765790343285 norm:0.00016878005408216268 max memory_allocated 29271.62548828125 
[2025-02-28 15:29:47 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 15 loss:0.030106239020824432 norm:0.00016394173144362867 max memory_allocated 29271.62548828125 
[2025-02-28 15:30:33 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 16 loss:0.030082806944847107 norm:0.00016549119027331471 max memory_allocated 29271.62548828125 
[2025-02-28 15:31:19 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 17 loss:0.030089933425188065 norm:0.00016614963533356786 max memory_allocated 29271.62548828125 
[2025-02-28 15:32:04 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 18 loss:0.03007132187485695 norm:0.0001600423565832898 max memory_allocated 29271.62548828125 
[2025-02-28 15:32:50 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 19 loss:0.030022062361240387 norm:0.00015479998546652496 max memory_allocated 29271.62548828125 
[2025-02-28 15:33:03 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-02-28 15:33:53 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 0 loss:0.04488737881183624 norm:0.001280221389606595 max memory_allocated 29271.62548828125 
[2025-02-28 15:34:38 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 1 loss:0.03859005495905876 norm:0.0006127806846052408 max memory_allocated 29271.62548828125 
[2025-02-28 15:35:24 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 2 loss:0.035833410918712616 norm:0.0003668306744657457 max memory_allocated 29271.62548828125 
[2025-02-28 15:36:10 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 3 loss:0.03448119014501572 norm:0.00025490770349279046 max memory_allocated 29271.62548828125 
[2025-02-28 15:36:55 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 4 loss:0.03358396142721176 norm:0.00020355045853648335 max memory_allocated 29271.62548828125 
[2025-02-28 15:37:41 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 5 loss:0.033048439770936966 norm:0.000177197769517079 max memory_allocated 29271.62548828125 
[2025-02-28 15:38:27 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 6 loss:0.03273914009332657 norm:0.00016091713041532785 max memory_allocated 29271.62548828125 
[2025-02-28 15:39:13 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 7 loss:0.03257547691464424 norm:0.00015516065468546003 max memory_allocated 29271.62548828125 
[2025-02-28 15:39:58 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 8 loss:0.03244161233305931 norm:0.0001499602512922138 max memory_allocated 29271.62548828125 
[2025-02-28 15:40:44 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 9 loss:0.03236488625407219 norm:0.00014313295832835138 max memory_allocated 29271.62548828125 
[2025-02-28 15:41:30 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 10 loss:0.032311491668224335 norm:0.0001439205661881715 max memory_allocated 29271.62548828125 
[2025-02-28 15:42:15 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 11 loss:0.03226133808493614 norm:0.00013786893396172673 max memory_allocated 29271.62548828125 
[2025-02-28 15:43:01 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 12 loss:0.03222673386335373 norm:0.0001427206298103556 max memory_allocated 29271.62548828125 
[2025-02-28 15:43:47 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 13 loss:0.032186344265937805 norm:0.00013435684377327561 max memory_allocated 29271.62548828125 
[2025-02-28 15:44:33 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 14 loss:0.03214862570166588 norm:0.00013048923574388027 max memory_allocated 29271.62548828125 
[2025-02-28 15:45:18 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 15 loss:0.03213317319750786 norm:0.00013677118113264441 max memory_allocated 29271.62548828125 
[2025-02-28 15:46:04 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 16 loss:0.032102927565574646 norm:0.0001296092086704448 max memory_allocated 29271.62548828125 
[2025-02-28 15:46:50 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 17 loss:0.03208114951848984 norm:0.00012773026537615806 max memory_allocated 29271.62548828125 
[2025-02-28 15:47:35 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 18 loss:0.03207280486822128 norm:0.0001322349562542513 max memory_allocated 29271.62548828125 
[2025-02-28 15:48:21 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 19 loss:0.03205357864499092 norm:0.00012397777754813433 max memory_allocated 29271.62548828125 
[2025-02-28 15:48:35 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-02-28 15:49:24 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 0 loss:0.06521578878164291 norm:0.0027627763338387012 max memory_allocated 29271.62548828125 
[2025-02-28 15:50:10 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 1 loss:0.04909070208668709 norm:0.0012506050989031792 max memory_allocated 29271.62548828125 
[2025-02-28 15:50:55 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 2 loss:0.042356956750154495 norm:0.0006548935198225081 max memory_allocated 29271.62548828125 
[2025-02-28 15:51:41 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 3 loss:0.03985288739204407 norm:0.0004364147607702762 max memory_allocated 29271.62548828125 
[2025-02-28 15:52:27 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 4 loss:0.038459960371255875 norm:0.00034314568620175123 max memory_allocated 29271.62548828125 
[2025-02-28 15:53:12 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 5 loss:0.037677593529224396 norm:0.0002955690724775195 max memory_allocated 29271.62548828125 
[2025-02-28 15:53:58 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 6 loss:0.03719398379325867 norm:0.00027007563039660454 max memory_allocated 29271.62548828125 
[2025-02-28 15:54:44 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 7 loss:0.0368821956217289 norm:0.00025745274615474045 max memory_allocated 29271.62548828125 
[2025-02-28 15:55:30 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 8 loss:0.03669299930334091 norm:0.00024662469513714314 max memory_allocated 29271.62548828125 
[2025-02-28 15:56:15 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 9 loss:0.03654944896697998 norm:0.00023750559194013476 max memory_allocated 29271.62548828125 
[2025-02-28 15:57:01 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 10 loss:0.03640197962522507 norm:0.0002247113297926262 max memory_allocated 29271.62548828125 
[2025-02-28 15:57:47 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 11 loss:0.03628215566277504 norm:0.0002192076062783599 max memory_allocated 29271.62548828125 
[2025-02-28 15:58:32 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 12 loss:0.036191247403621674 norm:0.0002182118914788589 max memory_allocated 29271.62548828125 
[2025-02-28 15:59:18 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 13 loss:0.03611122444272041 norm:0.00020512964692898095 max memory_allocated 29271.62548828125 
[2025-02-28 16:00:04 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 14 loss:0.03603160381317139 norm:0.00020268252410460263 max memory_allocated 29271.62548828125 
[2025-02-28 16:00:50 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 15 loss:0.035966191440820694 norm:0.00020991584460716695 max memory_allocated 29271.62548828125 
[2025-02-28 16:01:35 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 16 loss:0.0358993336558342 norm:0.00020358129404485226 max memory_allocated 29271.62548828125 
[2025-02-28 16:02:21 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 17 loss:0.03586818650364876 norm:0.00019461600459180772 max memory_allocated 29271.62548828125 
[2025-02-28 16:03:07 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 18 loss:0.03580864518880844 norm:0.0001877815229818225 max memory_allocated 29271.62548828125 
[2025-02-28 16:03:52 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 19 loss:0.03575762361288071 norm:0.0001820269535528496 max memory_allocated 29271.62548828125 
[2025-02-28 16:04:06 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-02-28 16:04:55 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 0 loss:0.05372878536581993 norm:0.0014743281062692404 max memory_allocated 29271.75048828125 
[2025-02-28 16:05:41 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 1 loss:0.04592330381274223 norm:0.0007483168737962842 max memory_allocated 29271.75048828125 
[2025-02-28 16:06:27 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 2 loss:0.042694296687841415 norm:0.00045118172420188785 max memory_allocated 29271.75048828125 
[2025-02-28 16:07:12 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 3 loss:0.04100044444203377 norm:0.00032171831117011607 max memory_allocated 29271.75048828125 
[2025-02-28 16:07:58 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 4 loss:0.03991375118494034 norm:0.0002542117435950786 max memory_allocated 29271.75048828125 
[2025-02-28 16:08:44 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 5 loss:0.0392598994076252 norm:0.00021700029901694506 max memory_allocated 29271.75048828125 
[2025-02-28 16:09:30 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 6 loss:0.038891345262527466 norm:0.00019265773880761117 max memory_allocated 29271.75048828125 
[2025-02-28 16:10:15 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 7 loss:0.038660768419504166 norm:0.00017822597874328494 max memory_allocated 29271.75048828125 
[2025-02-28 16:11:01 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 8 loss:0.03852115571498871 norm:0.00017112091882154346 max memory_allocated 29271.75048828125 
[2025-02-28 16:11:47 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 9 loss:0.03843638300895691 norm:0.0001669083721935749 max memory_allocated 29271.75048828125 
[2025-02-28 16:12:32 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 10 loss:0.038345158100128174 norm:0.00016049914120230824 max memory_allocated 29271.75048828125 
[2025-02-28 16:13:18 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 11 loss:0.03827106952667236 norm:0.0001547459396533668 max memory_allocated 29271.75048828125 
[2025-02-28 16:14:04 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 12 loss:0.038227666169404984 norm:0.00015476190310437232 max memory_allocated 29271.75048828125 
[2025-02-28 16:14:50 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 13 loss:0.03818301111459732 norm:0.0001494056050432846 max memory_allocated 29271.75048828125 
[2025-02-28 16:15:35 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 14 loss:0.03814235329627991 norm:0.00014064949937164783 max memory_allocated 29271.75048828125 
[2025-02-28 16:16:21 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 15 loss:0.03809253126382828 norm:0.00013943191152065992 max memory_allocated 29271.75048828125 
[2025-02-28 16:17:07 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 16 loss:0.038070302456617355 norm:0.00013235611550044268 max memory_allocated 29271.75048828125 
[2025-02-28 16:17:52 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 17 loss:0.03805430606007576 norm:0.00013090009451843798 max memory_allocated 29271.75048828125 
[2025-02-28 16:18:38 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 18 loss:0.03805830329656601 norm:0.00013491780555341393 max memory_allocated 29271.75048828125 
[2025-02-28 16:19:24 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 19 loss:0.0380418635904789 norm:0.00012916472041979432 max memory_allocated 29271.75048828125 
[2025-02-28 16:19:37 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-02-28 16:20:28 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 0 loss:0.054048530757427216 norm:0.001063754316419363 max memory_allocated 29271.93798828125 
[2025-02-28 16:21:13 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 1 loss:0.047297365963459015 norm:0.0005122805596329272 max memory_allocated 29271.93798828125 
[2025-02-28 16:21:59 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 2 loss:0.04456641525030136 norm:0.0003123612841591239 max memory_allocated 29271.93798828125 
[2025-02-28 16:22:45 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 3 loss:0.0431986078619957 norm:0.00023206172045320272 max memory_allocated 29271.93798828125 
[2025-02-28 16:23:31 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 4 loss:0.042285650968551636 norm:0.00019612902542576194 max memory_allocated 29271.93798828125 
[2025-02-28 16:24:16 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 5 loss:0.04174865782260895 norm:0.00017881274106912315 max memory_allocated 29271.93798828125 
[2025-02-28 16:25:02 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 6 loss:0.04142483323812485 norm:0.00016716195386834443 max memory_allocated 29271.93798828125 
[2025-02-28 16:25:48 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 7 loss:0.04122018814086914 norm:0.00015200900088530034 max memory_allocated 29271.93798828125 
[2025-02-28 16:26:33 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 8 loss:0.041113004088401794 norm:0.00014498863311018795 max memory_allocated 29271.93798828125 
[2025-02-28 16:27:19 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 9 loss:0.04102892801165581 norm:0.00014615480904467404 max memory_allocated 29271.93798828125 
[2025-02-28 16:28:05 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 10 loss:0.04096680507063866 norm:0.0001441657223040238 max memory_allocated 29271.93798828125 
[2025-02-28 16:28:51 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 11 loss:0.040913745760917664 norm:0.00013688451144844294 max memory_allocated 29271.93798828125 
[2025-02-28 16:29:36 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 12 loss:0.04087657108902931 norm:0.00013539582141675055 max memory_allocated 29271.93798828125 
[2025-02-28 16:30:22 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 13 loss:0.04082660377025604 norm:0.0001336253626504913 max memory_allocated 29271.93798828125 
[2025-02-28 16:31:08 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 14 loss:0.0407666452229023 norm:0.0001282524026464671 max memory_allocated 29271.93798828125 
[2025-02-28 16:31:53 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 15 loss:0.04074813053011894 norm:0.00012730652815662324 max memory_allocated 29271.93798828125 
[2025-02-28 16:32:39 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 16 loss:0.040710095316171646 norm:0.00012501532910391688 max memory_allocated 29271.93798828125 
[2025-02-28 16:33:25 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 17 loss:0.0406918004155159 norm:0.00011988947517238557 max memory_allocated 29271.93798828125 
[2025-02-28 16:34:11 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 18 loss:0.04069280996918678 norm:0.00012615461309906095 max memory_allocated 29271.93798828125 
[2025-02-28 16:34:56 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 19 loss:0.04068263992667198 norm:0.00011950630869250745 max memory_allocated 29271.93798828125 
[2025-02-28 16:35:10 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-02-28 16:35:59 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 0 loss:0.05719120427966118 norm:0.0010137484641745687 max memory_allocated 29272.12548828125 
[2025-02-28 16:36:45 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 1 loss:0.04949416220188141 norm:0.0005113901570439339 max memory_allocated 29272.12548828125 
[2025-02-28 16:37:30 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 2 loss:0.04631361737847328 norm:0.0002974413800984621 max memory_allocated 29272.12548828125 
[2025-02-28 16:38:16 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 3 loss:0.04500982165336609 norm:0.00023057115322444588 max memory_allocated 29272.12548828125 
[2025-02-28 16:39:02 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 4 loss:0.044107843190431595 norm:0.00019748759223148227 max memory_allocated 29272.12548828125 
[2025-02-28 16:39:48 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 5 loss:0.0435258187353611 norm:0.00017618288984522223 max memory_allocated 29272.12548828125 
[2025-02-28 16:40:33 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 6 loss:0.04318490996956825 norm:0.00016204090206883848 max memory_allocated 29272.12548828125 
[2025-02-28 16:41:19 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 7 loss:0.04297314211726189 norm:0.00015478617569897324 max memory_allocated 29272.12548828125 
[2025-02-28 16:42:05 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 8 loss:0.042824119329452515 norm:0.0001444072840968147 max memory_allocated 29272.12548828125 
[2025-02-28 16:42:50 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 9 loss:0.04272433742880821 norm:0.00013901616330258548 max memory_allocated 29272.12548828125 
[2025-02-28 16:43:36 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 10 loss:0.04266425967216492 norm:0.0001442536449758336 max memory_allocated 29272.12548828125 
[2025-02-28 16:44:22 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 11 loss:0.04258112609386444 norm:0.00013355967530515045 max memory_allocated 29272.12548828125 
[2025-02-28 16:45:08 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 12 loss:0.042528752237558365 norm:0.00012957045692019165 max memory_allocated 29272.12548828125 
[2025-02-28 16:45:53 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 13 loss:0.04247845336794853 norm:0.00012303932453505695 max memory_allocated 29272.12548828125 
[2025-02-28 16:46:39 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 14 loss:0.04242026060819626 norm:0.00011583138984860852 max memory_allocated 29272.12548828125 
[2025-02-28 16:47:25 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 15 loss:0.04240794852375984 norm:0.00011337681644363329 max memory_allocated 29272.12548828125 
[2025-02-28 16:48:10 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 16 loss:0.04237231984734535 norm:0.00011023001570720226 max memory_allocated 29272.12548828125 
[2025-02-28 16:48:56 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 17 loss:0.04234716296195984 norm:0.00010779131116578355 max memory_allocated 29272.12548828125 
[2025-02-28 16:49:42 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 18 loss:0.04233613237738609 norm:0.00010707750334404409 max memory_allocated 29272.12548828125 
[2025-02-28 16:50:28 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 19 loss:0.042320847511291504 norm:0.00011165262549184263 max memory_allocated 29272.12548828125 
[2025-02-28 16:50:41 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-02-28 16:51:30 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 0 loss:0.05766138434410095 norm:0.0011306889355182648 max memory_allocated 29272.31298828125 
[2025-02-28 16:52:16 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 1 loss:0.05106085538864136 norm:0.000570335250813514 max memory_allocated 29272.31298828125 
[2025-02-28 16:53:02 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 2 loss:0.048381172120571136 norm:0.0003646600234787911 max memory_allocated 29272.31298828125 
[2025-02-28 16:53:47 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 3 loss:0.04692108929157257 norm:0.0002736174501478672 max memory_allocated 29272.31298828125 
[2025-02-28 16:54:33 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 4 loss:0.045895494520664215 norm:0.0002260372566524893 max memory_allocated 29272.31298828125 
[2025-02-28 16:55:19 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 5 loss:0.04530014470219612 norm:0.00019991048611700535 max memory_allocated 29272.31298828125 
[2025-02-28 16:56:05 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 6 loss:0.04493186995387077 norm:0.00018336213543079793 max memory_allocated 29272.31298828125 
[2025-02-28 16:56:50 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 7 loss:0.04472936689853668 norm:0.00017650671361479908 max memory_allocated 29272.31298828125 
[2025-02-28 16:57:36 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 8 loss:0.044577743858098984 norm:0.0001651484053581953 max memory_allocated 29272.31298828125 
[2025-02-28 16:58:22 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 9 loss:0.04446659982204437 norm:0.00015898434503469616 max memory_allocated 29272.31298828125 
[2025-02-28 16:59:08 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 10 loss:0.04435020312666893 norm:0.00014979207480791956 max memory_allocated 29272.31298828125 
[2025-02-28 16:59:53 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 11 loss:0.04425344988703728 norm:0.0001423978101229295 max memory_allocated 29272.31298828125 
[2025-02-28 17:00:39 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 12 loss:0.044208381325006485 norm:0.00014115782687440515 max memory_allocated 29272.31298828125 
[2025-02-28 17:01:25 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 13 loss:0.04416729882359505 norm:0.00013721190043725073 max memory_allocated 29272.31298828125 
[2025-02-28 17:02:10 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 14 loss:0.04413101077079773 norm:0.0001340288872597739 max memory_allocated 29272.31298828125 
[2025-02-28 17:02:56 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 15 loss:0.04408334940671921 norm:0.00012949561642017215 max memory_allocated 29272.31298828125 
[2025-02-28 17:03:42 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 16 loss:0.044048577547073364 norm:0.00012965335918124765 max memory_allocated 29272.31298828125 
[2025-02-28 17:04:28 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 17 loss:0.04401203617453575 norm:0.00012187083484604955 max memory_allocated 29272.31298828125 
[2025-02-28 17:05:13 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 18 loss:0.04399426281452179 norm:0.00011872001778101549 max memory_allocated 29272.31298828125 
[2025-02-28 17:05:59 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 19 loss:0.04394751787185669 norm:0.00011072171037085354 max memory_allocated 29272.31298828125 
[2025-02-28 17:06:13 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-02-28 17:07:02 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 0 loss:0.05709543824195862 norm:0.0009542586631141603 max memory_allocated 29272.50048828125 
[2025-02-28 17:07:47 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 1 loss:0.051543496549129486 norm:0.0004886573879048228 max memory_allocated 29272.50048828125 
[2025-02-28 17:08:33 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 2 loss:0.04926936328411102 norm:0.00031173042953014374 max memory_allocated 29272.50048828125 
[2025-02-28 17:09:19 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 3 loss:0.048057202249765396 norm:0.00023690785747021437 max memory_allocated 29272.50048828125 
[2025-02-28 17:10:05 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 4 loss:0.04720909148454666 norm:0.0001971086603589356 max memory_allocated 29272.50048828125 
[2025-02-28 17:10:50 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 5 loss:0.04666033387184143 norm:0.00017569103511050344 max memory_allocated 29272.50048828125 
[2025-02-28 17:11:36 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 6 loss:0.04629842936992645 norm:0.00015835664817132056 max memory_allocated 29272.50048828125 
[2025-02-28 17:12:22 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 7 loss:0.04608761519193649 norm:0.00015272913151420653 max memory_allocated 29272.50048828125 
[2025-02-28 17:13:08 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 8 loss:0.04593030735850334 norm:0.000142503427923657 max memory_allocated 29272.50048828125 
[2025-02-28 17:13:53 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 9 loss:0.0458356998860836 norm:0.00013775851402897388 max memory_allocated 29272.50048828125 
[2025-02-28 17:14:39 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 10 loss:0.04574092477560043 norm:0.0001315241534030065 max memory_allocated 29272.50048828125 
[2025-02-28 17:15:25 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 11 loss:0.045668989419937134 norm:0.0001259036362171173 max memory_allocated 29272.50048828125 
[2025-02-28 17:16:11 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 12 loss:0.045599065721035004 norm:0.00011992751387879252 max memory_allocated 29272.50048828125 
[2025-02-28 17:16:56 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 13 loss:0.04556925594806671 norm:0.00011740085756173357 max memory_allocated 29272.50048828125 
[2025-02-28 17:17:42 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 14 loss:0.0455341562628746 norm:0.00011205606278963387 max memory_allocated 29272.50048828125 
[2025-02-28 17:18:28 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 15 loss:0.04550793394446373 norm:0.00011015772906830534 max memory_allocated 29272.50048828125 
[2025-02-28 17:19:14 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 16 loss:0.04548100009560585 norm:0.00010964954708470032 max memory_allocated 29272.50048828125 
[2025-02-28 17:19:59 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 17 loss:0.04547024145722389 norm:0.00010764906619442627 max memory_allocated 29272.50048828125 
[2025-02-28 17:20:45 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 18 loss:0.04546340927481651 norm:0.00011032554175471887 max memory_allocated 29272.50048828125 
[2025-02-28 17:21:31 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 19 loss:0.04543524235486984 norm:0.00010499908967176452 max memory_allocated 29272.50048828125 
[2025-02-28 17:21:44 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-02-28 17:22:34 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 0 loss:0.05412828549742699 norm:0.0006335586658678949 max memory_allocated 29272.68798828125 
[2025-02-28 17:23:20 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 1 loss:0.0504613071680069 norm:0.00034131851862184703 max memory_allocated 29272.68798828125 
[2025-02-28 17:24:05 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 2 loss:0.04870421066880226 norm:0.00023161666467785835 max memory_allocated 29272.68798828125 
[2025-02-28 17:24:51 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 3 loss:0.04765734449028969 norm:0.00018100689339917153 max memory_allocated 29272.68798828125 
[2025-02-28 17:25:37 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 4 loss:0.0469462014734745 norm:0.00015763055125717074 max memory_allocated 29272.68798828125 
[2025-02-28 17:26:22 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 5 loss:0.046448178589344025 norm:0.00014316604938358068 max memory_allocated 29272.68798828125 
[2025-02-28 17:27:08 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 6 loss:0.046125348657369614 norm:0.0001329286169493571 max memory_allocated 29272.68798828125 
[2025-02-28 17:27:54 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 7 loss:0.04593050479888916 norm:0.00012306368444114923 max memory_allocated 29272.68798828125 
[2025-02-28 17:28:40 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 8 loss:0.04580087959766388 norm:0.00011732486018445343 max memory_allocated 29272.68798828125 
[2025-02-28 17:29:25 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 9 loss:0.045721378177404404 norm:0.0001137193976319395 max memory_allocated 29272.68798828125 
[2025-02-28 17:30:11 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 10 loss:0.04564770311117172 norm:0.00010765932529466227 max memory_allocated 29272.68798828125 
[2025-02-28 17:30:57 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 11 loss:0.04559241980314255 norm:0.00010376396676292643 max memory_allocated 29272.68798828125 
[2025-02-28 17:31:42 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 12 loss:0.04556223377585411 norm:0.00010105260298587382 max memory_allocated 29272.68798828125 
[2025-02-28 17:32:28 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 13 loss:0.04551674425601959 norm:9.460178262088448e-05 max memory_allocated 29272.68798828125 
[2025-02-28 17:33:14 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 14 loss:0.0454850010573864 norm:9.39885139814578e-05 max memory_allocated 29272.68798828125 
[2025-02-28 17:34:00 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 15 loss:0.045468732714653015 norm:9.262246749131009e-05 max memory_allocated 29272.68798828125 
[2025-02-28 17:34:45 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 16 loss:0.04544626548886299 norm:8.805235847830772e-05 max memory_allocated 29272.68798828125 
[2025-02-28 17:35:31 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 17 loss:0.04542379081249237 norm:8.85029585333541e-05 max memory_allocated 29272.68798828125 
[2025-02-28 17:36:17 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 18 loss:0.04541170224547386 norm:8.883363625500351e-05 max memory_allocated 29272.68798828125 
[2025-02-28 17:37:02 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 19 loss:0.04539689049124718 norm:9.009442146634683e-05 max memory_allocated 29272.68798828125 
[2025-02-28 17:37:16 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-02-28 17:38:05 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 0 loss:0.05883786082267761 norm:0.0010599825764074922 max memory_allocated 29272.87548828125 
[2025-02-28 17:38:51 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 1 loss:0.053146395832300186 norm:0.0005298270843923092 max memory_allocated 29272.87548828125 
[2025-02-28 17:39:36 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 2 loss:0.050693292170763016 norm:0.0003438325074966997 max memory_allocated 29272.87548828125 
[2025-02-28 17:40:22 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 3 loss:0.04927513748407364 norm:0.00024411670165136456 max memory_allocated 29272.87548828125 
[2025-02-28 17:41:08 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 4 loss:0.048410672694444656 norm:0.00020576143288053572 max memory_allocated 29272.87548828125 
[2025-02-28 17:41:54 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 5 loss:0.04780782386660576 norm:0.00018482258019503206 max memory_allocated 29272.87548828125 
[2025-02-28 17:42:39 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 6 loss:0.04742714390158653 norm:0.00016825069906190038 max memory_allocated 29272.87548828125 
[2025-02-28 17:43:25 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 7 loss:0.047180209308862686 norm:0.00015555659774690866 max memory_allocated 29272.87548828125 
[2025-02-28 17:44:11 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 8 loss:0.04702284187078476 norm:0.00014974917576182634 max memory_allocated 29272.87548828125 
[2025-02-28 17:44:56 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 9 loss:0.04690886288881302 norm:0.0001432726567145437 max memory_allocated 29272.87548828125 
[2025-02-28 17:45:42 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 10 loss:0.04681817814707756 norm:0.0001409027900081128 max memory_allocated 29272.87548828125 
[2025-02-28 17:46:28 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 11 loss:0.04673193395137787 norm:0.00013504807429853827 max memory_allocated 29272.87548828125 
[2025-02-28 17:47:13 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 12 loss:0.04664870351552963 norm:0.00012601993512362242 max memory_allocated 29272.87548828125 
[2025-02-28 17:47:59 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 13 loss:0.046601999551057816 norm:0.0001285716862184927 max memory_allocated 29272.87548828125 
[2025-02-28 17:48:45 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 14 loss:0.04655489698052406 norm:0.0001230935158673674 max memory_allocated 29272.87548828125 
[2025-02-28 17:49:30 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 15 loss:0.04648686200380325 norm:0.0001198705576825887 max memory_allocated 29272.87548828125 
[2025-02-28 17:50:16 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 16 loss:0.046445898711681366 norm:0.00011728322715498507 max memory_allocated 29272.87548828125 
[2025-02-28 17:51:02 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 17 loss:0.04640632122755051 norm:0.00011581068974919617 max memory_allocated 29272.87548828125 
[2025-02-28 17:51:48 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 18 loss:0.046380072832107544 norm:0.0001113229664042592 max memory_allocated 29272.87548828125 
[2025-02-28 17:52:33 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 19 loss:0.04634726420044899 norm:0.00010392618423793465 max memory_allocated 29272.87548828125 
[2025-02-28 17:52:47 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-02-28 17:53:36 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 0 loss:0.05544289946556091 norm:0.0009961060713976622 max memory_allocated 29273.06298828125 
[2025-02-28 17:54:22 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 1 loss:0.05178835242986679 norm:0.0005349298007786274 max memory_allocated 29273.06298828125 
[2025-02-28 17:55:07 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 2 loss:0.050156399607658386 norm:0.0003521207836456597 max memory_allocated 29273.06298828125 
[2025-02-28 17:55:53 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 3 loss:0.04909702017903328 norm:0.00025487292441539466 max memory_allocated 29273.06298828125 
[2025-02-28 17:56:39 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 4 loss:0.04832669347524643 norm:0.00020797290198970586 max memory_allocated 29273.06298828125 
[2025-02-28 17:57:24 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 5 loss:0.047798920422792435 norm:0.00018150692631024867 max memory_allocated 29273.06298828125 
[2025-02-28 17:58:10 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 6 loss:0.04748350381851196 norm:0.00016095212777145207 max memory_allocated 29273.06298828125 
[2025-02-28 17:58:56 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 7 loss:0.047275297343730927 norm:0.00014524630387313664 max memory_allocated 29273.06298828125 
[2025-02-28 17:59:42 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 8 loss:0.04717196524143219 norm:0.000141483498737216 max memory_allocated 29273.06298828125 
[2025-02-28 18:00:27 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 9 loss:0.047084271907806396 norm:0.00013538890925701708 max memory_allocated 29273.06298828125 
[2025-02-28 18:01:13 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 10 loss:0.047016341239213943 norm:0.0001313968386966735 max memory_allocated 29273.06298828125 
[2025-02-28 18:01:59 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 11 loss:0.04696761071681976 norm:0.00012805119331460446 max memory_allocated 29273.06298828125 
[2025-02-28 18:02:44 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 12 loss:0.04691328853368759 norm:0.00012435944518074393 max memory_allocated 29273.06298828125 
[2025-02-28 18:03:30 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 13 loss:0.04687165841460228 norm:0.00012122943735448644 max memory_allocated 29273.06298828125 
[2025-02-28 18:04:16 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 14 loss:0.04683605208992958 norm:0.0001193497228086926 max memory_allocated 29273.06298828125 
[2025-02-28 18:05:01 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 15 loss:0.04680110141634941 norm:0.0001153002303908579 max memory_allocated 29273.06298828125 
[2025-02-28 18:05:47 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 16 loss:0.04676948860287666 norm:0.00011344764061504975 max memory_allocated 29273.06298828125 
[2025-02-28 18:06:33 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 17 loss:0.04674086719751358 norm:0.00011124375305371359 max memory_allocated 29273.06298828125 
[2025-02-28 18:07:19 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 18 loss:0.04673633351922035 norm:0.00010964628017973155 max memory_allocated 29273.06298828125 
[2025-02-28 18:08:04 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 19 loss:0.04671189934015274 norm:0.00010502873919904232 max memory_allocated 29273.06298828125 
[2025-02-28 18:08:18 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-02-28 18:09:07 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 0 loss:0.055258385837078094 norm:0.000711043830960989 max memory_allocated 29273.25048828125 
[2025-02-28 18:09:53 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 1 loss:0.05186723545193672 norm:0.0003416459949221462 max memory_allocated 29273.25048828125 
[2025-02-28 18:10:39 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 2 loss:0.050538595765829086 norm:0.00022897866438142955 max memory_allocated 29273.25048828125 
[2025-02-28 18:11:24 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 3 loss:0.04970230162143707 norm:0.00017775080050341785 max memory_allocated 29273.25048828125 
[2025-02-28 18:12:10 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 4 loss:0.04900079965591431 norm:0.00014587184705305845 max memory_allocated 29273.25048828125 
[2025-02-28 18:12:56 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 5 loss:0.04847200959920883 norm:0.00012651400174945593 max memory_allocated 29273.25048828125 
[2025-02-28 18:13:41 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 6 loss:0.04816450551152229 norm:0.00011671118409140036 max memory_allocated 29273.25048828125 
[2025-02-28 18:14:27 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 7 loss:0.047989241778850555 norm:0.00010640044638421386 max memory_allocated 29273.25048828125 
[2025-02-28 18:15:13 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 8 loss:0.04790089279413223 norm:0.00010220960393780842 max memory_allocated 29273.25048828125 
[2025-02-28 18:15:58 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 9 loss:0.04783111810684204 norm:9.65189392445609e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:16:44 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 10 loss:0.04777314141392708 norm:9.379102266393602e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:17:30 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 11 loss:0.04770791530609131 norm:8.797480404609814e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:18:16 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 12 loss:0.047665320336818695 norm:8.750603592488915e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:19:01 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 13 loss:0.04763250797986984 norm:8.514085493516177e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:19:47 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 14 loss:0.04759826883673668 norm:8.184354373952374e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:20:33 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 15 loss:0.04756849259138107 norm:7.865596126066521e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:21:18 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 16 loss:0.047553326934576035 norm:7.768257637508214e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:22:04 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 17 loss:0.0475425124168396 norm:7.520941289840266e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:22:50 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 18 loss:0.04753351956605911 norm:7.486987306037918e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:23:36 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 19 loss:0.04752293601632118 norm:7.329892832785845e-05 max memory_allocated 29273.25048828125 
[2025-02-28 18:23:49 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-02-28 18:24:38 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 0 loss:0.05650867149233818 norm:0.0005688691162504256 max memory_allocated 29273.43798828125 
[2025-02-28 18:25:24 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 1 loss:0.053869783878326416 norm:0.00029647882911376655 max memory_allocated 29273.43798828125 
[2025-02-28 18:26:10 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 2 loss:0.05261988192796707 norm:0.00020212205708958209 max memory_allocated 29273.43798828125 
[2025-02-28 18:26:55 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 3 loss:0.05180779844522476 norm:0.00015228120901156217 max memory_allocated 29273.43798828125 
[2025-02-28 18:27:41 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 4 loss:0.05117986723780632 norm:0.00013184358249418437 max memory_allocated 29273.43798828125 
[2025-02-28 18:28:27 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 5 loss:0.05069047585129738 norm:0.00011994630767730996 max memory_allocated 29273.43798828125 
[2025-02-28 18:29:12 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 6 loss:0.05038746818900108 norm:0.00010995484626619145 max memory_allocated 29273.43798828125 
[2025-02-28 18:29:58 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 7 loss:0.05021478608250618 norm:0.00010365500202169642 max memory_allocated 29273.43798828125 
[2025-02-28 18:30:44 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 8 loss:0.05010911822319031 norm:9.660418436396867e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:31:30 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 9 loss:0.05003565922379494 norm:9.307303844252601e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:32:15 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 10 loss:0.04998423159122467 norm:8.777934999670833e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:33:01 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 11 loss:0.0499308705329895 norm:8.444955165032297e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:33:47 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 12 loss:0.049902383238077164 norm:8.140670252032578e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:34:33 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 13 loss:0.04986663907766342 norm:7.957719208206981e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:35:18 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 14 loss:0.04982951655983925 norm:7.830130198271945e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:36:04 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 15 loss:0.049817316234111786 norm:7.434097642544657e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:36:50 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 16 loss:0.04980667680501938 norm:7.489767449442297e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:37:35 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 17 loss:0.04979068785905838 norm:7.14832276571542e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:38:21 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 18 loss:0.04977370798587799 norm:7.185803406173363e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:39:07 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 19 loss:0.049758750945329666 norm:7.241911953315139e-05 max memory_allocated 29273.43798828125 
[2025-02-28 18:39:20 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-02-28 18:40:10 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 0 loss:0.05835183709859848 norm:0.0005063770804554224 max memory_allocated 29273.62548828125 
[2025-02-28 18:40:55 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 1 loss:0.0560373030602932 norm:0.00026486653950996697 max memory_allocated 29273.62548828125 
[2025-02-28 18:41:41 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 2 loss:0.054951343685388565 norm:0.0001810643298085779 max memory_allocated 29273.62548828125 
[2025-02-28 18:42:27 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 3 loss:0.054179005324840546 norm:0.00014028701116330922 max memory_allocated 29273.62548828125 
[2025-02-28 18:43:12 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 4 loss:0.053556960076093674 norm:0.00011745830124709755 max memory_allocated 29273.62548828125 
[2025-02-28 18:43:58 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 5 loss:0.053071070462465286 norm:0.00010367714276071638 max memory_allocated 29273.62548828125 
[2025-02-28 18:44:44 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 6 loss:0.05277827009558678 norm:9.814680379349738e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:45:29 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 7 loss:0.052615001797676086 norm:9.199071791954339e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:46:15 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 8 loss:0.05251287668943405 norm:8.924746362026781e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:47:01 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 9 loss:0.0524609312415123 norm:8.464804704999551e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:47:46 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 10 loss:0.05240512639284134 norm:7.819024904165417e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:48:32 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 11 loss:0.05237447842955589 norm:7.739051943644881e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:49:18 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 12 loss:0.05235789716243744 norm:7.496877515222877e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:50:04 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 13 loss:0.05232005566358566 norm:6.989374378463253e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:50:49 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 14 loss:0.052307456731796265 norm:7.01895187376067e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:51:35 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 15 loss:0.05227905511856079 norm:6.782575655961409e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:52:21 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 16 loss:0.05227344483137131 norm:7.030981942079961e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:53:06 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 17 loss:0.05226108431816101 norm:6.708796718157828e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:53:52 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 18 loss:0.052223529666662216 norm:6.518327427329496e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:54:38 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 19 loss:0.05221061035990715 norm:6.540693721035495e-05 max memory_allocated 29273.62548828125 
[2025-02-28 18:54:51 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-02-28 18:55:41 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 0 loss:0.0632433071732521 norm:0.0007044853991828859 max memory_allocated 29273.81298828125 
[2025-02-28 18:56:26 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 1 loss:0.06031960994005203 norm:0.00037701986730098724 max memory_allocated 29273.81298828125 
[2025-02-28 18:57:12 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 2 loss:0.05895387381315231 norm:0.00024102063616737723 max memory_allocated 29273.81298828125 
[2025-02-28 18:57:57 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 3 loss:0.05809973552823067 norm:0.00018177110177930444 max memory_allocated 29273.81298828125 
[2025-02-28 18:58:43 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 4 loss:0.05741501972079277 norm:0.00015246152179315686 max memory_allocated 29273.81298828125 
[2025-02-28 18:59:29 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 5 loss:0.05686527118086815 norm:0.0001358786248601973 max memory_allocated 29273.81298828125 
[2025-02-28 19:00:15 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 6 loss:0.05653482675552368 norm:0.0001254731323570013 max memory_allocated 29273.81298828125 
[2025-02-28 19:01:00 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 7 loss:0.05635308846831322 norm:0.00011552395881153643 max memory_allocated 29273.81298828125 
[2025-02-28 19:01:46 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 8 loss:0.056261975318193436 norm:0.00011126093158964068 max memory_allocated 29273.81298828125 
[2025-02-28 19:02:32 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 9 loss:0.056167446076869965 norm:0.00010520443174755201 max memory_allocated 29273.81298828125 
[2025-02-28 19:03:17 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 10 loss:0.05609655752778053 norm:0.00010029019904322922 max memory_allocated 29273.81298828125 
[2025-02-28 19:04:03 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 11 loss:0.05604422092437744 norm:9.811790368985385e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:04:49 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 12 loss:0.05600612983107567 norm:9.529657108942047e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:05:34 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 13 loss:0.05597947910428047 norm:9.554249118082225e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:06:20 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 14 loss:0.055949367582798004 norm:9.103142656385899e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:07:06 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 15 loss:0.0559178926050663 norm:8.630692900624126e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:07:51 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 16 loss:0.0559004545211792 norm:8.314921433338895e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:08:37 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 17 loss:0.0558788925409317 norm:8.22733054519631e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:09:23 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 18 loss:0.055869121104478836 norm:8.042411354836076e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:10:08 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 19 loss:0.05585631728172302 norm:8.04919982329011e-05 max memory_allocated 29273.81298828125 
[2025-02-28 19:10:22 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-02-28 19:11:11 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 0 loss:0.06505130231380463 norm:0.0005447997245937586 max memory_allocated 29274.00048828125 
[2025-02-28 19:11:57 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 1 loss:0.06287696957588196 norm:0.00025476960581727326 max memory_allocated 29274.00048828125 
[2025-02-28 19:12:42 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 2 loss:0.061942435801029205 norm:0.00016261948621831834 max memory_allocated 29274.00048828125 
[2025-02-28 19:13:28 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 3 loss:0.06114821881055832 norm:0.000125261620269157 max memory_allocated 29274.00048828125 
[2025-02-28 19:14:14 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 4 loss:0.06055618077516556 norm:0.00010373282566433772 max memory_allocated 29274.00048828125 
[2025-02-28 19:14:59 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 5 loss:0.06020038202404976 norm:9.136427252087742e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:15:45 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 6 loss:0.05998149514198303 norm:8.632143726572394e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:16:31 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 7 loss:0.05984912067651749 norm:8.017283107619733e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:17:16 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 8 loss:0.05977124720811844 norm:7.686656317673624e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:18:02 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 9 loss:0.059719473123550415 norm:7.26899306755513e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:18:48 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 10 loss:0.05968056246638298 norm:7.097641355358064e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:19:34 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 11 loss:0.05965292453765869 norm:7.038289186311886e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:20:19 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 12 loss:0.05962229147553444 norm:6.793508509872481e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:21:05 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 13 loss:0.05960789695382118 norm:6.751415639882907e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:21:51 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 14 loss:0.05958309769630432 norm:6.823263538535684e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:22:36 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 15 loss:0.059553444385528564 norm:6.444648170145229e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:23:22 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 16 loss:0.05954764038324356 norm:6.725130515405908e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:24:08 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 17 loss:0.05954515561461449 norm:6.790430779801682e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:24:54 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 18 loss:0.05954738333821297 norm:6.736363138770685e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:25:39 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 19 loss:0.05954616516828537 norm:6.70713052386418e-05 max memory_allocated 29274.00048828125 
[2025-02-28 19:25:53 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-02-28 19:26:42 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 0 loss:0.07006029784679413 norm:0.0005000091041438282 max memory_allocated 29274.18798828125 
[2025-02-28 19:27:28 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 1 loss:0.06843181699514389 norm:0.00025994773022830486 max memory_allocated 29274.18798828125 
[2025-02-28 19:28:13 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 2 loss:0.06756611168384552 norm:0.00016648242308292538 max memory_allocated 29274.18798828125 
[2025-02-28 19:28:59 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 3 loss:0.06691189110279083 norm:0.00012790694017894566 max memory_allocated 29274.18798828125 
[2025-02-28 19:29:44 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 4 loss:0.06626883894205093 norm:0.00010804954945342615 max memory_allocated 29274.18798828125 
[2025-02-28 19:30:30 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 5 loss:0.0657312422990799 norm:9.475803381064907e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:31:16 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 6 loss:0.06548374891281128 norm:8.800286741461605e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:32:01 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 7 loss:0.06536222249269485 norm:8.242810145020485e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:32:47 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 8 loss:0.06529886275529861 norm:8.06106036179699e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:33:33 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 9 loss:0.06525278091430664 norm:7.584381819469854e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:34:18 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 10 loss:0.0651966854929924 norm:7.368695514742285e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:35:04 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 11 loss:0.06515733152627945 norm:7.058442133711651e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:35:50 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 12 loss:0.06515369564294815 norm:6.817400571890175e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:36:35 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 13 loss:0.06512539833784103 norm:6.752823537681252e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:37:21 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 14 loss:0.06510996073484421 norm:6.667147681582719e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:38:07 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 15 loss:0.06508769094944 norm:6.605824455618858e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:38:53 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 16 loss:0.0650615468621254 norm:6.322497210931033e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:39:38 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 17 loss:0.06504099071025848 norm:6.253953324630857e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:40:24 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 18 loss:0.0650344043970108 norm:6.30611611995846e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:41:10 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 19 loss:0.06502173840999603 norm:6.147353997221217e-05 max memory_allocated 29274.18798828125 
[2025-02-28 19:41:23 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-02-28 19:42:12 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 0 loss:0.07547403126955032 norm:0.0004870717239100486 max memory_allocated 29274.37548828125 
[2025-02-28 19:42:58 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 1 loss:0.07417651265859604 norm:0.0002580839500296861 max memory_allocated 29274.37548828125 
[2025-02-28 19:43:44 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 2 loss:0.07337456941604614 norm:0.00016653569764457643 max memory_allocated 29274.37548828125 
[2025-02-28 19:44:30 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 3 loss:0.07271618396043777 norm:0.00012486874766182154 max memory_allocated 29274.37548828125 
[2025-02-28 19:45:15 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 4 loss:0.0720224678516388 norm:0.00010293820378137752 max memory_allocated 29274.37548828125 
[2025-02-28 19:46:01 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 5 loss:0.0714784488081932 norm:8.843268733471632e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:46:47 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 6 loss:0.07123792916536331 norm:8.336119208252057e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:47:32 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 7 loss:0.07115040719509125 norm:7.827403169358149e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:48:18 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 8 loss:0.07108909636735916 norm:7.330444350373e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:49:04 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 9 loss:0.07104609906673431 norm:6.884721369715407e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:49:49 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 10 loss:0.07101687788963318 norm:6.858188135083765e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:50:35 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 11 loss:0.07099677622318268 norm:6.630309508182108e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:51:21 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 12 loss:0.07098117470741272 norm:0.0001867912069428712 max memory_allocated 29274.37548828125 
[2025-02-28 19:52:07 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 13 loss:0.07096956670284271 norm:6.558788300026208e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:52:52 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 14 loss:0.07095891982316971 norm:6.48030181764625e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:53:38 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 15 loss:0.07094813883304596 norm:6.23635423835367e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:54:24 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 16 loss:0.07094401866197586 norm:6.320232932921499e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:55:09 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 17 loss:0.07094226032495499 norm:6.400109850801528e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:55:55 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 18 loss:0.07093924283981323 norm:6.510517414426431e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:56:41 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 19 loss:0.07093038409948349 norm:6.448375643230975e-05 max memory_allocated 29274.37548828125 
[2025-02-28 19:56:54 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-02-28 19:57:43 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 0 loss:0.08173543214797974 norm:0.00048184406477957964 max memory_allocated 29274.56298828125 
[2025-02-28 19:58:29 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 1 loss:0.08010663092136383 norm:0.00026013515889644623 max memory_allocated 29274.56298828125 
[2025-02-28 19:59:15 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 2 loss:0.07927712798118591 norm:0.0001831489207688719 max memory_allocated 29274.56298828125 
[2025-02-28 20:00:00 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 3 loss:0.07856063544750214 norm:0.00012636728934012353 max memory_allocated 29274.56298828125 
[2025-02-28 20:00:46 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 4 loss:0.07794332504272461 norm:0.00010522476804908365 max memory_allocated 29274.56298828125 
[2025-02-28 20:01:32 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 5 loss:0.07752972096204758 norm:9.28513691178523e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:02:17 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 6 loss:0.07733520120382309 norm:8.215730485972017e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:03:03 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 7 loss:0.07723137736320496 norm:7.575834024464712e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:03:49 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 8 loss:0.07717228680849075 norm:7.009937689872459e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:04:34 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 9 loss:0.07713469862937927 norm:6.771678454242647e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:05:20 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 10 loss:0.07710723578929901 norm:6.71103480271995e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:06:06 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 11 loss:0.07707329094409943 norm:6.443275924539194e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:06:51 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 12 loss:0.07705503702163696 norm:6.31723742117174e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:07:37 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 13 loss:0.07703258097171783 norm:6.140800542198122e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:08:23 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 14 loss:0.07701326161623001 norm:5.982541188132018e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:09:09 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 15 loss:0.07699763774871826 norm:5.85771631449461e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:09:54 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 16 loss:0.07699396461248398 norm:5.7588393246987835e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:10:40 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 17 loss:0.07699292153120041 norm:5.708890239475295e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:11:26 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 18 loss:0.0769834816455841 norm:5.8179830375593156e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:12:11 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 19 loss:0.07698296010494232 norm:5.77792088733986e-05 max memory_allocated 29274.56298828125 
[2025-02-28 20:12:25 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-02-28 20:13:14 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 0 loss:0.08971447497606277 norm:0.0004780218587256968 max memory_allocated 29275.75048828125 
[2025-02-28 20:14:00 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 1 loss:0.08852371573448181 norm:0.00028068479150533676 max memory_allocated 29275.75048828125 
[2025-02-28 20:14:45 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 2 loss:0.08769207447767258 norm:0.00019447488011792302 max memory_allocated 29275.75048828125 
[2025-02-28 20:15:31 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 3 loss:0.08689961582422256 norm:0.00014495370851363987 max memory_allocated 29275.75048828125 
[2025-02-28 20:16:17 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 4 loss:0.08605607599020004 norm:0.00011929101310670376 max memory_allocated 29275.75048828125 
[2025-02-28 20:17:02 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 5 loss:0.08547579497098923 norm:0.00010758185817394406 max memory_allocated 29275.75048828125 
[2025-02-28 20:17:48 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 6 loss:0.08526927977800369 norm:9.838808909989893e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:18:34 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 7 loss:0.08517234772443771 norm:9.022642916534096e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:19:19 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 8 loss:0.0851171463727951 norm:8.798040653346106e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:20:05 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 9 loss:0.08507994562387466 norm:8.623710891697556e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:20:51 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 10 loss:0.085047647356987 norm:8.139663987094536e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:21:36 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 11 loss:0.08503048866987228 norm:8.082616841420531e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:22:22 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 12 loss:0.08501120656728745 norm:8.167502528522164e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:23:08 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 13 loss:0.08499384671449661 norm:7.932210428407416e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:23:54 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 14 loss:0.08498651534318924 norm:7.505213579861447e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:24:39 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 15 loss:0.0849810540676117 norm:7.426159572787583e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:25:25 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 16 loss:0.08497555553913116 norm:7.573467155452818e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:26:11 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 17 loss:0.0849648043513298 norm:7.603753329021856e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:26:56 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 18 loss:0.0849602147936821 norm:7.551821909146383e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:27:42 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 19 loss:0.08495522290468216 norm:7.433874270645902e-05 max memory_allocated 29275.75048828125 
[2025-02-28 20:27:56 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-02-28 20:28:45 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 0 loss:0.0961032286286354 norm:0.0003209658316336572 max memory_allocated 29275.93798828125 
[2025-02-28 20:29:30 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 1 loss:0.09484322369098663 norm:0.00018997779989149421 max memory_allocated 29275.93798828125 
[2025-02-28 20:30:16 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 2 loss:0.09414520859718323 norm:0.00013851070252712816 max memory_allocated 29275.93798828125 
[2025-02-28 20:31:02 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 3 loss:0.09351691603660583 norm:0.00011236070713493973 max memory_allocated 29275.93798828125 
[2025-02-28 20:31:48 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 4 loss:0.09284799546003342 norm:9.524245979264379e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:32:33 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 5 loss:0.09242329001426697 norm:8.529007754987106e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:33:19 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 6 loss:0.09226001799106598 norm:7.86803211667575e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:34:05 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 7 loss:0.0921654999256134 norm:7.208510942291468e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:34:50 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 8 loss:0.09210164099931717 norm:6.919389852555469e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:35:36 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 9 loss:0.09206937253475189 norm:6.779255636502057e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:36:22 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 10 loss:0.09204099327325821 norm:6.469224899774417e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:37:07 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 11 loss:0.09201617538928986 norm:6.395907985279337e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:37:53 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 12 loss:0.09200174361467361 norm:6.45245163468644e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:38:39 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 13 loss:0.09198159724473953 norm:6.277308420976624e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:39:24 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 14 loss:0.09197412431240082 norm:6.0479360399767756e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:40:10 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 15 loss:0.09195654839277267 norm:5.9585581766441464e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:40:56 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 16 loss:0.09195518493652344 norm:6.0208803915884346e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:41:42 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 17 loss:0.09194105863571167 norm:5.9909812989644706e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:42:27 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 18 loss:0.09194355458021164 norm:6.0766480601159856e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:43:13 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 19 loss:0.09193626791238785 norm:5.858320218976587e-05 max memory_allocated 29275.93798828125 
[2025-02-28 20:43:26 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-02-28 20:44:16 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 0 loss:0.1069502905011177 norm:0.0005985270254313946 max memory_allocated 29276.12548828125 
[2025-02-28 20:45:01 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 1 loss:0.10496814548969269 norm:0.00037381582660600543 max memory_allocated 29276.12548828125 
[2025-02-28 20:45:47 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 2 loss:0.10382582992315292 norm:0.00028257776284590364 max memory_allocated 29276.12548828125 
[2025-02-28 20:46:33 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 3 loss:0.10298924893140793 norm:0.00023407500702887774 max memory_allocated 29276.12548828125 
[2025-02-28 20:47:18 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 4 loss:0.10220563411712646 norm:0.00019312941003590822 max memory_allocated 29276.12548828125 
[2025-02-28 20:48:04 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 5 loss:0.10175971686840057 norm:0.00018293753964826465 max memory_allocated 29276.12548828125 
[2025-02-28 20:48:50 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 6 loss:0.10157185792922974 norm:0.00017209889483638108 max memory_allocated 29276.12548828125 
[2025-02-28 20:49:35 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 7 loss:0.10144022107124329 norm:0.00016239219985436648 max memory_allocated 29276.12548828125 
[2025-02-28 20:50:21 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 8 loss:0.10137417167425156 norm:0.00016113588935695589 max memory_allocated 29276.12548828125 
[2025-02-28 20:51:07 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 9 loss:0.10130953043699265 norm:0.00016196214710362256 max memory_allocated 29276.12548828125 
[2025-02-28 20:51:52 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 10 loss:0.1012473776936531 norm:0.000157392249093391 max memory_allocated 29276.12548828125 
[2025-02-28 20:52:38 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 11 loss:0.10120419412851334 norm:0.00016423147462774068 max memory_allocated 29276.12548828125 
[2025-02-28 20:53:24 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 12 loss:0.10117625445127487 norm:0.00016533216694369912 max memory_allocated 29276.12548828125 
[2025-02-28 20:54:09 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 13 loss:0.10115464776754379 norm:0.00018328068836126477 max memory_allocated 29276.12548828125 
[2025-02-28 20:54:55 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 14 loss:0.1011153981089592 norm:0.00016759232676122338 max memory_allocated 29276.12548828125 
[2025-02-28 20:55:41 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 15 loss:0.1010986939072609 norm:0.00016828399384394288 max memory_allocated 29276.12548828125 
[2025-02-28 20:56:26 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 16 loss:0.10110558569431305 norm:0.00014898994413670152 max memory_allocated 29276.12548828125 
[2025-02-28 20:57:12 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 17 loss:0.10110095143318176 norm:0.00014087487943470478 max memory_allocated 29276.12548828125 
[2025-02-28 20:57:58 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 18 loss:0.10107790678739548 norm:0.00014085865404922515 max memory_allocated 29276.12548828125 
[2025-02-28 20:58:43 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 19 loss:0.10105910897254944 norm:0.00015015943790785968 max memory_allocated 29276.12548828125 
[2025-02-28 20:58:57 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-02-28 20:59:46 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 0 loss:0.1144203469157219 norm:0.00044020035420544446 max memory_allocated 29276.31298828125 
[2025-02-28 21:00:32 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 1 loss:0.11317653954029083 norm:0.00025180354714393616 max memory_allocated 29276.31298828125 
[2025-02-28 21:01:17 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 2 loss:0.11248140782117844 norm:0.00017526974261272699 max memory_allocated 29276.31298828125 
[2025-02-28 21:02:03 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 3 loss:0.11178835481405258 norm:0.00013822958862874657 max memory_allocated 29276.31298828125 
[2025-02-28 21:02:49 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 4 loss:0.11104770749807358 norm:0.00011521761916810647 max memory_allocated 29276.31298828125 
[2025-02-28 21:03:34 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 5 loss:0.11064789444208145 norm:0.00010016257874667645 max memory_allocated 29276.31298828125 
[2025-02-28 21:04:20 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 6 loss:0.11049340665340424 norm:8.9509594545234e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:05:06 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 7 loss:0.11041285842657089 norm:9.735985076986253e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:05:51 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 8 loss:0.11035986989736557 norm:7.966745761223137e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:06:37 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 9 loss:0.11031508445739746 norm:7.368032675003633e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:07:23 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 10 loss:0.11028320342302322 norm:7.28186423657462e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:08:08 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 11 loss:0.1102629154920578 norm:7.019466283963993e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:08:54 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 12 loss:0.11024841666221619 norm:7.125486445147544e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:09:40 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 13 loss:0.11023499816656113 norm:7.046019163681194e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:10:25 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 14 loss:0.11022117733955383 norm:6.755702634109184e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:11:11 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 15 loss:0.11021159589290619 norm:6.661121733486652e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:11:57 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 16 loss:0.11019434779882431 norm:6.519869202747941e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:12:42 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 17 loss:0.11019104719161987 norm:6.355295772664249e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:13:28 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 18 loss:0.11018279194831848 norm:6.516890425700694e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:14:14 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 19 loss:0.11017636209726334 norm:6.584132643183693e-05 max memory_allocated 29276.31298828125 
[2025-02-28 21:14:27 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-02-28 21:15:17 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 0 loss:0.12624545395374298 norm:0.0004974246839992702 max memory_allocated 29276.50048828125 
[2025-02-28 21:16:03 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 1 loss:0.12437192350625992 norm:0.0002720287593547255 max memory_allocated 29276.50048828125 
[2025-02-28 21:16:48 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 2 loss:0.12329274415969849 norm:0.00018215671298094094 max memory_allocated 29276.50048828125 
[2025-02-28 21:17:34 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 3 loss:0.12235104292631149 norm:0.00014289302635006607 max memory_allocated 29276.50048828125 
[2025-02-28 21:18:20 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 4 loss:0.1215241551399231 norm:0.00012042908929288387 max memory_allocated 29276.50048828125 
[2025-02-28 21:19:05 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 5 loss:0.1211632564663887 norm:0.00010594981722533703 max memory_allocated 29276.50048828125 
[2025-02-28 21:19:51 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 6 loss:0.12102790921926498 norm:9.623727237340063e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:20:37 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 7 loss:0.12094370275735855 norm:8.94735858310014e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:21:22 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 8 loss:0.1208815798163414 norm:9.76720912149176e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:22:08 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 9 loss:0.12084532529115677 norm:8.271051046904176e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:22:53 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 10 loss:0.12079837173223495 norm:7.957329216878861e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:23:39 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 11 loss:0.12077498435974121 norm:7.744691538391635e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:24:25 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 12 loss:0.12074936181306839 norm:7.41779658710584e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:25:10 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 13 loss:0.12073147296905518 norm:7.492014265153557e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:25:56 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 14 loss:0.12070685625076294 norm:7.352992543019354e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:26:42 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 15 loss:0.12070270627737045 norm:7.184166315710172e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:27:27 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 16 loss:0.12068556994199753 norm:7.348715735133737e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:28:13 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 17 loss:0.1206788718700409 norm:7.326705963350832e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:28:59 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 18 loss:0.1206665113568306 norm:7.247446046676487e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:29:44 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 19 loss:0.12065707892179489 norm:7.278705743374303e-05 max memory_allocated 29276.50048828125 
[2025-02-28 21:29:58 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-02-28 21:30:47 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 0 loss:0.1365664005279541 norm:0.00043957470916211605 max memory_allocated 29276.68798828125 
[2025-02-28 21:31:33 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 1 loss:0.13515645265579224 norm:0.00023339371546171606 max memory_allocated 29276.68798828125 
[2025-02-28 21:32:18 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 2 loss:0.13427934050559998 norm:0.00015856963000260293 max memory_allocated 29276.68798828125 
[2025-02-28 21:33:04 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 3 loss:0.13332347571849823 norm:0.0001241713180206716 max memory_allocated 29276.68798828125 
[2025-02-28 21:33:49 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 4 loss:0.13256484270095825 norm:0.00010600180394249037 max memory_allocated 29276.68798828125 
[2025-02-28 21:34:35 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 5 loss:0.13228739798069 norm:9.741786925587803e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:35:20 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 6 loss:0.1321781873703003 norm:8.854641055222601e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:36:06 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 7 loss:0.1321207880973816 norm:8.335277379956096e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:36:52 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 8 loss:0.13208237290382385 norm:8.139494457282126e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:37:37 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 9 loss:0.132039412856102 norm:8.117409015540034e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:38:23 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 10 loss:0.13200825452804565 norm:7.930610445328057e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:39:08 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 11 loss:0.1319904327392578 norm:7.668494072277099e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:39:54 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 12 loss:0.13197638094425201 norm:7.573085167678073e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:40:40 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 13 loss:0.1319560706615448 norm:7.438993634423241e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:41:25 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 14 loss:0.13194692134857178 norm:7.375011045951396e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:42:11 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 15 loss:0.13193336129188538 norm:7.452560384990647e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:42:57 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 16 loss:0.13192471861839294 norm:7.364607154158875e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:43:42 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 17 loss:0.13192564249038696 norm:7.740367436781526e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:44:28 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 18 loss:0.13192670047283173 norm:7.890335837146267e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:45:13 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 19 loss:0.1319194734096527 norm:7.782495231367648e-05 max memory_allocated 29276.68798828125 
[2025-02-28 21:45:27 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 32 ===
[2025-02-28 21:46:16 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 0 loss:0.15063010156154633 norm:0.0005478655803017318 max memory_allocated 29276.87548828125 
[2025-02-28 21:47:01 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 1 loss:0.14869236946105957 norm:0.00029606273164972663 max memory_allocated 29276.87548828125 
[2025-02-28 21:47:47 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 2 loss:0.14750675857067108 norm:0.00020443496759980917 max memory_allocated 29276.87548828125 
[2025-02-28 21:48:33 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 3 loss:0.14631135761737823 norm:0.0001545600243844092 max memory_allocated 29276.87548828125 
[2025-02-28 21:49:18 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 4 loss:0.14547477662563324 norm:0.00012985209468752146 max memory_allocated 29276.87548828125 
[2025-02-28 21:50:04 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 5 loss:0.14517498016357422 norm:0.00011686301877489313 max memory_allocated 29276.87548828125 
[2025-02-28 21:50:49 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 6 loss:0.14503827691078186 norm:0.00010721060971263796 max memory_allocated 29276.87548828125 
[2025-02-28 21:51:35 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 7 loss:0.1449279636144638 norm:9.992024570237845e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:52:20 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 8 loss:0.14486908912658691 norm:9.591775597073138e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:53:06 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 9 loss:0.14483344554901123 norm:9.22089529922232e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:53:52 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 10 loss:0.14480048418045044 norm:8.923260611481965e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:54:37 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 11 loss:0.14475446939468384 norm:8.44823953229934e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:55:23 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 12 loss:0.144729346036911 norm:8.252052793977782e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:56:08 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 13 loss:0.14471277594566345 norm:8.118271216517314e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:56:54 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 14 loss:0.14469854533672333 norm:8.007010183064267e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:57:39 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 15 loss:0.14467720687389374 norm:7.930142601253465e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:58:25 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 16 loss:0.14466804265975952 norm:7.856557203922421e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:59:11 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 17 loss:0.1446806937456131 norm:8.09770863270387e-05 max memory_allocated 29276.87548828125 
[2025-02-28 21:59:56 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 18 loss:0.14467239379882812 norm:8.052705379668623e-05 max memory_allocated 29276.87548828125 
[2025-02-28 22:00:42 root] (abq_llm_calib_config2.py 396): INFO layer 32 iter 19 loss:0.14466074109077454 norm:7.951184670673683e-05 max memory_allocated 29276.87548828125 
[2025-02-28 22:00:55 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 33 ===
[2025-02-28 22:01:44 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 0 loss:0.16299039125442505 norm:0.0004867113020736724 max memory_allocated 29277.06298828125 
[2025-02-28 22:02:30 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 1 loss:0.16118738055229187 norm:0.000268420233624056 max memory_allocated 29277.06298828125 
[2025-02-28 22:03:16 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 2 loss:0.16008056700229645 norm:0.00018549300148151815 max memory_allocated 29277.06298828125 
[2025-02-28 22:04:01 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 3 loss:0.15896880626678467 norm:0.00013860811304766685 max memory_allocated 29277.06298828125 
[2025-02-28 22:04:47 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 4 loss:0.15817983448505402 norm:0.00011227926006540656 max memory_allocated 29277.06298828125 
[2025-02-28 22:05:33 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 5 loss:0.15791238844394684 norm:9.968866652343422e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:06:18 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 6 loss:0.1577993780374527 norm:9.429470082977787e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:07:04 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 7 loss:0.1577291190624237 norm:8.85355839272961e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:07:50 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 8 loss:0.15767475962638855 norm:8.939692634157836e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:08:35 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 9 loss:0.157631054520607 norm:8.450498717138544e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:09:21 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 10 loss:0.1576119065284729 norm:8.205424819607288e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:10:06 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 11 loss:0.15759065747261047 norm:7.927812839625403e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:10:52 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 12 loss:0.15756289660930634 norm:7.800411549396813e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:11:38 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 13 loss:0.15754789113998413 norm:7.804889173712581e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:12:23 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 14 loss:0.15753674507141113 norm:7.711846410529688e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:13:09 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 15 loss:0.1575234979391098 norm:7.807269867043942e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:13:55 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 16 loss:0.15752628445625305 norm:7.84762596595101e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:14:40 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 17 loss:0.1575198471546173 norm:7.772511889925227e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:15:26 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 18 loss:0.15750479698181152 norm:7.700855348957703e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:16:12 root] (abq_llm_calib_config2.py 396): INFO layer 33 iter 19 loss:0.1574891358613968 norm:7.528980495408177e-05 max memory_allocated 29277.06298828125 
[2025-02-28 22:16:25 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 34 ===
[2025-02-28 22:17:14 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 0 loss:0.1810002624988556 norm:0.0005475957877933979 max memory_allocated 29277.25048828125 
[2025-02-28 22:18:00 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 1 loss:0.1785251945257187 norm:0.00028760178247466683 max memory_allocated 29277.25048828125 
[2025-02-28 22:18:45 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 2 loss:0.17696677148342133 norm:0.00019735128444153816 max memory_allocated 29277.25048828125 
[2025-02-28 22:19:31 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 3 loss:0.17565281689167023 norm:0.00016033828433137387 max memory_allocated 29277.25048828125 
[2025-02-28 22:20:17 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 4 loss:0.17486132681369781 norm:0.0001383573398925364 max memory_allocated 29277.25048828125 
[2025-02-28 22:21:02 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 5 loss:0.1745777279138565 norm:0.00012353333295322955 max memory_allocated 29277.25048828125 
[2025-02-28 22:21:48 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 6 loss:0.174451544880867 norm:0.00011866360728163272 max memory_allocated 29277.25048828125 
[2025-02-28 22:22:34 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 7 loss:0.17437294125556946 norm:0.00012104035704396665 max memory_allocated 29277.25048828125 
[2025-02-28 22:23:19 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 8 loss:0.17429199814796448 norm:0.00011101416748715565 max memory_allocated 29277.25048828125 
[2025-02-28 22:24:05 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 9 loss:0.17421077191829681 norm:0.00010120345541508868 max memory_allocated 29277.25048828125 
[2025-02-28 22:24:51 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 10 loss:0.17417743802070618 norm:9.914110705722123e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:25:36 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 11 loss:0.17414404451847076 norm:9.624544327380136e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:26:22 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 12 loss:0.1741175651550293 norm:9.263833635486662e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:27:07 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 13 loss:0.17409981787204742 norm:9.149673860520124e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:27:53 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 14 loss:0.1740851253271103 norm:8.967245230451226e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:28:39 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 15 loss:0.17406615614891052 norm:8.980440179584548e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:29:24 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 16 loss:0.17405082285404205 norm:8.369776332983747e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:30:10 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 17 loss:0.17405195534229279 norm:8.469784370390698e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:30:56 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 18 loss:0.1740415245294571 norm:8.503080607624725e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:31:41 root] (abq_llm_calib_config2.py 396): INFO layer 34 iter 19 loss:0.1740366518497467 norm:8.582733426010236e-05 max memory_allocated 29277.25048828125 
[2025-02-28 22:31:55 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 35 ===
[2025-02-28 22:32:44 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 0 loss:0.19900543987751007 norm:0.0009816838428378105 max memory_allocated 29277.43798828125 
[2025-02-28 22:33:29 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 1 loss:0.19632962346076965 norm:0.0005143298185430467 max memory_allocated 29277.43798828125 
[2025-02-28 22:34:15 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 2 loss:0.1946244239807129 norm:0.0003244198742322624 max memory_allocated 29277.43798828125 
[2025-02-28 22:35:01 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 3 loss:0.19309237599372864 norm:0.0002311590505996719 max memory_allocated 29277.43798828125 
[2025-02-28 22:35:46 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 4 loss:0.19228899478912354 norm:0.00018024188466370106 max memory_allocated 29277.43798828125 
[2025-02-28 22:36:32 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 5 loss:0.19200992584228516 norm:0.00015105208149179816 max memory_allocated 29277.43798828125 
[2025-02-28 22:37:18 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 6 loss:0.19185633957386017 norm:0.0001314449036726728 max memory_allocated 29277.43798828125 
[2025-02-28 22:38:03 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 7 loss:0.1917790174484253 norm:0.00011921091936528683 max memory_allocated 29277.43798828125 
[2025-02-28 22:38:49 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 8 loss:0.19171646237373352 norm:0.00011032787733711302 max memory_allocated 29277.43798828125 
[2025-02-28 22:39:35 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 9 loss:0.1916748434305191 norm:0.00010407273657619953 max memory_allocated 29277.43798828125 
[2025-02-28 22:40:20 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 10 loss:0.19162620604038239 norm:0.0001009123443509452 max memory_allocated 29277.43798828125 
[2025-02-28 22:41:06 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 11 loss:0.19158218801021576 norm:9.662365482654423e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:41:51 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 12 loss:0.19156727194786072 norm:9.614000009605661e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:42:37 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 13 loss:0.1915457546710968 norm:9.397075336892158e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:43:23 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 14 loss:0.19153207540512085 norm:9.431478247279301e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:44:08 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 15 loss:0.19152328372001648 norm:9.308381413575262e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:44:54 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 16 loss:0.1915035992860794 norm:9.115862485487014e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:45:39 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 17 loss:0.19148629903793335 norm:9.154435974778607e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:46:25 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 18 loss:0.19147677719593048 norm:9.099605813389644e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:47:11 root] (abq_llm_calib_config2.py 396): INFO layer 35 iter 19 loss:0.19147473573684692 norm:9.104155469685793e-05 max memory_allocated 29277.43798828125 
[2025-02-28 22:47:24 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 36 ===
[2025-02-28 22:47:28 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 22:48:14 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 0 loss:0.22536934912204742 norm:0.006799634546041489 max memory_allocated 29277.77001953125 
[2025-02-28 22:48:59 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 1 loss:0.2204979509115219 norm:0.005157749634236097 max memory_allocated 29277.77001953125 
[2025-02-28 22:49:45 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 2 loss:0.2176489382982254 norm:0.004023040644824505 max memory_allocated 29277.77001953125 
[2025-02-28 22:50:31 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 3 loss:0.21546721458435059 norm:0.0031654140911996365 max memory_allocated 29277.77001953125 
[2025-02-28 22:51:17 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 4 loss:0.21449249982833862 norm:0.002577789593487978 max memory_allocated 29277.77001953125 
[2025-02-28 22:52:02 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 5 loss:0.2140733301639557 norm:0.002147142542526126 max memory_allocated 29277.77001953125 
[2025-02-28 22:52:48 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 6 loss:0.21384182572364807 norm:0.0019897515885531902 max memory_allocated 29277.77001953125 
[2025-02-28 22:53:34 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 7 loss:0.2136632353067398 norm:0.0018399381078779697 max memory_allocated 29277.77001953125 
[2025-02-28 22:54:20 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 8 loss:0.21351288259029388 norm:0.0017300581093877554 max memory_allocated 29277.77001953125 
[2025-02-28 22:55:06 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 9 loss:0.21339154243469238 norm:0.001660941168665886 max memory_allocated 29277.77001953125 
[2025-02-28 22:55:51 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 10 loss:0.21338386833667755 norm:0.0016729170456528664 max memory_allocated 29277.77001953125 
[2025-02-28 22:56:37 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 11 loss:0.21351714432239532 norm:0.0015163462376222014 max memory_allocated 29277.77001953125 
[2025-02-28 22:57:23 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 12 loss:0.213356614112854 norm:0.0015734127955511212 max memory_allocated 29277.77001953125 
[2025-02-28 22:58:09 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 13 loss:0.21331152319908142 norm:0.0017631680238991976 max memory_allocated 29277.77001953125 
[2025-02-28 22:58:55 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 14 loss:0.21332958340644836 norm:0.0013662413693964481 max memory_allocated 29277.77001953125 
[2025-02-28 22:59:40 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 15 loss:0.21317578852176666 norm:0.0014961827546358109 max memory_allocated 29277.77001953125 
[2025-02-28 23:00:26 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 16 loss:0.21312960982322693 norm:0.0014236340066418052 max memory_allocated 29277.77001953125 
[2025-02-28 23:01:12 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 17 loss:0.21313582360744476 norm:0.0014662949834018946 max memory_allocated 29277.77001953125 
[2025-02-28 23:01:58 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 18 loss:0.21313054859638214 norm:0.001399173284880817 max memory_allocated 29277.77001953125 
[2025-02-28 23:02:44 root] (abq_llm_calib_config2.py 396): INFO layer 36 iter 19 loss:0.2131073772907257 norm:0.0014278588350862265 max memory_allocated 29277.77001953125 
[2025-02-28 23:02:57 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 37 ===
[2025-02-28 23:03:01 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 23:03:46 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 0 loss:0.2531301975250244 norm:0.006884892005473375 max memory_allocated 29277.95751953125 
[2025-02-28 23:04:32 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 1 loss:0.24704384803771973 norm:0.005604843609035015 max memory_allocated 29277.95751953125 
[2025-02-28 23:05:18 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 2 loss:0.24283848702907562 norm:0.004198234528303146 max memory_allocated 29277.95751953125 
[2025-02-28 23:06:04 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 3 loss:0.2401638776063919 norm:0.0033207423985004425 max memory_allocated 29277.95751953125 
[2025-02-28 23:06:49 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 4 loss:0.23901286721229553 norm:0.0027563171461224556 max memory_allocated 29277.95751953125 
[2025-02-28 23:07:35 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 5 loss:0.2385045439004898 norm:0.0023102143313735723 max memory_allocated 29277.95751953125 
[2025-02-28 23:08:21 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 6 loss:0.23815365135669708 norm:0.0020077216904610395 max memory_allocated 29277.95751953125 
[2025-02-28 23:09:07 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 7 loss:0.2379416823387146 norm:0.0018975376151502132 max memory_allocated 29277.95751953125 
[2025-02-28 23:09:53 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 8 loss:0.23780189454555511 norm:0.0018880469724535942 max memory_allocated 29277.95751953125 
[2025-02-28 23:10:39 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 9 loss:0.23771488666534424 norm:0.0017748225945979357 max memory_allocated 29277.95751953125 
[2025-02-28 23:11:24 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 10 loss:0.23761668801307678 norm:0.0018071354134008288 max memory_allocated 29277.95751953125 
[2025-02-28 23:12:10 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 11 loss:0.23756928741931915 norm:0.001757616177201271 max memory_allocated 29277.95751953125 
[2025-02-28 23:12:56 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 12 loss:0.2375297248363495 norm:0.0017895643832162023 max memory_allocated 29277.95751953125 
[2025-02-28 23:13:42 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 13 loss:0.23747113347053528 norm:0.0017293437849730253 max memory_allocated 29277.95751953125 
[2025-02-28 23:14:27 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 14 loss:0.23743683099746704 norm:0.001691913465037942 max memory_allocated 29277.95751953125 
[2025-02-28 23:15:13 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 15 loss:0.23743727803230286 norm:0.001748279551975429 max memory_allocated 29277.95751953125 
[2025-02-28 23:15:59 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 16 loss:0.23753821849822998 norm:0.001662829308770597 max memory_allocated 29277.95751953125 
[2025-02-28 23:16:45 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 17 loss:0.23774346709251404 norm:0.0018457128899171948 max memory_allocated 29277.95751953125 
[2025-02-28 23:17:31 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 18 loss:0.23753485083580017 norm:0.0015581329353153706 max memory_allocated 29277.95751953125 
[2025-02-28 23:18:16 root] (abq_llm_calib_config2.py 396): INFO layer 37 iter 19 loss:0.23720338940620422 norm:0.001451658084988594 max memory_allocated 29277.95751953125 
[2025-02-28 23:18:30 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 38 ===
[2025-02-28 23:18:33 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 23:19:19 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 0 loss:0.30275601148605347 norm:0.005735477432608604 max memory_allocated 29278.14501953125 
[2025-02-28 23:20:05 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 1 loss:0.2910744547843933 norm:0.0034813617821782827 max memory_allocated 29278.14501953125 
[2025-02-28 23:20:51 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 2 loss:0.2834547758102417 norm:0.003618489019572735 max memory_allocated 29278.14501953125 
[2025-02-28 23:21:36 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 3 loss:0.27991437911987305 norm:0.0034999437630176544 max memory_allocated 29278.14501953125 
[2025-02-28 23:22:22 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 4 loss:0.2785990834236145 norm:0.003332765307277441 max memory_allocated 29278.14501953125 
[2025-02-28 23:23:08 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 5 loss:0.2778819799423218 norm:0.003161391941830516 max memory_allocated 29278.14501953125 
[2025-02-28 23:23:54 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 6 loss:0.27736830711364746 norm:0.0031434614211320877 max memory_allocated 29278.14501953125 
[2025-02-28 23:24:40 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 7 loss:0.27699801325798035 norm:0.003025130368769169 max memory_allocated 29278.14501953125 
[2025-02-28 23:25:25 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 8 loss:0.2767373025417328 norm:0.0029635268729180098 max memory_allocated 29278.14501953125 
[2025-02-28 23:26:11 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 9 loss:0.27649012207984924 norm:0.0027981558814644814 max memory_allocated 29278.14501953125 
[2025-02-28 23:26:57 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 10 loss:0.27624720335006714 norm:0.0027663561049848795 max memory_allocated 29278.14501953125 
[2025-02-28 23:27:43 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 11 loss:0.2760066092014313 norm:0.0027643339708447456 max memory_allocated 29278.14501953125 
[2025-02-28 23:28:29 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 12 loss:0.27586629986763 norm:0.002658373210579157 max memory_allocated 29278.14501953125 
[2025-02-28 23:29:14 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 13 loss:0.27578508853912354 norm:0.00271979090757668 max memory_allocated 29278.14501953125 
[2025-02-28 23:30:00 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 14 loss:0.2758549451828003 norm:0.0029964717105031013 max memory_allocated 29278.14501953125 
[2025-02-28 23:30:46 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 15 loss:0.27596449851989746 norm:0.003154133213683963 max memory_allocated 29278.14501953125 
[2025-02-28 23:31:32 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 16 loss:0.27591070532798767 norm:0.0032082251273095608 max memory_allocated 29278.14501953125 
[2025-02-28 23:32:18 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 17 loss:0.27578526735305786 norm:0.0029890553560107946 max memory_allocated 29278.14501953125 
[2025-02-28 23:33:03 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 18 loss:0.2756073474884033 norm:0.002865722868591547 max memory_allocated 29278.14501953125 
[2025-02-28 23:33:49 root] (abq_llm_calib_config2.py 396): INFO layer 38 iter 19 loss:0.2754611670970917 norm:0.0026529997121542692 max memory_allocated 29278.14501953125 
[2025-02-28 23:34:03 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 39 ===
[2025-02-28 23:34:06 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 23:34:52 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 0 loss:0.41149336099624634 norm:0.027682079002261162 max memory_allocated 29278.33251953125 
[2025-02-28 23:35:38 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 1 loss:0.38997361063957214 norm:0.018455300480127335 max memory_allocated 29278.33251953125 
[2025-02-28 23:36:23 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 2 loss:0.37930625677108765 norm:0.01170764584094286 max memory_allocated 29278.33251953125 
[2025-02-28 23:37:09 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 3 loss:0.3735139071941376 norm:0.009335095062851906 max memory_allocated 29278.33251953125 
[2025-02-28 23:37:55 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 4 loss:0.3705311715602875 norm:0.009997161105275154 max memory_allocated 29278.33251953125 
[2025-02-28 23:38:41 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 5 loss:0.36892443895339966 norm:0.009725771844387054 max memory_allocated 29278.33251953125 
[2025-02-28 23:39:27 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 6 loss:0.3677515983581543 norm:0.009959445334970951 max memory_allocated 29278.33251953125 
[2025-02-28 23:40:12 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 7 loss:0.36687329411506653 norm:0.009299937635660172 max memory_allocated 29278.33251953125 
[2025-02-28 23:40:58 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 8 loss:0.36640268564224243 norm:0.009210629388689995 max memory_allocated 29278.33251953125 
[2025-02-28 23:41:44 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 9 loss:0.36579716205596924 norm:0.00907597504556179 max memory_allocated 29278.33251953125 
[2025-02-28 23:42:30 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 10 loss:0.3656128942966461 norm:0.008885686285793781 max memory_allocated 29278.33251953125 
[2025-02-28 23:43:15 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 11 loss:0.3654974699020386 norm:0.008845996111631393 max memory_allocated 29278.33251953125 
[2025-02-28 23:44:01 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 12 loss:0.36450713872909546 norm:0.008642137050628662 max memory_allocated 29278.33251953125 
[2025-02-28 23:44:47 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 13 loss:0.36458131670951843 norm:0.008250032551586628 max memory_allocated 29278.33251953125 
[2025-02-28 23:45:33 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 14 loss:0.3645762801170349 norm:0.007701730355620384 max memory_allocated 29278.33251953125 
[2025-02-28 23:46:19 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 15 loss:0.36375176906585693 norm:0.008264224976301193 max memory_allocated 29278.33251953125 
[2025-02-28 23:47:04 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 16 loss:0.36357372999191284 norm:0.007524594664573669 max memory_allocated 29278.33251953125 
[2025-02-28 23:47:50 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 17 loss:0.3635199964046478 norm:0.0071383994072675705 max memory_allocated 29278.33251953125 
[2025-02-28 23:48:36 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 18 loss:0.36322057247161865 norm:0.007257005665451288 max memory_allocated 29278.33251953125 
[2025-02-28 23:49:22 root] (abq_llm_calib_config2.py 396): INFO layer 39 iter 19 loss:0.36325758695602417 norm:0.006991970352828503 max memory_allocated 29278.33251953125 
[2025-02-28 23:49:35 root] (main_calib_config2.py 380): INFO 37249.07187771797
[2025-02-28 23:49:50 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-28 23:51:37 root] (main_calib_config2.py 159): INFO wikitext2 : 5.026995658874512
[2025-02-28 23:51:37 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-28 23:54:22 root] (main_calib_config2.py 159): INFO c4 : 6.662954330444336
[2025-03-01 01:52:25 root] (main_calib_config2.py 170): INFO {'wikitext2': 5.026995658874512, 'c4': 6.662954330444336, 'results': {'boolq': {'acc': 0.6868501529051988, 'acc_stderr': 0.008111471401410516}, 'arc_challenge': {'acc': 0.4377133105802048, 'acc_stderr': 0.01449757388110829, 'acc_norm': 0.4300341296928328, 'acc_norm_stderr': 0.014467631559138}, 'arc_easy': {'acc': 0.7272727272727273, 'acc_stderr': 0.009138630726364233, 'acc_norm': 0.5803872053872053, 'acc_norm_stderr': 0.01012631584089154}, 'piqa': {'acc': 0.7834602829162133, 'acc_stderr': 0.009609984714384609, 'acc_norm': 0.7889009793253536, 'acc_norm_stderr': 0.00952137737873415}, 'winogrande': {'acc': 0.6906077348066298, 'acc_stderr': 0.012991329330822999}, 'hellaswag': {'acc': 0.5907189802828122, 'acc_stderr': 0.004906962980328296, 'acc_norm': 0.7555267874925313, 'acc_norm_stderr': 0.004288960926085642}}, 'versions': {'boolq': 1, 'arc_challenge': 0, 'arc_easy': 0, 'piqa': 0, 'winogrande': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
