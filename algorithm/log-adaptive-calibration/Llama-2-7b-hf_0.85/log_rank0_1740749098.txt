[2025-02-28 13:24:58 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/Llama-2-7b-hf_0.85', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.85.pkl', scale_calibration=True, compensation_calibration=True)
[2025-02-28 13:25:05 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-02-28 13:25:05 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-28 13:25:05 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-02-28 13:25:05 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.85.pkl
[2025-02-28 13:25:07 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-02-28 13:25:10 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:25:40 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 0 loss:0.0035218349657952785 norm:0.004006671719253063 max memory_allocated 22562.10693359375 
[2025-02-28 13:26:11 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 1 loss:0.0019654131028801203 norm:0.001991438213735819 max memory_allocated 22562.10693359375 
[2025-02-28 13:26:42 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 2 loss:0.0016221757978200912 norm:0.0018112706020474434 max memory_allocated 22562.10693359375 
[2025-02-28 13:27:13 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 3 loss:0.0014865536941215396 norm:0.001664513722062111 max memory_allocated 22562.10693359375 
[2025-02-28 13:27:43 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 4 loss:0.0014169764472171664 norm:0.001602805801667273 max memory_allocated 22562.10693359375 
[2025-02-28 13:28:14 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 5 loss:0.001370718120597303 norm:0.0014180635334923863 max memory_allocated 22562.10693359375 
[2025-02-28 13:28:45 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 6 loss:0.0013530957512557507 norm:0.0013730477076023817 max memory_allocated 22562.10693359375 
[2025-02-28 13:29:15 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 7 loss:0.001332018873654306 norm:0.001346169738098979 max memory_allocated 22562.10693359375 
[2025-02-28 13:29:46 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 8 loss:0.001317471032962203 norm:0.0012872237712144852 max memory_allocated 22562.10693359375 
[2025-02-28 13:30:17 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 9 loss:0.0012964793713763356 norm:0.0012176940217614174 max memory_allocated 22562.10693359375 
[2025-02-28 13:30:48 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 10 loss:0.0012971892720088363 norm:0.0012377017410472035 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:18 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 11 loss:0.001276692608371377 norm:0.0011462015099823475 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:49 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 12 loss:0.001276275608688593 norm:0.0011349674314260483 max memory_allocated 22562.10693359375 
[2025-02-28 13:32:20 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 13 loss:0.001262815436348319 norm:0.0010426362277939916 max memory_allocated 22562.10693359375 
[2025-02-28 13:32:51 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 14 loss:0.0012451051734387875 norm:0.0009525489876978099 max memory_allocated 22562.10693359375 
[2025-02-28 13:33:22 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 15 loss:0.0012374699581414461 norm:0.0008852104074321687 max memory_allocated 22562.10693359375 
[2025-02-28 13:33:52 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 16 loss:0.0012162568746134639 norm:0.0008120815036818385 max memory_allocated 22562.10693359375 
[2025-02-28 13:34:23 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 17 loss:0.001202992512844503 norm:0.0007427367963828146 max memory_allocated 22562.10693359375 
[2025-02-28 13:34:54 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 18 loss:0.0012038015993312001 norm:0.0007431649137288332 max memory_allocated 22562.10693359375 
[2025-02-28 13:35:25 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 19 loss:0.0012034302344545722 norm:0.0007081154617480934 max memory_allocated 22562.10693359375 
[2025-02-28 13:35:33 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-02-28 13:35:35 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:36:06 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 0 loss:0.025291169062256813 norm:0.012872222810983658 max memory_allocated 22562.27880859375 
[2025-02-28 13:36:37 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 1 loss:0.01732618920505047 norm:0.014141872525215149 max memory_allocated 22562.27880859375 
[2025-02-28 13:37:08 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 2 loss:0.014674167148768902 norm:0.017270764335989952 max memory_allocated 22562.27880859375 
[2025-02-28 13:37:38 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 3 loss:0.011363716796040535 norm:0.008977784775197506 max memory_allocated 22562.27880859375 
[2025-02-28 13:38:09 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 4 loss:0.010915931314229965 norm:0.007898402400314808 max memory_allocated 22562.27880859375 
[2025-02-28 13:38:40 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 5 loss:0.010677610523998737 norm:0.00798666663467884 max memory_allocated 22562.27880859375 
[2025-02-28 13:39:11 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 6 loss:0.010584312491118908 norm:0.007431455887854099 max memory_allocated 22562.27880859375 
[2025-02-28 13:39:41 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 7 loss:0.011564482003450394 norm:0.007685820572078228 max memory_allocated 22562.27880859375 
[2025-02-28 13:40:12 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 8 loss:0.010353930294513702 norm:0.00640275189653039 max memory_allocated 22562.27880859375 
[2025-02-28 13:40:43 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 9 loss:0.010270441882312298 norm:0.006354127079248428 max memory_allocated 22562.27880859375 
[2025-02-28 13:41:14 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 10 loss:0.010348387993872166 norm:0.0064800940454006195 max memory_allocated 22562.27880859375 
[2025-02-28 13:41:44 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 11 loss:0.01154389325529337 norm:0.006834479980170727 max memory_allocated 22562.27880859375 
[2025-02-28 13:42:15 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 12 loss:0.011766470037400723 norm:0.007722807582467794 max memory_allocated 22562.27880859375 
[2025-02-28 13:42:46 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 13 loss:0.010324993170797825 norm:0.006863732822239399 max memory_allocated 22562.27880859375 
[2025-02-28 13:43:16 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 14 loss:0.010164127685129642 norm:0.006472045090049505 max memory_allocated 22562.27880859375 
[2025-02-28 13:43:47 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 15 loss:0.010051644407212734 norm:0.006031143479049206 max memory_allocated 22562.27880859375 
[2025-02-28 13:44:18 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 16 loss:0.010086816735565662 norm:0.006006605923175812 max memory_allocated 22562.27880859375 
[2025-02-28 13:44:49 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 17 loss:0.01267164759337902 norm:0.007176036946475506 max memory_allocated 22562.27880859375 
[2025-02-28 13:45:19 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 18 loss:0.010721452534198761 norm:0.005844800733029842 max memory_allocated 22562.27880859375 
[2025-02-28 13:45:50 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 19 loss:0.01047168206423521 norm:0.005751926451921463 max memory_allocated 22562.27880859375 
[2025-02-28 13:45:58 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-02-28 13:46:01 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:46:31 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 0 loss:0.015394025482237339 norm:0.006574290804564953 max memory_allocated 22562.45068359375 
[2025-02-28 13:47:02 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 1 loss:0.011940574273467064 norm:0.004822069779038429 max memory_allocated 22562.45068359375 
[2025-02-28 13:47:33 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 2 loss:0.011034338735044003 norm:0.0036187004297971725 max memory_allocated 22562.45068359375 
[2025-02-28 13:48:04 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 3 loss:0.010515730828046799 norm:0.002957507036626339 max memory_allocated 22562.45068359375 
[2025-02-28 13:48:34 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 4 loss:0.010050300508737564 norm:0.0024504330940544605 max memory_allocated 22562.45068359375 
[2025-02-28 13:49:05 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 5 loss:0.009660475887358189 norm:0.002013513818383217 max memory_allocated 22562.45068359375 
[2025-02-28 13:49:36 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 6 loss:0.009449400939047337 norm:0.0016664956929162145 max memory_allocated 22562.45068359375 
[2025-02-28 13:50:07 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 7 loss:0.009342568926513195 norm:0.0013669207692146301 max memory_allocated 22562.45068359375 
[2025-02-28 13:50:37 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 8 loss:0.00927826389670372 norm:0.0011236559366807342 max memory_allocated 22562.45068359375 
[2025-02-28 13:51:08 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 9 loss:0.009241219609975815 norm:0.0009176319581456482 max memory_allocated 22562.45068359375 
[2025-02-28 13:51:39 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 10 loss:0.009231711737811565 norm:0.0008584755705669522 max memory_allocated 22562.45068359375 
[2025-02-28 13:52:09 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 11 loss:0.0092365313321352 norm:0.0009534254786558449 max memory_allocated 22562.45068359375 
[2025-02-28 13:52:40 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 12 loss:0.009262876585125923 norm:0.0009421003633178771 max memory_allocated 22562.45068359375 
[2025-02-28 13:53:11 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 13 loss:0.009344200603663921 norm:0.0009510297677479684 max memory_allocated 22562.45068359375 
[2025-02-28 13:53:42 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 14 loss:0.009771137498319149 norm:0.001569257816299796 max memory_allocated 22562.45068359375 
[2025-02-28 13:54:12 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 15 loss:0.009295034222304821 norm:0.0008504549041390419 max memory_allocated 22562.45068359375 
[2025-02-28 13:54:43 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 16 loss:0.009183398447930813 norm:0.0006002255249768496 max memory_allocated 22562.45068359375 
[2025-02-28 13:55:14 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 17 loss:0.009166418574750423 norm:0.00044828554382547736 max memory_allocated 22562.45068359375 
[2025-02-28 13:55:45 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 18 loss:0.009148628450930119 norm:0.0003992134879808873 max memory_allocated 22562.45068359375 
[2025-02-28 13:56:15 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 19 loss:0.00917606707662344 norm:0.00045829275040887296 max memory_allocated 22562.45068359375 
[2025-02-28 13:56:23 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-02-28 13:56:57 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 0 loss:0.017154529690742493 norm:0.0012080851010978222 max memory_allocated 22562.50732421875 
[2025-02-28 13:57:27 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 1 loss:0.014501934871077538 norm:0.0005216042627580464 max memory_allocated 22562.50732421875 
[2025-02-28 13:57:58 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 2 loss:0.013603724539279938 norm:0.0003151303972117603 max memory_allocated 22562.50732421875 
[2025-02-28 13:58:29 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 3 loss:0.012942993082106113 norm:0.001052815467119217 max memory_allocated 22562.50732421875 
[2025-02-28 13:58:59 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 4 loss:0.012389561161398888 norm:0.00014114381338004023 max memory_allocated 22562.50732421875 
[2025-02-28 13:59:30 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 5 loss:0.012120716273784637 norm:0.00013716169632971287 max memory_allocated 22562.50732421875 
[2025-02-28 14:00:00 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 6 loss:0.012047983705997467 norm:0.0001257586554856971 max memory_allocated 22562.50732421875 
[2025-02-28 14:00:31 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 7 loss:0.012021742761135101 norm:0.00011018960503861308 max memory_allocated 22562.50732421875 
[2025-02-28 14:01:02 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 8 loss:0.012005547061562538 norm:0.00011537715181475505 max memory_allocated 22562.50732421875 
[2025-02-28 14:01:32 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 9 loss:0.011990771628916264 norm:0.0001131476674345322 max memory_allocated 22562.50732421875 
[2025-02-28 14:02:03 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 10 loss:0.011984938755631447 norm:0.00010652567289071158 max memory_allocated 22562.50732421875 
[2025-02-28 14:02:34 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 11 loss:0.011993670836091042 norm:0.00010502389341127127 max memory_allocated 22562.50732421875 
[2025-02-28 14:03:04 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 12 loss:0.011991756968200207 norm:9.745187708176672e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:03:35 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 13 loss:0.011967619881033897 norm:9.549100650474429e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:04:05 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 14 loss:0.01197109092026949 norm:9.867958578979596e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:04:36 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 15 loss:0.011963989585638046 norm:9.224123641615734e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:07 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 16 loss:0.011958179995417595 norm:0.00010004777868743986 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:37 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 17 loss:0.011960502713918686 norm:0.00010372222459409386 max memory_allocated 22562.50732421875 
[2025-02-28 14:06:08 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 18 loss:0.01196410320699215 norm:0.00010515845497138798 max memory_allocated 22562.50732421875 
[2025-02-28 14:06:39 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 19 loss:0.011956382542848587 norm:9.901313751470298e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:06:47 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-02-28 14:07:20 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 0 loss:0.020296266302466393 norm:0.0010518119670450687 max memory_allocated 22562.67919921875 
[2025-02-28 14:07:50 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 1 loss:0.017467068508267403 norm:0.00044657065882347524 max memory_allocated 22562.67919921875 
[2025-02-28 14:08:21 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 2 loss:0.016276152804493904 norm:0.00024975824635475874 max memory_allocated 22562.67919921875 
[2025-02-28 14:08:52 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 3 loss:0.015468898229300976 norm:0.00017449163715355098 max memory_allocated 22562.67919921875 
[2025-02-28 14:09:22 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 4 loss:0.014885971322655678 norm:0.00012617094034794718 max memory_allocated 22562.67919921875 
[2025-02-28 14:09:53 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 5 loss:0.014660704880952835 norm:0.00011470908066257834 max memory_allocated 22562.67919921875 
[2025-02-28 14:10:23 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 6 loss:0.014598485082387924 norm:0.00010970425501000136 max memory_allocated 22562.67919921875 
[2025-02-28 14:10:54 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 7 loss:0.014558079652488232 norm:9.906408376991749e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:11:25 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 8 loss:0.01454364974051714 norm:9.605134255252779e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:11:55 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 9 loss:0.014520136639475822 norm:8.52282319101505e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:12:26 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 10 loss:0.014501942321658134 norm:8.935983350966126e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:12:56 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 11 loss:0.01449844054877758 norm:9.023362508742139e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:13:27 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 12 loss:0.014491211622953415 norm:9.349863103125244e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:13:58 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 13 loss:0.014481380581855774 norm:8.807003177935258e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:14:28 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 14 loss:0.01448996365070343 norm:9.176261664833874e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:14:59 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 15 loss:0.0144811375066638 norm:8.931724732974544e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:15:30 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 16 loss:0.014479693956673145 norm:8.878260996425524e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:16:00 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 17 loss:0.014476294629275799 norm:8.940606494434178e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:16:31 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 18 loss:0.014468586072325706 norm:8.636745769763365e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:17:01 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 19 loss:0.014471182599663734 norm:8.496161171933636e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:17:12 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-02-28 14:17:49 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 0 loss:0.021587343886494637 norm:0.0009085825877264142 max memory_allocated 22562.85107421875 
[2025-02-28 14:18:19 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 1 loss:0.019454536959528923 norm:0.00042783498065546155 max memory_allocated 22562.85107421875 
[2025-02-28 14:18:50 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 2 loss:0.018318673595786095 norm:0.00025550159625709057 max memory_allocated 22562.85107421875 
[2025-02-28 14:19:20 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 3 loss:0.017437413334846497 norm:0.0001601613184902817 max memory_allocated 22562.85107421875 
[2025-02-28 14:19:51 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 4 loss:0.0168744083493948 norm:0.00012330530444160104 max memory_allocated 22562.85107421875 
[2025-02-28 14:20:22 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 5 loss:0.016671249642968178 norm:9.995052096201107e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:20:52 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 6 loss:0.01661122404038906 norm:8.347119728568941e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:21:23 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 7 loss:0.016592061147093773 norm:8.512967178830877e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:21:53 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 8 loss:0.016566535457968712 norm:8.090715709840879e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:22:24 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 9 loss:0.016554584726691246 norm:8.595472172601148e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:22:55 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 10 loss:0.01654542051255703 norm:8.681982581038028e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:23:25 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 11 loss:0.016545603051781654 norm:8.734819130040705e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:23:56 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 12 loss:0.016541769728064537 norm:8.810936560621485e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:24:26 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 13 loss:0.01652628928422928 norm:9.064911864697933e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:24:57 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 14 loss:0.01651585102081299 norm:9.041136218002066e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:25:28 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 15 loss:0.016513120383024216 norm:8.581065776525065e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:25:58 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 16 loss:0.016510367393493652 norm:9.117565787164494e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:26:29 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 17 loss:0.016506725922226906 norm:8.633248216938227e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:00 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 18 loss:0.01651264727115631 norm:9.098603914026171e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:30 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 19 loss:0.01650843769311905 norm:8.802272350294515e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:38 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-02-28 14:28:12 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 0 loss:0.024993497878313065 norm:0.0010195040376856923 max memory_allocated 22563.02294921875 
[2025-02-28 14:28:42 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 1 loss:0.02237292006611824 norm:0.00047580257523804903 max memory_allocated 22563.02294921875 
[2025-02-28 14:29:13 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 2 loss:0.021092819049954414 norm:0.00027968641370534897 max memory_allocated 22563.02294921875 
[2025-02-28 14:29:43 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 3 loss:0.020085211843252182 norm:0.0002057970268651843 max memory_allocated 22563.02294921875 
[2025-02-28 14:30:14 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 4 loss:0.01948394998908043 norm:0.00014728732639923692 max memory_allocated 22563.02294921875 
[2025-02-28 14:30:45 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 5 loss:0.0192542914301157 norm:0.00012810931366402656 max memory_allocated 22563.02294921875 
[2025-02-28 14:31:15 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 6 loss:0.01916663534939289 norm:0.00011903355334652588 max memory_allocated 22563.02294921875 
[2025-02-28 14:31:46 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 7 loss:0.019100505858659744 norm:0.00010926378308795393 max memory_allocated 22563.02294921875 
[2025-02-28 14:32:16 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 8 loss:0.019068019464612007 norm:0.00010887655662372708 max memory_allocated 22563.02294921875 
[2025-02-28 14:32:47 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 9 loss:0.01903487928211689 norm:0.00010595764615572989 max memory_allocated 22563.02294921875 
[2025-02-28 14:33:18 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 10 loss:0.019022878259420395 norm:0.00011181226000189781 max memory_allocated 22563.02294921875 
[2025-02-28 14:33:48 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 11 loss:0.019008057191967964 norm:0.00011377265036571771 max memory_allocated 22563.02294921875 
[2025-02-28 14:34:19 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 12 loss:0.018986284732818604 norm:0.00010740998550318182 max memory_allocated 22563.02294921875 
[2025-02-28 14:34:50 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 13 loss:0.018972642719745636 norm:0.00010501396900508553 max memory_allocated 22563.02294921875 
[2025-02-28 14:35:20 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 14 loss:0.01896807737648487 norm:0.00010156136704608798 max memory_allocated 22563.02294921875 
[2025-02-28 14:35:51 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 15 loss:0.018959153443574905 norm:9.86367158475332e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:36:21 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 16 loss:0.018943991512060165 norm:9.983112249756232e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:36:52 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 17 loss:0.018932297825813293 norm:9.658681665314361e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:37:23 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 18 loss:0.018930289894342422 norm:9.936632704921067e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:37:53 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 19 loss:0.01892724633216858 norm:9.762548870639876e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:38:01 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-02-28 14:38:34 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 0 loss:0.029108397662639618 norm:0.0012060608714818954 max memory_allocated 22563.19482421875 
[2025-02-28 14:39:05 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 1 loss:0.02550388313829899 norm:0.000530682853423059 max memory_allocated 22563.19482421875 
[2025-02-28 14:39:36 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 2 loss:0.023894932121038437 norm:0.0003416980616748333 max memory_allocated 22563.19482421875 
[2025-02-28 14:40:06 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 3 loss:0.02280818484723568 norm:0.0002284910442540422 max memory_allocated 22563.19482421875 
[2025-02-28 14:40:37 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 4 loss:0.022215764969587326 norm:0.00018329970771446824 max memory_allocated 22563.19482421875 
[2025-02-28 14:41:08 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 5 loss:0.021953465417027473 norm:0.00014941174595151097 max memory_allocated 22563.19482421875 
[2025-02-28 14:41:38 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 6 loss:0.021824320778250694 norm:0.00012838075053878129 max memory_allocated 22563.19482421875 
[2025-02-28 14:42:09 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 7 loss:0.02175859734416008 norm:0.00012705777771770954 max memory_allocated 22563.19482421875 
[2025-02-28 14:42:40 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 8 loss:0.02171292155981064 norm:0.00011765461385948583 max memory_allocated 22563.19482421875 
[2025-02-28 14:43:10 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 9 loss:0.0216689370572567 norm:0.00010826584184542298 max memory_allocated 22563.19482421875 
[2025-02-28 14:43:41 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 10 loss:0.021634621545672417 norm:0.0001045954049914144 max memory_allocated 22563.19482421875 
[2025-02-28 14:44:11 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 11 loss:0.021606143563985825 norm:0.00010014688450610265 max memory_allocated 22563.19482421875 
[2025-02-28 14:44:42 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 12 loss:0.02159891650080681 norm:9.99230906018056e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:45:13 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 13 loss:0.02158026024699211 norm:9.40535610425286e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:45:43 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 14 loss:0.021553337574005127 norm:9.087133366847411e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:46:14 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 15 loss:0.021541763097047806 norm:9.01823295862414e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:46:44 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 16 loss:0.021536635234951973 norm:9.019696881296113e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:47:15 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 17 loss:0.021539194509387016 norm:9.210084681399167e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:47:46 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 18 loss:0.02152928337454796 norm:9.525493805995211e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:48:16 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 19 loss:0.02152465283870697 norm:9.540559403831139e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:48:24 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-02-28 14:48:57 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 0 loss:0.03104587271809578 norm:0.0009006871841847897 max memory_allocated 22563.36669921875 
[2025-02-28 14:49:28 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 1 loss:0.028024576604366302 norm:0.0004659498226828873 max memory_allocated 22563.36669921875 
[2025-02-28 14:49:59 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 2 loss:0.026352321729063988 norm:0.000295258651021868 max memory_allocated 22563.36669921875 
[2025-02-28 14:50:29 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 3 loss:0.02528730034828186 norm:0.00019526842515915632 max memory_allocated 22563.36669921875 
[2025-02-28 14:51:00 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 4 loss:0.02470511756837368 norm:0.00015648300177417696 max memory_allocated 22563.36669921875 
[2025-02-28 14:51:31 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 5 loss:0.024454981088638306 norm:0.00012973227421753109 max memory_allocated 22563.36669921875 
[2025-02-28 14:52:01 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 6 loss:0.024356188252568245 norm:0.00012710013834293932 max memory_allocated 22563.36669921875 
[2025-02-28 14:52:32 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 7 loss:0.0242825448513031 norm:0.00011880031524924561 max memory_allocated 22563.36669921875 
[2025-02-28 14:53:02 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 8 loss:0.024235764518380165 norm:0.00011535077646840364 max memory_allocated 22563.36669921875 
[2025-02-28 14:53:33 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 9 loss:0.024194492027163506 norm:0.00011560598068172112 max memory_allocated 22563.36669921875 
[2025-02-28 14:54:04 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 10 loss:0.02416304685175419 norm:0.0001057678455254063 max memory_allocated 22563.36669921875 
[2025-02-28 14:54:34 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 11 loss:0.024173002690076828 norm:0.00010819337330758572 max memory_allocated 22563.36669921875 
[2025-02-28 14:55:05 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 12 loss:0.024176865816116333 norm:0.00010719634883571416 max memory_allocated 22563.36669921875 
[2025-02-28 14:55:36 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 13 loss:0.024156909435987473 norm:0.00010840335016837344 max memory_allocated 22563.36669921875 
[2025-02-28 14:56:06 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 14 loss:0.0241475198417902 norm:0.00010832500993274152 max memory_allocated 22563.36669921875 
[2025-02-28 14:56:37 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 15 loss:0.024138370528817177 norm:0.00010854028369067237 max memory_allocated 22563.36669921875 
[2025-02-28 14:57:07 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 16 loss:0.024132318794727325 norm:0.00011116806126665324 max memory_allocated 22563.36669921875 
[2025-02-28 14:57:38 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 17 loss:0.024110129103064537 norm:0.00011133961379528046 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:09 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 18 loss:0.024117309600114822 norm:0.00011236406135139987 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:39 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 19 loss:0.024113863706588745 norm:0.0001083042734535411 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:47 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-02-28 14:59:21 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 0 loss:0.03255132958292961 norm:0.0009595872834324837 max memory_allocated 22563.53857421875 
[2025-02-28 14:59:51 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 1 loss:0.029431184753775597 norm:0.000399041804485023 max memory_allocated 22563.53857421875 
[2025-02-28 15:00:22 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 2 loss:0.028079282492399216 norm:0.00023549349862150848 max memory_allocated 22563.53857421875 
[2025-02-28 15:00:53 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 3 loss:0.027133122086524963 norm:0.00018628920952323824 max memory_allocated 22563.53857421875 
[2025-02-28 15:01:23 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 4 loss:0.026566889137029648 norm:0.00014548291801474988 max memory_allocated 22563.53857421875 
[2025-02-28 15:01:54 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 5 loss:0.026306793093681335 norm:0.00013174116611480713 max memory_allocated 22563.53857421875 
[2025-02-28 15:02:24 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 6 loss:0.026179997250437737 norm:0.00011888657900271937 max memory_allocated 22563.53857421875 
[2025-02-28 15:02:55 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 7 loss:0.02612302266061306 norm:0.00011700845061568543 max memory_allocated 22563.53857421875 
[2025-02-28 15:03:26 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 8 loss:0.026086492463946342 norm:0.00011062510020565242 max memory_allocated 22563.53857421875 
[2025-02-28 15:03:56 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 9 loss:0.02604147605597973 norm:0.00010268212645314634 max memory_allocated 22563.53857421875 
[2025-02-28 15:04:27 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 10 loss:0.026020709425210953 norm:0.0001049287457135506 max memory_allocated 22563.53857421875 
[2025-02-28 15:04:58 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 11 loss:0.025992192327976227 norm:9.981583571061492e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:05:28 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 12 loss:0.025992432609200478 norm:9.545866487314925e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:05:59 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 13 loss:0.025985032320022583 norm:9.163884533336386e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:06:30 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 14 loss:0.025969965383410454 norm:9.023846359923482e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:07:00 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 15 loss:0.02596290037035942 norm:9.018545824801549e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:07:31 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 16 loss:0.025958923622965813 norm:8.93504693522118e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:08:01 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 17 loss:0.025950582697987556 norm:8.972175419330597e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:08:32 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 18 loss:0.02594411000609398 norm:8.950453775469214e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:09:03 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 19 loss:0.025932546705007553 norm:9.079323353944346e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:09:11 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-02-28 15:09:44 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 0 loss:0.032374680042266846 norm:0.0005085757584311068 max memory_allocated 22563.71044921875 
[2025-02-28 15:10:15 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 1 loss:0.030390748754143715 norm:0.00025900808395817876 max memory_allocated 22563.71044921875 
[2025-02-28 15:10:45 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 2 loss:0.029264822602272034 norm:0.000172963147633709 max memory_allocated 22563.71044921875 
[2025-02-28 15:11:16 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 3 loss:0.028464779257774353 norm:0.00014470763562712818 max memory_allocated 22563.71044921875 
[2025-02-28 15:11:47 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 4 loss:0.027935262769460678 norm:0.00012226446415297687 max memory_allocated 22563.71044921875 
[2025-02-28 15:12:17 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 5 loss:0.02768765017390251 norm:0.0001020392301143147 max memory_allocated 22563.71044921875 
[2025-02-28 15:12:48 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 6 loss:0.027576502412557602 norm:9.965877688955516e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:13:19 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 7 loss:0.0275051761418581 norm:9.125583892455325e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:13:49 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 8 loss:0.02747020125389099 norm:8.721994527149945e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:14:20 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 9 loss:0.02745118923485279 norm:8.559288835385814e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:14:50 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 10 loss:0.027433104813098907 norm:8.20975546957925e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:15:21 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 11 loss:0.027417946606874466 norm:8.338313637068495e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:15:52 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 12 loss:0.027412306517362595 norm:8.576740219723433e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:16:22 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 13 loss:0.02741430327296257 norm:8.839680231176317e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:16:53 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 14 loss:0.027403319254517555 norm:8.968542533693835e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:17:24 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 15 loss:0.027392897754907608 norm:8.77861020853743e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:17:54 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 16 loss:0.02738983742892742 norm:8.644065383123234e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:18:25 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 17 loss:0.027385778725147247 norm:8.487498416798189e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:18:55 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 18 loss:0.027394764125347137 norm:8.496097871102393e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:26 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 19 loss:0.027398228645324707 norm:8.413434261456132e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:34 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-02-28 15:20:07 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 0 loss:0.03456142544746399 norm:0.0006728105363436043 max memory_allocated 22563.88232421875 
[2025-02-28 15:20:38 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 1 loss:0.032109275460243225 norm:0.00033303885720670223 max memory_allocated 22563.88232421875 
[2025-02-28 15:21:09 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 2 loss:0.03090236335992813 norm:0.0002103421138599515 max memory_allocated 22563.88232421875 
[2025-02-28 15:21:39 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 3 loss:0.030026139691472054 norm:0.0001713588135316968 max memory_allocated 22563.88232421875 
[2025-02-28 15:22:10 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 4 loss:0.029459167271852493 norm:0.00014552137872669846 max memory_allocated 22563.88232421875 
[2025-02-28 15:22:40 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 5 loss:0.029178064316511154 norm:0.00012366143346298486 max memory_allocated 22563.88232421875 
[2025-02-28 15:23:11 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 6 loss:0.0290240366011858 norm:0.00012123274791520089 max memory_allocated 22563.88232421875 
[2025-02-28 15:23:42 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 7 loss:0.028954196721315384 norm:0.00011943918070755899 max memory_allocated 22563.88232421875 
[2025-02-28 15:24:12 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 8 loss:0.02888227440416813 norm:0.00011238722072448581 max memory_allocated 22563.88232421875 
[2025-02-28 15:24:43 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 9 loss:0.028839441016316414 norm:9.809773473534733e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:25:13 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 10 loss:0.02881433255970478 norm:0.00010239736730000004 max memory_allocated 22563.88232421875 
[2025-02-28 15:25:44 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 11 loss:0.028776049613952637 norm:9.342608245788142e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:26:15 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 12 loss:0.028742698952555656 norm:8.941784471971914e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:26:45 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 13 loss:0.02872178889811039 norm:8.919992978917435e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:27:16 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 14 loss:0.02871185913681984 norm:8.361146319657564e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:27:47 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 15 loss:0.028707491233944893 norm:8.222206815844402e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:28:17 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 16 loss:0.02869945578277111 norm:8.318614709423855e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:28:48 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 17 loss:0.028685729950666428 norm:8.33806989248842e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:19 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 18 loss:0.02867857925593853 norm:8.135270763887092e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:49 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 19 loss:0.028680579736828804 norm:8.098196121864021e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:57 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-02-28 15:30:31 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 0 loss:0.03524244204163551 norm:0.0005610971711575985 max memory_allocated 22564.05419921875 
[2025-02-28 15:31:01 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 1 loss:0.03302447125315666 norm:0.0002851530443876982 max memory_allocated 22564.05419921875 
[2025-02-28 15:31:32 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 2 loss:0.03187946975231171 norm:0.0002027410955633968 max memory_allocated 22564.05419921875 
[2025-02-28 15:32:02 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 3 loss:0.03103671595454216 norm:0.00016309102647937834 max memory_allocated 22564.05419921875 
[2025-02-28 15:32:33 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 4 loss:0.030505074188113213 norm:0.00013757625129073858 max memory_allocated 22564.05419921875 
[2025-02-28 15:33:04 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 5 loss:0.030238859355449677 norm:0.00013196223881095648 max memory_allocated 22564.05419921875 
[2025-02-28 15:33:34 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 6 loss:0.030093993991613388 norm:0.00011625868501141667 max memory_allocated 22564.05419921875 
[2025-02-28 15:34:05 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 7 loss:0.030034029856324196 norm:0.00011564789747353643 max memory_allocated 22564.05419921875 
[2025-02-28 15:34:35 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 8 loss:0.029963333159685135 norm:0.00010318902786821127 max memory_allocated 22564.05419921875 
[2025-02-28 15:35:06 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 9 loss:0.029920924454927444 norm:0.00010016520536737517 max memory_allocated 22564.05419921875 
[2025-02-28 15:35:37 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 10 loss:0.029889728873968124 norm:9.353858331451192e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:36:07 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 11 loss:0.02985566109418869 norm:8.997954137157649e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:36:38 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 12 loss:0.029833097010850906 norm:9.088191291084513e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:37:09 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 13 loss:0.029809141531586647 norm:8.522401185473427e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:37:39 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 14 loss:0.029811805114150047 norm:8.567049371777102e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:38:10 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 15 loss:0.02979578636586666 norm:8.284906652988866e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:38:40 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 16 loss:0.02978544495999813 norm:7.943132368382066e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:39:11 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 17 loss:0.029785066843032837 norm:7.809272210579365e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:39:42 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 18 loss:0.02978077530860901 norm:8.13772639958188e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:12 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 19 loss:0.029773300513625145 norm:8.354076271643862e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:21 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-02-28 15:40:54 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 0 loss:0.03510085865855217 norm:0.0007239708793349564 max memory_allocated 22564.22607421875 
[2025-02-28 15:41:24 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 1 loss:0.03307776153087616 norm:0.000350562680978328 max memory_allocated 22564.22607421875 
[2025-02-28 15:41:55 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 2 loss:0.031819529831409454 norm:0.00022446087677963078 max memory_allocated 22564.22607421875 
[2025-02-28 15:42:26 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 3 loss:0.030997764319181442 norm:0.00016982293163891882 max memory_allocated 22564.22607421875 
[2025-02-28 15:42:56 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 4 loss:0.030464954674243927 norm:0.00014033315528649837 max memory_allocated 22564.22607421875 
[2025-02-28 15:43:27 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 5 loss:0.030186884105205536 norm:0.00011966206511715427 max memory_allocated 22564.22607421875 
[2025-02-28 15:43:58 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 6 loss:0.03005584515631199 norm:0.00011245025234529749 max memory_allocated 22564.22607421875 
[2025-02-28 15:44:28 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 7 loss:0.02997753582894802 norm:0.00010440951155032963 max memory_allocated 22564.22607421875 
[2025-02-28 15:44:59 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 8 loss:0.029914088547229767 norm:9.730366582516581e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:45:30 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 9 loss:0.02987086959183216 norm:9.451424557482824e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:46:00 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 10 loss:0.029836630448698997 norm:9.302151738665998e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:46:31 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 11 loss:0.029813790693879128 norm:9.358768875245005e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:47:02 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 12 loss:0.029788844287395477 norm:9.010776557261124e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:47:32 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 13 loss:0.029769379645586014 norm:8.584959141444415e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:48:03 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 14 loss:0.029747571796178818 norm:8.350252755917609e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:48:34 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 15 loss:0.02974696271121502 norm:8.413962495978922e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:49:04 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 16 loss:0.029747329652309418 norm:8.368604176212102e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:49:35 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 17 loss:0.029741745442152023 norm:8.056821388890967e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:06 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 18 loss:0.02973366342484951 norm:8.095581870293245e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:36 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 19 loss:0.029729057103395462 norm:8.161902951542288e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:44 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-02-28 15:51:17 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 0 loss:0.0352897010743618 norm:0.0005402351962402463 max memory_allocated 22564.39794921875 
[2025-02-28 15:51:48 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 1 loss:0.033327024430036545 norm:0.0002787491539493203 max memory_allocated 22564.39794921875 
[2025-02-28 15:52:19 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 2 loss:0.032265063375234604 norm:0.00018693599849939346 max memory_allocated 22564.39794921875 
[2025-02-28 15:52:49 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 3 loss:0.031497661024332047 norm:0.00014673249097540975 max memory_allocated 22564.39794921875 
[2025-02-28 15:53:20 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 4 loss:0.030994407832622528 norm:0.00012525622150860727 max memory_allocated 22564.39794921875 
[2025-02-28 15:53:51 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 5 loss:0.030682513490319252 norm:0.00011586322943912819 max memory_allocated 22564.39794921875 
[2025-02-28 15:54:21 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 6 loss:0.030537938699126244 norm:0.00010082791413879022 max memory_allocated 22564.39794921875 
[2025-02-28 15:54:52 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 7 loss:0.030458886176347733 norm:9.416009561391547e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:55:23 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 8 loss:0.03040519915521145 norm:9.650224092183635e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:55:53 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 9 loss:0.030348148196935654 norm:9.798235987545922e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:56:24 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 10 loss:0.030309831723570824 norm:8.621417509857565e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:56:54 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 11 loss:0.030283087864518166 norm:8.526509191142395e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:57:25 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 12 loss:0.030267033725976944 norm:8.397319470532238e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:57:56 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 13 loss:0.030239630490541458 norm:7.705966709181666e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:58:26 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 14 loss:0.03022785484790802 norm:7.294693205039948e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:58:57 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 15 loss:0.03021341934800148 norm:7.408573583234102e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:59:28 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 16 loss:0.030205614864826202 norm:6.717185897286981e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:59:58 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 17 loss:0.030195465311408043 norm:6.516035500681028e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:00:29 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 18 loss:0.030191345140337944 norm:6.611722346860915e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:01:00 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 19 loss:0.030191978439688683 norm:6.621396460104734e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:01:08 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-02-28 16:01:41 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 0 loss:0.03670532628893852 norm:0.0008490972686558962 max memory_allocated 22564.56982421875 
[2025-02-28 16:02:12 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 1 loss:0.033955786377191544 norm:0.00035908943391405046 max memory_allocated 22564.56982421875 
[2025-02-28 16:02:42 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 2 loss:0.03266514837741852 norm:0.00023739399330224842 max memory_allocated 22564.56982421875 
[2025-02-28 16:03:13 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 3 loss:0.03179240971803665 norm:0.00016992758901324123 max memory_allocated 22564.56982421875 
[2025-02-28 16:03:43 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 4 loss:0.03121601790189743 norm:0.00014053189079277217 max memory_allocated 22564.56982421875 
[2025-02-28 16:04:14 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 5 loss:0.03091854602098465 norm:0.00012826031888835132 max memory_allocated 22564.56982421875 
[2025-02-28 16:04:45 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 6 loss:0.030766453593969345 norm:0.00011861148232128471 max memory_allocated 22564.56982421875 
[2025-02-28 16:05:15 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 7 loss:0.030676579102873802 norm:0.0001053905434673652 max memory_allocated 22564.56982421875 
[2025-02-28 16:05:46 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 8 loss:0.030621392652392387 norm:0.00010022016795119271 max memory_allocated 22564.56982421875 
[2025-02-28 16:06:17 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 9 loss:0.030571389943361282 norm:9.796937956707552e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:06:47 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 10 loss:0.030524902045726776 norm:9.655257599661127e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:07:18 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 11 loss:0.030487842857837677 norm:9.258231148123741e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:07:49 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 12 loss:0.03044508956372738 norm:8.751338464207947e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:08:19 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 13 loss:0.030414607375860214 norm:8.203636389225721e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:08:50 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 14 loss:0.030395755544304848 norm:8.102804713416845e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:09:20 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 15 loss:0.030372310429811478 norm:8.098151010926813e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:09:51 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 16 loss:0.030358891934156418 norm:7.886503590270877e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:10:22 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 17 loss:0.030345790088176727 norm:7.785318302921951e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:10:52 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 18 loss:0.03033382073044777 norm:7.479219493689016e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:23 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 19 loss:0.030326828360557556 norm:7.190125324996188e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:31 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-02-28 16:12:04 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 0 loss:0.037082135677337646 norm:0.0010266411118209362 max memory_allocated 22564.74169921875 
[2025-02-28 16:12:35 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 1 loss:0.03422842174768448 norm:0.0004166998260188848 max memory_allocated 22564.74169921875 
[2025-02-28 16:13:06 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 2 loss:0.03290596231818199 norm:0.0002395340270595625 max memory_allocated 22564.74169921875 
[2025-02-28 16:13:36 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 3 loss:0.032076288014650345 norm:0.00018077126878779382 max memory_allocated 22564.74169921875 
[2025-02-28 16:14:07 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 4 loss:0.0314834788441658 norm:0.00014825636753812432 max memory_allocated 22564.74169921875 
[2025-02-28 16:14:38 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 5 loss:0.031135596334934235 norm:0.00013653156929649413 max memory_allocated 22564.74169921875 
[2025-02-28 16:15:08 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 6 loss:0.03096708096563816 norm:0.00012635540042538196 max memory_allocated 22564.74169921875 
[2025-02-28 16:15:39 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 7 loss:0.03086290881037712 norm:0.00011805371468653902 max memory_allocated 22564.74169921875 
[2025-02-28 16:16:09 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 8 loss:0.030787816271185875 norm:0.00010941114305751398 max memory_allocated 22564.74169921875 
[2025-02-28 16:16:40 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 9 loss:0.030732598155736923 norm:0.00010408329399069771 max memory_allocated 22564.74169921875 
[2025-02-28 16:17:11 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 10 loss:0.030681144446134567 norm:9.66978186625056e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:17:41 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 11 loss:0.03063192404806614 norm:9.413939551450312e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:18:12 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 12 loss:0.030602039769291878 norm:9.484209294896573e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:18:43 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 13 loss:0.03056112490594387 norm:9.064051846507937e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:19:13 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 14 loss:0.030529260635375977 norm:8.70716103236191e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:19:44 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 15 loss:0.030504463240504265 norm:7.888021355029196e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:20:15 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 16 loss:0.030479852110147476 norm:7.881046622060239e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:20:45 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 17 loss:0.0304613895714283 norm:8.214761328417808e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:21:16 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 18 loss:0.03043130785226822 norm:7.635387009941041e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:21:47 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 19 loss:0.03042091429233551 norm:7.053748413454741e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:21:55 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-02-28 16:22:28 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 0 loss:0.03526882082223892 norm:0.0006698538199998438 max memory_allocated 22564.91357421875 
[2025-02-28 16:22:58 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 1 loss:0.03369315713644028 norm:0.00023421614605467767 max memory_allocated 22564.91357421875 
[2025-02-28 16:23:29 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 2 loss:0.032935209572315216 norm:0.00016271496133413166 max memory_allocated 22564.91357421875 
[2025-02-28 16:24:00 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 3 loss:0.03226809203624725 norm:0.00013627868611365557 max memory_allocated 22564.91357421875 
[2025-02-28 16:24:30 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 4 loss:0.03175229951739311 norm:0.00011031978647224605 max memory_allocated 22564.91357421875 
[2025-02-28 16:25:01 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 5 loss:0.03144526109099388 norm:8.950278424890712e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:25:32 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 6 loss:0.031303975731134415 norm:9.025749022839591e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:26:02 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 7 loss:0.031225545331835747 norm:7.80784321250394e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:26:33 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 8 loss:0.031177489086985588 norm:6.932968972250819e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:27:03 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 9 loss:0.031147940084338188 norm:6.964946078369394e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:27:34 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 10 loss:0.031112616881728172 norm:6.990732799749821e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:28:05 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 11 loss:0.031087538227438927 norm:6.125294021330774e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:28:35 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 12 loss:0.03106693923473358 norm:5.695846630260348e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:29:06 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 13 loss:0.0310530848801136 norm:6.05715686106123e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:29:36 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 14 loss:0.031046239659190178 norm:5.469709503813647e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:30:07 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 15 loss:0.031031638383865356 norm:5.626068377750926e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:30:38 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 16 loss:0.031017430126667023 norm:5.298779797158204e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:31:08 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 17 loss:0.031017018482089043 norm:5.4672756959917024e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:31:39 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 18 loss:0.031010806560516357 norm:5.288234751787968e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:32:10 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 19 loss:0.031006470322608948 norm:5.129964119987562e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:32:24 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-02-28 16:33:04 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 0 loss:0.03786806762218475 norm:0.00073355803033337 max memory_allocated 22565.08544921875 
[2025-02-28 16:33:34 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 1 loss:0.036155786365270615 norm:0.0003480776504147798 max memory_allocated 22565.08544921875 
[2025-02-28 16:34:05 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 2 loss:0.035308144986629486 norm:0.000221945287194103 max memory_allocated 22565.08544921875 
[2025-02-28 16:34:35 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 3 loss:0.034585222601890564 norm:0.0001677554682828486 max memory_allocated 22565.08544921875 
[2025-02-28 16:35:06 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 4 loss:0.03397955000400543 norm:0.00013564142864197493 max memory_allocated 22565.08544921875 
[2025-02-28 16:35:37 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 5 loss:0.03364584222435951 norm:0.00011926137085538357 max memory_allocated 22565.08544921875 
[2025-02-28 16:36:07 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 6 loss:0.03348764777183533 norm:9.810866322368383e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:36:38 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 7 loss:0.033417366445064545 norm:9.254555334337056e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:37:08 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 8 loss:0.033358458429574966 norm:8.416281343670562e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:37:39 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 9 loss:0.033316753804683685 norm:8.106259338092059e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:38:10 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 10 loss:0.03327576816082001 norm:7.643878780072555e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:38:40 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 11 loss:0.03324175626039505 norm:7.4052280979231e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:39:11 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 12 loss:0.03320971503853798 norm:6.920994201209396e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:39:42 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 13 loss:0.0331818088889122 norm:6.878224667161703e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:40:12 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 14 loss:0.03315756469964981 norm:6.586869130842388e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:40:43 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 15 loss:0.03314025700092316 norm:6.796330853831023e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:41:13 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 16 loss:0.03311699628829956 norm:6.230128201423213e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:41:44 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 17 loss:0.03310266509652138 norm:6.241268420126289e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:42:15 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 18 loss:0.033091429620981216 norm:6.179320916999131e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:42:45 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 19 loss:0.03309054300189018 norm:5.888164014322683e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:42:53 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-02-28 16:43:27 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 0 loss:0.04075697809457779 norm:0.0007865959778428078 max memory_allocated 22565.25732421875 
[2025-02-28 16:43:57 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 1 loss:0.0390518493950367 norm:0.0003534529241733253 max memory_allocated 22565.25732421875 
[2025-02-28 16:44:28 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 2 loss:0.038091178983449936 norm:0.0002057219680864364 max memory_allocated 22565.25732421875 
[2025-02-28 16:44:59 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 3 loss:0.03739168122410774 norm:0.00015168248501140624 max memory_allocated 22565.25732421875 
[2025-02-28 16:45:29 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 4 loss:0.03679423779249191 norm:0.00012260019138921052 max memory_allocated 22565.25732421875 
[2025-02-28 16:46:00 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 5 loss:0.03646696358919144 norm:0.00010359633597545326 max memory_allocated 22565.25732421875 
[2025-02-28 16:46:31 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 6 loss:0.03635750710964203 norm:9.41601101658307e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:47:01 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 7 loss:0.03630244731903076 norm:8.884594717528671e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:47:32 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 8 loss:0.03625238686800003 norm:7.68972240621224e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:48:02 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 9 loss:0.03621829301118851 norm:7.188150630099699e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:48:33 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 10 loss:0.03618773818016052 norm:7.651982741663232e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:49:04 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 11 loss:0.03615749999880791 norm:7.597940566483885e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:49:34 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 12 loss:0.036133773624897 norm:6.881867011543363e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:50:05 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 13 loss:0.036114830523729324 norm:6.400803249562159e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:50:36 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 14 loss:0.03609005734324455 norm:6.576388841494918e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:51:06 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 15 loss:0.03606013208627701 norm:6.018870772095397e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:51:37 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 16 loss:0.03604299947619438 norm:5.5607404647162184e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:52:08 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 17 loss:0.03603236749768257 norm:5.808314017485827e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:52:38 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 18 loss:0.03602009639143944 norm:5.647065336233936e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:53:09 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 19 loss:0.03601420670747757 norm:5.233935371506959e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:53:19 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-02-28 16:53:52 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 0 loss:0.04577729478478432 norm:0.0012125787325203419 max memory_allocated 22565.42919921875 
[2025-02-28 16:54:22 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 1 loss:0.04321509599685669 norm:0.0004705043393187225 max memory_allocated 22565.42919921875 
[2025-02-28 16:54:53 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 2 loss:0.04199187830090523 norm:0.0002466853766236454 max memory_allocated 22565.42919921875 
[2025-02-28 16:55:24 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 3 loss:0.041264042258262634 norm:0.00017514231149107218 max memory_allocated 22565.42919921875 
[2025-02-28 16:55:54 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 4 loss:0.04063049331307411 norm:0.00014136225217953324 max memory_allocated 22565.42919921875 
[2025-02-28 16:56:25 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 5 loss:0.04027649760246277 norm:0.00012786781007889658 max memory_allocated 22565.42919921875 
[2025-02-28 16:56:56 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 6 loss:0.040137507021427155 norm:0.00011746295785997063 max memory_allocated 22565.42919921875 
[2025-02-28 16:57:26 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 7 loss:0.04005545377731323 norm:0.00011480255489004776 max memory_allocated 22565.42919921875 
[2025-02-28 16:57:57 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 8 loss:0.039987679570913315 norm:0.00011071426706621423 max memory_allocated 22565.42919921875 
[2025-02-28 16:58:28 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 9 loss:0.03993385657668114 norm:9.904551552608609e-05 max memory_allocated 22565.42919921875 
[2025-02-28 16:58:58 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 10 loss:0.039899639785289764 norm:9.31945105548948e-05 max memory_allocated 22565.42919921875 
[2025-02-28 16:59:29 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 11 loss:0.03986610472202301 norm:9.303943807026371e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:00:00 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 12 loss:0.03982831910252571 norm:8.663607877679169e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:00:30 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 13 loss:0.03980378434062004 norm:8.788976992946118e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:01:01 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 14 loss:0.039774779230356216 norm:8.215483831008896e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:01:32 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 15 loss:0.03975711762905121 norm:7.788738003000617e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:02:02 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 16 loss:0.03973924741148949 norm:7.34156055841595e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:02:33 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 17 loss:0.039720989763736725 norm:7.537442434113473e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:03:03 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 18 loss:0.03969990834593773 norm:7.915630703791976e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:03:34 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 19 loss:0.03967268764972687 norm:7.41344119887799e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:03:42 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-02-28 17:04:15 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 0 loss:0.04801740124821663 norm:0.000521582318469882 max memory_allocated 22565.60107421875 
[2025-02-28 17:04:46 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 1 loss:0.046268902719020844 norm:0.0002156027767341584 max memory_allocated 22565.60107421875 
[2025-02-28 17:05:17 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 2 loss:0.04545135051012039 norm:0.00014875520719215274 max memory_allocated 22565.60107421875 
[2025-02-28 17:05:47 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 3 loss:0.04482323303818703 norm:0.0001244865998160094 max memory_allocated 22565.60107421875 
[2025-02-28 17:06:18 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 4 loss:0.044329676777124405 norm:9.573005081620067e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:06:49 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 5 loss:0.044085368514060974 norm:9.673451131675392e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:07:19 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 6 loss:0.04398132488131523 norm:8.762390643823892e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:07:50 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 7 loss:0.043911129236221313 norm:8.383211388718337e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:08:21 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 8 loss:0.04386841878294945 norm:7.653632928850129e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:08:51 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 9 loss:0.04382743313908577 norm:7.489803829230368e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:09:22 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 10 loss:0.04378833249211311 norm:7.236289093270898e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:09:52 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 11 loss:0.043770961463451385 norm:7.349718362092972e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:10:23 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 12 loss:0.04373680800199509 norm:6.845978350611404e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:10:54 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 13 loss:0.04373195394873619 norm:7.038028707029298e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:11:24 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 14 loss:0.04371327906847 norm:6.564384966623038e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:11:55 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 15 loss:0.043698713183403015 norm:6.765739090042189e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:12:26 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 16 loss:0.04367697983980179 norm:6.368372851284221e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:12:56 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 17 loss:0.04367322847247124 norm:6.276042404351756e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:13:27 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 18 loss:0.04366586357355118 norm:6.0364818637026474e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:13:58 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 19 loss:0.043669819831848145 norm:5.9334586694603786e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:14:06 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-02-28 17:14:39 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 0 loss:0.055428408086299896 norm:0.0007233258802443743 max memory_allocated 22565.77294921875 
[2025-02-28 17:15:10 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 1 loss:0.05342850834131241 norm:0.0003101852198597044 max memory_allocated 22565.77294921875 
[2025-02-28 17:15:40 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 2 loss:0.052540190517902374 norm:0.0004795306595042348 max memory_allocated 22565.77294921875 
[2025-02-28 17:16:11 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 3 loss:0.051718972623348236 norm:0.00018368203018326312 max memory_allocated 22565.77294921875 
[2025-02-28 17:16:42 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 4 loss:0.050944678485393524 norm:0.0001710033684503287 max memory_allocated 22565.77294921875 
[2025-02-28 17:17:12 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 5 loss:0.050619885325431824 norm:0.00014497079246211797 max memory_allocated 22565.77294921875 
[2025-02-28 17:17:43 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 6 loss:0.050498928874731064 norm:0.00012948976655025035 max memory_allocated 22565.77294921875 
[2025-02-28 17:18:13 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 7 loss:0.050452351570129395 norm:0.00014200207078829408 max memory_allocated 22565.77294921875 
[2025-02-28 17:18:44 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 8 loss:0.05038117244839668 norm:0.0001204027867061086 max memory_allocated 22565.77294921875 
[2025-02-28 17:19:15 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 9 loss:0.05032212287187576 norm:0.00010762791498564184 max memory_allocated 22565.77294921875 
[2025-02-28 17:19:45 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 10 loss:0.05028371512889862 norm:0.0001003759098239243 max memory_allocated 22565.77294921875 
[2025-02-28 17:20:16 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 11 loss:0.05024680495262146 norm:0.00010354615369578823 max memory_allocated 22565.77294921875 
[2025-02-28 17:20:47 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 12 loss:0.05022137984633446 norm:9.84378348221071e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:21:17 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 13 loss:0.05017406865954399 norm:9.356523514725268e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:21:48 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 14 loss:0.05014973133802414 norm:8.492641791235656e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:22:19 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 15 loss:0.050130654126405716 norm:9.527703514322639e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:22:49 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 16 loss:0.05011135712265968 norm:8.90004012035206e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:23:20 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 17 loss:0.05008716881275177 norm:8.048606832744554e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:23:51 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 18 loss:0.050069306045770645 norm:7.815352000761777e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:24:21 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 19 loss:0.05004964396357536 norm:7.95092637417838e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:24:29 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-02-28 17:25:03 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 0 loss:0.061976030468940735 norm:0.0007706470205448568 max memory_allocated 22565.94482421875 
[2025-02-28 17:25:33 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 1 loss:0.06048890948295593 norm:0.00043402696610428393 max memory_allocated 22565.94482421875 
[2025-02-28 17:26:04 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 2 loss:0.05960385128855705 norm:0.00029758000164292753 max memory_allocated 22565.94482421875 
[2025-02-28 17:26:34 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 3 loss:0.058629922568798065 norm:0.00022523838561028242 max memory_allocated 22565.94482421875 
[2025-02-28 17:27:05 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 4 loss:0.057829082012176514 norm:0.00016274649533443153 max memory_allocated 22565.94482421875 
[2025-02-28 17:27:36 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 5 loss:0.05757155269384384 norm:0.00012983527267351747 max memory_allocated 22565.94482421875 
[2025-02-28 17:28:06 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 6 loss:0.05747978016734123 norm:0.00011839929356938228 max memory_allocated 22565.94482421875 
[2025-02-28 17:28:37 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 7 loss:0.05740400031208992 norm:0.00010661178384907544 max memory_allocated 22565.94482421875 
[2025-02-28 17:29:08 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 8 loss:0.05734988674521446 norm:9.637146285967901e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:29:38 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 9 loss:0.05730116367340088 norm:8.913603232940659e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:30:09 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 10 loss:0.05725632980465889 norm:8.128580520860851e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:30:40 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 11 loss:0.05722786858677864 norm:7.545955304522067e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:31:10 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 12 loss:0.057207562029361725 norm:7.546026608906686e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:31:41 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 13 loss:0.05718674883246422 norm:7.262995495693758e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:32:12 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 14 loss:0.0571761280298233 norm:6.534349813591689e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:32:42 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 15 loss:0.05716133862733841 norm:6.512817344628274e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:33:13 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 16 loss:0.057148244231939316 norm:6.697353819618002e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:33:43 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 17 loss:0.05713464692234993 norm:6.276791100390255e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:34:14 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 18 loss:0.05712376534938812 norm:5.9149228036403656e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:34:45 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 19 loss:0.0571107342839241 norm:5.654492633766495e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:34:53 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-02-28 17:35:26 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 0 loss:0.07260943949222565 norm:0.0021707534324377775 max memory_allocated 22566.11669921875 
[2025-02-28 17:35:57 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 1 loss:0.0688108503818512 norm:0.0006233099848031998 max memory_allocated 22566.11669921875 
[2025-02-28 17:36:27 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 2 loss:0.06725379079580307 norm:0.00039700104389339685 max memory_allocated 22566.11669921875 
[2025-02-28 17:36:58 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 3 loss:0.06615711003541946 norm:0.0003060379240196198 max memory_allocated 22566.11669921875 
[2025-02-28 17:37:29 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 4 loss:0.06549523770809174 norm:0.0002476501977071166 max memory_allocated 22566.11669921875 
[2025-02-28 17:37:59 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 5 loss:0.06524763256311417 norm:0.0002095600066240877 max memory_allocated 22566.11669921875 
[2025-02-28 17:38:30 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 6 loss:0.06514628231525421 norm:0.00019708515901584178 max memory_allocated 22566.11669921875 
[2025-02-28 17:39:01 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 7 loss:0.06505598872900009 norm:0.0001741008454700932 max memory_allocated 22566.11669921875 
[2025-02-28 17:39:31 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 8 loss:0.06498116999864578 norm:0.00016283131844829768 max memory_allocated 22566.11669921875 
[2025-02-28 17:40:02 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 9 loss:0.06492398679256439 norm:0.00016487676475662738 max memory_allocated 22566.11669921875 
[2025-02-28 17:40:33 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 10 loss:0.06486044824123383 norm:0.00014827883569523692 max memory_allocated 22566.11669921875 
[2025-02-28 17:41:03 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 11 loss:0.06482591480016708 norm:0.00014769716653972864 max memory_allocated 22566.11669921875 
[2025-02-28 17:41:34 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 12 loss:0.0648050382733345 norm:0.00013393584231380373 max memory_allocated 22566.11669921875 
[2025-02-28 17:42:05 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 13 loss:0.0647733137011528 norm:0.00012950556993018836 max memory_allocated 22566.11669921875 
[2025-02-28 17:42:35 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 14 loss:0.06474220752716064 norm:0.00013071176363155246 max memory_allocated 22566.11669921875 
[2025-02-28 17:43:06 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 15 loss:0.0647086352109909 norm:0.00012279575457796454 max memory_allocated 22566.11669921875 
[2025-02-28 17:43:36 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 16 loss:0.06467202305793762 norm:0.00011413103493396193 max memory_allocated 22566.11669921875 
[2025-02-28 17:44:07 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 17 loss:0.06463481485843658 norm:0.00011398457718314603 max memory_allocated 22566.11669921875 
[2025-02-28 17:44:38 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 18 loss:0.0646180734038353 norm:0.00010758094867924228 max memory_allocated 22566.11669921875 
[2025-02-28 17:45:08 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 19 loss:0.06460433453321457 norm:0.00010560821101535112 max memory_allocated 22566.11669921875 
[2025-02-28 17:45:18 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-02-28 17:45:52 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 0 loss:0.08089904487133026 norm:0.0006684939144179225 max memory_allocated 22566.28857421875 
[2025-02-28 17:46:22 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 1 loss:0.07911509275436401 norm:0.00035444097011350095 max memory_allocated 22566.28857421875 
[2025-02-28 17:46:53 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 2 loss:0.07769563794136047 norm:0.0002518891415093094 max memory_allocated 22566.28857421875 
[2025-02-28 17:47:23 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 3 loss:0.07644703984260559 norm:0.00021255790488794446 max memory_allocated 22566.28857421875 
[2025-02-28 17:47:54 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 4 loss:0.075649194419384 norm:0.0001810325775295496 max memory_allocated 22566.28857421875 
[2025-02-28 17:48:25 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 5 loss:0.07534237205982208 norm:0.00016264000441879034 max memory_allocated 22566.28857421875 
[2025-02-28 17:48:55 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 6 loss:0.07524533569812775 norm:0.00015319518570322543 max memory_allocated 22566.28857421875 
[2025-02-28 17:49:26 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 7 loss:0.07516802847385406 norm:0.0001435720914741978 max memory_allocated 22566.28857421875 
[2025-02-28 17:49:57 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 8 loss:0.07507389038801193 norm:0.00013285853492561728 max memory_allocated 22566.28857421875 
[2025-02-28 17:50:27 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 9 loss:0.07502603530883789 norm:0.00013446396042127162 max memory_allocated 22566.28857421875 
[2025-02-28 17:50:58 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 10 loss:0.07506976276636124 norm:0.0001634116197237745 max memory_allocated 22566.28857421875 
[2025-02-28 17:51:29 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 11 loss:0.07493342459201813 norm:0.0001279825810343027 max memory_allocated 22566.28857421875 
[2025-02-28 17:51:59 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 12 loss:0.07481226325035095 norm:0.00011657550930976868 max memory_allocated 22566.28857421875 
[2025-02-28 17:52:30 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 13 loss:0.07480903714895248 norm:0.0001085414260160178 max memory_allocated 22566.28857421875 
[2025-02-28 17:53:00 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 14 loss:0.07479184120893478 norm:0.00010582582763163373 max memory_allocated 22566.28857421875 
[2025-02-28 17:53:31 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 15 loss:0.0747363418340683 norm:0.00010616493818815798 max memory_allocated 22566.28857421875 
[2025-02-28 17:54:02 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 16 loss:0.07473856955766678 norm:0.0001050328355631791 max memory_allocated 22566.28857421875 
[2025-02-28 17:54:32 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 17 loss:0.07469742000102997 norm:9.872912050923333e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:55:03 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 18 loss:0.07470159977674484 norm:9.99858311843127e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:55:34 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 19 loss:0.07466591894626617 norm:9.543554915580899e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:55:42 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-02-28 17:56:15 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 0 loss:0.09212567657232285 norm:0.0009726461139507592 max memory_allocated 22566.46044921875 
[2025-02-28 17:56:46 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 1 loss:0.08979500830173492 norm:0.0004806394863408059 max memory_allocated 22566.46044921875 
[2025-02-28 17:57:16 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 2 loss:0.08831831812858582 norm:0.0003173184231854975 max memory_allocated 22566.46044921875 
[2025-02-28 17:57:47 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 3 loss:0.08695408701896667 norm:0.000253039674134925 max memory_allocated 22566.46044921875 
[2025-02-28 17:58:18 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 4 loss:0.08616378158330917 norm:0.00020325103832874447 max memory_allocated 22566.46044921875 
[2025-02-28 17:58:48 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 5 loss:0.08592203259468079 norm:0.00016793646500445902 max memory_allocated 22566.46044921875 
[2025-02-28 17:59:19 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 6 loss:0.085807204246521 norm:0.00015228525444399565 max memory_allocated 22566.46044921875 
[2025-02-28 17:59:49 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 7 loss:0.08571501076221466 norm:0.00014027324505150318 max memory_allocated 22566.46044921875 
[2025-02-28 18:00:20 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 8 loss:0.08562085777521133 norm:0.00013223612040746957 max memory_allocated 22566.46044921875 
[2025-02-28 18:00:51 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 9 loss:0.0855540782213211 norm:0.00012139146565459669 max memory_allocated 22566.46044921875 
[2025-02-28 18:01:21 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 10 loss:0.08549098670482635 norm:0.00012002806033706293 max memory_allocated 22566.46044921875 
[2025-02-28 18:01:52 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 11 loss:0.08543654531240463 norm:0.00011643562174867839 max memory_allocated 22566.46044921875 
[2025-02-28 18:02:23 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 12 loss:0.08538613468408585 norm:0.00010678233957150951 max memory_allocated 22566.46044921875 
[2025-02-28 18:02:53 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 13 loss:0.0853428840637207 norm:0.00010374117846367881 max memory_allocated 22566.46044921875 
[2025-02-28 18:03:24 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 14 loss:0.08532346785068512 norm:0.00010379616287536919 max memory_allocated 22566.46044921875 
[2025-02-28 18:03:55 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 15 loss:0.08529799431562424 norm:9.728218719828874e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:04:25 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 16 loss:0.0852670669555664 norm:9.787140879780054e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:04:56 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 17 loss:0.0852460116147995 norm:9.582722850609571e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:05:27 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 18 loss:0.08522975444793701 norm:9.027004853123799e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:05:57 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 19 loss:0.0852118507027626 norm:8.919290121411905e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:06:05 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-02-28 18:06:39 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 0 loss:0.1042688637971878 norm:0.0007493491284549236 max memory_allocated 22566.63232421875 
[2025-02-28 18:07:09 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 1 loss:0.10219015181064606 norm:0.0003993585705757141 max memory_allocated 22566.63232421875 
[2025-02-28 18:07:40 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 2 loss:0.10074946284294128 norm:0.000267761672148481 max memory_allocated 22566.63232421875 
[2025-02-28 18:08:10 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 3 loss:0.09929235279560089 norm:0.00019863272609654814 max memory_allocated 22566.63232421875 
[2025-02-28 18:08:41 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 4 loss:0.09867411851882935 norm:0.0001684304588707164 max memory_allocated 22566.63232421875 
[2025-02-28 18:09:12 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 5 loss:0.09851636737585068 norm:0.0001477473706472665 max memory_allocated 22566.63232421875 
[2025-02-28 18:09:42 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 6 loss:0.09843973070383072 norm:0.0001338635483989492 max memory_allocated 22566.63232421875 
[2025-02-28 18:10:13 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 7 loss:0.09837132692337036 norm:0.00012344252900220454 max memory_allocated 22566.63232421875 
[2025-02-28 18:10:44 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 8 loss:0.09831250458955765 norm:0.00011757775791920722 max memory_allocated 22566.63232421875 
[2025-02-28 18:11:14 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 9 loss:0.09826747328042984 norm:0.0001087126656784676 max memory_allocated 22566.63232421875 
[2025-02-28 18:11:45 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 10 loss:0.09823193401098251 norm:0.00010538157221162692 max memory_allocated 22566.63232421875 
[2025-02-28 18:12:16 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 11 loss:0.09819686412811279 norm:0.0001031408246490173 max memory_allocated 22566.63232421875 
[2025-02-28 18:12:46 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 12 loss:0.0981643795967102 norm:0.0001009961633826606 max memory_allocated 22566.63232421875 
[2025-02-28 18:13:17 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 13 loss:0.09813564270734787 norm:9.887412306852639e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:13:47 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 14 loss:0.09810259938240051 norm:9.39816382015124e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:14:18 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 15 loss:0.09807850420475006 norm:9.109327220357955e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:14:49 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 16 loss:0.09805566817522049 norm:8.928305032895878e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:15:19 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 17 loss:0.09804310649633408 norm:8.97538848221302e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:15:50 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 18 loss:0.09803304076194763 norm:8.713049464859068e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:16:21 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 19 loss:0.09802762418985367 norm:8.538091060472652e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:16:29 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-02-28 18:16:31 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:17:02 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 0 loss:0.12140034139156342 norm:0.003951190039515495 max memory_allocated 22566.91943359375 
[2025-02-28 18:17:33 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 1 loss:0.11902954429388046 norm:0.003087257966399193 max memory_allocated 22566.91943359375 
[2025-02-28 18:18:04 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 2 loss:0.11714807897806168 norm:0.0025750778149813414 max memory_allocated 22566.91943359375 
[2025-02-28 18:18:34 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 3 loss:0.11540714651346207 norm:0.0021580823231488466 max memory_allocated 22566.91943359375 
[2025-02-28 18:19:05 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 4 loss:0.11469376087188721 norm:0.0017657645512372255 max memory_allocated 22566.91943359375 
[2025-02-28 18:19:36 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 5 loss:0.11444306373596191 norm:0.0014883149415254593 max memory_allocated 22566.91943359375 
[2025-02-28 18:20:07 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 6 loss:0.1143091544508934 norm:0.0013631629990413785 max memory_allocated 22566.91943359375 
[2025-02-28 18:20:37 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 7 loss:0.11431144177913666 norm:0.0013980693183839321 max memory_allocated 22566.91943359375 
[2025-02-28 18:21:08 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 8 loss:0.11423302441835403 norm:0.0013956250622868538 max memory_allocated 22566.91943359375 
[2025-02-28 18:21:39 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 9 loss:0.11424218118190765 norm:0.0011689721141010523 max memory_allocated 22566.91943359375 
[2025-02-28 18:22:10 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 10 loss:0.11418399214744568 norm:0.0010651308111846447 max memory_allocated 22566.91943359375 
[2025-02-28 18:22:40 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 11 loss:0.1141032725572586 norm:0.0010104544926434755 max memory_allocated 22566.91943359375 
[2025-02-28 18:23:11 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 12 loss:0.11397800594568253 norm:0.0011374951573088765 max memory_allocated 22566.91943359375 
[2025-02-28 18:23:42 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 13 loss:0.11395826935768127 norm:0.0010565351694822311 max memory_allocated 22566.91943359375 
[2025-02-28 18:24:13 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 14 loss:0.11395666003227234 norm:0.0011330004781484604 max memory_allocated 22566.91943359375 
[2025-02-28 18:24:44 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 15 loss:0.1139439195394516 norm:0.0010813698172569275 max memory_allocated 22566.91943359375 
[2025-02-28 18:25:14 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 16 loss:0.11393848061561584 norm:0.001037001726217568 max memory_allocated 22566.91943359375 
[2025-02-28 18:25:45 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 17 loss:0.11389630287885666 norm:0.001055016997270286 max memory_allocated 22566.91943359375 
[2025-02-28 18:26:16 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 18 loss:0.11386020481586456 norm:0.0009965015342459083 max memory_allocated 22566.91943359375 
[2025-02-28 18:26:47 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 19 loss:0.11383220553398132 norm:0.0009872314985841513 max memory_allocated 22566.91943359375 
[2025-02-28 18:26:55 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-02-28 18:26:57 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:27:28 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 0 loss:0.14793069660663605 norm:0.009499678388237953 max memory_allocated 22567.09130859375 
[2025-02-28 18:27:59 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 1 loss:0.14064864814281464 norm:0.006996768061071634 max memory_allocated 22567.09130859375 
[2025-02-28 18:28:30 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 2 loss:0.13658565282821655 norm:0.004683467093855143 max memory_allocated 22567.09130859375 
[2025-02-28 18:29:00 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 3 loss:0.1343219131231308 norm:0.003615844063460827 max memory_allocated 22567.09130859375 
[2025-02-28 18:29:31 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 4 loss:0.1334409862756729 norm:0.003045211546123028 max memory_allocated 22567.09130859375 
[2025-02-28 18:30:02 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 5 loss:0.1329667568206787 norm:0.0025354346726089716 max memory_allocated 22567.09130859375 
[2025-02-28 18:30:33 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 6 loss:0.13256779313087463 norm:0.002078775782138109 max memory_allocated 22567.09130859375 
[2025-02-28 18:31:03 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 7 loss:0.1323177069425583 norm:0.0016219241078943014 max memory_allocated 22567.09130859375 
[2025-02-28 18:31:34 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 8 loss:0.1321498453617096 norm:0.0013155725318938494 max memory_allocated 22567.09130859375 
[2025-02-28 18:32:05 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 9 loss:0.1320773810148239 norm:0.0011629689252004027 max memory_allocated 22567.09130859375 
[2025-02-28 18:32:36 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 10 loss:0.13201619684696198 norm:0.001183086889795959 max memory_allocated 22567.09130859375 
[2025-02-28 18:33:07 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 11 loss:0.13192187249660492 norm:0.0010084635578095913 max memory_allocated 22567.09130859375 
[2025-02-28 18:33:37 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 12 loss:0.13182614743709564 norm:0.00093174068024382 max memory_allocated 22567.09130859375 
[2025-02-28 18:34:08 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 13 loss:0.1317586451768875 norm:0.0009155136649496853 max memory_allocated 22567.09130859375 
[2025-02-28 18:34:39 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 14 loss:0.1317281872034073 norm:0.0008344986708834767 max memory_allocated 22567.09130859375 
[2025-02-28 18:35:10 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 15 loss:0.1316744089126587 norm:0.0007685153395868838 max memory_allocated 22567.09130859375 
[2025-02-28 18:35:40 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 16 loss:0.1316554844379425 norm:0.0007435400621034205 max memory_allocated 22567.09130859375 
[2025-02-28 18:36:11 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 17 loss:0.13161879777908325 norm:0.0007167805451899767 max memory_allocated 22567.09130859375 
[2025-02-28 18:36:42 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 18 loss:0.13160516321659088 norm:0.0007015416049398482 max memory_allocated 22567.09130859375 
[2025-02-28 18:37:13 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 19 loss:0.13160474598407745 norm:0.0007260136771947145 max memory_allocated 22567.09130859375 
[2025-02-28 18:37:27 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-02-28 18:37:29 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:38:01 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 0 loss:1.1052418947219849 norm:0.19148513674736023 max memory_allocated 22567.26318359375 
[2025-02-28 18:38:32 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 1 loss:0.7684646844863892 norm:0.16687354445457458 max memory_allocated 22567.26318359375 
[2025-02-28 18:39:02 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 2 loss:0.39011669158935547 norm:0.06771400570869446 max memory_allocated 22567.26318359375 
[2025-02-28 18:39:33 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 3 loss:0.2886247932910919 norm:0.044484518468379974 max memory_allocated 22567.26318359375 
[2025-02-28 18:40:04 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 4 loss:0.2547827959060669 norm:0.04377944767475128 max memory_allocated 22567.26318359375 
[2025-02-28 18:40:35 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 5 loss:0.23735061287879944 norm:0.042233433574438095 max memory_allocated 22567.26318359375 
[2025-02-28 18:41:05 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 6 loss:0.2249288707971573 norm:0.04019355773925781 max memory_allocated 22567.26318359375 
[2025-02-28 18:41:36 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 7 loss:0.2167421132326126 norm:0.03873855993151665 max memory_allocated 22567.26318359375 
[2025-02-28 18:42:07 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 8 loss:0.21067693829536438 norm:0.03698524087667465 max memory_allocated 22567.26318359375 
[2025-02-28 18:42:38 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 9 loss:0.20535632967948914 norm:0.03541383147239685 max memory_allocated 22567.26318359375 
[2025-02-28 18:43:09 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 10 loss:0.201834037899971 norm:0.03236967697739601 max memory_allocated 22567.26318359375 
[2025-02-28 18:43:39 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 11 loss:0.19914834201335907 norm:0.03141148388385773 max memory_allocated 22567.26318359375 
[2025-02-28 18:44:10 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 12 loss:0.19666525721549988 norm:0.028416559100151062 max memory_allocated 22567.26318359375 
[2025-02-28 18:44:41 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 13 loss:0.19463154673576355 norm:0.025575997307896614 max memory_allocated 22567.26318359375 
[2025-02-28 18:45:12 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 14 loss:0.19380690157413483 norm:0.024576881900429726 max memory_allocated 22567.26318359375 
[2025-02-28 18:45:42 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 15 loss:0.19250242412090302 norm:0.022026481106877327 max memory_allocated 22567.26318359375 
[2025-02-28 18:46:13 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 16 loss:0.1916116625070572 norm:0.022216621786355972 max memory_allocated 22567.26318359375 
[2025-02-28 18:46:44 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 17 loss:0.19119243323802948 norm:0.021223029121756554 max memory_allocated 22567.26318359375 
[2025-02-28 18:47:15 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 18 loss:0.19123083353042603 norm:0.02194557711482048 max memory_allocated 22567.26318359375 
[2025-02-28 18:47:46 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 19 loss:0.19126340746879578 norm:0.021905910223722458 max memory_allocated 22567.26318359375 
[2025-02-28 18:47:54 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-02-28 18:47:56 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:48:27 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 0 loss:0.3678179979324341 norm:0.02716662362217903 max memory_allocated 22567.43505859375 
[2025-02-28 18:48:58 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 1 loss:0.3455704152584076 norm:0.01970146968960762 max memory_allocated 22567.43505859375 
[2025-02-28 18:49:29 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 2 loss:0.3336856961250305 norm:0.016105065122246742 max memory_allocated 22567.43505859375 
[2025-02-28 18:49:59 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 3 loss:0.3257877230644226 norm:0.013307408429682255 max memory_allocated 22567.43505859375 
[2025-02-28 18:50:30 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 4 loss:0.3208434581756592 norm:0.011273900978267193 max memory_allocated 22567.43505859375 
[2025-02-28 18:51:01 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 5 loss:0.3176882266998291 norm:0.01037035696208477 max memory_allocated 22567.43505859375 
[2025-02-28 18:51:32 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 6 loss:0.3157740831375122 norm:0.009868624620139599 max memory_allocated 22567.43505859375 
[2025-02-28 18:52:03 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 7 loss:0.31436535716056824 norm:0.009814461693167686 max memory_allocated 22567.43505859375 
[2025-02-28 18:52:33 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 8 loss:0.3136187493801117 norm:0.009784515015780926 max memory_allocated 22567.43505859375 
[2025-02-28 18:53:04 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 9 loss:0.3126066029071808 norm:0.009332826361060143 max memory_allocated 22567.43505859375 
[2025-02-28 18:53:35 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 10 loss:0.311916708946228 norm:0.009241720661520958 max memory_allocated 22567.43505859375 
[2025-02-28 18:54:06 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 11 loss:0.3115605115890503 norm:0.00929797999560833 max memory_allocated 22567.43505859375 
[2025-02-28 18:54:36 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 12 loss:0.31126782298088074 norm:0.009305519051849842 max memory_allocated 22567.43505859375 
[2025-02-28 18:55:07 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 13 loss:0.3110333979129791 norm:0.00946107879281044 max memory_allocated 22567.43505859375 
[2025-02-28 18:55:38 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 14 loss:0.31089600920677185 norm:0.00942570436745882 max memory_allocated 22567.43505859375 
[2025-02-28 18:56:09 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 15 loss:0.3107306957244873 norm:0.009287412278354168 max memory_allocated 22567.43505859375 
[2025-02-28 18:56:40 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 16 loss:0.31029364466667175 norm:0.008955825120210648 max memory_allocated 22567.43505859375 
[2025-02-28 18:57:10 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 17 loss:0.31004393100738525 norm:0.008875482715666294 max memory_allocated 22567.43505859375 
[2025-02-28 18:57:41 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 18 loss:0.3098605275154114 norm:0.008657421916723251 max memory_allocated 22567.43505859375 
[2025-02-28 18:58:12 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 19 loss:0.30997905135154724 norm:0.008943266235291958 max memory_allocated 22567.43505859375 
[2025-02-28 18:58:20 root] (main_calib_config2.py 380): INFO 19995.59391260147
[2025-02-28 18:58:25 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-28 18:59:28 root] (main_calib_config2.py 159): INFO wikitext2 : 5.635982990264893
[2025-02-28 18:59:28 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-28 19:01:06 root] (main_calib_config2.py 159): INFO c4 : 7.200427055358887
[2025-02-28 19:01:16 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/super_glue/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed (last modified on Tue Feb 18 02:57:49 2025) since it couldn't be found locally at super_glue., or remotely on the Hugging Face Hub.
[2025-02-28 20:43:23 root] (main_calib_config2.py 170): INFO {'wikitext2': 5.635982990264893, 'c4': 7.200427055358887, 'results': {'boolq': {'acc': 0.6749235474006117, 'acc_stderr': 0.008192427107041338}, 'arc_easy': {'acc': 0.656986531986532, 'acc_stderr': 0.009740965666489224, 'acc_norm': 0.5155723905723906, 'acc_norm_stderr': 0.0102548063319619}, 'arc_challenge': {'acc': 0.38139931740614336, 'acc_stderr': 0.014194389086685265, 'acc_norm': 0.40187713310580203, 'acc_norm_stderr': 0.014327268614578274}, 'winogrande': {'acc': 0.6716653512233622, 'acc_stderr': 0.013198299449717886}, 'piqa': {'acc': 0.7763873775843307, 'acc_stderr': 0.009721489519176302, 'acc_norm': 0.7665941240478781, 'acc_norm_stderr': 0.009869247889520993}, 'hellaswag': {'acc': 0.5579565823541127, 'acc_stderr': 0.0049561470461089675, 'acc_norm': 0.7151961760605458, 'acc_norm_stderr': 0.0045039858390419785}}, 'versions': {'boolq': 1, 'arc_easy': 0, 'arc_challenge': 0, 'winogrande': 0, 'piqa': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
