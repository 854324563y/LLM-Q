[2025-02-28 13:26:30 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/Llama-2-7b-hf_0.95', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.95.pkl', scale_calibration=True, compensation_calibration=True)
[2025-02-28 13:26:43 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-02-28 13:26:43 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-28 13:26:44 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-02-28 13:26:44 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.95.pkl
[2025-02-28 13:26:45 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-02-28 13:26:48 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:27:19 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 0 loss:0.003144044429063797 norm:0.003845520317554474 max memory_allocated 22562.10693359375 
[2025-02-28 13:27:50 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 1 loss:0.0016107277479022741 norm:0.0016445860965177417 max memory_allocated 22562.10693359375 
[2025-02-28 13:28:20 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 2 loss:0.0012839862611144781 norm:0.0016489755362272263 max memory_allocated 22562.10693359375 
[2025-02-28 13:28:51 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 3 loss:0.0011361745418980718 norm:0.0014893367188051343 max memory_allocated 22562.10693359375 
[2025-02-28 13:29:22 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 4 loss:0.0010591214522719383 norm:0.0013803348410874605 max memory_allocated 22562.10693359375 
[2025-02-28 13:29:52 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 5 loss:0.001038484857417643 norm:0.0013961161021143198 max memory_allocated 22562.10693359375 
[2025-02-28 13:30:23 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 6 loss:0.0010060409549623728 norm:0.0013223839923739433 max memory_allocated 22562.10693359375 
[2025-02-28 13:30:54 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 7 loss:0.0009869757341220975 norm:0.0012753941118717194 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:25 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 8 loss:0.0009658254566602409 norm:0.0011743156937882304 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:55 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 9 loss:0.0009584462968632579 norm:0.0011149914935231209 max memory_allocated 22562.10693359375 
[2025-02-28 13:32:26 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 10 loss:0.0009514286066405475 norm:0.0011113965883851051 max memory_allocated 22562.10693359375 
[2025-02-28 13:32:57 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 11 loss:0.0009415685781277716 norm:0.0010236758971586823 max memory_allocated 22562.10693359375 
[2025-02-28 13:33:28 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 12 loss:0.0009279813966713846 norm:0.0009654502500779927 max memory_allocated 22562.10693359375 
[2025-02-28 13:33:59 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 13 loss:0.0009204499656334519 norm:0.000909266120288521 max memory_allocated 22562.10693359375 
[2025-02-28 13:34:29 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 14 loss:0.0009119859314523637 norm:0.0008370940340682864 max memory_allocated 22562.10693359375 
[2025-02-28 13:35:00 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 15 loss:0.0009159243199974298 norm:0.000836686696857214 max memory_allocated 22562.10693359375 
[2025-02-28 13:35:31 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 16 loss:0.0008987085893750191 norm:0.000729711668100208 max memory_allocated 22562.10693359375 
[2025-02-28 13:36:02 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 17 loss:0.0008971105562523007 norm:0.0007038717158138752 max memory_allocated 22562.10693359375 
[2025-02-28 13:36:32 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 18 loss:0.0008845226839184761 norm:0.0006399581325240433 max memory_allocated 22562.10693359375 
[2025-02-28 13:37:03 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 19 loss:0.0008816124172881246 norm:0.0006062364554964006 max memory_allocated 22562.10693359375 
[2025-02-28 13:37:14 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-02-28 13:37:16 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:37:51 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 0 loss:0.024825112894177437 norm:0.012846387922763824 max memory_allocated 22562.27880859375 
[2025-02-28 13:38:22 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 1 loss:0.016175804659724236 norm:0.009885547682642937 max memory_allocated 22562.27880859375 
[2025-02-28 13:38:52 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 2 loss:0.012421888299286366 norm:0.00962562020868063 max memory_allocated 22562.27880859375 
[2025-02-28 13:39:23 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 3 loss:0.013602187857031822 norm:0.012786131352186203 max memory_allocated 22562.27880859375 
[2025-02-28 13:39:54 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 4 loss:0.013144605793058872 norm:0.010217747651040554 max memory_allocated 22562.27880859375 
[2025-02-28 13:40:25 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 5 loss:0.011979023925960064 norm:0.008319183252751827 max memory_allocated 22562.27880859375 
[2025-02-28 13:40:55 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 6 loss:0.010823212563991547 norm:0.007342903409153223 max memory_allocated 22562.27880859375 
[2025-02-28 13:41:26 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 7 loss:0.011200698092579842 norm:0.007010364904999733 max memory_allocated 22562.27880859375 
[2025-02-28 13:41:57 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 8 loss:0.010587643831968307 norm:0.006852090824395418 max memory_allocated 22562.27880859375 
[2025-02-28 13:42:28 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 9 loss:0.011282053776085377 norm:0.0070945462211966515 max memory_allocated 22562.27880859375 
[2025-02-28 13:42:58 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 10 loss:0.01208341121673584 norm:0.006854356732219458 max memory_allocated 22562.27880859375 
[2025-02-28 13:43:29 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 11 loss:0.012223595753312111 norm:0.006757202558219433 max memory_allocated 22562.27880859375 
[2025-02-28 13:44:00 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 12 loss:0.011423159390687943 norm:0.006698953919112682 max memory_allocated 22562.27880859375 
[2025-02-28 13:44:31 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 13 loss:0.0111826341599226 norm:0.006543880328536034 max memory_allocated 22562.27880859375 
[2025-02-28 13:45:01 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 14 loss:0.010675487108528614 norm:0.006194727495312691 max memory_allocated 22562.27880859375 
[2025-02-28 13:45:32 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 15 loss:0.010684853419661522 norm:0.00580567354336381 max memory_allocated 22562.27880859375 
[2025-02-28 13:46:03 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 16 loss:0.012604648247361183 norm:0.00634361244738102 max memory_allocated 22562.27880859375 
[2025-02-28 13:46:34 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 17 loss:0.011765336617827415 norm:0.006029277108609676 max memory_allocated 22562.27880859375 
[2025-02-28 13:47:04 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 18 loss:0.011364556849002838 norm:0.006236448884010315 max memory_allocated 22562.27880859375 
[2025-02-28 13:47:35 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 19 loss:0.011947599239647388 norm:0.006022007204592228 max memory_allocated 22562.27880859375 
[2025-02-28 13:47:43 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-02-28 13:47:46 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:48:17 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 0 loss:0.015373121947050095 norm:0.0066786338575184345 max memory_allocated 22562.45068359375 
[2025-02-28 13:48:47 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 1 loss:0.011918287724256516 norm:0.004827238153666258 max memory_allocated 22562.45068359375 
[2025-02-28 13:49:18 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 2 loss:0.011151377111673355 norm:0.0037287313025444746 max memory_allocated 22562.45068359375 
[2025-02-28 13:49:49 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 3 loss:0.010748124681413174 norm:0.0030496111139655113 max memory_allocated 22562.45068359375 
[2025-02-28 13:50:20 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 4 loss:0.01044504065066576 norm:0.0025268951430916786 max memory_allocated 22562.45068359375 
[2025-02-28 13:50:50 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 5 loss:0.010175500996410847 norm:0.0020504260901361704 max memory_allocated 22562.45068359375 
[2025-02-28 13:51:21 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 6 loss:0.009996545501053333 norm:0.0016277104150503874 max memory_allocated 22562.45068359375 
[2025-02-28 13:51:52 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 7 loss:0.009879229590296745 norm:0.0012737466022372246 max memory_allocated 22562.45068359375 
[2025-02-28 13:52:23 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 8 loss:0.009803110733628273 norm:0.0009692671010270715 max memory_allocated 22562.45068359375 
[2025-02-28 13:52:53 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 9 loss:0.009768054820597172 norm:0.0008348251576535404 max memory_allocated 22562.45068359375 
[2025-02-28 13:53:24 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 10 loss:0.009840530343353748 norm:0.0010325423208996654 max memory_allocated 22562.45068359375 
[2025-02-28 13:53:55 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 11 loss:0.009822656400501728 norm:0.0009005460888147354 max memory_allocated 22562.45068359375 
[2025-02-28 13:54:26 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 12 loss:0.009743593633174896 norm:0.0007962727686390281 max memory_allocated 22562.45068359375 
[2025-02-28 13:54:56 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 13 loss:0.009742660447955132 norm:0.0007979563670232892 max memory_allocated 22562.45068359375 
[2025-02-28 13:55:27 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 14 loss:0.009764503687620163 norm:0.0008548099431209266 max memory_allocated 22562.45068359375 
[2025-02-28 13:55:58 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 15 loss:0.009766878560185432 norm:0.0008683553314767778 max memory_allocated 22562.45068359375 
[2025-02-28 13:56:29 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 16 loss:0.009778660722076893 norm:0.0008449477609246969 max memory_allocated 22562.45068359375 
[2025-02-28 13:56:59 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 17 loss:0.009776987135410309 norm:0.0008044742280617356 max memory_allocated 22562.45068359375 
[2025-02-28 13:57:30 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 18 loss:0.009769919328391552 norm:0.0008049643947742879 max memory_allocated 22562.45068359375 
[2025-02-28 13:58:01 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 19 loss:0.009780955500900745 norm:0.0008106682798825204 max memory_allocated 22562.45068359375 
[2025-02-28 13:58:09 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-02-28 13:58:42 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 0 loss:0.015713218599557877 norm:0.0012331287143751979 max memory_allocated 22562.50732421875 
[2025-02-28 13:59:13 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 1 loss:0.013197999447584152 norm:0.0005328567349351943 max memory_allocated 22562.50732421875 
[2025-02-28 13:59:43 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 2 loss:0.01246938668191433 norm:0.0003058724687434733 max memory_allocated 22562.50732421875 
[2025-02-28 14:00:14 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 3 loss:0.012072586454451084 norm:0.00020506612781900913 max memory_allocated 22562.50732421875 
[2025-02-28 14:00:45 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 4 loss:0.011747470125555992 norm:0.0001328623911831528 max memory_allocated 22562.50732421875 
[2025-02-28 14:01:15 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 5 loss:0.011554425582289696 norm:0.00011709470709320158 max memory_allocated 22562.50732421875 
[2025-02-28 14:01:46 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 6 loss:0.011489253491163254 norm:0.00012229775893501937 max memory_allocated 22562.50732421875 
[2025-02-28 14:02:17 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 7 loss:0.011454996652901173 norm:9.494902042206377e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:02:47 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 8 loss:0.011435529217123985 norm:9.013153612613678e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:03:18 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 9 loss:0.011434429325163364 norm:9.920125739881769e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:03:48 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 10 loss:0.011427154764533043 norm:9.48985107243061e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:04:19 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 11 loss:0.011416420340538025 norm:8.757332398090512e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:04:50 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 12 loss:0.011402680538594723 norm:0.000496800581458956 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:20 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 13 loss:0.011418243870139122 norm:8.982466533780098e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:51 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 14 loss:0.011398300528526306 norm:8.38929699966684e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:06:22 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 15 loss:0.011396507732570171 norm:8.585897012380883e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:06:52 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 16 loss:0.011394694447517395 norm:8.780095231486484e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:07:23 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 17 loss:0.011387384496629238 norm:8.693913696333766e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:07:54 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 18 loss:0.011394884437322617 norm:8.495584916090593e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:08:24 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 19 loss:0.011395291425287724 norm:8.45616523292847e-05 max memory_allocated 22562.50732421875 
[2025-02-28 14:08:32 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-02-28 14:09:06 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 0 loss:0.017930183559656143 norm:0.001032265485264361 max memory_allocated 22562.67919921875 
[2025-02-28 14:09:36 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 1 loss:0.015161837451159954 norm:0.0004410352557897568 max memory_allocated 22562.67919921875 
[2025-02-28 14:10:07 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 2 loss:0.014180372469127178 norm:0.00024023123842198402 max memory_allocated 22562.67919921875 
[2025-02-28 14:10:38 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 3 loss:0.013674430549144745 norm:0.0001607482263352722 max memory_allocated 22562.67919921875 
[2025-02-28 14:11:08 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 4 loss:0.013325603678822517 norm:0.00012572370178531855 max memory_allocated 22562.67919921875 
[2025-02-28 14:11:39 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 5 loss:0.013153217732906342 norm:0.00011125872697448358 max memory_allocated 22562.67919921875 
[2025-02-28 14:12:10 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 6 loss:0.013079492375254631 norm:9.379247785545886e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:12:40 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 7 loss:0.01303726527839899 norm:8.611350494902581e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:13:11 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 8 loss:0.013018013909459114 norm:8.272588456748053e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:13:41 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 9 loss:0.013009040616452694 norm:8.295418228954077e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:14:12 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 10 loss:0.012991713359951973 norm:7.91498605394736e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:14:43 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 11 loss:0.012989947572350502 norm:8.181673911167309e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:15:13 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 12 loss:0.012993324548006058 norm:8.557156979804859e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:15:44 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 13 loss:0.013002945110201836 norm:8.423750841757283e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:16:15 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 14 loss:0.013003818690776825 norm:8.401578816119581e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:16:45 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 15 loss:0.013007340021431446 norm:8.255974535131827e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:17:16 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 16 loss:0.01299968734383583 norm:8.520066330675036e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:17:47 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 17 loss:0.013004583306610584 norm:8.488944877171889e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:18:17 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 18 loss:0.013007049448788166 norm:8.235242421505973e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:18:48 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 19 loss:0.01300562359392643 norm:8.23512818897143e-05 max memory_allocated 22562.67919921875 
[2025-02-28 14:18:56 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-02-28 14:19:29 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 0 loss:0.018207041546702385 norm:0.0008693290874361992 max memory_allocated 22562.85107421875 
[2025-02-28 14:20:00 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 1 loss:0.016180798411369324 norm:0.0004031754797324538 max memory_allocated 22562.85107421875 
[2025-02-28 14:20:31 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 2 loss:0.015335974283516407 norm:0.0002303551882505417 max memory_allocated 22562.85107421875 
[2025-02-28 14:21:01 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 3 loss:0.014843548648059368 norm:0.00014866508718114346 max memory_allocated 22562.85107421875 
[2025-02-28 14:21:32 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 4 loss:0.014496268704533577 norm:0.00011317102325847372 max memory_allocated 22562.85107421875 
[2025-02-28 14:22:03 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 5 loss:0.014336753636598587 norm:9.562271588947624e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:22:33 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 6 loss:0.014274151995778084 norm:9.482244786340743e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:23:04 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 7 loss:0.014235865324735641 norm:7.798257865943015e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:23:34 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 8 loss:0.014221366494894028 norm:7.825512147974223e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:24:05 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 9 loss:0.014211689122021198 norm:7.630109030287713e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:24:36 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 10 loss:0.014200780540704727 norm:7.835523138055578e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:25:06 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 11 loss:0.014191318303346634 norm:7.9098463174887e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:25:37 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 12 loss:0.01418263092637062 norm:7.641258707735687e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:26:08 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 13 loss:0.014181104488670826 norm:7.967658166307956e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:26:38 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 14 loss:0.014183931052684784 norm:8.498341776430607e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:09 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 15 loss:0.014173519797623158 norm:8.080391125986353e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:39 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 16 loss:0.014173326082527637 norm:8.849299047142267e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:28:10 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 17 loss:0.014170797541737556 norm:8.207725477404892e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:28:41 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 18 loss:0.014175188727676868 norm:8.980566053651273e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:29:11 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 19 loss:0.014165950007736683 norm:8.355172030860558e-05 max memory_allocated 22562.85107421875 
[2025-02-28 14:29:20 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-02-28 14:29:53 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 0 loss:0.020449597388505936 norm:0.0009532560361549258 max memory_allocated 22563.02294921875 
[2025-02-28 14:30:23 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 1 loss:0.01809837482869625 norm:0.0004562113608699292 max memory_allocated 22563.02294921875 
[2025-02-28 14:30:54 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 2 loss:0.017135635018348694 norm:0.00026200604042969644 max memory_allocated 22563.02294921875 
[2025-02-28 14:31:25 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 3 loss:0.016579926013946533 norm:0.00018138645100407302 max memory_allocated 22563.02294921875 
[2025-02-28 14:31:55 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 4 loss:0.016195712611079216 norm:0.00013118756760377437 max memory_allocated 22563.02294921875 
[2025-02-28 14:32:26 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 5 loss:0.015999874100089073 norm:0.00011590780195547268 max memory_allocated 22563.02294921875 
[2025-02-28 14:32:57 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 6 loss:0.01589866168797016 norm:0.00010184053826378658 max memory_allocated 22563.02294921875 
[2025-02-28 14:33:27 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 7 loss:0.015844622626900673 norm:9.26171342143789e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:33:58 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 8 loss:0.015798039734363556 norm:8.929569594329223e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:34:29 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 9 loss:0.01577077805995941 norm:8.832780440570787e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:34:59 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 10 loss:0.01575421914458275 norm:9.25672211451456e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:35:30 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 11 loss:0.015753744170069695 norm:9.171666897600517e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:36:01 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 12 loss:0.015738345682621002 norm:8.552850340493023e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:36:31 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 13 loss:0.01572963409125805 norm:7.927374099381268e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:37:02 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 14 loss:0.015709716826677322 norm:7.697143155382946e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:37:32 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 15 loss:0.015705667436122894 norm:8.067569433478639e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:38:03 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 16 loss:0.01570943370461464 norm:8.222507312893867e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:38:34 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 17 loss:0.01570427604019642 norm:8.364882523892447e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:39:04 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 18 loss:0.015704892575740814 norm:8.44448950374499e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:39:35 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 19 loss:0.015694504603743553 norm:8.051386248553172e-05 max memory_allocated 22563.02294921875 
[2025-02-28 14:39:43 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-02-28 14:40:16 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 0 loss:0.023308807983994484 norm:0.0011061570839956403 max memory_allocated 22563.19482421875 
[2025-02-28 14:40:47 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 1 loss:0.02016041986644268 norm:0.0005034729256294668 max memory_allocated 22563.19482421875 
[2025-02-28 14:41:18 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 2 loss:0.01900646835565567 norm:0.000311687879730016 max memory_allocated 22563.19482421875 
[2025-02-28 14:41:48 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 3 loss:0.018352240324020386 norm:0.00021378681412898004 max memory_allocated 22563.19482421875 
[2025-02-28 14:42:19 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 4 loss:0.017947467043995857 norm:0.00017216100241057575 max memory_allocated 22563.19482421875 
[2025-02-28 14:42:50 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 5 loss:0.017717143520712852 norm:0.0001403678470524028 max memory_allocated 22563.19482421875 
[2025-02-28 14:43:20 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 6 loss:0.017609640955924988 norm:0.00011831554002128541 max memory_allocated 22563.19482421875 
[2025-02-28 14:43:51 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 7 loss:0.017554335296154022 norm:0.00011913599882973358 max memory_allocated 22563.19482421875 
[2025-02-28 14:44:21 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 8 loss:0.017489459365606308 norm:0.00010296787513652816 max memory_allocated 22563.19482421875 
[2025-02-28 14:44:52 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 9 loss:0.01744183525443077 norm:9.581750782672316e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:45:23 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 10 loss:0.017415963113307953 norm:9.684931137599051e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:45:53 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 11 loss:0.017385568469762802 norm:9.6025803941302e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:46:24 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 12 loss:0.017374642193317413 norm:8.753878501011059e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:46:55 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 13 loss:0.01735459826886654 norm:8.366895781364292e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:47:25 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 14 loss:0.017341308295726776 norm:8.483575220452622e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:47:56 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 15 loss:0.01733904518187046 norm:8.374916797038168e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:48:27 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 16 loss:0.017334818840026855 norm:8.279659959953278e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:48:57 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 17 loss:0.017329782247543335 norm:8.416220953222364e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:49:28 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 18 loss:0.017322994768619537 norm:8.979829726740718e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:49:58 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 19 loss:0.01730966567993164 norm:8.79197905305773e-05 max memory_allocated 22563.19482421875 
[2025-02-28 14:50:07 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-02-28 14:50:40 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 0 loss:0.02462109923362732 norm:0.0008663145126774907 max memory_allocated 22563.36669921875 
[2025-02-28 14:51:10 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 1 loss:0.02187245339155197 norm:0.00043919734889641404 max memory_allocated 22563.36669921875 
[2025-02-28 14:51:41 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 2 loss:0.020637327805161476 norm:0.00026679859729483724 max memory_allocated 22563.36669921875 
[2025-02-28 14:52:12 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 3 loss:0.01996932178735733 norm:0.00018990573880728334 max memory_allocated 22563.36669921875 
[2025-02-28 14:52:42 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 4 loss:0.019585873931646347 norm:0.00014591800572816283 max memory_allocated 22563.36669921875 
[2025-02-28 14:53:13 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 5 loss:0.01937364600598812 norm:0.00012092960241716355 max memory_allocated 22563.36669921875 
[2025-02-28 14:53:44 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 6 loss:0.019252736121416092 norm:0.0001079376379493624 max memory_allocated 22563.36669921875 
[2025-02-28 14:54:14 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 7 loss:0.019168296828866005 norm:0.00010498014307813719 max memory_allocated 22563.36669921875 
[2025-02-28 14:54:45 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 8 loss:0.0191313698887825 norm:0.00010179010860156268 max memory_allocated 22563.36669921875 
[2025-02-28 14:55:15 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 9 loss:0.019088471308350563 norm:0.00010108272545039654 max memory_allocated 22563.36669921875 
[2025-02-28 14:55:46 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 10 loss:0.019063403829932213 norm:9.495802805759013e-05 max memory_allocated 22563.36669921875 
[2025-02-28 14:56:17 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 11 loss:0.01906357891857624 norm:9.120325557887554e-05 max memory_allocated 22563.36669921875 
[2025-02-28 14:56:47 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 12 loss:0.0190473310649395 norm:8.908093150239438e-05 max memory_allocated 22563.36669921875 
[2025-02-28 14:57:18 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 13 loss:0.0190325528383255 norm:8.768089901423082e-05 max memory_allocated 22563.36669921875 
[2025-02-28 14:57:49 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 14 loss:0.019019506871700287 norm:8.931868796935305e-05 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:19 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 15 loss:0.019010469317436218 norm:8.910372707759961e-05 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:50 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 16 loss:0.019006557762622833 norm:8.699521276867017e-05 max memory_allocated 22563.36669921875 
[2025-02-28 14:59:21 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 17 loss:0.01900528371334076 norm:8.931724005378783e-05 max memory_allocated 22563.36669921875 
[2025-02-28 14:59:51 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 18 loss:0.01899876445531845 norm:8.973838703241199e-05 max memory_allocated 22563.36669921875 
[2025-02-28 15:00:22 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 19 loss:0.01899634301662445 norm:8.964188600657508e-05 max memory_allocated 22563.36669921875 
[2025-02-28 15:00:30 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-02-28 15:01:03 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 0 loss:0.025475572794675827 norm:0.0008433447801508009 max memory_allocated 22563.53857421875 
[2025-02-28 15:01:34 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 1 loss:0.022769233211874962 norm:0.0003734233614522964 max memory_allocated 22563.53857421875 
[2025-02-28 15:02:05 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 2 loss:0.021770240738987923 norm:0.00022642529802396894 max memory_allocated 22563.53857421875 
[2025-02-28 15:02:35 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 3 loss:0.021181339398026466 norm:0.00016956502804532647 max memory_allocated 22563.53857421875 
[2025-02-28 15:03:06 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 4 loss:0.020836874842643738 norm:0.00013973095337860286 max memory_allocated 22563.53857421875 
[2025-02-28 15:03:36 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 5 loss:0.020636487752199173 norm:0.00012321502435952425 max memory_allocated 22563.53857421875 
[2025-02-28 15:04:07 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 6 loss:0.020510233938694 norm:0.00010871481208596379 max memory_allocated 22563.53857421875 
[2025-02-28 15:04:38 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 7 loss:0.020468560978770256 norm:0.00010903993825195357 max memory_allocated 22563.53857421875 
[2025-02-28 15:05:08 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 8 loss:0.020414941012859344 norm:9.630039858166128e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:05:39 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 9 loss:0.020372144877910614 norm:9.455594408791512e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:06:10 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 10 loss:0.02034299448132515 norm:8.572898514103144e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:06:40 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 11 loss:0.02033783495426178 norm:8.454652561340481e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:07:11 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 12 loss:0.020321590825915337 norm:8.040424290811643e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:07:42 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 13 loss:0.020304502919316292 norm:8.168884960468858e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:08:12 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 14 loss:0.020292464643716812 norm:7.834878488210961e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:08:43 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 15 loss:0.020290514454245567 norm:7.868077955208719e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:09:13 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 16 loss:0.020285047590732574 norm:7.685139280511066e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:09:44 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 17 loss:0.020274387672543526 norm:7.787885260768235e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:10:15 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 18 loss:0.02027105540037155 norm:7.899042248027399e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:10:45 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 19 loss:0.020270373672246933 norm:8.054974023252726e-05 max memory_allocated 22563.53857421875 
[2025-02-28 15:10:54 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-02-28 15:11:27 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 0 loss:0.02525252290070057 norm:0.0004995822091586888 max memory_allocated 22563.71044921875 
[2025-02-28 15:11:57 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 1 loss:0.023356139659881592 norm:0.0002555798855610192 max memory_allocated 22563.71044921875 
[2025-02-28 15:12:28 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 2 loss:0.022526761516928673 norm:0.0001691228389972821 max memory_allocated 22563.71044921875 
[2025-02-28 15:12:59 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 3 loss:0.02203517034649849 norm:0.00013701291754841805 max memory_allocated 22563.71044921875 
[2025-02-28 15:13:29 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 4 loss:0.02171025052666664 norm:0.0001129714073613286 max memory_allocated 22563.71044921875 
[2025-02-28 15:14:00 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 5 loss:0.02153501659631729 norm:9.530964598525316e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:14:31 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 6 loss:0.021435005590319633 norm:9.185807721223682e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:15:01 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 7 loss:0.021376321092247963 norm:8.60614818520844e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:15:32 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 8 loss:0.021348556503653526 norm:7.882313366280869e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:16:02 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 9 loss:0.02133287861943245 norm:7.97604734543711e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:16:33 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 10 loss:0.021306343376636505 norm:7.448823453160003e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:17:04 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 11 loss:0.021285908296704292 norm:7.783099135849625e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:17:34 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 12 loss:0.021274462342262268 norm:7.481674401788041e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:18:05 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 13 loss:0.02127191238105297 norm:7.248798647196963e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:18:36 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 14 loss:0.02127014473080635 norm:7.359790470218286e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:06 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 15 loss:0.021269677206873894 norm:7.435792940668762e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:37 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 16 loss:0.021272117272019386 norm:7.268970512086526e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:20:08 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 17 loss:0.02126547321677208 norm:7.215155346784741e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:20:38 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 18 loss:0.021269746124744415 norm:7.542179082520306e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:21:09 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 19 loss:0.02127673104405403 norm:7.604171696584672e-05 max memory_allocated 22563.71044921875 
[2025-02-28 15:21:19 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-02-28 15:21:52 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 0 loss:0.02691800892353058 norm:0.0006490309024229646 max memory_allocated 22563.88232421875 
[2025-02-28 15:22:22 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 1 loss:0.024614106863737106 norm:0.0003233181778341532 max memory_allocated 22563.88232421875 
[2025-02-28 15:22:53 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 2 loss:0.023707453161478043 norm:0.00020639656577259302 max memory_allocated 22563.88232421875 
[2025-02-28 15:23:24 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 3 loss:0.02315952070057392 norm:0.00016504156519658864 max memory_allocated 22563.88232421875 
[2025-02-28 15:23:54 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 4 loss:0.02281138487160206 norm:0.00013810727978125215 max memory_allocated 22563.88232421875 
[2025-02-28 15:24:25 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 5 loss:0.02259301394224167 norm:0.0001236997777596116 max memory_allocated 22563.88232421875 
[2025-02-28 15:24:56 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 6 loss:0.022475101053714752 norm:0.00011114393419120461 max memory_allocated 22563.88232421875 
[2025-02-28 15:25:26 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 7 loss:0.022395407781004906 norm:0.00011465534043963999 max memory_allocated 22563.88232421875 
[2025-02-28 15:25:57 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 8 loss:0.0223175510764122 norm:9.74462236627005e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:26:28 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 9 loss:0.022284403443336487 norm:8.822402742225677e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:26:58 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 10 loss:0.022252678871154785 norm:8.911930490285158e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:27:29 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 11 loss:0.022208185866475105 norm:8.612507372163236e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:28:00 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 12 loss:0.02217550203204155 norm:8.352956501767039e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:28:30 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 13 loss:0.022166360169649124 norm:7.755655678920448e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:01 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 14 loss:0.02214379422366619 norm:7.250165799632668e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:32 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 15 loss:0.022133296355605125 norm:7.35984358470887e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:30:02 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 16 loss:0.022129125893115997 norm:7.657062815269455e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:30:33 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 17 loss:0.022116510197520256 norm:7.047157851047814e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:31:03 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 18 loss:0.022118251770734787 norm:6.992967973928899e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:31:34 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 19 loss:0.022116977721452713 norm:7.101911614881828e-05 max memory_allocated 22563.88232421875 
[2025-02-28 15:31:42 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-02-28 15:32:15 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 0 loss:0.02725231647491455 norm:0.0005533241201192141 max memory_allocated 22564.05419921875 
[2025-02-28 15:32:46 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 1 loss:0.025174330919981003 norm:0.00028008135268464684 max memory_allocated 22564.05419921875 
[2025-02-28 15:33:17 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 2 loss:0.024347450584173203 norm:0.00019691175839398056 max memory_allocated 22564.05419921875 
[2025-02-28 15:33:47 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 3 loss:0.02383747324347496 norm:0.00015761233225930482 max memory_allocated 22564.05419921875 
[2025-02-28 15:34:18 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 4 loss:0.023512832820415497 norm:0.0001417614839738235 max memory_allocated 22564.05419921875 
[2025-02-28 15:34:49 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 5 loss:0.023316549137234688 norm:0.00012195747694931924 max memory_allocated 22564.05419921875 
[2025-02-28 15:35:19 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 6 loss:0.0231864545494318 norm:0.00010876497253775597 max memory_allocated 22564.05419921875 
[2025-02-28 15:35:50 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 7 loss:0.02310846745967865 norm:0.00010181459947489202 max memory_allocated 22564.05419921875 
[2025-02-28 15:36:20 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 8 loss:0.02306436002254486 norm:9.318577940575778e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:36:51 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 9 loss:0.02301160804927349 norm:9.008980850921944e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:37:22 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 10 loss:0.022980904206633568 norm:8.558202534914017e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:37:52 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 11 loss:0.022960728034377098 norm:8.76894555403851e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:38:23 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 12 loss:0.022927703335881233 norm:8.163195889210328e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:38:54 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 13 loss:0.02291865274310112 norm:7.872282003518194e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:39:24 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 14 loss:0.022905120626091957 norm:7.1983493398875e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:39:55 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 15 loss:0.022901637479662895 norm:7.483025547116995e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:26 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 16 loss:0.02288561500608921 norm:6.526890501845628e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:56 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 17 loss:0.022882476449012756 norm:6.717172800563276e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:41:27 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 18 loss:0.0228794626891613 norm:6.606331589864567e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:41:58 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 19 loss:0.02288043312728405 norm:6.801447307225317e-05 max memory_allocated 22564.05419921875 
[2025-02-28 15:42:06 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-02-28 15:42:42 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 0 loss:0.02679230086505413 norm:0.0006535936263389885 max memory_allocated 22564.22607421875 
[2025-02-28 15:43:13 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 1 loss:0.02495148405432701 norm:0.00031951660639606416 max memory_allocated 22564.22607421875 
[2025-02-28 15:43:43 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 2 loss:0.024090584367513657 norm:0.00020681861496996135 max memory_allocated 22564.22607421875 
[2025-02-28 15:44:14 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 3 loss:0.023585092276334763 norm:0.00015461504517588764 max memory_allocated 22564.22607421875 
[2025-02-28 15:44:44 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 4 loss:0.023277483880519867 norm:0.00013144145486876369 max memory_allocated 22564.22607421875 
[2025-02-28 15:45:15 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 5 loss:0.023068172857165337 norm:0.00011034686031052843 max memory_allocated 22564.22607421875 
[2025-02-28 15:45:46 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 6 loss:0.022952523082494736 norm:9.969181701308116e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:46:16 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 7 loss:0.0228877030313015 norm:9.281105303671211e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:46:47 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 8 loss:0.022829648107290268 norm:8.691286348039284e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:47:18 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 9 loss:0.022792931646108627 norm:8.400253864238039e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:47:48 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 10 loss:0.022756531834602356 norm:7.710096542723477e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:48:19 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 11 loss:0.022731812670826912 norm:7.701419963268563e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:48:50 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 12 loss:0.022711925208568573 norm:7.483190711354837e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:49:20 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 13 loss:0.022700656205415726 norm:7.215023651951924e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:49:51 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 14 loss:0.022688226774334908 norm:7.008020475041121e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:22 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 15 loss:0.022676018998026848 norm:6.6524458816275e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:52 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 16 loss:0.022665126249194145 norm:6.221685907803476e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:51:23 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 17 loss:0.022658172994852066 norm:6.108870729804039e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:51:54 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 18 loss:0.022654831409454346 norm:6.438916170736775e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:52:24 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 19 loss:0.022647004574537277 norm:6.18823614786379e-05 max memory_allocated 22564.22607421875 
[2025-02-28 15:52:33 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-02-28 15:53:07 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 0 loss:0.0271457452327013 norm:0.00053185666911304 max memory_allocated 22564.39794921875 
[2025-02-28 15:53:37 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 1 loss:0.02527126669883728 norm:0.00026977009838446975 max memory_allocated 22564.39794921875 
[2025-02-28 15:54:08 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 2 loss:0.024505211040377617 norm:0.00018294257461093366 max memory_allocated 22564.39794921875 
[2025-02-28 15:54:39 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 3 loss:0.024026967585086823 norm:0.00014020164962857962 max memory_allocated 22564.39794921875 
[2025-02-28 15:55:09 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 4 loss:0.023706872016191483 norm:0.00011924393766093999 max memory_allocated 22564.39794921875 
[2025-02-28 15:55:40 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 5 loss:0.02350466698408127 norm:0.00011275570432189852 max memory_allocated 22564.39794921875 
[2025-02-28 15:56:11 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 6 loss:0.023379886522889137 norm:9.624454833101481e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:56:41 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 7 loss:0.02329964004456997 norm:8.921626431401819e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:57:12 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 8 loss:0.02324124611914158 norm:8.852263272274286e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:57:43 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 9 loss:0.023192519322037697 norm:9.122784831561148e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:58:13 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 10 loss:0.023166663944721222 norm:8.049049210967496e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:58:44 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 11 loss:0.0231375340372324 norm:7.972799357958138e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:59:14 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 12 loss:0.02310800552368164 norm:7.651967462152243e-05 max memory_allocated 22564.39794921875 
[2025-02-28 15:59:45 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 13 loss:0.023080291226506233 norm:6.729410961270332e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:00:16 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 14 loss:0.023066755384206772 norm:7.052670116536319e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:00:46 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 15 loss:0.02305864356458187 norm:6.382624997058883e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:01:17 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 16 loss:0.023045890033245087 norm:5.738336403737776e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:01:48 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 17 loss:0.023035157471895218 norm:5.5445823818445206e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:02:18 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 18 loss:0.02303244173526764 norm:5.459814201458357e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:02:49 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 19 loss:0.023027922958135605 norm:5.4958825785433874e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:02:57 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-02-28 16:03:31 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 0 loss:0.028227638453245163 norm:0.0008019751403480768 max memory_allocated 22564.56982421875 
[2025-02-28 16:04:01 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 1 loss:0.025701148435473442 norm:0.0003409658093005419 max memory_allocated 22564.56982421875 
[2025-02-28 16:04:32 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 2 loss:0.02469228394329548 norm:0.00022335126413963735 max memory_allocated 22564.56982421875 
[2025-02-28 16:05:03 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 3 loss:0.02412337064743042 norm:0.00015978046576492488 max memory_allocated 22564.56982421875 
[2025-02-28 16:05:33 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 4 loss:0.02377173863351345 norm:0.00013459175534080714 max memory_allocated 22564.56982421875 
[2025-02-28 16:06:04 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 5 loss:0.023562001064419746 norm:0.0001263515732716769 max memory_allocated 22564.56982421875 
[2025-02-28 16:06:35 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 6 loss:0.023438835516572 norm:0.00011175860709045082 max memory_allocated 22564.56982421875 
[2025-02-28 16:07:05 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 7 loss:0.023351144045591354 norm:9.790588228497654e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:07:36 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 8 loss:0.0232878215610981 norm:9.214316378347576e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:08:07 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 9 loss:0.023240212351083755 norm:9.123451309278607e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:08:37 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 10 loss:0.023185407742857933 norm:8.548860932933167e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:09:08 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 11 loss:0.023149127140641212 norm:8.306447125505656e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:09:39 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 12 loss:0.02311401255428791 norm:7.962934614624828e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:10:09 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 13 loss:0.023084506392478943 norm:7.46386795071885e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:10:40 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 14 loss:0.023062413558363914 norm:7.016500603640452e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:11 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 15 loss:0.02304147370159626 norm:6.740906974300742e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:41 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 16 loss:0.023030025884509087 norm:6.615180609514937e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:12:12 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 17 loss:0.023017048835754395 norm:6.506445060949773e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:12:43 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 18 loss:0.023007908836007118 norm:6.187028338899836e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:13:13 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 19 loss:0.02300821989774704 norm:6.22677180217579e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:13:21 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-02-28 16:13:55 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 0 loss:0.028704149648547173 norm:0.0009743428672663867 max memory_allocated 22564.74169921875 
[2025-02-28 16:14:25 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 1 loss:0.025927012786269188 norm:0.00039467442547902465 max memory_allocated 22564.74169921875 
[2025-02-28 16:14:56 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 2 loss:0.024890605360269547 norm:0.00023121957201510668 max memory_allocated 22564.74169921875 
[2025-02-28 16:15:27 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 3 loss:0.024331944063305855 norm:0.0001748028735164553 max memory_allocated 22564.74169921875 
[2025-02-28 16:15:57 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 4 loss:0.023960504680871964 norm:0.00014334281149785966 max memory_allocated 22564.74169921875 
[2025-02-28 16:16:28 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 5 loss:0.023727815598249435 norm:0.00012779960525222123 max memory_allocated 22564.74169921875 
[2025-02-28 16:16:58 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 6 loss:0.023579081520438194 norm:0.00012343580601736903 max memory_allocated 22564.74169921875 
[2025-02-28 16:17:29 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 7 loss:0.02348269894719124 norm:0.00011181161971762776 max memory_allocated 22564.74169921875 
[2025-02-28 16:18:00 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 8 loss:0.023411825299263 norm:0.00010306499461876228 max memory_allocated 22564.74169921875 
[2025-02-28 16:18:30 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 9 loss:0.023361554369330406 norm:9.611198038328439e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:19:01 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 10 loss:0.023300129920244217 norm:8.984291343949735e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:19:32 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 11 loss:0.02325281873345375 norm:8.555584645364434e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:20:02 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 12 loss:0.02322544902563095 norm:8.701893966645002e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:20:33 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 13 loss:0.02319100871682167 norm:8.58031926327385e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:21:04 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 14 loss:0.023159200325608253 norm:7.856865704525262e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:21:34 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 15 loss:0.023133521899580956 norm:7.339184958254918e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:22:05 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 16 loss:0.023105353116989136 norm:7.422836642945185e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:22:36 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 17 loss:0.023078909143805504 norm:7.323086902033538e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:23:06 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 18 loss:0.023063622415065765 norm:6.16971155977808e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:23:37 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 19 loss:0.023053117096424103 norm:6.335353828035295e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:23:45 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-02-28 16:24:18 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 0 loss:0.02668035961687565 norm:0.000591952120885253 max memory_allocated 22564.91357421875 
[2025-02-28 16:24:49 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 1 loss:0.025287462398409843 norm:0.00022132680169306695 max memory_allocated 22564.91357421875 
[2025-02-28 16:25:20 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 2 loss:0.02479538321495056 norm:0.0001543791440781206 max memory_allocated 22564.91357421875 
[2025-02-28 16:25:50 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 3 loss:0.024420026689767838 norm:0.00013416283763945103 max memory_allocated 22564.91357421875 
[2025-02-28 16:26:21 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 4 loss:0.024128030985593796 norm:0.00010338809806853533 max memory_allocated 22564.91357421875 
[2025-02-28 16:26:52 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 5 loss:0.023929696530103683 norm:8.641926979180425e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:27:22 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 6 loss:0.023810535669326782 norm:8.843467367114499e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:27:53 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 7 loss:0.023734908550977707 norm:7.53332715248689e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:28:24 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 8 loss:0.02369137853384018 norm:6.880433647893369e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:28:54 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 9 loss:0.02365286462008953 norm:6.679637590423226e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:29:25 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 10 loss:0.023629294708371162 norm:5.893155321246013e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:29:56 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 11 loss:0.023605138063430786 norm:5.394233812694438e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:30:26 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 12 loss:0.023592976853251457 norm:5.359612259780988e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:30:57 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 13 loss:0.023577775806188583 norm:5.290761328069493e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:31:28 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 14 loss:0.023564238101243973 norm:4.848554090131074e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:31:58 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 15 loss:0.023558704182505608 norm:5.152907033334486e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:32:29 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 16 loss:0.02354723960161209 norm:4.834855644730851e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:32:59 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 17 loss:0.023536067456007004 norm:4.531060767476447e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:33:30 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 18 loss:0.023528579622507095 norm:4.394221105030738e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:34:01 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 19 loss:0.023526381701231003 norm:4.296018960303627e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:34:09 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-02-28 16:34:42 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 0 loss:0.028691018000245094 norm:0.0006512359832413495 max memory_allocated 22565.08544921875 
[2025-02-28 16:35:13 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 1 loss:0.027154814451932907 norm:0.00030801320099271834 max memory_allocated 22565.08544921875 
[2025-02-28 16:35:43 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 2 loss:0.026557112112641335 norm:0.00020060680981259793 max memory_allocated 22565.08544921875 
[2025-02-28 16:36:14 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 3 loss:0.026133961975574493 norm:0.0001505654799984768 max memory_allocated 22565.08544921875 
[2025-02-28 16:36:45 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 4 loss:0.025784004479646683 norm:0.0001239855046151206 max memory_allocated 22565.08544921875 
[2025-02-28 16:37:15 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 5 loss:0.025569740682840347 norm:0.00010751279478427023 max memory_allocated 22565.08544921875 
[2025-02-28 16:37:46 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 6 loss:0.025434445589780807 norm:9.06477143871598e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:38:16 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 7 loss:0.025369934737682343 norm:8.464184065815061e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:38:47 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 8 loss:0.02530721016228199 norm:7.931093568913639e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:39:18 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 9 loss:0.025257276371121407 norm:7.14138732291758e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:39:48 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 10 loss:0.02522033080458641 norm:6.77636853652075e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:40:19 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 11 loss:0.025186670944094658 norm:6.60153673379682e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:40:50 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 12 loss:0.025153707712888718 norm:6.166744424263015e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:41:20 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 13 loss:0.02512792870402336 norm:5.9996327763656154e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:41:51 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 14 loss:0.0251096710562706 norm:6.101905455579981e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:42:21 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 15 loss:0.025090018287301064 norm:5.5992670240812004e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:42:52 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 16 loss:0.025070007890462875 norm:5.470123142004013e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:43:23 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 17 loss:0.025059178471565247 norm:5.147873889654875e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:43:53 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 18 loss:0.025046885013580322 norm:5.124429299030453e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:44:24 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 19 loss:0.025035981088876724 norm:4.625686415238306e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:44:32 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-02-28 16:45:05 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 0 loss:0.030938569456338882 norm:0.0007612322806380689 max memory_allocated 22565.25732421875 
[2025-02-28 16:45:36 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 1 loss:0.029321977868676186 norm:0.00034447418875060976 max memory_allocated 22565.25732421875 
[2025-02-28 16:46:07 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 2 loss:0.02858077920973301 norm:0.0002009063755394891 max memory_allocated 22565.25732421875 
[2025-02-28 16:46:37 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 3 loss:0.028167298063635826 norm:0.00014753184223081917 max memory_allocated 22565.25732421875 
[2025-02-28 16:47:08 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 4 loss:0.027803001925349236 norm:0.00012007484474452212 max memory_allocated 22565.25732421875 
[2025-02-28 16:47:39 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 5 loss:0.027594424784183502 norm:0.00010024901712313294 max memory_allocated 22565.25732421875 
[2025-02-28 16:48:09 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 6 loss:0.027499476447701454 norm:9.338372183265164e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:48:40 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 7 loss:0.027439363300800323 norm:8.310719567816705e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:49:10 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 8 loss:0.027399828657507896 norm:7.161471876315773e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:49:41 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 9 loss:0.027363305911421776 norm:6.956390279810876e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:50:12 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 10 loss:0.027330920100212097 norm:7.267437467817217e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:50:42 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 11 loss:0.027303112670779228 norm:6.9318717578426e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:51:13 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 12 loss:0.027281146496534348 norm:6.125385698396713e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:51:44 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 13 loss:0.02725730463862419 norm:5.8749839809024706e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:52:14 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 14 loss:0.027233673259615898 norm:6.186663085827604e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:52:45 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 15 loss:0.027217041701078415 norm:5.522037099581212e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:53:16 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 16 loss:0.027208201587200165 norm:5.320667696651071e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:53:46 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 17 loss:0.027192072942852974 norm:5.5370383051922545e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:54:17 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 18 loss:0.027176152914762497 norm:5.238084486336447e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:54:47 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 19 loss:0.027165023609995842 norm:4.7574518248438835e-05 max memory_allocated 22565.25732421875 
[2025-02-28 16:54:56 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-02-28 16:55:29 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 0 loss:0.03483390435576439 norm:0.001085472758859396 max memory_allocated 22565.42919921875 
[2025-02-28 16:56:00 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 1 loss:0.03249984234571457 norm:0.00043950072722509503 max memory_allocated 22565.42919921875 
[2025-02-28 16:56:30 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 2 loss:0.03145746886730194 norm:0.00022983778035268188 max memory_allocated 22565.42919921875 
[2025-02-28 16:57:01 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 3 loss:0.03100806288421154 norm:0.00017037430370692164 max memory_allocated 22565.42919921875 
[2025-02-28 16:57:31 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 4 loss:0.030638225376605988 norm:0.00013122808013577014 max memory_allocated 22565.42919921875 
[2025-02-28 16:58:02 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 5 loss:0.030408184975385666 norm:0.0001211722192238085 max memory_allocated 22565.42919921875 
[2025-02-28 16:58:33 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 6 loss:0.030273305252194405 norm:0.0001086838892661035 max memory_allocated 22565.42919921875 
[2025-02-28 16:59:03 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 7 loss:0.030202573165297508 norm:0.00010541416850173846 max memory_allocated 22565.42919921875 
[2025-02-28 16:59:34 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 8 loss:0.030150480568408966 norm:0.00010084111272590235 max memory_allocated 22565.42919921875 
[2025-02-28 17:00:05 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 9 loss:0.030101191252470016 norm:9.35866410145536e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:00:35 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 10 loss:0.03006140887737274 norm:8.728005195735022e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:01:06 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 11 loss:0.03003423847258091 norm:8.257204899564385e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:01:37 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 12 loss:0.029996676370501518 norm:8.078808605205268e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:02:07 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 13 loss:0.02996777929365635 norm:8.251560211647302e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:02:38 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 14 loss:0.02994646318256855 norm:7.752738747512922e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:03:09 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 15 loss:0.029921047389507294 norm:6.95621274644509e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:03:39 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 16 loss:0.029901359230279922 norm:6.767167360521853e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:04:10 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 17 loss:0.029881935566663742 norm:6.921649037394673e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:04:41 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 18 loss:0.029860328882932663 norm:6.785563891753554e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:05:11 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 19 loss:0.029840126633644104 norm:6.0375248722266406e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:05:20 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-02-28 17:05:53 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 0 loss:0.03633660078048706 norm:0.0005371688748709857 max memory_allocated 22565.60107421875 
[2025-02-28 17:06:23 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 1 loss:0.035081733018159866 norm:0.0002366252156207338 max memory_allocated 22565.60107421875 
[2025-02-28 17:06:54 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 2 loss:0.03457547351717949 norm:0.00016568574937991798 max memory_allocated 22565.60107421875 
[2025-02-28 17:07:25 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 3 loss:0.03418468311429024 norm:0.00012190773122711107 max memory_allocated 22565.60107421875 
[2025-02-28 17:07:55 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 4 loss:0.03380541503429413 norm:0.00010980028309859335 max memory_allocated 22565.60107421875 
[2025-02-28 17:08:26 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 5 loss:0.03358964994549751 norm:0.0001092899328796193 max memory_allocated 22565.60107421875 
[2025-02-28 17:08:57 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 6 loss:0.03348631039261818 norm:8.771722059464082e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:09:27 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 7 loss:0.033434443175792694 norm:8.7116495706141e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:09:58 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 8 loss:0.03339133411645889 norm:7.793926488375291e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:10:29 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 9 loss:0.03335876762866974 norm:6.813187064835802e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:10:59 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 10 loss:0.0333322212100029 norm:6.221847434062511e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:11:30 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 11 loss:0.03331062197685242 norm:6.512695108540356e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:12:01 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 12 loss:0.03329111635684967 norm:6.127568485680968e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:12:31 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 13 loss:0.03326941654086113 norm:5.860801684320904e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:13:02 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 14 loss:0.033254474401474 norm:5.693387356586754e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:13:33 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 15 loss:0.03323924168944359 norm:5.7092009228654206e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:14:03 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 16 loss:0.033226028084754944 norm:5.284146755002439e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:14:34 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 17 loss:0.03322063013911247 norm:5.200839223107323e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:15:05 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 18 loss:0.03320982679724693 norm:5.0322058086749166e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:15:35 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 19 loss:0.03321065008640289 norm:4.892528158961795e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:15:44 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-02-28 17:16:17 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 0 loss:0.04207528755068779 norm:0.0007112310850061476 max memory_allocated 22565.77294921875 
[2025-02-28 17:16:48 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 1 loss:0.04010554403066635 norm:0.0002983657759614289 max memory_allocated 22565.77294921875 
[2025-02-28 17:17:18 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 2 loss:0.03941554203629494 norm:0.0001837816962506622 max memory_allocated 22565.77294921875 
[2025-02-28 17:17:49 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 3 loss:0.03888680040836334 norm:0.00016046127711888403 max memory_allocated 22565.77294921875 
[2025-02-28 17:18:19 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 4 loss:0.038430988788604736 norm:0.000143434401252307 max memory_allocated 22565.77294921875 
[2025-02-28 17:18:50 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 5 loss:0.03818102926015854 norm:0.00012503453763201833 max memory_allocated 22565.77294921875 
[2025-02-28 17:19:21 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 6 loss:0.03807307034730911 norm:0.00010761847806861624 max memory_allocated 22565.77294921875 
[2025-02-28 17:19:51 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 7 loss:0.0380275584757328 norm:0.00018634296429809183 max memory_allocated 22565.77294921875 
[2025-02-28 17:20:22 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 8 loss:0.0379650816321373 norm:0.00010505941463634372 max memory_allocated 22565.77294921875 
[2025-02-28 17:20:53 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 9 loss:0.03791864216327667 norm:9.542157204123214e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:21:23 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 10 loss:0.03787726163864136 norm:8.81757223396562e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:21:54 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 11 loss:0.03783489018678665 norm:9.104545461013913e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:22:25 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 12 loss:0.037807829678058624 norm:8.641299791634083e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:22:55 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 13 loss:0.03776773437857628 norm:7.739893771940842e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:23:26 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 14 loss:0.03773809224367142 norm:7.547864515800029e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:23:56 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 15 loss:0.03771059587597847 norm:7.379602175205946e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:24:27 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 16 loss:0.037684883922338486 norm:7.318591815419495e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:24:58 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 17 loss:0.0376637801527977 norm:6.515040149679407e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:25:28 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 18 loss:0.03765260800719261 norm:7.225372974062338e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:25:59 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 19 loss:0.037639401853084564 norm:6.686238339170814e-05 max memory_allocated 22565.77294921875 
[2025-02-28 17:26:07 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-02-28 17:26:41 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 0 loss:0.046610794961452484 norm:0.000737223366741091 max memory_allocated 22565.94482421875 
[2025-02-28 17:27:11 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 1 loss:0.04522024467587471 norm:0.00042014935752376914 max memory_allocated 22565.94482421875 
[2025-02-28 17:27:42 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 2 loss:0.04455708712339401 norm:0.0002866813447326422 max memory_allocated 22565.94482421875 
[2025-02-28 17:28:12 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 3 loss:0.04394914582371712 norm:0.00021716656920034438 max memory_allocated 22565.94482421875 
[2025-02-28 17:28:43 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 4 loss:0.04346272349357605 norm:0.0001584768178872764 max memory_allocated 22565.94482421875 
[2025-02-28 17:29:14 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 5 loss:0.0432688444852829 norm:0.00013031346315983683 max memory_allocated 22565.94482421875 
[2025-02-28 17:29:44 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 6 loss:0.04319098964333534 norm:0.00011857739445986226 max memory_allocated 22565.94482421875 
[2025-02-28 17:30:15 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 7 loss:0.043114494532346725 norm:9.950625099008903e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:30:46 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 8 loss:0.04305858165025711 norm:9.12862378754653e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:31:16 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 9 loss:0.043010979890823364 norm:8.602799789514393e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:31:47 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 10 loss:0.04298084229230881 norm:8.104690641630441e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:32:18 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 11 loss:0.04294782504439354 norm:7.224056753329933e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:32:48 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 12 loss:0.04292594641447067 norm:7.464392547262833e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:33:19 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 13 loss:0.0429011806845665 norm:6.922135798959062e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:33:50 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 14 loss:0.04288212209939957 norm:6.223758100531995e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:34:20 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 15 loss:0.04287277162075043 norm:6.397964898496866e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:34:51 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 16 loss:0.04286012053489685 norm:6.018449494149536e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:35:22 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 17 loss:0.04285062849521637 norm:5.949155456619337e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:35:52 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 18 loss:0.04284321889281273 norm:5.566781692323275e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:36:23 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 19 loss:0.04283163323998451 norm:5.476354999700561e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:36:31 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-02-28 17:37:04 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 0 loss:0.05309468135237694 norm:0.000730948697309941 max memory_allocated 22566.11669921875 
[2025-02-28 17:37:35 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 1 loss:0.05139349773526192 norm:0.00035499411751516163 max memory_allocated 22566.11669921875 
[2025-02-28 17:38:06 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 2 loss:0.05058184266090393 norm:0.00024823262356221676 max memory_allocated 22566.11669921875 
[2025-02-28 17:38:36 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 3 loss:0.049907833337783813 norm:0.00018181113409809768 max memory_allocated 22566.11669921875 
[2025-02-28 17:39:07 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 4 loss:0.04940159246325493 norm:0.00015570881078019738 max memory_allocated 22566.11669921875 
[2025-02-28 17:39:38 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 5 loss:0.04925987496972084 norm:0.0001407768577337265 max memory_allocated 22566.11669921875 
[2025-02-28 17:40:08 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 6 loss:0.049157753586769104 norm:0.00012655719183385372 max memory_allocated 22566.11669921875 
[2025-02-28 17:40:39 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 7 loss:0.04908613860607147 norm:0.0001219889018102549 max memory_allocated 22566.11669921875 
[2025-02-28 17:41:09 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 8 loss:0.04901251941919327 norm:0.0001132695033447817 max memory_allocated 22566.11669921875 
[2025-02-28 17:41:40 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 9 loss:0.04896562919020653 norm:0.00010599454253679141 max memory_allocated 22566.11669921875 
[2025-02-28 17:42:11 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 10 loss:0.0489158034324646 norm:0.00010461804049555212 max memory_allocated 22566.11669921875 
[2025-02-28 17:42:41 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 11 loss:0.04886521026492119 norm:9.426249016541988e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:43:12 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 12 loss:0.048832062631845474 norm:9.095817222259939e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:43:43 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 13 loss:0.04879269376397133 norm:8.691783295944333e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:44:13 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 14 loss:0.04876529052853584 norm:8.603791502537206e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:44:44 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 15 loss:0.048746418207883835 norm:8.873389015207067e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:45:15 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 16 loss:0.04872674494981766 norm:8.835876360535622e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:45:45 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 17 loss:0.04870728403329849 norm:8.719527977518737e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:46:16 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 18 loss:0.048685140907764435 norm:8.152613008860499e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:46:47 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 19 loss:0.04866950586438179 norm:7.768320210743695e-05 max memory_allocated 22566.11669921875 
[2025-02-28 17:46:55 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-02-28 17:47:28 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 0 loss:0.0609811469912529 norm:0.0006662940140813589 max memory_allocated 22566.28857421875 
[2025-02-28 17:47:59 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 1 loss:0.05935192108154297 norm:0.0003456459962762892 max memory_allocated 22566.28857421875 
[2025-02-28 17:48:29 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 2 loss:0.05822235345840454 norm:0.0002370529546169564 max memory_allocated 22566.28857421875 
[2025-02-28 17:49:00 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 3 loss:0.057439371943473816 norm:0.00020553094509523362 max memory_allocated 22566.28857421875 
[2025-02-28 17:49:31 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 4 loss:0.056913357228040695 norm:0.00017334101721644402 max memory_allocated 22566.28857421875 
[2025-02-28 17:50:01 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 5 loss:0.05679257586598396 norm:0.0001751280651660636 max memory_allocated 22566.28857421875 
[2025-02-28 17:50:32 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 6 loss:0.05655486136674881 norm:0.00014874899352435023 max memory_allocated 22566.28857421875 
[2025-02-28 17:51:03 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 7 loss:0.05648833513259888 norm:0.0001364556374028325 max memory_allocated 22566.28857421875 
[2025-02-28 17:51:33 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 8 loss:0.056418221443891525 norm:0.00013262925494927913 max memory_allocated 22566.28857421875 
[2025-02-28 17:52:04 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 9 loss:0.05636220425367355 norm:0.00012662418885156512 max memory_allocated 22566.28857421875 
[2025-02-28 17:52:35 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 10 loss:0.05630426108837128 norm:0.00012422871077433228 max memory_allocated 22566.28857421875 
[2025-02-28 17:53:05 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 11 loss:0.056349046528339386 norm:0.0001647992612561211 max memory_allocated 22566.28857421875 
[2025-02-28 17:53:36 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 12 loss:0.05617021396756172 norm:0.00010373057739343494 max memory_allocated 22566.28857421875 
[2025-02-28 17:54:06 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 13 loss:0.056127045303583145 norm:9.84157231869176e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:54:37 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 14 loss:0.05612548813223839 norm:0.00010501778160687536 max memory_allocated 22566.28857421875 
[2025-02-28 17:55:08 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 15 loss:0.056088536977767944 norm:9.548448724672198e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:55:38 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 16 loss:0.05606691911816597 norm:9.264492109650746e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:56:09 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 17 loss:0.05604446679353714 norm:8.849422738421708e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:56:40 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 18 loss:0.05605410411953926 norm:8.619164873380214e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:57:10 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 19 loss:0.05602108687162399 norm:8.51009608595632e-05 max memory_allocated 22566.28857421875 
[2025-02-28 17:57:21 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-02-28 17:57:57 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 0 loss:0.06945471465587616 norm:0.0009566518710926175 max memory_allocated 22566.46044921875 
[2025-02-28 17:58:28 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 1 loss:0.06719320267438889 norm:0.0004502255469560623 max memory_allocated 22566.46044921875 
[2025-02-28 17:58:58 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 2 loss:0.06604994833469391 norm:0.00030748723656870425 max memory_allocated 22566.46044921875 
[2025-02-28 17:59:29 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 3 loss:0.06517473608255386 norm:0.00024316628696396947 max memory_allocated 22566.46044921875 
[2025-02-28 18:00:00 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 4 loss:0.06469288468360901 norm:0.00019355177937541157 max memory_allocated 22566.46044921875 
[2025-02-28 18:00:30 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 5 loss:0.06448594480752945 norm:0.000160679075634107 max memory_allocated 22566.46044921875 
[2025-02-28 18:01:01 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 6 loss:0.06437619030475616 norm:0.00014559415285475552 max memory_allocated 22566.46044921875 
[2025-02-28 18:01:31 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 7 loss:0.0642930343747139 norm:0.00013478045002557337 max memory_allocated 22566.46044921875 
[2025-02-28 18:02:02 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 8 loss:0.06420014053583145 norm:0.00012990640243515372 max memory_allocated 22566.46044921875 
[2025-02-28 18:02:33 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 9 loss:0.06413010507822037 norm:0.00011893493501702324 max memory_allocated 22566.46044921875 
[2025-02-28 18:03:03 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 10 loss:0.06406595557928085 norm:0.00011274549615336582 max memory_allocated 22566.46044921875 
[2025-02-28 18:03:34 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 11 loss:0.06401446461677551 norm:0.00010945076064672321 max memory_allocated 22566.46044921875 
[2025-02-28 18:04:05 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 12 loss:0.0639740601181984 norm:0.00010213591303909197 max memory_allocated 22566.46044921875 
[2025-02-28 18:04:36 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 13 loss:0.06393177807331085 norm:0.00010153129551326856 max memory_allocated 22566.46044921875 
[2025-02-28 18:05:06 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 14 loss:0.06389056891202927 norm:9.619959018891677e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:05:37 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 15 loss:0.06386074423789978 norm:8.982473082141951e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:06:07 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 16 loss:0.06383714824914932 norm:8.673085540067405e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:06:38 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 17 loss:0.06381496787071228 norm:8.524367876816541e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:07:09 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 18 loss:0.06379802525043488 norm:8.068700117291883e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:07:39 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 19 loss:0.06378836184740067 norm:8.268310921266675e-05 max memory_allocated 22566.46044921875 
[2025-02-28 18:07:48 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-02-28 18:08:21 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 0 loss:0.07799500226974487 norm:0.000737304741051048 max memory_allocated 22566.63232421875 
[2025-02-28 18:08:52 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 1 loss:0.07606399804353714 norm:0.00038369325920939445 max memory_allocated 22566.63232421875 
[2025-02-28 18:09:22 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 2 loss:0.07507987320423126 norm:0.00025980797363445163 max memory_allocated 22566.63232421875 
[2025-02-28 18:09:53 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 3 loss:0.07420862466096878 norm:0.0001973966573132202 max memory_allocated 22566.63232421875 
[2025-02-28 18:10:23 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 4 loss:0.0738152414560318 norm:0.00016146308917086571 max memory_allocated 22566.63232421875 
[2025-02-28 18:10:54 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 5 loss:0.0736808255314827 norm:0.00014383223606273532 max memory_allocated 22566.63232421875 
[2025-02-28 18:11:25 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 6 loss:0.07359675318002701 norm:0.00012697954662144184 max memory_allocated 22566.63232421875 
[2025-02-28 18:11:55 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 7 loss:0.07351651787757874 norm:0.00011722098861355335 max memory_allocated 22566.63232421875 
[2025-02-28 18:12:26 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 8 loss:0.07345741242170334 norm:0.00011000846279785037 max memory_allocated 22566.63232421875 
[2025-02-28 18:12:57 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 9 loss:0.07340671867132187 norm:0.00010254365770379081 max memory_allocated 22566.63232421875 
[2025-02-28 18:13:27 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 10 loss:0.07336551696062088 norm:9.713784675113857e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:13:58 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 11 loss:0.0733349621295929 norm:9.361198317492381e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:14:29 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 12 loss:0.07330256700515747 norm:9.135478467214853e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:14:59 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 13 loss:0.07327386736869812 norm:8.85396875673905e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:15:30 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 14 loss:0.07325205951929092 norm:8.809797145659104e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:16:00 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 15 loss:0.07322858273983002 norm:8.398546924581751e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:16:31 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 16 loss:0.0732138603925705 norm:8.378512575291097e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:17:02 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 17 loss:0.07319968938827515 norm:8.022900874493644e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:17:32 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 18 loss:0.07319356501102448 norm:8.072764467215165e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:18:03 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 19 loss:0.07318294793367386 norm:7.844275387469679e-05 max memory_allocated 22566.63232421875 
[2025-02-28 18:18:11 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-02-28 18:18:14 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:18:45 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 0 loss:0.09044826775789261 norm:0.003384058829396963 max memory_allocated 22566.91943359375 
[2025-02-28 18:19:15 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 1 loss:0.08845740556716919 norm:0.002625328954309225 max memory_allocated 22566.91943359375 
[2025-02-28 18:19:46 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 2 loss:0.08717751502990723 norm:0.0022247759625315666 max memory_allocated 22566.91943359375 
[2025-02-28 18:20:17 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 3 loss:0.08609342575073242 norm:0.0018177246674895287 max memory_allocated 22566.91943359375 
[2025-02-28 18:20:48 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 4 loss:0.08561237156391144 norm:0.001472149626351893 max memory_allocated 22566.91943359375 
[2025-02-28 18:21:18 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 5 loss:0.08539847284555435 norm:0.0012487756321206689 max memory_allocated 22566.91943359375 
[2025-02-28 18:21:49 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 6 loss:0.08528334647417068 norm:0.001166011206805706 max memory_allocated 22566.91943359375 
[2025-02-28 18:22:20 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 7 loss:0.08522781729698181 norm:0.0011151102371513844 max memory_allocated 22566.91943359375 
[2025-02-28 18:22:51 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 8 loss:0.08516698330640793 norm:0.001181144267320633 max memory_allocated 22566.91943359375 
[2025-02-28 18:23:21 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 9 loss:0.08513142168521881 norm:0.0009490984957665205 max memory_allocated 22566.91943359375 
[2025-02-28 18:23:52 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 10 loss:0.08500373363494873 norm:0.0009205736569128931 max memory_allocated 22566.91943359375 
[2025-02-28 18:24:23 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 11 loss:0.08492250740528107 norm:0.0009278265060856938 max memory_allocated 22566.91943359375 
[2025-02-28 18:24:54 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 12 loss:0.08488603681325912 norm:0.0008693370618857443 max memory_allocated 22566.91943359375 
[2025-02-28 18:25:25 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 13 loss:0.0848572701215744 norm:0.0009119657916016877 max memory_allocated 22566.91943359375 
[2025-02-28 18:25:55 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 14 loss:0.08486342430114746 norm:0.0009070270461961627 max memory_allocated 22566.91943359375 
[2025-02-28 18:26:26 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 15 loss:0.08484350144863129 norm:0.0008841916569508612 max memory_allocated 22566.91943359375 
[2025-02-28 18:26:57 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 16 loss:0.08482804149389267 norm:0.0008793098968453705 max memory_allocated 22566.91943359375 
[2025-02-28 18:27:28 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 17 loss:0.08481092005968094 norm:0.0008564035524614155 max memory_allocated 22566.91943359375 
[2025-02-28 18:27:58 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 18 loss:0.08480183780193329 norm:0.0008424549014307559 max memory_allocated 22566.91943359375 
[2025-02-28 18:28:29 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 19 loss:0.08479269593954086 norm:0.0008481938857585192 max memory_allocated 22566.91943359375 
[2025-02-28 18:28:37 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-02-28 18:28:40 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:29:11 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 0 loss:0.10962668061256409 norm:0.003738624509423971 max memory_allocated 22567.09130859375 
[2025-02-28 18:29:42 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 1 loss:0.10462788492441177 norm:0.0026018284261226654 max memory_allocated 22567.09130859375 
[2025-02-28 18:30:12 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 2 loss:0.10173505544662476 norm:0.0017661342862993479 max memory_allocated 22567.09130859375 
[2025-02-28 18:30:43 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 3 loss:0.09998665750026703 norm:0.001195932156406343 max memory_allocated 22567.09130859375 
[2025-02-28 18:31:14 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 4 loss:0.09930892288684845 norm:0.0009381037671118975 max memory_allocated 22567.09130859375 
[2025-02-28 18:31:45 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 5 loss:0.0989181250333786 norm:0.0007696310058236122 max memory_allocated 22567.09130859375 
[2025-02-28 18:32:15 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 6 loss:0.09866806864738464 norm:0.0007319484138861299 max memory_allocated 22567.09130859375 
[2025-02-28 18:32:46 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 7 loss:0.09851674735546112 norm:0.0007856833981350064 max memory_allocated 22567.09130859375 
[2025-02-28 18:33:17 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 8 loss:0.09840202331542969 norm:0.0007460387423634529 max memory_allocated 22567.09130859375 
[2025-02-28 18:33:48 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 9 loss:0.09832990914583206 norm:0.0006901382585056126 max memory_allocated 22567.09130859375 
[2025-02-28 18:34:18 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 10 loss:0.09826328605413437 norm:0.0006919353036209941 max memory_allocated 22567.09130859375 
[2025-02-28 18:34:49 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 11 loss:0.09823600947856903 norm:0.0008222575415857136 max memory_allocated 22567.09130859375 
[2025-02-28 18:35:20 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 12 loss:0.09822686016559601 norm:0.0009212237782776356 max memory_allocated 22567.09130859375 
[2025-02-28 18:35:51 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 13 loss:0.09822987020015717 norm:0.0009413236402906477 max memory_allocated 22567.09130859375 
[2025-02-28 18:36:21 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 14 loss:0.0981910303235054 norm:0.000857029459439218 max memory_allocated 22567.09130859375 
[2025-02-28 18:36:52 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 15 loss:0.09821633994579315 norm:0.0009138245368376374 max memory_allocated 22567.09130859375 
[2025-02-28 18:37:23 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 16 loss:0.09826639294624329 norm:0.0009849495254456997 max memory_allocated 22567.09130859375 
[2025-02-28 18:37:54 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 17 loss:0.09828591346740723 norm:0.0009790662443265319 max memory_allocated 22567.09130859375 
[2025-02-28 18:38:24 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 18 loss:0.09830577671527863 norm:0.0010314416140317917 max memory_allocated 22567.09130859375 
[2025-02-28 18:38:55 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 19 loss:0.09826839715242386 norm:0.0008848547586239874 max memory_allocated 22567.09130859375 
[2025-02-28 18:39:03 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-02-28 18:39:06 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:39:37 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 0 loss:1.0326638221740723 norm:0.18405018746852875 max memory_allocated 22567.26318359375 
[2025-02-28 18:40:08 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 1 loss:0.700802206993103 norm:0.15911534428596497 max memory_allocated 22567.26318359375 
[2025-02-28 18:40:38 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 2 loss:0.3433421850204468 norm:0.06397612392902374 max memory_allocated 22567.26318359375 
[2025-02-28 18:41:09 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 3 loss:0.24582098424434662 norm:0.04035172611474991 max memory_allocated 22567.26318359375 
[2025-02-28 18:41:40 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 4 loss:0.21359530091285706 norm:0.040668901056051254 max memory_allocated 22567.26318359375 
[2025-02-28 18:42:11 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 5 loss:0.194717675447464 norm:0.038625236600637436 max memory_allocated 22567.26318359375 
[2025-02-28 18:42:42 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 6 loss:0.1833125501871109 norm:0.038236044347286224 max memory_allocated 22567.26318359375 
[2025-02-28 18:43:12 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 7 loss:0.17423439025878906 norm:0.034818485379219055 max memory_allocated 22567.26318359375 
[2025-02-28 18:43:43 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 8 loss:0.1677662432193756 norm:0.031175795942544937 max memory_allocated 22567.26318359375 
[2025-02-28 18:44:14 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 9 loss:0.16355589032173157 norm:0.028920501470565796 max memory_allocated 22567.26318359375 
[2025-02-28 18:44:45 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 10 loss:0.15996550023555756 norm:0.02777663990855217 max memory_allocated 22567.26318359375 
[2025-02-28 18:45:16 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 11 loss:0.1568700522184372 norm:0.02481628768146038 max memory_allocated 22567.26318359375 
[2025-02-28 18:45:46 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 12 loss:0.1548643857240677 norm:0.02322627417743206 max memory_allocated 22567.26318359375 
[2025-02-28 18:46:17 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 13 loss:0.1534675508737564 norm:0.02080560103058815 max memory_allocated 22567.26318359375 
[2025-02-28 18:46:48 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 14 loss:0.15210659801959991 norm:0.019691623747348785 max memory_allocated 22567.26318359375 
[2025-02-28 18:47:19 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 15 loss:0.15106946229934692 norm:0.018328893929719925 max memory_allocated 22567.26318359375 
[2025-02-28 18:47:50 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 16 loss:0.1507030725479126 norm:0.0183910820633173 max memory_allocated 22567.26318359375 
[2025-02-28 18:48:20 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 17 loss:0.15050694346427917 norm:0.017805803567171097 max memory_allocated 22567.26318359375 
[2025-02-28 18:48:51 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 18 loss:0.15082122385501862 norm:0.019043875858187675 max memory_allocated 22567.26318359375 
[2025-02-28 18:49:22 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 19 loss:0.1515752673149109 norm:0.02100514806807041 max memory_allocated 22567.26318359375 
[2025-02-28 18:49:30 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-02-28 18:49:33 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:50:04 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 0 loss:0.30370068550109863 norm:0.02547786943614483 max memory_allocated 22567.43505859375 
[2025-02-28 18:50:34 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 1 loss:0.2831253111362457 norm:0.019405728206038475 max memory_allocated 22567.43505859375 
[2025-02-28 18:51:05 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 2 loss:0.27123886346817017 norm:0.016203470528125763 max memory_allocated 22567.43505859375 
[2025-02-28 18:51:36 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 3 loss:0.264611154794693 norm:0.01375100389122963 max memory_allocated 22567.43505859375 
[2025-02-28 18:52:07 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 4 loss:0.26017558574676514 norm:0.011369599029421806 max memory_allocated 22567.43505859375 
[2025-02-28 18:52:38 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 5 loss:0.2577071785926819 norm:0.011033296585083008 max memory_allocated 22567.43505859375 
[2025-02-28 18:53:08 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 6 loss:0.2560022473335266 norm:0.010028265416622162 max memory_allocated 22567.43505859375 
[2025-02-28 18:53:39 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 7 loss:0.2546749413013458 norm:0.009574828669428825 max memory_allocated 22567.43505859375 
[2025-02-28 18:54:10 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 8 loss:0.253601998090744 norm:0.009184975177049637 max memory_allocated 22567.43505859375 
[2025-02-28 18:54:41 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 9 loss:0.253018319606781 norm:0.009465866722166538 max memory_allocated 22567.43505859375 
[2025-02-28 18:55:11 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 10 loss:0.2523413598537445 norm:0.00900568813085556 max memory_allocated 22567.43505859375 
[2025-02-28 18:55:42 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 11 loss:0.25198280811309814 norm:0.009443007409572601 max memory_allocated 22567.43505859375 
[2025-02-28 18:56:13 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 12 loss:0.2517169117927551 norm:0.00925680436193943 max memory_allocated 22567.43505859375 
[2025-02-28 18:56:44 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 13 loss:0.2513664662837982 norm:0.009321159683167934 max memory_allocated 22567.43505859375 
[2025-02-28 18:57:14 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 14 loss:0.2511536478996277 norm:0.009277310222387314 max memory_allocated 22567.43505859375 
[2025-02-28 18:57:45 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 15 loss:0.2507181167602539 norm:0.008906970731914043 max memory_allocated 22567.43505859375 
[2025-02-28 18:58:16 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 16 loss:0.25076186656951904 norm:0.009226936846971512 max memory_allocated 22567.43505859375 
[2025-02-28 18:58:47 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 17 loss:0.2503490149974823 norm:0.008756856434047222 max memory_allocated 22567.43505859375 
[2025-02-28 18:59:18 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 18 loss:0.25005578994750977 norm:0.008404144085943699 max memory_allocated 22567.43505859375 
[2025-02-28 18:59:48 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 19 loss:0.24988757073879242 norm:0.008328001946210861 max memory_allocated 22567.43505859375 
[2025-02-28 18:59:57 root] (main_calib_config2.py 380): INFO 19993.32324719429
[2025-02-28 19:00:01 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-28 19:01:05 root] (main_calib_config2.py 159): INFO wikitext2 : 5.599809646606445
[2025-02-28 19:01:05 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-28 19:02:42 root] (main_calib_config2.py 159): INFO c4 : 7.140528202056885
[2025-02-28 19:02:52 datasets.load] (load.py 1272): WARNING Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/winogrande/a826c3d3506aefe0e9e9390dcb53271070536586bab95849876b2c1743df56e2 (last modified on Tue Feb 18 03:11:31 2025) since it couldn't be found locally at winogrande., or remotely on the Hugging Face Hub.
[2025-02-28 20:42:42 root] (main_calib_config2.py 170): INFO {'wikitext2': 5.599809646606445, 'c4': 7.140528202056885, 'results': {'winogrande': {'acc': 0.6787687450670876, 'acc_stderr': 0.013123599324558317}, 'piqa': {'acc': 0.7774755168661589, 'acc_stderr': 0.009704600975718243, 'acc_norm': 0.7693144722524483, 'acc_norm_stderr': 0.009828959550983096}, 'arc_challenge': {'acc': 0.40102389078498296, 'acc_stderr': 0.014322255790719869, 'acc_norm': 0.39761092150170646, 'acc_norm_stderr': 0.014301752223279533}, 'boolq': {'acc': 0.7076452599388379, 'acc_stderr': 0.007955278909905735}, 'arc_easy': {'acc': 0.6826599326599326, 'acc_stderr': 0.009550648343947771, 'acc_norm': 0.5340909090909091, 'acc_norm_stderr': 0.01023590810343869}, 'hellaswag': {'acc': 0.5615415255925115, 'acc_stderr': 0.004951840978219682, 'acc_norm': 0.7234614618601872, 'acc_norm_stderr': 0.004463721071319088}}, 'versions': {'winogrande': 0, 'piqa': 0, 'arc_challenge': 0, 'boolq': 1, 'arc_easy': 0, 'hellaswag': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
