[2025-02-28 13:17:24 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/Llama-2-7b-hf_0.65', save_dir=None, resume=None, real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='piqa,arc_easy,arc_challenge,boolq,hellaswag,winogrande', eval_ppl=True, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=20, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.65.pkl', scale_calibration=True, compensation_calibration=True)
[2025-02-28 13:20:27 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-02-28 13:20:27 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-02-28 13:20:28 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-02-28 13:20:28 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.65.pkl
[2025-02-28 13:20:30 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-02-28 13:20:33 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:21:05 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 0 loss:0.008756226859986782 norm:0.00910954736173153 max memory_allocated 22562.10693359375 
[2025-02-28 13:21:38 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 1 loss:0.004777463153004646 norm:0.004820666741579771 max memory_allocated 22562.10693359375 
[2025-02-28 13:22:12 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 2 loss:0.0033112443052232265 norm:0.0029934823978692293 max memory_allocated 22562.10693359375 
[2025-02-28 13:22:45 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 3 loss:0.002802284900099039 norm:0.002317736390978098 max memory_allocated 22562.10693359375 
[2025-02-28 13:23:18 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 4 loss:0.00269630691036582 norm:0.002067149616777897 max memory_allocated 22562.10693359375 
[2025-02-28 13:23:52 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 5 loss:0.0026352840941399336 norm:0.0019442442571744323 max memory_allocated 22562.10693359375 
[2025-02-28 13:24:25 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 6 loss:0.0025698337703943253 norm:0.0017437664791941643 max memory_allocated 22562.10693359375 
[2025-02-28 13:24:58 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 7 loss:0.002524764509871602 norm:0.001569082960486412 max memory_allocated 22562.10693359375 
[2025-02-28 13:25:32 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 8 loss:0.0024902308359742165 norm:0.0014724064385518432 max memory_allocated 22562.10693359375 
[2025-02-28 13:26:05 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 9 loss:0.0024576056748628616 norm:0.0013511264696717262 max memory_allocated 22562.10693359375 
[2025-02-28 13:26:38 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 10 loss:0.0024338229559361935 norm:0.0012735340278595686 max memory_allocated 22562.10693359375 
[2025-02-28 13:27:11 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 11 loss:0.0024123545736074448 norm:0.0011729075340554118 max memory_allocated 22562.10693359375 
[2025-02-28 13:27:45 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 12 loss:0.0024026879109442234 norm:0.0011267043882980943 max memory_allocated 22562.10693359375 
[2025-02-28 13:28:18 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 13 loss:0.002390909008681774 norm:0.0010811635293066502 max memory_allocated 22562.10693359375 
[2025-02-28 13:28:51 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 14 loss:0.00238022836856544 norm:0.0010627461597323418 max memory_allocated 22562.10693359375 
[2025-02-28 13:29:24 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 15 loss:0.0023499506060034037 norm:0.0009512087563052773 max memory_allocated 22562.10693359375 
[2025-02-28 13:29:58 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 16 loss:0.002330388408154249 norm:0.0008683272753842175 max memory_allocated 22562.10693359375 
[2025-02-28 13:30:31 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 17 loss:0.0023345947265625 norm:0.0008506719605065882 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:04 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 18 loss:0.0023138991091400385 norm:0.0007905454258434474 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:38 root] (abq_llm_calib_config2.py 396): INFO layer 0 iter 19 loss:0.0023112748749554157 norm:0.0007756728446111083 max memory_allocated 22562.10693359375 
[2025-02-28 13:31:47 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-02-28 13:31:50 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:32:23 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 0 loss:0.028058186173439026 norm:0.013082864694297314 max memory_allocated 22562.27880859375 
[2025-02-28 13:32:56 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 1 loss:0.019327085465192795 norm:0.008985228836536407 max memory_allocated 22562.27880859375 
[2025-02-28 13:33:30 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 2 loss:0.016122663393616676 norm:0.015272904187440872 max memory_allocated 22562.27880859375 
[2025-02-28 13:34:03 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 3 loss:0.013725531287491322 norm:0.009162074886262417 max memory_allocated 22562.27880859375 
[2025-02-28 13:34:37 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 4 loss:0.013262494467198849 norm:0.007012528367340565 max memory_allocated 22562.27880859375 
[2025-02-28 13:35:10 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 5 loss:0.014521516859531403 norm:0.007621816359460354 max memory_allocated 22562.27880859375 
[2025-02-28 13:35:43 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 6 loss:0.013146542012691498 norm:0.006966250482946634 max memory_allocated 22562.27880859375 
[2025-02-28 13:36:17 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 7 loss:0.012816101312637329 norm:0.006523050833493471 max memory_allocated 22562.27880859375 
[2025-02-28 13:36:50 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 8 loss:0.012556510977447033 norm:0.0061698914505541325 max memory_allocated 22562.27880859375 
[2025-02-28 13:37:24 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 9 loss:0.012364374473690987 norm:0.006266524083912373 max memory_allocated 22562.27880859375 
[2025-02-28 13:37:57 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 10 loss:0.012365064583718777 norm:0.005902571603655815 max memory_allocated 22562.27880859375 
[2025-02-28 13:38:30 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 11 loss:0.0132894953712821 norm:0.0060404567047953606 max memory_allocated 22562.27880859375 
[2025-02-28 13:39:04 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 12 loss:0.012632164172828197 norm:0.00584649620577693 max memory_allocated 22562.27880859375 
[2025-02-28 13:39:37 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 13 loss:0.012431453913450241 norm:0.005631681997328997 max memory_allocated 22562.27880859375 
[2025-02-28 13:40:11 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 14 loss:0.012305058538913727 norm:0.005666063167154789 max memory_allocated 22562.27880859375 
[2025-02-28 13:40:44 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 15 loss:0.012128911912441254 norm:0.005833351984620094 max memory_allocated 22562.27880859375 
[2025-02-28 13:41:17 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 16 loss:0.012363252229988575 norm:0.005988644436001778 max memory_allocated 22562.27880859375 
[2025-02-28 13:41:51 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 17 loss:0.012084968388080597 norm:0.005585968494415283 max memory_allocated 22562.27880859375 
[2025-02-28 13:42:24 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 18 loss:0.011962720192968845 norm:0.005495685618370771 max memory_allocated 22562.27880859375 
[2025-02-28 13:42:57 root] (abq_llm_calib_config2.py 396): INFO layer 1 iter 19 loss:0.012780380435287952 norm:0.005877557676285505 max memory_allocated 22562.27880859375 
[2025-02-28 13:43:07 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-02-28 13:43:10 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 13:43:43 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 0 loss:0.025348413735628128 norm:0.007342783734202385 max memory_allocated 22562.45068359375 
[2025-02-28 13:44:16 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 1 loss:0.018604226410388947 norm:0.005211451090872288 max memory_allocated 22562.45068359375 
[2025-02-28 13:44:50 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 2 loss:0.015909794718027115 norm:0.003698277985677123 max memory_allocated 22562.45068359375 
[2025-02-28 13:45:23 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 3 loss:0.014993470162153244 norm:0.002908574417233467 max memory_allocated 22562.45068359375 
[2025-02-28 13:45:57 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 4 loss:0.01446699071675539 norm:0.0023941590916365385 max memory_allocated 22562.45068359375 
[2025-02-28 13:46:30 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 5 loss:0.01406010054051876 norm:0.0019972568843513727 max memory_allocated 22562.45068359375 
[2025-02-28 13:47:04 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 6 loss:0.013839084655046463 norm:0.001677531166933477 max memory_allocated 22562.45068359375 
[2025-02-28 13:47:37 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 7 loss:0.013741634786128998 norm:0.0013794833794236183 max memory_allocated 22562.45068359375 
[2025-02-28 13:48:10 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 8 loss:0.013679852709174156 norm:0.0011144215241074562 max memory_allocated 22562.45068359375 
[2025-02-28 13:48:44 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 9 loss:0.013649645261466503 norm:0.0009625746752135456 max memory_allocated 22562.45068359375 
[2025-02-28 13:49:17 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 10 loss:0.0137220723554492 norm:0.0010699862614274025 max memory_allocated 22562.45068359375 
[2025-02-28 13:49:51 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 11 loss:0.013693299144506454 norm:0.0010323487222194672 max memory_allocated 22562.45068359375 
[2025-02-28 13:50:24 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 12 loss:0.013771298341453075 norm:0.0010709764901548624 max memory_allocated 22562.45068359375 
[2025-02-28 13:50:57 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 13 loss:0.01364917866885662 norm:0.0007785403868183494 max memory_allocated 22562.45068359375 
[2025-02-28 13:51:31 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 14 loss:0.01365000568330288 norm:0.0007533971802331507 max memory_allocated 22562.45068359375 
[2025-02-28 13:52:04 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 15 loss:0.013708770275115967 norm:0.0007733217789791524 max memory_allocated 22562.45068359375 
[2025-02-28 13:52:38 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 16 loss:0.013729185797274113 norm:0.000875375815667212 max memory_allocated 22562.45068359375 
[2025-02-28 13:53:11 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 17 loss:0.013709341175854206 norm:0.0006982824415899813 max memory_allocated 22562.45068359375 
[2025-02-28 13:53:44 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 18 loss:0.01366434060037136 norm:0.0006766383885405958 max memory_allocated 22562.45068359375 
[2025-02-28 13:54:18 root] (abq_llm_calib_config2.py 396): INFO layer 2 iter 19 loss:0.013650909997522831 norm:0.0007019421318545938 max memory_allocated 22562.45068359375 
[2025-02-28 13:54:27 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-02-28 13:55:03 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 0 loss:0.031030431389808655 norm:0.0013068121625110507 max memory_allocated 22562.50732421875 
[2025-02-28 13:55:37 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 1 loss:0.02381170354783535 norm:0.0005599769065156579 max memory_allocated 22562.50732421875 
[2025-02-28 13:56:10 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 2 loss:0.020355110988020897 norm:0.00032991106854751706 max memory_allocated 22562.50732421875 
[2025-02-28 13:56:43 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 3 loss:0.019192369654774666 norm:0.0002104095765389502 max memory_allocated 22562.50732421875 
[2025-02-28 13:57:17 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 4 loss:0.01854519173502922 norm:0.0001813958224374801 max memory_allocated 22562.50732421875 
[2025-02-28 13:57:50 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 5 loss:0.01823098585009575 norm:0.0001518918143119663 max memory_allocated 22562.50732421875 
[2025-02-28 13:58:23 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 6 loss:0.0181211419403553 norm:0.00013286204193718731 max memory_allocated 22562.50732421875 
[2025-02-28 13:58:56 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 7 loss:0.018083561211824417 norm:0.00011676997382892296 max memory_allocated 22562.50732421875 
[2025-02-28 13:59:30 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 8 loss:0.018050989136099815 norm:0.00011969550541834906 max memory_allocated 22562.50732421875 
[2025-02-28 14:00:03 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 9 loss:0.018040835857391357 norm:0.00011731121776392683 max memory_allocated 22562.50732421875 
[2025-02-28 14:00:36 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 10 loss:0.018037334084510803 norm:0.00011891241592820734 max memory_allocated 22562.50732421875 
[2025-02-28 14:01:10 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 11 loss:0.01801678165793419 norm:0.00011866640124935657 max memory_allocated 22562.50732421875 
[2025-02-28 14:01:43 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 12 loss:0.01800929754972458 norm:0.00011580183490877971 max memory_allocated 22562.50732421875 
[2025-02-28 14:02:16 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 13 loss:0.01800019107758999 norm:0.00011319432087475434 max memory_allocated 22562.50732421875 
[2025-02-28 14:02:50 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 14 loss:0.017994247376918793 norm:0.00011078897659899667 max memory_allocated 22562.50732421875 
[2025-02-28 14:03:23 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 15 loss:0.017988190054893494 norm:0.0001051596918841824 max memory_allocated 22562.50732421875 
[2025-02-28 14:03:56 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 16 loss:0.017983945086598396 norm:0.00010316963744116947 max memory_allocated 22562.50732421875 
[2025-02-28 14:04:30 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 17 loss:0.017977476119995117 norm:0.00010406339424662292 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:03 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 18 loss:0.017997536808252335 norm:0.00010468962136656046 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:36 root] (abq_llm_calib_config2.py 396): INFO layer 3 iter 19 loss:0.01799067296087742 norm:0.00010779357398860157 max memory_allocated 22562.50732421875 
[2025-02-28 14:05:45 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-02-28 14:06:22 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 0 loss:0.038178540766239166 norm:0.0014254137640818954 max memory_allocated 22562.67919921875 
[2025-02-28 14:06:55 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 1 loss:0.02943423204123974 norm:0.0005712701822631061 max memory_allocated 22562.67919921875 
[2025-02-28 14:07:28 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 2 loss:0.024852752685546875 norm:0.0003004229802172631 max memory_allocated 22562.67919921875 
[2025-02-28 14:08:01 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 3 loss:0.023408301174640656 norm:0.00020678836153820157 max memory_allocated 22562.67919921875 
[2025-02-28 14:08:35 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 4 loss:0.0226763803511858 norm:0.00015730866289231926 max memory_allocated 22562.67919921875 
[2025-02-28 14:09:08 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 5 loss:0.022322027012705803 norm:0.0001396915758959949 max memory_allocated 22562.67919921875 
[2025-02-28 14:09:41 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 6 loss:0.02219339832663536 norm:0.00012047523341607302 max memory_allocated 22562.67919921875 
[2025-02-28 14:10:15 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 7 loss:0.022114573046565056 norm:0.00011561589781194925 max memory_allocated 22562.67919921875 
[2025-02-28 14:10:48 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 8 loss:0.022061195224523544 norm:0.0001066931799869053 max memory_allocated 22562.67919921875 
[2025-02-28 14:11:21 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 9 loss:0.022028768435120583 norm:0.00011164803436258808 max memory_allocated 22562.67919921875 
[2025-02-28 14:11:54 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 10 loss:0.021976521238684654 norm:0.000113593865535222 max memory_allocated 22562.67919921875 
[2025-02-28 14:12:28 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 11 loss:0.021962279453873634 norm:0.00011437318607931957 max memory_allocated 22562.67919921875 
[2025-02-28 14:13:01 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 12 loss:0.021943487226963043 norm:0.00011314809671603143 max memory_allocated 22562.67919921875 
[2025-02-28 14:13:34 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 13 loss:0.021941961720585823 norm:0.00011938909301534295 max memory_allocated 22562.67919921875 
[2025-02-28 14:14:07 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 14 loss:0.021934842690825462 norm:0.00011509111936902627 max memory_allocated 22562.67919921875 
[2025-02-28 14:14:41 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 15 loss:0.02193353697657585 norm:0.00011480195098556578 max memory_allocated 22562.67919921875 
[2025-02-28 14:15:14 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 16 loss:0.021908842027187347 norm:0.0001080018118955195 max memory_allocated 22562.67919921875 
[2025-02-28 14:15:47 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 17 loss:0.021901965141296387 norm:0.00010515228495933115 max memory_allocated 22562.67919921875 
[2025-02-28 14:16:21 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 18 loss:0.02190450020134449 norm:0.00010841965558938682 max memory_allocated 22562.67919921875 
[2025-02-28 14:16:54 root] (abq_llm_calib_config2.py 396): INFO layer 4 iter 19 loss:0.021903324872255325 norm:0.00010750223009381443 max memory_allocated 22562.67919921875 
[2025-02-28 14:17:03 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-02-28 14:17:39 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 0 loss:0.03809679299592972 norm:0.0009634774178266525 max memory_allocated 22562.85107421875 
[2025-02-28 14:18:13 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 1 loss:0.03129542991518974 norm:0.00045145279727876186 max memory_allocated 22562.85107421875 
[2025-02-28 14:18:46 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 2 loss:0.027308426797389984 norm:0.0002808385470416397 max memory_allocated 22562.85107421875 
[2025-02-28 14:19:19 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 3 loss:0.02599574439227581 norm:0.00019611863535828888 max memory_allocated 22562.85107421875 
[2025-02-28 14:19:53 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 4 loss:0.025256281718611717 norm:0.00016194941417779773 max memory_allocated 22562.85107421875 
[2025-02-28 14:20:26 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 5 loss:0.02493739314377308 norm:0.00014657704741694033 max memory_allocated 22562.85107421875 
[2025-02-28 14:20:59 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 6 loss:0.024859560653567314 norm:0.0001478802878409624 max memory_allocated 22562.85107421875 
[2025-02-28 14:21:32 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 7 loss:0.024750977754592896 norm:0.00014966506569180638 max memory_allocated 22562.85107421875 
[2025-02-28 14:22:06 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 8 loss:0.024677511304616928 norm:0.00013810416567139328 max memory_allocated 22562.85107421875 
[2025-02-28 14:22:39 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 9 loss:0.024594880640506744 norm:0.00014285760698840022 max memory_allocated 22562.85107421875 
[2025-02-28 14:23:12 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 10 loss:0.024560395628213882 norm:0.00015911587979644537 max memory_allocated 22562.85107421875 
[2025-02-28 14:23:46 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 11 loss:0.024567417800426483 norm:0.00014153996016830206 max memory_allocated 22562.85107421875 
[2025-02-28 14:24:19 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 12 loss:0.02449859119951725 norm:0.00013337444397620857 max memory_allocated 22562.85107421875 
[2025-02-28 14:24:52 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 13 loss:0.024486824870109558 norm:0.00014388974523171782 max memory_allocated 22562.85107421875 
[2025-02-28 14:25:26 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 14 loss:0.024460749700665474 norm:0.00014450118760578334 max memory_allocated 22562.85107421875 
[2025-02-28 14:25:59 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 15 loss:0.024443401023745537 norm:0.00014204288891050965 max memory_allocated 22562.85107421875 
[2025-02-28 14:26:32 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 16 loss:0.024426762014627457 norm:0.00013522560766432434 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:06 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 17 loss:0.02442382462322712 norm:0.00014004508557263762 max memory_allocated 22562.85107421875 
[2025-02-28 14:27:39 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 18 loss:0.0244442131370306 norm:0.0001489365240558982 max memory_allocated 22562.85107421875 
[2025-02-28 14:28:12 root] (abq_llm_calib_config2.py 396): INFO layer 5 iter 19 loss:0.024416809901595116 norm:0.0001379100140184164 max memory_allocated 22562.85107421875 
[2025-02-28 14:28:21 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-02-28 14:28:58 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 0 loss:0.045099593698978424 norm:0.0011827542912214994 max memory_allocated 22563.02294921875 
[2025-02-28 14:29:31 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 1 loss:0.03672081604599953 norm:0.0005369561840780079 max memory_allocated 22563.02294921875 
[2025-02-28 14:30:04 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 2 loss:0.03174513578414917 norm:0.0003203317173756659 max memory_allocated 22563.02294921875 
[2025-02-28 14:30:38 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 3 loss:0.03009418211877346 norm:0.00022746837930753827 max memory_allocated 22563.02294921875 
[2025-02-28 14:31:11 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 4 loss:0.029324917122721672 norm:0.0001721614971756935 max memory_allocated 22563.02294921875 
[2025-02-28 14:31:44 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 5 loss:0.028959127143025398 norm:0.00014164425374474376 max memory_allocated 22563.02294921875 
[2025-02-28 14:32:17 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 6 loss:0.02879452332854271 norm:0.00013010139809921384 max memory_allocated 22563.02294921875 
[2025-02-28 14:32:51 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 7 loss:0.028693314641714096 norm:0.00012274895561859012 max memory_allocated 22563.02294921875 
[2025-02-28 14:33:24 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 8 loss:0.028609957545995712 norm:0.00012011112994514406 max memory_allocated 22563.02294921875 
[2025-02-28 14:33:57 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 9 loss:0.028541363775730133 norm:0.00011432013707235456 max memory_allocated 22563.02294921875 
[2025-02-28 14:34:31 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 10 loss:0.028497707098722458 norm:0.00012140262697357684 max memory_allocated 22563.02294921875 
[2025-02-28 14:35:04 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 11 loss:0.028455382212996483 norm:0.00012004603195236996 max memory_allocated 22563.02294921875 
[2025-02-28 14:35:37 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 12 loss:0.028414329513907433 norm:0.00012313124898355454 max memory_allocated 22563.02294921875 
[2025-02-28 14:36:11 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 13 loss:0.02837963029742241 norm:0.00012112109106965363 max memory_allocated 22563.02294921875 
[2025-02-28 14:36:44 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 14 loss:0.028368625789880753 norm:0.00011907897714991122 max memory_allocated 22563.02294921875 
[2025-02-28 14:37:17 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 15 loss:0.0283458661288023 norm:0.00012055064871674404 max memory_allocated 22563.02294921875 
[2025-02-28 14:37:51 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 16 loss:0.028323253616690636 norm:0.00011184771574335173 max memory_allocated 22563.02294921875 
[2025-02-28 14:38:24 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 17 loss:0.02831212244927883 norm:0.000110914210381452 max memory_allocated 22563.02294921875 
[2025-02-28 14:38:57 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 18 loss:0.02830282412469387 norm:0.00010925609240075573 max memory_allocated 22563.02294921875 
[2025-02-28 14:39:30 root] (abq_llm_calib_config2.py 396): INFO layer 6 iter 19 loss:0.028292406350374222 norm:0.00011041238758480176 max memory_allocated 22563.02294921875 
[2025-02-28 14:39:40 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-02-28 14:40:16 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 0 loss:0.050258755683898926 norm:0.0012970988173037767 max memory_allocated 22563.19482421875 
[2025-02-28 14:40:49 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 1 loss:0.04084741324186325 norm:0.0005726952222175896 max memory_allocated 22563.19482421875 
[2025-02-28 14:41:22 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 2 loss:0.035405755043029785 norm:0.00035993653000332415 max memory_allocated 22563.19482421875 
[2025-02-28 14:41:56 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 3 loss:0.033675774931907654 norm:0.0002457691007293761 max memory_allocated 22563.19482421875 
[2025-02-28 14:42:29 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 4 loss:0.03293391689658165 norm:0.00019648367015179247 max memory_allocated 22563.19482421875 
[2025-02-28 14:43:02 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 5 loss:0.03258445858955383 norm:0.0001661593996686861 max memory_allocated 22563.19482421875 
[2025-02-28 14:43:36 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 6 loss:0.03240252658724785 norm:0.00014524257858283818 max memory_allocated 22563.19482421875 
[2025-02-28 14:44:09 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 7 loss:0.03228461742401123 norm:0.00013426695659291 max memory_allocated 22563.19482421875 
[2025-02-28 14:44:42 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 8 loss:0.03219850733876228 norm:0.00012714980402961373 max memory_allocated 22563.19482421875 
[2025-02-28 14:45:16 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 9 loss:0.032140396535396576 norm:0.00011797570914495736 max memory_allocated 22563.19482421875 
[2025-02-28 14:45:49 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 10 loss:0.03207648545503616 norm:0.00011450757301645353 max memory_allocated 22563.19482421875 
[2025-02-28 14:46:22 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 11 loss:0.032007910311222076 norm:0.00011367704428266734 max memory_allocated 22563.19482421875 
[2025-02-28 14:46:56 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 12 loss:0.0319591648876667 norm:0.00011468911543488503 max memory_allocated 22563.19482421875 
[2025-02-28 14:47:29 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 13 loss:0.03193863481283188 norm:0.00011197151616215706 max memory_allocated 22563.19482421875 
[2025-02-28 14:48:02 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 14 loss:0.031921517103910446 norm:0.00010751913214335218 max memory_allocated 22563.19482421875 
[2025-02-28 14:48:35 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 15 loss:0.031893957406282425 norm:0.00010552177991485223 max memory_allocated 22563.19482421875 
[2025-02-28 14:49:09 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 16 loss:0.03188369423151016 norm:0.00010706376633606851 max memory_allocated 22563.19482421875 
[2025-02-28 14:49:42 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 17 loss:0.03186875581741333 norm:0.00010581194510450587 max memory_allocated 22563.19482421875 
[2025-02-28 14:50:15 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 18 loss:0.031856365501880646 norm:0.00010442599887028337 max memory_allocated 22563.19482421875 
[2025-02-28 14:50:49 root] (abq_llm_calib_config2.py 396): INFO layer 7 iter 19 loss:0.031843360513448715 norm:0.00010478320473339409 max memory_allocated 22563.19482421875 
[2025-02-28 14:50:58 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-02-28 14:51:34 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 0 loss:0.05194114148616791 norm:0.0010261248098686337 max memory_allocated 22563.36669921875 
[2025-02-28 14:52:07 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 1 loss:0.04349822178483009 norm:0.0005284641520120203 max memory_allocated 22563.36669921875 
[2025-02-28 14:52:41 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 2 loss:0.03851671144366264 norm:0.00032362304045818746 max memory_allocated 22563.36669921875 
[2025-02-28 14:53:14 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 3 loss:0.036966487765312195 norm:0.00026119028916582465 max memory_allocated 22563.36669921875 
[2025-02-28 14:53:47 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 4 loss:0.03613150492310524 norm:0.0002094327937811613 max memory_allocated 22563.36669921875 
[2025-02-28 14:54:21 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 5 loss:0.03573920577764511 norm:0.00018934346735477448 max memory_allocated 22563.36669921875 
[2025-02-28 14:54:54 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 6 loss:0.035501010715961456 norm:0.0001795311545720324 max memory_allocated 22563.36669921875 
[2025-02-28 14:55:27 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 7 loss:0.03531455248594284 norm:0.00017427005514036864 max memory_allocated 22563.36669921875 
[2025-02-28 14:56:01 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 8 loss:0.03521576151251793 norm:0.00017161552386824042 max memory_allocated 22563.36669921875 
[2025-02-28 14:56:34 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 9 loss:0.03520021215081215 norm:0.0001830866967793554 max memory_allocated 22563.36669921875 
[2025-02-28 14:57:07 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 10 loss:0.03508152812719345 norm:0.00016894537839107215 max memory_allocated 22563.36669921875 
[2025-02-28 14:57:41 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 11 loss:0.03503294661641121 norm:0.0001645447628106922 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:14 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 12 loss:0.03498329222202301 norm:0.0001672758226050064 max memory_allocated 22563.36669921875 
[2025-02-28 14:58:47 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 13 loss:0.03493976220488548 norm:0.00016172497998923063 max memory_allocated 22563.36669921875 
[2025-02-28 14:59:21 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 14 loss:0.03490711748600006 norm:0.00016600466915406287 max memory_allocated 22563.36669921875 
[2025-02-28 14:59:54 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 15 loss:0.034879256039857864 norm:0.0001509946014266461 max memory_allocated 22563.36669921875 
[2025-02-28 15:00:27 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 16 loss:0.03486562520265579 norm:0.0001549209118820727 max memory_allocated 22563.36669921875 
[2025-02-28 15:01:00 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 17 loss:0.03488955646753311 norm:0.0001608368183951825 max memory_allocated 22563.36669921875 
[2025-02-28 15:01:34 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 18 loss:0.034863509237766266 norm:0.00015682463708799332 max memory_allocated 22563.36669921875 
[2025-02-28 15:02:07 root] (abq_llm_calib_config2.py 396): INFO layer 8 iter 19 loss:0.034822016954422 norm:0.00014827690029051155 max memory_allocated 22563.36669921875 
[2025-02-28 15:02:16 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-02-28 15:02:54 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 0 loss:0.052586715668439865 norm:0.0009059585863724351 max memory_allocated 22563.53857421875 
[2025-02-28 15:03:27 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 1 loss:0.04474746435880661 norm:0.00040363564039580524 max memory_allocated 22563.53857421875 
[2025-02-28 15:04:00 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 2 loss:0.04031359776854515 norm:0.0002481850970070809 max memory_allocated 22563.53857421875 
[2025-02-28 15:04:33 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 3 loss:0.038930073380470276 norm:0.00019031116971746087 max memory_allocated 22563.53857421875 
[2025-02-28 15:05:07 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 4 loss:0.03817378729581833 norm:0.00018083309987559915 max memory_allocated 22563.53857421875 
[2025-02-28 15:05:40 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 5 loss:0.037813570350408554 norm:0.00016289186896756291 max memory_allocated 22563.53857421875 
[2025-02-28 15:06:13 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 6 loss:0.03757528215646744 norm:0.0001402803318342194 max memory_allocated 22563.53857421875 
[2025-02-28 15:06:46 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 7 loss:0.03743608295917511 norm:0.0001345179189229384 max memory_allocated 22563.53857421875 
[2025-02-28 15:07:20 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 8 loss:0.03735982999205589 norm:0.00013588809815701097 max memory_allocated 22563.53857421875 
[2025-02-28 15:07:53 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 9 loss:0.037286221981048584 norm:0.0001321090676356107 max memory_allocated 22563.53857421875 
[2025-02-28 15:08:26 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 10 loss:0.03724748641252518 norm:0.00013432666310109198 max memory_allocated 22563.53857421875 
[2025-02-28 15:09:00 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 11 loss:0.037222180515527725 norm:0.00012718432117253542 max memory_allocated 22563.53857421875 
[2025-02-28 15:09:33 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 12 loss:0.037160955369472504 norm:0.00012966993381269276 max memory_allocated 22563.53857421875 
[2025-02-28 15:10:06 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 13 loss:0.03714565187692642 norm:0.00012754365161526948 max memory_allocated 22563.53857421875 
[2025-02-28 15:10:40 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 14 loss:0.03713628277182579 norm:0.00012367153249215335 max memory_allocated 22563.53857421875 
[2025-02-28 15:11:13 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 15 loss:0.03711627796292305 norm:0.00012206226529087871 max memory_allocated 22563.53857421875 
[2025-02-28 15:11:46 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 16 loss:0.03709917142987251 norm:0.0001287421036977321 max memory_allocated 22563.53857421875 
[2025-02-28 15:12:20 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 17 loss:0.03708195313811302 norm:0.00012996306759305298 max memory_allocated 22563.53857421875 
[2025-02-28 15:12:53 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 18 loss:0.03706718608736992 norm:0.00012861784489359707 max memory_allocated 22563.53857421875 
[2025-02-28 15:13:26 root] (abq_llm_calib_config2.py 396): INFO layer 9 iter 19 loss:0.03708074986934662 norm:0.0001317685964750126 max memory_allocated 22563.53857421875 
[2025-02-28 15:13:35 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-02-28 15:14:12 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 0 loss:0.051880352199077606 norm:0.0006414324743673205 max memory_allocated 22563.71044921875 
[2025-02-28 15:14:45 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 1 loss:0.04582255333662033 norm:0.00032461073715239763 max memory_allocated 22563.71044921875 
[2025-02-28 15:15:18 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 2 loss:0.04181074723601341 norm:0.00020459886582102627 max memory_allocated 22563.71044921875 
[2025-02-28 15:15:52 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 3 loss:0.04056943207979202 norm:0.00017107086023315787 max memory_allocated 22563.71044921875 
[2025-02-28 15:16:25 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 4 loss:0.03984227031469345 norm:0.00015477843408007175 max memory_allocated 22563.71044921875 
[2025-02-28 15:16:58 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 5 loss:0.03948258236050606 norm:0.00014879051013849676 max memory_allocated 22563.71044921875 
[2025-02-28 15:17:32 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 6 loss:0.03928197920322418 norm:0.00013444903015624732 max memory_allocated 22563.71044921875 
[2025-02-28 15:18:05 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 7 loss:0.039137888699769974 norm:0.0001336057612206787 max memory_allocated 22563.71044921875 
[2025-02-28 15:18:38 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 8 loss:0.03906577080488205 norm:0.00014251863467507064 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:12 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 9 loss:0.0389847606420517 norm:0.00012924332986585796 max memory_allocated 22563.71044921875 
[2025-02-28 15:19:45 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 10 loss:0.03893338888883591 norm:0.00012538296869024634 max memory_allocated 22563.71044921875 
[2025-02-28 15:20:18 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 11 loss:0.03889504075050354 norm:0.00012530619278550148 max memory_allocated 22563.71044921875 
[2025-02-28 15:20:52 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 12 loss:0.03890962898731232 norm:0.00013957085320726037 max memory_allocated 22563.71044921875 
[2025-02-28 15:21:25 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 13 loss:0.03889558091759682 norm:0.0001223728177137673 max memory_allocated 22563.71044921875 
[2025-02-28 15:21:58 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 14 loss:0.03887392207980156 norm:0.00012471008813008666 max memory_allocated 22563.71044921875 
[2025-02-28 15:22:31 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 15 loss:0.03884435445070267 norm:0.00012435765529517084 max memory_allocated 22563.71044921875 
[2025-02-28 15:23:05 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 16 loss:0.038825057446956635 norm:0.00012132072151871398 max memory_allocated 22563.71044921875 
[2025-02-28 15:23:38 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 17 loss:0.0388161838054657 norm:0.00012366764713078737 max memory_allocated 22563.71044921875 
[2025-02-28 15:24:11 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 18 loss:0.03881913796067238 norm:0.0001257715339306742 max memory_allocated 22563.71044921875 
[2025-02-28 15:24:45 root] (abq_llm_calib_config2.py 396): INFO layer 10 iter 19 loss:0.03880700096487999 norm:0.0001230757188750431 max memory_allocated 22563.71044921875 
[2025-02-28 15:24:54 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-02-28 15:25:30 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 0 loss:0.054825976490974426 norm:0.0007364103803411126 max memory_allocated 22563.88232421875 
[2025-02-28 15:26:04 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 1 loss:0.048067424446344376 norm:0.00037535896990448236 max memory_allocated 22563.88232421875 
[2025-02-28 15:26:37 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 2 loss:0.04386008903384209 norm:0.00025028380332514644 max memory_allocated 22563.88232421875 
[2025-02-28 15:27:10 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 3 loss:0.042521581053733826 norm:0.00020715815480798483 max memory_allocated 22563.88232421875 
[2025-02-28 15:27:43 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 4 loss:0.04175741970539093 norm:0.00018473603995516896 max memory_allocated 22563.88232421875 
[2025-02-28 15:28:17 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 5 loss:0.04131968319416046 norm:0.0001671276695560664 max memory_allocated 22563.88232421875 
[2025-02-28 15:28:50 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 6 loss:0.04108627140522003 norm:0.00016076370957307518 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:23 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 7 loss:0.040931586176157 norm:0.000159749950398691 max memory_allocated 22563.88232421875 
[2025-02-28 15:29:57 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 8 loss:0.0408325269818306 norm:0.00014454504707828164 max memory_allocated 22563.88232421875 
[2025-02-28 15:30:30 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 9 loss:0.040762029588222504 norm:0.0001575778442202136 max memory_allocated 22563.88232421875 
[2025-02-28 15:31:03 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 10 loss:0.04067413508892059 norm:0.0001429646508768201 max memory_allocated 22563.88232421875 
[2025-02-28 15:31:37 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 11 loss:0.040665581822395325 norm:0.00013147779100108892 max memory_allocated 22563.88232421875 
[2025-02-28 15:32:10 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 12 loss:0.040562648326158524 norm:0.00012102836626581848 max memory_allocated 22563.88232421875 
[2025-02-28 15:32:43 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 13 loss:0.040509939193725586 norm:0.00011676757276291028 max memory_allocated 22563.88232421875 
[2025-02-28 15:33:17 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 14 loss:0.04048925265669823 norm:0.0001288695930270478 max memory_allocated 22563.88232421875 
[2025-02-28 15:33:50 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 15 loss:0.04046405106782913 norm:0.00012314325431361794 max memory_allocated 22563.88232421875 
[2025-02-28 15:34:23 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 16 loss:0.04045235365629196 norm:0.00013308294001035392 max memory_allocated 22563.88232421875 
[2025-02-28 15:34:57 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 17 loss:0.04045421630144119 norm:0.00012224726378917694 max memory_allocated 22563.88232421875 
[2025-02-28 15:35:30 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 18 loss:0.04045295715332031 norm:0.00011912023182958364 max memory_allocated 22563.88232421875 
[2025-02-28 15:36:03 root] (abq_llm_calib_config2.py 396): INFO layer 11 iter 19 loss:0.04044268652796745 norm:0.00012529807281680405 max memory_allocated 22563.88232421875 
[2025-02-28 15:36:12 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-02-28 15:36:49 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 0 loss:0.05522867664694786 norm:0.0006116299773566425 max memory_allocated 22564.05419921875 
[2025-02-28 15:37:22 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 1 loss:0.049128297716379166 norm:0.00032626400934532285 max memory_allocated 22564.05419921875 
[2025-02-28 15:37:55 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 2 loss:0.04498390853404999 norm:0.00021921380539424717 max memory_allocated 22564.05419921875 
[2025-02-28 15:38:28 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 3 loss:0.043699413537979126 norm:0.00017922781989909708 max memory_allocated 22564.05419921875 
[2025-02-28 15:39:02 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 4 loss:0.04304901510477066 norm:0.00016403621702920645 max memory_allocated 22564.05419921875 
[2025-02-28 15:39:35 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 5 loss:0.04263821244239807 norm:0.00014646702038589865 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:08 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 6 loss:0.042447347193956375 norm:0.0001426476810593158 max memory_allocated 22564.05419921875 
[2025-02-28 15:40:42 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 7 loss:0.04229748994112015 norm:0.0001281224103877321 max memory_allocated 22564.05419921875 
[2025-02-28 15:41:15 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 8 loss:0.04220957309007645 norm:0.00013412637053988874 max memory_allocated 22564.05419921875 
[2025-02-28 15:41:48 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 9 loss:0.04215620458126068 norm:0.00012762266851495951 max memory_allocated 22564.05419921875 
[2025-02-28 15:42:22 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 10 loss:0.042108964174985886 norm:0.0001328724029008299 max memory_allocated 22564.05419921875 
[2025-02-28 15:42:55 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 11 loss:0.04202103987336159 norm:0.00012469639477785677 max memory_allocated 22564.05419921875 
[2025-02-28 15:43:28 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 12 loss:0.04196576774120331 norm:0.00011117841495433822 max memory_allocated 22564.05419921875 
[2025-02-28 15:44:02 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 13 loss:0.041950225830078125 norm:0.00011016760254278779 max memory_allocated 22564.05419921875 
[2025-02-28 15:44:35 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 14 loss:0.041912347078323364 norm:0.00011659723531920463 max memory_allocated 22564.05419921875 
[2025-02-28 15:45:08 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 15 loss:0.041887007653713226 norm:0.00012446720211301 max memory_allocated 22564.05419921875 
[2025-02-28 15:45:41 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 16 loss:0.041881222277879715 norm:0.00011453065235400572 max memory_allocated 22564.05419921875 
[2025-02-28 15:46:15 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 17 loss:0.04187284782528877 norm:0.00011221090971957892 max memory_allocated 22564.05419921875 
[2025-02-28 15:46:48 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 18 loss:0.04186011850833893 norm:0.00011677573638735339 max memory_allocated 22564.05419921875 
[2025-02-28 15:47:21 root] (abq_llm_calib_config2.py 396): INFO layer 12 iter 19 loss:0.04184599220752716 norm:0.00011734799045370892 max memory_allocated 22564.05419921875 
[2025-02-28 15:47:31 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-02-28 15:48:07 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 0 loss:0.05478736013174057 norm:0.000803367467597127 max memory_allocated 22564.22607421875 
[2025-02-28 15:48:40 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 1 loss:0.04898029938340187 norm:0.0003983800415880978 max memory_allocated 22564.22607421875 
[2025-02-28 15:49:14 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 2 loss:0.044870417565107346 norm:0.00024460823624394834 max memory_allocated 22564.22607421875 
[2025-02-28 15:49:47 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 3 loss:0.04364427551627159 norm:0.00019067799439653754 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:20 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 4 loss:0.04294981434941292 norm:0.00015764980344101787 max memory_allocated 22564.22607421875 
[2025-02-28 15:50:54 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 5 loss:0.04255472123622894 norm:0.000146353937452659 max memory_allocated 22564.22607421875 
[2025-02-28 15:51:27 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 6 loss:0.04233958572149277 norm:0.00013305828906595707 max memory_allocated 22564.22607421875 
[2025-02-28 15:52:00 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 7 loss:0.04219004511833191 norm:0.00012798576790373772 max memory_allocated 22564.22607421875 
[2025-02-28 15:52:34 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 8 loss:0.042070042341947556 norm:0.0001260527060367167 max memory_allocated 22564.22607421875 
[2025-02-28 15:53:07 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 9 loss:0.04200213402509689 norm:0.00012097596481908113 max memory_allocated 22564.22607421875 
[2025-02-28 15:53:40 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 10 loss:0.04193199798464775 norm:0.0001218839461216703 max memory_allocated 22564.22607421875 
[2025-02-28 15:54:13 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 11 loss:0.041908182203769684 norm:0.00011895628995262086 max memory_allocated 22564.22607421875 
[2025-02-28 15:54:47 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 12 loss:0.041860781610012054 norm:0.00011422006355132908 max memory_allocated 22564.22607421875 
[2025-02-28 15:55:20 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 13 loss:0.041829463094472885 norm:0.00011196947889402509 max memory_allocated 22564.22607421875 
[2025-02-28 15:55:53 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 14 loss:0.041795261204242706 norm:0.00011987252219114453 max memory_allocated 22564.22607421875 
[2025-02-28 15:56:27 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 15 loss:0.04175686091184616 norm:0.00011489165626699105 max memory_allocated 22564.22607421875 
[2025-02-28 15:57:00 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 16 loss:0.04174121841788292 norm:0.00011877212091349065 max memory_allocated 22564.22607421875 
[2025-02-28 15:57:33 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 17 loss:0.04175259917974472 norm:0.00012204117956571281 max memory_allocated 22564.22607421875 
[2025-02-28 15:58:07 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 18 loss:0.0417168065905571 norm:0.00011136026296298951 max memory_allocated 22564.22607421875 
[2025-02-28 15:58:40 root] (abq_llm_calib_config2.py 396): INFO layer 13 iter 19 loss:0.04169922694563866 norm:0.00011544530570972711 max memory_allocated 22564.22607421875 
[2025-02-28 15:58:49 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-02-28 15:59:26 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 0 loss:0.05507329851388931 norm:0.000583886809181422 max memory_allocated 22564.39794921875 
[2025-02-28 15:59:59 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 1 loss:0.049441128969192505 norm:0.00030886963941156864 max memory_allocated 22564.39794921875 
[2025-02-28 16:00:32 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 2 loss:0.045544546097517014 norm:0.00020706004579551518 max memory_allocated 22564.39794921875 
[2025-02-28 16:01:06 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 3 loss:0.04438294842839241 norm:0.00016927230171859264 max memory_allocated 22564.39794921875 
[2025-02-28 16:01:39 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 4 loss:0.043707408010959625 norm:0.0001458431506762281 max memory_allocated 22564.39794921875 
[2025-02-28 16:02:12 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 5 loss:0.043294843286275864 norm:0.00012589909601956606 max memory_allocated 22564.39794921875 
[2025-02-28 16:02:45 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 6 loss:0.043055128306150436 norm:0.00011496881779748946 max memory_allocated 22564.39794921875 
[2025-02-28 16:03:19 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 7 loss:0.042907871305942535 norm:0.00011445015843492001 max memory_allocated 22564.39794921875 
[2025-02-28 16:03:52 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 8 loss:0.0428067110478878 norm:0.00011348719999659806 max memory_allocated 22564.39794921875 
[2025-02-28 16:04:25 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 9 loss:0.04271173104643822 norm:0.00010693824879126623 max memory_allocated 22564.39794921875 
[2025-02-28 16:04:59 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 10 loss:0.04263322800397873 norm:9.86132727121003e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:05:32 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 11 loss:0.04258817434310913 norm:9.32127222768031e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:06:05 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 12 loss:0.04254606366157532 norm:9.410288475919515e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:06:39 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 13 loss:0.04251902922987938 norm:9.845094609772786e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:07:12 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 14 loss:0.0424969457089901 norm:9.42764017963782e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:07:45 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 15 loss:0.042471498250961304 norm:8.563496521674097e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:08:18 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 16 loss:0.042451146990060806 norm:8.642056491225958e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:08:52 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 17 loss:0.04242595657706261 norm:8.387207344640046e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:09:25 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 18 loss:0.042415522038936615 norm:8.415981574216858e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:09:58 root] (abq_llm_calib_config2.py 396): INFO layer 14 iter 19 loss:0.042405977845191956 norm:8.930295007303357e-05 max memory_allocated 22564.39794921875 
[2025-02-28 16:10:08 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-02-28 16:10:44 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 0 loss:0.05844585970044136 norm:0.0009257769561372697 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:17 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 1 loss:0.05133877322077751 norm:0.00039355491753667593 max memory_allocated 22564.56982421875 
[2025-02-28 16:11:51 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 2 loss:0.04674394056200981 norm:0.0002463855780661106 max memory_allocated 22564.56982421875 
[2025-02-28 16:12:24 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 3 loss:0.04535441845655441 norm:0.00017690344247967005 max memory_allocated 22564.56982421875 
[2025-02-28 16:12:57 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 4 loss:0.04465089365839958 norm:0.0001460794301237911 max memory_allocated 22564.56982421875 
[2025-02-28 16:13:30 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 5 loss:0.0442507378757 norm:0.00013467721873894334 max memory_allocated 22564.56982421875 
[2025-02-28 16:14:04 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 6 loss:0.04402678832411766 norm:0.00012438355770427734 max memory_allocated 22564.56982421875 
[2025-02-28 16:14:37 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 7 loss:0.04389204457402229 norm:0.0001142680921475403 max memory_allocated 22564.56982421875 
[2025-02-28 16:15:10 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 8 loss:0.043780308216810226 norm:0.00010548633872531354 max memory_allocated 22564.56982421875 
[2025-02-28 16:15:44 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 9 loss:0.04369751363992691 norm:0.000100238888990134 max memory_allocated 22564.56982421875 
[2025-02-28 16:16:17 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 10 loss:0.04361213371157646 norm:9.955361019819975e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:16:50 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 11 loss:0.04354681819677353 norm:9.752005280461162e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:17:24 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 12 loss:0.04349127411842346 norm:9.37860895646736e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:17:57 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 13 loss:0.04344531148672104 norm:8.942023850977421e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:18:30 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 14 loss:0.04340915009379387 norm:8.651439566165209e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:19:04 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 15 loss:0.043371062725782394 norm:8.675429853610694e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:19:37 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 16 loss:0.043333109468221664 norm:8.458623779006302e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:20:10 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 17 loss:0.04330969601869583 norm:8.10758865554817e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:20:43 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 18 loss:0.04329010099172592 norm:8.266269287560135e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:21:17 root] (abq_llm_calib_config2.py 396): INFO layer 15 iter 19 loss:0.04328136146068573 norm:8.042845001909882e-05 max memory_allocated 22564.56982421875 
[2025-02-28 16:21:26 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-02-28 16:22:02 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 0 loss:0.060475412756204605 norm:0.0011307180393487215 max memory_allocated 22564.74169921875 
[2025-02-28 16:22:36 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 1 loss:0.052987512201070786 norm:0.00045910367043688893 max memory_allocated 22564.74169921875 
[2025-02-28 16:23:09 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 2 loss:0.04798506945371628 norm:0.0002552029909566045 max memory_allocated 22564.74169921875 
[2025-02-28 16:23:42 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 3 loss:0.04657638445496559 norm:0.00019181992684025317 max memory_allocated 22564.74169921875 
[2025-02-28 16:24:16 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 4 loss:0.0458076074719429 norm:0.00016097634215839207 max memory_allocated 22564.74169921875 
[2025-02-28 16:24:49 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 5 loss:0.0453534796833992 norm:0.00014457403449341655 max memory_allocated 22564.74169921875 
[2025-02-28 16:25:22 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 6 loss:0.04510711133480072 norm:0.00013643100101035088 max memory_allocated 22564.74169921875 
[2025-02-28 16:25:56 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 7 loss:0.04493416100740433 norm:0.0001251408102689311 max memory_allocated 22564.74169921875 
[2025-02-28 16:26:29 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 8 loss:0.04482051357626915 norm:0.00011856743367388844 max memory_allocated 22564.74169921875 
[2025-02-28 16:27:02 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 9 loss:0.04472024738788605 norm:0.00011192775855306536 max memory_allocated 22564.74169921875 
[2025-02-28 16:27:36 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 10 loss:0.04463551566004753 norm:0.00010649633622961119 max memory_allocated 22564.74169921875 
[2025-02-28 16:28:09 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 11 loss:0.044556230306625366 norm:0.00010085861867992207 max memory_allocated 22564.74169921875 
[2025-02-28 16:28:42 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 12 loss:0.04450633004307747 norm:9.883163147605956e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:29:16 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 13 loss:0.04444485902786255 norm:0.0001019706396618858 max memory_allocated 22564.74169921875 
[2025-02-28 16:29:49 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 14 loss:0.04439367726445198 norm:9.766440780367702e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:30:22 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 15 loss:0.04434622824192047 norm:9.134560241363943e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:30:56 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 16 loss:0.04430604353547096 norm:8.648483344586566e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:31:29 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 17 loss:0.04427482932806015 norm:8.810842700768262e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:32:02 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 18 loss:0.04423411190509796 norm:8.850860467646271e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:32:36 root] (abq_llm_calib_config2.py 396): INFO layer 16 iter 19 loss:0.04420770704746246 norm:8.073537901509553e-05 max memory_allocated 22564.74169921875 
[2025-02-28 16:32:45 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-02-28 16:33:21 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 0 loss:0.051834605634212494 norm:0.001596926711499691 max memory_allocated 22564.91357421875 
[2025-02-28 16:33:54 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 1 loss:0.048547618091106415 norm:0.0004872246936429292 max memory_allocated 22564.91357421875 
[2025-02-28 16:34:28 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 2 loss:0.04678495600819588 norm:0.0002602589374873787 max memory_allocated 22564.91357421875 
[2025-02-28 16:35:01 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 3 loss:0.045553550124168396 norm:0.0001939115609275177 max memory_allocated 22564.91357421875 
[2025-02-28 16:35:34 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 4 loss:0.04480663686990738 norm:0.0001571421162225306 max memory_allocated 22564.91357421875 
[2025-02-28 16:36:08 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 5 loss:0.04441755264997482 norm:0.0001351491955574602 max memory_allocated 22564.91357421875 
[2025-02-28 16:36:41 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 6 loss:0.04425101727247238 norm:0.00012952799443155527 max memory_allocated 22564.91357421875 
[2025-02-28 16:37:15 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 7 loss:0.04416711628437042 norm:0.00012882534065283835 max memory_allocated 22564.91357421875 
[2025-02-28 16:37:48 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 8 loss:0.044081851840019226 norm:0.00011532385542523116 max memory_allocated 22564.91357421875 
[2025-02-28 16:38:21 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 9 loss:0.044059690088033676 norm:9.862476144917309e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:38:55 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 10 loss:0.04403318464756012 norm:9.678426431491971e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:39:28 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 11 loss:0.04401824250817299 norm:9.41454345593229e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:40:01 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 12 loss:0.04400103539228439 norm:9.36225987970829e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:40:35 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 13 loss:0.04396273195743561 norm:8.292013080790639e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:41:08 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 14 loss:0.04395501688122749 norm:8.720413461560383e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:41:41 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 15 loss:0.04394816979765892 norm:7.917963375803083e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:42:15 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 16 loss:0.04394058138132095 norm:8.16656174720265e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:42:48 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 17 loss:0.043928321450948715 norm:8.069720206549391e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:43:21 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 18 loss:0.04392043128609657 norm:8.087707101367414e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:43:55 root] (abq_llm_calib_config2.py 396): INFO layer 17 iter 19 loss:0.043939054012298584 norm:8.364995301235467e-05 max memory_allocated 22564.91357421875 
[2025-02-28 16:44:04 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-02-28 16:44:40 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 0 loss:0.05540947616100311 norm:0.001336892368271947 max memory_allocated 22565.08544921875 
[2025-02-28 16:45:14 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 1 loss:0.051749736070632935 norm:0.0004984858678653836 max memory_allocated 22565.08544921875 
[2025-02-28 16:45:47 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 2 loss:0.049903541803359985 norm:0.0002943634463008493 max memory_allocated 22565.08544921875 
[2025-02-28 16:46:20 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 3 loss:0.04850724712014198 norm:0.00021533167455345392 max memory_allocated 22565.08544921875 
[2025-02-28 16:46:54 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 4 loss:0.047584086656570435 norm:0.0001743807370075956 max memory_allocated 22565.08544921875 
[2025-02-28 16:47:27 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 5 loss:0.04714052006602287 norm:0.00016217507072724402 max memory_allocated 22565.08544921875 
[2025-02-28 16:48:00 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 6 loss:0.046928443014621735 norm:0.00013950763968750834 max memory_allocated 22565.08544921875 
[2025-02-28 16:48:34 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 7 loss:0.04686935245990753 norm:0.00013131077867001295 max memory_allocated 22565.08544921875 
[2025-02-28 16:49:07 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 8 loss:0.0467785969376564 norm:0.00012442785373423249 max memory_allocated 22565.08544921875 
[2025-02-28 16:49:40 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 9 loss:0.04670843109488487 norm:0.00011525188165251166 max memory_allocated 22565.08544921875 
[2025-02-28 16:50:14 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 10 loss:0.04666117578744888 norm:0.00011174406245118007 max memory_allocated 22565.08544921875 
[2025-02-28 16:50:47 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 11 loss:0.046635501086711884 norm:0.00010338617721572518 max memory_allocated 22565.08544921875 
[2025-02-28 16:51:21 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 12 loss:0.04661155492067337 norm:0.00010006835509557277 max memory_allocated 22565.08544921875 
[2025-02-28 16:51:54 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 13 loss:0.046607453376054764 norm:9.751245670486242e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:52:27 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 14 loss:0.04657638445496559 norm:9.618814510758966e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:53:01 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 15 loss:0.04654291272163391 norm:9.138519817497581e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:53:34 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 16 loss:0.04652072861790657 norm:9.330289321951568e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:54:07 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 17 loss:0.04650171846151352 norm:8.803964010439813e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:54:41 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 18 loss:0.04649735242128372 norm:9.045017941389233e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:55:14 root] (abq_llm_calib_config2.py 396): INFO layer 18 iter 19 loss:0.046491410583257675 norm:9.088802471524104e-05 max memory_allocated 22565.08544921875 
[2025-02-28 16:55:23 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-02-28 16:56:00 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 0 loss:0.05885690078139305 norm:0.001469364739023149 max memory_allocated 22565.25732421875 
[2025-02-28 16:56:33 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 1 loss:0.05552821233868599 norm:0.0005502274143509567 max memory_allocated 22565.25732421875 
[2025-02-28 16:57:06 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 2 loss:0.053210627287626266 norm:0.0002440560347167775 max memory_allocated 22565.25732421875 
[2025-02-28 16:57:40 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 3 loss:0.05195474624633789 norm:0.00018370950419921428 max memory_allocated 22565.25732421875 
[2025-02-28 16:58:13 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 4 loss:0.05110764130949974 norm:0.00015355269715655595 max memory_allocated 22565.25732421875 
[2025-02-28 16:58:47 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 5 loss:0.0506940521299839 norm:0.00013578677317127585 max memory_allocated 22565.25732421875 
[2025-02-28 16:59:20 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 6 loss:0.05051765590906143 norm:0.0001272450463147834 max memory_allocated 22565.25732421875 
[2025-02-28 16:59:53 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 7 loss:0.050438523292541504 norm:0.00010904080409090966 max memory_allocated 22565.25732421875 
[2025-02-28 17:00:27 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 8 loss:0.05037738382816315 norm:0.00011620776786003262 max memory_allocated 22565.25732421875 
[2025-02-28 17:01:00 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 9 loss:0.05033281818032265 norm:0.00010535262117628008 max memory_allocated 22565.25732421875 
[2025-02-28 17:01:33 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 10 loss:0.05030742660164833 norm:9.364985453430563e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:02:07 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 11 loss:0.0502571202814579 norm:9.005332685774192e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:02:40 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 12 loss:0.050252657383680344 norm:9.776871593203396e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:03:14 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 13 loss:0.05022873356938362 norm:9.546801447868347e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:03:47 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 14 loss:0.050203025341033936 norm:8.754357986617833e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:04:20 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 15 loss:0.050179287791252136 norm:8.166750922100618e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:04:54 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 16 loss:0.050148073583841324 norm:8.021338726393878e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:05:27 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 17 loss:0.05013557896018028 norm:8.148214692482725e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:06:00 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 18 loss:0.05012482404708862 norm:8.038082160055637e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:06:34 root] (abq_llm_calib_config2.py 396): INFO layer 19 iter 19 loss:0.05012652277946472 norm:7.510861905757338e-05 max memory_allocated 22565.25732421875 
[2025-02-28 17:06:43 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-02-28 17:07:19 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 0 loss:0.06481000781059265 norm:0.0021940828301012516 max memory_allocated 22565.42919921875 
[2025-02-28 17:07:53 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 1 loss:0.060498639941215515 norm:0.0007886713719926775 max memory_allocated 22565.42919921875 
[2025-02-28 17:08:26 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 2 loss:0.05814601480960846 norm:0.00030837810481898487 max memory_allocated 22565.42919921875 
[2025-02-28 17:08:59 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 3 loss:0.05686447024345398 norm:0.0002080436097458005 max memory_allocated 22565.42919921875 
[2025-02-28 17:09:33 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 4 loss:0.05600760132074356 norm:0.00018149059906136245 max memory_allocated 22565.42919921875 
[2025-02-28 17:10:06 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 5 loss:0.055530838668346405 norm:0.00015774885832797736 max memory_allocated 22565.42919921875 
[2025-02-28 17:10:39 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 6 loss:0.055317919701337814 norm:0.0001517851633252576 max memory_allocated 22565.42919921875 
[2025-02-28 17:11:13 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 7 loss:0.05522081255912781 norm:0.0001524508697912097 max memory_allocated 22565.42919921875 
[2025-02-28 17:11:46 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 8 loss:0.05515527352690697 norm:0.00014197200653143227 max memory_allocated 22565.42919921875 
[2025-02-28 17:12:19 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 9 loss:0.05510065704584122 norm:0.00014236947754397988 max memory_allocated 22565.42919921875 
[2025-02-28 17:12:53 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 10 loss:0.0550592876970768 norm:0.0001346046046819538 max memory_allocated 22565.42919921875 
[2025-02-28 17:13:26 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 11 loss:0.05501363053917885 norm:0.00012018316192552447 max memory_allocated 22565.42919921875 
[2025-02-28 17:13:59 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 12 loss:0.05497553199529648 norm:0.00010791113891173154 max memory_allocated 22565.42919921875 
[2025-02-28 17:14:33 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 13 loss:0.054923173040151596 norm:0.00010626274161040783 max memory_allocated 22565.42919921875 
[2025-02-28 17:15:06 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 14 loss:0.05491376295685768 norm:0.00011312623973935843 max memory_allocated 22565.42919921875 
[2025-02-28 17:15:40 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 15 loss:0.054883841425180435 norm:0.00010602240945445374 max memory_allocated 22565.42919921875 
[2025-02-28 17:16:13 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 16 loss:0.05486219376325607 norm:0.00010719189594965428 max memory_allocated 22565.42919921875 
[2025-02-28 17:16:46 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 17 loss:0.054847486317157745 norm:9.907981439027935e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:17:20 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 18 loss:0.05483429506421089 norm:9.81134144240059e-05 max memory_allocated 22565.42919921875 
[2025-02-28 17:17:53 root] (abq_llm_calib_config2.py 396): INFO layer 20 iter 19 loss:0.05482394993305206 norm:0.00010034210572484881 max memory_allocated 22565.42919921875 
[2025-02-28 17:18:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-02-28 17:18:39 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 0 loss:0.06796959787607193 norm:0.0016602074028924108 max memory_allocated 22565.60107421875 
[2025-02-28 17:19:12 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 1 loss:0.06499063223600388 norm:0.0005078473477624357 max memory_allocated 22565.60107421875 
[2025-02-28 17:19:45 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 2 loss:0.06352964788675308 norm:0.0002457250957377255 max memory_allocated 22565.60107421875 
[2025-02-28 17:20:19 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 3 loss:0.06247003749012947 norm:0.0001891613646876067 max memory_allocated 22565.60107421875 
[2025-02-28 17:20:52 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 4 loss:0.06159277260303497 norm:0.00016717877588234842 max memory_allocated 22565.60107421875 
[2025-02-28 17:21:25 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 5 loss:0.06115956977009773 norm:0.00015787976735737175 max memory_allocated 22565.60107421875 
[2025-02-28 17:21:59 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 6 loss:0.06097474321722984 norm:0.0001420531189069152 max memory_allocated 22565.60107421875 
[2025-02-28 17:22:32 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 7 loss:0.06091151759028435 norm:0.00012371796765364707 max memory_allocated 22565.60107421875 
[2025-02-28 17:23:05 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 8 loss:0.06086333096027374 norm:0.00010376767750130966 max memory_allocated 22565.60107421875 
[2025-02-28 17:23:39 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 9 loss:0.060824960470199585 norm:0.000108195366919972 max memory_allocated 22565.60107421875 
[2025-02-28 17:24:12 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 10 loss:0.060775164514780045 norm:0.0001091658414225094 max memory_allocated 22565.60107421875 
[2025-02-28 17:24:45 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 11 loss:0.060763515532016754 norm:0.00010599014058243483 max memory_allocated 22565.60107421875 
[2025-02-28 17:25:19 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 12 loss:0.06074012443423271 norm:9.150967525783926e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:25:52 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 13 loss:0.06070695072412491 norm:9.474756370764226e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:26:26 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 14 loss:0.060687605291604996 norm:8.986260218080133e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:26:59 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 15 loss:0.060674626380205154 norm:8.50471478770487e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:27:32 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 16 loss:0.060671187937259674 norm:8.529140905011445e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:28:06 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 17 loss:0.06065567955374718 norm:8.464416896458715e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:28:39 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 18 loss:0.060646381229162216 norm:8.13635706435889e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:29:12 root] (abq_llm_calib_config2.py 396): INFO layer 21 iter 19 loss:0.06065365672111511 norm:8.197204442694783e-05 max memory_allocated 22565.60107421875 
[2025-02-28 17:29:22 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-02-28 17:29:59 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 0 loss:0.0785374566912651 norm:0.0013800128363072872 max memory_allocated 22565.77294921875 
[2025-02-28 17:30:33 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 1 loss:0.0745772123336792 norm:0.0004383976920507848 max memory_allocated 22565.77294921875 
[2025-02-28 17:31:06 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 2 loss:0.07249323278665543 norm:0.00024042109725996852 max memory_allocated 22565.77294921875 
[2025-02-28 17:31:39 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 3 loss:0.07103113830089569 norm:0.00020412492449395359 max memory_allocated 22565.77294921875 
[2025-02-28 17:32:13 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 4 loss:0.07008177787065506 norm:0.00020211050286889076 max memory_allocated 22565.77294921875 
[2025-02-28 17:32:46 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 5 loss:0.06963527202606201 norm:0.00017085183935705572 max memory_allocated 22565.77294921875 
[2025-02-28 17:33:19 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 6 loss:0.06949887424707413 norm:0.00017239057342521846 max memory_allocated 22565.77294921875 
[2025-02-28 17:33:53 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 7 loss:0.06938803941011429 norm:0.00015304838598240167 max memory_allocated 22565.77294921875 
[2025-02-28 17:34:26 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 8 loss:0.06931846588850021 norm:0.00015399012772832066 max memory_allocated 22565.77294921875 
[2025-02-28 17:34:59 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 9 loss:0.06925655156373978 norm:0.00013773614773526788 max memory_allocated 22565.77294921875 
[2025-02-28 17:35:33 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 10 loss:0.0692000761628151 norm:0.0001279842108488083 max memory_allocated 22565.77294921875 
[2025-02-28 17:36:06 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 11 loss:0.06915363669395447 norm:0.00012345126015134156 max memory_allocated 22565.77294921875 
[2025-02-28 17:36:40 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 12 loss:0.06917642056941986 norm:0.00012185859668534249 max memory_allocated 22565.77294921875 
[2025-02-28 17:37:13 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 13 loss:0.06912782043218613 norm:0.00012288716970942914 max memory_allocated 22565.77294921875 
[2025-02-28 17:37:46 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 14 loss:0.06911379098892212 norm:0.00013035765732638538 max memory_allocated 22565.77294921875 
[2025-02-28 17:38:20 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 15 loss:0.06906086206436157 norm:0.0001172230186057277 max memory_allocated 22565.77294921875 
[2025-02-28 17:38:53 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 16 loss:0.06903331726789474 norm:0.00011594666284509003 max memory_allocated 22565.77294921875 
[2025-02-28 17:39:26 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 17 loss:0.069053053855896 norm:0.0001242879225173965 max memory_allocated 22565.77294921875 
[2025-02-28 17:40:00 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 18 loss:0.06902811676263809 norm:0.00012293810141272843 max memory_allocated 22565.77294921875 
[2025-02-28 17:40:33 root] (abq_llm_calib_config2.py 396): INFO layer 22 iter 19 loss:0.06902383267879486 norm:0.0001206488668685779 max memory_allocated 22565.77294921875 
[2025-02-28 17:40:43 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-02-28 17:41:19 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 0 loss:0.08738589286804199 norm:0.0016065381933003664 max memory_allocated 22565.94482421875 
[2025-02-28 17:41:52 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 1 loss:0.08406377583742142 norm:0.0006368000176735222 max memory_allocated 22565.94482421875 
[2025-02-28 17:42:26 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 2 loss:0.08156377077102661 norm:0.00033906142925843596 max memory_allocated 22565.94482421875 
[2025-02-28 17:42:59 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 3 loss:0.0801452100276947 norm:0.00025981225189752877 max memory_allocated 22565.94482421875 
[2025-02-28 17:43:32 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 4 loss:0.0791313424706459 norm:0.000218112429138273 max memory_allocated 22565.94482421875 
[2025-02-28 17:44:06 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 5 loss:0.07876791059970856 norm:0.00018450924835633487 max memory_allocated 22565.94482421875 
[2025-02-28 17:44:39 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 6 loss:0.07863124459981918 norm:0.00016171361494343728 max memory_allocated 22565.94482421875 
[2025-02-28 17:45:12 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 7 loss:0.07855406403541565 norm:0.00014360919885803014 max memory_allocated 22565.94482421875 
[2025-02-28 17:45:46 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 8 loss:0.07849230617284775 norm:0.00013873339048586786 max memory_allocated 22565.94482421875 
[2025-02-28 17:46:19 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 9 loss:0.07844771444797516 norm:0.00012474045797716826 max memory_allocated 22565.94482421875 
[2025-02-28 17:46:53 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 10 loss:0.07840023934841156 norm:0.00010887942335102707 max memory_allocated 22565.94482421875 
[2025-02-28 17:47:26 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 11 loss:0.07835012674331665 norm:0.00010883028153330088 max memory_allocated 22565.94482421875 
[2025-02-28 17:47:59 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 12 loss:0.0783175379037857 norm:0.0001126351417042315 max memory_allocated 22565.94482421875 
[2025-02-28 17:48:33 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 13 loss:0.07832066714763641 norm:0.00010562218812992796 max memory_allocated 22565.94482421875 
[2025-02-28 17:49:06 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 14 loss:0.07828471064567566 norm:9.870161738945171e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:49:39 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 15 loss:0.07827231287956238 norm:9.554142161505297e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:50:13 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 16 loss:0.0782497450709343 norm:9.964677883544937e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:50:46 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 17 loss:0.0782395601272583 norm:9.899784345179796e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:51:20 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 18 loss:0.07821706682443619 norm:9.89841646514833e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:51:53 root] (abq_llm_calib_config2.py 396): INFO layer 23 iter 19 loss:0.07819756120443344 norm:9.251736628357321e-05 max memory_allocated 22565.94482421875 
[2025-02-28 17:52:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-02-28 17:52:38 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 0 loss:0.09867671877145767 norm:0.002053216565400362 max memory_allocated 22566.11669921875 
[2025-02-28 17:53:12 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 1 loss:0.09419818222522736 norm:0.0005104067968204618 max memory_allocated 22566.11669921875 
[2025-02-28 17:53:45 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 2 loss:0.0922703891992569 norm:0.00036117277340963483 max memory_allocated 22566.11669921875 
[2025-02-28 17:54:19 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 3 loss:0.09059188514947891 norm:0.00023964748834259808 max memory_allocated 22566.11669921875 
[2025-02-28 17:54:52 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 4 loss:0.08952170610427856 norm:0.0002214021369582042 max memory_allocated 22566.11669921875 
[2025-02-28 17:55:25 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 5 loss:0.08912404626607895 norm:0.00020038479124195874 max memory_allocated 22566.11669921875 
[2025-02-28 17:55:59 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 6 loss:0.08899638801813126 norm:0.00019187138241250068 max memory_allocated 22566.11669921875 
[2025-02-28 17:56:32 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 7 loss:0.08894100785255432 norm:0.00017635792028158903 max memory_allocated 22566.11669921875 
[2025-02-28 17:57:05 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 8 loss:0.08883538842201233 norm:0.0001543439138913527 max memory_allocated 22566.11669921875 
[2025-02-28 17:57:39 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 9 loss:0.08876034617424011 norm:0.00014697924780193716 max memory_allocated 22566.11669921875 
[2025-02-28 17:58:12 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 10 loss:0.08868486434221268 norm:0.00014616758562624454 max memory_allocated 22566.11669921875 
[2025-02-28 17:58:45 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 11 loss:0.08864695578813553 norm:0.0001507408742327243 max memory_allocated 22566.11669921875 
[2025-02-28 17:59:19 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 12 loss:0.08864293247461319 norm:0.00014849026047158986 max memory_allocated 22566.11669921875 
[2025-02-28 17:59:52 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 13 loss:0.08861718326807022 norm:0.00014214194379746914 max memory_allocated 22566.11669921875 
[2025-02-28 18:00:26 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 14 loss:0.08860470354557037 norm:0.0001453272270737216 max memory_allocated 22566.11669921875 
[2025-02-28 18:00:59 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 15 loss:0.08857814967632294 norm:0.0001436149177607149 max memory_allocated 22566.11669921875 
[2025-02-28 18:01:32 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 16 loss:0.08854715526103973 norm:0.0001300202711718157 max memory_allocated 22566.11669921875 
[2025-02-28 18:02:06 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 17 loss:0.08852951228618622 norm:0.00012699046055786312 max memory_allocated 22566.11669921875 
[2025-02-28 18:02:39 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 18 loss:0.08848992735147476 norm:0.0001297315611736849 max memory_allocated 22566.11669921875 
[2025-02-28 18:03:12 root] (abq_llm_calib_config2.py 396): INFO layer 24 iter 19 loss:0.08848738670349121 norm:0.00011380471551092342 max memory_allocated 22566.11669921875 
[2025-02-28 18:03:22 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-02-28 18:03:58 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 0 loss:0.11179377883672714 norm:0.0009098410955630243 max memory_allocated 22566.28857421875 
[2025-02-28 18:04:31 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 1 loss:0.10827179998159409 norm:0.0003944232303183526 max memory_allocated 22566.28857421875 
[2025-02-28 18:05:05 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 2 loss:0.10561944544315338 norm:0.00029757441370747983 max memory_allocated 22566.28857421875 
[2025-02-28 18:05:38 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 3 loss:0.10382341593503952 norm:0.00024781253887340426 max memory_allocated 22566.28857421875 
[2025-02-28 18:06:11 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 4 loss:0.10275355726480484 norm:0.0002126450272044167 max memory_allocated 22566.28857421875 
[2025-02-28 18:06:45 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 5 loss:0.10240937024354935 norm:0.00019921096100006253 max memory_allocated 22566.28857421875 
[2025-02-28 18:07:18 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 6 loss:0.10223668813705444 norm:0.00017577051767148077 max memory_allocated 22566.28857421875 
[2025-02-28 18:07:51 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 7 loss:0.10214565694332123 norm:0.00015836900274734944 max memory_allocated 22566.28857421875 
[2025-02-28 18:08:25 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 8 loss:0.10204924643039703 norm:0.00015154671564232558 max memory_allocated 22566.28857421875 
[2025-02-28 18:08:58 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 9 loss:0.101946622133255 norm:0.00014894941705279052 max memory_allocated 22566.28857421875 
[2025-02-28 18:09:31 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 10 loss:0.10193953663110733 norm:0.00013906433014199138 max memory_allocated 22566.28857421875 
[2025-02-28 18:10:05 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 11 loss:0.10191196203231812 norm:0.00013765349285677075 max memory_allocated 22566.28857421875 
[2025-02-28 18:10:38 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 12 loss:0.10184623301029205 norm:0.00013769617362413555 max memory_allocated 22566.28857421875 
[2025-02-28 18:11:11 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 13 loss:0.10180294513702393 norm:0.0001335626729996875 max memory_allocated 22566.28857421875 
[2025-02-28 18:11:45 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 14 loss:0.10175997018814087 norm:0.0001320109877269715 max memory_allocated 22566.28857421875 
[2025-02-28 18:12:18 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 15 loss:0.1017419695854187 norm:0.00012154920841567218 max memory_allocated 22566.28857421875 
[2025-02-28 18:12:52 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 16 loss:0.10172943025827408 norm:0.00012950203381478786 max memory_allocated 22566.28857421875 
[2025-02-28 18:13:25 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 17 loss:0.10169246047735214 norm:0.00012559244350995868 max memory_allocated 22566.28857421875 
[2025-02-28 18:13:58 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 18 loss:0.10166048258543015 norm:0.00012021855218335986 max memory_allocated 22566.28857421875 
[2025-02-28 18:14:32 root] (abq_llm_calib_config2.py 396): INFO layer 25 iter 19 loss:0.1016496792435646 norm:0.00012720913218799978 max memory_allocated 22566.28857421875 
[2025-02-28 18:14:41 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-02-28 18:15:17 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 0 loss:0.12790973484516144 norm:0.0019291879143565893 max memory_allocated 22566.46044921875 
[2025-02-28 18:15:50 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 1 loss:0.12350086867809296 norm:0.0008185907499864697 max memory_allocated 22566.46044921875 
[2025-02-28 18:16:24 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 2 loss:0.1201678141951561 norm:0.0003606756217777729 max memory_allocated 22566.46044921875 
[2025-02-28 18:16:57 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 3 loss:0.11810637265443802 norm:0.0003006423939950764 max memory_allocated 22566.46044921875 
[2025-02-28 18:17:30 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 4 loss:0.11703575402498245 norm:0.0002544897433836013 max memory_allocated 22566.46044921875 
[2025-02-28 18:18:04 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 5 loss:0.11664611846208572 norm:0.00021471348009072244 max memory_allocated 22566.46044921875 
[2025-02-28 18:18:37 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 6 loss:0.11646558344364166 norm:0.00020689624943770468 max memory_allocated 22566.46044921875 
[2025-02-28 18:19:10 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 7 loss:0.11631467193365097 norm:0.00019457847520243376 max memory_allocated 22566.46044921875 
[2025-02-28 18:19:44 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 8 loss:0.11622363328933716 norm:0.0001814686693251133 max memory_allocated 22566.46044921875 
[2025-02-28 18:20:17 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 9 loss:0.11611519008874893 norm:0.0001692175428615883 max memory_allocated 22566.46044921875 
[2025-02-28 18:20:50 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 10 loss:0.11602486670017242 norm:0.000160018855240196 max memory_allocated 22566.46044921875 
[2025-02-28 18:21:24 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 11 loss:0.11597821116447449 norm:0.0001572035253047943 max memory_allocated 22566.46044921875 
[2025-02-28 18:21:57 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 12 loss:0.11591985821723938 norm:0.00014820753131061792 max memory_allocated 22566.46044921875 
[2025-02-28 18:22:31 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 13 loss:0.11589079350233078 norm:0.00014161727449391037 max memory_allocated 22566.46044921875 
[2025-02-28 18:23:04 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 14 loss:0.11586558818817139 norm:0.0001381470647174865 max memory_allocated 22566.46044921875 
[2025-02-28 18:23:37 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 15 loss:0.11585316061973572 norm:0.00014095494407229125 max memory_allocated 22566.46044921875 
[2025-02-28 18:24:10 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 16 loss:0.11580540984869003 norm:0.0001403848291374743 max memory_allocated 22566.46044921875 
[2025-02-28 18:24:44 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 17 loss:0.1157849133014679 norm:0.0001372000842820853 max memory_allocated 22566.46044921875 
[2025-02-28 18:25:17 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 18 loss:0.11578954011201859 norm:0.00013101367221679538 max memory_allocated 22566.46044921875 
[2025-02-28 18:25:51 root] (abq_llm_calib_config2.py 396): INFO layer 26 iter 19 loss:0.11578375101089478 norm:0.00013163132825866342 max memory_allocated 22566.46044921875 
[2025-02-28 18:26:00 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-02-28 18:26:36 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 0 loss:0.14352980256080627 norm:0.0020753394346684217 max memory_allocated 22566.63232421875 
[2025-02-28 18:27:09 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 1 loss:0.13919314742088318 norm:0.0007713431259617209 max memory_allocated 22566.63232421875 
[2025-02-28 18:27:43 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 2 loss:0.1361854374408722 norm:0.00035044702235609293 max memory_allocated 22566.63232421875 
[2025-02-28 18:28:16 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 3 loss:0.13413789868354797 norm:0.00025906742666848004 max memory_allocated 22566.63232421875 
[2025-02-28 18:28:50 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 4 loss:0.13330169022083282 norm:0.00023640708241146058 max memory_allocated 22566.63232421875 
[2025-02-28 18:29:23 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 5 loss:0.13303348422050476 norm:0.00020319322356954217 max memory_allocated 22566.63232421875 
[2025-02-28 18:29:56 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 6 loss:0.1328781545162201 norm:0.0001794988347683102 max memory_allocated 22566.63232421875 
[2025-02-28 18:30:30 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 7 loss:0.13276618719100952 norm:0.0001734149846015498 max memory_allocated 22566.63232421875 
[2025-02-28 18:31:03 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 8 loss:0.13270165026187897 norm:0.00016278533439617604 max memory_allocated 22566.63232421875 
[2025-02-28 18:31:36 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 9 loss:0.132650688290596 norm:0.00015367110609076917 max memory_allocated 22566.63232421875 
[2025-02-28 18:32:10 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 10 loss:0.13263320922851562 norm:0.0001515740732429549 max memory_allocated 22566.63232421875 
[2025-02-28 18:32:43 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 11 loss:0.13257047533988953 norm:0.00014345397357828915 max memory_allocated 22566.63232421875 
[2025-02-28 18:33:16 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 12 loss:0.13252800703048706 norm:0.00013866194058209658 max memory_allocated 22566.63232421875 
[2025-02-28 18:33:50 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 13 loss:0.13248825073242188 norm:0.00013753613166045398 max memory_allocated 22566.63232421875 
[2025-02-28 18:34:23 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 14 loss:0.1324530839920044 norm:0.000135601861984469 max memory_allocated 22566.63232421875 
[2025-02-28 18:34:56 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 15 loss:0.13244079053401947 norm:0.00013480131747201085 max memory_allocated 22566.63232421875 
[2025-02-28 18:35:30 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 16 loss:0.13241857290267944 norm:0.00013519756612367928 max memory_allocated 22566.63232421875 
[2025-02-28 18:36:03 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 17 loss:0.1324073225259781 norm:0.00013415006105788052 max memory_allocated 22566.63232421875 
[2025-02-28 18:36:37 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 18 loss:0.1324092000722885 norm:0.0001277160772588104 max memory_allocated 22566.63232421875 
[2025-02-28 18:37:10 root] (abq_llm_calib_config2.py 396): INFO layer 27 iter 19 loss:0.13240790367126465 norm:0.00012987523223273456 max memory_allocated 22566.63232421875 
[2025-02-28 18:37:19 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-02-28 18:37:22 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:37:56 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 0 loss:0.16555480659008026 norm:0.010469886474311352 max memory_allocated 22566.91943359375 
[2025-02-28 18:38:29 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 1 loss:0.16110070049762726 norm:0.007729692384600639 max memory_allocated 22566.91943359375 
[2025-02-28 18:39:02 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 2 loss:0.1577894687652588 norm:0.0053226398304104805 max memory_allocated 22566.91943359375 
[2025-02-28 18:39:36 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 3 loss:0.15526483952999115 norm:0.004191868472844362 max memory_allocated 22566.91943359375 
[2025-02-28 18:40:09 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 4 loss:0.15428826212882996 norm:0.0035734164994210005 max memory_allocated 22566.91943359375 
[2025-02-28 18:40:43 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 5 loss:0.15390795469284058 norm:0.003035289468243718 max memory_allocated 22566.91943359375 
[2025-02-28 18:41:16 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 6 loss:0.1536838263273239 norm:0.002581681590527296 max memory_allocated 22566.91943359375 
[2025-02-28 18:41:50 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 7 loss:0.15350808203220367 norm:0.0021912110969424248 max memory_allocated 22566.91943359375 
[2025-02-28 18:42:23 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 8 loss:0.15336428582668304 norm:0.0019766564946621656 max memory_allocated 22566.91943359375 
[2025-02-28 18:42:57 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 9 loss:0.15332041680812836 norm:0.0020012857858091593 max memory_allocated 22566.91943359375 
[2025-02-28 18:43:30 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 10 loss:0.15334264934062958 norm:0.0020605958998203278 max memory_allocated 22566.91943359375 
[2025-02-28 18:44:04 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 11 loss:0.15337246656417847 norm:0.0021022246219217777 max memory_allocated 22566.91943359375 
[2025-02-28 18:44:37 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 12 loss:0.15330421924591064 norm:0.0018978649750351906 max memory_allocated 22566.91943359375 
[2025-02-28 18:45:11 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 13 loss:0.1532803624868393 norm:0.0019344077445566654 max memory_allocated 22566.91943359375 
[2025-02-28 18:45:44 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 14 loss:0.15326577425003052 norm:0.0018330138409510255 max memory_allocated 22566.91943359375 
[2025-02-28 18:46:17 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 15 loss:0.15324734151363373 norm:0.0018113815458491445 max memory_allocated 22566.91943359375 
[2025-02-28 18:46:51 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 16 loss:0.15320812165737152 norm:0.001744752051308751 max memory_allocated 22566.91943359375 
[2025-02-28 18:47:24 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 17 loss:0.15320561826229095 norm:0.0017137605464085937 max memory_allocated 22566.91943359375 
[2025-02-28 18:47:58 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 18 loss:0.15318535268306732 norm:0.0016991512384265661 max memory_allocated 22566.91943359375 
[2025-02-28 18:48:31 root] (abq_llm_calib_config2.py 396): INFO layer 28 iter 19 loss:0.1531940996646881 norm:0.0016317821573466063 max memory_allocated 22566.91943359375 
[2025-02-28 18:48:41 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-02-28 18:48:44 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 18:49:17 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 0 loss:0.21413417160511017 norm:0.0097438283264637 max memory_allocated 22567.09130859375 
[2025-02-28 18:49:50 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 1 loss:0.19957983493804932 norm:0.007262021768838167 max memory_allocated 22567.09130859375 
[2025-02-28 18:50:24 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 2 loss:0.18873010575771332 norm:0.005056947469711304 max memory_allocated 22567.09130859375 
[2025-02-28 18:50:57 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 3 loss:0.18529768288135529 norm:0.0040154969319701195 max memory_allocated 22567.09130859375 
[2025-02-28 18:51:31 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 4 loss:0.1838686764240265 norm:0.003358695423230529 max memory_allocated 22567.09130859375 
[2025-02-28 18:52:04 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 5 loss:0.1832621991634369 norm:0.002882347907871008 max memory_allocated 22567.09130859375 
[2025-02-28 18:52:38 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 6 loss:0.1828458458185196 norm:0.0025441765319556 max memory_allocated 22567.09130859375 
[2025-02-28 18:53:11 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 7 loss:0.18258978426456451 norm:0.002318460261449218 max memory_allocated 22567.09130859375 
[2025-02-28 18:53:45 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 8 loss:0.1823970377445221 norm:0.002157534472644329 max memory_allocated 22567.09130859375 
[2025-02-28 18:54:18 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 9 loss:0.18227435648441315 norm:0.0020942219998687506 max memory_allocated 22567.09130859375 
[2025-02-28 18:54:51 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 10 loss:0.18214738368988037 norm:0.0019410905661061406 max memory_allocated 22567.09130859375 
[2025-02-28 18:55:25 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 11 loss:0.1820158213376999 norm:0.0018188146641477942 max memory_allocated 22567.09130859375 
[2025-02-28 18:55:58 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 12 loss:0.18195973336696625 norm:0.0017617489211261272 max memory_allocated 22567.09130859375 
[2025-02-28 18:56:32 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 13 loss:0.18177618086338043 norm:0.0018037963891401887 max memory_allocated 22567.09130859375 
[2025-02-28 18:57:05 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 14 loss:0.18168899416923523 norm:0.0017220483860000968 max memory_allocated 22567.09130859375 
[2025-02-28 18:57:39 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 15 loss:0.1816457211971283 norm:0.0017603944288566709 max memory_allocated 22567.09130859375 
[2025-02-28 18:58:12 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 16 loss:0.1816421002149582 norm:0.0017541761044412851 max memory_allocated 22567.09130859375 
[2025-02-28 18:58:46 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 17 loss:0.18166354298591614 norm:0.0018141077598556876 max memory_allocated 22567.09130859375 
[2025-02-28 18:59:19 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 18 loss:0.18172748386859894 norm:0.0017589288763701916 max memory_allocated 22567.09130859375 
[2025-02-28 18:59:53 root] (abq_llm_calib_config2.py 396): INFO layer 29 iter 19 loss:0.18168790638446808 norm:0.001794589450582862 max memory_allocated 22567.09130859375 
[2025-02-28 19:00:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-02-28 19:00:05 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 19:00:38 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 0 loss:1.066450834274292 norm:0.1522475779056549 max memory_allocated 22567.26318359375 
[2025-02-28 19:01:12 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 1 loss:0.7518602013587952 norm:0.1364389955997467 max memory_allocated 22567.26318359375 
[2025-02-28 19:01:45 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 2 loss:0.44566333293914795 norm:0.059254735708236694 max memory_allocated 22567.26318359375 
[2025-02-28 19:02:19 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 3 loss:0.35494109988212585 norm:0.03670812398195267 max memory_allocated 22567.26318359375 
[2025-02-28 19:02:52 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 4 loss:0.3206885755062103 norm:0.03969239816069603 max memory_allocated 22567.26318359375 
[2025-02-28 19:03:26 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 5 loss:0.2943737804889679 norm:0.040129221975803375 max memory_allocated 22567.26318359375 
[2025-02-28 19:03:59 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 6 loss:0.282412588596344 norm:0.03662245348095894 max memory_allocated 22567.26318359375 
[2025-02-28 19:04:33 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 7 loss:0.2718501389026642 norm:0.03399091958999634 max memory_allocated 22567.26318359375 
[2025-02-28 19:05:06 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 8 loss:0.2662610411643982 norm:0.032526180148124695 max memory_allocated 22567.26318359375 
[2025-02-28 19:05:39 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 9 loss:0.26100221276283264 norm:0.028204554691910744 max memory_allocated 22567.26318359375 
[2025-02-28 19:06:13 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 10 loss:0.25744864344596863 norm:0.02746848575770855 max memory_allocated 22567.26318359375 
[2025-02-28 19:06:47 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 11 loss:0.25344565510749817 norm:0.024292122572660446 max memory_allocated 22567.26318359375 
[2025-02-28 19:07:20 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 12 loss:0.2509657144546509 norm:0.022427719086408615 max memory_allocated 22567.26318359375 
[2025-02-28 19:07:53 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 13 loss:0.24902567267417908 norm:0.021972086280584335 max memory_allocated 22567.26318359375 
[2025-02-28 19:08:27 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 14 loss:0.24771833419799805 norm:0.020975274965167046 max memory_allocated 22567.26318359375 
[2025-02-28 19:09:00 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 15 loss:0.24712884426116943 norm:0.02165020816028118 max memory_allocated 22567.26318359375 
[2025-02-28 19:09:34 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 16 loss:0.2460727095603943 norm:0.01970110833644867 max memory_allocated 22567.26318359375 
[2025-02-28 19:10:07 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 17 loss:0.24557559192180634 norm:0.019525757059454918 max memory_allocated 22567.26318359375 
[2025-02-28 19:10:41 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 18 loss:0.24544094502925873 norm:0.019796349108219147 max memory_allocated 22567.26318359375 
[2025-02-28 19:11:14 root] (abq_llm_calib_config2.py 396): INFO layer 30 iter 19 loss:0.24598589539527893 norm:0.020349932834506035 max memory_allocated 22567.26318359375 
[2025-02-28 19:11:24 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-02-28 19:11:27 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-02-28 19:12:00 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 0 loss:0.482639342546463 norm:0.030432697385549545 max memory_allocated 22567.43505859375 
[2025-02-28 19:12:33 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 1 loss:0.4530969560146332 norm:0.022017234936356544 max memory_allocated 22567.43505859375 
[2025-02-28 19:13:07 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 2 loss:0.43717658519744873 norm:0.017876077443361282 max memory_allocated 22567.43505859375 
[2025-02-28 19:13:40 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 3 loss:0.4286044239997864 norm:0.016119565814733505 max memory_allocated 22567.43505859375 
[2025-02-28 19:14:14 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 4 loss:0.4237687289714813 norm:0.015226070769131184 max memory_allocated 22567.43505859375 
[2025-02-28 19:14:47 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 5 loss:0.4199921488761902 norm:0.013608927838504314 max memory_allocated 22567.43505859375 
[2025-02-28 19:15:21 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 6 loss:0.41790372133255005 norm:0.014443974010646343 max memory_allocated 22567.43505859375 
[2025-02-28 19:15:54 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 7 loss:0.415792316198349 norm:0.013515783473849297 max memory_allocated 22567.43505859375 
[2025-02-28 19:16:28 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 8 loss:0.41538915038108826 norm:0.01426888257265091 max memory_allocated 22567.43505859375 
[2025-02-28 19:17:01 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 9 loss:0.41433852910995483 norm:0.013428748585283756 max memory_allocated 22567.43505859375 
[2025-02-28 19:17:35 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 10 loss:0.4137294292449951 norm:0.013743577525019646 max memory_allocated 22567.43505859375 
[2025-02-28 19:18:08 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 11 loss:0.4136834144592285 norm:0.014886029064655304 max memory_allocated 22567.43505859375 
[2025-02-28 19:18:42 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 12 loss:0.4127260446548462 norm:0.013071614317595959 max memory_allocated 22567.43505859375 
[2025-02-28 19:19:15 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 13 loss:0.41227424144744873 norm:0.014024212956428528 max memory_allocated 22567.43505859375 
[2025-02-28 19:19:49 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 14 loss:0.41220563650131226 norm:0.013132710009813309 max memory_allocated 22567.43505859375 
[2025-02-28 19:20:22 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 15 loss:0.41227269172668457 norm:0.014616777189075947 max memory_allocated 22567.43505859375 
[2025-02-28 19:20:55 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 16 loss:0.41129961609840393 norm:0.012860102578997612 max memory_allocated 22567.43505859375 
[2025-02-28 19:21:29 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 17 loss:0.41061997413635254 norm:0.013484729453921318 max memory_allocated 22567.43505859375 
[2025-02-28 19:22:02 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 18 loss:0.4101325571537018 norm:0.011800257489085197 max memory_allocated 22567.43505859375 
[2025-02-28 19:22:36 root] (abq_llm_calib_config2.py 396): INFO layer 31 iter 19 loss:0.410264253616333 norm:0.01275189034640789 max memory_allocated 22567.43505859375 
[2025-02-28 19:22:45 root] (main_calib_config2.py 380): INFO 21738.560950517654
[2025-02-28 19:22:50 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_wikitext2_all.cache
[2025-02-28 19:24:01 root] (main_calib_config2.py 159): INFO wikitext2 : 5.705236911773682
[2025-02-28 19:24:01 root] (main_calib_config2.py 115): INFO load calibration from ./cache/testloader_Llama_c4_all.cache
[2025-02-28 19:25:50 root] (main_calib_config2.py 159): INFO c4 : 7.2913947105407715
[2025-02-28 21:09:07 root] (main_calib_config2.py 170): INFO {'wikitext2': 5.705236911773682, 'c4': 7.2913947105407715, 'results': {'hellaswag': {'acc': 0.5529774945230034, 'acc_stderr': 0.004961693567208827, 'acc_norm': 0.7114120693089027, 'acc_norm_stderr': 0.00452179857792214}, 'arc_challenge': {'acc': 0.3916382252559727, 'acc_stderr': 0.014264122124938218, 'acc_norm': 0.39419795221843, 'acc_norm_stderr': 0.014280522667467325}, 'boolq': {'acc': 0.7045871559633028, 'acc_stderr': 0.007979479946630336}, 'arc_easy': {'acc': 0.6738215488215489, 'acc_stderr': 0.009619849417035172, 'acc_norm': 0.5218855218855218, 'acc_norm_stderr': 0.010249950427234167}, 'piqa': {'acc': 0.779651795429815, 'acc_stderr': 0.009670535456853133, 'acc_norm': 0.7698585418933623, 'acc_norm_stderr': 0.0098208328268398}, 'winogrande': {'acc': 0.6692975532754538, 'acc_stderr': 0.013222435887002698}}, 'versions': {'hellaswag': 0, 'arc_challenge': 0, 'boolq': 1, 'arc_easy': 0, 'piqa': 0, 'winogrande': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
