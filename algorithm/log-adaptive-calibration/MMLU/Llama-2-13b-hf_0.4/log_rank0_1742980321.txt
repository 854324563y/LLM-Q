[2025-03-26 09:12:01 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-13b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/MMLU/Llama-2-13b-hf_0.4', save_dir=None, resume='./log-adaptive-calibration/Llama-2-13b-hf_0.4/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.4.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-26 09:12:03 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-03-26 09:12:03 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-26 09:12:03 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-03-26 09:12:03 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/Llama-2-13b-hf/quant_map_Llama-2-13b-hf_0.4.pkl
[2025-03-26 09:12:06 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-03-26 09:12:07 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:12:09 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-03-26 09:12:09 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:12:10 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-03-26 09:12:10 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:12:10 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-03-26 09:12:11 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-03-26 09:12:11 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-03-26 09:12:12 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-03-26 09:12:12 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-03-26 09:12:13 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-03-26 09:12:13 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-03-26 09:12:14 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-03-26 09:12:14 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-03-26 09:12:15 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-03-26 09:12:16 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-03-26 09:12:16 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-03-26 09:12:17 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-03-26 09:12:18 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-03-26 09:12:18 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-03-26 09:12:19 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-03-26 09:12:19 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-03-26 09:12:20 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-03-26 09:12:20 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-03-26 09:12:21 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-03-26 09:12:22 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-03-26 09:12:22 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-03-26 09:12:23 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-03-26 09:12:23 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-03-26 09:12:24 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-03-26 09:12:24 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-03-26 09:12:25 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-03-26 09:12:25 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-03-26 09:12:26 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-03-26 09:12:27 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 32 ===
[2025-03-26 09:12:28 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 33 ===
[2025-03-26 09:12:28 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 34 ===
[2025-03-26 09:12:29 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 35 ===
[2025-03-26 09:12:29 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 36 ===
[2025-03-26 09:12:30 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:12:30 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 37 ===
[2025-03-26 09:12:30 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:12:31 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 38 ===
[2025-03-26 09:12:31 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:12:31 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 39 ===
[2025-03-26 09:12:31 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:12:32 root] (main_calib_config2.py 380): INFO 29.46928858757019
[2025-03-26 10:57:49 root] (main_calib_config2.py 172): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.371900826446281, 'acc_stderr': 0.04412015806624504, 'acc_norm': 0.5537190082644629, 'acc_norm_stderr': 0.0453793517794788}, 'hendrycksTest-high_school_biology': {'acc': 0.33225806451612905, 'acc_stderr': 0.026795560848122804, 'acc_norm': 0.3096774193548387, 'acc_norm_stderr': 0.026302774983517418}, 'hendrycksTest-college_physics': {'acc': 0.2647058823529412, 'acc_stderr': 0.04389869956808777, 'acc_norm': 0.2549019607843137, 'acc_norm_stderr': 0.04336432707993177}, 'hendrycksTest-college_biology': {'acc': 0.22916666666666666, 'acc_stderr': 0.035146974678623884, 'acc_norm': 0.2847222222222222, 'acc_norm_stderr': 0.03773809990686934}, 'hendrycksTest-electrical_engineering': {'acc': 0.3448275862068966, 'acc_stderr': 0.03960933549451208, 'acc_norm': 0.3103448275862069, 'acc_norm_stderr': 0.03855289616378948}, 'hendrycksTest-high_school_physics': {'acc': 0.32450331125827814, 'acc_stderr': 0.03822746937658753, 'acc_norm': 0.304635761589404, 'acc_norm_stderr': 0.03757949922943342}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2777777777777778, 'acc_stderr': 0.02306818884826111, 'acc_norm': 0.28835978835978837, 'acc_norm_stderr': 0.0233306540545359}, 'hendrycksTest-formal_logic': {'acc': 0.29365079365079366, 'acc_stderr': 0.04073524322147127, 'acc_norm': 0.29365079365079366, 'acc_norm_stderr': 0.04073524322147126}, 'hendrycksTest-human_aging': {'acc': 0.30493273542600896, 'acc_stderr': 0.030898610882477515, 'acc_norm': 0.2242152466367713, 'acc_norm_stderr': 0.027991534258519524}, 'hendrycksTest-jurisprudence': {'acc': 0.35185185185185186, 'acc_stderr': 0.04616631111801714, 'acc_norm': 0.4351851851851852, 'acc_norm_stderr': 0.04792898170907062}, 'hendrycksTest-moral_scenarios': {'acc': 0.2782122905027933, 'acc_stderr': 0.014987325439963575, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-astronomy': {'acc': 0.3881578947368421, 'acc_stderr': 0.03965842097512744, 'acc_norm': 0.40789473684210525, 'acc_norm_stderr': 0.03999309712777471}, 'hendrycksTest-conceptual_physics': {'acc': 0.2680851063829787, 'acc_stderr': 0.02895734278834235, 'acc_norm': 0.19148936170212766, 'acc_norm_stderr': 0.025722149992637805}, 'hendrycksTest-college_chemistry': {'acc': 0.19, 'acc_stderr': 0.039427724440366234, 'acc_norm': 0.23, 'acc_norm_stderr': 0.04229525846816506}, 'hendrycksTest-human_sexuality': {'acc': 0.4122137404580153, 'acc_stderr': 0.043171711948702556, 'acc_norm': 0.3282442748091603, 'acc_norm_stderr': 0.04118438565806298}, 'hendrycksTest-sociology': {'acc': 0.30845771144278605, 'acc_stderr': 0.03265819588512699, 'acc_norm': 0.2885572139303483, 'acc_norm_stderr': 0.032038410402133226}, 'hendrycksTest-prehistory': {'acc': 0.3148148148148148, 'acc_stderr': 0.025842248700902175, 'acc_norm': 0.28703703703703703, 'acc_norm_stderr': 0.02517104191530968}, 'hendrycksTest-professional_law': {'acc': 0.2790091264667536, 'acc_stderr': 0.011455208832803545, 'acc_norm': 0.30638852672750977, 'acc_norm_stderr': 0.011773980329380714}, 'hendrycksTest-virology': {'acc': 0.3493975903614458, 'acc_stderr': 0.03711725190740751, 'acc_norm': 0.3132530120481928, 'acc_norm_stderr': 0.036108050180310235}, 'hendrycksTest-machine_learning': {'acc': 0.25892857142857145, 'acc_stderr': 0.041577515398656284, 'acc_norm': 0.24107142857142858, 'acc_norm_stderr': 0.04059867246952687}, 'hendrycksTest-marketing': {'acc': 0.4829059829059829, 'acc_stderr': 0.032736940493481824, 'acc_norm': 0.405982905982906, 'acc_norm_stderr': 0.03217180182641087}, 'hendrycksTest-us_foreign_policy': {'acc': 0.47, 'acc_stderr': 0.05016135580465919, 'acc_norm': 0.42, 'acc_norm_stderr': 0.049604496374885836}, 'hendrycksTest-high_school_european_history': {'acc': 0.4, 'acc_stderr': 0.03825460278380026, 'acc_norm': 0.3696969696969697, 'acc_norm_stderr': 0.03769430314512568}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.36923076923076925, 'acc_stderr': 0.024468615241478923, 'acc_norm': 0.31025641025641026, 'acc_norm_stderr': 0.02345467488940429}, 'hendrycksTest-logical_fallacies': {'acc': 0.26380368098159507, 'acc_stderr': 0.034624199316156234, 'acc_norm': 0.32515337423312884, 'acc_norm_stderr': 0.03680350371286461}, 'hendrycksTest-abstract_algebra': {'acc': 0.27, 'acc_stderr': 0.0446196043338474, 'acc_norm': 0.21, 'acc_norm_stderr': 0.04093601807403326}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.3626943005181347, 'acc_stderr': 0.034697137917043715, 'acc_norm': 0.37305699481865284, 'acc_norm_stderr': 0.03490205592048573}, 'hendrycksTest-moral_disputes': {'acc': 0.30057803468208094, 'acc_stderr': 0.024685316867257796, 'acc_norm': 0.3179190751445087, 'acc_norm_stderr': 0.025070713719153176}, 'hendrycksTest-high_school_world_history': {'acc': 0.3881856540084388, 'acc_stderr': 0.031722950043323296, 'acc_norm': 0.3459915611814346, 'acc_norm_stderr': 0.03096481058878671}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.3487394957983193, 'acc_stderr': 0.030956636328566545, 'acc_norm': 0.41596638655462187, 'acc_norm_stderr': 0.03201650100739614}, 'hendrycksTest-college_medicine': {'acc': 0.2658959537572254, 'acc_stderr': 0.0336876293225943, 'acc_norm': 0.2774566473988439, 'acc_norm_stderr': 0.034140140070440354}, 'hendrycksTest-security_studies': {'acc': 0.44081632653061226, 'acc_stderr': 0.03178419114175363, 'acc_norm': 0.35918367346938773, 'acc_norm_stderr': 0.03071356045510849}, 'hendrycksTest-high_school_geography': {'acc': 0.36363636363636365, 'acc_stderr': 0.03427308652999935, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.033586181457325226}, 'hendrycksTest-world_religions': {'acc': 0.4853801169590643, 'acc_stderr': 0.038331852752130205, 'acc_norm': 0.4152046783625731, 'acc_norm_stderr': 0.03779275945503201}, 'hendrycksTest-business_ethics': {'acc': 0.39, 'acc_stderr': 0.04902071300001975, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117317}, 'hendrycksTest-professional_accounting': {'acc': 0.2624113475177305, 'acc_stderr': 0.026244920349843007, 'acc_norm': 0.2978723404255319, 'acc_norm_stderr': 0.02728160834446941}, 'hendrycksTest-global_facts': {'acc': 0.28, 'acc_stderr': 0.045126085985421276, 'acc_norm': 0.2, 'acc_norm_stderr': 0.040201512610368445}, 'hendrycksTest-high_school_us_history': {'acc': 0.3333333333333333, 'acc_stderr': 0.033086111132364336, 'acc_norm': 0.3137254901960784, 'acc_norm_stderr': 0.03256685484460389}, 'hendrycksTest-professional_psychology': {'acc': 0.3284313725490196, 'acc_stderr': 0.01899970738316267, 'acc_norm': 0.3022875816993464, 'acc_norm_stderr': 0.018579232711113874}, 'hendrycksTest-college_computer_science': {'acc': 0.3, 'acc_stderr': 0.046056618647183814, 'acc_norm': 0.27, 'acc_norm_stderr': 0.044619604333847394}, 'hendrycksTest-philosophy': {'acc': 0.31511254019292606, 'acc_stderr': 0.026385273703464496, 'acc_norm': 0.3408360128617363, 'acc_norm_stderr': 0.02692084126077616}, 'hendrycksTest-anatomy': {'acc': 0.37777777777777777, 'acc_stderr': 0.04188307537595852, 'acc_norm': 0.26666666666666666, 'acc_norm_stderr': 0.038201699145179055}, 'hendrycksTest-nutrition': {'acc': 0.3790849673202614, 'acc_stderr': 0.027780141207023327, 'acc_norm': 0.4084967320261438, 'acc_norm_stderr': 0.028146405993096358}, 'hendrycksTest-computer_security': {'acc': 0.34, 'acc_stderr': 0.04760952285695236, 'acc_norm': 0.34, 'acc_norm_stderr': 0.04760952285695235}, 'hendrycksTest-econometrics': {'acc': 0.23684210526315788, 'acc_stderr': 0.03999423879281335, 'acc_norm': 0.20175438596491227, 'acc_norm_stderr': 0.037752050135836386}, 'hendrycksTest-clinical_knowledge': {'acc': 0.26037735849056604, 'acc_stderr': 0.027008766090708076, 'acc_norm': 0.3584905660377358, 'acc_norm_stderr': 0.029514703583981755}, 'hendrycksTest-high_school_mathematics': {'acc': 0.2222222222222222, 'acc_stderr': 0.02534809746809784, 'acc_norm': 0.2814814814814815, 'acc_norm_stderr': 0.027420019350945277}, 'hendrycksTest-medical_genetics': {'acc': 0.27, 'acc_stderr': 0.044619604333847394, 'acc_norm': 0.4, 'acc_norm_stderr': 0.049236596391733084}, 'hendrycksTest-public_relations': {'acc': 0.35454545454545455, 'acc_stderr': 0.04582004841505415, 'acc_norm': 0.22727272727272727, 'acc_norm_stderr': 0.04013964554072773}, 'hendrycksTest-miscellaneous': {'acc': 0.384418901660281, 'acc_stderr': 0.01739568874281962, 'acc_norm': 0.3065134099616858, 'acc_norm_stderr': 0.01648695289304151}, 'hendrycksTest-management': {'acc': 0.2815533980582524, 'acc_stderr': 0.044532548363264694, 'acc_norm': 0.3300970873786408, 'acc_norm_stderr': 0.046561471100123514}, 'hendrycksTest-professional_medicine': {'acc': 0.3602941176470588, 'acc_stderr': 0.02916312857067073, 'acc_norm': 0.3014705882352941, 'acc_norm_stderr': 0.027875982114273168}, 'hendrycksTest-high_school_computer_science': {'acc': 0.33, 'acc_stderr': 0.047258156262526045, 'acc_norm': 0.35, 'acc_norm_stderr': 0.04793724854411022}, 'hendrycksTest-high_school_psychology': {'acc': 0.3651376146788991, 'acc_stderr': 0.020642801454384005, 'acc_norm': 0.28623853211009176, 'acc_norm_stderr': 0.019379436628919965}, 'hendrycksTest-high_school_chemistry': {'acc': 0.2019704433497537, 'acc_stderr': 0.02824735012218027, 'acc_norm': 0.31527093596059114, 'acc_norm_stderr': 0.03269080871970186}, 'hendrycksTest-college_mathematics': {'acc': 0.27, 'acc_stderr': 0.0446196043338474, 'acc_norm': 0.29, 'acc_norm_stderr': 0.04560480215720683}, 'hendrycksTest-high_school_statistics': {'acc': 0.3055555555555556, 'acc_stderr': 0.03141554629402544, 'acc_norm': 0.32407407407407407, 'acc_norm_stderr': 0.03191923445686185}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-26 10:57:49 root] (main_calib_config2.py 202): INFO Average accuracy 0.2843 - STEM
[2025-03-26 10:57:49 root] (main_calib_config2.py 202): INFO Average accuracy 0.3366 - humanities
[2025-03-26 10:57:49 root] (main_calib_config2.py 202): INFO Average accuracy 0.3634 - social sciences
[2025-03-26 10:57:49 root] (main_calib_config2.py 202): INFO Average accuracy 0.3321 - other (business, health, misc.)
[2025-03-26 10:57:49 root] (main_calib_config2.py 204): INFO Average accuracy: 0.3246
