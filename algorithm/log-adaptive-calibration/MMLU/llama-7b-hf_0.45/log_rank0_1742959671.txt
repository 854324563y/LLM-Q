[2025-03-26 03:27:51 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-7b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/MMLU/llama-7b-hf_0.45', save_dir=None, resume='./log-adaptive-calibration/llama-7b-hf_0.45/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.45.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-26 03:30:52 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-03-26 03:30:52 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-26 03:30:53 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-03-26 03:30:53 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/llama-7b-hf/quant_map_llama-7b-hf_0.45.pkl
[2025-03-26 03:30:56 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-03-26 03:30:56 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 03:30:58 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-03-26 03:30:58 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 03:30:59 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-03-26 03:30:59 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 03:30:59 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-03-26 03:31:00 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-03-26 03:31:00 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-03-26 03:31:00 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-03-26 03:31:01 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-03-26 03:31:01 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-03-26 03:31:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-03-26 03:31:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-03-26 03:31:03 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-03-26 03:31:03 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-03-26 03:31:04 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-03-26 03:31:04 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-03-26 03:31:05 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-03-26 03:31:05 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-03-26 03:31:05 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-03-26 03:31:06 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-03-26 03:31:07 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-03-26 03:31:08 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-03-26 03:31:08 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-03-26 03:31:08 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-03-26 03:31:09 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-03-26 03:31:09 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-03-26 03:31:10 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-03-26 03:31:10 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-03-26 03:31:11 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-03-26 03:31:11 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-03-26 03:31:11 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 03:31:11 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-03-26 03:31:12 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 03:31:12 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-03-26 03:31:12 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 03:31:12 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-03-26 03:31:13 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 03:31:13 root] (main_calib_config2.py 380): INFO 21.229656457901
[2025-03-26 05:10:30 root] (main_calib_config2.py 172): INFO {'results': {'hendrycksTest-marketing': {'acc': 0.4358974358974359, 'acc_stderr': 0.032485775115783995, 'acc_norm': 0.43162393162393164, 'acc_norm_stderr': 0.032448355353114904}, 'hendrycksTest-high_school_geography': {'acc': 0.2727272727272727, 'acc_stderr': 0.03173071239071724, 'acc_norm': 0.29797979797979796, 'acc_norm_stderr': 0.03258630383836556}, 'hendrycksTest-econometrics': {'acc': 0.2543859649122807, 'acc_stderr': 0.040969851398436716, 'acc_norm': 0.20175438596491227, 'acc_norm_stderr': 0.03775205013583638}, 'hendrycksTest-college_medicine': {'acc': 0.3063583815028902, 'acc_stderr': 0.035149425512674394, 'acc_norm': 0.3236994219653179, 'acc_norm_stderr': 0.0356760379963917}, 'hendrycksTest-international_law': {'acc': 0.256198347107438, 'acc_stderr': 0.03984979653302872, 'acc_norm': 0.5206611570247934, 'acc_norm_stderr': 0.04560456086387235}, 'hendrycksTest-high_school_biology': {'acc': 0.2709677419354839, 'acc_stderr': 0.025284416114900152, 'acc_norm': 0.3161290322580645, 'acc_norm_stderr': 0.026450874489042767}, 'hendrycksTest-college_biology': {'acc': 0.22916666666666666, 'acc_stderr': 0.03514697467862388, 'acc_norm': 0.24305555555555555, 'acc_norm_stderr': 0.0358687928008034}, 'hendrycksTest-us_foreign_policy': {'acc': 0.4, 'acc_stderr': 0.04923659639173309, 'acc_norm': 0.36, 'acc_norm_stderr': 0.04824181513244218}, 'hendrycksTest-college_computer_science': {'acc': 0.34, 'acc_stderr': 0.04760952285695235, 'acc_norm': 0.29, 'acc_norm_stderr': 0.045604802157206845}, 'hendrycksTest-miscellaneous': {'acc': 0.36270753512132825, 'acc_stderr': 0.0171927086746023, 'acc_norm': 0.3065134099616858, 'acc_norm_stderr': 0.01648695289304151}, 'hendrycksTest-high_school_us_history': {'acc': 0.3284313725490196, 'acc_stderr': 0.03296245110172229, 'acc_norm': 0.29901960784313725, 'acc_norm_stderr': 0.03213325717373616}, 'hendrycksTest-conceptual_physics': {'acc': 0.28085106382978725, 'acc_stderr': 0.029379170464124815, 'acc_norm': 0.2170212765957447, 'acc_norm_stderr': 0.02694748312149623}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.29411764705882354, 'acc_stderr': 0.029597329730978075, 'acc_norm': 0.3445378151260504, 'acc_norm_stderr': 0.03086868260412163}, 'hendrycksTest-nutrition': {'acc': 0.3627450980392157, 'acc_stderr': 0.027530078447110307, 'acc_norm': 0.43137254901960786, 'acc_norm_stderr': 0.028358956313423552}, 'hendrycksTest-high_school_european_history': {'acc': 0.3333333333333333, 'acc_stderr': 0.03681050869161549, 'acc_norm': 0.3393939393939394, 'acc_norm_stderr': 0.036974422050315967}, 'hendrycksTest-management': {'acc': 0.3300970873786408, 'acc_stderr': 0.04656147110012351, 'acc_norm': 0.32038834951456313, 'acc_norm_stderr': 0.0462028408228004}, 'hendrycksTest-college_mathematics': {'acc': 0.18, 'acc_stderr': 0.038612291966536955, 'acc_norm': 0.27, 'acc_norm_stderr': 0.044619604333847394}, 'hendrycksTest-business_ethics': {'acc': 0.38, 'acc_stderr': 0.04878317312145633, 'acc_norm': 0.35, 'acc_norm_stderr': 0.0479372485441102}, 'hendrycksTest-high_school_physics': {'acc': 0.23178807947019867, 'acc_stderr': 0.034454062719870546, 'acc_norm': 0.2251655629139073, 'acc_norm_stderr': 0.034104352820089376}, 'hendrycksTest-moral_scenarios': {'acc': 0.2435754189944134, 'acc_stderr': 0.014355911964767867, 'acc_norm': 0.27150837988826815, 'acc_norm_stderr': 0.014874252168095273}, 'hendrycksTest-professional_psychology': {'acc': 0.28431372549019607, 'acc_stderr': 0.01824902441120766, 'acc_norm': 0.2761437908496732, 'acc_norm_stderr': 0.018087276935663137}, 'hendrycksTest-college_physics': {'acc': 0.29411764705882354, 'acc_stderr': 0.04533838195929775, 'acc_norm': 0.3235294117647059, 'acc_norm_stderr': 0.046550104113196177}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.30512820512820515, 'acc_stderr': 0.023346335293325884, 'acc_norm': 0.3128205128205128, 'acc_norm_stderr': 0.023507579020645337}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2724867724867725, 'acc_stderr': 0.022930973071633345, 'acc_norm': 0.25396825396825395, 'acc_norm_stderr': 0.02241804289111393}, 'hendrycksTest-high_school_psychology': {'acc': 0.3321100917431193, 'acc_stderr': 0.020192682985423344, 'acc_norm': 0.26788990825688075, 'acc_norm_stderr': 0.01898746225797865}, 'hendrycksTest-college_chemistry': {'acc': 0.2, 'acc_stderr': 0.04020151261036845, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-public_relations': {'acc': 0.3090909090909091, 'acc_stderr': 0.044262946482000985, 'acc_norm': 0.15454545454545454, 'acc_norm_stderr': 0.03462262571262667}, 'hendrycksTest-virology': {'acc': 0.35542168674698793, 'acc_stderr': 0.03726214354322415, 'acc_norm': 0.2891566265060241, 'acc_norm_stderr': 0.035294868015111155}, 'hendrycksTest-jurisprudence': {'acc': 0.3055555555555556, 'acc_stderr': 0.04453197507374983, 'acc_norm': 0.42592592592592593, 'acc_norm_stderr': 0.0478034362693679}, 'hendrycksTest-electrical_engineering': {'acc': 0.35172413793103446, 'acc_stderr': 0.03979236637497411, 'acc_norm': 0.3448275862068966, 'acc_norm_stderr': 0.039609335494512087}, 'hendrycksTest-astronomy': {'acc': 0.3881578947368421, 'acc_stderr': 0.03965842097512744, 'acc_norm': 0.40789473684210525, 'acc_norm_stderr': 0.03999309712777472}, 'hendrycksTest-high_school_statistics': {'acc': 0.3194444444444444, 'acc_stderr': 0.031798763421768524, 'acc_norm': 0.35185185185185186, 'acc_norm_stderr': 0.032568505702936464}, 'hendrycksTest-high_school_world_history': {'acc': 0.270042194092827, 'acc_stderr': 0.028900721906293426, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.030685820596610798}, 'hendrycksTest-computer_security': {'acc': 0.33, 'acc_stderr': 0.04725815626252604, 'acc_norm': 0.39, 'acc_norm_stderr': 0.04902071300001975}, 'hendrycksTest-prehistory': {'acc': 0.29012345679012347, 'acc_stderr': 0.025251173936495012, 'acc_norm': 0.25308641975308643, 'acc_norm_stderr': 0.024191808600713002}, 'hendrycksTest-clinical_knowledge': {'acc': 0.30943396226415093, 'acc_stderr': 0.028450154794118627, 'acc_norm': 0.3433962264150943, 'acc_norm_stderr': 0.02922452646912479}, 'hendrycksTest-security_studies': {'acc': 0.43673469387755104, 'acc_stderr': 0.03175195237583323, 'acc_norm': 0.3183673469387755, 'acc_norm_stderr': 0.029822533793982066}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.2538860103626943, 'acc_stderr': 0.03141024780565318, 'acc_norm': 0.2849740932642487, 'acc_norm_stderr': 0.03257714077709661}, 'hendrycksTest-human_aging': {'acc': 0.30493273542600896, 'acc_stderr': 0.030898610882477515, 'acc_norm': 0.2242152466367713, 'acc_norm_stderr': 0.027991534258519527}, 'hendrycksTest-human_sexuality': {'acc': 0.40458015267175573, 'acc_stderr': 0.043046937953806645, 'acc_norm': 0.29770992366412213, 'acc_norm_stderr': 0.040103589424622034}, 'hendrycksTest-logical_fallacies': {'acc': 0.25766871165644173, 'acc_stderr': 0.03436150827846917, 'acc_norm': 0.3128834355828221, 'acc_norm_stderr': 0.03642914578292405}, 'hendrycksTest-machine_learning': {'acc': 0.24107142857142858, 'acc_stderr': 0.040598672469526864, 'acc_norm': 0.23214285714285715, 'acc_norm_stderr': 0.04007341809755805}, 'hendrycksTest-anatomy': {'acc': 0.25925925925925924, 'acc_stderr': 0.037857144650666544, 'acc_norm': 0.23703703703703705, 'acc_norm_stderr': 0.03673731683969506}, 'hendrycksTest-world_religions': {'acc': 0.4327485380116959, 'acc_stderr': 0.03799978644370607, 'acc_norm': 0.4327485380116959, 'acc_norm_stderr': 0.03799978644370608}, 'hendrycksTest-formal_logic': {'acc': 0.3253968253968254, 'acc_stderr': 0.041905964388711366, 'acc_norm': 0.3253968253968254, 'acc_norm_stderr': 0.04190596438871136}, 'hendrycksTest-abstract_algebra': {'acc': 0.22, 'acc_stderr': 0.0416333199893227, 'acc_norm': 0.21, 'acc_norm_stderr': 0.040936018074033256}, 'hendrycksTest-philosophy': {'acc': 0.28938906752411575, 'acc_stderr': 0.025755865922632928, 'acc_norm': 0.3086816720257235, 'acc_norm_stderr': 0.02623696588115326}, 'hendrycksTest-professional_law': {'acc': 0.26727509778357234, 'acc_stderr': 0.011302607515637528, 'acc_norm': 0.29139504563233376, 'acc_norm_stderr': 0.011605720214257601}, 'hendrycksTest-high_school_chemistry': {'acc': 0.2413793103448276, 'acc_stderr': 0.03010833071801162, 'acc_norm': 0.27586206896551724, 'acc_norm_stderr': 0.03144712581678242}, 'hendrycksTest-professional_medicine': {'acc': 0.2647058823529412, 'acc_stderr': 0.026799562024887674, 'acc_norm': 0.28308823529411764, 'acc_norm_stderr': 0.02736586113151381}, 'hendrycksTest-moral_disputes': {'acc': 0.26878612716763006, 'acc_stderr': 0.023868003262500118, 'acc_norm': 0.3179190751445087, 'acc_norm_stderr': 0.025070713719153183}, 'hendrycksTest-sociology': {'acc': 0.29850746268656714, 'acc_stderr': 0.03235743789355042, 'acc_norm': 0.29850746268656714, 'acc_norm_stderr': 0.03235743789355042}, 'hendrycksTest-high_school_mathematics': {'acc': 0.24074074074074073, 'acc_stderr': 0.026067159222275805, 'acc_norm': 0.3111111111111111, 'acc_norm_stderr': 0.028226446749683515}, 'hendrycksTest-medical_genetics': {'acc': 0.21, 'acc_stderr': 0.040936018074033256, 'acc_norm': 0.36, 'acc_norm_stderr': 0.048241815132442176}, 'hendrycksTest-professional_accounting': {'acc': 0.19148936170212766, 'acc_stderr': 0.023472645247949415, 'acc_norm': 0.23404255319148937, 'acc_norm_stderr': 0.025257861359432407}, 'hendrycksTest-global_facts': {'acc': 0.21, 'acc_stderr': 0.040936018074033256, 'acc_norm': 0.2, 'acc_norm_stderr': 0.04020151261036845}, 'hendrycksTest-high_school_computer_science': {'acc': 0.29, 'acc_stderr': 0.04560480215720683, 'acc_norm': 0.27, 'acc_norm_stderr': 0.044619604333847415}}, 'versions': {'hendrycksTest-marketing': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-management': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-high_school_statistics': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_computer_science': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-26 05:10:30 root] (main_calib_config2.py 202): INFO Average accuracy 0.2734 - STEM
[2025-03-26 05:10:30 root] (main_calib_config2.py 202): INFO Average accuracy 0.2976 - humanities
[2025-03-26 05:10:30 root] (main_calib_config2.py 202): INFO Average accuracy 0.3205 - social sciences
[2025-03-26 05:10:30 root] (main_calib_config2.py 202): INFO Average accuracy 0.3059 - other (business, health, misc.)
[2025-03-26 05:10:30 root] (main_calib_config2.py 204): INFO Average accuracy: 0.2968
