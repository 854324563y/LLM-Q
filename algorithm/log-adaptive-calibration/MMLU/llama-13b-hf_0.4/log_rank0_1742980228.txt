[2025-03-26 09:10:28 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/llama-13b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/MMLU/llama-13b-hf_0.4', save_dir=None, resume='./log-adaptive-calibration/llama-13b-hf_0.4/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.4.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-26 09:10:44 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-03-26 09:10:44 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_llama_wikitext2_128.cache
[2025-03-26 09:10:44 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-03-26 09:10:44 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/llama-13b-hf/quant_map_llama-13b-hf_0.4.pkl
[2025-03-26 09:10:47 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-03-26 09:10:47 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:10:49 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-03-26 09:10:49 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:10:49 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-03-26 09:10:50 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:10:50 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-03-26 09:10:51 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-03-26 09:10:51 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-03-26 09:10:52 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-03-26 09:10:53 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-03-26 09:10:53 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-03-26 09:10:54 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-03-26 09:10:54 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-03-26 09:10:55 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-03-26 09:10:55 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-03-26 09:10:56 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-03-26 09:10:56 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-03-26 09:10:57 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-03-26 09:10:57 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-03-26 09:10:58 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-03-26 09:10:58 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-03-26 09:10:59 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-03-26 09:10:59 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-03-26 09:11:00 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-03-26 09:11:01 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-03-26 09:11:01 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-03-26 09:11:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-03-26 09:11:02 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-03-26 09:11:03 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-03-26 09:11:03 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-03-26 09:11:04 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-03-26 09:11:04 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-03-26 09:11:05 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-03-26 09:11:05 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-03-26 09:11:06 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 32 ===
[2025-03-26 09:11:07 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 33 ===
[2025-03-26 09:11:08 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 34 ===
[2025-03-26 09:11:09 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 35 ===
[2025-03-26 09:11:09 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 36 ===
[2025-03-26 09:11:09 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:11:10 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 37 ===
[2025-03-26 09:11:10 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:11:10 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 38 ===
[2025-03-26 09:11:10 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:11:11 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 39 ===
[2025-03-26 09:11:11 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-26 09:11:11 root] (main_calib_config2.py 380): INFO 27.26180863380432
[2025-03-26 10:53:24 root] (main_calib_config2.py 172): INFO {'results': {'hendrycksTest-international_law': {'acc': 0.34710743801652894, 'acc_stderr': 0.04345724570292534, 'acc_norm': 0.5619834710743802, 'acc_norm_stderr': 0.045291468044357915}, 'hendrycksTest-high_school_biology': {'acc': 0.2838709677419355, 'acc_stderr': 0.02564938106302927, 'acc_norm': 0.2838709677419355, 'acc_norm_stderr': 0.025649381063029265}, 'hendrycksTest-college_physics': {'acc': 0.21568627450980393, 'acc_stderr': 0.04092563958237655, 'acc_norm': 0.2549019607843137, 'acc_norm_stderr': 0.04336432707993177}, 'hendrycksTest-college_biology': {'acc': 0.2916666666666667, 'acc_stderr': 0.03800968060554858, 'acc_norm': 0.2916666666666667, 'acc_norm_stderr': 0.03800968060554858}, 'hendrycksTest-electrical_engineering': {'acc': 0.23448275862068965, 'acc_stderr': 0.035306258743465914, 'acc_norm': 0.2827586206896552, 'acc_norm_stderr': 0.037528339580033376}, 'hendrycksTest-high_school_physics': {'acc': 0.271523178807947, 'acc_stderr': 0.03631329803969653, 'acc_norm': 0.2781456953642384, 'acc_norm_stderr': 0.03658603262763743}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2619047619047619, 'acc_stderr': 0.02264421261552521, 'acc_norm': 0.29365079365079366, 'acc_norm_stderr': 0.02345603738398203}, 'hendrycksTest-formal_logic': {'acc': 0.3253968253968254, 'acc_stderr': 0.04190596438871137, 'acc_norm': 0.31746031746031744, 'acc_norm_stderr': 0.04163453031302859}, 'hendrycksTest-human_aging': {'acc': 0.24663677130044842, 'acc_stderr': 0.028930413120910877, 'acc_norm': 0.21076233183856502, 'acc_norm_stderr': 0.027373095500540193}, 'hendrycksTest-jurisprudence': {'acc': 0.3055555555555556, 'acc_stderr': 0.044531975073749834, 'acc_norm': 0.3888888888888889, 'acc_norm_stderr': 0.0471282125742677}, 'hendrycksTest-moral_scenarios': {'acc': 0.2335195530726257, 'acc_stderr': 0.014149575348976273, 'acc_norm': 0.2748603351955307, 'acc_norm_stderr': 0.014931316703220517}, 'hendrycksTest-astronomy': {'acc': 0.3684210526315789, 'acc_stderr': 0.03925523381052932, 'acc_norm': 0.39473684210526316, 'acc_norm_stderr': 0.039777499346220734}, 'hendrycksTest-conceptual_physics': {'acc': 0.2978723404255319, 'acc_stderr': 0.029896145682095462, 'acc_norm': 0.225531914893617, 'acc_norm_stderr': 0.02732107841738754}, 'hendrycksTest-college_chemistry': {'acc': 0.26, 'acc_stderr': 0.0440844002276808, 'acc_norm': 0.36, 'acc_norm_stderr': 0.04824181513244218}, 'hendrycksTest-human_sexuality': {'acc': 0.4351145038167939, 'acc_stderr': 0.043482080516448585, 'acc_norm': 0.32061068702290074, 'acc_norm_stderr': 0.04093329229834277}, 'hendrycksTest-sociology': {'acc': 0.2835820895522388, 'acc_stderr': 0.03187187537919798, 'acc_norm': 0.24378109452736318, 'acc_norm_stderr': 0.030360490154014673}, 'hendrycksTest-prehistory': {'acc': 0.3055555555555556, 'acc_stderr': 0.025630824975621344, 'acc_norm': 0.25925925925925924, 'acc_norm_stderr': 0.02438366553103545}, 'hendrycksTest-professional_law': {'acc': 0.28226857887874834, 'acc_stderr': 0.011495852176241935, 'acc_norm': 0.2953063885267275, 'acc_norm_stderr': 0.011651061936208816}, 'hendrycksTest-virology': {'acc': 0.3614457831325301, 'acc_stderr': 0.03740059382029321, 'acc_norm': 0.29518072289156627, 'acc_norm_stderr': 0.03550920185689631}, 'hendrycksTest-machine_learning': {'acc': 0.25, 'acc_stderr': 0.04109974682633932, 'acc_norm': 0.25892857142857145, 'acc_norm_stderr': 0.04157751539865629}, 'hendrycksTest-marketing': {'acc': 0.3547008547008547, 'acc_stderr': 0.03134250486245402, 'acc_norm': 0.3547008547008547, 'acc_norm_stderr': 0.03134250486245402}, 'hendrycksTest-us_foreign_policy': {'acc': 0.44, 'acc_stderr': 0.04988876515698589, 'acc_norm': 0.41, 'acc_norm_stderr': 0.049431107042371025}, 'hendrycksTest-high_school_european_history': {'acc': 0.2909090909090909, 'acc_stderr': 0.03546563019624337, 'acc_norm': 0.34545454545454546, 'acc_norm_stderr': 0.037131580674819135}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.3076923076923077, 'acc_stderr': 0.023400928918310505, 'acc_norm': 0.30512820512820515, 'acc_norm_stderr': 0.023346335293325887}, 'hendrycksTest-logical_fallacies': {'acc': 0.1901840490797546, 'acc_stderr': 0.030833491146281235, 'acc_norm': 0.3006134969325153, 'acc_norm_stderr': 0.0360251131880677}, 'hendrycksTest-abstract_algebra': {'acc': 0.27, 'acc_stderr': 0.0446196043338474, 'acc_norm': 0.26, 'acc_norm_stderr': 0.0440844002276808}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.32124352331606215, 'acc_stderr': 0.033699508685490674, 'acc_norm': 0.2849740932642487, 'acc_norm_stderr': 0.03257714077709661}, 'hendrycksTest-moral_disputes': {'acc': 0.26011560693641617, 'acc_stderr': 0.02361867831006937, 'acc_norm': 0.30057803468208094, 'acc_norm_stderr': 0.02468531686725781}, 'hendrycksTest-high_school_world_history': {'acc': 0.34177215189873417, 'acc_stderr': 0.030874537537553617, 'acc_norm': 0.3037974683544304, 'acc_norm_stderr': 0.029936696387138605}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.3277310924369748, 'acc_stderr': 0.030489911417673224, 'acc_norm': 0.37815126050420167, 'acc_norm_stderr': 0.031499305777849054}, 'hendrycksTest-college_medicine': {'acc': 0.23699421965317918, 'acc_stderr': 0.03242414757483098, 'acc_norm': 0.24277456647398843, 'acc_norm_stderr': 0.0326926380614177}, 'hendrycksTest-security_studies': {'acc': 0.4204081632653061, 'acc_stderr': 0.03160106993449603, 'acc_norm': 0.3224489795918367, 'acc_norm_stderr': 0.029923100563683906}, 'hendrycksTest-high_school_geography': {'acc': 0.2727272727272727, 'acc_stderr': 0.03173071239071724, 'acc_norm': 0.30808080808080807, 'acc_norm_stderr': 0.032894773300986155}, 'hendrycksTest-world_religions': {'acc': 0.391812865497076, 'acc_stderr': 0.037439798259264016, 'acc_norm': 0.391812865497076, 'acc_norm_stderr': 0.037439798259264016}, 'hendrycksTest-business_ethics': {'acc': 0.37, 'acc_stderr': 0.048523658709391, 'acc_norm': 0.31, 'acc_norm_stderr': 0.04648231987117316}, 'hendrycksTest-professional_accounting': {'acc': 0.23404255319148937, 'acc_stderr': 0.025257861359432407, 'acc_norm': 0.26595744680851063, 'acc_norm_stderr': 0.02635806569888059}, 'hendrycksTest-global_facts': {'acc': 0.17, 'acc_stderr': 0.03775251680686371, 'acc_norm': 0.14, 'acc_norm_stderr': 0.03487350880197771}, 'hendrycksTest-high_school_us_history': {'acc': 0.35294117647058826, 'acc_stderr': 0.03354092437591521, 'acc_norm': 0.28431372549019607, 'acc_norm_stderr': 0.031660096793998116}, 'hendrycksTest-professional_psychology': {'acc': 0.24673202614379086, 'acc_stderr': 0.017440820367402507, 'acc_norm': 0.2630718954248366, 'acc_norm_stderr': 0.017812676542320653}, 'hendrycksTest-college_computer_science': {'acc': 0.26, 'acc_stderr': 0.04408440022768078, 'acc_norm': 0.28, 'acc_norm_stderr': 0.04512608598542127}, 'hendrycksTest-philosophy': {'acc': 0.31189710610932475, 'acc_stderr': 0.026311858071854155, 'acc_norm': 0.3279742765273312, 'acc_norm_stderr': 0.026664410886937606}, 'hendrycksTest-anatomy': {'acc': 0.28888888888888886, 'acc_stderr': 0.0391545063041425, 'acc_norm': 0.23703703703703705, 'acc_norm_stderr': 0.03673731683969506}, 'hendrycksTest-nutrition': {'acc': 0.3300653594771242, 'acc_stderr': 0.026925654653615693, 'acc_norm': 0.3888888888888889, 'acc_norm_stderr': 0.027914055510468015}, 'hendrycksTest-computer_security': {'acc': 0.32, 'acc_stderr': 0.046882617226215034, 'acc_norm': 0.39, 'acc_norm_stderr': 0.04902071300001975}, 'hendrycksTest-econometrics': {'acc': 0.3508771929824561, 'acc_stderr': 0.044895393502706986, 'acc_norm': 0.2543859649122807, 'acc_norm_stderr': 0.040969851398436716}, 'hendrycksTest-clinical_knowledge': {'acc': 0.25660377358490566, 'acc_stderr': 0.02688064788905198, 'acc_norm': 0.3433962264150943, 'acc_norm_stderr': 0.02922452646912479}, 'hendrycksTest-high_school_mathematics': {'acc': 0.1925925925925926, 'acc_stderr': 0.02404307518194519, 'acc_norm': 0.2740740740740741, 'acc_norm_stderr': 0.027195934804085622}, 'hendrycksTest-medical_genetics': {'acc': 0.33, 'acc_stderr': 0.047258156262526045, 'acc_norm': 0.35, 'acc_norm_stderr': 0.0479372485441102}, 'hendrycksTest-public_relations': {'acc': 0.2636363636363636, 'acc_stderr': 0.04220224692971987, 'acc_norm': 0.14545454545454545, 'acc_norm_stderr': 0.03376898319833083}, 'hendrycksTest-miscellaneous': {'acc': 0.3218390804597701, 'acc_stderr': 0.0167063814150579, 'acc_norm': 0.2771392081736909, 'acc_norm_stderr': 0.016005636294122425}, 'hendrycksTest-management': {'acc': 0.3592233009708738, 'acc_stderr': 0.04750458399041693, 'acc_norm': 0.36893203883495146, 'acc_norm_stderr': 0.0477761518115674}, 'hendrycksTest-professional_medicine': {'acc': 0.24632352941176472, 'acc_stderr': 0.02617343857052, 'acc_norm': 0.2610294117647059, 'acc_norm_stderr': 0.026679252270103128}, 'hendrycksTest-high_school_computer_science': {'acc': 0.33, 'acc_stderr': 0.047258156262526045, 'acc_norm': 0.34, 'acc_norm_stderr': 0.047609522856952365}, 'hendrycksTest-high_school_psychology': {'acc': 0.28990825688073396, 'acc_stderr': 0.019453066609201604, 'acc_norm': 0.22201834862385322, 'acc_norm_stderr': 0.017818849564796624}, 'hendrycksTest-high_school_chemistry': {'acc': 0.2561576354679803, 'acc_stderr': 0.030712730070982592, 'acc_norm': 0.30049261083743845, 'acc_norm_stderr': 0.03225799476233485}, 'hendrycksTest-college_mathematics': {'acc': 0.28, 'acc_stderr': 0.04512608598542127, 'acc_norm': 0.34, 'acc_norm_stderr': 0.04760952285695235}, 'hendrycksTest-high_school_statistics': {'acc': 0.2824074074074074, 'acc_stderr': 0.030701372111510934, 'acc_norm': 0.32407407407407407, 'acc_norm_stderr': 0.03191923445686186}}, 'versions': {'hendrycksTest-international_law': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-high_school_physics': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-management': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_statistics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-26 10:53:24 root] (main_calib_config2.py 202): INFO Average accuracy 0.2737 - STEM
[2025-03-26 10:53:24 root] (main_calib_config2.py 202): INFO Average accuracy 0.3030 - humanities
[2025-03-26 10:53:24 root] (main_calib_config2.py 202): INFO Average accuracy 0.3300 - social sciences
[2025-03-26 10:53:24 root] (main_calib_config2.py 202): INFO Average accuracy 0.2933 - other (business, health, misc.)
[2025-03-26 10:53:24 root] (main_calib_config2.py 204): INFO Average accuracy: 0.2971
