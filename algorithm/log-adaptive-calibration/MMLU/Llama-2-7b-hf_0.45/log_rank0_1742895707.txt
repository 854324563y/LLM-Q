[2025-03-25 09:41:47 root] (main_calib_config2.py 284): INFO Namespace(model='/workspace/volume/inference-soft-data/AE/llm/models/Llama-2-7b-hf', cache_dir='./cache', output_dir='./log-adaptive-calibration/MMLU/Llama-2-7b-hf_0.45', save_dir=None, resume='./log-adaptive-calibration/Llama-2-7b-hf_0.45/abq_parameters.pth', real_quant=False, calib_dataset='wikitext2', nsamples=128, batch_size=1, seed=2, tasks='hendrycksTest*', eval_ppl=False, num_fewshot=0, wbits=4, abits=4, group_size=None, alpha=0.5, let_lr=0.005, lwc_lr=0.01, wd=0, epochs=0, let=True, lwc=True, symmetric=False, disable_zero_point=False, a_dynamic_method='per_token', w_dynamic_method='per_channel', limit=-1, multigpu=False, deactive_amp=True, attn_implementation='eager', net=None, act_scales=None, act_shifts=None, quant_map='log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.45.pkl', scale_calibration=True, compensation_calibration=True)
[2025-03-25 09:44:28 root] (main_calib_config2.py 351): INFO === start quantization ===
[2025-03-25 09:44:28 root] (main_calib_config2.py 357): INFO load calibration from ./cache/dataloader_Llama_wikitext2_128.cache
[2025-03-25 09:44:28 root] (abq_llm_calib_config2.py 82): INFO Starting ...
[2025-03-25 09:44:28 root] (abq_llm_calib_config2.py 89): INFO Loaded quant_map from log-adaptive/Llama-2-7b-hf/quant_map_Llama-2-7b-hf_0.45.pkl
[2025-03-25 09:44:30 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 0 ===
[2025-03-25 09:44:30 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-25 09:44:30 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 1 ===
[2025-03-25 09:44:30 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-25 09:44:31 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 2 ===
[2025-03-25 09:44:31 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-25 09:44:31 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 3 ===
[2025-03-25 09:44:31 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 4 ===
[2025-03-25 09:44:32 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 5 ===
[2025-03-25 09:44:32 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 6 ===
[2025-03-25 09:44:32 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 7 ===
[2025-03-25 09:44:33 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 8 ===
[2025-03-25 09:44:33 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 9 ===
[2025-03-25 09:44:33 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 10 ===
[2025-03-25 09:44:34 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 11 ===
[2025-03-25 09:44:34 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 12 ===
[2025-03-25 09:44:34 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 13 ===
[2025-03-25 09:44:35 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 14 ===
[2025-03-25 09:44:35 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 15 ===
[2025-03-25 09:44:35 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 16 ===
[2025-03-25 09:44:35 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 17 ===
[2025-03-25 09:44:36 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 18 ===
[2025-03-25 09:44:36 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 19 ===
[2025-03-25 09:44:36 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 20 ===
[2025-03-25 09:44:36 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 21 ===
[2025-03-25 09:44:37 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 22 ===
[2025-03-25 09:44:37 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 23 ===
[2025-03-25 09:44:37 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 24 ===
[2025-03-25 09:44:38 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 25 ===
[2025-03-25 09:44:38 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 26 ===
[2025-03-25 09:44:38 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 27 ===
[2025-03-25 09:44:38 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 28 ===
[2025-03-25 09:44:38 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-25 09:44:39 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 29 ===
[2025-03-25 09:44:39 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-25 09:44:39 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 30 ===
[2025-03-25 09:44:39 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-25 09:44:39 root] (abq_llm_calib_config2.py 239): INFO === Start quantize layer 31 ===
[2025-03-25 09:44:39 root] (abq_llm_calib_config2.py 314): INFO use compensation vector
[2025-03-25 09:44:40 root] (main_calib_config2.py 380): INFO 11.623802900314331
[2025-03-25 11:23:41 root] (main_calib_config2.py 172): INFO {'results': {'hendrycksTest-high_school_world_history': {'acc': 0.379746835443038, 'acc_stderr': 0.031591887529658504, 'acc_norm': 0.350210970464135, 'acc_norm_stderr': 0.031052391937584353}, 'hendrycksTest-elementary_mathematics': {'acc': 0.2857142857142857, 'acc_stderr': 0.02326651221373057, 'acc_norm': 0.2962962962962963, 'acc_norm_stderr': 0.023517294335963286}, 'hendrycksTest-computer_security': {'acc': 0.32, 'acc_stderr': 0.046882617226215034, 'acc_norm': 0.32, 'acc_norm_stderr': 0.046882617226215034}, 'hendrycksTest-formal_logic': {'acc': 0.31746031746031744, 'acc_stderr': 0.04163453031302859, 'acc_norm': 0.2777777777777778, 'acc_norm_stderr': 0.040061680838488774}, 'hendrycksTest-public_relations': {'acc': 0.2909090909090909, 'acc_stderr': 0.04350271442923243, 'acc_norm': 0.2, 'acc_norm_stderr': 0.038313051408846034}, 'hendrycksTest-clinical_knowledge': {'acc': 0.2981132075471698, 'acc_stderr': 0.028152837942493875, 'acc_norm': 0.36981132075471695, 'acc_norm_stderr': 0.02971142188010793}, 'hendrycksTest-college_computer_science': {'acc': 0.32, 'acc_stderr': 0.04688261722621503, 'acc_norm': 0.29, 'acc_norm_stderr': 0.045604802157206845}, 'hendrycksTest-professional_law': {'acc': 0.26401564537157757, 'acc_stderr': 0.011258435537723804, 'acc_norm': 0.2966101694915254, 'acc_norm_stderr': 0.011665946586082861}, 'hendrycksTest-security_studies': {'acc': 0.47346938775510206, 'acc_stderr': 0.03196412734523272, 'acc_norm': 0.3510204081632653, 'acc_norm_stderr': 0.03055531675557364}, 'hendrycksTest-high_school_computer_science': {'acc': 0.25, 'acc_stderr': 0.04351941398892446, 'acc_norm': 0.38, 'acc_norm_stderr': 0.048783173121456316}, 'hendrycksTest-human_aging': {'acc': 0.3004484304932735, 'acc_stderr': 0.030769352008229143, 'acc_norm': 0.242152466367713, 'acc_norm_stderr': 0.028751392398694755}, 'hendrycksTest-college_biology': {'acc': 0.2847222222222222, 'acc_stderr': 0.03773809990686936, 'acc_norm': 0.2569444444444444, 'acc_norm_stderr': 0.03653946969442099}, 'hendrycksTest-high_school_us_history': {'acc': 0.31862745098039214, 'acc_stderr': 0.032702871814820796, 'acc_norm': 0.29901960784313725, 'acc_norm_stderr': 0.03213325717373617}, 'hendrycksTest-electrical_engineering': {'acc': 0.3724137931034483, 'acc_stderr': 0.0402873153294756, 'acc_norm': 0.3724137931034483, 'acc_norm_stderr': 0.0402873153294756}, 'hendrycksTest-professional_medicine': {'acc': 0.2867647058823529, 'acc_stderr': 0.027472274473233818, 'acc_norm': 0.27205882352941174, 'acc_norm_stderr': 0.027033041151681456}, 'hendrycksTest-virology': {'acc': 0.3433734939759036, 'acc_stderr': 0.036965843170106004, 'acc_norm': 0.28313253012048195, 'acc_norm_stderr': 0.03507295431370518}, 'hendrycksTest-management': {'acc': 0.33980582524271846, 'acc_stderr': 0.046897659372781335, 'acc_norm': 0.2912621359223301, 'acc_norm_stderr': 0.04498676320572921}, 'hendrycksTest-anatomy': {'acc': 0.32592592592592595, 'acc_stderr': 0.040491220417025055, 'acc_norm': 0.25925925925925924, 'acc_norm_stderr': 0.037857144650666544}, 'hendrycksTest-human_sexuality': {'acc': 0.4122137404580153, 'acc_stderr': 0.043171711948702556, 'acc_norm': 0.29770992366412213, 'acc_norm_stderr': 0.040103589424622034}, 'hendrycksTest-medical_genetics': {'acc': 0.3, 'acc_stderr': 0.046056618647183814, 'acc_norm': 0.36, 'acc_norm_stderr': 0.04824181513244218}, 'hendrycksTest-high_school_government_and_politics': {'acc': 0.32642487046632124, 'acc_stderr': 0.033840286211432945, 'acc_norm': 0.31088082901554404, 'acc_norm_stderr': 0.03340361906276588}, 'hendrycksTest-college_physics': {'acc': 0.3431372549019608, 'acc_stderr': 0.04724007352383889, 'acc_norm': 0.37254901960784315, 'acc_norm_stderr': 0.04810840148082634}, 'hendrycksTest-nutrition': {'acc': 0.3431372549019608, 'acc_stderr': 0.027184498909941613, 'acc_norm': 0.39215686274509803, 'acc_norm_stderr': 0.027956046165424513}, 'hendrycksTest-high_school_geography': {'acc': 0.2878787878787879, 'acc_stderr': 0.03225883512300993, 'acc_norm': 0.30303030303030304, 'acc_norm_stderr': 0.03274287914026866}, 'hendrycksTest-jurisprudence': {'acc': 0.2962962962962963, 'acc_stderr': 0.044143436668549335, 'acc_norm': 0.39814814814814814, 'acc_norm_stderr': 0.047323326159788126}, 'hendrycksTest-sociology': {'acc': 0.3482587064676617, 'acc_stderr': 0.03368787466115459, 'acc_norm': 0.2885572139303483, 'acc_norm_stderr': 0.03203841040213323}, 'hendrycksTest-us_foreign_policy': {'acc': 0.42, 'acc_stderr': 0.049604496374885836, 'acc_norm': 0.35, 'acc_norm_stderr': 0.0479372485441102}, 'hendrycksTest-logical_fallacies': {'acc': 0.25153374233128833, 'acc_stderr': 0.034089978868575295, 'acc_norm': 0.3006134969325153, 'acc_norm_stderr': 0.03602511318806771}, 'hendrycksTest-marketing': {'acc': 0.42735042735042733, 'acc_stderr': 0.032408473935163266, 'acc_norm': 0.3717948717948718, 'acc_norm_stderr': 0.03166098891888078}, 'hendrycksTest-astronomy': {'acc': 0.35526315789473684, 'acc_stderr': 0.03894734487013317, 'acc_norm': 0.40131578947368424, 'acc_norm_stderr': 0.03988903703336284}, 'hendrycksTest-college_medicine': {'acc': 0.24277456647398843, 'acc_stderr': 0.0326926380614177, 'acc_norm': 0.28901734104046245, 'acc_norm_stderr': 0.03456425745087}, 'hendrycksTest-high_school_microeconomics': {'acc': 0.2605042016806723, 'acc_stderr': 0.02851025151234194, 'acc_norm': 0.3319327731092437, 'acc_norm_stderr': 0.030588697013783663}, 'hendrycksTest-high_school_mathematics': {'acc': 0.21481481481481482, 'acc_stderr': 0.025040443877000676, 'acc_norm': 0.2777777777777778, 'acc_norm_stderr': 0.027309140588230182}, 'hendrycksTest-philosophy': {'acc': 0.31511254019292606, 'acc_stderr': 0.026385273703464496, 'acc_norm': 0.3311897106109325, 'acc_norm_stderr': 0.02673062072800492}, 'hendrycksTest-high_school_european_history': {'acc': 0.3393939393939394, 'acc_stderr': 0.03697442205031596, 'acc_norm': 0.3333333333333333, 'acc_norm_stderr': 0.0368105086916155}, 'hendrycksTest-prehistory': {'acc': 0.33641975308641975, 'acc_stderr': 0.026289734945952926, 'acc_norm': 0.25308641975308643, 'acc_norm_stderr': 0.024191808600713002}, 'hendrycksTest-econometrics': {'acc': 0.2982456140350877, 'acc_stderr': 0.04303684033537315, 'acc_norm': 0.2719298245614035, 'acc_norm_stderr': 0.04185774424022056}, 'hendrycksTest-college_chemistry': {'acc': 0.25, 'acc_stderr': 0.04351941398892446, 'acc_norm': 0.29, 'acc_norm_stderr': 0.04560480215720684}, 'hendrycksTest-college_mathematics': {'acc': 0.17, 'acc_stderr': 0.0377525168068637, 'acc_norm': 0.3, 'acc_norm_stderr': 0.046056618647183814}, 'hendrycksTest-high_school_biology': {'acc': 0.33225806451612905, 'acc_stderr': 0.02679556084812281, 'acc_norm': 0.3096774193548387, 'acc_norm_stderr': 0.026302774983517418}, 'hendrycksTest-high_school_statistics': {'acc': 0.2777777777777778, 'acc_stderr': 0.030546745264953202, 'acc_norm': 0.2777777777777778, 'acc_norm_stderr': 0.030546745264953202}, 'hendrycksTest-machine_learning': {'acc': 0.2857142857142857, 'acc_stderr': 0.04287858751340456, 'acc_norm': 0.25, 'acc_norm_stderr': 0.04109974682633932}, 'hendrycksTest-high_school_psychology': {'acc': 0.3100917431192661, 'acc_stderr': 0.019830849684439756, 'acc_norm': 0.26055045871559634, 'acc_norm_stderr': 0.018819182034850068}, 'hendrycksTest-international_law': {'acc': 0.4132231404958678, 'acc_stderr': 0.04495087843548408, 'acc_norm': 0.5867768595041323, 'acc_norm_stderr': 0.04495087843548408}, 'hendrycksTest-moral_scenarios': {'acc': 0.25139664804469275, 'acc_stderr': 0.01450897945355396, 'acc_norm': 0.27262569832402234, 'acc_norm_stderr': 0.014893391735249588}, 'hendrycksTest-world_religions': {'acc': 0.39766081871345027, 'acc_stderr': 0.03753638955761691, 'acc_norm': 0.42105263157894735, 'acc_norm_stderr': 0.037867207062342145}, 'hendrycksTest-professional_psychology': {'acc': 0.3104575163398693, 'acc_stderr': 0.018718067052623227, 'acc_norm': 0.28104575163398693, 'acc_norm_stderr': 0.018185218954318082}, 'hendrycksTest-moral_disputes': {'acc': 0.2774566473988439, 'acc_stderr': 0.024105712607754307, 'acc_norm': 0.3265895953757225, 'acc_norm_stderr': 0.025248264774242826}, 'hendrycksTest-high_school_macroeconomics': {'acc': 0.3128205128205128, 'acc_stderr': 0.023507579020645344, 'acc_norm': 0.2948717948717949, 'acc_norm_stderr': 0.02311936275823229}, 'hendrycksTest-professional_accounting': {'acc': 0.25177304964539005, 'acc_stderr': 0.0258921511567094, 'acc_norm': 0.24822695035460993, 'acc_norm_stderr': 0.025770015644290396}, 'hendrycksTest-abstract_algebra': {'acc': 0.18, 'acc_stderr': 0.03861229196653697, 'acc_norm': 0.21, 'acc_norm_stderr': 0.040936018074033256}, 'hendrycksTest-conceptual_physics': {'acc': 0.31063829787234043, 'acc_stderr': 0.030251237579213167, 'acc_norm': 0.2127659574468085, 'acc_norm_stderr': 0.026754391348039773}, 'hendrycksTest-business_ethics': {'acc': 0.44, 'acc_stderr': 0.04988876515698589, 'acc_norm': 0.35, 'acc_norm_stderr': 0.047937248544110175}, 'hendrycksTest-miscellaneous': {'acc': 0.39208173690932313, 'acc_stderr': 0.01745852405014764, 'acc_norm': 0.30395913154533843, 'acc_norm_stderr': 0.016448321686769046}, 'hendrycksTest-high_school_chemistry': {'acc': 0.2561576354679803, 'acc_stderr': 0.030712730070982592, 'acc_norm': 0.30049261083743845, 'acc_norm_stderr': 0.032257994762334825}, 'hendrycksTest-global_facts': {'acc': 0.22, 'acc_stderr': 0.0416333199893227, 'acc_norm': 0.22, 'acc_norm_stderr': 0.0416333199893227}, 'hendrycksTest-high_school_physics': {'acc': 0.2847682119205298, 'acc_stderr': 0.03684881521389023, 'acc_norm': 0.2847682119205298, 'acc_norm_stderr': 0.03684881521389023}}, 'versions': {'hendrycksTest-high_school_world_history': 0, 'hendrycksTest-elementary_mathematics': 0, 'hendrycksTest-computer_security': 0, 'hendrycksTest-formal_logic': 0, 'hendrycksTest-public_relations': 0, 'hendrycksTest-clinical_knowledge': 0, 'hendrycksTest-college_computer_science': 0, 'hendrycksTest-professional_law': 0, 'hendrycksTest-security_studies': 0, 'hendrycksTest-high_school_computer_science': 0, 'hendrycksTest-human_aging': 0, 'hendrycksTest-college_biology': 0, 'hendrycksTest-high_school_us_history': 0, 'hendrycksTest-electrical_engineering': 0, 'hendrycksTest-professional_medicine': 0, 'hendrycksTest-virology': 0, 'hendrycksTest-management': 0, 'hendrycksTest-anatomy': 0, 'hendrycksTest-human_sexuality': 0, 'hendrycksTest-medical_genetics': 0, 'hendrycksTest-high_school_government_and_politics': 0, 'hendrycksTest-college_physics': 0, 'hendrycksTest-nutrition': 0, 'hendrycksTest-high_school_geography': 0, 'hendrycksTest-jurisprudence': 0, 'hendrycksTest-sociology': 0, 'hendrycksTest-us_foreign_policy': 0, 'hendrycksTest-logical_fallacies': 0, 'hendrycksTest-marketing': 0, 'hendrycksTest-astronomy': 0, 'hendrycksTest-college_medicine': 0, 'hendrycksTest-high_school_microeconomics': 0, 'hendrycksTest-high_school_mathematics': 0, 'hendrycksTest-philosophy': 0, 'hendrycksTest-high_school_european_history': 0, 'hendrycksTest-prehistory': 0, 'hendrycksTest-econometrics': 0, 'hendrycksTest-college_chemistry': 0, 'hendrycksTest-college_mathematics': 0, 'hendrycksTest-high_school_biology': 0, 'hendrycksTest-high_school_statistics': 0, 'hendrycksTest-machine_learning': 0, 'hendrycksTest-high_school_psychology': 0, 'hendrycksTest-international_law': 0, 'hendrycksTest-moral_scenarios': 0, 'hendrycksTest-world_religions': 0, 'hendrycksTest-professional_psychology': 0, 'hendrycksTest-moral_disputes': 0, 'hendrycksTest-high_school_macroeconomics': 0, 'hendrycksTest-professional_accounting': 0, 'hendrycksTest-abstract_algebra': 0, 'hendrycksTest-conceptual_physics': 0, 'hendrycksTest-business_ethics': 0, 'hendrycksTest-miscellaneous': 0, 'hendrycksTest-high_school_chemistry': 0, 'hendrycksTest-global_facts': 0, 'hendrycksTest-high_school_physics': 0}, 'config': {'model_args': None, 'num_fewshot': 0, 'limit': None, 'bootstrap_iters': 100000, 'description_dict': None}}
[2025-03-25 11:23:41 root] (main_calib_config2.py 202): INFO Average accuracy 0.2830 - STEM
[2025-03-25 11:23:41 root] (main_calib_config2.py 202): INFO Average accuracy 0.3199 - humanities
[2025-03-25 11:23:41 root] (main_calib_config2.py 202): INFO Average accuracy 0.3376 - social sciences
[2025-03-25 11:23:41 root] (main_calib_config2.py 202): INFO Average accuracy 0.3223 - other (business, health, misc.)
[2025-03-25 11:23:41 root] (main_calib_config2.py 204): INFO Average accuracy: 0.3125
